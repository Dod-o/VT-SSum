{
    "id": "c352kuualjaosyuyd4lsauaetq76ck3y",
    "title": "From Language Modelling to Machine Translation",
    "info": {
        "author": [
            "Phil Blunsom, Department of Computer Science, University of Oxford"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_blunsom_machine_translation/",
    "segmentation": [
        [
            "So yeah, I think today is all about natural language processing, which would be fun.",
            "So this talk, I can't remember what was actually in the schedule, but this will actually be is sort of a journey from language modeling, which I think has been mentioned a few Times Now in the summer school, but will go a little bit deeper into it and also start out in a very sort of traditional view.",
            "So Jenny from language modeling and then how we can we can transition from modeling sentences to doing things like machine translation.",
            "OK."
        ],
        [
            "So hopefully by now you've sort of got a rough idea of what a language model is.",
            "Language model is language.",
            "Modeling is a term that's used sort of both informally and formally.",
            "In NLP, I think a formerly a language model is something that that assigns a joint probability to a sequence of words.",
            "People often just use it for anything.",
            "That process of language in classifiers or something like that.",
            "But when you say language model, I think of something that takes a sentence like the House is small and assigns a probability to that a joint probability.",
            "And we can do that in a well defined way.",
            "We could the space of utterances, a space of sentence is infinite.",
            "You can keep adding words forever, but we can still have a well defined joint probability.",
            "And if we have that, the sort of traditional view is that we can do lots of things like comparing the probabilities of sentences.",
            "And that can tell us a bit about what a more likely ordering of words is, so this is.",
            "I'm really became first most useful in speech recognition.",
            "We have this problem of of trying to assess whether your output is likely in some sense.",
            "So language model is crucial there.",
            "And so you can.",
            "You can use it to to assess choice, lexical choice, but also ordering.",
            "And that's then used in translation, but lots of other things you can use.",
            "You can use the language model to classify if you have text documents in multiple classes, you can build a language model for each class and then given some text you can just ask which language model most probably produces text.",
            "So all sorts of things you can do with."
        ],
        [
            "For the language model.",
            "So.",
            "This is not really a standard view and something I just sort of throw in for this talk, but now we're starting to see that.",
            "Lots of tasks in NLP you can think of as language modeling problems or assigning probabilities to sequences of words and symbols.",
            "It's easy to see how machine translation fits this paradigm rather than just modeling sentences in one language, we can just concatenate two sentences, one in French and one in English.",
            "And if we can model the probability of that sequence, then we can do translation.",
            "Obviously, in translation we're interested in the conditional output, but we can think of it this way in language modeling.",
            "We can go further and think of things like question answering and dialogue where we have some sort of natural language input and we want to produce natural language output.",
            "Again, we can just think of these as a sequences.",
            "I model them with joint probabilities and people are doing this now and one crucial thing is, once you get to things like question answering in dialogue like this, you really need some.",
            "You need to not forget about the sort of world world knowledge and such.",
            "It also has to be conditioned on.",
            "But this is sort of.",
            "An idea that maybe maybe if we can just get language modeling right then we can solve lots of other problems.",
            "And from.",
            "From my point of view, deep learning and NLP you see lots of things like classifications, lots of work on sentiment and word embeddings and things like that, where I think it's having the most impact is in these sorts of things where deep learning now allows us to model much more complex joint distributions over sequences.",
            "And thus generate language conditioned on complex information.",
            "And that's one thing that deep learning is really given us, that that wasn't so much there before most of the other things are really just refinements on on existing models.",
            "OK, so language."
        ],
        [
            "Modeling.",
            "So, at least from my knowledge, really, when people got interested in statistical language modeling was in the Second World War, when people like Alan Turing and IJ good on the right there were trying to.",
            "Decript German communications and if you're going to do this from traditionally at that time, people would do decryption using linguists.",
            "Basically, they try and treat it like a translation problem.",
            "These guys thought of it more of a statistical problem.",
            "They tried to model it as.",
            "An information theoretic decryption problem.",
            "And in doing so, they realized that.",
            "If they wanted to decrypt German, they needed to know something about the.",
            "Statistical irregularities of German.",
            "So they put a lot of thought into how do we actually model frequencies of words and sequences of words?",
            "And that was really the start of the idea of language modeling and they came up with some some good ideas that persist today."
        ],
        [
            "So, formally, what we're interested in is we have our sequence that we might call big W there a sequence of words.",
            "And we want the joint distribution over that.",
            "Of course we are interested in the order as well.",
            "The order is important.",
            "We can have arbitrary length.",
            "Strings, we just have some special end of sequence string that we also have to include in our distribution.",
            "So obviously just modeling a big joint distribution is is hard, so we want to decompose it.",
            "The product rule tells us that we can just decompose the joint into a series of conditionals.",
            "This is exact.",
            "There's no approximation here.",
            "So that tells us that we don't need to model the joint if we can just model conditional probabilities so it can predict the probability of a word given a history.",
            "Then we good.",
            "Now, doing this is pretty hard because this is some arbitrary length string.",
            "To condemn."
        ],
        [
            "So the traditional approach and still by far the most popular is the so-called N gram approach to language modeling, where we just assume that most of the history doesn't matter and we just keep the most recent things we see.",
            "So a bigram model, as in 2 words.",
            "Just conditions WN on WN minus one.",
            "So we now we now approximating the joint.",
            "With just these bigram conditionals.",
            "And this is the.",
            "This is the basic idea behind all of the traditional language modeling work.",
            "The idea of N grams where the depending on how many of these words you condition on the simplest, is a bigram you can go to Tri grams or more.",
            "We get different powers and models.",
            "So then.",
            "In this view, we've reduced the problem to modeling these conditionals.",
            "So now what we just care about is modeling the probability of a word given the previous word or the previous couple of words."
        ],
        [
            "So.",
            "The obvious thing to do is to start with maximum likelihood, and that's the easiest idea.",
            "Now you have.",
            "So the first thing you have to do is collect lots of data.",
            "Today that's easy and back when people first started doing this that wasn't so easy.",
            "They didn't have so much machine readable text.",
            "Now, of course you can get billions of words in most languages, trillions in English off the web.",
            "And it's very easy to estimate a maximum likelihood by gram conditional simply by counting the number of times you saw those two words together and dividing by the number of times you saw the the word conditioned on.",
            "So that straightforward problem is maximum likelihood.",
            "Rarely ever works so."
        ],
        [
            "Something to keep in mind for the later on.",
            "OK, so we've now got a basic idea for how we might estimate a simple language model.",
            "We also need some way of assessing whether it works, so the classic assessment, the intrinsic evaluation.",
            "Of course, the best evaluation for your language model is stick it into your speech recognizer or empty model and see how it performs.",
            "But if you want some intrinsic way.",
            "Evaluating your language model, you can just look at the probability @ assigns tenyu sequences.",
            "And so we can do that with the cross entropy at the top there.",
            "Or if we raise it to the power of two, we get something called the perplexity, and this is what you'll see most often in language modeling.",
            "People report this quantity.",
            "So you train your language model, you collect your counts on a few billion words of text.",
            "You have some held out test data that doesn't overlap with that, that training data.",
            "And then you look at your model and you see what probability at the signs to those strings.",
            "And you calculate your perplexity and that gives you a measure of how good it is.",
            "OK, and obviously you want your perplexity to go to one if you have a perplexity of 1, then your models knows the future.",
            "Given a word, it knows exactly what's going to come next.",
            "So this so the other thing here is to realize that this connects language modeling with compression, so language modeling and compression are basically the same problem, so that is compressing text or files.",
            "You want to have a model of the data that is as sure as possible about predicting what can come next year."
        ],
        [
            "Turn any language model into a compression.",
            "Any language model of this sort into a into a compression algorithm?",
            "OK, so if we actually look at some data so this is.",
            "So there's a very popular set of corpora called the Europower corpus, which is European Parliament proceedings.",
            "Across many languages, and if we look at if we collect unigram bigram and trigram and four gram counts from this corpus, and then look at a sentence and look at each of the conditional.",
            "Probabilities that assigns to each each word in the sequence given the previous ones.",
            "You get an idea of how these models behave.",
            "So you see basically this pattern that as you add more conditioning context, things get better.",
            "Do they always get better?",
            "I think there's one that does that.",
            "OK, that one gets worse.",
            "Mostly they get better.",
            "And you see that the probabilities vary a great deal through the sequence.",
            "So for instance, predicting the end of sentence symbol is easy.",
            "So given that you saw a full stop figuring out that the sentence is probably going to end is not very hard.",
            "If you look at things like so that the hardest thing was up here, so commend.",
            "So I would like to.",
            "If you ask if you say I would like to announce someone to tell you what word would come next, they're not going to have much idea, so there's a lot of words that come next there, and so the model doesn't have much idea about command.",
            "But once it sees command, it has a pretty good idea.",
            "The next word is going to be determined or like that.",
            "So you get this pattern through the through the data, but the key.",
            "The key thing that becomes obvious quickly is that.",
            "Higher engrams are better but only with."
        ],
        [
            "Then within reason.",
            "So the obvious issue is that if you do this sort of count based estimation.",
            "Play."
        ],
        [
            "Yeah, so so you assume that you know there's some start of sentence that you're conditioning on.",
            "So if you look at the unigram probability, that's just the probability of at any point in a sentence using the word I, the bigram probability is that, given that you know the previous thing was nothing to start of the sentence probability of by.",
            "So that's the probability that I will be the first word in the sentence, so that drops quickly, 'cause that's.",
            "Eyes are pretty good way to start a sentence.",
            "The trigram infogram give you know more information because once you know that it's the start of the sentence, there's nothing more to know, so that's why they're exactly the same probability.",
            "And you'll see the same pattern for wood, so the bigram trigram picks up start.",
            "I would wear as a bigram is just picking up, I would."
        ],
        [
            "OK, so the first problem you're going to run into is once you start counting trigrams and four grams, you're not going to see that you're not going to see them that often.",
            "So here we have the example.",
            "I like to smooth.",
            "You may never have seen that.",
            "Full gram in your corpus you assign at zero probability.",
            "Remember the probability of the whole string is the product of the probabilities of each conditional, so anything having zero probability is bad, because in the whole string has zero probability.",
            "So.",
            "Higher order N grams are good, but until you stop seeing them then they're bad.",
            "So you need to deal with this somewhere."
        ],
        [
            "So the most simplistic method in language modeling in popular across lots of other sort of multinomial based machine learning methods is what's called.",
            "Add one smoothing.",
            "So rather than just the raw counts, we just add 1 to every account.",
            "That way will never have a count of 0.",
            "And then we had to have to add the number of words in our vocabulary to the denominator as well.",
            "So that's that's a trivial smoothing.",
            "This is very popular for other sorts of anyway, and people are using multi normals.",
            "And this is equivalent to some sort of display multinomial model.",
            "If you're Bayesian with a parameter of two problem is, this really doesn't work for most interesting cases.",
            "So the problem is that V. If we're talking bout by grams, is the space of all all possible pairs of words.",
            "So if you have sort of 90,000 words in your vocabulary, you have the square of that the number of bigrams.",
            "That's a lot of bigrams so that V is going to be very big.",
            "So what happens is you going to radically underestimate the probability.",
            "Of."
        ],
        [
            "Not seeing any particular bigram, so that's bad.",
            "So let's do something slightly more clever and have add Alpha smoothing so we're Alpha is less than one.",
            "So now we can add a small amount.",
            "Doesn't have to be as big as one for each word.",
            "So that works."
        ],
        [
            "Better now, if we look at.",
            "Sort of real frequencies from the europower corpus again.",
            "So.",
            "This is the count of the number of times a word might have occurred in the training corpus.",
            "This is the adjusted count.",
            "If you do add one smoothing.",
            "This is basically the what the.",
            "They add one smoothing model, thinks the frequency of the word will be in a held out corpus, or the probability of it in holdout corpus.",
            "This is at Alpha.",
            "We were going to tune Alpha to be optimal by looking at the test set.",
            "And here's the test account, so this is on some held out data, the probability.",
            "So if things that you never saw, the bigrams you never saw in the training time.",
            "So they have this probability in the test set.",
            "So you'll always see things you never saw before.",
            "Now if you look at these two columns, clearly add one smoothing is bad, it radically underestimates the probabilities of especially frequent things.",
            "If we if we look at the test set that we can tune this Alpha to try and make these match and we can get quite close.",
            "You see that we're still so singletons words.",
            "It occur once in orbit, bigrams that occur once in the training data are roughly overestimated by twice the probability of seeing them again.",
            "So the model is very overconfident, so even when we're tuning on the test set.",
            "So it gives you an idea about frequency, so we want to do this better and we want to do it without cheating.",
            "So this is this is cheating to look at the test set and workout what the best value of Alpha is."
        ],
        [
            "So this is where.",
            "Alan Turing and IJ good came in.",
            "So they looked at this problem and tried to work out.",
            "Given that I've seen a word a certain number of times in my sample, how?",
            "How likely am I to see it again in a holdout sample?",
            "And.",
            "There's lots of math behind this, this estimate and what they what they tried to do is estimate the expected probability of a word that occurs a certain number of times at training time.",
            "The expected underlying probability of it in the model.",
            "So starting out with this, you can do a whole lot of derivations.",
            "You have to do a few approximations on the along the way and you end up with this estimate.",
            "And what this estimate says is that these are is our account from our training data.",
            "These capital end their account of count, so that's a number of of things that we saw that in many times the number of things we thought our times or number of things we saw our plus one times.",
            "So basically take this ratio of the number of things we saw.",
            "Plus one times divided by number of things we saw our times scale it a little bit and we get what's called the good Turing estimate, and that's an approximation of this quantity.",
            "So I'll give a reference later on where you can find the full derivation of that.",
            "This turns out to be pretty effective and, well, the first thing to note is that this is this is going to be less than one, so the number on the top will be smaller than number on the bottom, so."
        ],
        [
            "So your adjusted count will be less than the observed count, which is basically what we want if we see it.",
            "If we see something once, then we'll probably see it less frequently in the test data.",
            "Just 'cause we have a poor estimate.",
            "So again if we look at the frequencies here are counter counts.",
            "The number of things that occurs 0 times user by grams.",
            "You see most things we never see most things.",
            "This is the key problem in language modeling.",
            "As we get more frequently, see less and less of these.",
            "Here are the good touring estimates.",
            "Here are the test accounts.",
            "When you see that that's pretty good.",
            "That's a pretty good match for the frequency, so those guys were pretty smart.",
            "So that gives you a way of basically guessing from my sample, how likely am I to actually see this?",
            "In the future my test sample so that I don't overestimate."
        ],
        [
            "So now we have a way of adjusting our counts, but we still have this problem of zeros.",
            "So if we have these admittedly very rare trigrams Montreal, Poutine eaters or drinkers, and we want to know which ones more likely if they both just we've not seen either of them ever before.",
            "So we're just going to say both 0, but we may have seen Poutine eaters quite a few times.",
            "We may have seen this by Graham, and we may have never seen a poutine drinker, because it's just that doesn't make much sense.",
            "So this is where the idea of backoff comes.",
            "If we haven't seen the trigram or the higher order Ngram ever before, or frequently enough.",
            "Then why don't we look at how frequent the bigram is as a way of smoothing or adjusting our estimate?",
            "This is what's called back.",
            "Often is the key."
        ],
        [
            "Idea in traditional language modeling.",
            "And this is hard back off, so this this idea is that OK if we want to estimate the probability of some ngram here.",
            "If if we've seen that in Gram.",
            "Then we're going to use some adjusted adjusted probability of it.",
            "And if we haven't seen it, then we're going to back off to a lower order Ngram.",
            "So we're going to drop one of the words were conditioning on, so we're going to go from a trigram to a bigram, and then we're going to use its probability.",
            "And these values out that these values have to be adjusted so the distribution still sums to one.",
            "So the Alpha up there is going to be some quantity slightly less than the maximum likelihood probability, and then we get D, which we call the discount.",
            "Which is the left over probability, so.",
            "So some probability leftover and we can assign that to the lower order ngram.",
            "So then we have to make decisions about how we should set these weights.",
            "Essentially, these backoff backoff weights.",
            "Sir."
        ],
        [
            "Given what we saw previously.",
            "The obvious thing to do is to use good Turing estimate, so this is what's called good during smoothing N gram language models.",
            "So we just use good Turing estimate, so there's our maximum likelihood.",
            "As I said, the good Turing estimate is going to be less than the.",
            "The observed count.",
            "So we replace the observed count with the good Turing count that gives us our Alpha as it's less than there's going to be some probability mass leftover here.",
            "We assign that to D, the discounting function, and we're good.",
            "And we do that through all the engrams.",
            "And so that's a simple N gram language model.",
            "It's pretty effective.",
            "It's extremely easy to estimate and scale.",
            "These things are incredibly scalable.",
            "It's not to do this on a trillion words."
        ],
        [
            "But language is more interesting than that so.",
            "People who spend a lot of time thinking about language modeling and made various observations and two key ones are this idea of diversity in amongst out in grams.",
            "So in this example we have the words Spuyten constant, so we're predicting.",
            "So, given that we say spite.",
            "We'll almost certainly see of afterwards.",
            "'cause that's just a very frequent phrase, and we're very unlikely to see anything else.",
            "Where is constant?",
            "We could see all sorts of other things.",
            "So we don't really want to.",
            "Back off in the same way for these two different words that we're conditioning on.",
            "So given what we are conditioning on, the probability of seeing something new is is different, and so we want to adjust for this.",
            "So I'm not going to go through the whole ways of doing this, but as something called within Bell smoothing which does this, it tries to adjust for the diversity of predicted words.",
            "Again, these are all in the references."
        ],
        [
            "If you want to know how this actually works, so the reverse of that idea is the diversity of histories.",
            "So if I see the word York.",
            "And so York is quite a frequent word.",
            "In Europe, all it's a political corpus.",
            "You're quite often see the word York.",
            "It's almost always preceded by knew.",
            "As in the reference to the City New York.",
            "It's a frequency.",
            "There's lots of other words, food as examples.",
            "There foods indicates these words have far less restrictive of the words that come before them.",
            "So again, we want to take this diversity into account and this is what Canessa nice moving does.",
            "It takes that idea.",
            "That you shouldn't.",
            "So if you look at your unigram probability of York, it's it's going to radically overestimate the probability of seeing the word York unless you saw the word knew before it.",
            "So you want to adjust for that.",
            "That's connection ice moving."
        ],
        [
            "OK.",
            "So again, there's a big derivation behind Canadian ice moving.",
            "It's quite interesting.",
            "Lots of analysis of it, and there's been various connections made to various Bayesian nonparametric models of language.",
            "It's also the most popular approach to language modeling, and here are some some rough.",
            "Testing perplexities, for these different smoothing methods.",
            "So there's lots of variants of connection, either something called, modified and interpolated.",
            "That give you different performance.",
            "I won't go into those, but.",
            "The general pattern is that the connection is a very good ngram smoothing method, and that's what pretty much everyone uses.",
            "Even good Turing smoothing is pretty effective considering how simple it is.",
            "So interpolated connection.",
            "I just interpolates the engrams rather than doing this hard back off.",
            "And that that tends to be a little bit more effective.",
            "Especially at the higher order ngram.",
            "So if you're if you're sort of, if you're building a traditional speech recognition algorithm or machine translation system, or any of these things, one of the first things you are going to do is collect huge amounts of monolingual data.",
            "Billions of words, preferably and estimator.",
            "Can Essanay language model on that, and that will be your.",
            "Your language model in speech people tend to use relatively short histories, often just trigrams in machine translation.",
            "People use much longer histories depending on.",
            "Often you'll go out to five or 6 grams on.",
            "On some sort of selected smaller amount of data and four grams on a big amount of data.",
            "So one of the problems with the N gram language model is that once you get the four grams is a lot of them.",
            "So if you have a billion or a few billion.",
            "Words, there's going to be a lot of different 4 grams, so there's going to be.",
            "You're going to need a lot of memory to store these sorts of models."
        ],
        [
            "OK, so that's traditional engram language money to give you an idea of of really whether the traditional state of the art is in N gram language modeling.",
            "These things are extremely scalable, they're just very easy to estimate on massive amounts of data.",
            "There are very effective in these in existing speech recognition and translation models such that the most important part in in, say, a translation model currently is the language model.",
            "If you want to make the if you want to make your translation model better, often getting more language modeling data, especially in domain data, is that the fastest way to do it.",
            "So if you want to know more about this sort of stuff, this the Shannon Goodman references an excellent survey of all of these different techniques and has all the derivations."
        ],
        [
            "OK, so why would we want to do anything else than that?",
            "So the problem with these count based traditional ngram models, the main there's two main problems.",
            "One is they take a very symbolic view of of words, so so words are just symbols.",
            "I mean they they turn out being integers in your implementation, but they don't have a relationship to each other, so we have no relationship between dog and cat and words like that that we think might be related.",
            "So we don't capture semantic relationships.",
            "We also don't capture morphological relationships, so learning that after in a particular context you're likely to see a verb or that after an adjective with a particular gender, you're likely to see a noun with a matching gender, and all these sorts of things.",
            "That pattern is not learned.",
            "You have to see each individual combination.",
            "So that means that that ngram models at I'd say, data inefficient.",
            "You need lots of data to get good engram estimates, and especially if you have a morphologically rich language, because you're going to have lots of different ways of realizing your underlying words, and you have to see each one of them to get good counts.",
            "And the other thing is that because of this ngram assumption, we don't capture long range dependencies, so that's bad for modeling things like the syntactic structure of a sentence.",
            "You're not going to capture that, but it's also bad if you start wanting to extend this idea to the sorts of tasks I talked about earlier where we just wanted to do something like translation as a big joint sequence model, then that approach is going to have very long range dependencies because you're predicting some word 40 words into your translation, which is dependent on the word it's translating.",
            "40 words back in the input translation, so an ngram model is never going to be able to do that sort of thing.",
            "OK."
        ],
        [
            "So that leads us into probably somewhat more comfortable ground for everyone, which is neural networks.",
            "So.",
            "The classic neural network language model neural network approach to this idea is from Yoshua from about 12 years ago now.",
            "And the idea is simple.",
            "Let's just feed the we're going to stick with the N gram approach to language modeling on a fixed window.",
            "We're going to predict the next word, but now, rather than just storing counts, were going to feed the context words into a feedforward neural network and predict the next word with a big softmax.",
            "So that's where we're heading, but we're going to."
        ],
        [
            "Rewind a bit further back in history.",
            "So after.",
            "Sort of early boom in speech recognition and machine translation.",
            "Early 90s people got very excited about what they called log linear language models in the in the late 90s and this is getting halfway between our traditional ngram model and our newer model.",
            "And these are just based on log linear models or Maxim models as they are also called at the time and these are just a softmax classifier, so we assume that we take out.",
            "So this is just a classification version.",
            "We have some features, so in this case we just.",
            "Assume that they are given to us by some feature detection functions and then we are predicting some class given some context X using our softmax.",
            "So this should be all pretty familiar.",
            "And when we when we grind through the gradient for this we see.",
            "So this is the log negative log likelihood here.",
            "We see the pattern we always see with this sort of models that we, the gradient is a difference of the features that we observe in our data and the expectation over the features that the model is giving us.",
            "So we try and make these things match."
        ],
        [
            "So that log linear classification and we can turn this into a language model very simply by just defining features over our ngram contexts.",
            "So the obvious thing to do is just to have a feature for every engram we might count in our traditional ngram model, so that's so if we got this, this trigram, this is America, we could have a feature function Phi here, that fires is has the value one when it sees the bigram context, this is.",
            "So that's exactly like.",
            "The weights on that are going to be learning our distribution for that that we would learn for that trigram.",
            "If we're counting the reason we might want to take this log, linear approaches.",
            "We can also have features which don't correspond to such engram, so we can just look at the word N -- 2 ago or N -- 1 just obviously equivalent to the bigram, but this has no equivalent in the traditional ngram model, so now we're overcoming a bit of the sparsity.",
            "We can say that we're going to predict the word given the word two ago.",
            "And skip the one in between.",
            "So the the reason people got excited about log linear language modeling is it just gives you a lot more flexibility in how you define the features.",
            "And you can have all sorts of things you can condition on the if you have part of speech tags, syntactic categories for the words you can condition on that you can get generalization across things like.",
            "Context that nouns will occur in versus verbs, any sort of morphological analysis.",
            "So now you have a lot more flexibility.",
            "And you end up with a model defined something like this."
        ],
        [
            "OK.",
            "So I'm going to take a little sort of journey towards a neural language model.",
            "So if we start out with our log linear model and we get rid of this traditional trigram feature and just keep these features which independently look at the context words, so then we can factor our our model based on these.",
            "Now we have a different feature function for each word in the context, so that we're going to make that assumption.",
            "I'm going to see where this takes us."
        ],
        [
            "So the first thing we want to do is we want to get rid of this these feature functions.",
            "We don't want to have to hand to find them.",
            "It would be nice if we could just learn these things.",
            "So what we're going to do is say that each word in our context has some representation in a vector, and we're going to get those from a matrix.",
            "Q.",
            "So Q is just going to index all out all of our context words.",
            "And I'll feature functions are just going to apply transformations.",
            "The square matrices there C -- 2 and C -- 1 to each of these context words and give us some vector which is going to be our five features.",
            "So now rather than finding our features, we're going to define.",
            "Basically, this is a BI linear relationship too.",
            "Now what I call the predicted features."
        ],
        [
            "And if we stick that into our model, we end up with something like this, and this is what's called the log by linear language model.",
            "I think Russell mentioned this in his talk, but this is what it actually looks like, sort of unpacked formally.",
            "So.",
            "So yes, we just got this bilinear relationship between the context and the predicted word.",
            "This is very simple.",
            "These models are actually very effective.",
            "They're not very popular, but they are extremely effective.",
            "They train really quickly because not having a nonlinearity in there means that you can train them really quickly, and they're pretty much as good as some of the more complicated neural language models, but there are very good simple.",
            "Model so I think I'm."
        ],
        [
            "OK, so I actually got so if we quickly go through the derivation of the gradients I won't labor this.",
            "But the idea is that if you if you do all this, you see that what's happening is that the representation for networking representation for the word you're predicting.",
            "Gets.",
            "Gets pushed to match the representation for the predicted context.",
            "And."
        ],
        [
            "Yeah so.",
            "And this is just like what we saw with the log linear model."
        ],
        [
            "And the.",
            "The representations of the context words get pushed to match the representation of the word.",
            "There, they're predicting modulo the transformations.",
            "So basically the context in the word predicting is sort of being pushed to match each other and."
        ],
        [
            "So why you get this?",
            "This idea of these models learning representations of the words that match their context?",
            "OK, and we can go through the same for the transformation matrices.",
            "OK, so.",
            "Now we want to make this a bit more interesting.",
            "Very simply, if we just wrap our context transformation in some nonlinear function, like a sigmoid, tanh H rectified linear unit, then what we have is what you might call a proper neural language model.",
            "So that's all we have to do, very simple we just stick F in there.",
            "Now we've got a nonlinear relationship."
        ],
        [
            "The gradient hardly changes thanks to the beauty of the chain rule and back propagation.",
            "So those equations from previous slides are just unpacking backpropagation, so that's very simple going nonlinear is relatively trivial.",
            "Downside is your models now much slower to train, but."
        ],
        [
            "Does work slightly better.",
            "OK, so that's that's how we can transgo make this journey from traditional N gram language models into a sort of neural distributed representation language model.",
            "But we still have this problem that we're making the ngram assumption, so we're still cutting off our history and we're ignoring things way in the past and we don't want to do that if we want to do complex.",
            "More complex joint distributions.",
            "So the obvious thing to do is to make the transition to recurrent neural networks.",
            "Ann, I think these have been talked about quite a bit through the summer school, but the idea is is now rather than conditioning on a fixed context, we just keep the previous hidden layer around and we just currently use it to predict the next word.",
            "So now, so that's in here.",
            "So this is our hidden layer from our previous time step.",
            "We transform it.",
            "We take the previous word, we feed that in an.",
            "We just do a prediction in the same way.",
            "So simple recurrent language models like this don't work.",
            "There's been plenty of papers saying they do work, but they don't.",
            "Ngram models work better.",
            "The problem is as probably being talked about is the idea of the vanishing gradient problem.",
            "Recurrent neural networks just have a great deal of trouble propagating gradient signals very far, and generally they propagate them less far than you can just condition on with an N gram and thus get a more representative model.",
            "So straight out sort of order what I'd call it a vanilla recurrent model is not really a great idea for language modeling, but there are plenty of ways of making this a better idea.",
            "In the Classic One that's very popular at the moment.",
            "Is the STM which gets around this problem.",
            "So."
        ],
        [
            "So that's roughly a sort of.",
            "Not very informative diagrammatic representation of a recurrent network."
        ],
        [
            "What is feeding the hidden layers in so to train these things, they're very simple to train.",
            "We just unroll the recurrent network overtime over our sequence and then treat it like a big feedforward network.",
            "There's a couple of there's some complexities in that idea in that if you have a very long sequence, say you want to model all of your language.",
            "Modeling data is 1 big sequence, and you're not going to be able to do that in memory, so you want to cut it up into smaller sequences.",
            "Referred to as backpropagation through time, so you can just cut your.",
            "Cut your long sequence up into fixed length equal to maybe 20 time steps and do backpropagation through those and then pass the forward values onto the next segment.",
            "Or if you're modeling fix, I think maybe you're modeling sentence is then you can just do the full full unrolling of the whole sentence so you're doing translation.",
            "You can unroll across the full sequence and back propagate across the full sequence.",
            "The complexity there becomes.",
            "To do this efficiently, you want to do it on a GPU.",
            "You then want to pack these sequences into a mini.",
            "Batch sentences have different lengths, so your mini batch is no longer going to be square and then you'll have some interesting engineering problems to get the optimal.",
            "Performance through that, but the basic idea is simple that you can just unroll."
        ],
        [
            "Unroll your current network and treat it like a big feed feedforward network.",
            "OK, so.",
            "So I think I was told that no one had really gone through the STM equations at all, so I thought I'd throw them in there.",
            "Not entirely informative, but they're not as crazy as they look, so, so the LM is this idea of a better recurrent cell that's going to get around this problem of the gradient disappearing and the way it does.",
            "That is what's highlighted in red here, and this is the key thing.",
            "So C is sort of the, although we have a H down here, you can really sink C as the recurrent state that's going to be preserved through time.",
            "And rather than multiplying the key thing here is that we're going to some, so that's really the key idea in the STM.",
            "Rather than multiplying overtime, we're going to some and some other nice and linear.",
            "They propagate gradients very well.",
            "But we can't just some, because then we'll get a very boring linear model that's not really going to model anything interesting is just going to decompose.",
            "So we need to produce to put back some nonlinearity, and we do that with these gating functions.",
            "So we start out with this.",
            "Starting ideas we want to some rather than the multiply we need to make some nonlinearity.",
            "So what we're going to do is rather than just mixing the previous state and this big thing, here is just our sort of information from our next.",
            "Next input.",
            "So rather than just summing these, we're going to sort of interpolate the mixom according to some gating functions F, which we call forget, which just means how much of the previous state do we do.",
            "We keep and eye, which is the input gate, which is how much of the input information do we pass into the state.",
            "Both of these are just defined a sigmoid function, so there between zero and one, so they're just scaling functions are at these these two things here are just multiplying by some value less than one, so they just down waiting each of these.",
            "And that's that's really.",
            "If you start there, the LM is relatively simple.",
            "We just want to some.",
            "We need to.",
            "Therefore we need to get our previous information now in put in some way.",
            "Once we go down this gating sort of idea, we might as well get the output as well.",
            "Why not?",
            "And.",
            "In that vein, this is just one instantiation of the STM.",
            "There's lots of different variants of this.",
            "People connections, and various things.",
            "This ones relatively simple and also easy to optimize on a GPU, because if you look closely, all of these gates have very similar.",
            "Very similar transformations here, so you can pack these all together.",
            "OK, so that's what an STM."
        ],
        [
            "Looks like that's my rough diagrammatic output, I've just I've not shown at all what these gates are being conditioned on, but they just modulate the previous history, the input, and the output."
        ],
        [
            "For completeness, I mean it's cool now to have deep LCMS, so this is what a deep LSM looks like.",
            "All we have done is added the second dimension, which is the layer, and we feed the outputs.",
            "Use wise of the previous layer into the next one as an input.",
            "The way I've written it here, I've also include what I call skip connections, which means so X prime is the input to each layer.",
            "It's a combination of the whole input, so this is the word we're conditioning on in our language model, and the input of the previous layer.",
            "That's not terribly standard, but it works really well for language modeling, and we concatenate all of the outputs into the final output as well, so we get bigger."
        ],
        [
            "Put Victor.",
            "That's sort of what your deep recurrent network looks like."
        ],
        [
            "That's what it looks like.",
            "If you had these skip connections.",
            "For language modeling for all language modeling.",
            "It depends how how keen you are to make it perform better, so it's still a bit up in the air and it depends how much data you have so.",
            "To have a comment on this, but so.",
            "Basically, if you don't have much data.",
            "The neural models perform much better than the traditional ngram models.",
            "As you get more data.",
            "The well the traditional models overtake the sort of feedforward neural models.",
            "After a few billion.",
            "Words, it's still.",
            "I think the jury is still out in the STM because no ones really trained one that I've seen sort of passed about a billion words.",
            "Not sure they've got much past that.",
            "But I mean the thing to say about language modeling is a billion words is not a big language model.",
            "It's a so.",
            "And if you look in.",
            "In in the sort of machine learning community, people are still publishing perplexity results in the Wall Street Journal, which is 1,000,000 words language modeling, and there it's easy to kill the traditional models.",
            "But in industry, if you're building a translation system, you're going to be using trillions if you're going into English of words.",
            "So that's that's what you actually care about, so.",
            "I'm pretty sure Nelson will be better on that scale.",
            "The best results with these models are sort of often all sorts of model averaging combinations and things like that.",
            "So it's a bit complicated."
        ],
        [
            "But in most medium size scales that the LCM is going to win.",
            "Yeah it works and it works better.",
            "I mean you always combining models gives you a bit of a win, so the traditional Ingram models and all of the various newer models have different strengths.",
            "If you actually analyze their probabilities over different types of engrams, they're quite complementary, so they're not not the newer models doing what the N gram model does better or anything like that.",
            "They're doing different things.",
            "The newer models are great at generalizing their greater things that you don't see very often.",
            "The N gram models are great things you see often.",
            "Because you just memorize them, and the interesting thing about languages, it's this combination of memorization and generalization.",
            "So to do well at language, you have to memorize because that's part of the the language process, but you also have to generalize to the long tail.",
            "So yes, combining them gets both, and that's still sort of interesting weakness of the neural models that are still not that great at memorizing.",
            "Sorry, how many.",
            "A person I have that number later on I have something related to I think I think I don't know about a particular age, but I think that the rough estimate I have later on is that an educated person would read about 100 million words in their lifetime.",
            "Something like that.",
            "So, so we're training these models on a lot more than that.",
            "But then again, people have a much more focused.",
            "Corporate that they're they're reading from where is the model to just reading the web?",
            "And there's there's a lot of stuff out there.",
            "The mix quality.",
            "OK, I probably better speed up a bit so.",
            "I."
        ],
        [
            "A few comments on efficiency.",
            "I think some of this being gone over, so I'll go over it quickly.",
            "So the key problem with training neural models and why they're not so easy to scale is because you have this big softmax over cabul airy, and if you want these things to work well, you need a big vocabulary you really want to be looking for, well, 100,000.",
            "Word types is is sort of.",
            "Maybe the minimum you really want to be looking at a million.",
            "So that's a big softmax and you don't want to be doing that for every time step, so there's lots of different ideas for how to do this.",
            "Um?",
            "One of the first ideas with the idea of shortlists.",
            "Let's just do the softmax for the 10,000 most frequent words, and let's use our traditional ngram model.",
            "For the rest.",
            "The problem is that, as I said that the newer models are actually best at the infrequent thing, so just restricting them to the frequent things is sort of throwing away one of the main advantages, so that's not a great idea, sort of you're playing to the weaknesses of both models by doing that.",
            "In a translation where the guys here at Montreal have been using this idea of having different shortlists for different segments of the training corpus, and this works quite well.",
            "I mean you can see sort of statistically how these things will overlap and you're, and if they're big enough, then you'll probably get pretty stable learning.",
            "So that idea is pretty effective."
        ],
        [
            "And it works very well on a GPU.",
            "These other ideas are also very good, but don't work so well on GPU's.",
            "Why yeah?",
            "Examples.",
            "Yeah, that's why.",
            "I mean, it's sort of a local local shortly, so you've got different ones.",
            "Yeah, you're just partitioning up the vocabulary and there's going to be overlap and it should all.",
            "If those those partitions are big enough, it should all be stable.",
            "Yeah yeah, it's fixed so.",
            "Yeah, yeah, and all of the frequent words will always end up in the normalizer, so you'll get.",
            "And I think there's some theory.",
            "If these shortlists are fully connected.",
            "If you can get if they're not, they're not actually separate partitions, then the information will flow eventually through the model.",
            "OK, but so other ideas, so this.",
            "Idea of what's this contrastive estimation is also very effective, so the problem in in neural language models is that the normalizer.",
            "If we could just treat that as some parameter that we estimated rather than actually."
        ],
        [
            "Regulating it that would be good.",
            "We can't do that becausw.",
            "We could just make that value.",
            "Well, you make it sort of infinitely small and then get sort of infinite likelihood.",
            "So a better idea is what Andre Min and UI proposed called contrastive estimation, where you sort of treat the problem rather than.",
            "Predicting the next word as such, you predict the you do a binary prediction of what samples are in the data and then you sample some noise and you try and predict which which.",
            "Which samples are from the noise in which samples are from the true data and you end up with an objective like this?",
            "This works very well.",
            "I won't Labour that too much."
        ],
        [
            "The main problem with that is it's a bit hard to get to work on a on a GPU really quickly.",
            "Another idea is to factorize your output so rather than just predicting the words straight out, predict the class, predict some segmented class set of the words, and then predict the word in 2 steps.",
            "This again is very effective.",
            "You need to estimate the class is something that works very well, is known as Brown clustering.",
            "People have used frequency being in other things.",
            "That doesn't work.",
            "Sort of looks nice, but it doesn't actually work."
        ],
        [
            "If you look at what gets learned, you can take the factorization right down to a tree binary tree.",
            "The problem with that is it's much harder to come up with a good tree.",
            "We've tried quite a bit and it's it seems like it."
        ],
        [
            "Not worth the effort.",
            "So.",
            "You know, sort of quick comparison to traditional models, so we've overcome these problems of sort of covariance between words.",
            "We now in a neural model.",
            "We're going to learn that certain words have similar distributions.",
            "The neural models are often very small.",
            "I said that your big connection model is going to be huge even on a few billion words.",
            "If you got 5 grams, you're going to end up with 30 gigabytes of counts on disk, and that's big, whereas neural language models are nice and small.",
            "The ngram neural language models if you do a sort of like for like comparison and things like translations, even when they have much better perplexities.",
            "They don't give us good translations which is interesting, and that's partly because of this difference in strengths.",
            "The neural models really got the generalization that ngram models good at memorization.",
            "It turns out that the memorization aspect is more important in those sorts of tasks if you use them for rescoring then then they're great.",
            "'cause I gotta extra information?",
            "And again, as I talked about it, you get this sort of crossover effect with, with most of the newer models where if you have enough data then the sort of Canessa 9 memorization type approach tends to win out eventually."
        ],
        [
            "So I had some slides."
        ],
        [
            "Morphology and Rep."
        ],
        [
            "Stations, but I might skip."
        ],
        [
            "Over that.",
            "In the interest of time and fire into machine translation.",
            "OK, so.",
            "Quick polls for breath and then.",
            "So now we've done language modeling, so we've got an idea of how to model these joint distributions of sequences.",
            "Once we got to the recurrent networks then we can start modeling quite long range dependencies and now we might have the tools to do this sort of.",
            "These translation ideas that I said from earlier on before going into that, let's start out with the traditional approach to translation and quick sort of my potted history of empty.",
            "Part of this was for Joe, who hadn't heard some of these quotes before, so I've stuck them in, but the idea of translation.",
            "Yeah, if you if you know your Bible, you know the reference to the the Tower of Babel and this idea that.",
            "Humans were cast out into the world with many different languages for being naughty, and now we have this problem of how do we communicate with each other?",
            "And we."
        ],
        [
            "Machine translation we want to use our technology to overcome this.",
            "So why is this hard?",
            "Why can't we just look at look up the words in a dictionary?",
            "And translate in that way and it's because languages are different.",
            "They express things in different ways.",
            "So here's some Arabic and Chinese examples now, to an English speaker that doesn't speak Arabic or China, use both of these.",
            "Look equally incomprehensible.",
            "It turns out that Arabic is much closer to English.",
            "In the way it expresses things in Chinese is so if you look at these, the plane is fast in the train.",
            "The Arabic is a rough gloss of that.",
            "If you look at the Chinese then things are just said in a completely different way.",
            "There's no one to one correspondence between words, so translation is hard.",
            "And that's sort of the moral."
        ],
        [
            "If you look at European languages, you can get very close, but if you look at the full.",
            "The full range of languages.",
            "There's lots of interesting phenomena going on.",
            "So.",
            "This is the basic problem we want to translate.",
            "Given an input sentence, we want to produce output sentence.",
            "Traditional view of translation is to ignore the discourse context and we just think of individual sentences.",
            "So the first thing anyone doing this realizes that you can't just directly model this probability.",
            "You have to break this down in some way.",
            "And that's where all the work into."
        ],
        [
            "Relation comes into.",
            "OK, so here's the here's some of the history, so there's a famous memorandum from the 40s from known as the Weaver Memorandum, and Warren Weaver had a couple of choice quotes in this that in some sense sort of set the tone for a lot of translation research.",
            "So in this one he's talking about languages, towers, and this idea of rather than trying to jump on your between these towels, we should just send to our some common language.",
            "So the idea of an Interlingua universal language, and if we can learn this mapping.",
            "From each language to the Interlingua, and from each from the Interlingua switch language.",
            "Then we can do any form of translation.",
            "And in a sense, this sort of led to what's known as a rule based approach to translation.",
            "Trying to map to some generalization of language, some symbolic generalization that we can reason about."
        ],
        [
            "That didn't get very far because language is just not that nicely behaved, and it's hard to analyze symbolically.",
            "We also had another quote that was sort of forgotten about for awhile and then came back in the 80s, which is this idea that maybe we should.",
            "Forget about thinking of these things as languages so much as just codes and this problem is cryptography.",
            "We see a Chinese sentence in English sentence.",
            "So we just think of the Chinese sentence as sort of a corrupted transmission of the English in some way, and we wanted to code it.",
            "So that's very much an information theoretic view of translation, and that's really the origin of the statistical approach to translation."
        ],
        [
            "So way back then people were thinking about this.",
            "So.",
            "So we've sort of gave us this idea of statistical translation.",
            "The next piece of the puzzle was.",
            "You need you need data to get statistics and the great thing about translation is that the data is everywhere, so this is a Rosetta Stone.",
            "From ancient Egypt way back then, people reducing producing parallel corpora.",
            "This is that's Egyptian hieroglyphs, that's ancient.",
            "That's Coptic, and that's ancient Greek.",
            "When people found this stone they couldn't read.",
            "The hieroglyphs they didn't know how to read Egyptian, but I knew quite a lot about ancient Greek and they they discovered quite a bit about Coptic, and they discovered that the three sections of the stone said exactly the same thing, so they use these two to code the hieroglyphs, and then we could.",
            "We could read Egyptian an."
        ],
        [
            "In translation, we just want to do the same thing statistically.",
            "So the great thing about the modern world is we have the Internet and parallel corpora everywhere.",
            "If you look at the this is the UN web page.",
            "You can get it in Chinese.",
            "You can get in English.",
            "We can scrape this.",
            "You can immediately discover how to translate United Nations.",
            "An you can sort of guy.",
            "So you can see that it's like this little this little sort of triangle symbol here.",
            "Here it's around here and if you look on this side.",
            "In the same place you see that I'm talking about people, humans, humanitarian spokesperson.",
            "You can pretty quickly start to learn translation, so obviously that that Chinese character has something to do with people.",
            "So we can get lots of this, and as I was saying.",
            "In English we can get.",
            "Well, the biggest the biggest parallel corpus in English is that the French, English Giga corpus, which is close to a billion words, translated text.",
            "But for most European."
        ],
        [
            "In which we can get sort of half a billion or more.",
            "No, sorry, that's more like 100 million.",
            "OK.",
            "So.",
            "In the 80s there was this.",
            "Sort of very well known group at IBM that had been doing a lot of pioneering work on speech recognition.",
            "Led by Fred Jelinek.",
            "And the story goes that they realized that their speech recognition system could be turned into a translation system, 'cause it's basically the same problem.",
            "Fred said they weren't allowed to do this.",
            "They had to make a speech recognition system better.",
            "Fred went on holiday, so then the guys left behind implemented machine translation and the rest is history.",
            "But one of the famous quotes from this time was was from Fred.",
            "That said, who was?"
        ],
        [
            "Very much.",
            "I think you could say a linguist.",
            "But the frame is quite the saying, every time we got rid of language from his team, is this system got better?",
            "But these guys are really pioneered the statistical approach to to empty.",
            "Another great bit from history that I threw in.",
            "Is so.",
            "The IBM guys say they published a very famous paper that the origins of statistical machine translation in the early 90s.",
            "Peter Brown and Bob Mercer, but they first submitted it to a conference and this is the review they got.",
            "So back then reviews were handwritten.",
            "You fax off your paper, wait for awhile and what is true so it got rejected and you probably can't read this.",
            "But this says the validity validity of statistical statistical approach to machine translation has indeed been recognized.",
            "As the authors mentioned by Weaver as early as 49.",
            "And was universally recognized as mistaken by 1950.",
            "With some references and then this great quote down the bottom, which is the crude force of computers, is not science so.",
            "So the paper was rejected.",
            "Thankfully it was published and that's that.",
            "Play was the origin of."
        ],
        [
            "Of the statistical approach to machine translation.",
            "And the basic idea with this, which is the same formula that you have seen in the speech recognition talk.",
            "Known as the noisy channel model.",
            "Now, rather than trying to map from a microphone signal to a sentence, we're trying to map from some input source sentence to an output sentence, and we can use the same Bayes rule decomposition.",
            "And we end up with this problem of estimating a conditional translation model and our old friend here joint Language model.",
            "So again, This is why language modeling is crucial to MBTI, as it was to speech recognition.",
            "So then they came up with a number of different models known as the IBM models.",
            "For particularly estimating this bit.",
            "They by that point they knew how to do this quite well with with ngram models."
        ],
        [
            "So you can roughly think of this as this process, where you feed, say, French, is that the input language you feed it through your translation model, you get some sort of garbled English.",
            "You feed it through the language model and you get something better.",
            "But the real."
        ],
        [
            "The real power behind that decomposition is just that we can get so much more data to estimate our language model.",
            "We want to be able to leverage that in our in our model, where the translation model is estimated from a lot less.",
            "So also through this slide in, but I think some one of the other speaker, I think maybe Russell and said something about maybe someone will try so later on I'll talk about attention models in translation.",
            "IBM tried it in the in the 90s and that's the first IBM model, so back then they have this great model which anyone that working on translation should implement.",
            "This model, 'cause it's just one of the great models of sort of graphical models, so it's IBM Model 1 and this is this.",
            "We're modeling the probability of a source sentence given the target sentence.",
            "With this product.",
            "These are just the individual source words.",
            "Given the target words indexed by an alignment.",
            "Asoue is a vector of alignments.",
            "And that's latent.",
            "So in today's terms, you call that the attention.",
            "So for each word that you're predicting you're working out which word in the input it's coming from, and then you're predicting it.",
            "The great thing about this model is one we can sum over these alignments in polynomial time.",
            "Which is cool, so it's easy to estimate.",
            "It's extremely scalable.",
            "Later IBM models lost that that property, but this very simple first model you can do polynomial inference.",
            "The second thing is that this objective function is actually convex, which is pretty rare for these sorts of mixture EM models.",
            "So this model is both scalable and it's convex, so you don't need to worry too much about where you start.",
            "That's just just for historical purposes."
        ],
        [
            "Post.",
            "Post the IBM models that were word based.",
            "People around about 2000 started building what we call phrase based models, where rather than looking at which producing a word condition on a single word in the in."
        ],
        [
            "But we produce groups of."
        ],
        [
            "Words produce condition on groups of words, so it looks something like this, and that's all pretty word based."
        ],
        [
            "But then we might produce our phrase for this book.",
            "With the."
        ],
        [
            "Ordering again for the phrase to read to a single word on the output.",
            "So this is what's known as a phrase based approach to translation.",
            "We need to estimate big tables of phrases.",
            "To do this, but this is basically sort of the state of the empty at the moment.",
            "If you go to any industrial empty system, they're going to be doing something like this phrase based empty."
        ],
        [
            "To estimate your phrase based empty model you need to go through a process like this, which is long complicated.",
            "You start out with a large parallel corpus.",
            "You train the old IBM word models.",
            "That gives you an alignment.",
            "You extract phrases, you get a phrase table.",
            "Then you do some more training on a small corpus called discriminative training.",
            "Then finally you got translation model.",
            "So this long process, one of the motivations for the sort of neural stuff we talk about later is just to flatten all of this.",
            "'cause this is long, complicated, very hand tuned, very difficult to adapt."
        ],
        [
            "So we'd like to get rid of all of that.",
            "OK.",
            "So that brings us to the.",
            "Again, back to more familiar territory, which is OK, so that's traditional translation.",
            "Phrase based translation is great, but it's not this big, complicated process.",
            "It's all pretty heuristic, so there's not a great deal of principle behind what's going on.",
            "It's just stuff people tried and it worked, and so we stuck with it.",
            "The original IBM models they were they were very well conceived probabilistic models, but they didn't work as well as this sort of phrasal approach.",
            "But now we want to.",
            "Sort of take a step into the deep learning world and say, well, can we just simplify this down and say what?",
            "If we just want to take some input sentence and produce some output sentence and we're just going to do it by generalizing this to a vector and then generating from the vector.",
            "So we sort of going back to the old interlingual idea, but.",
            "Not so much yet still still very language specific, so we need 2 parts.",
            "We need to be able to generalize sentences to a vector and then we need to be able to generate from a vector."
        ],
        [
            "So we need two different parts.",
            "Let's for a moment assume we have some way of generalizing to vector the question of how to generate it.",
            "Turns out we can do that with our language model.",
            "So this is you can't do this very easily with the traditional model, but this is one of the great things about neural models.",
            "They're very easy to condition on extra information.",
            "So now at the top I've got my sort of diagrammatic representation of the.",
            "Ngram neural model, so we're just going to start very simply.",
            "That's just going to be our language model over the output language, but at each step we're also going to incorporate this this hidden vector from the input, so it's just going to say we've got a language model, but it's going to be biased by this input it observes.",
            "And can we with this simple setup, do translation?",
            "So now the problem will come to search for that language model.",
            "Basically start out with the start symbol and the vector from our input and look at the the basically greedily or with some sort of beam, enumerate the best words and see if we get a good translation.",
            "So yes, this is just exactly the neural model from earlier with extra vector from the source sentence."
        ],
        [
            "OK, and so again we need some way of getting the vector for the source sentence.",
            "You should always start with the stupidest thing first to make sure that you're more complicated thing later on is actually working.",
            "But the stupidest thing is just to say that maybe each source word has a vector.",
            "We sum them all together and we get a vector for the whole sentence.",
            "That stupid because we're throwing away the word order.",
            "These vectors are just overwriting each other.",
            "You can't possibly do translation that way, but you should always, as I say, start with the simplest one so we sum together vectors from the input.",
            "We get a vector for the source sentence.",
            "Then we generate from our translation model from our neural language model conditioned on that vector."
        ],
        [
            "So some examples."
        ],
        [
            "Some Chinese we sum all those vectors together, so obviously with backpropagation we can just learn all the parameters of this easily with these vectors cannot be learned from a parallel corpus, so there's a model I trained on a very small parallel corpus, only 50,000 sentences or something like that."
        ],
        [
            "We generate from the language model an we get a perfect translation, which is pretty cool so.",
            "First time I saw this, I didn't really understand what happened.",
            "Because this shouldn't really work, but it did.",
            "And if you can."
        ],
        [
            "Can be repeated.",
            "So there's another example that works very well."
        ],
        [
            "And."
        ],
        [
            "Yeah, and they even sort of about 10 words.",
            "Reasonably long."
        ],
        [
            "And it says, and the Chinese is not simple.",
            "So if you gloss that Chinese, you get something quite garbled from English."
        ],
        [
            "If you if you feed it in the Google Translate, it fails pretty dramatically."
        ],
        [
            "The sort of truth in advertising.",
            "Obviously I handpicked those examples when I did this.",
            "It doesn't always work.",
            "So in the red, that's what it should translate that sentence too."
        ],
        [
            "That's what it does.",
            "Translate that sentence to.",
            "So it's pretty high variance, But the interesting thing about this was just that.",
            "Stop the generation so we include an end of sequence symbol so when the model predicts stop we stop.",
            "I didn't say it was incorrect.",
            "I said the the work that followed that, which was a symbolic approach to try and find this Interlingua.",
            "Didn't didn't turn out like it's.",
            "There's been quite a bit of work on Interlingua San and this has never really worked out, so the idea of a universal language.",
            "Well, at least we've come nowhere near to finding such a thing.",
            "Maybe, and as people are starting to do now with these deep learning models, I think more like a universal embedding space or something like that.",
            "That's quite different to a something that looks like a human language but is universal in some way.",
            "We're still some way to that.",
            "These anything I'm training here.",
            "We've got some vector generalization, but it still specific to this language bear.",
            "But obviously you could think about training this for many languages and trying to find one vector embedding, so Christian.",
            "I can't remember these examples.",
            "It was pretty greedy, at least I sometimes used to be my can't remember them.",
            "Yeah, which is not true of traditional translation models.",
            "You need a big beam unless you've got quite a depends on the language pair.",
            "And how much reordering is going on?",
            "But for traditional models there's quite a lot of beam searching going on."
        ],
        [
            "OK, so that was my sort of simple example that the take home message from that is not that you should build that sort of translation model.",
            "It doesn't work there.",
            "They handpicked examples and obviously once you go past about 10 words, it just stops working entirely.",
            "The interesting thing is just that it works at all and that tells you something about language.",
            "That especially for output language like English, the word order is very constrained.",
            "So if you know a few of the points in space, you know a few of the words, even without much information you can fill in what the rest is.",
            "So in those Chinese examples, if I just give you the individual words in the Chinese and tell you to like, I give you the English translations and tell you to form an English sentence from those words, it will probably be close to the translation.",
            "So particularly for English, which has quite fixed word order.",
            "It's it's illustrative of just how constraining the local structure of languages, so it's not us.",
            "This problem is not as hard as mapping from some arbitrary sequence to some arbitrary sequence.",
            "There's a lot of structure in the output which constrains what you could have.",
            "OK, so after.",
            "So that was just a toy experiment people started to do this more seriously.",
            "So one thing that was done at Google from people like Ilya Master think, well, what if we just feed this into a big deep LS GM as a long sequence, like I sort of talked about to start with.",
            "So here's our French Now separator symbol.",
            "Then we feed that into the LS GM and then once we get to the separator, we just start generating from our language model.",
            "The idea being that their model is just going to learn that dependencies.",
            "And it turned out that they actually could get some pretty good translation, so this gets rounded.",
            "So now the word order is going to be encoded in some way.",
            "You get a lot more structure in the input than those examples I gave previously, so this starts to sound more plausable as a as a new."
        ],
        [
            "Translation model.",
            "Problem is you've got this.",
            "You still got this at a big bottleneck here that some fixed width vector that you're pushing all of the input into this fixed width vector and this is not adaptive to how long the sentence is.",
            "So whether you've got what we got there 5 words or 50 words.",
            "You're pushing all of this into the same width vector and trying to read out the translation.",
            "And so that doesn't seem very optimal, and we also know from.",
            "We know about the structure of translation, especially for languages like French and English, that these roughly follow each other in a monotone.",
            "So the first word here is going to be a translation of something near the start of the sentence, and it seems like a good idea to use this structure."
        ],
        [
            "Illustrative of that is that when the Google guys actually did this, they had to reverse this sentence to make it work well, and the reason they do this.",
            "Again, coming back to that to the idea of the constraining structure of the output language is that this brings these words.",
            "Close together.",
            "So and the problem in translation is to get the start of the sentence right.",
            "If you can get the first couple of words right and you know the context, then it's much easier to get these words afterwards, right?",
            "Because once you've seen a few words, there's a lot of constraint over what comes next, so if you look at the perplexities per word on the translations, the hardest thing is getting the first words right.",
            "Once you've got that, then the rest sort of follows.",
            "So what they're doing here is just bringing this starts together closely, so it's easier for them to learn.",
            "And the bit that's easier, which is later on.",
            "Is a bit easier.",
            "That said, this still doesn't actually really work as a translation model.",
            "It does surprisingly well, but once the sentences get long, you're going to have to have very deep big models and a lot of training time, and it's very in that sense.",
            "It's very inefficient because you end up having to have a model that's big enough to represent the longest sequence you might encounter."
        ],
        [
            "So a better idea which sort of brings together the.",
            "Old ideas of alignment in translation with these newer models and tries to exploit the structure as if the idea of attention which was done here in Montreal.",
            "So now rather than just treating this as one big sequence and running our big.",
            "Running the whole big sequence model through all of our input and output first, we're going to embed the input in some way and the the the classic thing to do now is to run bidirectional recurrent networks.",
            "Over your input.",
            "So this is a recurrent network going left to right and recurrent network going right to left and you can take the output hidden layers at each time step and concatenate them together and get a sort of concatenated representation of each time step.",
            "And now we're going to have a translation."
        ],
        [
            "Process where again we're going to generate from out.",
            "Output language model and at each time step we're going to do this attention operation, so we're going to compute a function of our output hidden representation.",
            "All of these embeddings.",
            "We're going to do this for each of our input time steps, and that's what all those little lines play there.",
            "And then we're going to do a softmax over those you've got a few options.",
            "You can do individual sigmoids or you could do a softmax.",
            "Everyone seems to be doing a softmax I think.",
            "Seems to be the most popular.",
            "That sort of enforces this idea that the word you're generating probably is coming from one or a small set of words in the input, not from lots of them.",
            "That assumption is more or less good depending on the languages.",
            "They're not.",
            "They're not ordinary RNS there either.",
            "So most of the Montreal ones were GI use, the gated recurrent units, which.",
            "Similar using gate similar to STM to achieve the same sort of outcome and that slightly simpler package people are also doing this with LS teams.",
            "So for this for the embedding, it matters less.",
            "You definitely want something on the output, which is not an ordinary RNN.",
            "On the input you because you're doing this attention and mixing, it's not clear how much you need information to propagate through that sequence.",
            "So yeah.",
            "So we compute these attention variables and will come out with sort of some of them being greater, and I sort of tried to show that with the darker lines and all we do is just do like an expectation across seas or some so given the expectation of the attention waiting, we multiply that by the embedding vector, sum all these together.",
            "That's what little some means up there."
        ],
        [
            "And then we feed that into the next step, or in this case, the first step of our generation recurrent network and generate our first translation and."
        ],
        [
            "Then we just keep doing this.",
            "So at each time."
        ],
        [
            "I'm step we compute in attention.",
            "We mixed together the the source representations given that attention, and then we predict the next word and so."
        ],
        [
            "That's how we do our translation.",
            "And this this is a much more plausible model of translation.",
            "One it gets us much closer to the original model, so this is basically at least in my mind, a soft version of IBM Model 1, so I'd be a model one was computing individual alignments for each word generated.",
            "It was computing individual alignments using stochastic variables, so therefore you could consider it hard alignment.",
            "In that sense, this is a sort of soft alignment version.",
            "There's a lot more going on 'cause we are obviously mixing together information on the input and output alot more.",
            "But just like one thing to think about this is just like IBM Model 1.",
            "These attention decisions are alignment decisions are relatively independent.",
            "You get a little bit of dependence through this hidden layer, but we're not directly conditioning on a previous alignment.",
            "I think lots of people have tried this or are trying this now, but that's one sort of obvious thing to think about this model, which doesn't capture some information we have in translation.",
            "That is especially for French.",
            "If the word to the left of me was attended to on the last time step, then I'll probably be attended to on the next time step, so that's something that a model can easily learn.",
            "You then, if you could learn that, that's sort of what we call a monotone bias in translation.",
            "If we can learn that, then we want to be able to more interesting things, like if if I'm a French noun.",
            "Sorry if I'm a French adjective and always tended to in the last time step, then actually we go backwards in direction and will probably be the noun to my left which gets attended to the next time 'cause we have to reverse the order in English.",
            "So we'd like to be able to sort things, but this model?"
        ],
        [
            "Does very well, so I so I stole some tables so that.",
            "Yeah, the.",
            "Letters on the top are a bit out of order, but I stole these tables from Cho.",
            "With results from WMT with the attention model and these are all so the top two are going out of English in the bottom two are going into English out of English you can see that the the Montreal system is is winning and there's an into English in this case.",
            "The models further down the rankings of all these, all the other systems are sort of traditional systems, probably also using various new neural networks and other things in them as well.",
            "But you can see just from this you can see that the neural the attention model is is very competitive with the state of the art.",
            "There's a lot of logic in why initially this might work well in languages other than English.",
            "English.",
            "So.",
            "The nice the nice thing about the structure is you're making you don't have to make any sort of structural.",
            "Assumptions about how the words are dropped, or any or insert or anything like that.",
            "This is all left up to the generating language model because it just chooses the next word.",
            "So that's really nice, because if you've seen enough data, the language model in context is very strong.",
            "It tells you whether you need to insert a article or things like that to make the just the local sequence high probability.",
            "This is partly tide to this idea that initially these are working very well in languages other than English.",
            "Because English is quite boring as a language in that there's no or very little morphology and agenda and these sorts of things.",
            "These other languages like Check has a great deal more word internal structure in agreement structure going on, which traditional translation models have a great deal of trouble handling because they they basically have to have seen all the different combinations in order to give them a good probability, whereas the newer models tend to learn they can learn gender, they can learn these sorts of things and they can know that locali these things should agree.",
            "So if I have a choice between.",
            "A feminine or masculine.",
            "Adjective or something like that.",
            "The local context can tell me which one to choose.",
            "Yeah so.",
            "Yeah, so, and that's a key thing, so the traditional as I think I said this earlier in the traditional approach to empty, you have your translation model and you have your language model in your language models trained on a lot more data than the translation model an in English, it's a lot more data, so there's a big big difference and let it run it in a traditional translation model.",
            "The language model is extremely important to performance, so the more language modeling data you get, the better the model gets because traditional translation models are actually pretty rubbish.",
            "If you look at if you get rid of the language bowl and just look at the sort of distribution they have over translation, it's awful.",
            "So most of the work is huge.",
            "Amount of work is being done by the language model.",
            "It's sort of been given this sort of rough collection of phrases and being told stitches together into something that looks roughly like English.",
            "So the better you can make that, the higher your score.",
            "So all of these sort of Edinburgh models and such for English.",
            "They're going to be using.",
            "Least 5 billion, probably more words of English in their language model, whereas the nearly empty system is probably trained on 30,000,000 or 50,000,000 words from the parallel corpus.",
            "The Montreal guys have more recent work and how to overcome that problem, but that's that's another key.",
            "Reason for those those different results, but one of I mean one of the things about this is that.",
            "One of the.",
            "Strong motivations for the neural approach machine translation is actually to go into languages other than English.",
            "I mean, we were actually pretty good at translating into English.",
            "And that's also in most industrial settings.",
            "That's also not the most common scenario.",
            "If you look at the Google Translate and things like that, mostly their translating out of English, because most of the web is in English and people want to be able to know what it means in their language.",
            "And we're actually pretty bad at translating out of English becausw.",
            "Because English is a bit of an outlier.",
            "As language goes, it has a very fixed word order, not much morphology, so that very much plays into the strength of a traditional translation model, whereas less fixed word order.",
            "Lots of morphology, in agreement which you see in sort of lots of other languages, especially European languages.",
            "They're much worse at and the newer models are much better suited for.",
            "So.",
            "Translation sorry, I skipped that so there's something called the blue score.",
            "Which is basically an engram overlap measure, so you have for a test set for each input sentence you have, normally more than one.",
            "Actually for WMT is normally only one, but in better death sets you have more than one possible translation and given a hypothesis translation you calculate engram overlap.",
            "So you start with unigrams and bigrams and you calculate the what we call an engram precision.",
            "So how many of the engrams I predicted were actually in the reference?",
            "Then you basically just mix those together in a in an average with some with some weights.",
            "And then there's something crucial which is called the brevity penalty, which means that one of the problems with precision is that you can just if you just output the as the translation of every sentence into English, you'll get 100% because precision doesn't account for the missing words, so you know the sentence is probably going to have in it, and so you get 100% precision.",
            "So you need a brevity penalty that penalizes an output, which is much shorter than the.",
            "The input, which is very important to know about becausw, that penalty is a bit strange.",
            "It's exponential and getting a good Bleu score is often largely about getting the brevity penalty right, so blue scores are a flawed metric, but.",
            "Yes, basically it's in gram overlap with some reference translations.",
            "OK. OK, thank you."
        ],
        [
            "Pretty much done."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, I think today is all about natural language processing, which would be fun.",
                    "label": 0
                },
                {
                    "sent": "So this talk, I can't remember what was actually in the schedule, but this will actually be is sort of a journey from language modeling, which I think has been mentioned a few Times Now in the summer school, but will go a little bit deeper into it and also start out in a very sort of traditional view.",
                    "label": 0
                },
                {
                    "sent": "So Jenny from language modeling and then how we can we can transition from modeling sentences to doing things like machine translation.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hopefully by now you've sort of got a rough idea of what a language model is.",
                    "label": 0
                },
                {
                    "sent": "Language model is language.",
                    "label": 0
                },
                {
                    "sent": "Modeling is a term that's used sort of both informally and formally.",
                    "label": 0
                },
                {
                    "sent": "In NLP, I think a formerly a language model is something that that assigns a joint probability to a sequence of words.",
                    "label": 0
                },
                {
                    "sent": "People often just use it for anything.",
                    "label": 0
                },
                {
                    "sent": "That process of language in classifiers or something like that.",
                    "label": 0
                },
                {
                    "sent": "But when you say language model, I think of something that takes a sentence like the House is small and assigns a probability to that a joint probability.",
                    "label": 0
                },
                {
                    "sent": "And we can do that in a well defined way.",
                    "label": 0
                },
                {
                    "sent": "We could the space of utterances, a space of sentence is infinite.",
                    "label": 0
                },
                {
                    "sent": "You can keep adding words forever, but we can still have a well defined joint probability.",
                    "label": 0
                },
                {
                    "sent": "And if we have that, the sort of traditional view is that we can do lots of things like comparing the probabilities of sentences.",
                    "label": 0
                },
                {
                    "sent": "And that can tell us a bit about what a more likely ordering of words is, so this is.",
                    "label": 0
                },
                {
                    "sent": "I'm really became first most useful in speech recognition.",
                    "label": 0
                },
                {
                    "sent": "We have this problem of of trying to assess whether your output is likely in some sense.",
                    "label": 0
                },
                {
                    "sent": "So language model is crucial there.",
                    "label": 0
                },
                {
                    "sent": "And so you can.",
                    "label": 0
                },
                {
                    "sent": "You can use it to to assess choice, lexical choice, but also ordering.",
                    "label": 0
                },
                {
                    "sent": "And that's then used in translation, but lots of other things you can use.",
                    "label": 0
                },
                {
                    "sent": "You can use the language model to classify if you have text documents in multiple classes, you can build a language model for each class and then given some text you can just ask which language model most probably produces text.",
                    "label": 0
                },
                {
                    "sent": "So all sorts of things you can do with.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the language model.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is not really a standard view and something I just sort of throw in for this talk, but now we're starting to see that.",
                    "label": 0
                },
                {
                    "sent": "Lots of tasks in NLP you can think of as language modeling problems or assigning probabilities to sequences of words and symbols.",
                    "label": 0
                },
                {
                    "sent": "It's easy to see how machine translation fits this paradigm rather than just modeling sentences in one language, we can just concatenate two sentences, one in French and one in English.",
                    "label": 0
                },
                {
                    "sent": "And if we can model the probability of that sequence, then we can do translation.",
                    "label": 0
                },
                {
                    "sent": "Obviously, in translation we're interested in the conditional output, but we can think of it this way in language modeling.",
                    "label": 1
                },
                {
                    "sent": "We can go further and think of things like question answering and dialogue where we have some sort of natural language input and we want to produce natural language output.",
                    "label": 1
                },
                {
                    "sent": "Again, we can just think of these as a sequences.",
                    "label": 0
                },
                {
                    "sent": "I model them with joint probabilities and people are doing this now and one crucial thing is, once you get to things like question answering in dialogue like this, you really need some.",
                    "label": 0
                },
                {
                    "sent": "You need to not forget about the sort of world world knowledge and such.",
                    "label": 0
                },
                {
                    "sent": "It also has to be conditioned on.",
                    "label": 0
                },
                {
                    "sent": "But this is sort of.",
                    "label": 0
                },
                {
                    "sent": "An idea that maybe maybe if we can just get language modeling right then we can solve lots of other problems.",
                    "label": 0
                },
                {
                    "sent": "And from.",
                    "label": 0
                },
                {
                    "sent": "From my point of view, deep learning and NLP you see lots of things like classifications, lots of work on sentiment and word embeddings and things like that, where I think it's having the most impact is in these sorts of things where deep learning now allows us to model much more complex joint distributions over sequences.",
                    "label": 0
                },
                {
                    "sent": "And thus generate language conditioned on complex information.",
                    "label": 0
                },
                {
                    "sent": "And that's one thing that deep learning is really given us, that that wasn't so much there before most of the other things are really just refinements on on existing models.",
                    "label": 0
                },
                {
                    "sent": "OK, so language.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Modeling.",
                    "label": 0
                },
                {
                    "sent": "So, at least from my knowledge, really, when people got interested in statistical language modeling was in the Second World War, when people like Alan Turing and IJ good on the right there were trying to.",
                    "label": 0
                },
                {
                    "sent": "Decript German communications and if you're going to do this from traditionally at that time, people would do decryption using linguists.",
                    "label": 0
                },
                {
                    "sent": "Basically, they try and treat it like a translation problem.",
                    "label": 0
                },
                {
                    "sent": "These guys thought of it more of a statistical problem.",
                    "label": 0
                },
                {
                    "sent": "They tried to model it as.",
                    "label": 0
                },
                {
                    "sent": "An information theoretic decryption problem.",
                    "label": 0
                },
                {
                    "sent": "And in doing so, they realized that.",
                    "label": 0
                },
                {
                    "sent": "If they wanted to decrypt German, they needed to know something about the.",
                    "label": 0
                },
                {
                    "sent": "Statistical irregularities of German.",
                    "label": 0
                },
                {
                    "sent": "So they put a lot of thought into how do we actually model frequencies of words and sequences of words?",
                    "label": 0
                },
                {
                    "sent": "And that was really the start of the idea of language modeling and they came up with some some good ideas that persist today.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, formally, what we're interested in is we have our sequence that we might call big W there a sequence of words.",
                    "label": 0
                },
                {
                    "sent": "And we want the joint distribution over that.",
                    "label": 0
                },
                {
                    "sent": "Of course we are interested in the order as well.",
                    "label": 0
                },
                {
                    "sent": "The order is important.",
                    "label": 0
                },
                {
                    "sent": "We can have arbitrary length.",
                    "label": 0
                },
                {
                    "sent": "Strings, we just have some special end of sequence string that we also have to include in our distribution.",
                    "label": 0
                },
                {
                    "sent": "So obviously just modeling a big joint distribution is is hard, so we want to decompose it.",
                    "label": 0
                },
                {
                    "sent": "The product rule tells us that we can just decompose the joint into a series of conditionals.",
                    "label": 0
                },
                {
                    "sent": "This is exact.",
                    "label": 0
                },
                {
                    "sent": "There's no approximation here.",
                    "label": 0
                },
                {
                    "sent": "So that tells us that we don't need to model the joint if we can just model conditional probabilities so it can predict the probability of a word given a history.",
                    "label": 0
                },
                {
                    "sent": "Then we good.",
                    "label": 0
                },
                {
                    "sent": "Now, doing this is pretty hard because this is some arbitrary length string.",
                    "label": 0
                },
                {
                    "sent": "To condemn.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the traditional approach and still by far the most popular is the so-called N gram approach to language modeling, where we just assume that most of the history doesn't matter and we just keep the most recent things we see.",
                    "label": 0
                },
                {
                    "sent": "So a bigram model, as in 2 words.",
                    "label": 0
                },
                {
                    "sent": "Just conditions WN on WN minus one.",
                    "label": 0
                },
                {
                    "sent": "So we now we now approximating the joint.",
                    "label": 0
                },
                {
                    "sent": "With just these bigram conditionals.",
                    "label": 0
                },
                {
                    "sent": "And this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the basic idea behind all of the traditional language modeling work.",
                    "label": 0
                },
                {
                    "sent": "The idea of N grams where the depending on how many of these words you condition on the simplest, is a bigram you can go to Tri grams or more.",
                    "label": 0
                },
                {
                    "sent": "We get different powers and models.",
                    "label": 0
                },
                {
                    "sent": "So then.",
                    "label": 0
                },
                {
                    "sent": "In this view, we've reduced the problem to modeling these conditionals.",
                    "label": 0
                },
                {
                    "sent": "So now what we just care about is modeling the probability of a word given the previous word or the previous couple of words.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The obvious thing to do is to start with maximum likelihood, and that's the easiest idea.",
                    "label": 0
                },
                {
                    "sent": "Now you have.",
                    "label": 0
                },
                {
                    "sent": "So the first thing you have to do is collect lots of data.",
                    "label": 0
                },
                {
                    "sent": "Today that's easy and back when people first started doing this that wasn't so easy.",
                    "label": 0
                },
                {
                    "sent": "They didn't have so much machine readable text.",
                    "label": 0
                },
                {
                    "sent": "Now, of course you can get billions of words in most languages, trillions in English off the web.",
                    "label": 1
                },
                {
                    "sent": "And it's very easy to estimate a maximum likelihood by gram conditional simply by counting the number of times you saw those two words together and dividing by the number of times you saw the the word conditioned on.",
                    "label": 1
                },
                {
                    "sent": "So that straightforward problem is maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Rarely ever works so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something to keep in mind for the later on.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've now got a basic idea for how we might estimate a simple language model.",
                    "label": 0
                },
                {
                    "sent": "We also need some way of assessing whether it works, so the classic assessment, the intrinsic evaluation.",
                    "label": 0
                },
                {
                    "sent": "Of course, the best evaluation for your language model is stick it into your speech recognizer or empty model and see how it performs.",
                    "label": 0
                },
                {
                    "sent": "But if you want some intrinsic way.",
                    "label": 0
                },
                {
                    "sent": "Evaluating your language model, you can just look at the probability @ assigns tenyu sequences.",
                    "label": 0
                },
                {
                    "sent": "And so we can do that with the cross entropy at the top there.",
                    "label": 1
                },
                {
                    "sent": "Or if we raise it to the power of two, we get something called the perplexity, and this is what you'll see most often in language modeling.",
                    "label": 0
                },
                {
                    "sent": "People report this quantity.",
                    "label": 0
                },
                {
                    "sent": "So you train your language model, you collect your counts on a few billion words of text.",
                    "label": 0
                },
                {
                    "sent": "You have some held out test data that doesn't overlap with that, that training data.",
                    "label": 0
                },
                {
                    "sent": "And then you look at your model and you see what probability at the signs to those strings.",
                    "label": 0
                },
                {
                    "sent": "And you calculate your perplexity and that gives you a measure of how good it is.",
                    "label": 1
                },
                {
                    "sent": "OK, and obviously you want your perplexity to go to one if you have a perplexity of 1, then your models knows the future.",
                    "label": 0
                },
                {
                    "sent": "Given a word, it knows exactly what's going to come next.",
                    "label": 0
                },
                {
                    "sent": "So this so the other thing here is to realize that this connects language modeling with compression, so language modeling and compression are basically the same problem, so that is compressing text or files.",
                    "label": 0
                },
                {
                    "sent": "You want to have a model of the data that is as sure as possible about predicting what can come next year.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turn any language model into a compression.",
                    "label": 0
                },
                {
                    "sent": "Any language model of this sort into a into a compression algorithm?",
                    "label": 0
                },
                {
                    "sent": "OK, so if we actually look at some data so this is.",
                    "label": 0
                },
                {
                    "sent": "So there's a very popular set of corpora called the Europower corpus, which is European Parliament proceedings.",
                    "label": 0
                },
                {
                    "sent": "Across many languages, and if we look at if we collect unigram bigram and trigram and four gram counts from this corpus, and then look at a sentence and look at each of the conditional.",
                    "label": 0
                },
                {
                    "sent": "Probabilities that assigns to each each word in the sequence given the previous ones.",
                    "label": 0
                },
                {
                    "sent": "You get an idea of how these models behave.",
                    "label": 0
                },
                {
                    "sent": "So you see basically this pattern that as you add more conditioning context, things get better.",
                    "label": 0
                },
                {
                    "sent": "Do they always get better?",
                    "label": 0
                },
                {
                    "sent": "I think there's one that does that.",
                    "label": 0
                },
                {
                    "sent": "OK, that one gets worse.",
                    "label": 0
                },
                {
                    "sent": "Mostly they get better.",
                    "label": 0
                },
                {
                    "sent": "And you see that the probabilities vary a great deal through the sequence.",
                    "label": 0
                },
                {
                    "sent": "So for instance, predicting the end of sentence symbol is easy.",
                    "label": 0
                },
                {
                    "sent": "So given that you saw a full stop figuring out that the sentence is probably going to end is not very hard.",
                    "label": 0
                },
                {
                    "sent": "If you look at things like so that the hardest thing was up here, so commend.",
                    "label": 0
                },
                {
                    "sent": "So I would like to.",
                    "label": 0
                },
                {
                    "sent": "If you ask if you say I would like to announce someone to tell you what word would come next, they're not going to have much idea, so there's a lot of words that come next there, and so the model doesn't have much idea about command.",
                    "label": 0
                },
                {
                    "sent": "But once it sees command, it has a pretty good idea.",
                    "label": 0
                },
                {
                    "sent": "The next word is going to be determined or like that.",
                    "label": 0
                },
                {
                    "sent": "So you get this pattern through the through the data, but the key.",
                    "label": 0
                },
                {
                    "sent": "The key thing that becomes obvious quickly is that.",
                    "label": 0
                },
                {
                    "sent": "Higher engrams are better but only with.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then within reason.",
                    "label": 0
                },
                {
                    "sent": "So the obvious issue is that if you do this sort of count based estimation.",
                    "label": 0
                },
                {
                    "sent": "Play.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so so you assume that you know there's some start of sentence that you're conditioning on.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the unigram probability, that's just the probability of at any point in a sentence using the word I, the bigram probability is that, given that you know the previous thing was nothing to start of the sentence probability of by.",
                    "label": 0
                },
                {
                    "sent": "So that's the probability that I will be the first word in the sentence, so that drops quickly, 'cause that's.",
                    "label": 0
                },
                {
                    "sent": "Eyes are pretty good way to start a sentence.",
                    "label": 0
                },
                {
                    "sent": "The trigram infogram give you know more information because once you know that it's the start of the sentence, there's nothing more to know, so that's why they're exactly the same probability.",
                    "label": 0
                },
                {
                    "sent": "And you'll see the same pattern for wood, so the bigram trigram picks up start.",
                    "label": 0
                },
                {
                    "sent": "I would wear as a bigram is just picking up, I would.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the first problem you're going to run into is once you start counting trigrams and four grams, you're not going to see that you're not going to see them that often.",
                    "label": 0
                },
                {
                    "sent": "So here we have the example.",
                    "label": 1
                },
                {
                    "sent": "I like to smooth.",
                    "label": 1
                },
                {
                    "sent": "You may never have seen that.",
                    "label": 0
                },
                {
                    "sent": "Full gram in your corpus you assign at zero probability.",
                    "label": 0
                },
                {
                    "sent": "Remember the probability of the whole string is the product of the probabilities of each conditional, so anything having zero probability is bad, because in the whole string has zero probability.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Higher order N grams are good, but until you stop seeing them then they're bad.",
                    "label": 0
                },
                {
                    "sent": "So you need to deal with this somewhere.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the most simplistic method in language modeling in popular across lots of other sort of multinomial based machine learning methods is what's called.",
                    "label": 0
                },
                {
                    "sent": "Add one smoothing.",
                    "label": 0
                },
                {
                    "sent": "So rather than just the raw counts, we just add 1 to every account.",
                    "label": 0
                },
                {
                    "sent": "That way will never have a count of 0.",
                    "label": 1
                },
                {
                    "sent": "And then we had to have to add the number of words in our vocabulary to the denominator as well.",
                    "label": 0
                },
                {
                    "sent": "So that's that's a trivial smoothing.",
                    "label": 1
                },
                {
                    "sent": "This is very popular for other sorts of anyway, and people are using multi normals.",
                    "label": 0
                },
                {
                    "sent": "And this is equivalent to some sort of display multinomial model.",
                    "label": 1
                },
                {
                    "sent": "If you're Bayesian with a parameter of two problem is, this really doesn't work for most interesting cases.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that V. If we're talking bout by grams, is the space of all all possible pairs of words.",
                    "label": 0
                },
                {
                    "sent": "So if you have sort of 90,000 words in your vocabulary, you have the square of that the number of bigrams.",
                    "label": 0
                },
                {
                    "sent": "That's a lot of bigrams so that V is going to be very big.",
                    "label": 0
                },
                {
                    "sent": "So what happens is you going to radically underestimate the probability.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not seeing any particular bigram, so that's bad.",
                    "label": 0
                },
                {
                    "sent": "So let's do something slightly more clever and have add Alpha smoothing so we're Alpha is less than one.",
                    "label": 0
                },
                {
                    "sent": "So now we can add a small amount.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have to be as big as one for each word.",
                    "label": 0
                },
                {
                    "sent": "So that works.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Better now, if we look at.",
                    "label": 0
                },
                {
                    "sent": "Sort of real frequencies from the europower corpus again.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the count of the number of times a word might have occurred in the training corpus.",
                    "label": 0
                },
                {
                    "sent": "This is the adjusted count.",
                    "label": 1
                },
                {
                    "sent": "If you do add one smoothing.",
                    "label": 0
                },
                {
                    "sent": "This is basically the what the.",
                    "label": 0
                },
                {
                    "sent": "They add one smoothing model, thinks the frequency of the word will be in a held out corpus, or the probability of it in holdout corpus.",
                    "label": 0
                },
                {
                    "sent": "This is at Alpha.",
                    "label": 0
                },
                {
                    "sent": "We were going to tune Alpha to be optimal by looking at the test set.",
                    "label": 0
                },
                {
                    "sent": "And here's the test account, so this is on some held out data, the probability.",
                    "label": 0
                },
                {
                    "sent": "So if things that you never saw, the bigrams you never saw in the training time.",
                    "label": 0
                },
                {
                    "sent": "So they have this probability in the test set.",
                    "label": 0
                },
                {
                    "sent": "So you'll always see things you never saw before.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at these two columns, clearly add one smoothing is bad, it radically underestimates the probabilities of especially frequent things.",
                    "label": 0
                },
                {
                    "sent": "If we if we look at the test set that we can tune this Alpha to try and make these match and we can get quite close.",
                    "label": 1
                },
                {
                    "sent": "You see that we're still so singletons words.",
                    "label": 0
                },
                {
                    "sent": "It occur once in orbit, bigrams that occur once in the training data are roughly overestimated by twice the probability of seeing them again.",
                    "label": 0
                },
                {
                    "sent": "So the model is very overconfident, so even when we're tuning on the test set.",
                    "label": 0
                },
                {
                    "sent": "So it gives you an idea about frequency, so we want to do this better and we want to do it without cheating.",
                    "label": 0
                },
                {
                    "sent": "So this is this is cheating to look at the test set and workout what the best value of Alpha is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is where.",
                    "label": 0
                },
                {
                    "sent": "Alan Turing and IJ good came in.",
                    "label": 0
                },
                {
                    "sent": "So they looked at this problem and tried to work out.",
                    "label": 0
                },
                {
                    "sent": "Given that I've seen a word a certain number of times in my sample, how?",
                    "label": 1
                },
                {
                    "sent": "How likely am I to see it again in a holdout sample?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "There's lots of math behind this, this estimate and what they what they tried to do is estimate the expected probability of a word that occurs a certain number of times at training time.",
                    "label": 1
                },
                {
                    "sent": "The expected underlying probability of it in the model.",
                    "label": 0
                },
                {
                    "sent": "So starting out with this, you can do a whole lot of derivations.",
                    "label": 0
                },
                {
                    "sent": "You have to do a few approximations on the along the way and you end up with this estimate.",
                    "label": 0
                },
                {
                    "sent": "And what this estimate says is that these are is our account from our training data.",
                    "label": 1
                },
                {
                    "sent": "These capital end their account of count, so that's a number of of things that we saw that in many times the number of things we thought our times or number of things we saw our plus one times.",
                    "label": 0
                },
                {
                    "sent": "So basically take this ratio of the number of things we saw.",
                    "label": 0
                },
                {
                    "sent": "Plus one times divided by number of things we saw our times scale it a little bit and we get what's called the good Turing estimate, and that's an approximation of this quantity.",
                    "label": 0
                },
                {
                    "sent": "So I'll give a reference later on where you can find the full derivation of that.",
                    "label": 0
                },
                {
                    "sent": "This turns out to be pretty effective and, well, the first thing to note is that this is this is going to be less than one, so the number on the top will be smaller than number on the bottom, so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So your adjusted count will be less than the observed count, which is basically what we want if we see it.",
                    "label": 0
                },
                {
                    "sent": "If we see something once, then we'll probably see it less frequently in the test data.",
                    "label": 0
                },
                {
                    "sent": "Just 'cause we have a poor estimate.",
                    "label": 0
                },
                {
                    "sent": "So again if we look at the frequencies here are counter counts.",
                    "label": 0
                },
                {
                    "sent": "The number of things that occurs 0 times user by grams.",
                    "label": 0
                },
                {
                    "sent": "You see most things we never see most things.",
                    "label": 0
                },
                {
                    "sent": "This is the key problem in language modeling.",
                    "label": 0
                },
                {
                    "sent": "As we get more frequently, see less and less of these.",
                    "label": 0
                },
                {
                    "sent": "Here are the good touring estimates.",
                    "label": 0
                },
                {
                    "sent": "Here are the test accounts.",
                    "label": 0
                },
                {
                    "sent": "When you see that that's pretty good.",
                    "label": 0
                },
                {
                    "sent": "That's a pretty good match for the frequency, so those guys were pretty smart.",
                    "label": 0
                },
                {
                    "sent": "So that gives you a way of basically guessing from my sample, how likely am I to actually see this?",
                    "label": 0
                },
                {
                    "sent": "In the future my test sample so that I don't overestimate.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we have a way of adjusting our counts, but we still have this problem of zeros.",
                    "label": 0
                },
                {
                    "sent": "So if we have these admittedly very rare trigrams Montreal, Poutine eaters or drinkers, and we want to know which ones more likely if they both just we've not seen either of them ever before.",
                    "label": 0
                },
                {
                    "sent": "So we're just going to say both 0, but we may have seen Poutine eaters quite a few times.",
                    "label": 1
                },
                {
                    "sent": "We may have seen this by Graham, and we may have never seen a poutine drinker, because it's just that doesn't make much sense.",
                    "label": 0
                },
                {
                    "sent": "So this is where the idea of backoff comes.",
                    "label": 0
                },
                {
                    "sent": "If we haven't seen the trigram or the higher order Ngram ever before, or frequently enough.",
                    "label": 0
                },
                {
                    "sent": "Then why don't we look at how frequent the bigram is as a way of smoothing or adjusting our estimate?",
                    "label": 0
                },
                {
                    "sent": "This is what's called back.",
                    "label": 0
                },
                {
                    "sent": "Often is the key.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Idea in traditional language modeling.",
                    "label": 0
                },
                {
                    "sent": "And this is hard back off, so this this idea is that OK if we want to estimate the probability of some ngram here.",
                    "label": 0
                },
                {
                    "sent": "If if we've seen that in Gram.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to use some adjusted adjusted probability of it.",
                    "label": 0
                },
                {
                    "sent": "And if we haven't seen it, then we're going to back off to a lower order Ngram.",
                    "label": 0
                },
                {
                    "sent": "So we're going to drop one of the words were conditioning on, so we're going to go from a trigram to a bigram, and then we're going to use its probability.",
                    "label": 0
                },
                {
                    "sent": "And these values out that these values have to be adjusted so the distribution still sums to one.",
                    "label": 0
                },
                {
                    "sent": "So the Alpha up there is going to be some quantity slightly less than the maximum likelihood probability, and then we get D, which we call the discount.",
                    "label": 0
                },
                {
                    "sent": "Which is the left over probability, so.",
                    "label": 0
                },
                {
                    "sent": "So some probability leftover and we can assign that to the lower order ngram.",
                    "label": 0
                },
                {
                    "sent": "So then we have to make decisions about how we should set these weights.",
                    "label": 0
                },
                {
                    "sent": "Essentially, these backoff backoff weights.",
                    "label": 0
                },
                {
                    "sent": "Sir.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Given what we saw previously.",
                    "label": 0
                },
                {
                    "sent": "The obvious thing to do is to use good Turing estimate, so this is what's called good during smoothing N gram language models.",
                    "label": 0
                },
                {
                    "sent": "So we just use good Turing estimate, so there's our maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "As I said, the good Turing estimate is going to be less than the.",
                    "label": 0
                },
                {
                    "sent": "The observed count.",
                    "label": 0
                },
                {
                    "sent": "So we replace the observed count with the good Turing count that gives us our Alpha as it's less than there's going to be some probability mass leftover here.",
                    "label": 1
                },
                {
                    "sent": "We assign that to D, the discounting function, and we're good.",
                    "label": 1
                },
                {
                    "sent": "And we do that through all the engrams.",
                    "label": 0
                },
                {
                    "sent": "And so that's a simple N gram language model.",
                    "label": 0
                },
                {
                    "sent": "It's pretty effective.",
                    "label": 0
                },
                {
                    "sent": "It's extremely easy to estimate and scale.",
                    "label": 0
                },
                {
                    "sent": "These things are incredibly scalable.",
                    "label": 0
                },
                {
                    "sent": "It's not to do this on a trillion words.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But language is more interesting than that so.",
                    "label": 0
                },
                {
                    "sent": "People who spend a lot of time thinking about language modeling and made various observations and two key ones are this idea of diversity in amongst out in grams.",
                    "label": 0
                },
                {
                    "sent": "So in this example we have the words Spuyten constant, so we're predicting.",
                    "label": 0
                },
                {
                    "sent": "So, given that we say spite.",
                    "label": 0
                },
                {
                    "sent": "We'll almost certainly see of afterwards.",
                    "label": 0
                },
                {
                    "sent": "'cause that's just a very frequent phrase, and we're very unlikely to see anything else.",
                    "label": 1
                },
                {
                    "sent": "Where is constant?",
                    "label": 0
                },
                {
                    "sent": "We could see all sorts of other things.",
                    "label": 0
                },
                {
                    "sent": "So we don't really want to.",
                    "label": 1
                },
                {
                    "sent": "Back off in the same way for these two different words that we're conditioning on.",
                    "label": 0
                },
                {
                    "sent": "So given what we are conditioning on, the probability of seeing something new is is different, and so we want to adjust for this.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to go through the whole ways of doing this, but as something called within Bell smoothing which does this, it tries to adjust for the diversity of predicted words.",
                    "label": 1
                },
                {
                    "sent": "Again, these are all in the references.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you want to know how this actually works, so the reverse of that idea is the diversity of histories.",
                    "label": 1
                },
                {
                    "sent": "So if I see the word York.",
                    "label": 1
                },
                {
                    "sent": "And so York is quite a frequent word.",
                    "label": 1
                },
                {
                    "sent": "In Europe, all it's a political corpus.",
                    "label": 1
                },
                {
                    "sent": "You're quite often see the word York.",
                    "label": 0
                },
                {
                    "sent": "It's almost always preceded by knew.",
                    "label": 0
                },
                {
                    "sent": "As in the reference to the City New York.",
                    "label": 0
                },
                {
                    "sent": "It's a frequency.",
                    "label": 0
                },
                {
                    "sent": "There's lots of other words, food as examples.",
                    "label": 0
                },
                {
                    "sent": "There foods indicates these words have far less restrictive of the words that come before them.",
                    "label": 0
                },
                {
                    "sent": "So again, we want to take this diversity into account and this is what Canessa nice moving does.",
                    "label": 0
                },
                {
                    "sent": "It takes that idea.",
                    "label": 0
                },
                {
                    "sent": "That you shouldn't.",
                    "label": 0
                },
                {
                    "sent": "So if you look at your unigram probability of York, it's it's going to radically overestimate the probability of seeing the word York unless you saw the word knew before it.",
                    "label": 0
                },
                {
                    "sent": "So you want to adjust for that.",
                    "label": 0
                },
                {
                    "sent": "That's connection ice moving.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So again, there's a big derivation behind Canadian ice moving.",
                    "label": 0
                },
                {
                    "sent": "It's quite interesting.",
                    "label": 0
                },
                {
                    "sent": "Lots of analysis of it, and there's been various connections made to various Bayesian nonparametric models of language.",
                    "label": 0
                },
                {
                    "sent": "It's also the most popular approach to language modeling, and here are some some rough.",
                    "label": 0
                },
                {
                    "sent": "Testing perplexities, for these different smoothing methods.",
                    "label": 1
                },
                {
                    "sent": "So there's lots of variants of connection, either something called, modified and interpolated.",
                    "label": 0
                },
                {
                    "sent": "That give you different performance.",
                    "label": 0
                },
                {
                    "sent": "I won't go into those, but.",
                    "label": 0
                },
                {
                    "sent": "The general pattern is that the connection is a very good ngram smoothing method, and that's what pretty much everyone uses.",
                    "label": 1
                },
                {
                    "sent": "Even good Turing smoothing is pretty effective considering how simple it is.",
                    "label": 0
                },
                {
                    "sent": "So interpolated connection.",
                    "label": 0
                },
                {
                    "sent": "I just interpolates the engrams rather than doing this hard back off.",
                    "label": 0
                },
                {
                    "sent": "And that that tends to be a little bit more effective.",
                    "label": 0
                },
                {
                    "sent": "Especially at the higher order ngram.",
                    "label": 0
                },
                {
                    "sent": "So if you're if you're sort of, if you're building a traditional speech recognition algorithm or machine translation system, or any of these things, one of the first things you are going to do is collect huge amounts of monolingual data.",
                    "label": 0
                },
                {
                    "sent": "Billions of words, preferably and estimator.",
                    "label": 0
                },
                {
                    "sent": "Can Essanay language model on that, and that will be your.",
                    "label": 0
                },
                {
                    "sent": "Your language model in speech people tend to use relatively short histories, often just trigrams in machine translation.",
                    "label": 0
                },
                {
                    "sent": "People use much longer histories depending on.",
                    "label": 0
                },
                {
                    "sent": "Often you'll go out to five or 6 grams on.",
                    "label": 0
                },
                {
                    "sent": "On some sort of selected smaller amount of data and four grams on a big amount of data.",
                    "label": 0
                },
                {
                    "sent": "So one of the problems with the N gram language model is that once you get the four grams is a lot of them.",
                    "label": 0
                },
                {
                    "sent": "So if you have a billion or a few billion.",
                    "label": 0
                },
                {
                    "sent": "Words, there's going to be a lot of different 4 grams, so there's going to be.",
                    "label": 0
                },
                {
                    "sent": "You're going to need a lot of memory to store these sorts of models.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's traditional engram language money to give you an idea of of really whether the traditional state of the art is in N gram language modeling.",
                    "label": 0
                },
                {
                    "sent": "These things are extremely scalable, they're just very easy to estimate on massive amounts of data.",
                    "label": 0
                },
                {
                    "sent": "There are very effective in these in existing speech recognition and translation models such that the most important part in in, say, a translation model currently is the language model.",
                    "label": 0
                },
                {
                    "sent": "If you want to make the if you want to make your translation model better, often getting more language modeling data, especially in domain data, is that the fastest way to do it.",
                    "label": 0
                },
                {
                    "sent": "So if you want to know more about this sort of stuff, this the Shannon Goodman references an excellent survey of all of these different techniques and has all the derivations.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so why would we want to do anything else than that?",
                    "label": 0
                },
                {
                    "sent": "So the problem with these count based traditional ngram models, the main there's two main problems.",
                    "label": 1
                },
                {
                    "sent": "One is they take a very symbolic view of of words, so so words are just symbols.",
                    "label": 0
                },
                {
                    "sent": "I mean they they turn out being integers in your implementation, but they don't have a relationship to each other, so we have no relationship between dog and cat and words like that that we think might be related.",
                    "label": 0
                },
                {
                    "sent": "So we don't capture semantic relationships.",
                    "label": 0
                },
                {
                    "sent": "We also don't capture morphological relationships, so learning that after in a particular context you're likely to see a verb or that after an adjective with a particular gender, you're likely to see a noun with a matching gender, and all these sorts of things.",
                    "label": 0
                },
                {
                    "sent": "That pattern is not learned.",
                    "label": 0
                },
                {
                    "sent": "You have to see each individual combination.",
                    "label": 0
                },
                {
                    "sent": "So that means that that ngram models at I'd say, data inefficient.",
                    "label": 0
                },
                {
                    "sent": "You need lots of data to get good engram estimates, and especially if you have a morphologically rich language, because you're going to have lots of different ways of realizing your underlying words, and you have to see each one of them to get good counts.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is that because of this ngram assumption, we don't capture long range dependencies, so that's bad for modeling things like the syntactic structure of a sentence.",
                    "label": 1
                },
                {
                    "sent": "You're not going to capture that, but it's also bad if you start wanting to extend this idea to the sorts of tasks I talked about earlier where we just wanted to do something like translation as a big joint sequence model, then that approach is going to have very long range dependencies because you're predicting some word 40 words into your translation, which is dependent on the word it's translating.",
                    "label": 0
                },
                {
                    "sent": "40 words back in the input translation, so an ngram model is never going to be able to do that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that leads us into probably somewhat more comfortable ground for everyone, which is neural networks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The classic neural network language model neural network approach to this idea is from Yoshua from about 12 years ago now.",
                    "label": 0
                },
                {
                    "sent": "And the idea is simple.",
                    "label": 0
                },
                {
                    "sent": "Let's just feed the we're going to stick with the N gram approach to language modeling on a fixed window.",
                    "label": 0
                },
                {
                    "sent": "We're going to predict the next word, but now, rather than just storing counts, were going to feed the context words into a feedforward neural network and predict the next word with a big softmax.",
                    "label": 0
                },
                {
                    "sent": "So that's where we're heading, but we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rewind a bit further back in history.",
                    "label": 0
                },
                {
                    "sent": "So after.",
                    "label": 0
                },
                {
                    "sent": "Sort of early boom in speech recognition and machine translation.",
                    "label": 1
                },
                {
                    "sent": "Early 90s people got very excited about what they called log linear language models in the in the late 90s and this is getting halfway between our traditional ngram model and our newer model.",
                    "label": 0
                },
                {
                    "sent": "And these are just based on log linear models or Maxim models as they are also called at the time and these are just a softmax classifier, so we assume that we take out.",
                    "label": 0
                },
                {
                    "sent": "So this is just a classification version.",
                    "label": 0
                },
                {
                    "sent": "We have some features, so in this case we just.",
                    "label": 0
                },
                {
                    "sent": "Assume that they are given to us by some feature detection functions and then we are predicting some class given some context X using our softmax.",
                    "label": 0
                },
                {
                    "sent": "So this should be all pretty familiar.",
                    "label": 0
                },
                {
                    "sent": "And when we when we grind through the gradient for this we see.",
                    "label": 0
                },
                {
                    "sent": "So this is the log negative log likelihood here.",
                    "label": 0
                },
                {
                    "sent": "We see the pattern we always see with this sort of models that we, the gradient is a difference of the features that we observe in our data and the expectation over the features that the model is giving us.",
                    "label": 0
                },
                {
                    "sent": "So we try and make these things match.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that log linear classification and we can turn this into a language model very simply by just defining features over our ngram contexts.",
                    "label": 0
                },
                {
                    "sent": "So the obvious thing to do is just to have a feature for every engram we might count in our traditional ngram model, so that's so if we got this, this trigram, this is America, we could have a feature function Phi here, that fires is has the value one when it sees the bigram context, this is.",
                    "label": 0
                },
                {
                    "sent": "So that's exactly like.",
                    "label": 0
                },
                {
                    "sent": "The weights on that are going to be learning our distribution for that that we would learn for that trigram.",
                    "label": 0
                },
                {
                    "sent": "If we're counting the reason we might want to take this log, linear approaches.",
                    "label": 0
                },
                {
                    "sent": "We can also have features which don't correspond to such engram, so we can just look at the word N -- 2 ago or N -- 1 just obviously equivalent to the bigram, but this has no equivalent in the traditional ngram model, so now we're overcoming a bit of the sparsity.",
                    "label": 0
                },
                {
                    "sent": "We can say that we're going to predict the word given the word two ago.",
                    "label": 0
                },
                {
                    "sent": "And skip the one in between.",
                    "label": 0
                },
                {
                    "sent": "So the the reason people got excited about log linear language modeling is it just gives you a lot more flexibility in how you define the features.",
                    "label": 0
                },
                {
                    "sent": "And you can have all sorts of things you can condition on the if you have part of speech tags, syntactic categories for the words you can condition on that you can get generalization across things like.",
                    "label": 0
                },
                {
                    "sent": "Context that nouns will occur in versus verbs, any sort of morphological analysis.",
                    "label": 0
                },
                {
                    "sent": "So now you have a lot more flexibility.",
                    "label": 0
                },
                {
                    "sent": "And you end up with a model defined something like this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to take a little sort of journey towards a neural language model.",
                    "label": 1
                },
                {
                    "sent": "So if we start out with our log linear model and we get rid of this traditional trigram feature and just keep these features which independently look at the context words, so then we can factor our our model based on these.",
                    "label": 1
                },
                {
                    "sent": "Now we have a different feature function for each word in the context, so that we're going to make that assumption.",
                    "label": 0
                },
                {
                    "sent": "I'm going to see where this takes us.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first thing we want to do is we want to get rid of this these feature functions.",
                    "label": 0
                },
                {
                    "sent": "We don't want to have to hand to find them.",
                    "label": 0
                },
                {
                    "sent": "It would be nice if we could just learn these things.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is say that each word in our context has some representation in a vector, and we're going to get those from a matrix.",
                    "label": 0
                },
                {
                    "sent": "Q.",
                    "label": 0
                },
                {
                    "sent": "So Q is just going to index all out all of our context words.",
                    "label": 0
                },
                {
                    "sent": "And I'll feature functions are just going to apply transformations.",
                    "label": 0
                },
                {
                    "sent": "The square matrices there C -- 2 and C -- 1 to each of these context words and give us some vector which is going to be our five features.",
                    "label": 1
                },
                {
                    "sent": "So now rather than finding our features, we're going to define.",
                    "label": 1
                },
                {
                    "sent": "Basically, this is a BI linear relationship too.",
                    "label": 0
                },
                {
                    "sent": "Now what I call the predicted features.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if we stick that into our model, we end up with something like this, and this is what's called the log by linear language model.",
                    "label": 1
                },
                {
                    "sent": "I think Russell mentioned this in his talk, but this is what it actually looks like, sort of unpacked formally.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So yes, we just got this bilinear relationship between the context and the predicted word.",
                    "label": 0
                },
                {
                    "sent": "This is very simple.",
                    "label": 0
                },
                {
                    "sent": "These models are actually very effective.",
                    "label": 0
                },
                {
                    "sent": "They're not very popular, but they are extremely effective.",
                    "label": 0
                },
                {
                    "sent": "They train really quickly because not having a nonlinearity in there means that you can train them really quickly, and they're pretty much as good as some of the more complicated neural language models, but there are very good simple.",
                    "label": 0
                },
                {
                    "sent": "Model so I think I'm.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I actually got so if we quickly go through the derivation of the gradients I won't labor this.",
                    "label": 0
                },
                {
                    "sent": "But the idea is that if you if you do all this, you see that what's happening is that the representation for networking representation for the word you're predicting.",
                    "label": 0
                },
                {
                    "sent": "Gets.",
                    "label": 0
                },
                {
                    "sent": "Gets pushed to match the representation for the predicted context.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "And this is just like what we saw with the log linear model.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "The representations of the context words get pushed to match the representation of the word.",
                    "label": 0
                },
                {
                    "sent": "There, they're predicting modulo the transformations.",
                    "label": 0
                },
                {
                    "sent": "So basically the context in the word predicting is sort of being pushed to match each other and.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why you get this?",
                    "label": 0
                },
                {
                    "sent": "This idea of these models learning representations of the words that match their context?",
                    "label": 0
                },
                {
                    "sent": "OK, and we can go through the same for the transformation matrices.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Now we want to make this a bit more interesting.",
                    "label": 0
                },
                {
                    "sent": "Very simply, if we just wrap our context transformation in some nonlinear function, like a sigmoid, tanh H rectified linear unit, then what we have is what you might call a proper neural language model.",
                    "label": 0
                },
                {
                    "sent": "So that's all we have to do, very simple we just stick F in there.",
                    "label": 0
                },
                {
                    "sent": "Now we've got a nonlinear relationship.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The gradient hardly changes thanks to the beauty of the chain rule and back propagation.",
                    "label": 0
                },
                {
                    "sent": "So those equations from previous slides are just unpacking backpropagation, so that's very simple going nonlinear is relatively trivial.",
                    "label": 0
                },
                {
                    "sent": "Downside is your models now much slower to train, but.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does work slightly better.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's how we can transgo make this journey from traditional N gram language models into a sort of neural distributed representation language model.",
                    "label": 0
                },
                {
                    "sent": "But we still have this problem that we're making the ngram assumption, so we're still cutting off our history and we're ignoring things way in the past and we don't want to do that if we want to do complex.",
                    "label": 0
                },
                {
                    "sent": "More complex joint distributions.",
                    "label": 0
                },
                {
                    "sent": "So the obvious thing to do is to make the transition to recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "Ann, I think these have been talked about quite a bit through the summer school, but the idea is is now rather than conditioning on a fixed context, we just keep the previous hidden layer around and we just currently use it to predict the next word.",
                    "label": 0
                },
                {
                    "sent": "So now, so that's in here.",
                    "label": 0
                },
                {
                    "sent": "So this is our hidden layer from our previous time step.",
                    "label": 0
                },
                {
                    "sent": "We transform it.",
                    "label": 0
                },
                {
                    "sent": "We take the previous word, we feed that in an.",
                    "label": 0
                },
                {
                    "sent": "We just do a prediction in the same way.",
                    "label": 0
                },
                {
                    "sent": "So simple recurrent language models like this don't work.",
                    "label": 0
                },
                {
                    "sent": "There's been plenty of papers saying they do work, but they don't.",
                    "label": 0
                },
                {
                    "sent": "Ngram models work better.",
                    "label": 0
                },
                {
                    "sent": "The problem is as probably being talked about is the idea of the vanishing gradient problem.",
                    "label": 0
                },
                {
                    "sent": "Recurrent neural networks just have a great deal of trouble propagating gradient signals very far, and generally they propagate them less far than you can just condition on with an N gram and thus get a more representative model.",
                    "label": 0
                },
                {
                    "sent": "So straight out sort of order what I'd call it a vanilla recurrent model is not really a great idea for language modeling, but there are plenty of ways of making this a better idea.",
                    "label": 0
                },
                {
                    "sent": "In the Classic One that's very popular at the moment.",
                    "label": 0
                },
                {
                    "sent": "Is the STM which gets around this problem.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's roughly a sort of.",
                    "label": 0
                },
                {
                    "sent": "Not very informative diagrammatic representation of a recurrent network.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is feeding the hidden layers in so to train these things, they're very simple to train.",
                    "label": 0
                },
                {
                    "sent": "We just unroll the recurrent network overtime over our sequence and then treat it like a big feedforward network.",
                    "label": 0
                },
                {
                    "sent": "There's a couple of there's some complexities in that idea in that if you have a very long sequence, say you want to model all of your language.",
                    "label": 0
                },
                {
                    "sent": "Modeling data is 1 big sequence, and you're not going to be able to do that in memory, so you want to cut it up into smaller sequences.",
                    "label": 0
                },
                {
                    "sent": "Referred to as backpropagation through time, so you can just cut your.",
                    "label": 0
                },
                {
                    "sent": "Cut your long sequence up into fixed length equal to maybe 20 time steps and do backpropagation through those and then pass the forward values onto the next segment.",
                    "label": 0
                },
                {
                    "sent": "Or if you're modeling fix, I think maybe you're modeling sentence is then you can just do the full full unrolling of the whole sentence so you're doing translation.",
                    "label": 0
                },
                {
                    "sent": "You can unroll across the full sequence and back propagate across the full sequence.",
                    "label": 0
                },
                {
                    "sent": "The complexity there becomes.",
                    "label": 0
                },
                {
                    "sent": "To do this efficiently, you want to do it on a GPU.",
                    "label": 0
                },
                {
                    "sent": "You then want to pack these sequences into a mini.",
                    "label": 0
                },
                {
                    "sent": "Batch sentences have different lengths, so your mini batch is no longer going to be square and then you'll have some interesting engineering problems to get the optimal.",
                    "label": 0
                },
                {
                    "sent": "Performance through that, but the basic idea is simple that you can just unroll.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unroll your current network and treat it like a big feed feedforward network.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So I think I was told that no one had really gone through the STM equations at all, so I thought I'd throw them in there.",
                    "label": 0
                },
                {
                    "sent": "Not entirely informative, but they're not as crazy as they look, so, so the LM is this idea of a better recurrent cell that's going to get around this problem of the gradient disappearing and the way it does.",
                    "label": 0
                },
                {
                    "sent": "That is what's highlighted in red here, and this is the key thing.",
                    "label": 0
                },
                {
                    "sent": "So C is sort of the, although we have a H down here, you can really sink C as the recurrent state that's going to be preserved through time.",
                    "label": 0
                },
                {
                    "sent": "And rather than multiplying the key thing here is that we're going to some, so that's really the key idea in the STM.",
                    "label": 0
                },
                {
                    "sent": "Rather than multiplying overtime, we're going to some and some other nice and linear.",
                    "label": 0
                },
                {
                    "sent": "They propagate gradients very well.",
                    "label": 0
                },
                {
                    "sent": "But we can't just some, because then we'll get a very boring linear model that's not really going to model anything interesting is just going to decompose.",
                    "label": 0
                },
                {
                    "sent": "So we need to produce to put back some nonlinearity, and we do that with these gating functions.",
                    "label": 0
                },
                {
                    "sent": "So we start out with this.",
                    "label": 0
                },
                {
                    "sent": "Starting ideas we want to some rather than the multiply we need to make some nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is rather than just mixing the previous state and this big thing, here is just our sort of information from our next.",
                    "label": 0
                },
                {
                    "sent": "Next input.",
                    "label": 0
                },
                {
                    "sent": "So rather than just summing these, we're going to sort of interpolate the mixom according to some gating functions F, which we call forget, which just means how much of the previous state do we do.",
                    "label": 0
                },
                {
                    "sent": "We keep and eye, which is the input gate, which is how much of the input information do we pass into the state.",
                    "label": 0
                },
                {
                    "sent": "Both of these are just defined a sigmoid function, so there between zero and one, so they're just scaling functions are at these these two things here are just multiplying by some value less than one, so they just down waiting each of these.",
                    "label": 0
                },
                {
                    "sent": "And that's that's really.",
                    "label": 0
                },
                {
                    "sent": "If you start there, the LM is relatively simple.",
                    "label": 0
                },
                {
                    "sent": "We just want to some.",
                    "label": 0
                },
                {
                    "sent": "We need to.",
                    "label": 0
                },
                {
                    "sent": "Therefore we need to get our previous information now in put in some way.",
                    "label": 0
                },
                {
                    "sent": "Once we go down this gating sort of idea, we might as well get the output as well.",
                    "label": 0
                },
                {
                    "sent": "Why not?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In that vein, this is just one instantiation of the STM.",
                    "label": 0
                },
                {
                    "sent": "There's lots of different variants of this.",
                    "label": 0
                },
                {
                    "sent": "People connections, and various things.",
                    "label": 0
                },
                {
                    "sent": "This ones relatively simple and also easy to optimize on a GPU, because if you look closely, all of these gates have very similar.",
                    "label": 0
                },
                {
                    "sent": "Very similar transformations here, so you can pack these all together.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what an STM.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looks like that's my rough diagrammatic output, I've just I've not shown at all what these gates are being conditioned on, but they just modulate the previous history, the input, and the output.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For completeness, I mean it's cool now to have deep LCMS, so this is what a deep LSM looks like.",
                    "label": 0
                },
                {
                    "sent": "All we have done is added the second dimension, which is the layer, and we feed the outputs.",
                    "label": 0
                },
                {
                    "sent": "Use wise of the previous layer into the next one as an input.",
                    "label": 0
                },
                {
                    "sent": "The way I've written it here, I've also include what I call skip connections, which means so X prime is the input to each layer.",
                    "label": 0
                },
                {
                    "sent": "It's a combination of the whole input, so this is the word we're conditioning on in our language model, and the input of the previous layer.",
                    "label": 0
                },
                {
                    "sent": "That's not terribly standard, but it works really well for language modeling, and we concatenate all of the outputs into the final output as well, so we get bigger.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Put Victor.",
                    "label": 0
                },
                {
                    "sent": "That's sort of what your deep recurrent network looks like.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's what it looks like.",
                    "label": 0
                },
                {
                    "sent": "If you had these skip connections.",
                    "label": 0
                },
                {
                    "sent": "For language modeling for all language modeling.",
                    "label": 0
                },
                {
                    "sent": "It depends how how keen you are to make it perform better, so it's still a bit up in the air and it depends how much data you have so.",
                    "label": 0
                },
                {
                    "sent": "To have a comment on this, but so.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you don't have much data.",
                    "label": 0
                },
                {
                    "sent": "The neural models perform much better than the traditional ngram models.",
                    "label": 0
                },
                {
                    "sent": "As you get more data.",
                    "label": 0
                },
                {
                    "sent": "The well the traditional models overtake the sort of feedforward neural models.",
                    "label": 0
                },
                {
                    "sent": "After a few billion.",
                    "label": 0
                },
                {
                    "sent": "Words, it's still.",
                    "label": 0
                },
                {
                    "sent": "I think the jury is still out in the STM because no ones really trained one that I've seen sort of passed about a billion words.",
                    "label": 0
                },
                {
                    "sent": "Not sure they've got much past that.",
                    "label": 0
                },
                {
                    "sent": "But I mean the thing to say about language modeling is a billion words is not a big language model.",
                    "label": 0
                },
                {
                    "sent": "It's a so.",
                    "label": 0
                },
                {
                    "sent": "And if you look in.",
                    "label": 0
                },
                {
                    "sent": "In in the sort of machine learning community, people are still publishing perplexity results in the Wall Street Journal, which is 1,000,000 words language modeling, and there it's easy to kill the traditional models.",
                    "label": 0
                },
                {
                    "sent": "But in industry, if you're building a translation system, you're going to be using trillions if you're going into English of words.",
                    "label": 0
                },
                {
                    "sent": "So that's that's what you actually care about, so.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty sure Nelson will be better on that scale.",
                    "label": 0
                },
                {
                    "sent": "The best results with these models are sort of often all sorts of model averaging combinations and things like that.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit complicated.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in most medium size scales that the LCM is going to win.",
                    "label": 0
                },
                {
                    "sent": "Yeah it works and it works better.",
                    "label": 0
                },
                {
                    "sent": "I mean you always combining models gives you a bit of a win, so the traditional Ingram models and all of the various newer models have different strengths.",
                    "label": 1
                },
                {
                    "sent": "If you actually analyze their probabilities over different types of engrams, they're quite complementary, so they're not not the newer models doing what the N gram model does better or anything like that.",
                    "label": 0
                },
                {
                    "sent": "They're doing different things.",
                    "label": 0
                },
                {
                    "sent": "The newer models are great at generalizing their greater things that you don't see very often.",
                    "label": 0
                },
                {
                    "sent": "The N gram models are great things you see often.",
                    "label": 0
                },
                {
                    "sent": "Because you just memorize them, and the interesting thing about languages, it's this combination of memorization and generalization.",
                    "label": 0
                },
                {
                    "sent": "So to do well at language, you have to memorize because that's part of the the language process, but you also have to generalize to the long tail.",
                    "label": 1
                },
                {
                    "sent": "So yes, combining them gets both, and that's still sort of interesting weakness of the neural models that are still not that great at memorizing.",
                    "label": 0
                },
                {
                    "sent": "Sorry, how many.",
                    "label": 0
                },
                {
                    "sent": "A person I have that number later on I have something related to I think I think I don't know about a particular age, but I think that the rough estimate I have later on is that an educated person would read about 100 million words in their lifetime.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "So, so we're training these models on a lot more than that.",
                    "label": 0
                },
                {
                    "sent": "But then again, people have a much more focused.",
                    "label": 0
                },
                {
                    "sent": "Corporate that they're they're reading from where is the model to just reading the web?",
                    "label": 0
                },
                {
                    "sent": "And there's there's a lot of stuff out there.",
                    "label": 0
                },
                {
                    "sent": "The mix quality.",
                    "label": 0
                },
                {
                    "sent": "OK, I probably better speed up a bit so.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A few comments on efficiency.",
                    "label": 0
                },
                {
                    "sent": "I think some of this being gone over, so I'll go over it quickly.",
                    "label": 0
                },
                {
                    "sent": "So the key problem with training neural models and why they're not so easy to scale is because you have this big softmax over cabul airy, and if you want these things to work well, you need a big vocabulary you really want to be looking for, well, 100,000.",
                    "label": 0
                },
                {
                    "sent": "Word types is is sort of.",
                    "label": 0
                },
                {
                    "sent": "Maybe the minimum you really want to be looking at a million.",
                    "label": 0
                },
                {
                    "sent": "So that's a big softmax and you don't want to be doing that for every time step, so there's lots of different ideas for how to do this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "One of the first ideas with the idea of shortlists.",
                    "label": 1
                },
                {
                    "sent": "Let's just do the softmax for the 10,000 most frequent words, and let's use our traditional ngram model.",
                    "label": 1
                },
                {
                    "sent": "For the rest.",
                    "label": 0
                },
                {
                    "sent": "The problem is that, as I said that the newer models are actually best at the infrequent thing, so just restricting them to the frequent things is sort of throwing away one of the main advantages, so that's not a great idea, sort of you're playing to the weaknesses of both models by doing that.",
                    "label": 0
                },
                {
                    "sent": "In a translation where the guys here at Montreal have been using this idea of having different shortlists for different segments of the training corpus, and this works quite well.",
                    "label": 0
                },
                {
                    "sent": "I mean you can see sort of statistically how these things will overlap and you're, and if they're big enough, then you'll probably get pretty stable learning.",
                    "label": 0
                },
                {
                    "sent": "So that idea is pretty effective.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it works very well on a GPU.",
                    "label": 0
                },
                {
                    "sent": "These other ideas are also very good, but don't work so well on GPU's.",
                    "label": 0
                },
                {
                    "sent": "Why yeah?",
                    "label": 0
                },
                {
                    "sent": "Examples.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's why.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's sort of a local local shortly, so you've got different ones.",
                    "label": 1
                },
                {
                    "sent": "Yeah, you're just partitioning up the vocabulary and there's going to be overlap and it should all.",
                    "label": 1
                },
                {
                    "sent": "If those those partitions are big enough, it should all be stable.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, it's fixed so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, and all of the frequent words will always end up in the normalizer, so you'll get.",
                    "label": 0
                },
                {
                    "sent": "And I think there's some theory.",
                    "label": 0
                },
                {
                    "sent": "If these shortlists are fully connected.",
                    "label": 0
                },
                {
                    "sent": "If you can get if they're not, they're not actually separate partitions, then the information will flow eventually through the model.",
                    "label": 0
                },
                {
                    "sent": "OK, but so other ideas, so this.",
                    "label": 1
                },
                {
                    "sent": "Idea of what's this contrastive estimation is also very effective, so the problem in in neural language models is that the normalizer.",
                    "label": 0
                },
                {
                    "sent": "If we could just treat that as some parameter that we estimated rather than actually.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regulating it that would be good.",
                    "label": 0
                },
                {
                    "sent": "We can't do that becausw.",
                    "label": 0
                },
                {
                    "sent": "We could just make that value.",
                    "label": 0
                },
                {
                    "sent": "Well, you make it sort of infinitely small and then get sort of infinite likelihood.",
                    "label": 0
                },
                {
                    "sent": "So a better idea is what Andre Min and UI proposed called contrastive estimation, where you sort of treat the problem rather than.",
                    "label": 0
                },
                {
                    "sent": "Predicting the next word as such, you predict the you do a binary prediction of what samples are in the data and then you sample some noise and you try and predict which which.",
                    "label": 0
                },
                {
                    "sent": "Which samples are from the noise in which samples are from the true data and you end up with an objective like this?",
                    "label": 0
                },
                {
                    "sent": "This works very well.",
                    "label": 0
                },
                {
                    "sent": "I won't Labour that too much.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main problem with that is it's a bit hard to get to work on a on a GPU really quickly.",
                    "label": 0
                },
                {
                    "sent": "Another idea is to factorize your output so rather than just predicting the words straight out, predict the class, predict some segmented class set of the words, and then predict the word in 2 steps.",
                    "label": 0
                },
                {
                    "sent": "This again is very effective.",
                    "label": 0
                },
                {
                    "sent": "You need to estimate the class is something that works very well, is known as Brown clustering.",
                    "label": 0
                },
                {
                    "sent": "People have used frequency being in other things.",
                    "label": 0
                },
                {
                    "sent": "That doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Sort of looks nice, but it doesn't actually work.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at what gets learned, you can take the factorization right down to a tree binary tree.",
                    "label": 0
                },
                {
                    "sent": "The problem with that is it's much harder to come up with a good tree.",
                    "label": 0
                },
                {
                    "sent": "We've tried quite a bit and it's it seems like it.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not worth the effort.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You know, sort of quick comparison to traditional models, so we've overcome these problems of sort of covariance between words.",
                    "label": 0
                },
                {
                    "sent": "We now in a neural model.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn that certain words have similar distributions.",
                    "label": 0
                },
                {
                    "sent": "The neural models are often very small.",
                    "label": 1
                },
                {
                    "sent": "I said that your big connection model is going to be huge even on a few billion words.",
                    "label": 1
                },
                {
                    "sent": "If you got 5 grams, you're going to end up with 30 gigabytes of counts on disk, and that's big, whereas neural language models are nice and small.",
                    "label": 0
                },
                {
                    "sent": "The ngram neural language models if you do a sort of like for like comparison and things like translations, even when they have much better perplexities.",
                    "label": 0
                },
                {
                    "sent": "They don't give us good translations which is interesting, and that's partly because of this difference in strengths.",
                    "label": 0
                },
                {
                    "sent": "The neural models really got the generalization that ngram models good at memorization.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the memorization aspect is more important in those sorts of tasks if you use them for rescoring then then they're great.",
                    "label": 1
                },
                {
                    "sent": "'cause I gotta extra information?",
                    "label": 0
                },
                {
                    "sent": "And again, as I talked about it, you get this sort of crossover effect with, with most of the newer models where if you have enough data then the sort of Canessa 9 memorization type approach tends to win out eventually.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I had some slides.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Morphology and Rep.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stations, but I might skip.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over that.",
                    "label": 0
                },
                {
                    "sent": "In the interest of time and fire into machine translation.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Quick polls for breath and then.",
                    "label": 0
                },
                {
                    "sent": "So now we've done language modeling, so we've got an idea of how to model these joint distributions of sequences.",
                    "label": 0
                },
                {
                    "sent": "Once we got to the recurrent networks then we can start modeling quite long range dependencies and now we might have the tools to do this sort of.",
                    "label": 0
                },
                {
                    "sent": "These translation ideas that I said from earlier on before going into that, let's start out with the traditional approach to translation and quick sort of my potted history of empty.",
                    "label": 0
                },
                {
                    "sent": "Part of this was for Joe, who hadn't heard some of these quotes before, so I've stuck them in, but the idea of translation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you if you know your Bible, you know the reference to the the Tower of Babel and this idea that.",
                    "label": 0
                },
                {
                    "sent": "Humans were cast out into the world with many different languages for being naughty, and now we have this problem of how do we communicate with each other?",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Machine translation we want to use our technology to overcome this.",
                    "label": 0
                },
                {
                    "sent": "So why is this hard?",
                    "label": 0
                },
                {
                    "sent": "Why can't we just look at look up the words in a dictionary?",
                    "label": 0
                },
                {
                    "sent": "And translate in that way and it's because languages are different.",
                    "label": 0
                },
                {
                    "sent": "They express things in different ways.",
                    "label": 0
                },
                {
                    "sent": "So here's some Arabic and Chinese examples now, to an English speaker that doesn't speak Arabic or China, use both of these.",
                    "label": 0
                },
                {
                    "sent": "Look equally incomprehensible.",
                    "label": 0
                },
                {
                    "sent": "It turns out that Arabic is much closer to English.",
                    "label": 0
                },
                {
                    "sent": "In the way it expresses things in Chinese is so if you look at these, the plane is fast in the train.",
                    "label": 0
                },
                {
                    "sent": "The Arabic is a rough gloss of that.",
                    "label": 0
                },
                {
                    "sent": "If you look at the Chinese then things are just said in a completely different way.",
                    "label": 0
                },
                {
                    "sent": "There's no one to one correspondence between words, so translation is hard.",
                    "label": 0
                },
                {
                    "sent": "And that's sort of the moral.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you look at European languages, you can get very close, but if you look at the full.",
                    "label": 0
                },
                {
                    "sent": "The full range of languages.",
                    "label": 0
                },
                {
                    "sent": "There's lots of interesting phenomena going on.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the basic problem we want to translate.",
                    "label": 0
                },
                {
                    "sent": "Given an input sentence, we want to produce output sentence.",
                    "label": 1
                },
                {
                    "sent": "Traditional view of translation is to ignore the discourse context and we just think of individual sentences.",
                    "label": 0
                },
                {
                    "sent": "So the first thing anyone doing this realizes that you can't just directly model this probability.",
                    "label": 0
                },
                {
                    "sent": "You have to break this down in some way.",
                    "label": 1
                },
                {
                    "sent": "And that's where all the work into.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Relation comes into.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the here's some of the history, so there's a famous memorandum from the 40s from known as the Weaver Memorandum, and Warren Weaver had a couple of choice quotes in this that in some sense sort of set the tone for a lot of translation research.",
                    "label": 0
                },
                {
                    "sent": "So in this one he's talking about languages, towers, and this idea of rather than trying to jump on your between these towels, we should just send to our some common language.",
                    "label": 0
                },
                {
                    "sent": "So the idea of an Interlingua universal language, and if we can learn this mapping.",
                    "label": 1
                },
                {
                    "sent": "From each language to the Interlingua, and from each from the Interlingua switch language.",
                    "label": 1
                },
                {
                    "sent": "Then we can do any form of translation.",
                    "label": 0
                },
                {
                    "sent": "And in a sense, this sort of led to what's known as a rule based approach to translation.",
                    "label": 0
                },
                {
                    "sent": "Trying to map to some generalization of language, some symbolic generalization that we can reason about.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That didn't get very far because language is just not that nicely behaved, and it's hard to analyze symbolically.",
                    "label": 0
                },
                {
                    "sent": "We also had another quote that was sort of forgotten about for awhile and then came back in the 80s, which is this idea that maybe we should.",
                    "label": 0
                },
                {
                    "sent": "Forget about thinking of these things as languages so much as just codes and this problem is cryptography.",
                    "label": 0
                },
                {
                    "sent": "We see a Chinese sentence in English sentence.",
                    "label": 0
                },
                {
                    "sent": "So we just think of the Chinese sentence as sort of a corrupted transmission of the English in some way, and we wanted to code it.",
                    "label": 0
                },
                {
                    "sent": "So that's very much an information theoretic view of translation, and that's really the origin of the statistical approach to translation.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So way back then people were thinking about this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we've sort of gave us this idea of statistical translation.",
                    "label": 0
                },
                {
                    "sent": "The next piece of the puzzle was.",
                    "label": 0
                },
                {
                    "sent": "You need you need data to get statistics and the great thing about translation is that the data is everywhere, so this is a Rosetta Stone.",
                    "label": 0
                },
                {
                    "sent": "From ancient Egypt way back then, people reducing producing parallel corpora.",
                    "label": 0
                },
                {
                    "sent": "This is that's Egyptian hieroglyphs, that's ancient.",
                    "label": 0
                },
                {
                    "sent": "That's Coptic, and that's ancient Greek.",
                    "label": 0
                },
                {
                    "sent": "When people found this stone they couldn't read.",
                    "label": 0
                },
                {
                    "sent": "The hieroglyphs they didn't know how to read Egyptian, but I knew quite a lot about ancient Greek and they they discovered quite a bit about Coptic, and they discovered that the three sections of the stone said exactly the same thing, so they use these two to code the hieroglyphs, and then we could.",
                    "label": 0
                },
                {
                    "sent": "We could read Egyptian an.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In translation, we just want to do the same thing statistically.",
                    "label": 0
                },
                {
                    "sent": "So the great thing about the modern world is we have the Internet and parallel corpora everywhere.",
                    "label": 1
                },
                {
                    "sent": "If you look at the this is the UN web page.",
                    "label": 0
                },
                {
                    "sent": "You can get it in Chinese.",
                    "label": 0
                },
                {
                    "sent": "You can get in English.",
                    "label": 0
                },
                {
                    "sent": "We can scrape this.",
                    "label": 0
                },
                {
                    "sent": "You can immediately discover how to translate United Nations.",
                    "label": 0
                },
                {
                    "sent": "An you can sort of guy.",
                    "label": 0
                },
                {
                    "sent": "So you can see that it's like this little this little sort of triangle symbol here.",
                    "label": 0
                },
                {
                    "sent": "Here it's around here and if you look on this side.",
                    "label": 0
                },
                {
                    "sent": "In the same place you see that I'm talking about people, humans, humanitarian spokesperson.",
                    "label": 0
                },
                {
                    "sent": "You can pretty quickly start to learn translation, so obviously that that Chinese character has something to do with people.",
                    "label": 0
                },
                {
                    "sent": "So we can get lots of this, and as I was saying.",
                    "label": 0
                },
                {
                    "sent": "In English we can get.",
                    "label": 1
                },
                {
                    "sent": "Well, the biggest the biggest parallel corpus in English is that the French, English Giga corpus, which is close to a billion words, translated text.",
                    "label": 0
                },
                {
                    "sent": "But for most European.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In which we can get sort of half a billion or more.",
                    "label": 0
                },
                {
                    "sent": "No, sorry, that's more like 100 million.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In the 80s there was this.",
                    "label": 0
                },
                {
                    "sent": "Sort of very well known group at IBM that had been doing a lot of pioneering work on speech recognition.",
                    "label": 1
                },
                {
                    "sent": "Led by Fred Jelinek.",
                    "label": 0
                },
                {
                    "sent": "And the story goes that they realized that their speech recognition system could be turned into a translation system, 'cause it's basically the same problem.",
                    "label": 0
                },
                {
                    "sent": "Fred said they weren't allowed to do this.",
                    "label": 0
                },
                {
                    "sent": "They had to make a speech recognition system better.",
                    "label": 0
                },
                {
                    "sent": "Fred went on holiday, so then the guys left behind implemented machine translation and the rest is history.",
                    "label": 0
                },
                {
                    "sent": "But one of the famous quotes from this time was was from Fred.",
                    "label": 1
                },
                {
                    "sent": "That said, who was?",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very much.",
                    "label": 0
                },
                {
                    "sent": "I think you could say a linguist.",
                    "label": 0
                },
                {
                    "sent": "But the frame is quite the saying, every time we got rid of language from his team, is this system got better?",
                    "label": 0
                },
                {
                    "sent": "But these guys are really pioneered the statistical approach to to empty.",
                    "label": 0
                },
                {
                    "sent": "Another great bit from history that I threw in.",
                    "label": 0
                },
                {
                    "sent": "Is so.",
                    "label": 0
                },
                {
                    "sent": "The IBM guys say they published a very famous paper that the origins of statistical machine translation in the early 90s.",
                    "label": 0
                },
                {
                    "sent": "Peter Brown and Bob Mercer, but they first submitted it to a conference and this is the review they got.",
                    "label": 0
                },
                {
                    "sent": "So back then reviews were handwritten.",
                    "label": 0
                },
                {
                    "sent": "You fax off your paper, wait for awhile and what is true so it got rejected and you probably can't read this.",
                    "label": 0
                },
                {
                    "sent": "But this says the validity validity of statistical statistical approach to machine translation has indeed been recognized.",
                    "label": 0
                },
                {
                    "sent": "As the authors mentioned by Weaver as early as 49.",
                    "label": 0
                },
                {
                    "sent": "And was universally recognized as mistaken by 1950.",
                    "label": 0
                },
                {
                    "sent": "With some references and then this great quote down the bottom, which is the crude force of computers, is not science so.",
                    "label": 0
                },
                {
                    "sent": "So the paper was rejected.",
                    "label": 0
                },
                {
                    "sent": "Thankfully it was published and that's that.",
                    "label": 0
                },
                {
                    "sent": "Play was the origin of.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the statistical approach to machine translation.",
                    "label": 1
                },
                {
                    "sent": "And the basic idea with this, which is the same formula that you have seen in the speech recognition talk.",
                    "label": 0
                },
                {
                    "sent": "Known as the noisy channel model.",
                    "label": 1
                },
                {
                    "sent": "Now, rather than trying to map from a microphone signal to a sentence, we're trying to map from some input source sentence to an output sentence, and we can use the same Bayes rule decomposition.",
                    "label": 0
                },
                {
                    "sent": "And we end up with this problem of estimating a conditional translation model and our old friend here joint Language model.",
                    "label": 1
                },
                {
                    "sent": "So again, This is why language modeling is crucial to MBTI, as it was to speech recognition.",
                    "label": 0
                },
                {
                    "sent": "So then they came up with a number of different models known as the IBM models.",
                    "label": 0
                },
                {
                    "sent": "For particularly estimating this bit.",
                    "label": 0
                },
                {
                    "sent": "They by that point they knew how to do this quite well with with ngram models.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can roughly think of this as this process, where you feed, say, French, is that the input language you feed it through your translation model, you get some sort of garbled English.",
                    "label": 0
                },
                {
                    "sent": "You feed it through the language model and you get something better.",
                    "label": 0
                },
                {
                    "sent": "But the real.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The real power behind that decomposition is just that we can get so much more data to estimate our language model.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to leverage that in our in our model, where the translation model is estimated from a lot less.",
                    "label": 0
                },
                {
                    "sent": "So also through this slide in, but I think some one of the other speaker, I think maybe Russell and said something about maybe someone will try so later on I'll talk about attention models in translation.",
                    "label": 0
                },
                {
                    "sent": "IBM tried it in the in the 90s and that's the first IBM model, so back then they have this great model which anyone that working on translation should implement.",
                    "label": 0
                },
                {
                    "sent": "This model, 'cause it's just one of the great models of sort of graphical models, so it's IBM Model 1 and this is this.",
                    "label": 0
                },
                {
                    "sent": "We're modeling the probability of a source sentence given the target sentence.",
                    "label": 0
                },
                {
                    "sent": "With this product.",
                    "label": 0
                },
                {
                    "sent": "These are just the individual source words.",
                    "label": 0
                },
                {
                    "sent": "Given the target words indexed by an alignment.",
                    "label": 0
                },
                {
                    "sent": "Asoue is a vector of alignments.",
                    "label": 0
                },
                {
                    "sent": "And that's latent.",
                    "label": 0
                },
                {
                    "sent": "So in today's terms, you call that the attention.",
                    "label": 0
                },
                {
                    "sent": "So for each word that you're predicting you're working out which word in the input it's coming from, and then you're predicting it.",
                    "label": 0
                },
                {
                    "sent": "The great thing about this model is one we can sum over these alignments in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "Which is cool, so it's easy to estimate.",
                    "label": 0
                },
                {
                    "sent": "It's extremely scalable.",
                    "label": 0
                },
                {
                    "sent": "Later IBM models lost that that property, but this very simple first model you can do polynomial inference.",
                    "label": 0
                },
                {
                    "sent": "The second thing is that this objective function is actually convex, which is pretty rare for these sorts of mixture EM models.",
                    "label": 0
                },
                {
                    "sent": "So this model is both scalable and it's convex, so you don't need to worry too much about where you start.",
                    "label": 0
                },
                {
                    "sent": "That's just just for historical purposes.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Post.",
                    "label": 0
                },
                {
                    "sent": "Post the IBM models that were word based.",
                    "label": 0
                },
                {
                    "sent": "People around about 2000 started building what we call phrase based models, where rather than looking at which producing a word condition on a single word in the in.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we produce groups of.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Words produce condition on groups of words, so it looks something like this, and that's all pretty word based.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But then we might produce our phrase for this book.",
                    "label": 0
                },
                {
                    "sent": "With the.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ordering again for the phrase to read to a single word on the output.",
                    "label": 1
                },
                {
                    "sent": "So this is what's known as a phrase based approach to translation.",
                    "label": 0
                },
                {
                    "sent": "We need to estimate big tables of phrases.",
                    "label": 0
                },
                {
                    "sent": "To do this, but this is basically sort of the state of the empty at the moment.",
                    "label": 1
                },
                {
                    "sent": "If you go to any industrial empty system, they're going to be doing something like this phrase based empty.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To estimate your phrase based empty model you need to go through a process like this, which is long complicated.",
                    "label": 0
                },
                {
                    "sent": "You start out with a large parallel corpus.",
                    "label": 1
                },
                {
                    "sent": "You train the old IBM word models.",
                    "label": 0
                },
                {
                    "sent": "That gives you an alignment.",
                    "label": 1
                },
                {
                    "sent": "You extract phrases, you get a phrase table.",
                    "label": 0
                },
                {
                    "sent": "Then you do some more training on a small corpus called discriminative training.",
                    "label": 1
                },
                {
                    "sent": "Then finally you got translation model.",
                    "label": 0
                },
                {
                    "sent": "So this long process, one of the motivations for the sort of neural stuff we talk about later is just to flatten all of this.",
                    "label": 0
                },
                {
                    "sent": "'cause this is long, complicated, very hand tuned, very difficult to adapt.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we'd like to get rid of all of that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that brings us to the.",
                    "label": 0
                },
                {
                    "sent": "Again, back to more familiar territory, which is OK, so that's traditional translation.",
                    "label": 0
                },
                {
                    "sent": "Phrase based translation is great, but it's not this big, complicated process.",
                    "label": 0
                },
                {
                    "sent": "It's all pretty heuristic, so there's not a great deal of principle behind what's going on.",
                    "label": 0
                },
                {
                    "sent": "It's just stuff people tried and it worked, and so we stuck with it.",
                    "label": 0
                },
                {
                    "sent": "The original IBM models they were they were very well conceived probabilistic models, but they didn't work as well as this sort of phrasal approach.",
                    "label": 0
                },
                {
                    "sent": "But now we want to.",
                    "label": 0
                },
                {
                    "sent": "Sort of take a step into the deep learning world and say, well, can we just simplify this down and say what?",
                    "label": 0
                },
                {
                    "sent": "If we just want to take some input sentence and produce some output sentence and we're just going to do it by generalizing this to a vector and then generating from the vector.",
                    "label": 0
                },
                {
                    "sent": "So we sort of going back to the old interlingual idea, but.",
                    "label": 0
                },
                {
                    "sent": "Not so much yet still still very language specific, so we need 2 parts.",
                    "label": 0
                },
                {
                    "sent": "We need to be able to generalize sentences to a vector and then we need to be able to generate from a vector.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we need two different parts.",
                    "label": 0
                },
                {
                    "sent": "Let's for a moment assume we have some way of generalizing to vector the question of how to generate it.",
                    "label": 0
                },
                {
                    "sent": "Turns out we can do that with our language model.",
                    "label": 0
                },
                {
                    "sent": "So this is you can't do this very easily with the traditional model, but this is one of the great things about neural models.",
                    "label": 0
                },
                {
                    "sent": "They're very easy to condition on extra information.",
                    "label": 0
                },
                {
                    "sent": "So now at the top I've got my sort of diagrammatic representation of the.",
                    "label": 0
                },
                {
                    "sent": "Ngram neural model, so we're just going to start very simply.",
                    "label": 0
                },
                {
                    "sent": "That's just going to be our language model over the output language, but at each step we're also going to incorporate this this hidden vector from the input, so it's just going to say we've got a language model, but it's going to be biased by this input it observes.",
                    "label": 0
                },
                {
                    "sent": "And can we with this simple setup, do translation?",
                    "label": 0
                },
                {
                    "sent": "So now the problem will come to search for that language model.",
                    "label": 0
                },
                {
                    "sent": "Basically start out with the start symbol and the vector from our input and look at the the basically greedily or with some sort of beam, enumerate the best words and see if we get a good translation.",
                    "label": 0
                },
                {
                    "sent": "So yes, this is just exactly the neural model from earlier with extra vector from the source sentence.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so again we need some way of getting the vector for the source sentence.",
                    "label": 0
                },
                {
                    "sent": "You should always start with the stupidest thing first to make sure that you're more complicated thing later on is actually working.",
                    "label": 0
                },
                {
                    "sent": "But the stupidest thing is just to say that maybe each source word has a vector.",
                    "label": 0
                },
                {
                    "sent": "We sum them all together and we get a vector for the whole sentence.",
                    "label": 0
                },
                {
                    "sent": "That stupid because we're throwing away the word order.",
                    "label": 0
                },
                {
                    "sent": "These vectors are just overwriting each other.",
                    "label": 0
                },
                {
                    "sent": "You can't possibly do translation that way, but you should always, as I say, start with the simplest one so we sum together vectors from the input.",
                    "label": 0
                },
                {
                    "sent": "We get a vector for the source sentence.",
                    "label": 0
                },
                {
                    "sent": "Then we generate from our translation model from our neural language model conditioned on that vector.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some examples.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some Chinese we sum all those vectors together, so obviously with backpropagation we can just learn all the parameters of this easily with these vectors cannot be learned from a parallel corpus, so there's a model I trained on a very small parallel corpus, only 50,000 sentences or something like that.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We generate from the language model an we get a perfect translation, which is pretty cool so.",
                    "label": 0
                },
                {
                    "sent": "First time I saw this, I didn't really understand what happened.",
                    "label": 0
                },
                {
                    "sent": "Because this shouldn't really work, but it did.",
                    "label": 0
                },
                {
                    "sent": "And if you can.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can be repeated.",
                    "label": 0
                },
                {
                    "sent": "So there's another example that works very well.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, and they even sort of about 10 words.",
                    "label": 0
                },
                {
                    "sent": "Reasonably long.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it says, and the Chinese is not simple.",
                    "label": 0
                },
                {
                    "sent": "So if you gloss that Chinese, you get something quite garbled from English.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you if you feed it in the Google Translate, it fails pretty dramatically.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The sort of truth in advertising.",
                    "label": 0
                },
                {
                    "sent": "Obviously I handpicked those examples when I did this.",
                    "label": 0
                },
                {
                    "sent": "It doesn't always work.",
                    "label": 0
                },
                {
                    "sent": "So in the red, that's what it should translate that sentence too.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's what it does.",
                    "label": 0
                },
                {
                    "sent": "Translate that sentence to.",
                    "label": 0
                },
                {
                    "sent": "So it's pretty high variance, But the interesting thing about this was just that.",
                    "label": 0
                },
                {
                    "sent": "Stop the generation so we include an end of sequence symbol so when the model predicts stop we stop.",
                    "label": 0
                },
                {
                    "sent": "I didn't say it was incorrect.",
                    "label": 0
                },
                {
                    "sent": "I said the the work that followed that, which was a symbolic approach to try and find this Interlingua.",
                    "label": 0
                },
                {
                    "sent": "Didn't didn't turn out like it's.",
                    "label": 0
                },
                {
                    "sent": "There's been quite a bit of work on Interlingua San and this has never really worked out, so the idea of a universal language.",
                    "label": 0
                },
                {
                    "sent": "Well, at least we've come nowhere near to finding such a thing.",
                    "label": 0
                },
                {
                    "sent": "Maybe, and as people are starting to do now with these deep learning models, I think more like a universal embedding space or something like that.",
                    "label": 0
                },
                {
                    "sent": "That's quite different to a something that looks like a human language but is universal in some way.",
                    "label": 0
                },
                {
                    "sent": "We're still some way to that.",
                    "label": 0
                },
                {
                    "sent": "These anything I'm training here.",
                    "label": 0
                },
                {
                    "sent": "We've got some vector generalization, but it still specific to this language bear.",
                    "label": 0
                },
                {
                    "sent": "But obviously you could think about training this for many languages and trying to find one vector embedding, so Christian.",
                    "label": 0
                },
                {
                    "sent": "I can't remember these examples.",
                    "label": 0
                },
                {
                    "sent": "It was pretty greedy, at least I sometimes used to be my can't remember them.",
                    "label": 0
                },
                {
                    "sent": "Yeah, which is not true of traditional translation models.",
                    "label": 0
                },
                {
                    "sent": "You need a big beam unless you've got quite a depends on the language pair.",
                    "label": 0
                },
                {
                    "sent": "And how much reordering is going on?",
                    "label": 0
                },
                {
                    "sent": "But for traditional models there's quite a lot of beam searching going on.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that was my sort of simple example that the take home message from that is not that you should build that sort of translation model.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work there.",
                    "label": 0
                },
                {
                    "sent": "They handpicked examples and obviously once you go past about 10 words, it just stops working entirely.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is just that it works at all and that tells you something about language.",
                    "label": 0
                },
                {
                    "sent": "That especially for output language like English, the word order is very constrained.",
                    "label": 0
                },
                {
                    "sent": "So if you know a few of the points in space, you know a few of the words, even without much information you can fill in what the rest is.",
                    "label": 0
                },
                {
                    "sent": "So in those Chinese examples, if I just give you the individual words in the Chinese and tell you to like, I give you the English translations and tell you to form an English sentence from those words, it will probably be close to the translation.",
                    "label": 0
                },
                {
                    "sent": "So particularly for English, which has quite fixed word order.",
                    "label": 0
                },
                {
                    "sent": "It's it's illustrative of just how constraining the local structure of languages, so it's not us.",
                    "label": 0
                },
                {
                    "sent": "This problem is not as hard as mapping from some arbitrary sequence to some arbitrary sequence.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of structure in the output which constrains what you could have.",
                    "label": 0
                },
                {
                    "sent": "OK, so after.",
                    "label": 0
                },
                {
                    "sent": "So that was just a toy experiment people started to do this more seriously.",
                    "label": 0
                },
                {
                    "sent": "So one thing that was done at Google from people like Ilya Master think, well, what if we just feed this into a big deep LS GM as a long sequence, like I sort of talked about to start with.",
                    "label": 0
                },
                {
                    "sent": "So here's our French Now separator symbol.",
                    "label": 0
                },
                {
                    "sent": "Then we feed that into the LS GM and then once we get to the separator, we just start generating from our language model.",
                    "label": 0
                },
                {
                    "sent": "The idea being that their model is just going to learn that dependencies.",
                    "label": 0
                },
                {
                    "sent": "And it turned out that they actually could get some pretty good translation, so this gets rounded.",
                    "label": 0
                },
                {
                    "sent": "So now the word order is going to be encoded in some way.",
                    "label": 0
                },
                {
                    "sent": "You get a lot more structure in the input than those examples I gave previously, so this starts to sound more plausable as a as a new.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Translation model.",
                    "label": 0
                },
                {
                    "sent": "Problem is you've got this.",
                    "label": 0
                },
                {
                    "sent": "You still got this at a big bottleneck here that some fixed width vector that you're pushing all of the input into this fixed width vector and this is not adaptive to how long the sentence is.",
                    "label": 0
                },
                {
                    "sent": "So whether you've got what we got there 5 words or 50 words.",
                    "label": 0
                },
                {
                    "sent": "You're pushing all of this into the same width vector and trying to read out the translation.",
                    "label": 0
                },
                {
                    "sent": "And so that doesn't seem very optimal, and we also know from.",
                    "label": 0
                },
                {
                    "sent": "We know about the structure of translation, especially for languages like French and English, that these roughly follow each other in a monotone.",
                    "label": 0
                },
                {
                    "sent": "So the first word here is going to be a translation of something near the start of the sentence, and it seems like a good idea to use this structure.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Illustrative of that is that when the Google guys actually did this, they had to reverse this sentence to make it work well, and the reason they do this.",
                    "label": 0
                },
                {
                    "sent": "Again, coming back to that to the idea of the constraining structure of the output language is that this brings these words.",
                    "label": 0
                },
                {
                    "sent": "Close together.",
                    "label": 0
                },
                {
                    "sent": "So and the problem in translation is to get the start of the sentence right.",
                    "label": 0
                },
                {
                    "sent": "If you can get the first couple of words right and you know the context, then it's much easier to get these words afterwards, right?",
                    "label": 0
                },
                {
                    "sent": "Because once you've seen a few words, there's a lot of constraint over what comes next, so if you look at the perplexities per word on the translations, the hardest thing is getting the first words right.",
                    "label": 0
                },
                {
                    "sent": "Once you've got that, then the rest sort of follows.",
                    "label": 0
                },
                {
                    "sent": "So what they're doing here is just bringing this starts together closely, so it's easier for them to learn.",
                    "label": 0
                },
                {
                    "sent": "And the bit that's easier, which is later on.",
                    "label": 0
                },
                {
                    "sent": "Is a bit easier.",
                    "label": 0
                },
                {
                    "sent": "That said, this still doesn't actually really work as a translation model.",
                    "label": 0
                },
                {
                    "sent": "It does surprisingly well, but once the sentences get long, you're going to have to have very deep big models and a lot of training time, and it's very in that sense.",
                    "label": 0
                },
                {
                    "sent": "It's very inefficient because you end up having to have a model that's big enough to represent the longest sequence you might encounter.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a better idea which sort of brings together the.",
                    "label": 0
                },
                {
                    "sent": "Old ideas of alignment in translation with these newer models and tries to exploit the structure as if the idea of attention which was done here in Montreal.",
                    "label": 0
                },
                {
                    "sent": "So now rather than just treating this as one big sequence and running our big.",
                    "label": 0
                },
                {
                    "sent": "Running the whole big sequence model through all of our input and output first, we're going to embed the input in some way and the the the classic thing to do now is to run bidirectional recurrent networks.",
                    "label": 0
                },
                {
                    "sent": "Over your input.",
                    "label": 0
                },
                {
                    "sent": "So this is a recurrent network going left to right and recurrent network going right to left and you can take the output hidden layers at each time step and concatenate them together and get a sort of concatenated representation of each time step.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to have a translation.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Process where again we're going to generate from out.",
                    "label": 0
                },
                {
                    "sent": "Output language model and at each time step we're going to do this attention operation, so we're going to compute a function of our output hidden representation.",
                    "label": 0
                },
                {
                    "sent": "All of these embeddings.",
                    "label": 0
                },
                {
                    "sent": "We're going to do this for each of our input time steps, and that's what all those little lines play there.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to do a softmax over those you've got a few options.",
                    "label": 0
                },
                {
                    "sent": "You can do individual sigmoids or you could do a softmax.",
                    "label": 0
                },
                {
                    "sent": "Everyone seems to be doing a softmax I think.",
                    "label": 0
                },
                {
                    "sent": "Seems to be the most popular.",
                    "label": 0
                },
                {
                    "sent": "That sort of enforces this idea that the word you're generating probably is coming from one or a small set of words in the input, not from lots of them.",
                    "label": 0
                },
                {
                    "sent": "That assumption is more or less good depending on the languages.",
                    "label": 0
                },
                {
                    "sent": "They're not.",
                    "label": 0
                },
                {
                    "sent": "They're not ordinary RNS there either.",
                    "label": 0
                },
                {
                    "sent": "So most of the Montreal ones were GI use, the gated recurrent units, which.",
                    "label": 0
                },
                {
                    "sent": "Similar using gate similar to STM to achieve the same sort of outcome and that slightly simpler package people are also doing this with LS teams.",
                    "label": 0
                },
                {
                    "sent": "So for this for the embedding, it matters less.",
                    "label": 0
                },
                {
                    "sent": "You definitely want something on the output, which is not an ordinary RNN.",
                    "label": 0
                },
                {
                    "sent": "On the input you because you're doing this attention and mixing, it's not clear how much you need information to propagate through that sequence.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "So we compute these attention variables and will come out with sort of some of them being greater, and I sort of tried to show that with the darker lines and all we do is just do like an expectation across seas or some so given the expectation of the attention waiting, we multiply that by the embedding vector, sum all these together.",
                    "label": 0
                },
                {
                    "sent": "That's what little some means up there.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we feed that into the next step, or in this case, the first step of our generation recurrent network and generate our first translation and.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we just keep doing this.",
                    "label": 0
                },
                {
                    "sent": "So at each time.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm step we compute in attention.",
                    "label": 0
                },
                {
                    "sent": "We mixed together the the source representations given that attention, and then we predict the next word and so.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's how we do our translation.",
                    "label": 0
                },
                {
                    "sent": "And this this is a much more plausible model of translation.",
                    "label": 0
                },
                {
                    "sent": "One it gets us much closer to the original model, so this is basically at least in my mind, a soft version of IBM Model 1, so I'd be a model one was computing individual alignments for each word generated.",
                    "label": 0
                },
                {
                    "sent": "It was computing individual alignments using stochastic variables, so therefore you could consider it hard alignment.",
                    "label": 0
                },
                {
                    "sent": "In that sense, this is a sort of soft alignment version.",
                    "label": 0
                },
                {
                    "sent": "There's a lot more going on 'cause we are obviously mixing together information on the input and output alot more.",
                    "label": 0
                },
                {
                    "sent": "But just like one thing to think about this is just like IBM Model 1.",
                    "label": 0
                },
                {
                    "sent": "These attention decisions are alignment decisions are relatively independent.",
                    "label": 0
                },
                {
                    "sent": "You get a little bit of dependence through this hidden layer, but we're not directly conditioning on a previous alignment.",
                    "label": 0
                },
                {
                    "sent": "I think lots of people have tried this or are trying this now, but that's one sort of obvious thing to think about this model, which doesn't capture some information we have in translation.",
                    "label": 0
                },
                {
                    "sent": "That is especially for French.",
                    "label": 0
                },
                {
                    "sent": "If the word to the left of me was attended to on the last time step, then I'll probably be attended to on the next time step, so that's something that a model can easily learn.",
                    "label": 0
                },
                {
                    "sent": "You then, if you could learn that, that's sort of what we call a monotone bias in translation.",
                    "label": 0
                },
                {
                    "sent": "If we can learn that, then we want to be able to more interesting things, like if if I'm a French noun.",
                    "label": 0
                },
                {
                    "sent": "Sorry if I'm a French adjective and always tended to in the last time step, then actually we go backwards in direction and will probably be the noun to my left which gets attended to the next time 'cause we have to reverse the order in English.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to be able to sort things, but this model?",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Does very well, so I so I stole some tables so that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the.",
                    "label": 0
                },
                {
                    "sent": "Letters on the top are a bit out of order, but I stole these tables from Cho.",
                    "label": 0
                },
                {
                    "sent": "With results from WMT with the attention model and these are all so the top two are going out of English in the bottom two are going into English out of English you can see that the the Montreal system is is winning and there's an into English in this case.",
                    "label": 0
                },
                {
                    "sent": "The models further down the rankings of all these, all the other systems are sort of traditional systems, probably also using various new neural networks and other things in them as well.",
                    "label": 0
                },
                {
                    "sent": "But you can see just from this you can see that the neural the attention model is is very competitive with the state of the art.",
                    "label": 1
                },
                {
                    "sent": "There's a lot of logic in why initially this might work well in languages other than English.",
                    "label": 0
                },
                {
                    "sent": "English.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The nice the nice thing about the structure is you're making you don't have to make any sort of structural.",
                    "label": 0
                },
                {
                    "sent": "Assumptions about how the words are dropped, or any or insert or anything like that.",
                    "label": 0
                },
                {
                    "sent": "This is all left up to the generating language model because it just chooses the next word.",
                    "label": 0
                },
                {
                    "sent": "So that's really nice, because if you've seen enough data, the language model in context is very strong.",
                    "label": 0
                },
                {
                    "sent": "It tells you whether you need to insert a article or things like that to make the just the local sequence high probability.",
                    "label": 0
                },
                {
                    "sent": "This is partly tide to this idea that initially these are working very well in languages other than English.",
                    "label": 0
                },
                {
                    "sent": "Because English is quite boring as a language in that there's no or very little morphology and agenda and these sorts of things.",
                    "label": 0
                },
                {
                    "sent": "These other languages like Check has a great deal more word internal structure in agreement structure going on, which traditional translation models have a great deal of trouble handling because they they basically have to have seen all the different combinations in order to give them a good probability, whereas the newer models tend to learn they can learn gender, they can learn these sorts of things and they can know that locali these things should agree.",
                    "label": 0
                },
                {
                    "sent": "So if I have a choice between.",
                    "label": 0
                },
                {
                    "sent": "A feminine or masculine.",
                    "label": 0
                },
                {
                    "sent": "Adjective or something like that.",
                    "label": 0
                },
                {
                    "sent": "The local context can tell me which one to choose.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so, and that's a key thing, so the traditional as I think I said this earlier in the traditional approach to empty, you have your translation model and you have your language model in your language models trained on a lot more data than the translation model an in English, it's a lot more data, so there's a big big difference and let it run it in a traditional translation model.",
                    "label": 0
                },
                {
                    "sent": "The language model is extremely important to performance, so the more language modeling data you get, the better the model gets because traditional translation models are actually pretty rubbish.",
                    "label": 0
                },
                {
                    "sent": "If you look at if you get rid of the language bowl and just look at the sort of distribution they have over translation, it's awful.",
                    "label": 0
                },
                {
                    "sent": "So most of the work is huge.",
                    "label": 1
                },
                {
                    "sent": "Amount of work is being done by the language model.",
                    "label": 0
                },
                {
                    "sent": "It's sort of been given this sort of rough collection of phrases and being told stitches together into something that looks roughly like English.",
                    "label": 0
                },
                {
                    "sent": "So the better you can make that, the higher your score.",
                    "label": 0
                },
                {
                    "sent": "So all of these sort of Edinburgh models and such for English.",
                    "label": 0
                },
                {
                    "sent": "They're going to be using.",
                    "label": 0
                },
                {
                    "sent": "Least 5 billion, probably more words of English in their language model, whereas the nearly empty system is probably trained on 30,000,000 or 50,000,000 words from the parallel corpus.",
                    "label": 0
                },
                {
                    "sent": "The Montreal guys have more recent work and how to overcome that problem, but that's that's another key.",
                    "label": 0
                },
                {
                    "sent": "Reason for those those different results, but one of I mean one of the things about this is that.",
                    "label": 0
                },
                {
                    "sent": "One of the.",
                    "label": 0
                },
                {
                    "sent": "Strong motivations for the neural approach machine translation is actually to go into languages other than English.",
                    "label": 0
                },
                {
                    "sent": "I mean, we were actually pretty good at translating into English.",
                    "label": 0
                },
                {
                    "sent": "And that's also in most industrial settings.",
                    "label": 0
                },
                {
                    "sent": "That's also not the most common scenario.",
                    "label": 0
                },
                {
                    "sent": "If you look at the Google Translate and things like that, mostly their translating out of English, because most of the web is in English and people want to be able to know what it means in their language.",
                    "label": 0
                },
                {
                    "sent": "And we're actually pretty bad at translating out of English becausw.",
                    "label": 1
                },
                {
                    "sent": "Because English is a bit of an outlier.",
                    "label": 0
                },
                {
                    "sent": "As language goes, it has a very fixed word order, not much morphology, so that very much plays into the strength of a traditional translation model, whereas less fixed word order.",
                    "label": 0
                },
                {
                    "sent": "Lots of morphology, in agreement which you see in sort of lots of other languages, especially European languages.",
                    "label": 0
                },
                {
                    "sent": "They're much worse at and the newer models are much better suited for.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Translation sorry, I skipped that so there's something called the blue score.",
                    "label": 0
                },
                {
                    "sent": "Which is basically an engram overlap measure, so you have for a test set for each input sentence you have, normally more than one.",
                    "label": 0
                },
                {
                    "sent": "Actually for WMT is normally only one, but in better death sets you have more than one possible translation and given a hypothesis translation you calculate engram overlap.",
                    "label": 0
                },
                {
                    "sent": "So you start with unigrams and bigrams and you calculate the what we call an engram precision.",
                    "label": 0
                },
                {
                    "sent": "So how many of the engrams I predicted were actually in the reference?",
                    "label": 0
                },
                {
                    "sent": "Then you basically just mix those together in a in an average with some with some weights.",
                    "label": 0
                },
                {
                    "sent": "And then there's something crucial which is called the brevity penalty, which means that one of the problems with precision is that you can just if you just output the as the translation of every sentence into English, you'll get 100% because precision doesn't account for the missing words, so you know the sentence is probably going to have in it, and so you get 100% precision.",
                    "label": 0
                },
                {
                    "sent": "So you need a brevity penalty that penalizes an output, which is much shorter than the.",
                    "label": 0
                },
                {
                    "sent": "The input, which is very important to know about becausw, that penalty is a bit strange.",
                    "label": 0
                },
                {
                    "sent": "It's exponential and getting a good Bleu score is often largely about getting the brevity penalty right, so blue scores are a flawed metric, but.",
                    "label": 0
                },
                {
                    "sent": "Yes, basically it's in gram overlap with some reference translations.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty much done.",
                    "label": 0
                }
            ]
        }
    }
}