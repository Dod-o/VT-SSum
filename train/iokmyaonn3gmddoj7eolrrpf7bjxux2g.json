{
    "id": "iokmyaonn3gmddoj7eolrrpf7bjxux2g",
    "title": "Polyhedral Outer Approximations with Application to Natural Language Parsing",
    "info": {
        "author": [
            "Andr\u00e9 F. T. Martins, Language Technologies Institute, Carnegie Mellon University"
        ],
        "published": "Sept. 17, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/icml09_martins_poaa/",
    "segmentation": [
        [
            "Right, so this is joint work with no Smith and direction at CMU.",
            "The title is polyhedral outer approximations with application to natural language parsing and.",
            "So this talk is not about symmetric learning."
        ],
        [
            "So this is about structure prediction and in this framework we are interested in modeling interdependence among output variables and this has been done has been done in models like the CRF's.",
            "Also, in entry ends and structured SVM, and what's all this?"
        ],
        [
            "Evan Common is that usually exact inference is only tractable if we resort to very local models.",
            "So we need to do strong locality assumptions.",
            "And."
        ],
        [
            "However, it happens that we can often improve performance by considering models that are nonlocal, and if we do approximate inference.",
            "At."
        ],
        [
            "On the other hand, sometimes there is some inherent constraints on the way the output variables should be defined, so sometimes they are globally constrained in some sense, and this happens every time we are dealing with computer problems that involve matchings, permutations, spanning trees, and things like that."
        ],
        [
            "So the question that we are trying to address here is out, does approximate inference affects the learning process?",
            "And this has also been studied before by Alyssa and Pereira and by filling in the vacuums."
        ],
        [
            "And in this paper we're going to focus on the LP relaxed inference scenario, and in the Max margin learning framework.",
            "We are going to provide guarantees for algorithm acceptability.",
            "We are going to interpret this framework as a way of balancing accuracy and penalizing computational costs, and we will provide also some bounds based on geometric characterizations of the proximation and application of interest here will be dependency parsing using rich set of features."
        ],
        [
            "So.",
            "Let's think about this example.",
            "We have a sentence."
        ],
        [
            "X.",
            "And we are going to parse this sentence and the formal formalism that we're going to use is dependency dependency trees, which are representation that tries to capture lexical relationships between words.",
            "So there's no constituents in the."
        ],
        [
            "Some relation, so for this sentence this will be the correct parts tree."
        ],
        [
            "We're going to denote by."
        ],
        [
            "My way of extra sets of legal dependency trees of X.",
            "So this is our search space and."
        ],
        [
            "Each element in this set is going to be spanning tree of the complete graph that links all pairs of words.",
            "So we are considering for those familiar with this problem.",
            "This is the non projective dependency parsing scenario."
        ],
        [
            "And our goal is to learn a parser which is a function H which Maps sentences in X to parse reason Y.",
            "And this is clearly a structural classification problem that involves nonlocal interactions among output variables.",
            "So if you notice, for example, these arcs are constrained to form a tree, so there is a ignorant global constraint here and also they interact in a known local way as we're going to see.",
            "So this is just to give a flyer flavor of."
        ],
        [
            "The kind of problems that we are interested in.",
            "In general, what we have."
        ],
        [
            "Is an input set X and output set Y.",
            "We are assuming this supervised scenario."
        ],
        [
            "Here we have a label data set."
        ],
        [
            "There is some loss function.",
            "And they are."
        ],
        [
            "Is to learn hypothesis with small X."
        ],
        [
            "Active loss on unseen data and we're going to focus on the case where with linear classifiers, which can be written as the argmax of linear discriminant function, where W represents the parameter vector and F of X&Y is the joint feature vector.",
            "So our hypothesis space is going to consist of these linear classifiers and we are constraining the parameter region to be a convex region of Euclidean space.",
            "OK, so typically instruc."
        ],
        [
            "Prediction, we assume that we we can decompose our outputs into parts.",
            "And examples of situations where this happens is that, for example, when parts are click assignments in a Markov network, this case has been intense."
        ],
        [
            "We studied before.",
            "Also, parts could be arcs, independency part three as."
        ],
        [
            "We have shown in the previous slide or even pairs or tuples of arcsin independency tree and we are going there in the experimental section.",
            "So in general we define a set of parts that we denote by R. We are going to replace our variable Y by an indicator vector Z with."
        ],
        [
            "One component breach parts and this vector is going to be one on those parts which are present in the output and zero on the other ones.",
            "And further, we're going to assume that the feature is decomposed over the parts, so this is also related to the way we define the parts themselves.",
            "And this means that the joint feature vector can be written as a Sam for all parts that exist in R of local feature vectors that look at X and that particular part are and we can get rid of the wise and write everything in terms of these using these feature matrix FX.",
            "So this is equal to F of X * Y where F of X is the future metrics.",
            "So each column of this matrix is going to be a feature vector that with respect to each of these parts.",
            "R. OK."
        ],
        [
            "So graphically, this is what we have.",
            "We started with a with an output set that we denoted by calligraphic Y.",
            "And instead."
        ],
        [
            "Otherwise, we're going to use indicator vector Z, so we have now this set that we call."
        ],
        [
            "V of Z and now we are going to consider the convex or which is this polytope here.",
            "So this is the convex Hull defined by the points that represent our outputs.",
            "And there is."
        ],
        [
            "The theorem that says that every time we have a polytope we can represent it using linear inequalities.",
            "For some metrics, A and some vector B.",
            "This is what we're doing here."
        ],
        [
            "And because of that, we can cast inference problem as a linear program and this has been done before by tascar."
        ],
        [
            "For other problems other than parsing so we can rewrite inference, which is this problem over here as the one of, so we're going to replace the variables wise by disease, so we can write everything using the feature matrix and vector indicator vector Z and now cause.",
            "This is equivalent to this linear program here, because it's known that the item is going to be at 10 at a vertex point of the polytope.",
            "OK, so this is how we can cast inference.",
            "Your S denotes the score vector.",
            "OK, so this seems."
        ],
        [
            "To be done, but we're not done because finding the Matrix A and vector B is something that is problem dependent sometimes."
        ],
        [
            "It's very hard, and in general we need exponentially many constraints in our in our linear program, so it's not often it's not feasible to look at the vertex points."
        ],
        [
            "Rectally.",
            "And what do we have sometimes?",
            "Unfortunately we have that in our problem is a concise representation of an outdoor polytope that we denote by Z bar which contains Z and that as a property that the integer points of the bar are precisely the vertices we are interested in.",
            "OK, so in that case we can rewrite our inference problem as this other problem which is an integer linear program.",
            "Notice that we are using these new polytopes.",
            "Are here and the bar."
        ],
        [
            "Contains Z, which is what we call the outer polytope, and we have this integer constraints here.",
            "So what we're going to do is to relax the constraints.",
            "This is not surprising has been done before, and we have this upper bound over here.",
            "OK, so we're just considering the other linear program that uses Z bar instead of Z.",
            "And what we're going to do here is to try to measure the impact of this relaxation in the learning problem.",
            "So."
        ],
        [
            "Let's now see what happens in learning."
        ],
        [
            "We're going to assume that the loss function also decomposes."
        ],
        [
            "Over the parts.",
            "And this happens with many losses of interest, like for them loss which you can write as the L1 distance between Z prime and Z.",
            "If we replace the wise by disease and we can also write it as an affine function of the prime.",
            "And because you have an affine function, this is going to be a nice property, so the learning framework that you are dealing with here is the structured SVM we can write."
        ],
        [
            "The objective is a combination of a regularization term.",
            "So this Lambda is a is a parameter that controls the amount of regularization and an average slack that we note by RT of W, and this lack is the solution of what is called the loss augmented inference problem.",
            "So this problem is is in some sense similar to influence, but you are augmented with the last term, so it's essentially the maximum over all possible outputs of the score of the particular output Z prime minus the score of the two outputs.",
            "So we're kind of interchanging the wise and disease here.",
            "This is the true output, ziti and this is augmented by the loss between Z prime and ziti that we can write as we saw in the previous slide as linear function.",
            "So the result is that everything becomes linear and we also have a linear program for the loss augmented inference problem.",
            "Let's see now what happens."
        ],
        [
            "When you when you consider the LP relaxation, so this is the exact formulation."
        ],
        [
            "That we just saw and."
        ],
        [
            "Now we relax and we get these.",
            "So what happens is that now you have relaxed Slack WRT bar and this relaxed Slack is a solution of the relaxed last submitted in France."
        ],
        [
            "Problem generally difference with respect to the previous problem is that now we have these outer polytopes Barry here."
        ],
        [
            "And this is obviously an upper bound of the exact slack becausw zebra contains Z, so this means that you are augmenting the physical set."
        ],
        [
            "And.",
            "It's also an upper bound of the true loss, because the exact slack was already an upper bound of the true loss."
        ],
        [
            "OK, so we can see these step of LP relaxed inference as augmenting the output space.",
            "So in some sense we are making up some artificial negative examples.",
            "Or"
        ],
        [
            "Presently you can think that we have an approximate algorithm that you are going to denote by calligraphic A of W, that sometimes."
        ],
        [
            "Instructional solutions so by fractional solutions I mean the fractional vertex of the polytope.",
            "And some definitions that were presented by: Pereira.",
            "We said at the data set is separable if there exists some parametrisation such that the parameterized hypothesis classifieds, all training data correctly.",
            "And we say that it's algorithmically separable if there is some parameter such that the parameterized algorithm is able to classify all data correctly, so it happens, it's clear that algorithmically separability implies a probability in our context, cause if we can separate out the negative examples, plus artificial ones, then you can also separate just the negative ones."
        ],
        [
            "OK, so another definition margin of separation.",
            "This is the standard definition.",
            "It's a minimal gamma such that for all data points in the training set and all possible YT prime candidates outputs, we have that the true score, the score of the true output is going to be larger than any other score plus gamma times the loss between draw outputs and 20 prime, and we are fixing the norm of W in this definition, so this is a standard definition of marginal separation."
        ],
        [
            "OK, we are now going to present our results regarding algorithmics separability.",
            "We first need to define some geometric quantities here.",
            "So let's go back to the polytope.",
            "Let's concentrate on the fractional vertex and that's going to to consider."
        ],
        [
            "A lost ball which is an L1 Bolt centered around one of these vertex until it's an exact vertex and we find this quantity L that."
        ],
        [
            "Is the radius of such ball we're going also to define nail prime, which is a similar ball, but is allowed to contain at least at most one integer."
        ],
        [
            "Optics in its interior.",
            "OK, so given that we're going to assume that in our problem we have binary valid features."
        ],
        [
            "We will define NF as the maximum number of active features per parts and we get these results.",
            "So if a data set."
        ],
        [
            "Is separable enough in the sense that the margin of separation is greater than this quantity?",
            "That depends on the quantity L prime that we define here and also in F. Then it is also algorithmically separable.",
            "OK, this is a nice result.",
            "So this means that for example, algorithms like the structured perceptron, when you replace the exact inference by our approximation is going to finish in a finite number of steps.",
            "Anne."
        ],
        [
            "But so there are also other ones that we're not going to discuss.",
            "We have bounds for the non separable case in the paper bounds in terms of the epsilon approximation that characterize the algorithm and so forth.",
            "I'm now going to talk about an interpretation in terms of balancing accuracy and run."
        ],
        [
            "Typical goal in machine learning algorithms is minimizing or trying to minimize expected loss."
        ],
        [
            "On unseen data, but in such a prediction, it's also important to take into consideration the computational costs of computing the hypothesis.",
            "So given hypothesis H and letter point X, we're going to define this quantity else of X of H-index as the cost of computing at the edge of X."
        ],
        [
            "And we're going to define the average compressional cost of H over the random variable."
        ],
        [
            "And we're going to put forth this alternative goal of minimizing a combination of the expected loss and the average costs of the hypothesis.",
            "And there is this thread of parameter ETA that controls that trades off."
        ],
        [
            "Can do things.",
            "OK, so let's recall that exact inference discusses an RFP, so we have this.",
            "We have seen this before."
        ],
        [
            "And this is again our polytopes.",
            "We have integer and fractional vertices."
        ],
        [
            "Let's imagine that our score vector is pointing into this direction if that happens."
        ],
        [
            "Going to be the fractional vertex, so this is bad because it means that we need to do further search to get the integer solutions right."
        ],
        [
            "On the other hand, if the score vector points in a nice direction, it's going to eat immediately introvertive, so it doesn't need to do further research.",
            "We."
        ],
        [
            "Done.",
            "And we call it an ice core vector."
        ],
        [
            "Situation."
        ],
        [
            "OK, so at this time it turns out that the score vector is a random variable.",
            "It depends on X via the feature matrix F of X.",
            "It also depends on our parameters double and."
        ],
        [
            "We are defining a low-cost hypothesis as one that puts lots of mass into nice core vectors.",
            "OK, so we're trying to model this kind of situation.",
            "And the idea is not to approximate computational costs by relaxation gap.",
            "We think that this is reasonable because most ILP solvers.",
            "Actually converge faster if this gap is smaller, so it becomes easier to get to the integer solution and we have some empirical evidence that."
        ],
        [
            "This is true."
        ],
        [
            "OK, so we are now going to read the empirical version of these relics."
        ],
        [
            "Gap.",
            "And if you do that, the learning problem becomes this."
        ],
        [
            "So this is exactly the same that we had before, but now you have a combination of the exact slack with relax Slack and there is this parameter ETA year OK?"
        ],
        [
            "So we're not talking."
        ],
        [
            "About the algorithm, but in the paper."
        ],
        [
            "You have an adaptation of the online subgradient algorithm of Ratliff.",
            "So this is basically it's gradient and online, so gradient descent.",
            "Algorithm to compute the solution of the SVM.",
            "We change it to be able to cope with this tradeoff between exact and relaxed."
        ],
        [
            "Augmented influence and we have also abound with respect to the best exact learner.",
            "So Deathbound tries to measure what is the impact in learning, so this is different from what has been done before, where the bounds were with respect of the approximate learning itself you were trying to quantify.",
            "What do you get with respect to the exact learner?",
            "Let's know discuss experiments."
        ],
        [
            "We did experiment."
        ],
        [
            "Dependency parsing.",
            "It's now."
        ],
        [
            "Um, that so I'm going just to summarize the state of the art methods for this problem, so it's known that exact inference is efficient if you have an arc factored model that assigns individual scores which arc and to solve the get the most likely parses equivalent of finding a maximum spanning tree, which is easy."
        ],
        [
            "Problem, but beyond that, it's NPR.",
            "Then this has been shown by McDonald's."
        ],
        [
            "1007 our model is an IOP formulation that includes features that are not arc factored.",
            "And so it models things like interactions between grandparents and siblings.",
            "It models valacia non projective arcs.",
            "And it does everything with a polynomial number of variables and constraints, so this is done using flow model.",
            "I'm not going to discuss the model.",
            "There is another paper that's going to appear on ACL.",
            "With this formulation is discussed.",
            "OK, so our first experiment is just training with you."
        ],
        [
            "Relaxed last augmented inference.",
            "So this is the most proximate case we are."
        ],
        [
            "Setting our parameter 8 to one so we are penalizing computational cost the most.",
            "And we tried two different decoders at Test time and exactly coder, which means solving the LP and then approximate decoder that solves the relaxation and round the solutions in polynomial time by using the maximum spanning tree."
        ],
        [
            "And we compare this against some strong baselines.",
            "The approximate 2nd order parser of McDonald in prayer and also stacked parser that uses 2 parsers, and it uses the outputs of the first parser as features to the second parser.",
            "So it's not a very nice architecture, but it pays off alot, so let's see what happens."
        ],
        [
            "This is what we get with the arc factored model.",
            "Overall all languages.",
            "And this is what we got the full model, so."
        ],
        [
            "There is roughly 2% improvements across all."
        ],
        [
            "Images and we can see that with approximate decoding it doesn't harm a lot, so this is not affecting accuracy by a lot.",
            "And the baseline."
        ],
        [
            "Are here so this is the 2nd order parser of McDonald's.",
            "We are doing a lot better for some language and not so much better in others."
        ],
        [
            "And this is what we get to the stack parser.",
            "So across all languages it performs comparably to our method."
        ],
        [
            "OK, I'm just finished finishing so this is the last experiment to measure.",
            "If ETA is really penalizing computational costs so."
        ],
        [
            "We are plotting here on the X axis the value of data set at training time and on the Y axis.",
            "The relaxation gap observed at Test time and it."
        ],
        [
            "Happens that if you're doing more exact training, we get a larger relaxation gap at Test time.",
            "So it's like 80 is if when you increase 8 at the model is learning to avoid fractional solutions.",
            "And this actually correlates with runtime.",
            "So we can see that runtime also increases when there are."
        ],
        [
            "Gap increases as we expected, but the proximity color is significantly faster, so there is a green line over here.",
            "Maybe you don't see it.",
            "This is runtime that we get with approximately coder.",
            "It's very fast."
        ],
        [
            "And in the full model, so this set of experiments is not and in full model, but in the full model the order of magnitude that we have is less than one second on average per sentence across."
        ],
        [
            "Language."
        ],
        [
            "And this is all that we have so."
        ],
        [
            "Here we studied the impact of helping relaxed inference in a Max margin framework we got.",
            "We stablished sufficient conditions that guarantee algorithmics, probability we prove."
        ],
        [
            "And interpretation in terms of penalizing computational costs."
        ],
        [
            "We show that this is effective for dependency parsing using an arc factored model."
        ],
        [
            "And in the future we are going to look at other characterizations to guarantee tighter bounds.",
            "And we want to investigate all this is relative with variable reception we did."
        ],
        [
            "Play a lot to be deregulation here.",
            "And."
        ],
        [
            "This is all that that thanks.",
            "Thanks for we do have time for a few questions."
        ],
        [
            "The sufficient condition for algorithmic separability.",
            "We have you ever looked at what L is for some instantiations.",
            "To me, like I will sometimes be of the order of magnitude N which case with your theorem says that determine huge margin in order to have.",
            "That is a very good question, and indeed the theorem is very weak in that sense.",
            "So this means that.",
            "So there is a Max, the maximum value of margin that we could achieve.",
            "We need to get at least half of that.",
            "So this condition is very is very very weak.",
            "We need to have a great margin in the separable case to guarantee algorithmics probability even if the approximation is good.",
            "But I mean there is a factor of L prime there over there which measures outside is the polytope, but we did not try for a particular problem.",
            "So not in this case in parsing it, so it's hard to do that.",
            "At least three in.",
            "And like we are in a very specific case.",
            "Right any other time?",
            "For one more question, quick question.",
            "OK, so let's think I'll speak again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so this is joint work with no Smith and direction at CMU.",
                    "label": 0
                },
                {
                    "sent": "The title is polyhedral outer approximations with application to natural language parsing and.",
                    "label": 1
                },
                {
                    "sent": "So this talk is not about symmetric learning.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is about structure prediction and in this framework we are interested in modeling interdependence among output variables and this has been done has been done in models like the CRF's.",
                    "label": 0
                },
                {
                    "sent": "Also, in entry ends and structured SVM, and what's all this?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Evan Common is that usually exact inference is only tractable if we resort to very local models.",
                    "label": 1
                },
                {
                    "sent": "So we need to do strong locality assumptions.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, it happens that we can often improve performance by considering models that are nonlocal, and if we do approximate inference.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, sometimes there is some inherent constraints on the way the output variables should be defined, so sometimes they are globally constrained in some sense, and this happens every time we are dealing with computer problems that involve matchings, permutations, spanning trees, and things like that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question that we are trying to address here is out, does approximate inference affects the learning process?",
                    "label": 0
                },
                {
                    "sent": "And this has also been studied before by Alyssa and Pereira and by filling in the vacuums.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in this paper we're going to focus on the LP relaxed inference scenario, and in the Max margin learning framework.",
                    "label": 1
                },
                {
                    "sent": "We are going to provide guarantees for algorithm acceptability.",
                    "label": 1
                },
                {
                    "sent": "We are going to interpret this framework as a way of balancing accuracy and penalizing computational costs, and we will provide also some bounds based on geometric characterizations of the proximation and application of interest here will be dependency parsing using rich set of features.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's think about this example.",
                    "label": 0
                },
                {
                    "sent": "We have a sentence.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "X.",
                    "label": 0
                },
                {
                    "sent": "And we are going to parse this sentence and the formal formalism that we're going to use is dependency dependency trees, which are representation that tries to capture lexical relationships between words.",
                    "label": 1
                },
                {
                    "sent": "So there's no constituents in the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some relation, so for this sentence this will be the correct parts tree.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to denote by.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My way of extra sets of legal dependency trees of X.",
                    "label": 0
                },
                {
                    "sent": "So this is our search space and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each element in this set is going to be spanning tree of the complete graph that links all pairs of words.",
                    "label": 1
                },
                {
                    "sent": "So we are considering for those familiar with this problem.",
                    "label": 1
                },
                {
                    "sent": "This is the non projective dependency parsing scenario.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And our goal is to learn a parser which is a function H which Maps sentences in X to parse reason Y.",
                    "label": 1
                },
                {
                    "sent": "And this is clearly a structural classification problem that involves nonlocal interactions among output variables.",
                    "label": 0
                },
                {
                    "sent": "So if you notice, for example, these arcs are constrained to form a tree, so there is a ignorant global constraint here and also they interact in a known local way as we're going to see.",
                    "label": 0
                },
                {
                    "sent": "So this is just to give a flyer flavor of.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The kind of problems that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "In general, what we have.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is an input set X and output set Y.",
                    "label": 0
                },
                {
                    "sent": "We are assuming this supervised scenario.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have a label data set.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is some loss function.",
                    "label": 0
                },
                {
                    "sent": "And they are.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is to learn hypothesis with small X.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Active loss on unseen data and we're going to focus on the case where with linear classifiers, which can be written as the argmax of linear discriminant function, where W represents the parameter vector and F of X&Y is the joint feature vector.",
                    "label": 0
                },
                {
                    "sent": "So our hypothesis space is going to consist of these linear classifiers and we are constraining the parameter region to be a convex region of Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "OK, so typically instruc.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prediction, we assume that we we can decompose our outputs into parts.",
                    "label": 0
                },
                {
                    "sent": "And examples of situations where this happens is that, for example, when parts are click assignments in a Markov network, this case has been intense.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We studied before.",
                    "label": 0
                },
                {
                    "sent": "Also, parts could be arcs, independency part three as.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have shown in the previous slide or even pairs or tuples of arcsin independency tree and we are going there in the experimental section.",
                    "label": 0
                },
                {
                    "sent": "So in general we define a set of parts that we denote by R. We are going to replace our variable Y by an indicator vector Z with.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One component breach parts and this vector is going to be one on those parts which are present in the output and zero on the other ones.",
                    "label": 0
                },
                {
                    "sent": "And further, we're going to assume that the feature is decomposed over the parts, so this is also related to the way we define the parts themselves.",
                    "label": 0
                },
                {
                    "sent": "And this means that the joint feature vector can be written as a Sam for all parts that exist in R of local feature vectors that look at X and that particular part are and we can get rid of the wise and write everything in terms of these using these feature matrix FX.",
                    "label": 0
                },
                {
                    "sent": "So this is equal to F of X * Y where F of X is the future metrics.",
                    "label": 0
                },
                {
                    "sent": "So each column of this matrix is going to be a feature vector that with respect to each of these parts.",
                    "label": 0
                },
                {
                    "sent": "R. OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So graphically, this is what we have.",
                    "label": 0
                },
                {
                    "sent": "We started with a with an output set that we denoted by calligraphic Y.",
                    "label": 1
                },
                {
                    "sent": "And instead.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Otherwise, we're going to use indicator vector Z, so we have now this set that we call.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "V of Z and now we are going to consider the convex or which is this polytope here.",
                    "label": 0
                },
                {
                    "sent": "So this is the convex Hull defined by the points that represent our outputs.",
                    "label": 0
                },
                {
                    "sent": "And there is.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The theorem that says that every time we have a polytope we can represent it using linear inequalities.",
                    "label": 0
                },
                {
                    "sent": "For some metrics, A and some vector B.",
                    "label": 0
                },
                {
                    "sent": "This is what we're doing here.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And because of that, we can cast inference problem as a linear program and this has been done before by tascar.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For other problems other than parsing so we can rewrite inference, which is this problem over here as the one of, so we're going to replace the variables wise by disease, so we can write everything using the feature matrix and vector indicator vector Z and now cause.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent to this linear program here, because it's known that the item is going to be at 10 at a vertex point of the polytope.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is how we can cast inference.",
                    "label": 0
                },
                {
                    "sent": "Your S denotes the score vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so this seems.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be done, but we're not done because finding the Matrix A and vector B is something that is problem dependent sometimes.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's very hard, and in general we need exponentially many constraints in our in our linear program, so it's not often it's not feasible to look at the vertex points.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rectally.",
                    "label": 0
                },
                {
                    "sent": "And what do we have sometimes?",
                    "label": 0
                },
                {
                    "sent": "Unfortunately we have that in our problem is a concise representation of an outdoor polytope that we denote by Z bar which contains Z and that as a property that the integer points of the bar are precisely the vertices we are interested in.",
                    "label": 1
                },
                {
                    "sent": "OK, so in that case we can rewrite our inference problem as this other problem which is an integer linear program.",
                    "label": 0
                },
                {
                    "sent": "Notice that we are using these new polytopes.",
                    "label": 0
                },
                {
                    "sent": "Are here and the bar.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Contains Z, which is what we call the outer polytope, and we have this integer constraints here.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is to relax the constraints.",
                    "label": 0
                },
                {
                    "sent": "This is not surprising has been done before, and we have this upper bound over here.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're just considering the other linear program that uses Z bar instead of Z.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do here is to try to measure the impact of this relaxation in the learning problem.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's now see what happens in learning.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to assume that the loss function also decomposes.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Over the parts.",
                    "label": 0
                },
                {
                    "sent": "And this happens with many losses of interest, like for them loss which you can write as the L1 distance between Z prime and Z.",
                    "label": 0
                },
                {
                    "sent": "If we replace the wise by disease and we can also write it as an affine function of the prime.",
                    "label": 1
                },
                {
                    "sent": "And because you have an affine function, this is going to be a nice property, so the learning framework that you are dealing with here is the structured SVM we can write.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The objective is a combination of a regularization term.",
                    "label": 0
                },
                {
                    "sent": "So this Lambda is a is a parameter that controls the amount of regularization and an average slack that we note by RT of W, and this lack is the solution of what is called the loss augmented inference problem.",
                    "label": 1
                },
                {
                    "sent": "So this problem is is in some sense similar to influence, but you are augmented with the last term, so it's essentially the maximum over all possible outputs of the score of the particular output Z prime minus the score of the two outputs.",
                    "label": 0
                },
                {
                    "sent": "So we're kind of interchanging the wise and disease here.",
                    "label": 0
                },
                {
                    "sent": "This is the true output, ziti and this is augmented by the loss between Z prime and ziti that we can write as we saw in the previous slide as linear function.",
                    "label": 0
                },
                {
                    "sent": "So the result is that everything becomes linear and we also have a linear program for the loss augmented inference problem.",
                    "label": 0
                },
                {
                    "sent": "Let's see now what happens.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When you when you consider the LP relaxation, so this is the exact formulation.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we just saw and.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we relax and we get these.",
                    "label": 0
                },
                {
                    "sent": "So what happens is that now you have relaxed Slack WRT bar and this relaxed Slack is a solution of the relaxed last submitted in France.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem generally difference with respect to the previous problem is that now we have these outer polytopes Barry here.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is obviously an upper bound of the exact slack becausw zebra contains Z, so this means that you are augmenting the physical set.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It's also an upper bound of the true loss, because the exact slack was already an upper bound of the true loss.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we can see these step of LP relaxed inference as augmenting the output space.",
                    "label": 1
                },
                {
                    "sent": "So in some sense we are making up some artificial negative examples.",
                    "label": 0
                },
                {
                    "sent": "Or",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Presently you can think that we have an approximate algorithm that you are going to denote by calligraphic A of W, that sometimes.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instructional solutions so by fractional solutions I mean the fractional vertex of the polytope.",
                    "label": 1
                },
                {
                    "sent": "And some definitions that were presented by: Pereira.",
                    "label": 1
                },
                {
                    "sent": "We said at the data set is separable if there exists some parametrisation such that the parameterized hypothesis classifieds, all training data correctly.",
                    "label": 1
                },
                {
                    "sent": "And we say that it's algorithmically separable if there is some parameter such that the parameterized algorithm is able to classify all data correctly, so it happens, it's clear that algorithmically separability implies a probability in our context, cause if we can separate out the negative examples, plus artificial ones, then you can also separate just the negative ones.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so another definition margin of separation.",
                    "label": 0
                },
                {
                    "sent": "This is the standard definition.",
                    "label": 0
                },
                {
                    "sent": "It's a minimal gamma such that for all data points in the training set and all possible YT prime candidates outputs, we have that the true score, the score of the true output is going to be larger than any other score plus gamma times the loss between draw outputs and 20 prime, and we are fixing the norm of W in this definition, so this is a standard definition of marginal separation.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we are now going to present our results regarding algorithmics separability.",
                    "label": 0
                },
                {
                    "sent": "We first need to define some geometric quantities here.",
                    "label": 0
                },
                {
                    "sent": "So let's go back to the polytope.",
                    "label": 0
                },
                {
                    "sent": "Let's concentrate on the fractional vertex and that's going to to consider.",
                    "label": 1
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lost ball which is an L1 Bolt centered around one of these vertex until it's an exact vertex and we find this quantity L that.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the radius of such ball we're going also to define nail prime, which is a similar ball, but is allowed to contain at least at most one integer.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optics in its interior.",
                    "label": 0
                },
                {
                    "sent": "OK, so given that we're going to assume that in our problem we have binary valid features.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will define NF as the maximum number of active features per parts and we get these results.",
                    "label": 0
                },
                {
                    "sent": "So if a data set.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is separable enough in the sense that the margin of separation is greater than this quantity?",
                    "label": 0
                },
                {
                    "sent": "That depends on the quantity L prime that we define here and also in F. Then it is also algorithmically separable.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a nice result.",
                    "label": 0
                },
                {
                    "sent": "So this means that for example, algorithms like the structured perceptron, when you replace the exact inference by our approximation is going to finish in a finite number of steps.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But so there are also other ones that we're not going to discuss.",
                    "label": 0
                },
                {
                    "sent": "We have bounds for the non separable case in the paper bounds in terms of the epsilon approximation that characterize the algorithm and so forth.",
                    "label": 0
                },
                {
                    "sent": "I'm now going to talk about an interpretation in terms of balancing accuracy and run.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Typical goal in machine learning algorithms is minimizing or trying to minimize expected loss.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On unseen data, but in such a prediction, it's also important to take into consideration the computational costs of computing the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So given hypothesis H and letter point X, we're going to define this quantity else of X of H-index as the cost of computing at the edge of X.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we're going to define the average compressional cost of H over the random variable.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we're going to put forth this alternative goal of minimizing a combination of the expected loss and the average costs of the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And there is this thread of parameter ETA that controls that trades off.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can do things.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's recall that exact inference discusses an RFP, so we have this.",
                    "label": 0
                },
                {
                    "sent": "We have seen this before.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is again our polytopes.",
                    "label": 0
                },
                {
                    "sent": "We have integer and fractional vertices.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's imagine that our score vector is pointing into this direction if that happens.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to be the fractional vertex, so this is bad because it means that we need to do further search to get the integer solutions right.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, if the score vector points in a nice direction, it's going to eat immediately introvertive, so it doesn't need to do further research.",
                    "label": 0
                },
                {
                    "sent": "We.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Done.",
                    "label": 0
                },
                {
                    "sent": "And we call it an ice core vector.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Situation.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so at this time it turns out that the score vector is a random variable.",
                    "label": 0
                },
                {
                    "sent": "It depends on X via the feature matrix F of X.",
                    "label": 0
                },
                {
                    "sent": "It also depends on our parameters double and.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are defining a low-cost hypothesis as one that puts lots of mass into nice core vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're trying to model this kind of situation.",
                    "label": 0
                },
                {
                    "sent": "And the idea is not to approximate computational costs by relaxation gap.",
                    "label": 0
                },
                {
                    "sent": "We think that this is reasonable because most ILP solvers.",
                    "label": 0
                },
                {
                    "sent": "Actually converge faster if this gap is smaller, so it becomes easier to get to the integer solution and we have some empirical evidence that.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is true.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we are now going to read the empirical version of these relics.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gap.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, the learning problem becomes this.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is exactly the same that we had before, but now you have a combination of the exact slack with relax Slack and there is this parameter ETA year OK?",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're not talking.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About the algorithm, but in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have an adaptation of the online subgradient algorithm of Ratliff.",
                    "label": 0
                },
                {
                    "sent": "So this is basically it's gradient and online, so gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Algorithm to compute the solution of the SVM.",
                    "label": 0
                },
                {
                    "sent": "We change it to be able to cope with this tradeoff between exact and relaxed.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Augmented influence and we have also abound with respect to the best exact learner.",
                    "label": 0
                },
                {
                    "sent": "So Deathbound tries to measure what is the impact in learning, so this is different from what has been done before, where the bounds were with respect of the approximate learning itself you were trying to quantify.",
                    "label": 0
                },
                {
                    "sent": "What do you get with respect to the exact learner?",
                    "label": 0
                },
                {
                    "sent": "Let's know discuss experiments.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did experiment.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dependency parsing.",
                    "label": 0
                },
                {
                    "sent": "It's now.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, that so I'm going just to summarize the state of the art methods for this problem, so it's known that exact inference is efficient if you have an arc factored model that assigns individual scores which arc and to solve the get the most likely parses equivalent of finding a maximum spanning tree, which is easy.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem, but beyond that, it's NPR.",
                    "label": 0
                },
                {
                    "sent": "Then this has been shown by McDonald's.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1007 our model is an IOP formulation that includes features that are not arc factored.",
                    "label": 0
                },
                {
                    "sent": "And so it models things like interactions between grandparents and siblings.",
                    "label": 0
                },
                {
                    "sent": "It models valacia non projective arcs.",
                    "label": 0
                },
                {
                    "sent": "And it does everything with a polynomial number of variables and constraints, so this is done using flow model.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to discuss the model.",
                    "label": 0
                },
                {
                    "sent": "There is another paper that's going to appear on ACL.",
                    "label": 0
                },
                {
                    "sent": "With this formulation is discussed.",
                    "label": 0
                },
                {
                    "sent": "OK, so our first experiment is just training with you.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relaxed last augmented inference.",
                    "label": 0
                },
                {
                    "sent": "So this is the most proximate case we are.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Setting our parameter 8 to one so we are penalizing computational cost the most.",
                    "label": 0
                },
                {
                    "sent": "And we tried two different decoders at Test time and exactly coder, which means solving the LP and then approximate decoder that solves the relaxation and round the solutions in polynomial time by using the maximum spanning tree.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we compare this against some strong baselines.",
                    "label": 0
                },
                {
                    "sent": "The approximate 2nd order parser of McDonald in prayer and also stacked parser that uses 2 parsers, and it uses the outputs of the first parser as features to the second parser.",
                    "label": 0
                },
                {
                    "sent": "So it's not a very nice architecture, but it pays off alot, so let's see what happens.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is what we get with the arc factored model.",
                    "label": 0
                },
                {
                    "sent": "Overall all languages.",
                    "label": 0
                },
                {
                    "sent": "And this is what we got the full model, so.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is roughly 2% improvements across all.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Images and we can see that with approximate decoding it doesn't harm a lot, so this is not affecting accuracy by a lot.",
                    "label": 0
                },
                {
                    "sent": "And the baseline.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are here so this is the 2nd order parser of McDonald's.",
                    "label": 0
                },
                {
                    "sent": "We are doing a lot better for some language and not so much better in others.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is what we get to the stack parser.",
                    "label": 0
                },
                {
                    "sent": "So across all languages it performs comparably to our method.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm just finished finishing so this is the last experiment to measure.",
                    "label": 0
                },
                {
                    "sent": "If ETA is really penalizing computational costs so.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are plotting here on the X axis the value of data set at training time and on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "The relaxation gap observed at Test time and it.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Happens that if you're doing more exact training, we get a larger relaxation gap at Test time.",
                    "label": 0
                },
                {
                    "sent": "So it's like 80 is if when you increase 8 at the model is learning to avoid fractional solutions.",
                    "label": 1
                },
                {
                    "sent": "And this actually correlates with runtime.",
                    "label": 0
                },
                {
                    "sent": "So we can see that runtime also increases when there are.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gap increases as we expected, but the proximity color is significantly faster, so there is a green line over here.",
                    "label": 0
                },
                {
                    "sent": "Maybe you don't see it.",
                    "label": 0
                },
                {
                    "sent": "This is runtime that we get with approximately coder.",
                    "label": 0
                },
                {
                    "sent": "It's very fast.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the full model, so this set of experiments is not and in full model, but in the full model the order of magnitude that we have is less than one second on average per sentence across.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Language.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is all that we have so.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we studied the impact of helping relaxed inference in a Max margin framework we got.",
                    "label": 0
                },
                {
                    "sent": "We stablished sufficient conditions that guarantee algorithmics, probability we prove.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And interpretation in terms of penalizing computational costs.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We show that this is effective for dependency parsing using an arc factored model.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the future we are going to look at other characterizations to guarantee tighter bounds.",
                    "label": 0
                },
                {
                    "sent": "And we want to investigate all this is relative with variable reception we did.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Play a lot to be deregulation here.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is all that that thanks.",
                    "label": 0
                },
                {
                    "sent": "Thanks for we do have time for a few questions.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The sufficient condition for algorithmic separability.",
                    "label": 1
                },
                {
                    "sent": "We have you ever looked at what L is for some instantiations.",
                    "label": 0
                },
                {
                    "sent": "To me, like I will sometimes be of the order of magnitude N which case with your theorem says that determine huge margin in order to have.",
                    "label": 0
                },
                {
                    "sent": "That is a very good question, and indeed the theorem is very weak in that sense.",
                    "label": 0
                },
                {
                    "sent": "So this means that.",
                    "label": 0
                },
                {
                    "sent": "So there is a Max, the maximum value of margin that we could achieve.",
                    "label": 0
                },
                {
                    "sent": "We need to get at least half of that.",
                    "label": 0
                },
                {
                    "sent": "So this condition is very is very very weak.",
                    "label": 0
                },
                {
                    "sent": "We need to have a great margin in the separable case to guarantee algorithmics probability even if the approximation is good.",
                    "label": 0
                },
                {
                    "sent": "But I mean there is a factor of L prime there over there which measures outside is the polytope, but we did not try for a particular problem.",
                    "label": 0
                },
                {
                    "sent": "So not in this case in parsing it, so it's hard to do that.",
                    "label": 0
                },
                {
                    "sent": "At least three in.",
                    "label": 0
                },
                {
                    "sent": "And like we are in a very specific case.",
                    "label": 0
                },
                {
                    "sent": "Right any other time?",
                    "label": 0
                },
                {
                    "sent": "For one more question, quick question.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's think I'll speak again.",
                    "label": 0
                }
            ]
        }
    }
}