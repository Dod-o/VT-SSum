{
    "id": "6zcujxlyhoqkzsbdlw5kpg6j5nqou6um",
    "title": "Deep-er Kernels",
    "info": {
        "author": [
            "John Shawe-Taylor, Centre for Computational Statistics and Machine Learning, University College London"
        ],
        "published": "Nov. 7, 2013",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Decision Support",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/lsoldm2013_shawe_taylor_kernels/",
    "segmentation": [
        [
            "OK, so."
        ],
        [
            "This is sort of if you like, the subtitle might be, you know.",
            "Colonel Empire Strikes Back or something like that.",
            "So the deep learning is kind of re emerged as this really important research topic with real commercial value.",
            "So you know there's been a really interesting.",
            "Resurgence in interest in it and and effective use of it in.",
            "In real systems, deep belief networks have been sort of leading this charge.",
            "And there's been this sort of implication and sometimes overt reference to kernels as being shallow, you know.",
            "When that said by deep people, then you feel kind of hurt, you know so.",
            "Name of this talk is to sort of discuss what we mean by deep learning and describe a number of ways in which kernel learning has been made a bit deeper.",
            "I wouldn't make any deep claims, but a little bit deeper.",
            "And so that's that's what I would like to sort of convey.",
            "A little bit of some of the things that have been going on, and I think you know, there's actually quite a nice set of activities in this area, and perhaps you know they could be developed further.",
            "So I think there's also open ended questions at the end that are."
        ],
        [
            "So why is my kernel shallow?",
            "So the idea is that kernels learn this nonlinear function in the input space.",
            "So in a sense they should appear to be as flexible as a deep learning system.",
            "I mean, they have the flexibility potentially.",
            "But the reason they are viewed as shallow is there actually implementing a linear function in a kernel defined feature space.",
            "So the problem is, if I know how to put this pointer, there's this sort of fixed mapping here, and then there's a learned mapping here with this weight vector being the learned part.",
            "Now you don't learn that W explicitly, but still, that's what's actually going on.",
            "So effectively you're learning one layer, so it's a single their learning and therefore shallow.",
            "This is contrasted to deep learning where parameters are spread across several layers, and typically with nonlinear transfer functions.",
            "So learning of the deep layers I should say is often unsupervised with the final classifier trained on the earlier layers, fixed possibly using it outputs, and from the earlier layers as well.",
            "So we're effectively in deep learning pre learning representation.",
            "So you know at some point this starts to look a bit similar to learning the kernel.",
            "You know in some sense you're learning a representation.",
            "In order to then do your actual learning.",
            "So of course I don't want to claim that all deep learning does that, But that's certainly a flavor, and probably in some people's view of becoming more popular."
        ],
        [
            "So what happens in practice with kernel learning?",
            "I think that's worth sort of thinking about for a moment.",
            "So we typically do perform some learning of the kernel.",
            "OK, so you might have some hyperparameters that you fix with some heuristic, perhaps so just the width of the Gaussian kernel.",
            "You might use cross validation to adapt the high parameter to optimize performance, which would be for classification or regression.",
            "So in some sense you're already doing some aspect of adapting the representation for the task that you are actually trying to solve.",
            "So, however.",
            "Sense you might say this is good.",
            "We're trying to do some sort of deeper learning, but in some sense it rather undermines the whole sort of philosophy of kernel methods which were originally based on this idea, or to some extent, anyway, one of their strength is that they're principled in there based on some sort of statistical generalization bounds.",
            "That inform the structure of the algorithm and you know by optimizing the algorithm, you're optimizing performance on a test set, and so on.",
            "However, none of those generalization bands actually run to adapting the.",
            "The kernel or they haven't done until very recently.",
            "I mean, the standard one certainly do not.",
            "I mean, I'll talk a little bit about some of the developments in this area later on, but so your standard generalization bounds assumes that you have a fixed kernel fix feature space, and then you do some learning in that space and so therefore you know this sort of adaptation stuff undermines to some extent what you you based, the strength of your whole approach on, and if you start adapting your feature space using test data, then that will even in Val.",
            "They test set button so you really will be, you know, letting yourself run a little bit loose in terms of what you set out to do.",
            "So often more sophisticated representations are actually used in.",
            "I'm going I'm talking in practice that encode deep prior knowledge, but I learn by trial and error.",
            "So for instance, you know if you're thinking about.",
            "Computer vision, you know, the standard techniques use sort of either Fisher kernels or histograms of Patch cluster presence in object detection, and these have been developed over a number of iterations of various researchers.",
            "Sort of trying to understand the way to represent images in order to get good performance, and you know they've iterated so there is deep learning going on, but it's in the minds of the.",
            "Researchers rather than in the machine.",
            "I think it's fair to say."
        ],
        [
            "OK, So what I want to do is to present a number of promising directions that take some of these boxes so there are sort of three boxes here.",
            "One is learning a kernel representation, possibly tuned to the main learning task, provide an analysis of the resulting system that supports design and bounds its performance so that you know if you like overcomes that weakness.",
            "I mentioned on the previous slide and then provide empirical evidence that supports the approach on real world data.",
            "So this is sort of like doing some sort of deep learning or deeper learning.",
            "So there's going to be a few different things I'm going to mention.",
            "I hope you know they may appear disjointed, but I hope that sort of convincing and coherent story will emerge and that is deeper.",
            "Learning of kernels is alive and kicking OK, and I think and in."
        ],
        [
            "Testing area.",
            "OK, so here's the first thing I wanted to mention.",
            "This is actually joint work with Tom, who is in the audience.",
            "Zach and a few others that I'll give the references in a minute so matching pursuit is a method that greedily chooses training examples that determine directions in feature space that are well suited to some task.",
            "Some learning task.",
            "So now you're actually learning your representation by developing it based on your training data, so it's a data dependent generated representation.",
            "Mean it leverages.",
            "Obviously your prior knowledge, maybe in terms of the kernel.",
            "However, we were able to do an analysis that combine sparse reconstruction, so the fact that you're using this.",
            "Reconstructing this representation by specifying a subset of the training data allows you to wheel on these sort of sparse representation bounds so that we were able to generate then generalization error bands that I believe are the first to give a bound on a learned representation.",
            "So it allows different criteria for selection to be implemented, and in this sort of framework that we set up.",
            "So you can do a sparse PCA, get a bound for that sparse generation for classification, bound for that as sparse regression, and also for Canonical correlation analysis.",
            "I'm not going to dwell anymore on this, but just to give you a sort of flavor that the bounds seemed to be."
        ],
        [
            "Doing something useful here is a kind of.",
            "A diagram that shows sort of test performance of one of these CCA directions as compared to the bound, and it does seem the bound is picking up some sort of structure.",
            "Loosely, that is reflected in the actual performance and certainly you know if you chose the optimal dimension here based on the bound, you wouldn't do too badly in terms of the actual test error.",
            "So I think you know this is just showing that there are methods that can.",
            "Actually keep the bound inspiration behind kernel methods and still actually learn some feature representation and some."
        ],
        [
            "Way of representing the data, perhaps a more obvious way of using prior information that you might learn in order to generate a representation of kernels from probabilistic models, and they have a long history, so if we consider learning as a representation of preprocessing stage so this is where we separate out, we don't use the labels for learning the representation, we just use the data for learning the representation.",
            "And there are a couple of methods that are quite well studied.",
            "One is this averaging over a model class.",
            "You might have a set of models and you essentially generate one feature for each model, which is the probability of the data in that model, and then you have you know the two data points are compared by looking at their probabilities in the models, and then you wait this over the different models.",
            "It's known as the marginalization kernel, but perhaps the more common ones are study.",
            "Is this Fisher kernel.",
            "Which looks at where the model is determined by a real parameter vector.",
            "So you have a sort of a continuously parameterized set of models, and just in this example."
        ],
        [
            "Considered a Gaussian distribution, this is just to sort of give you an idea of how it works and maybe some intuition.",
            "You have a model of data based just on two parameters, the mean and the variance standard deviation of the Gaussian.",
            "You compute the derivatives of this probability log probability vector.",
            "Sorry of the log probability of the data point with respect to those two parameters.",
            "So here's the log probability you workout the derivative of this with respect to the two parameters.",
            "And that gives you the."
        ],
        [
            "So called Fisher score Vector and that is the representation that you use for your input point.",
            "So in this case it's a 2 dimensional vector 'cause there are two parameters.",
            "Notice that you take the derivative at some particular values of those parameters.",
            "In this case I've denoted the Mu Norton Sigma nought.",
            "So these are the parameters that you can adjust to change your representation.",
            "And if you take some standard values like the mean being zero and the standard deviation being one, you get the following.",
            "Representation, So what I've done here.",
            "Note that by taking these two values, New Zero and Sigma one, the first dimension just is the input point.",
            "Input Points 1 dimensional.",
            "So what we're effectively doing is augmenting the input dimension with an extra dimension in this case."
        ],
        [
            "And this gives us this structure here, and that clearly gives us some extra representational power.",
            "For instance, we could, with a linear function, learn intervals as opposed to just learning raise.",
            "And so it does give us an extra dimension.",
            "But the important thing, I think from the intuition here is to try and understand how your use of.",
            "The setting these learn."
        ],
        [
            "In these parameters, mu nought an signal from the data might affect the performance of your algorithms."
        ],
        [
            "So for instance, if the data was all centered around 1:00, this particular representation would not be so useful in terms of separating out parts of the data set, you know maybe a central part from the outside part.",
            "You could do it, but it would be quite difficult, but if you move the mean to the mean of the data and the standard deviation, then this actually will be very easy to use to separate out.",
            "So the learning of those parameters actually does improve the representation, potentially for doing useful things.",
            "With the data.",
            "And just to mention that."
        ],
        [
            "Fisher kernels are are used quite widely in many application.",
            "I'll talk about that in a minute, but just as another example of Fisher kernels, we can also view string kernels as Fisher kernels.",
            "And the probabilistic model that we have to consider is the Markov model for generating text conditioned on the previous N characters.",
            "So if you take the uniform distribution over the next character to appear after your current observation, you get the class of string kernels as the Fisher kernel of that particular model.",
            "However, this now gives you the option of saying OK. That's if you take this vanilla version of the uniform distribution.",
            "Maybe we can tune that distribution to actually fit the particular sample that we're looking at and learn the probable transition dynamics of this Markov model and use those in our Fisher kernel representation.",
            "Connection between how you should be setting the parameters and those that I guess, is commended for movies.",
            "Where you went in for the Colonel later, I think.",
            "I think it's.",
            "I.",
            "Sort of.",
            "Are directly related to the input space right or distribution of the data exactly that?",
            "The thing is, they're not affected by the labels, so in a sense it is difficult to say what's a good choice.",
            "I would agree with you.",
            "I think the maximum likelihood choice if you like, is most likely to be helpful, but it would depend on the label structure and the way the labels work.",
            "I'll present some results that show that you can improve performance, but it doesn't actually make that much difference in this particular case.",
            "In the experiments we did actually we extended then this idea to using a probabilistic finite state automaton rather than just a.",
            "In a feedforward so we could actually vary the.",
            "If you like the length of.",
            "Markov model, depending on the frequency of the characters.",
            "So if we have particular sequences of characters that are very infrequent, we don't need to take a precondition of N characters.",
            "You can take smaller numbers.",
            "Similar work has been done in different ways, but.",
            "But you get sorry.",
            "I'm not actually going to present the results, but we got competitive results with TF.",
            "IDF bag of words on Reuters and some improvements in average precision using this particular method.",
            "So there wasn't strong evidence this was enormously helpful, but it just shows that you can match state of the art performance and essentially you get if you like.",
            "You get round this hack of the TF IDF, which maybe this is a more principled way of doing it, but you could say.",
            "Anne."
        ],
        [
            "So my next sort of example of deeper learning is multiple kernel learning and this is where I think it starts to get a little bit more interesting in that you're actually using the label data to learn the representation, so that up until now we've had this phase of learning the representation, which is mainly been independent of the actual task, may be the first one.",
            "To some extent we were using the task, but the typical way of using a Fisher kernels are, so use some sort of maximum likelihood.",
            "Which may not be a good choice in terms of the labeling data, so this is sort of potentially so.",
            "How does multiple kernel learning learning work while you have this set of kernels and you make a linear combination of them in such a way that the coefficients are positive, that's to ensure that this is a positive definite kernel and that they sum to one.",
            "So it's like a one norm regularization over the choice of different kernels that you provide.",
            "And you optimize the usual.",
            "SVM criterion the margin or the you know the norm of the weight vector minimize while allowing said T. To adapt, and perhaps at first sight, it might not appear so, but this is still a convex problem, so you actually have efficient algorithms.",
            "You actually can arrive at this bound for multiple kernel.",
            "The performance of your learned classifier based on the N different kernels that I've given here.",
            "Which is just an application of Rademacher complexity, but it uses a very interesting property of Rademacher complexity, which is this convex Hull bound.",
            "So when you move from the convex Hull of a set of functions, sorry from a set of functions to their convex Hull, you pay no price in terms of Rademacher complexity.",
            "There's a sort of freebie there, so you could take you know 20 functions.",
            "They have a certain Rademacher complexity, take their convex, how they have the same Rademacher complexity so.",
            "That's implicit here because I've just taken the union of these function classes corresponding to each kernel.",
            "This is just the linear clients from class for kernel T, I've just taken them the actual bound that I would need is the convex Hull of these, but we don't need to compute that Rademacher complexity because it comes for free.",
            "So we just need to compute this Rademacher complexity."
        ],
        [
            "And this is what?",
            "By the way, Rademacher complexity is is just an average of your correlation with random noise.",
            "I'm going to skip over this, but."
        ],
        [
            "Is a bit of a bounding proof here, which shows you that actually the.",
            "Rademacher complexity of that union just is the Max of the individual regular complexities with a penalty of a log of the number of kernels that you actually use.",
            "So the.",
            "Actual number of kernels comes in under the log with.",
            "These could if you had sort of Gaussian kernels for each.",
            "These would all just be the same, so this would just be square root of M here for this quantity here.",
            "So you pay no penalty here effectively and you just take a log penalty in terms of the number of kernels.",
            "Which is extraordinary, but because it implies that you can really go wild in terms of throwing in kernels, and I think people have tended to be a bit conservative when they run multiple kernel learning, they think of, you know, maybe I'll try 10 kernels mean.",
            "Maybe 20, you know?",
            "And this is saying you shouldn't."
        ],
        [
            "Should really go for it.",
            "And Interestingly, these this work by Manic who should be giving this talk so I'm very happy to be able to cite his work here was actually did do that with very large millions of kernels and they got really significant improvements on the standard data set for vision getting being winners of this challenge in 17 out of 20 and half of them, they gotta increase in average, precision 25%.",
            "So it's really.",
            "You know significant hiking performance, so I think you know the lesson would be for me here.",
            "You know, we need to be a bit more courageous in throwing in kernels.",
            "I wouldn't want that one extravagant kernel that has.",
            "You know very high expressivity which would cost you in the bond well."
        ],
        [
            "With that single out that that would be true, but the Max power you need to form a lot of different.",
            "But this actual Max.",
            "If you using normalized kernels.",
            "You're OK because this actually for a normalized kernel is just root M, so this will be two over gamma router."
        ],
        [
            "So I think I'm going to have to skip.",
            "Probably this what this was going to be showing a linear programming boosting algorithm that can be adapted to multiple kernel learning.",
            "So just this is what you get if you replace the two norm regularization SVM with the one norm R and you'd end up with a linear program and you can solve it through.",
            "Jewel approach using an iterative method which is essentially a column generation method and it corresponds to boosting and you're boosting criterion is exactly the same for standard boosting, but at each iteration rather than just adding in an extra week learner with and you waiting, you have to solve a linear program and it gives you a nice stopping criti."
        ],
        [
            "But Interestingly, can apply exactly the same method for multiple kernel learning, because you can solve this week learning criterion over the whole function class and you actually end up with a solution that can be represented Jewel Way.",
            "So there's just a kind of neat connection to boosting at this."
        ],
        [
            "Points you sort of boosting your kernels effectively, and this is the dual representation of the weak learner.",
            "Or you know the function that you need to add into your.",
            "So you gotta sort of an implementation.",
            "More generally, this vector U, which was the coming out of the."
        ],
        [
            "The solution of the linear program at each stage you get this UI, which is a distribution over the examples, is like your boosting distribution can be used as a sick."
        ],
        [
            "And you can actually use that to drive the learning of your representation.",
            "So we did this with Fisher kernels and applied that knew signal to adapt the parameters of the Fisher kernel.",
            "So rather than use maximum likelihood, use this as your criterion for learning the actual representation.",
            "We used it in and HMM, Fisher kernel for modeling time series with some encouraging results.",
            "I won't say they're fantastic, but they're kind of OK results in foreign exchange.",
            "Application."
        ],
        [
            "So final topic 2 minutes.",
            "OK, so this is about nonlinear feature selection.",
            "There's an interesting connection between the target alignment, which is this expected value of this correlation between the sort of targets and the.",
            "The kernel matrix and the degree of correlation that you can get between the target sorry and and some choice of weight vector here.",
            "So the learned weight vector can correlate with the target to a degree that is given by this.",
            "So there's this suggestion from this that you can assess the contribution of a feature by looking at the way in which adding that feature in.",
            "2 random sets of features.",
            "So we're taking a random subset of features that includes that variable I and those that similar size that don't include that variable.",
            "I an seeing what the correlation this alignment is, and if there are any hike in this alignment."
        ],
        [
            "And here's an example where we've artificial example the X or function on the first 2 features take a Gaussian kernel, and you do this.",
            "These plots of the contributions.",
            "In other words, this value CI for the different features and what we've done is just.",
            "Get rid of the bottom, say 25% of the features at each iteration, so this is the start.",
            "Get rid of the bottom 25%.",
            "Next one bed and get rid of the bottom 25% and so on.",
            "And you see how these two features survive this call.",
            "And as the number of features reduces their.",
            "Influence is seen to grow, so these are the two features.",
            "These are all mistake features."
        ],
        [
            "That's sort of the inspiration, and there's some nice properties of this thing that I won't go into.",
            "But basically it motivates this algorithm of culling the features based on the rankings of their contributions."
        ],
        [
            "And this is to show that it actually performed similarly to more expensive methods in terms of identifying the features."
        ],
        [
            "And also on some real world ohmic and microarray data.",
            "But I won't."
        ],
        [
            "Spend time on that.",
            "This is the punch line which we applied this idea to a deep learning challenge.",
            "Furnal swipe back and after an initial filtering step we applied this culling as described and then an LP boost MCL to the corresponding features and we came third in the ranking of this.",
            "Sort of deep learning challenge, so we're very pleased anyway."
        ],
        [
            "So summary is basically learning deep representations is important for real data.",
            "Many kernel practitioners are using deep learning, but typically in a sort of more ad hoc manner.",
            "And what we hope I presented as some.",
            "You know, the value of having slightly more principled methods, and I think there's already a range of sort of theoretical results that are quite beginning to put deeper kernels on a firmer footing.",
            "I hope, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is sort of if you like, the subtitle might be, you know.",
                    "label": 0
                },
                {
                    "sent": "Colonel Empire Strikes Back or something like that.",
                    "label": 0
                },
                {
                    "sent": "So the deep learning is kind of re emerged as this really important research topic with real commercial value.",
                    "label": 0
                },
                {
                    "sent": "So you know there's been a really interesting.",
                    "label": 0
                },
                {
                    "sent": "Resurgence in interest in it and and effective use of it in.",
                    "label": 0
                },
                {
                    "sent": "In real systems, deep belief networks have been sort of leading this charge.",
                    "label": 0
                },
                {
                    "sent": "And there's been this sort of implication and sometimes overt reference to kernels as being shallow, you know.",
                    "label": 0
                },
                {
                    "sent": "When that said by deep people, then you feel kind of hurt, you know so.",
                    "label": 0
                },
                {
                    "sent": "Name of this talk is to sort of discuss what we mean by deep learning and describe a number of ways in which kernel learning has been made a bit deeper.",
                    "label": 1
                },
                {
                    "sent": "I wouldn't make any deep claims, but a little bit deeper.",
                    "label": 0
                },
                {
                    "sent": "And so that's that's what I would like to sort of convey.",
                    "label": 0
                },
                {
                    "sent": "A little bit of some of the things that have been going on, and I think you know, there's actually quite a nice set of activities in this area, and perhaps you know they could be developed further.",
                    "label": 0
                },
                {
                    "sent": "So I think there's also open ended questions at the end that are.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why is my kernel shallow?",
                    "label": 0
                },
                {
                    "sent": "So the idea is that kernels learn this nonlinear function in the input space.",
                    "label": 1
                },
                {
                    "sent": "So in a sense they should appear to be as flexible as a deep learning system.",
                    "label": 1
                },
                {
                    "sent": "I mean, they have the flexibility potentially.",
                    "label": 0
                },
                {
                    "sent": "But the reason they are viewed as shallow is there actually implementing a linear function in a kernel defined feature space.",
                    "label": 0
                },
                {
                    "sent": "So the problem is, if I know how to put this pointer, there's this sort of fixed mapping here, and then there's a learned mapping here with this weight vector being the learned part.",
                    "label": 0
                },
                {
                    "sent": "Now you don't learn that W explicitly, but still, that's what's actually going on.",
                    "label": 0
                },
                {
                    "sent": "So effectively you're learning one layer, so it's a single their learning and therefore shallow.",
                    "label": 1
                },
                {
                    "sent": "This is contrasted to deep learning where parameters are spread across several layers, and typically with nonlinear transfer functions.",
                    "label": 1
                },
                {
                    "sent": "So learning of the deep layers I should say is often unsupervised with the final classifier trained on the earlier layers, fixed possibly using it outputs, and from the earlier layers as well.",
                    "label": 0
                },
                {
                    "sent": "So we're effectively in deep learning pre learning representation.",
                    "label": 0
                },
                {
                    "sent": "So you know at some point this starts to look a bit similar to learning the kernel.",
                    "label": 0
                },
                {
                    "sent": "You know in some sense you're learning a representation.",
                    "label": 0
                },
                {
                    "sent": "In order to then do your actual learning.",
                    "label": 0
                },
                {
                    "sent": "So of course I don't want to claim that all deep learning does that, But that's certainly a flavor, and probably in some people's view of becoming more popular.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what happens in practice with kernel learning?",
                    "label": 1
                },
                {
                    "sent": "I think that's worth sort of thinking about for a moment.",
                    "label": 1
                },
                {
                    "sent": "So we typically do perform some learning of the kernel.",
                    "label": 1
                },
                {
                    "sent": "OK, so you might have some hyperparameters that you fix with some heuristic, perhaps so just the width of the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "You might use cross validation to adapt the high parameter to optimize performance, which would be for classification or regression.",
                    "label": 0
                },
                {
                    "sent": "So in some sense you're already doing some aspect of adapting the representation for the task that you are actually trying to solve.",
                    "label": 0
                },
                {
                    "sent": "So, however.",
                    "label": 0
                },
                {
                    "sent": "Sense you might say this is good.",
                    "label": 0
                },
                {
                    "sent": "We're trying to do some sort of deeper learning, but in some sense it rather undermines the whole sort of philosophy of kernel methods which were originally based on this idea, or to some extent, anyway, one of their strength is that they're principled in there based on some sort of statistical generalization bounds.",
                    "label": 0
                },
                {
                    "sent": "That inform the structure of the algorithm and you know by optimizing the algorithm, you're optimizing performance on a test set, and so on.",
                    "label": 0
                },
                {
                    "sent": "However, none of those generalization bands actually run to adapting the.",
                    "label": 0
                },
                {
                    "sent": "The kernel or they haven't done until very recently.",
                    "label": 0
                },
                {
                    "sent": "I mean, the standard one certainly do not.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'll talk a little bit about some of the developments in this area later on, but so your standard generalization bounds assumes that you have a fixed kernel fix feature space, and then you do some learning in that space and so therefore you know this sort of adaptation stuff undermines to some extent what you you based, the strength of your whole approach on, and if you start adapting your feature space using test data, then that will even in Val.",
                    "label": 0
                },
                {
                    "sent": "They test set button so you really will be, you know, letting yourself run a little bit loose in terms of what you set out to do.",
                    "label": 1
                },
                {
                    "sent": "So often more sophisticated representations are actually used in.",
                    "label": 1
                },
                {
                    "sent": "I'm going I'm talking in practice that encode deep prior knowledge, but I learn by trial and error.",
                    "label": 0
                },
                {
                    "sent": "So for instance, you know if you're thinking about.",
                    "label": 0
                },
                {
                    "sent": "Computer vision, you know, the standard techniques use sort of either Fisher kernels or histograms of Patch cluster presence in object detection, and these have been developed over a number of iterations of various researchers.",
                    "label": 0
                },
                {
                    "sent": "Sort of trying to understand the way to represent images in order to get good performance, and you know they've iterated so there is deep learning going on, but it's in the minds of the.",
                    "label": 0
                },
                {
                    "sent": "Researchers rather than in the machine.",
                    "label": 0
                },
                {
                    "sent": "I think it's fair to say.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what I want to do is to present a number of promising directions that take some of these boxes so there are sort of three boxes here.",
                    "label": 0
                },
                {
                    "sent": "One is learning a kernel representation, possibly tuned to the main learning task, provide an analysis of the resulting system that supports design and bounds its performance so that you know if you like overcomes that weakness.",
                    "label": 1
                },
                {
                    "sent": "I mentioned on the previous slide and then provide empirical evidence that supports the approach on real world data.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of like doing some sort of deep learning or deeper learning.",
                    "label": 1
                },
                {
                    "sent": "So there's going to be a few different things I'm going to mention.",
                    "label": 0
                },
                {
                    "sent": "I hope you know they may appear disjointed, but I hope that sort of convincing and coherent story will emerge and that is deeper.",
                    "label": 0
                },
                {
                    "sent": "Learning of kernels is alive and kicking OK, and I think and in.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Testing area.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the first thing I wanted to mention.",
                    "label": 0
                },
                {
                    "sent": "This is actually joint work with Tom, who is in the audience.",
                    "label": 0
                },
                {
                    "sent": "Zach and a few others that I'll give the references in a minute so matching pursuit is a method that greedily chooses training examples that determine directions in feature space that are well suited to some task.",
                    "label": 1
                },
                {
                    "sent": "Some learning task.",
                    "label": 0
                },
                {
                    "sent": "So now you're actually learning your representation by developing it based on your training data, so it's a data dependent generated representation.",
                    "label": 0
                },
                {
                    "sent": "Mean it leverages.",
                    "label": 0
                },
                {
                    "sent": "Obviously your prior knowledge, maybe in terms of the kernel.",
                    "label": 0
                },
                {
                    "sent": "However, we were able to do an analysis that combine sparse reconstruction, so the fact that you're using this.",
                    "label": 0
                },
                {
                    "sent": "Reconstructing this representation by specifying a subset of the training data allows you to wheel on these sort of sparse representation bounds so that we were able to generate then generalization error bands that I believe are the first to give a bound on a learned representation.",
                    "label": 1
                },
                {
                    "sent": "So it allows different criteria for selection to be implemented, and in this sort of framework that we set up.",
                    "label": 0
                },
                {
                    "sent": "So you can do a sparse PCA, get a bound for that sparse generation for classification, bound for that as sparse regression, and also for Canonical correlation analysis.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to dwell anymore on this, but just to give you a sort of flavor that the bounds seemed to be.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing something useful here is a kind of.",
                    "label": 0
                },
                {
                    "sent": "A diagram that shows sort of test performance of one of these CCA directions as compared to the bound, and it does seem the bound is picking up some sort of structure.",
                    "label": 0
                },
                {
                    "sent": "Loosely, that is reflected in the actual performance and certainly you know if you chose the optimal dimension here based on the bound, you wouldn't do too badly in terms of the actual test error.",
                    "label": 0
                },
                {
                    "sent": "So I think you know this is just showing that there are methods that can.",
                    "label": 0
                },
                {
                    "sent": "Actually keep the bound inspiration behind kernel methods and still actually learn some feature representation and some.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Way of representing the data, perhaps a more obvious way of using prior information that you might learn in order to generate a representation of kernels from probabilistic models, and they have a long history, so if we consider learning as a representation of preprocessing stage so this is where we separate out, we don't use the labels for learning the representation, we just use the data for learning the representation.",
                    "label": 0
                },
                {
                    "sent": "And there are a couple of methods that are quite well studied.",
                    "label": 0
                },
                {
                    "sent": "One is this averaging over a model class.",
                    "label": 1
                },
                {
                    "sent": "You might have a set of models and you essentially generate one feature for each model, which is the probability of the data in that model, and then you have you know the two data points are compared by looking at their probabilities in the models, and then you wait this over the different models.",
                    "label": 1
                },
                {
                    "sent": "It's known as the marginalization kernel, but perhaps the more common ones are study.",
                    "label": 0
                },
                {
                    "sent": "Is this Fisher kernel.",
                    "label": 0
                },
                {
                    "sent": "Which looks at where the model is determined by a real parameter vector.",
                    "label": 1
                },
                {
                    "sent": "So you have a sort of a continuously parameterized set of models, and just in this example.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Considered a Gaussian distribution, this is just to sort of give you an idea of how it works and maybe some intuition.",
                    "label": 1
                },
                {
                    "sent": "You have a model of data based just on two parameters, the mean and the variance standard deviation of the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "You compute the derivatives of this probability log probability vector.",
                    "label": 0
                },
                {
                    "sent": "Sorry of the log probability of the data point with respect to those two parameters.",
                    "label": 1
                },
                {
                    "sent": "So here's the log probability you workout the derivative of this with respect to the two parameters.",
                    "label": 1
                },
                {
                    "sent": "And that gives you the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So called Fisher score Vector and that is the representation that you use for your input point.",
                    "label": 0
                },
                {
                    "sent": "So in this case it's a 2 dimensional vector 'cause there are two parameters.",
                    "label": 0
                },
                {
                    "sent": "Notice that you take the derivative at some particular values of those parameters.",
                    "label": 0
                },
                {
                    "sent": "In this case I've denoted the Mu Norton Sigma nought.",
                    "label": 0
                },
                {
                    "sent": "So these are the parameters that you can adjust to change your representation.",
                    "label": 0
                },
                {
                    "sent": "And if you take some standard values like the mean being zero and the standard deviation being one, you get the following.",
                    "label": 0
                },
                {
                    "sent": "Representation, So what I've done here.",
                    "label": 0
                },
                {
                    "sent": "Note that by taking these two values, New Zero and Sigma one, the first dimension just is the input point.",
                    "label": 0
                },
                {
                    "sent": "Input Points 1 dimensional.",
                    "label": 0
                },
                {
                    "sent": "So what we're effectively doing is augmenting the input dimension with an extra dimension in this case.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this gives us this structure here, and that clearly gives us some extra representational power.",
                    "label": 0
                },
                {
                    "sent": "For instance, we could, with a linear function, learn intervals as opposed to just learning raise.",
                    "label": 0
                },
                {
                    "sent": "And so it does give us an extra dimension.",
                    "label": 0
                },
                {
                    "sent": "But the important thing, I think from the intuition here is to try and understand how your use of.",
                    "label": 0
                },
                {
                    "sent": "The setting these learn.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In these parameters, mu nought an signal from the data might affect the performance of your algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for instance, if the data was all centered around 1:00, this particular representation would not be so useful in terms of separating out parts of the data set, you know maybe a central part from the outside part.",
                    "label": 0
                },
                {
                    "sent": "You could do it, but it would be quite difficult, but if you move the mean to the mean of the data and the standard deviation, then this actually will be very easy to use to separate out.",
                    "label": 0
                },
                {
                    "sent": "So the learning of those parameters actually does improve the representation, potentially for doing useful things.",
                    "label": 0
                },
                {
                    "sent": "With the data.",
                    "label": 0
                },
                {
                    "sent": "And just to mention that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fisher kernels are are used quite widely in many application.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about that in a minute, but just as another example of Fisher kernels, we can also view string kernels as Fisher kernels.",
                    "label": 1
                },
                {
                    "sent": "And the probabilistic model that we have to consider is the Markov model for generating text conditioned on the previous N characters.",
                    "label": 0
                },
                {
                    "sent": "So if you take the uniform distribution over the next character to appear after your current observation, you get the class of string kernels as the Fisher kernel of that particular model.",
                    "label": 0
                },
                {
                    "sent": "However, this now gives you the option of saying OK. That's if you take this vanilla version of the uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can tune that distribution to actually fit the particular sample that we're looking at and learn the probable transition dynamics of this Markov model and use those in our Fisher kernel representation.",
                    "label": 0
                },
                {
                    "sent": "Connection between how you should be setting the parameters and those that I guess, is commended for movies.",
                    "label": 0
                },
                {
                    "sent": "Where you went in for the Colonel later, I think.",
                    "label": 0
                },
                {
                    "sent": "I think it's.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "Are directly related to the input space right or distribution of the data exactly that?",
                    "label": 0
                },
                {
                    "sent": "The thing is, they're not affected by the labels, so in a sense it is difficult to say what's a good choice.",
                    "label": 0
                },
                {
                    "sent": "I would agree with you.",
                    "label": 0
                },
                {
                    "sent": "I think the maximum likelihood choice if you like, is most likely to be helpful, but it would depend on the label structure and the way the labels work.",
                    "label": 0
                },
                {
                    "sent": "I'll present some results that show that you can improve performance, but it doesn't actually make that much difference in this particular case.",
                    "label": 0
                },
                {
                    "sent": "In the experiments we did actually we extended then this idea to using a probabilistic finite state automaton rather than just a.",
                    "label": 1
                },
                {
                    "sent": "In a feedforward so we could actually vary the.",
                    "label": 0
                },
                {
                    "sent": "If you like the length of.",
                    "label": 0
                },
                {
                    "sent": "Markov model, depending on the frequency of the characters.",
                    "label": 0
                },
                {
                    "sent": "So if we have particular sequences of characters that are very infrequent, we don't need to take a precondition of N characters.",
                    "label": 0
                },
                {
                    "sent": "You can take smaller numbers.",
                    "label": 0
                },
                {
                    "sent": "Similar work has been done in different ways, but.",
                    "label": 0
                },
                {
                    "sent": "But you get sorry.",
                    "label": 1
                },
                {
                    "sent": "I'm not actually going to present the results, but we got competitive results with TF.",
                    "label": 0
                },
                {
                    "sent": "IDF bag of words on Reuters and some improvements in average precision using this particular method.",
                    "label": 0
                },
                {
                    "sent": "So there wasn't strong evidence this was enormously helpful, but it just shows that you can match state of the art performance and essentially you get if you like.",
                    "label": 0
                },
                {
                    "sent": "You get round this hack of the TF IDF, which maybe this is a more principled way of doing it, but you could say.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So my next sort of example of deeper learning is multiple kernel learning and this is where I think it starts to get a little bit more interesting in that you're actually using the label data to learn the representation, so that up until now we've had this phase of learning the representation, which is mainly been independent of the actual task, may be the first one.",
                    "label": 0
                },
                {
                    "sent": "To some extent we were using the task, but the typical way of using a Fisher kernels are, so use some sort of maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Which may not be a good choice in terms of the labeling data, so this is sort of potentially so.",
                    "label": 0
                },
                {
                    "sent": "How does multiple kernel learning learning work while you have this set of kernels and you make a linear combination of them in such a way that the coefficients are positive, that's to ensure that this is a positive definite kernel and that they sum to one.",
                    "label": 1
                },
                {
                    "sent": "So it's like a one norm regularization over the choice of different kernels that you provide.",
                    "label": 0
                },
                {
                    "sent": "And you optimize the usual.",
                    "label": 0
                },
                {
                    "sent": "SVM criterion the margin or the you know the norm of the weight vector minimize while allowing said T. To adapt, and perhaps at first sight, it might not appear so, but this is still a convex problem, so you actually have efficient algorithms.",
                    "label": 0
                },
                {
                    "sent": "You actually can arrive at this bound for multiple kernel.",
                    "label": 0
                },
                {
                    "sent": "The performance of your learned classifier based on the N different kernels that I've given here.",
                    "label": 1
                },
                {
                    "sent": "Which is just an application of Rademacher complexity, but it uses a very interesting property of Rademacher complexity, which is this convex Hull bound.",
                    "label": 0
                },
                {
                    "sent": "So when you move from the convex Hull of a set of functions, sorry from a set of functions to their convex Hull, you pay no price in terms of Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "There's a sort of freebie there, so you could take you know 20 functions.",
                    "label": 0
                },
                {
                    "sent": "They have a certain Rademacher complexity, take their convex, how they have the same Rademacher complexity so.",
                    "label": 0
                },
                {
                    "sent": "That's implicit here because I've just taken the union of these function classes corresponding to each kernel.",
                    "label": 0
                },
                {
                    "sent": "This is just the linear clients from class for kernel T, I've just taken them the actual bound that I would need is the convex Hull of these, but we don't need to compute that Rademacher complexity because it comes for free.",
                    "label": 0
                },
                {
                    "sent": "So we just need to compute this Rademacher complexity.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is what?",
                    "label": 0
                },
                {
                    "sent": "By the way, Rademacher complexity is is just an average of your correlation with random noise.",
                    "label": 1
                },
                {
                    "sent": "I'm going to skip over this, but.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a bit of a bounding proof here, which shows you that actually the.",
                    "label": 0
                },
                {
                    "sent": "Rademacher complexity of that union just is the Max of the individual regular complexities with a penalty of a log of the number of kernels that you actually use.",
                    "label": 1
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Actual number of kernels comes in under the log with.",
                    "label": 0
                },
                {
                    "sent": "These could if you had sort of Gaussian kernels for each.",
                    "label": 0
                },
                {
                    "sent": "These would all just be the same, so this would just be square root of M here for this quantity here.",
                    "label": 0
                },
                {
                    "sent": "So you pay no penalty here effectively and you just take a log penalty in terms of the number of kernels.",
                    "label": 1
                },
                {
                    "sent": "Which is extraordinary, but because it implies that you can really go wild in terms of throwing in kernels, and I think people have tended to be a bit conservative when they run multiple kernel learning, they think of, you know, maybe I'll try 10 kernels mean.",
                    "label": 0
                },
                {
                    "sent": "Maybe 20, you know?",
                    "label": 0
                },
                {
                    "sent": "And this is saying you shouldn't.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Should really go for it.",
                    "label": 0
                },
                {
                    "sent": "And Interestingly, these this work by Manic who should be giving this talk so I'm very happy to be able to cite his work here was actually did do that with very large millions of kernels and they got really significant improvements on the standard data set for vision getting being winners of this challenge in 17 out of 20 and half of them, they gotta increase in average, precision 25%.",
                    "label": 1
                },
                {
                    "sent": "So it's really.",
                    "label": 0
                },
                {
                    "sent": "You know significant hiking performance, so I think you know the lesson would be for me here.",
                    "label": 0
                },
                {
                    "sent": "You know, we need to be a bit more courageous in throwing in kernels.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't want that one extravagant kernel that has.",
                    "label": 0
                },
                {
                    "sent": "You know very high expressivity which would cost you in the bond well.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With that single out that that would be true, but the Max power you need to form a lot of different.",
                    "label": 0
                },
                {
                    "sent": "But this actual Max.",
                    "label": 0
                },
                {
                    "sent": "If you using normalized kernels.",
                    "label": 0
                },
                {
                    "sent": "You're OK because this actually for a normalized kernel is just root M, so this will be two over gamma router.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think I'm going to have to skip.",
                    "label": 0
                },
                {
                    "sent": "Probably this what this was going to be showing a linear programming boosting algorithm that can be adapted to multiple kernel learning.",
                    "label": 1
                },
                {
                    "sent": "So just this is what you get if you replace the two norm regularization SVM with the one norm R and you'd end up with a linear program and you can solve it through.",
                    "label": 0
                },
                {
                    "sent": "Jewel approach using an iterative method which is essentially a column generation method and it corresponds to boosting and you're boosting criterion is exactly the same for standard boosting, but at each iteration rather than just adding in an extra week learner with and you waiting, you have to solve a linear program and it gives you a nice stopping criti.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But Interestingly, can apply exactly the same method for multiple kernel learning, because you can solve this week learning criterion over the whole function class and you actually end up with a solution that can be represented Jewel Way.",
                    "label": 0
                },
                {
                    "sent": "So there's just a kind of neat connection to boosting at this.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Points you sort of boosting your kernels effectively, and this is the dual representation of the weak learner.",
                    "label": 1
                },
                {
                    "sent": "Or you know the function that you need to add into your.",
                    "label": 0
                },
                {
                    "sent": "So you gotta sort of an implementation.",
                    "label": 1
                },
                {
                    "sent": "More generally, this vector U, which was the coming out of the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The solution of the linear program at each stage you get this UI, which is a distribution over the examples, is like your boosting distribution can be used as a sick.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you can actually use that to drive the learning of your representation.",
                    "label": 0
                },
                {
                    "sent": "So we did this with Fisher kernels and applied that knew signal to adapt the parameters of the Fisher kernel.",
                    "label": 1
                },
                {
                    "sent": "So rather than use maximum likelihood, use this as your criterion for learning the actual representation.",
                    "label": 1
                },
                {
                    "sent": "We used it in and HMM, Fisher kernel for modeling time series with some encouraging results.",
                    "label": 0
                },
                {
                    "sent": "I won't say they're fantastic, but they're kind of OK results in foreign exchange.",
                    "label": 0
                },
                {
                    "sent": "Application.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So final topic 2 minutes.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is about nonlinear feature selection.",
                    "label": 1
                },
                {
                    "sent": "There's an interesting connection between the target alignment, which is this expected value of this correlation between the sort of targets and the.",
                    "label": 0
                },
                {
                    "sent": "The kernel matrix and the degree of correlation that you can get between the target sorry and and some choice of weight vector here.",
                    "label": 0
                },
                {
                    "sent": "So the learned weight vector can correlate with the target to a degree that is given by this.",
                    "label": 0
                },
                {
                    "sent": "So there's this suggestion from this that you can assess the contribution of a feature by looking at the way in which adding that feature in.",
                    "label": 1
                },
                {
                    "sent": "2 random sets of features.",
                    "label": 0
                },
                {
                    "sent": "So we're taking a random subset of features that includes that variable I and those that similar size that don't include that variable.",
                    "label": 0
                },
                {
                    "sent": "I an seeing what the correlation this alignment is, and if there are any hike in this alignment.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's an example where we've artificial example the X or function on the first 2 features take a Gaussian kernel, and you do this.",
                    "label": 0
                },
                {
                    "sent": "These plots of the contributions.",
                    "label": 0
                },
                {
                    "sent": "In other words, this value CI for the different features and what we've done is just.",
                    "label": 0
                },
                {
                    "sent": "Get rid of the bottom, say 25% of the features at each iteration, so this is the start.",
                    "label": 0
                },
                {
                    "sent": "Get rid of the bottom 25%.",
                    "label": 0
                },
                {
                    "sent": "Next one bed and get rid of the bottom 25% and so on.",
                    "label": 0
                },
                {
                    "sent": "And you see how these two features survive this call.",
                    "label": 0
                },
                {
                    "sent": "And as the number of features reduces their.",
                    "label": 0
                },
                {
                    "sent": "Influence is seen to grow, so these are the two features.",
                    "label": 0
                },
                {
                    "sent": "These are all mistake features.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's sort of the inspiration, and there's some nice properties of this thing that I won't go into.",
                    "label": 0
                },
                {
                    "sent": "But basically it motivates this algorithm of culling the features based on the rankings of their contributions.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is to show that it actually performed similarly to more expensive methods in terms of identifying the features.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also on some real world ohmic and microarray data.",
                    "label": 0
                },
                {
                    "sent": "But I won't.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Spend time on that.",
                    "label": 0
                },
                {
                    "sent": "This is the punch line which we applied this idea to a deep learning challenge.",
                    "label": 1
                },
                {
                    "sent": "Furnal swipe back and after an initial filtering step we applied this culling as described and then an LP boost MCL to the corresponding features and we came third in the ranking of this.",
                    "label": 1
                },
                {
                    "sent": "Sort of deep learning challenge, so we're very pleased anyway.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So summary is basically learning deep representations is important for real data.",
                    "label": 1
                },
                {
                    "sent": "Many kernel practitioners are using deep learning, but typically in a sort of more ad hoc manner.",
                    "label": 1
                },
                {
                    "sent": "And what we hope I presented as some.",
                    "label": 1
                },
                {
                    "sent": "You know, the value of having slightly more principled methods, and I think there's already a range of sort of theoretical results that are quite beginning to put deeper kernels on a firmer footing.",
                    "label": 0
                },
                {
                    "sent": "I hope, thanks.",
                    "label": 0
                }
            ]
        }
    }
}