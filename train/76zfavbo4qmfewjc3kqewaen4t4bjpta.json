{
    "id": "76zfavbo4qmfewjc3kqewaen4t4bjpta",
    "title": "Combining Predictions for Accurate Recommender Systems",
    "info": {
        "author": [
            "Michael Jahrer, commendo research & consulting GmbH"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Databases"
        ]
    },
    "url": "http://videolectures.net/kdd2010_jahrer_cpar/",
    "segmentation": [
        [
            "Welcome to my presentation.",
            "The topic is combining predictions for accurate recommender systems."
        ],
        [
            "So the outline of my talk is first multi rate.",
            "How we should combine predictions in order to improve the accuracy in a recommender system?",
            "Then they give an overview which corporate filtering algorithms was used.",
            "Then I go to the planning process.",
            "We have this algorithms get get combined and finally every percent experimental results on the Netflix Prize data set and give an example how to combine predictions on their credit on TCS KDD Cup 2010 data set."
        ],
        [
            "So why should you make accurate recommendations or accurate predictions in the recommender setup?",
            "So I create recommendations made directly increase their sales in an online shop or online portal.",
            "Good recommendations guides, users to the product they want to purchase and it also can increase cross selling ability, often recommender system and what's also very obvious is that the user activity on the given side can be increased when their recommendations are better fitted to the to the customers and to the.",
            "Tastes OK, this is just an example of a recommendation from Amazon."
        ],
        [
            "OK, So what kind of corporate filtering algorithms we're successfully applied in the past?",
            "So since the Netflix prices started, this single relative composition or matrix factorization techniques becomes very popular.",
            "We have two this SVT models that came nearest neighbor algorithms which are also very popular.",
            "Then we have so-called estimated factor models, which is an extension to the matrix factorization technique.",
            "We have restricted Boltzmann machines, which can be applied on this problem on collaborative filtering, and we have some global effects which cover some basic effects in the data set."
        ],
        [
            "So the first algorithm for rating prediction is the single value to composition, so it's very popular and and has some nice properties such as.",
            "It scales linearly with the number of samples and it you can request a prediction within Dot product of an item times the user feature.",
            "So prediction is given in linear in constant time and it delivers also very good.",
            "Accuracy.",
            "If.",
            "That is, if this matrix factorization is regularised carefully.",
            "Yeah, and and training is just unused stochastic gradient decent."
        ],
        [
            "Then our next approach is the key nearest neighbor algorithm.",
            "It's very natural in giving a rating prediction because it is just a weighted sum of K best correlating items.",
            "If you have an item Keinan Order K best correlating users.",
            "If you look at the user side.",
            "And.",
            "It hasn't this nice property of a constant prediction time.",
            "Yeah, the prediction for one rating is quadratic with respect to the number of items."
        ],
        [
            "So the next couple filtering model, which is also very popularized as metric factor model.",
            "What's the difference between this model and plain single ability composition at the user feature?",
            "What you can see here on the right side is so user is represented via his rated items or purchased items.",
            "This is called this setenv renew and the nice thing in this model is that new user can be integrated.",
            "Without retraining the whole model.",
            "And yeah, so user is parameterized via his rated items and.",
            "Training is also done in this model with simple stochastic gradient descent."
        ],
        [
            "Then they are also very popular model.",
            "Is this restricted Boltzmann machine.",
            "So basically this is a two layer undirected graphical model.",
            "The learning is down here with this.",
            "Contrastive directions learning rule and the basic idea is that there are paying tries to reconstruct the visible units itself and.",
            "The predictions are given by.",
            "Rating probabilities.",
            "So this is the figure below you can see here the basic structure of the net.",
            "So training is done with this model, but you are user Vice."
        ],
        [
            "And finally we have some global effects.",
            "This can be compared to a matrix decomposition with either fixed item or fixed user features.",
            "So for example, these features can be a user time effect or a movie time effect or such other handcrafted effects in the data.",
            "And got."
        ],
        [
            "OK, so now I want to demonstrate how this blending works so planning is apply a supervised learner in order to combine the predictions to to have at the end the best accuracy.",
            "So our error measure is the root mean squared error.",
            "And this figure shows you you have at the beginning data set which contains ratings from users on items.",
            "Then we apply different collaborative filtering algorithms such this matrix decomposition, K nearest neighbor restricted Boltzmann machines and all of these models generate predictions.",
            "And now we want to plant them.",
            "So enter, planned are finally results in a single estimated rating for end user who and item I."
        ],
        [
            "We evaluate our predictions and blenders.",
            "On the Netflix Prize data set, so this data set contains 100 million sofa ratings and the data set which was used for our blending experiments.",
            "Is this proper data set and this probe data set is again splitted in two datasets.",
            "One is this P test set which is never touched during training and this P train set is used to train this planning algorithms.",
            "This different blenders and.",
            "Yeah, so so.",
            "The summary is that we.",
            "Drain the collaborative filtering algorithms on the train set except the probe set and this probe set is used for for our planning experiments and the qualifying set, which is another test set in the spice data set.",
            "So we have also predictions for the qualifying set and at the end we can compare the values the Army values on the qualifying set to the leaderboard scores."
        ],
        [
            "So what what is an awkward body filtering on sombre we have for SVD predictions for a fan predictions and various other predictions?",
            "So here is the detailed list.",
            "Adjust parameter tweaks, different feature sizes, different learning rates.",
            "Some of them are trained on receivers of other particulars, so we have 18 predictors and we enlarge this feature set with the logarithm of the support as additional input, and so this is our training set for the planning algorithms."
        ],
        [
            "So the planning set-up is we have a train set which is this capital X matrix.",
            "This has around 700,000 training samples and we have 19 features which has continuous values.",
            "And the targets to these samples are this five star ratings.",
            "So one to five are the targets and this is a regression problem and we want to minimize the quadratic error on the targets by using a good planning algorithm."
        ],
        [
            "So this is a short view of what is inside this matrix.",
            "So every column represents a separate predictor and you see they're just small variations in the predictions.",
            "Yeah, and then we want to approximate this target column."
        ],
        [
            "So our first planning algorithm is a simple linear regression.",
            "Yeah, this this model is regularised with rich aggression and this regression coefficient.",
            "This is very small term across validation and it results in harmacy of.",
            ".8752 and we use this linear regression as a baseline for comparing other planning algorithms.",
            "So what you can see here is the individual blending weights that each particular receives, and in the brackets there are the individual RMS values on the predictors and you can see predictors with accurate or lower MSE gets higher weight in the linear combination."
        ],
        [
            "The next extension to linear regression is.",
            "We can use a pinning, so we will have different criteria is how we can split the data set.",
            "For example, we can split the data set according to number of ratings per user, which is called support.",
            "So for example we can calculate separate regression weights for user with one to 10 volts, then for 11 to 20 volts and so on.",
            "And we make experiments with this.",
            "Paintings on the blank data set and we see that the support pinning works best and it gives an RMC of .8747.",
            "So your orders can compare the values to the linear regression based and you see this arm assist improved.",
            "So what we should keep in mind this Army see there is a overrated on 700,000 test samples.",
            "So at the tests it is quite large and significant.",
            "Is it is a significant improvement over the linear regression baseline."
        ],
        [
            "Next approach is using a neural network.",
            "Is blender so we train a single hidden layer neural network with stochastic gradient dissent.",
            "We decrease the initial run rate target towards 0 using a linear function and but also improves the currencies is to use a large bag of neural networks.",
            "So our best result is calculated by a single layer 17 neuron network with packing size of 128.",
            "What's the nice property of this method is that it improves the harmacy greatly so the arm see will is for this neural network.",
            ".8731, which is around.",
            ".002 better compared to the linear regression baseline.",
            "So the drawback of this blending method is that it requires long train time.",
            "Due to the large number of.",
            "Neurons an independent set up and justice to hostage gradient descent, is rather slower learning algorithm."
        ],
        [
            "Yeah, I will next.",
            "Blender is a grading boosted decision tree, so we use these three to combine our predictions.",
            "Yeah, the tree is built by the help of begging so.",
            "That retrains epox wise and what you can see is that gradient boosted decision tree lower the arm.",
            "See when they learn rates get smaller and when we increase the padding size and the other two para meters, the Max maximum number of leaves in the tree and the random subspace sizes data set dependent and we get the best harmacy values with 300 number of leaves in the tree and subspaces of only two features.",
            "Which are considered in a single split when we build the tree.",
            "And this resulting in the score of .8742."
        ],
        [
            "The next algorithm we want to consider is this kernel Ridge regression.",
            "The drawback of this algorithm is that it has very large runtime at it.",
            "Also, it only can be applied to a subset of the train set, and what our experiment was is to take random subsets of the training set and average of a number of discourage aggression models and what you can see here is that just with one person's upset, we can.",
            "Yeah, improve the harmacy over linear regression when you take only 10 models in average and this.",
            "Our experiment shows is that larger subsets results in much more accurate are messy."
        ],
        [
            "Next operating be dry.",
            "To apply on the planning process was the key nearest neighbor algorithm.",
            "So it has also bad runtime constraints and what we see here that it simply doesn't work for this regression problem.",
            "It delivers very bad, are messy."
        ],
        [
            "So our most accurate Landa is ensemble of Flanders.",
            "So for demonstration, I have again here it is this figure which shows you the process of planning.",
            "So again we we have a data set.",
            "Many corporate filtering algorithms construct predictions, and now we apply many planters and and at the end we simply combine them with with linear regression to get the final rating prediction.",
            "Our goal, we repeat this example of blenders in a greedy fashion, so we train just one after another and we.",
            "Minimizing each step just a linear combination of them."
        ],
        [
            "So this is the exactly the exact setup we used for this experiment, so we at the first time between India and Network we stop them when their linear combination with the constant prediction reaches the minimum.",
            "Then we add a gradient boosted decision tree and look at the linear combination of this and solver and so on.",
            "So we trained all seven different planters and what you can see here in here in the brackets.",
            "This is the plantar Missy and this all this plantar Missy gets better and better when we add.",
            "Another model with different paramaters set up Parramatta settings.",
            "Yeah, and it results in the Embassy of .8729, which is our best result on the speed test speed test set at.",
            "The drawback is that requires a lot of training time, so we train the model for 158 hours."
        ],
        [
            "Now I want to present results on the qualifying set.",
            "So as mentioned before, the qualifying set was the real test set in the Netflix Prize competition and our linear regression baseline model delivers this RMC.",
            "So .8681 is the linear regression baseline, and our best models are best modeled is begging with seven single planters results in harmacy of .8660.",
            "Which is an improvement of points you through to one, and we look at our competitors so they also published a paper which is called feature weighted linear stacking and teleporting harmacy on the on the qualifying set of points you 02, which is comparable to our research but with a totally different approach."
        ],
        [
            "So what's what's the summary of my talk?",
            "Finding many collaborative filtering algorithms improves the final accuracy.",
            "And I would recommend a neural network is blender because it has the best tradeoff between training time and finally take currency and what you can see here in this figure at the bottom.",
            "So this is the absolute harmacy scale and on the bottom you see the best as individual predictions from from different collaborative filtering algorithms and you can see here a linear combination greatly improves the accuracy over single predictor.",
            "And we put more emphasis in this blending and put more computational power into this planning process.",
            "This RMC can be further improved."
        ],
        [
            "What I want to mention is this complete software data is open source, so you can download here and play around and I provide many examples on other datasets."
        ],
        [
            "So I mentioned at the beginning of the presentation we apply this.",
            "Is Blender on this KDD Cup 2010 data set?",
            "So what you can see here is an example feature vector.",
            "So in this data set we have generated 36 predictors and we enlarge this predictions by such meta features on this data set we have available information.",
            "For example knowledge component, program view hierarchy encoding and so on.",
            "And at the end 2 hidden layer neural network mixes them together and we can also show that it proves that.",
            "Obviously over linear regression baseline."
        ],
        [
            "So thank you.",
            "We have time for one quick question.",
            "Why you report?",
            "The memory can watch appears, paper test and square.",
            "Huawei.",
            "Yeah, maybe.",
            "Maybe this is my mistake maybe?",
            "Maybe they should NN log NI think.",
            "Memory complexity, so I assume that we precalculate or item item correlations, so this is just the basic idea to get fast access to this correlations.",
            "So OK quick one.",
            "Sounds of the degree of variation of running around.",
            "There's some stuff that is the best value ordered.",
            "Sorry I can't understand you clearly.",
            "Asking about variation from one to one stability of the results.",
            "I think this measured Argosy values are can be compared up to the 4th, each behind the comma we have because we use a very large test set to evaluate these results.",
            "So we have nearly 101 million samples in the validation set to generate the comparisons between them.",
            "OK, so let's send the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Welcome to my presentation.",
                    "label": 0
                },
                {
                    "sent": "The topic is combining predictions for accurate recommender systems.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the outline of my talk is first multi rate.",
                    "label": 0
                },
                {
                    "sent": "How we should combine predictions in order to improve the accuracy in a recommender system?",
                    "label": 0
                },
                {
                    "sent": "Then they give an overview which corporate filtering algorithms was used.",
                    "label": 0
                },
                {
                    "sent": "Then I go to the planning process.",
                    "label": 0
                },
                {
                    "sent": "We have this algorithms get get combined and finally every percent experimental results on the Netflix Prize data set and give an example how to combine predictions on their credit on TCS KDD Cup 2010 data set.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why should you make accurate recommendations or accurate predictions in the recommender setup?",
                    "label": 0
                },
                {
                    "sent": "So I create recommendations made directly increase their sales in an online shop or online portal.",
                    "label": 0
                },
                {
                    "sent": "Good recommendations guides, users to the product they want to purchase and it also can increase cross selling ability, often recommender system and what's also very obvious is that the user activity on the given side can be increased when their recommendations are better fitted to the to the customers and to the.",
                    "label": 1
                },
                {
                    "sent": "Tastes OK, this is just an example of a recommendation from Amazon.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what kind of corporate filtering algorithms we're successfully applied in the past?",
                    "label": 0
                },
                {
                    "sent": "So since the Netflix prices started, this single relative composition or matrix factorization techniques becomes very popular.",
                    "label": 0
                },
                {
                    "sent": "We have two this SVT models that came nearest neighbor algorithms which are also very popular.",
                    "label": 0
                },
                {
                    "sent": "Then we have so-called estimated factor models, which is an extension to the matrix factorization technique.",
                    "label": 0
                },
                {
                    "sent": "We have restricted Boltzmann machines, which can be applied on this problem on collaborative filtering, and we have some global effects which cover some basic effects in the data set.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first algorithm for rating prediction is the single value to composition, so it's very popular and and has some nice properties such as.",
                    "label": 0
                },
                {
                    "sent": "It scales linearly with the number of samples and it you can request a prediction within Dot product of an item times the user feature.",
                    "label": 0
                },
                {
                    "sent": "So prediction is given in linear in constant time and it delivers also very good.",
                    "label": 0
                },
                {
                    "sent": "Accuracy.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "That is, if this matrix factorization is regularised carefully.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and and training is just unused stochastic gradient decent.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then our next approach is the key nearest neighbor algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's very natural in giving a rating prediction because it is just a weighted sum of K best correlating items.",
                    "label": 1
                },
                {
                    "sent": "If you have an item Keinan Order K best correlating users.",
                    "label": 0
                },
                {
                    "sent": "If you look at the user side.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It hasn't this nice property of a constant prediction time.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the prediction for one rating is quadratic with respect to the number of items.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next couple filtering model, which is also very popularized as metric factor model.",
                    "label": 0
                },
                {
                    "sent": "What's the difference between this model and plain single ability composition at the user feature?",
                    "label": 0
                },
                {
                    "sent": "What you can see here on the right side is so user is represented via his rated items or purchased items.",
                    "label": 1
                },
                {
                    "sent": "This is called this setenv renew and the nice thing in this model is that new user can be integrated.",
                    "label": 1
                },
                {
                    "sent": "Without retraining the whole model.",
                    "label": 0
                },
                {
                    "sent": "And yeah, so user is parameterized via his rated items and.",
                    "label": 0
                },
                {
                    "sent": "Training is also done in this model with simple stochastic gradient descent.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then they are also very popular model.",
                    "label": 0
                },
                {
                    "sent": "Is this restricted Boltzmann machine.",
                    "label": 1
                },
                {
                    "sent": "So basically this is a two layer undirected graphical model.",
                    "label": 1
                },
                {
                    "sent": "The learning is down here with this.",
                    "label": 1
                },
                {
                    "sent": "Contrastive directions learning rule and the basic idea is that there are paying tries to reconstruct the visible units itself and.",
                    "label": 0
                },
                {
                    "sent": "The predictions are given by.",
                    "label": 0
                },
                {
                    "sent": "Rating probabilities.",
                    "label": 0
                },
                {
                    "sent": "So this is the figure below you can see here the basic structure of the net.",
                    "label": 0
                },
                {
                    "sent": "So training is done with this model, but you are user Vice.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally we have some global effects.",
                    "label": 1
                },
                {
                    "sent": "This can be compared to a matrix decomposition with either fixed item or fixed user features.",
                    "label": 1
                },
                {
                    "sent": "So for example, these features can be a user time effect or a movie time effect or such other handcrafted effects in the data.",
                    "label": 0
                },
                {
                    "sent": "And got.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I want to demonstrate how this blending works so planning is apply a supervised learner in order to combine the predictions to to have at the end the best accuracy.",
                    "label": 1
                },
                {
                    "sent": "So our error measure is the root mean squared error.",
                    "label": 0
                },
                {
                    "sent": "And this figure shows you you have at the beginning data set which contains ratings from users on items.",
                    "label": 0
                },
                {
                    "sent": "Then we apply different collaborative filtering algorithms such this matrix decomposition, K nearest neighbor restricted Boltzmann machines and all of these models generate predictions.",
                    "label": 0
                },
                {
                    "sent": "And now we want to plant them.",
                    "label": 1
                },
                {
                    "sent": "So enter, planned are finally results in a single estimated rating for end user who and item I.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We evaluate our predictions and blenders.",
                    "label": 0
                },
                {
                    "sent": "On the Netflix Prize data set, so this data set contains 100 million sofa ratings and the data set which was used for our blending experiments.",
                    "label": 0
                },
                {
                    "sent": "Is this proper data set and this probe data set is again splitted in two datasets.",
                    "label": 0
                },
                {
                    "sent": "One is this P test set which is never touched during training and this P train set is used to train this planning algorithms.",
                    "label": 1
                },
                {
                    "sent": "This different blenders and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so.",
                    "label": 0
                },
                {
                    "sent": "The summary is that we.",
                    "label": 0
                },
                {
                    "sent": "Drain the collaborative filtering algorithms on the train set except the probe set and this probe set is used for for our planning experiments and the qualifying set, which is another test set in the spice data set.",
                    "label": 1
                },
                {
                    "sent": "So we have also predictions for the qualifying set and at the end we can compare the values the Army values on the qualifying set to the leaderboard scores.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what what is an awkward body filtering on sombre we have for SVD predictions for a fan predictions and various other predictions?",
                    "label": 0
                },
                {
                    "sent": "So here is the detailed list.",
                    "label": 0
                },
                {
                    "sent": "Adjust parameter tweaks, different feature sizes, different learning rates.",
                    "label": 0
                },
                {
                    "sent": "Some of them are trained on receivers of other particulars, so we have 18 predictors and we enlarge this feature set with the logarithm of the support as additional input, and so this is our training set for the planning algorithms.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the planning set-up is we have a train set which is this capital X matrix.",
                    "label": 0
                },
                {
                    "sent": "This has around 700,000 training samples and we have 19 features which has continuous values.",
                    "label": 0
                },
                {
                    "sent": "And the targets to these samples are this five star ratings.",
                    "label": 0
                },
                {
                    "sent": "So one to five are the targets and this is a regression problem and we want to minimize the quadratic error on the targets by using a good planning algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a short view of what is inside this matrix.",
                    "label": 1
                },
                {
                    "sent": "So every column represents a separate predictor and you see they're just small variations in the predictions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then we want to approximate this target column.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our first planning algorithm is a simple linear regression.",
                    "label": 1
                },
                {
                    "sent": "Yeah, this this model is regularised with rich aggression and this regression coefficient.",
                    "label": 0
                },
                {
                    "sent": "This is very small term across validation and it results in harmacy of.",
                    "label": 0
                },
                {
                    "sent": ".8752 and we use this linear regression as a baseline for comparing other planning algorithms.",
                    "label": 0
                },
                {
                    "sent": "So what you can see here is the individual blending weights that each particular receives, and in the brackets there are the individual RMS values on the predictors and you can see predictors with accurate or lower MSE gets higher weight in the linear combination.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next extension to linear regression is.",
                    "label": 1
                },
                {
                    "sent": "We can use a pinning, so we will have different criteria is how we can split the data set.",
                    "label": 0
                },
                {
                    "sent": "For example, we can split the data set according to number of ratings per user, which is called support.",
                    "label": 1
                },
                {
                    "sent": "So for example we can calculate separate regression weights for user with one to 10 volts, then for 11 to 20 volts and so on.",
                    "label": 0
                },
                {
                    "sent": "And we make experiments with this.",
                    "label": 0
                },
                {
                    "sent": "Paintings on the blank data set and we see that the support pinning works best and it gives an RMC of .8747.",
                    "label": 0
                },
                {
                    "sent": "So your orders can compare the values to the linear regression based and you see this arm assist improved.",
                    "label": 0
                },
                {
                    "sent": "So what we should keep in mind this Army see there is a overrated on 700,000 test samples.",
                    "label": 0
                },
                {
                    "sent": "So at the tests it is quite large and significant.",
                    "label": 0
                },
                {
                    "sent": "Is it is a significant improvement over the linear regression baseline.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next approach is using a neural network.",
                    "label": 0
                },
                {
                    "sent": "Is blender so we train a single hidden layer neural network with stochastic gradient dissent.",
                    "label": 1
                },
                {
                    "sent": "We decrease the initial run rate target towards 0 using a linear function and but also improves the currencies is to use a large bag of neural networks.",
                    "label": 0
                },
                {
                    "sent": "So our best result is calculated by a single layer 17 neuron network with packing size of 128.",
                    "label": 0
                },
                {
                    "sent": "What's the nice property of this method is that it improves the harmacy greatly so the arm see will is for this neural network.",
                    "label": 0
                },
                {
                    "sent": ".8731, which is around.",
                    "label": 0
                },
                {
                    "sent": ".002 better compared to the linear regression baseline.",
                    "label": 0
                },
                {
                    "sent": "So the drawback of this blending method is that it requires long train time.",
                    "label": 0
                },
                {
                    "sent": "Due to the large number of.",
                    "label": 1
                },
                {
                    "sent": "Neurons an independent set up and justice to hostage gradient descent, is rather slower learning algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, I will next.",
                    "label": 0
                },
                {
                    "sent": "Blender is a grading boosted decision tree, so we use these three to combine our predictions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the tree is built by the help of begging so.",
                    "label": 0
                },
                {
                    "sent": "That retrains epox wise and what you can see is that gradient boosted decision tree lower the arm.",
                    "label": 1
                },
                {
                    "sent": "See when they learn rates get smaller and when we increase the padding size and the other two para meters, the Max maximum number of leaves in the tree and the random subspace sizes data set dependent and we get the best harmacy values with 300 number of leaves in the tree and subspaces of only two features.",
                    "label": 1
                },
                {
                    "sent": "Which are considered in a single split when we build the tree.",
                    "label": 0
                },
                {
                    "sent": "And this resulting in the score of .8742.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next algorithm we want to consider is this kernel Ridge regression.",
                    "label": 1
                },
                {
                    "sent": "The drawback of this algorithm is that it has very large runtime at it.",
                    "label": 0
                },
                {
                    "sent": "Also, it only can be applied to a subset of the train set, and what our experiment was is to take random subsets of the training set and average of a number of discourage aggression models and what you can see here is that just with one person's upset, we can.",
                    "label": 0
                },
                {
                    "sent": "Yeah, improve the harmacy over linear regression when you take only 10 models in average and this.",
                    "label": 0
                },
                {
                    "sent": "Our experiment shows is that larger subsets results in much more accurate are messy.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next operating be dry.",
                    "label": 0
                },
                {
                    "sent": "To apply on the planning process was the key nearest neighbor algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it has also bad runtime constraints and what we see here that it simply doesn't work for this regression problem.",
                    "label": 0
                },
                {
                    "sent": "It delivers very bad, are messy.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our most accurate Landa is ensemble of Flanders.",
                    "label": 0
                },
                {
                    "sent": "So for demonstration, I have again here it is this figure which shows you the process of planning.",
                    "label": 0
                },
                {
                    "sent": "So again we we have a data set.",
                    "label": 0
                },
                {
                    "sent": "Many corporate filtering algorithms construct predictions, and now we apply many planters and and at the end we simply combine them with with linear regression to get the final rating prediction.",
                    "label": 0
                },
                {
                    "sent": "Our goal, we repeat this example of blenders in a greedy fashion, so we train just one after another and we.",
                    "label": 1
                },
                {
                    "sent": "Minimizing each step just a linear combination of them.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the exactly the exact setup we used for this experiment, so we at the first time between India and Network we stop them when their linear combination with the constant prediction reaches the minimum.",
                    "label": 0
                },
                {
                    "sent": "Then we add a gradient boosted decision tree and look at the linear combination of this and solver and so on.",
                    "label": 0
                },
                {
                    "sent": "So we trained all seven different planters and what you can see here in here in the brackets.",
                    "label": 0
                },
                {
                    "sent": "This is the plantar Missy and this all this plantar Missy gets better and better when we add.",
                    "label": 0
                },
                {
                    "sent": "Another model with different paramaters set up Parramatta settings.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and it results in the Embassy of .8729, which is our best result on the speed test speed test set at.",
                    "label": 0
                },
                {
                    "sent": "The drawback is that requires a lot of training time, so we train the model for 158 hours.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I want to present results on the qualifying set.",
                    "label": 0
                },
                {
                    "sent": "So as mentioned before, the qualifying set was the real test set in the Netflix Prize competition and our linear regression baseline model delivers this RMC.",
                    "label": 1
                },
                {
                    "sent": "So .8681 is the linear regression baseline, and our best models are best modeled is begging with seven single planters results in harmacy of .8660.",
                    "label": 0
                },
                {
                    "sent": "Which is an improvement of points you through to one, and we look at our competitors so they also published a paper which is called feature weighted linear stacking and teleporting harmacy on the on the qualifying set of points you 02, which is comparable to our research but with a totally different approach.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's what's the summary of my talk?",
                    "label": 0
                },
                {
                    "sent": "Finding many collaborative filtering algorithms improves the final accuracy.",
                    "label": 1
                },
                {
                    "sent": "And I would recommend a neural network is blender because it has the best tradeoff between training time and finally take currency and what you can see here in this figure at the bottom.",
                    "label": 1
                },
                {
                    "sent": "So this is the absolute harmacy scale and on the bottom you see the best as individual predictions from from different collaborative filtering algorithms and you can see here a linear combination greatly improves the accuracy over single predictor.",
                    "label": 0
                },
                {
                    "sent": "And we put more emphasis in this blending and put more computational power into this planning process.",
                    "label": 0
                },
                {
                    "sent": "This RMC can be further improved.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I want to mention is this complete software data is open source, so you can download here and play around and I provide many examples on other datasets.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I mentioned at the beginning of the presentation we apply this.",
                    "label": 0
                },
                {
                    "sent": "Is Blender on this KDD Cup 2010 data set?",
                    "label": 1
                },
                {
                    "sent": "So what you can see here is an example feature vector.",
                    "label": 0
                },
                {
                    "sent": "So in this data set we have generated 36 predictors and we enlarge this predictions by such meta features on this data set we have available information.",
                    "label": 0
                },
                {
                    "sent": "For example knowledge component, program view hierarchy encoding and so on.",
                    "label": 0
                },
                {
                    "sent": "And at the end 2 hidden layer neural network mixes them together and we can also show that it proves that.",
                    "label": 0
                },
                {
                    "sent": "Obviously over linear regression baseline.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you.",
                    "label": 0
                },
                {
                    "sent": "We have time for one quick question.",
                    "label": 0
                },
                {
                    "sent": "Why you report?",
                    "label": 0
                },
                {
                    "sent": "The memory can watch appears, paper test and square.",
                    "label": 0
                },
                {
                    "sent": "Huawei.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is my mistake maybe?",
                    "label": 0
                },
                {
                    "sent": "Maybe they should NN log NI think.",
                    "label": 0
                },
                {
                    "sent": "Memory complexity, so I assume that we precalculate or item item correlations, so this is just the basic idea to get fast access to this correlations.",
                    "label": 0
                },
                {
                    "sent": "So OK quick one.",
                    "label": 0
                },
                {
                    "sent": "Sounds of the degree of variation of running around.",
                    "label": 0
                },
                {
                    "sent": "There's some stuff that is the best value ordered.",
                    "label": 0
                },
                {
                    "sent": "Sorry I can't understand you clearly.",
                    "label": 0
                },
                {
                    "sent": "Asking about variation from one to one stability of the results.",
                    "label": 0
                },
                {
                    "sent": "I think this measured Argosy values are can be compared up to the 4th, each behind the comma we have because we use a very large test set to evaluate these results.",
                    "label": 0
                },
                {
                    "sent": "So we have nearly 101 million samples in the validation set to generate the comparisons between them.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's send the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}