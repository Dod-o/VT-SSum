{
    "id": "ia4hjuex2l6ry7ache3yhguios22pcmq",
    "title": "Unsupervised Learning for Natural Language Processing",
    "info": {
        "author": [
            "Dan Klein, UC Berkeley"
        ],
        "published": "July 30, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/uai08_klein_ul/",
    "segmentation": [
        [
            "OK, thank you.",
            "I should warn you all.",
            "I have nothing whatsoever to say about the optics of light.",
            "What I am going to talk about today is some work by my group on unsupervised learning for language."
        ],
        [
            "So.",
            "I'm not going to make any cognitive claims whatsoever in this talk, but I'm going to draw one cognitive analogy briefly, so please forgive me.",
            "As humans, there are two ways that we learn languages.",
            "You've probably done both of these.",
            "One is to learn a language in a classroom that looks something like this.",
            "You get taught rules, you get shown, examples.",
            "You get told how everything works and you get all the irregular verb forms listed for you and so on.",
            "At some point you take a test and you're asked to do some combination of regurgitation and generalization.",
            "OK, turns out for humans this doesn't work so great, but this is one of the ways we have, and this is basically what supervised NLP is like.",
            "The other way we learn languages as humans is something like this.",
            "And in this case, basically the data washes over you.",
            "We all did this as a kid.",
            "The data washes over you and you sort out for yourself what's going on underneath.",
            "This is what unsupervised NLP should be like and could be like an I'm interested in building systems like the system on the right.",
            "That pick apart their data and figure out for themselves what kinds of structure is going on underneath."
        ],
        [
            "So what's my goal in this talk?",
            "In this talk I'm going to give kind of an illustration by by several examples.",
            "I'm going to talk about inducing various kinds of linguistic structure that are not in the data.",
            "So each example is going to be slightly different, but they're all going to show the property that the kind of structure I'm interested in is some kind of complex linguistic phenomenon.",
            "OK, in one case, this will be syntactic parsing.",
            "In one case it will be Co reference.",
            "In the last will be translation.",
            "These are going to be kind of rich interacting combinatorial structures.",
            "And will be taking a lot of data and trying to figure out what's going on underneath that data, in a way that's not annotated in the data itself.",
            "So the characteristics those are basically the characteristics of the problem, the characteristics of the solutions that I'm going to sketch.",
            "They all have something in common, and the thread that's going to title together is some combination of kind of incremental or hierarchical learning, and I mean that in a very vague way, because what that means is going to vary from case to case.",
            "But I find this to be a very important tying thread that you either can't or don't want to learn everything all at once.",
            "This is also the solutions I'm going to sketch are going to involve kind of very careful and often iterative choice of what to model, and sometimes even more importantly, what not to model.",
            "So we'll see a couple of cases where you can't model everything you don't want to model everything an one good way to make progress that's worked very well in our group is to kind of look at what your system is doing, build something simple and then kind of only introduced the variables that you absolutely."
        ],
        [
            "Last model OK so from here on out I'm going to talk about several concrete examples.",
            "We're going to 1st talk about the task of unsupervised grammar refinement.",
            "This is related to but not exactly the same as grammar induction.",
            "I'll talk about this first, then we'll talk about coreference resolution and finally some work on translation."
        ],
        [
            "Cape.",
            "So hopefully many of you were at Mikes invited talk earlier and he talked a little bit about syntactic analysis, syntactic parsing.",
            "I'm going to talk about the same, the same problem.",
            "The goal here is to take sentence is an.",
            "Remember, most senses are actually very complicated, so I'll show simple examples, but please don't take from this that the examples in practice are simple, so here is a basically an average length sentence from newswire.",
            "Hurricane Emily held Tord Mexico's Caribbean coast on Sunday, packing 135 mile per hour winds and torrential rain.",
            "And causing panic in Cancun were frightened tour squeezed into musty shelters.",
            "OK, this is not.",
            "He saw the cat, right?",
            "If it were, he saw the cat, this would be easy.",
            "This is hard because, for example, there's wins an rain and panic.",
            "But it's not all the same.",
            "It's not a conjunction of three things here, and you need to sort out what the various pieces of this sentence are and how they combine together.",
            "And that's going to be the job of a syntactic parser.",
            "Why do we do this?",
            "We do this.",
            "We do this because it's kind of an important intermediary step between surface.",
            "Language and semantics.",
            "And it's also these kinds of tree structures are very useful for lots of problems like math."
        ],
        [
            "In translation.",
            "OK so back to simple examples, but please if there's nothing else you remember from this about syntactic parsing, Please remember that the reality is very complicated and it's not always he was right.",
            "So that's going to be the recurring example here.",
            "In particular, if you want to get a grammar and you would like to be parsing these structures well, you might look at them and say those look awful lot like context free grammars to me.",
            "And then you think, well, where am I going to get a context free grammar?",
            "Well, I could try to brainstorm one.",
            "It turns out that doesn't work very well.",
            "I could have a bunch of annotated sentences like this, where somebody has been kind enough to painstakingly mark out what the syntactic structures are like this to say.",
            "Well, there's a noun phrase and it contains a preposition, and it contains a pronoun.",
            "And so on.",
            "OK, if we had a bunch of these and we do, these are called treebanks.",
            "We could read off what the rules are from the grammar, so if I had this tree, I would read off.",
            "Well, there's one instance of a route going to sentence.",
            "There's one instance of a sentence going to a noun phrase, a verb phrase, followed by a period, and so on.",
            "Now I'd like to emphasize that if you just take, for example, we have 40,000 parsed English sentences, and I look at just the noun phrases alone.",
            "There aren't 30 or 40 kinds of noun phrase rules.",
            "There are 10,000, and you find new ones as you look at new sentence is.",
            "And this is because there's all kinds of phenomena interacting, punctuation and all kinds of modification, and all kinds of complexities that don't show up in these simple sentences.",
            "And they all interact, kind of combinatorially.",
            "OK, so but you could take this.",
            "These 40,000 sentences you could read off the grammar and then take these counts and normalize them and kind of the obvious way to get a probabilistic context free grammar.",
            "It's kind of a generalization of a hidden Markov model, and if you did this and ignoring any efficiency questions about how you parse and you use this to infer most likely parses, you would have a parser that has what I'll call her baseline accuracy of 72%.",
            "This F1 score is something like how many of your nodes are correct.",
            "I'm not going to go into the details.",
            "OK, so this people have been doing this for awhile.",
            "It turns out this isn't a very good number.",
            "You don't really want your parser to make a miss."
        ],
        [
            "1/4 of the nodes are wrong and you might ask, well, kind of our context.",
            "Free grammars, probabilistic, context free grammars, somehow the wrong model class and the answer is they're not necessarily the wrong model class.",
            "Just because your parser doesn't work very well, and one of the reasons is that the kind of the trees in the data that have symbols like noun phrase make overly strong conditional independence assertions when kind of viewed as probabilistic objects.",
            "So for example, here's another sentence.",
            "She heard the noise, and in the sentence there are the two green nodes.",
            "They're both noun phrases.",
            "So they're both noun phrases, linguistically, in terms of the roles they play in the sentence at some level.",
            "But somehow, as a kind of a context free grammar, it's not right because we can't grammatically swap these, we can't have the noise heard, she it would be the noise heard her aside from it being semantically anomalous, it wouldn't be syntactically correct, so there's something these aren't.",
            "Noun phrases aren't really the same, and if we build grammars that have such symbols, they're not going to be very good."
        ],
        [
            "So this brings us to the task of grammar refinement.",
            "What's grammar refinement all about we're observing these trees, but they're kind of only the courses description of what's going on linguistically.",
            "What's really going on is much finer grain, their subjects, their objects.",
            "There's passivization all kinds of phenomena are happening, and they're all happening at once, and they're not labeled in these trees, so we'd like to have some description of what's really going on.",
            "An one way to do this kind of one of the earliest ways was Mark Johnson's parent annotation, where we go along, saying, well.",
            "Those MPs are different because one is a subjective ones.",
            "An object we can tell that by their parents.",
            "So you go around marking all the nodes with their parents.",
            "Well, this is good.",
            "We've got more symbols, so our context freedom assumptions are less wrong.",
            "But now we also need more data to estimate anything and so you."
        ],
        [
            "Getting sparsity problems.",
            "Hey, what can you do?",
            "You can take an even more extreme case and say actually this isn't just any noun phrase in object position, it's something about noise and I want to say this is a noun phrase that's about noise because, well, it's an object of the verb heard and somehow heard goes with noise and I can try to kind of get in a little bit of not only lexical syntax, but also some lexical semantics here as well.",
            "OK, so this is what's called head lexicalization, where you go along annotating all of your nodes with actual words.",
            "Well, this makes it even harder sparsity problems.",
            "You can make good parsers in this way, but you have to work very, very hard and the whole game becomes overcoming the."
        ],
        [
            "Sparsity problems.",
            "So what I'm going to talk about here is an unsupervised approach to the refinement, so we're going to have the trees, but we're going to find them automatically and try to figure out in an automatic fashion what these true underlying syntactic processes are so.",
            "In this case, we might say that there's an NP1 and NP2, but I don't know what that means and I don't know which ones are which, but I'm going to sort it out statistically."
        ],
        [
            "What does this mean?",
            "This means that where we used to have parses, we observe parses in our tree bank.",
            "Are these processes that have sentences and noun phrases and verb phrases we observe them and at Test time we want to produce them.",
            "But we know that underneath everything is some more complex phenomenon and this more complex phenomenon is going to be represented by grammars over water called derivations shown on the right.",
            "So for example, here are two derivations on the right that both derive the same parse.",
            "OK, so we have these parses that'll be capital T. We have these derivations.",
            "These are lower case T and what exactly?"
        ],
        [
            "Are we trying to do well what we're trying to do here is?",
            "We're trying to find some grammar over these derivation trees that best explains in some sense, and being deliberately vague here.",
            "That best explains the observed parses OK. One option, one of the easiest options, and what I'll talk through here is to use maximum likelihood estimation with them.",
            "I'll briefly mention some other options, including being more beige and about this and looking at doing this so you could either want to be more beige and or more discriminative, or maybe somehow both.",
            "And we've looked at both of those things, but I'm going to mainly talk today about kind of the basic case of maximum likelihood in AM here.",
            "OK, So what do we want?",
            "We want these derivation parameters that maximize the likelihood of the parses and you look at that and say, well, that's exactly what year was built for.",
            "OK, so."
        ],
        [
            "So let's take that a little bit further.",
            "So what does M mean in this case?",
            "It means we're going to see structures like what's shown here on the left, where we have sentences and noun phrases and verb phrases there going to be latent variables that represent the clusters.",
            "At each of these points.",
            "Now, of course, this isn't like point clustering or mixture of Gaussians are selling.",
            "These clusters are structured, they're interacting because the cluster at one point determines the probability of reaching the clusters at other points.",
            "But there are bunch of latent variables in there kind of more or less on a chain.",
            "Here it's a tree, but the brackets are known and so.",
            "Essentially, learning here really isn't much more complicated than it is for Hmm's, and we can run a basic generalization of the forward backward that somewhere between forward, backward and the inside outside algorithm.",
            "OK, so we can use EM here to learn these sub categories.",
            "Anne."
        ],
        [
            "And what might we learn?",
            "Well, if we did this, we'd be splitting stuff everywhere.",
            "We have lots of kinds of sentences, lots of kinds of noun phrases, and lots of kinds of verb phrases.",
            "In particular, we have lots of kinds of each of the parts of speech.",
            "I'm going to show you lots of them because they're very easy to kind of.",
            "Get your head around, but don't confuse that this is only about word clustering.",
            "So here's the determiner category and the most common determiners in the tree Bank RVA and capital V cases preserved here.",
            "Right, if we split that into four and learn those clusters with them, and remember we're not just learning, determine clusters were learning clusters across the whole grammar.",
            "We would find in this case the algorithm produces four categories.",
            "You get one which is more or less the indefinite determiners.",
            "The second one, which is more or less definite, determiners, the third ones more or less demonstratives and the last ones more or less quantifiers.",
            "They're not perfect, but this is basically what's what's kind of being captured by the distributional clustering."
        ],
        [
            "OK, that's great, but what if we wanted eight and in fact we're going to end up with something more like 64128 well?",
            "Turns out it doesn't work very well.",
            "You have kind of basically have lots of search problems and you have lots of model problems when you just try to."
        ],
        [
            "Actually split like this, so here's something that worked very well in the speech literature and people have been doing in kind of a lot of different places, and that's don't learn all of your structure at once, learning hierarchically, so instead of just splitting it, all of my grammar everywhere, I'm going to maybe save that for every single observed symbol there's two underlying symbols.",
            "We might get the proper determiners in the demonstratives.",
            "Then we'll take those symbols and will further split them and we'll get this tree of incremental refinement.",
            "OK again.",
            "Now we're back to four, which is where we started.",
            "But it turns out you can go much."
        ],
        [
            "Farther without without hitting problems.",
            "So if we do this, we're essentially going to get in ontogeny of grammars, so we'll have some basic underlying simple grammar that then gets split and split and split in a way that's kind of incrementally structurally clustered until finally we get is a grammar that's as refined as we want.",
            "OK.",
            "So remember, we're going to have this sequence of grammars, and in each kind of at each step the symbols get more and more refined, but they correspond to symbols of the level at one level up."
        ],
        [
            "OK.",
            "So what does this give you well?",
            "So the green line here, the black dots are.",
            "Increasing rounds of this kind of flat training training with two categories for categories, eight categories and so on.",
            "The Green round, the green is kind of doing this hierarchical training, and the first thing to note is were a lot higher than the baseline now, so we actually start in this game lower than the baseline because we started with an extremely simple grammar and we end up with basically reasonable results.",
            "So now we're up to about 88% and just to give you a sense, kind of the range in which people report kind of state of the art parsing numbers is kind of in the.",
            "High 80s to kind of very, very low 90s range.",
            "OK, and you can see that the kind of the further you split the better it is to have trained things hierarchically."
        ],
        [
            "So it turns out this is a problem."
        ],
        [
            "Because this graph doesn't keep going, it doesn't look like it's awesome to did, right.",
            "You think we can keep splitting in one day?",
            "Our accuracy will hit like 300%, right?",
            "But it doesn't work.",
            "Those are joke.",
            "It doesn't work because you run out of kind of computational resources.",
            "This gets these rumors, get big, the number of rules grows."
        ],
        [
            "Grows cubically in the number of States and it turns out that we're really wasting a lot splitting Everything Everywhere, and maybe this should be obvious, but took us awhile to realize this and in particular there's a part of speech called, OK.",
            "The words in the part of speech, are.",
            "The comma.",
            "And so we split this into many, many clusters, each which contains the comma.",
            "So this is clearly a waste, so we don't want to be doing this.",
            "But even beyond that, what if we were just splitting determiners again?",
            "And here's kind of further down the determiner split tree, and if you look over there on the left, you can see that we've got two categories that really aren't so different.",
            "So are these two similar?",
            "To keep this split, or do we want to kind of stop splitting at this point?",
            "Well, I don't know.",
            "This is why I kind of."
        ],
        [
            "The computer sort this out and so I have to have some kind of test.",
            "The test we used is.",
            "We look at the likelihood with and without the split and if the gain in likelihood isn't enough then we reverse the split and we get these kind of ragged trees that only split where where it's useful."
        ],
        [
            "OK.",
            "So now essentially we're splitting all the categories, but then we take half of half of the splits that are at least useful and we merge them back together.",
            "What does this do?",
            "It kind of shrinks the rate at which our grammars grow substantially and suddenly all of these points that we were achieving with very big grammars.",
            "They all shift over to the left.",
            "We get similar accuracies with much smaller grammars.",
            "The accuracy is actually go up too, and we let us do more rounds of this without blowing up the grammar size, and now we can get a lot further.",
            "So now if we do some kind of merging as well.",
            "So that we're not allocating parameters everywhere, but only where they're needed, then we gain not only in size, but also in accuracy.",
            "Here up to 89.5%.",
            "Game.",
            "And this is already starting to be a state of the art number."
        ],
        [
            "So I said we were allocating complexity where it's needed.",
            "I'll return to this later, but for now we might kind of look and see what kinds of."
        ],
        [
            "Kinds of things we made complex, and it turns out noun phrases and verb phrases and prepositional phrases and other things that you've heard of are in fact very common, and they're also very complex, and we've captured that by kind of allocating."
        ],
        [
            "Out of parameters to them.",
            "But then things that you haven't heard about an you trust me.",
            "You really don't want to know about like the nada category and the X symbol with these are remained very simple, both because they aren't very frequent an because."
        ],
        [
            "They're not very complex.",
            "Same thing if you look at parts of speech, so nouns and adjectives.",
            "These kinds of things are very complicated, they."
        ],
        [
            "Split a lot, whereas our friend, the comma and the part of speech two which also only contains one word, don't get split at all because there's never any likelihood gain from it.",
            "OK, so we've kind of allocated parameters in an incremental way so that we learn reasonable things and we also stop when there's when there's kind of no more support."
        ],
        [
            "For these splits.",
            "What kinds of things do we learn?",
            "Well, here's some candy.",
            "Here are some of the proper nouns, so it's remember in the tree bank, there's just one kind of proper noun, but it turns out months are very different from people and their syntactic distribution in the system is learned that appropriately.",
            "So months in particular, abbreviated months, because they happen to be used in a stylistically particular way.",
            "Get one category, first names, middle names, last names.",
            "I'll get their own category.",
            "You can see the first word of a place in the second word of a place are syntactically different in their distribution, so they get their own categories in terms of pronouns.",
            "There's only one pronoun, but it turns out that according to the statistics, there are.",
            "There are three kinds of case.",
            "In English, there's nominative case there's Accusatives case, and there's uppercase, and the system is learned.",
            "This generalization, which never would have come kind of from a top down kind of linguistic notion where upper case is not a case.",
            "That was a joke too."
        ],
        [
            "Here's some other splits so you can see adverbs have been split.",
            "I personally would have had trouble figuring out how to split the adverbs, but it turns out that there's various kinds of degree adverbs and they pattern differently.",
            "Syntactically numbers.",
            "There's a very big difference between numbers that mean years and numbers that mean stock prices, and so they get different categories because they kind of occur in different places in the tree."
        ],
        [
            "No one.",
            "OK, so this is a case where what we've learned.",
            "We kind of this is an incremental learning regime where we're starting with only the coarsest observation of the course is kind of syntax, and we're starting to learn the complex phenomenon that underlies that in that kind of an incremental fashion.",
            "And remember, we get this sequence of grammars, and so far I've motivated this sequence purely as being a way of kind of getting EM to do the right thing, that somehow if you try to learn everything all at once, it's not going to work.",
            "And maybe this is nice because we can merge it every round.",
            "If I really only wanted to tell you about unsupervised learning, I would stop here and move on to the next topic.",
            "There's actually something else I want to talk about that has almost nothing to do with unsupervised learning is a small digression and that has to do with kind of course defined analysis.",
            "This is essentially course define model learning.",
            "OK, so we're starting with the course model and we're gradually refining it, but it turns out we can do the same thing when it's time to do inference, so when it's time to actually figure out what the right parses are, remember we have this big grammar now that has all these symbols.",
            "Doesn't have one noun phrase with.",
            "A gazillion rules it now has 32 or 128 or however many.",
            "There are noun phrases with a gazillion rules and so the grammars are getting quite complex and to directly do inference with them would kind of scale."
        ],
        [
            "Very badly, but we can do something we can exploit.",
            "These other grammars we have and we can do course defined analysis.",
            "This is not something new, but it's something important.",
            "So this goes back to kind of charniak's earlier parsers, and there's a multipath version of this, more like what I'm going to talk about here in Chiniak and Johnson and the way it basically works is well.",
            "If we had a course grammar, it would only have symbols like quantifier phrase and noun phrases and verb phrase.",
            "And if I was analyzing my dynamic program some span, say from word 5 toward 12.",
            "I want to figure out what belonged here.",
            "I could say maybe this grammar isn't very good, but it's good enough to rule out some of these things right?",
            "It's so confident that some of these things aren't here.",
            "Maybe I can prune some stuff.",
            "Why would I ever want to do this?",
            "I'd want to do this because then when the grammar splits in two at the next stage, now I can maybe avoid doing any kind of inference about these elements in the dynamic programming chart, and maybe I can even print out some more stuff at this level.",
            "'cause remember, this grammar is better.",
            "OK, So what I mean by pruning is kind of looking at model posteriors and Xing off things where the posteriors are too small, so then I might split in four and remember almost all of my charts already gone from previous grammars, and any new distinctions that can be made at this level that weren't available at previous levels kind of now can be pruned, and this basically transforms these grammars that I've been talking about, which take weeks to parse with to parse a large corpus.",
            "With we can parse the same corpus in 15 minutes.",
            "By doing this kind of course define analysis, it's a huge speedup.",
            "And in particular, every single layer of analysis helps, so every kind of going every grammar rather than skipping some."
        ],
        [
            "Is very important.",
            "Um?",
            "So here's kind of a visualization of what happens, so here's kind of all of the possible nodes that could be in the tree representatives.",
            "Kind of this dynamic programming triangle here.",
            "So here are the posteriors in the coarsest model.",
            "So the courses model isn't very sure, but there are certain areas that we can be pretty confident.",
            "Have no kind of are not in the correct tree.",
            "We can prune them out right and then we can look at the posteriors after the next grammar, which is even sparser.",
            "We prune again.",
            "We look at a better grammar.",
            "It's able to make even more distinctions.",
            "We prune again, and as we do this kind of we can eliminate about half half or more of the work at each level.",
            "OK, so it's kind of keeps going until all you're left with is the small number of ambiguities that are really there in the sentence, and it turns out in this sense there really are three or four parses that are very hard to tell.",
            "The difference and only the finer grammar is going to make a decision about those things."
        ],
        [
            "OK, so this kind of course define inference is really useful and a lot of people are already using this, But I encourage you to think in your problems whether this can be helpful.",
            "I want to say one more thing about this kind of course.",
            "Define inference where you have this sequence of models, each one finer but more expensive, and you kind of start with the course grammar and kind of zoom on in.",
            "The first is that remember we had the sequence of grammars and they kind of all relate to each other, but they kind of don't write.",
            "Each one is trained by me.",
            "It's kind of based on the previous grammar, but they can drift around a little bit and what we're interested in is parsing in the grammar at the bottom.",
            "The biggest grammar that we finally get, and this actually goes to a point that was raised in the question period from Mike Stock, and that is, is it possible to learn these kinds of these kinds of pruning grammars?",
            "And the answer is it's possible and it's necessary, and it's really useful.",
            "So what we can do is we can say actually I don't care about the actual ontogeny of this grammar.",
            "I don't care about what it was like when it was a kid.",
            "What I care about is approximating the final state of the grammar.",
            "And if what I care about is this, I can take this final grammar and project it back up to kind of simpler grammars.",
            "And they're not going to be exactly the same as what was learned in the 1st place.",
            "These are going to be the grammars, not just trained at an earlier stage, but rather.",
            "The grammars which are optimized in kind of KL projection to modeling the the kind of the final grammar, and if you do this you get different grammars.",
            "You get much better pruning and you get huge speedups from actually fitting these things rather than just taking any grammars you have lying around, calling them course models and hoping for the best."
        ],
        [
            "OK, so here's some final results of this split merge stuff.",
            "So what I'm comparing to is a vanishing fraction of the parsing literature.",
            "Here I'm comparing to kind of somehow the relevant relevant competitors here of kind of these generative dynamic programming models, and this approach works quite well, and it works well cross linguistically.",
            "It also works well on an axis that you might look at this access and think the main thing that's changing here is the languages are all very different, and that's true.",
            "So one of the things that's nice about this approach is is kind of.",
            "You learn one thing for German, and you learn another thing for Chinese and you learn another thing for English because the relevant syntactic patterns that need to be modeled are different in these languages.",
            "But another thing that's different is the size of these corpora and the complexity and the style of annotation and kind of in all of these cases.",
            "This approach seems to work as well or better than other approaches that have been kind of much more manually intensive.",
            "Now.",
            "I mean, you can improve on these numbers by kind of taking the results and ranking them in various kind of less low."
        ],
        [
            "Always.",
            "Get one thing I should say, especially to this audience is you may have been a little bit worried about this whole split and merge thing where I'm kind of introducing structure and throwing away based on some kind of weird likelihood test and the beige and and you may feel like there's something nonparametric going on here.",
            "You can reformulate all of this as as kind of kind of hierarchical Dirichlet process based generalization of a PC FG, and in that case we can kind of try to do some automatic more beige and control of the structure.",
            "It turns out you can do this.",
            "You can formulate it through and it does work, but you largely get similar results."
        ],
        [
            "OK. One other thing I'd like to say is the split merge stuff has really worked great for us.",
            "Basically, there have been a lot of cases where there's some hidden structure.",
            "We only see the coarsest trace of it, but we know there's some kind of incredibly complicated pattern, and we're hoping that the model will just learn it and it really does, and so one other case for this work very well is in a sequence case looking at modeling phones and kind of basic phonetic phonology patterns.",
            "Here, where this is the task, it's a speech recognition task, is kind of the baby.",
            "Speech recognition tasks.",
            "At recognizing at doing, doing phone recognition and kind of a kind of a good baseline.",
            "Hmm, the kind of the standard approach circa maybe I don't know.",
            "Five or ten years ago, maybe more like 10 years ago gives you about a 25% error rate.",
            "And doing the same split and merge thing for five rounds improves on that substantially.",
            "Now what's going on?",
            "What's going on is your starting off saying, here's a sequence of phonemes.",
            "I don't know what's going on beyond that, but I suspect there's some kind of tricky phone, a logical pattern, and there is.",
            "There's a bunch of kind of phone, a logical classes where kind of some sounds are similar and they act the same contextually, like all kind of nasals may influence the next sound in the same way, and Moreover those kind of duration patterns were in certain contexts certain kind of sounds get longer or shorter, and all of this is kind of being learned automatically.",
            "By the model.",
            "So in this particular case I'm showing the kind of the simplest structure here for one phone, and what you what the model is learned here is that in this particular case kind of stops pattern in one way on one side and various kinds of Sonoran pattern.",
            "And on the other side.",
            "And it's kind of learning these clusters which we could have imposed based on our knowledge of phonology.",
            "But we didn't have to.",
            "This is another case of kind of linguistic structure that in some cases verifies what we suspected.",
            "Another case is kind of improves on it.",
            "Um?",
            "So this."
        ],
        [
            "Works here too.",
            "So in summary, so far I've been talking about this problem of latent variable grammar, refinement to take in kind of much more.",
            "Generally this is.",
            "I see I see a course trace of a complex process and I want to learn about that process in an unsupervised fashion and to do this with grammars.",
            "You get kind of very, very good and interpretable grammar splits.",
            "This gives rise to state of the art parsing.",
            "I mean, this works as well as kind of any other class of techniques for parsing, and in my opinion, is substantially simpler.",
            "And Moreover, because you've got this ontology of ontogeny of grammars, you can kind of use that again at inference time, so you kind of get your inference algorithm along with your learning algorithm for free.",
            "I suspect there's a lot of applications beyond parsing and the other linguistic cases we've looked at, so I encourage you to think about it.",
            "And if you can think of any, if you can find a use, great.",
            "If you can tell me of problems that you think have this character, I'd love to hear about.",
            "OK."
        ],
        [
            "Alright.",
            "So back to our outline.",
            "The next thing I'm going to talk about is a very different kind of linguistic structure, but we're going to be using some of the same methods, so in this case, again, we're going to do some unsupervised learning, but we're going to be looking at the problem of coreference."
        ],
        [
            "What's this problem about?",
            "So here's a short discourse.",
            "The Ware group whose headquarters is in the US is a large specialized Corporation.",
            "This power plant, which will be situated in Jangsu, has a large generation capacity.",
            "So what's coreference about?",
            "There's a lot going on here.",
            "One thing that we could want to do, a discourse kind of break it up in some syntactic fashion with the parser we already talked about, that that was.",
            "Last five minutes.",
            "What I want to do in Co references.",
            "I want to basically say what are the mentions here.",
            "So what are the entities being talked about?",
            "An which of them are the same right?",
            "So each of these boxes is what I'm going to call mention, so it's some syntactic item which evokes some underlying entity and I want to know which of them are the same.",
            "Maybe inside a document and also across documents.",
            "I'm going to be talking about models that basically ignore the rest of the structure, so the discourse looks like this.",
            "It's not like the rest of the structure isn't important, but we can't model everything, so we're going to talk about models which are basically view a discourse as kind of a sequence of nominal mentions of various kinds."
        ],
        [
            "And that means we're going to have some kind of generative model, the generative model, and I'm being deliberately vague 'cause we're going to go through a lot of iterations of this fairly quickly.",
            "The generative model is going to have this kind of flavor.",
            "There's a bunch of slots in which I have to say something, and I'm going to draw a bunch of entities that I'm going to keep talking about.",
            "For some reason, discourse is seem to be about.",
            "Things OK so I have some sequence of entities that comes from some prior distribution and we'll talk about that and then I'm going to go 1 by 1 to each of these entities, figuring out in some linguistic way how to render it into a string.",
            "So these underlying entities sometimes pop up as proper names.",
            "Sometimes they pop up as pronouns according to some other data process.",
            "The time that we're also going to talk about and go through some iterations on."
        ],
        [
            "OK and inference time.",
            "What we're going to observe is the mentions, and we're going to want to infer what the entities are and the reason we're going to want that is basically so we can figure out which of them are the same, and so on.",
            "I want to repeat that this particular task is going to take as input a bunch of documents, each of which is basically kind of pre segmented into these kinds of lists of nouns.",
            "OK, but we're never going to see an example of coreference.",
            "This is going to be done entirely in an unsupervised fashion in a model based way.",
            "This includes the classical anaphora problem.",
            "Thank you.",
            "That's a good point.",
            "This includes the anaphora problem, which is what are the pronouns refer to, but it also includes that Bush is the president, and that George Bush in this document is Mr Bush in that document.",
            "So it is a generalization of the reference problem.",
            "Thank you."
        ],
        [
            "OK, so let's talk about the simplest thing that could possibly work.",
            "It will not, and then we want to fix it.",
            "OK, the simplest thing you could imagine is saying, well, I have a bunch of slots to fill for each one.",
            "I'm going to have a random variable which has whose value is among the space of entities.",
            "So I'm writing things like where group, Anwhere headquarters.",
            "But really these are probably just going to be integers or something like that.",
            "And I could imagine that I have some prior distribution over which entities get used, and again it says where group, but that's probably entity #307.",
            "Then each one of these slots independently gets transformed into a string.",
            "OK, what is this parameterized by?",
            "Well, the simplest thing that could possibly work and will not is that each entity comes along with the distribution over strings.",
            "So this is where these would be the mention parameters.",
            "These characterize what strings are appropriate for each entity.",
            "We would have to learn both of these things from data in an unsupervised."
        ],
        [
            "And in this kind of endeavour, I'm going to call this entity distribution, which kind of just the relative frequencies really of these entities.",
            "This is going to be beta going to some vector."
        ],
        [
            "Proportions and dimension parameter is.",
            "There's going to be one distribution for every entity.",
            "Which is here just a multinomial over strings.",
            "We're going to call those fee.",
            "And there's going to be a of those two were here.",
            "There are K different entities.",
            "And what this would do is at each position you choose according to beta and entity and then you look at the relevant multinomial and choose a string.",
            "OK, if we happen to have these strings.",
            "This might even work because then we know that Bush and the president are both strings for the same entity.",
            "OK, but we don't have the strings.",
            "This doesn't work very well, and in particular we don't even know the number of entities K. So this."
        ],
        [
            "Kind of the easiest answer is to just erase K right Infinity and call it a day.",
            "Early process mixture model.",
            "So that's what we're going to do.",
            "We're not going to determine the number in advance, there's going to be some inference that I'm not going to talk about here.",
            "We use a sampler.",
            "That allocates these."
        ],
        [
            "So how does this work?",
            "I'm going to be reporting numbers.",
            "There are a lot of measures of how well you're doing in this task.",
            "The measure I'm going to be reporting here is the muck F1 measure, and it turns out you get about a 54%.",
            "Now.",
            "This isn't a good number, but it may be higher than you think, and the reason is higher than you think is 'cause there are a lot of singletons in the world, and they're going to be in their own class with their own Singleton multinomial and you'll get them right, and there's a lot of times where you just save the President, the President, the President or him him him and you'll get those right too, so you won't get everything wrong.",
            "But basically you'll be doing.",
            "Quite badly, right?",
            "What kinds of things do you do wrong?",
            "Well, kind of.",
            "The biggest thing that's going wrong here is every unique string is going to turn out to be its own entity, so there's the Ware Group great, but there's also who's which is some entity, even though that's a pronoun here and which won't be Co reference to anything.",
            "And the basic thing that is missing from this model.",
            "There are many things missing from this model.",
            "The basic thing linguistically, that's missing from this model is the fact that some of these words are pronouns, and they work in a different way from all of those other strings.",
            "I'm deliberately being vague."
        ],
        [
            "Recruit here will get more refined.",
            "So what's the simplest thing we could do?",
            "We could look at our mention model and say, well, that seems kind of naive, but what's the simplest thing?"
        ],
        [
            "We could do to fix it.",
            "Well, I could say well, non pronouns work this way.",
            "Non pronouns you pick an entity and choose a string for that entity according to its magic string distribution for pronouns somehow.",
            "What's going on here with a pronoun is that entities, in addition to having their magic string distributions, have a number, singular or plural, which by the way, we will never observe.",
            "A gender masculine feminine or newer in English which we will never observe and.",
            "An entity, type, person, location, organization, miscellaneous.",
            "This kind of thing which we will not observe even though in principle we have systems that could label this.",
            "In this task we will in this particular model these experiments we do not observe these things.",
            "OK, so we have to learn these.",
            "And then once we have these, somehow you when you're a pronoun, you select your actual string from these properties.",
            "This is basically what's going on is still quite crude, but now it's kind of more or less recognized that pronouns exist."
        ],
        [
            "Which is important for coreference.",
            "OK, we now have more parameters than we used to because we've got information like, hey, what pronouns do you use for a single male person?",
            "And it turns out he and him are the typical suspects.",
            "OK, but we have to learn this, 'cause we're not going to tell the system these things and these parameters which we're going to learn.",
            "There will be some parameters that are going to be global."
        ],
        [
            "No, have some value, so we have the pronoun way of Jenner."
        ],
        [
            "Strings the non pronoun way.",
            "We need some kind of gate to switch between them, and this will be the mention type.",
            "This will be the one thing other than the words that we observe.",
            "This is the only thing other than the words of this model observes is we know whether or not something is a pronoun, and that's easy because the list of pronouns are small and finite.",
            "OK, so there's going to be this gating function that determines which route you use and if you go the pronoun route, you are generated in accordance with these properties that we don't know, so we're going to have to learn that that entities may land."
        ],
        [
            "Ones plural and so on.",
            "OK, but somehow even though these are latent variables and we're going to marginalized them right out, it's still going to help help us do a better job.",
            "So here's our old model at each kind of.",
            "At each position, we're choosing an entity, and then each of those entities kind of gave rise directly to a string."
        ],
        [
            "From its multinomial distribution.",
            "Now we've got something more complicated here that says, well, maybe you're a pronoun and something special happens."
        ],
        [
            "OK. How's this work?",
            "Well, it works substantially better, but not well.",
            "And adding this program model helps.",
            "What do you get?",
            "Well, in retrospect is kind of unsurprising what you get here is the Weir group example who's is now reference to the Weir Group.",
            "That's good, which is also referenced to the Weir Group.",
            "OK, so the basic problem here is which and the basic issue is that it's referenced to the biggest entity, not the most recent entities.",
            "So the biggest thing we failed to model is that there's something about recency or salience that governs anaphora, and we don't have anything like that.",
            "We've got an exchangeable model.",
            "They just they can't even tell what precedes what.",
            "OK, so we're going to have to do something to sacrifice the exchangeability of our model and."
        ],
        [
            "Give rise to some kind of sequence and here is the simplest thing we could think of.",
            "OK, you may be able to think of something simpler and I'm sure you can think of something more complex.",
            "So the simplest thing is to say there's some new variable LL is going to represent in a very crude way, kind of what entities are alive in the discourse.",
            "Kind of you can think you can think, for example, in the user's head, but kind of doesn't necessarily represent anything.",
            "And so, for example, it will be a list of the entities that have been invoked and their activations, whatever that means.",
            "It's just some number here, OK?",
            "If we have some list L and we've got some particular entities, you which is again just going to be some integer number of an entity, we can kind of put them together and I can tell you that Z has this kind of salience in that list, and the values are it's going to be the most salient that stop.",
            "It could be none, which means it's not.",
            "It hasn't been mentioned yet, or it can be somewhere in between.",
            "So this bucket S which kind of tells you that entity, how active is it in the discourse here?",
            "How salient is it is going to be used for one thing.",
            "Only this will help govern this.",
            "This will kind of be conditioned on when choosing that observed variable of whether or not it's a pronoun.",
            "That's all that this is going to be used for."
        ],
        [
            "And it's very simple.",
            "So how is this going to work?",
            "Well, you start off with kind of some some list where nothing has been mentioned.",
            "Everything's activation 0 maybe for the first slot, my prior chooses that it wants to talk about entity one now.",
            "OK, well that has salience value none and maybe the mention that's chosen in a good setting of the parameters that hopefully we will learn is that now we're going to introduce this with a proper noun, 'cause we've never mentioned this before.",
            "OK, so there's a new L which evolves deterministically.",
            "So the thing we just mentioned is more active than it was before.",
            "Maybe now I'm going to talk about entity to its salience value is again none, so I might use a proper mention again.",
            "KL evolves again.",
            "The first entity decays and the second entity is evoked here in the previous step.",
            "Maybe I decide to talk about entity two again, and in this case it would have the value top.",
            "It's the most salient entity and maybe now choose to use a pronoun.",
            "I'm telling the."
        ],
        [
            "Best case story here.",
            "So what does this do?",
            "This now adds this kind of thin bit of structure that makes this model non exchangeable.",
            "But now let's us say something about recency.",
            "OK, there's some pressure now, or there's some capacity for this model to learn about pronouns and recency.",
            "It could learn the opposite, right?",
            "It could learn that pronouns like to refer to the least salient thing, though of course."
        ],
        [
            "It doesn't.",
            "OK, so we can kind of peek inside and look at kind of the parameters that are learned at some at some phase and you can see that it's kind of learned what you'd expect if you had to hand code something, which is that when you're introducing something new, it's typically a proper noun mention, whereas the most salient thing is typically referred to as a pronoun and then kind of in between you get various in between proportions.",
            "OK, basic."
        ],
        [
            "Learning the right kinds of patterns.",
            "This gives you another pretty big boost.",
            "Now you're up to 71.5, adding the salience, and in this particular example, all of the problems are gone.",
            "So this particular example is now right, though of course you're making tons of errors still in various places.",
            "OK, I'd like to point out that in general these are crude models like.",
            "I mean, I could give you a story about very, very complicated rules of anaphora, right?",
            "I know about these rules and I could pull them all in, but the question here is, what is the simplest thing that could work and kind of trying to model as little as possible to learn?"
        ],
        [
            "As much as possible, OK?",
            "So the one thing that's missing here is that this model is just completely incapable of associating entities across documents as I've described it so far.",
            "Each document has its own special entities that don't occur anywhere else.",
            "The obvious thing to do is to add 1 layer of a hierarchy, and I'm sure you can see where this is going.",
            "I would like to have some model where some of the entities occur in many documents and then documents of course can have their own weird entities that are unique to them, and there's some global set that each."
        ],
        [
            "Documents is drawing from.",
            "I can do this in kind of."
        ],
        [
            "Wait forward way by taking what we had before sticking.",
            "Then this kind of always used to live inside a document plate that I wasn't showing.",
            "So there's all this truck."
        ],
        [
            "Inside a document I can say, well, actually there's a global set of these entities.",
            "There's global entity proportions, beta, beta, not.",
            "There's a global parameters which include both their magic string distributions and also things like male and female, and so on, and then each document simply subselects, and this is now a hierarchical Dirichlet.",
            "OK."
        ],
        [
            "This turns out to actually help, even though the whole point of this is to now be able to answer questions about cross Document Reference, which isn't reflected in the score.",
            "This actually helps even in this within Document reference score, and you might wonder why we wondered why."
        ],
        [
            "The reason why this helps is because there are a lot of cases like where you have documents like the one on the bottom where you have rice and then Bush and then sheet.",
            "And if you're looking from a single document perspective, she is going to get linked up to Bush because it's the closest thing.",
            "And if that's all you know that's all you're going to do.",
            "But if you have the first document you already have good evidence.",
            "That's pretty unambiguous that Bush is in fact got the other, got the other value for the gender parameter, and now you can have a much better shot at getting the second document right.",
            "So you can kind of share information across documents that turns out to be useful.",
            "'cause newswire really does talk about the same people over."
        ],
        [
            "Turned over.",
            "OK, here's some scores just to kind of give you a sense of how this is going in an absolute sense.",
            "What I have here is the closest fully unsupervised system.",
            "Cardion Wagstaff, 99, that I know about, and this system is substantially better than that, right.",
            "It works substantially better, and in fact, it's on par with a lot of somewhat recent supervised systems.",
            "Now, of course, there are supervised systems that do better.",
            "I would be shocked if there were not, but many of them that have similar features really don't do very much better.",
            "And the advantages here is you can kind of look at more data.",
            "You can look at languages for which we don't have the supervision, which is actually one of the more expensive kinds.",
            "The note here about supervised plus plus this 81 is the best number I could find on this measure on this data set.",
            "They take as they have gold access to various kinds of information like the named entity type.",
            "That is a huge clue that they get to observe the truth.",
            "Whereas we just model it as a latent variable.",
            "So that's a huge clue, and so it's not entirely comperable, but I want to give you the sense that the system, though fully unsupervised, is kind of in the pack with a lot of."
        ],
        [
            "A lot of supervised systems.",
            "OK, so in summary, what happened here OK?",
            "Hear what I'm talking about is another thing that's working fairly well for us in terms of unsupervised learning, and that is starting with some very, very simple model and slowly adding kind of the minimal random variable that's necessary to fix the big problems, and this seems to work a lot better than just writing down every variable we could think of.",
            "Kind of Opry, Ori and then trying to fit the whole mess.",
            "I see this as another kind of incremental learning or what you're doing is adding complexity.",
            "Kind of 1 modeled phenomenon at a time and seeing whether or not you're still able to learn.",
            "OK, so this is a model of pronoun structure.",
            "There was this kind of sequential model because that was very important.",
            "We added this hierarchy because that was necessary for cross document.",
            "You could imagine adding more things and I encourage anybody who's interested in this to do so.",
            "I am definitely not claiming that this is somehow kind of maximal modeling here, but just that from some surprises from a surprisingly simple system.",
            "I was extremely surprised at how well this does at this task in a fully unsupervised way."
        ],
        [
            "OK, there's one more thing I'm going to talk about today.",
            "And I'm going to talk about very briefly.",
            "This is the problem of unsupervised translation mind."
        ],
        [
            "What's this about so many of you may know something about the standard machine translation approach.",
            "In this approach, you've got a bunch of documents that contain more or less literal translation sentence by sentence.",
            "So you have examples of exact kind of literal translations, and when you have this, this is great.",
            "Obviously, you can leverage this, even though there's."
        ],
        [
            "Complicated things that mediate OK.",
            "I want to talk very briefly about the problem of getting translations from mono text.",
            "So in this case you've got a bunch of source documents.",
            "A bunch of target documents and you don't have any examples of translations yet somehow we'd still."
        ],
        [
            "Like to figure out what words are translations.",
            "The task here is lexecon induction, so there's going to be a bunch of source words.",
            "There's going to be a bunch of target words, and there's going to be some matching between them, and this we're going to look at matches that are at most one to one, and somehow we'd like to learn these in an unsupervised fashion, even though we don't see exam."
        ],
        [
            "Of them.",
            "OK, how would I do this?",
            "Here's kind of the simplest data representation that could work for every word I look at the source text.",
            "I write down the kind of the letters in that word, so I write down various features that talk about its orthography.",
            "So the various engrams I also write down what words this kind of what other words this occurs with in that language."
        ],
        [
            "So all of these features are monolingual.",
            "And I do that for the other language too.",
            "All of these features are now target lingual."
        ],
        [
            "So now I've got these pairs and The thing is really going to make all this work.",
            "Is Canonical correlation analysis, which I'm sure is familiar to all of you.",
            "You may or may not be aware of the probabilistic formulation that I'm sure most of you are there too, but I want to quickly illustrate why I'm talking about CCA here.",
            "If I just had a bunch of vectors like this in a source space, right, I might use PCA to describe them to kind of represent them in some way.",
            "If I had a bunch of word features in the target space, I might use."
        ],
        [
            "PCA to describe them to the problem would be if they actually had pairs.",
            "If they came in pairs like they do in translation, the projections onto these subspaces would be kind of arbitrarily related."
        ],
        [
            "Unrelated OK, what I'd like to do is I'd like to somehow find representations for the data that correlate stem, and that's what CC does, as I'm sure most of you know.",
            "Here what I now get is I get these subspaces where."
        ],
        [
            "These things are correlated, and in particular in the probabilistic formulation of Bach and Jordan.",
            "What I can do is talk about there being a single language independent latent space that gives rise to these language specific."
        ],
        [
            "Concepts.",
            "OK, so this basically means that those latent space that I can wiggle around concepts and out pop source and target."
        ],
        [
            "Translations OK, so I need some model.",
            "The model is very simple.",
            "I generate source a bunch of slots for source words, I choose a number of target words and I choose a matching.",
            "Now I've got some pairs and some."
        ],
        [
            "Paired words for each of the paired words.",
            "I'm going to take a point in this Canonical space, which is language independent.",
            "OK, I'm going to choose that according to some simple distribution I'm going to project it to the source and target space with some transformation vector that of course I have to learn plus noise.",
            "Same on the target side transform."
        ],
        [
            "Plus noise.",
            "And that's going to generate all of my parent items.",
            "Separately, I'm going to have some noise model in the source that generates source items, and I'm going to have some noise model on the target."
        ],
        [
            "Generates target items.",
            "OK, how would I learn this?",
            "Well M is my friend so I might want to obtain some posterior matchings that turns out to be bad, 'cause that's intractable.",
            "And then I might want to kind of maximize my parameters, but kind of even if I had those posteriors, I wouldn't know how to find those projections."
        ],
        [
            "So it turns out to be bad, but it turns out in this case Hardy M is my even better friend where finding the best matching is easy with fixed parameters.",
            "I just look at some kind of joint likelihood with some with the noise models factored in some way.",
            "And if I had a fixed matching then I would have paired data, even if it's wrongly paired, and then I can find these parameters of the."
        ],
        [
            "Action using CCA.",
            "OK, So what do I do?",
            "Quick experiment and then that will be the end here is I take the 2000 most frequent nouns.",
            "I take some text from Wikipedia so they're not translations, but they do contain some of the same stuff.",
            "And I might or might not take 100 translation pairs.",
            "It turns out you don't need this, but these experiments have them, and the evaluation is going to be some kind of precision at some recall.",
            "The reason why use a recall low recall like 33% is 'cause."
        ],
        [
            "As good as the bass lines ever do.",
            "So if I use edit distance on English Spanish, which is a particularly easy case, I get 61% because there are a lot of very similar words, especially technical words."
        ],
        [
            "OK, If however I simply use features of the orthography, this kind of alternating learning with M and CCA and matchings gets me to 80%."
        ],
        [
            "So does looking at the context without any information about the spelling of the words at all.",
            "This is pure."
        ],
        [
            "Context.",
            "And if you put them together, they stack.",
            "OK, so this is a huge improvement over previous work which was not able to beat the added distance baseline."
        ],
        [
            "OK, here's kind of precision and recall of edit distance.",
            "It drops off very quickly."
        ],
        [
            "As you get out of the space of cognates, precision stays much higher for much longer with this match."
        ],
        [
            "CCA model you might say that's not fair.",
            "You started with some seed lexicons.",
            "Well, what happens if we just take the lowest edit distance words as the seed?",
            "Turns out that works almost as well this thing."
        ],
        [
            "Somewhat robust in that case.",
            "And the last things I want to show you are some examples of what's learned 'cause.",
            "I think they're really neat.",
            "So these are the most confident non identical matches.",
            "Of course the most confident ones are the ones that occur in the same context, and they're also identical string.",
            "Identical.",
            "Here are some cases that are.",
            "There are, of course, they're cognates.",
            "Right for me?"
        ],
        [
            "Spanish, of course those are going to be at the top but you learn non Cognates 2.",
            "Here are the top non cognates and you can see some of these are kind of semi cognates.",
            "They aren't cognates, but you probably have an easy time learning them."
        ],
        [
            "Anne.",
            "Here are some mistakes that are interesting, so the system can't tell the difference between action and reaction, but neither could I guess Newton.",
            "And there's a couple other confusions here that are reasonable."
        ],
        [
            "OK, this works for French and you say OK, well, that's not too surprise."
        ],
        [
            "Well, it works for Chinese.",
            "Doesn't work as well, but that's not surprising either because."
        ],
        [
            "You don't have as many queues.",
            "OK. What's being learned well here is looking at features on one side and seeing what CCA pairs them with.",
            "People have questions.",
            "I can say what that means on the other side, so you learn something that edit distance.",
            "Can't you learn regular morphological pairing?",
            "So kind of some suffixes in English, matches some suffixes and Spanish regularly, but not string identically, and you learn those, and that's why you can do better than edit distance without context.",
            "And if you look at context, you learn that kind of similar semantic fields.",
            "So Democrat looks the same as socialist.",
            "OK, well the system is a little bit confused."
        ],
        [
            "Politically, but what can you do?",
            "OK, so in summary, here's a model that learns bilingual lexicons from monolingual text.",
            "It works much better than baselines and much better than kind of previous work in this."
        ],
        [
            "Area.",
            "OK, so this is my conclusion.",
            "I've talked through more kind of an argument by example and what I'm basically trying to argue here.",
            "There's three cases of unsupervised learning.",
            "We're kind of we've got nontrivial, interesting linguistic structure for some NLP problems that we really care about.",
            "We've done this by kind of incremental learning, being very careful about what we do and don't model and what we do and don't learn at any given time.",
            "In a lot of these cases, these unsupervised methods or unsupervised refinement methods work quite well.",
            "In the case of parsing, actually better than a lot of kind of more manual techniques.",
            "There's a lot left to do, so I don't want to give the impression that kind of we're ready to kind of pipe data at the system and suddenly all of language will be learned.",
            "But I'm very excited about what unsupervised systems have been able to do recently, and I hope this will help get get this this audience excited too about unsupervised systems, both as an interesting problem to think about, and also."
        ],
        [
            "So for other domains, so that's it, thank you.",
            "Questions.",
            "Yes.",
            "He did not make reference to quite a lot of work which falls under unsupervised learning like.",
            "Leaving semantic analysis work for the solving general problems and many others right?",
            "That's totally fair.",
            "I should say that in all of this is not unique to that.",
            "And all of these cases I talked about only a small fraction of the most related work, and I neither attempted.",
            "That that is kind of as far as I'm concerned.",
            "Truth in advertising what I said on the kind of direct comparison side.",
            "I did not attempt to compare kind of all the relevant methods.",
            "I did attempt to compare on kind of this data in these conditions.",
            "The things people have tried that have worked the best of those methods an the representative there I chose was the Cardium Wagstaff paper if there if there's work from that field that does better than theirs that I should be comparing to.",
            "I would love to know about it, but everything I know about that's fully unsupervised.",
            "That I could find data did not work as well.",
            "That's not, it does not mean that I do not think that the ideas there are interesting and in a kind of Fuller treatment of that space should not be discussed.",
            "Nor do I think that my system somehow captures everything that those kinds of methods can.",
            "It absolutely doesn't.",
            "In particular, I'm not leveraging anywhere near as much text, though.",
            "That's more limitation of the experiments in the model.",
            "Sure.",
            "He said that Hardy M was working better than than soft again.",
            "In that I know how to run it.",
            "Not that I ran both and hard work better.",
            "I guess the question is, do you think there's have you experiment with hard?",
            "Again?",
            "The earlier problems think that that if there's any point in that experiment.",
            "Oh, if I could do softeam I absolutely would.",
            "I find that kind of every time we've done the comparison, soft DM is more stable and gives better answers.",
            "The end result.",
            "And then use the label data that's out there.",
            "With the performance right, so this is a very kind of.",
            "It's in the space of.",
            "How do you look at?",
            "Kind of the semi supervised world where you've got a lot of data and you want to do something and maybe we know all this about unsupervised learning.",
            "We've got some labels to.",
            "How do we combine them best?",
            "I have no doubt that with lots of data and some labels you can do better than you can without those labels, especially if you're going to get judged on whatever task is represented by those labels.",
            "So a lot of times with unsupervised system, the frustration is that it's something basically reasonable, but it didn't read your mind about the details of the task.",
            "An no system is going to do that, so a lot of this kind of being very careful about how what model you choose to model and not does end up kind of interacting with what annotations you're trying to reproduce, which can be a little frustrating.",
            "I'm wondering.",
            "Radical.",
            "Models which were you just kind of making small perturbations or perturbations in order to improve the performance, right?",
            "So you're doing this manually, and I'm wondering.",
            "You could imagine nothing automatically.",
            "Looking at her.",
            "Right, so there's there's kind of this.",
            "You can imagine something where I have just enough labeled data that I can tell when I've made a good improvement to my model, which is actually usually the case, and then I could try to do some automatic search over the space of models.",
            "I could absolutely imagine that, and I think that would be very interesting, I mean.",
            "Like everything else, then there's the question of you know can you automate it as well as what we do, and then immediately the question is maybe you can automate it better.",
            "I think it's interesting.",
            "There's a question in the back.",
            "I don't know how many we can take one more.",
            "Yeah.",
            "Brown stop.",
            "Simple things.",
            "Yeah, I think that's another great is the question is about training from kind of easy examples to hard examples.",
            "I think it's I think that's a great point.",
            "I think it's another case of this kind of incremental learning being the key to learning good stuff.",
            "There are a couple of cases where I think we've benefited from that.",
            "Though we've never explicitly sorted out, so the cases we've benefited so one line of work I didn't talk about it all is any of the kind of learning with agreement where you have multiple models and they have to agree somehow when you learn and you get kind of a multiview kind of effect.",
            "Another effect you get in the particular way we've done, we've done multiple models being forced to agree is it tends to be that kind of an simple examples they'll agree right away, and on the simple example is not necessarily short examples right in the way you're saying, but on the simple examples.",
            "You kind of you learn immediately and then as the models get better, they agree on harder and harder cases.",
            "And learning somehow proceeds out.",
            "So although I can't quantify that, I really do have a sense that in some of our other work we are seeing exactly that effect where although we haven't expressly built a system for incremental learning in that way that we're benefiting from that, and that there's a lot of potential there.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "I should warn you all.",
                    "label": 0
                },
                {
                    "sent": "I have nothing whatsoever to say about the optics of light.",
                    "label": 0
                },
                {
                    "sent": "What I am going to talk about today is some work by my group on unsupervised learning for language.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to make any cognitive claims whatsoever in this talk, but I'm going to draw one cognitive analogy briefly, so please forgive me.",
                    "label": 0
                },
                {
                    "sent": "As humans, there are two ways that we learn languages.",
                    "label": 0
                },
                {
                    "sent": "You've probably done both of these.",
                    "label": 0
                },
                {
                    "sent": "One is to learn a language in a classroom that looks something like this.",
                    "label": 0
                },
                {
                    "sent": "You get taught rules, you get shown, examples.",
                    "label": 0
                },
                {
                    "sent": "You get told how everything works and you get all the irregular verb forms listed for you and so on.",
                    "label": 0
                },
                {
                    "sent": "At some point you take a test and you're asked to do some combination of regurgitation and generalization.",
                    "label": 0
                },
                {
                    "sent": "OK, turns out for humans this doesn't work so great, but this is one of the ways we have, and this is basically what supervised NLP is like.",
                    "label": 0
                },
                {
                    "sent": "The other way we learn languages as humans is something like this.",
                    "label": 0
                },
                {
                    "sent": "And in this case, basically the data washes over you.",
                    "label": 0
                },
                {
                    "sent": "We all did this as a kid.",
                    "label": 0
                },
                {
                    "sent": "The data washes over you and you sort out for yourself what's going on underneath.",
                    "label": 0
                },
                {
                    "sent": "This is what unsupervised NLP should be like and could be like an I'm interested in building systems like the system on the right.",
                    "label": 1
                },
                {
                    "sent": "That pick apart their data and figure out for themselves what kinds of structure is going on underneath.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's my goal in this talk?",
                    "label": 0
                },
                {
                    "sent": "In this talk I'm going to give kind of an illustration by by several examples.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about inducing various kinds of linguistic structure that are not in the data.",
                    "label": 1
                },
                {
                    "sent": "So each example is going to be slightly different, but they're all going to show the property that the kind of structure I'm interested in is some kind of complex linguistic phenomenon.",
                    "label": 0
                },
                {
                    "sent": "OK, in one case, this will be syntactic parsing.",
                    "label": 0
                },
                {
                    "sent": "In one case it will be Co reference.",
                    "label": 0
                },
                {
                    "sent": "In the last will be translation.",
                    "label": 1
                },
                {
                    "sent": "These are going to be kind of rich interacting combinatorial structures.",
                    "label": 0
                },
                {
                    "sent": "And will be taking a lot of data and trying to figure out what's going on underneath that data, in a way that's not annotated in the data itself.",
                    "label": 0
                },
                {
                    "sent": "So the characteristics those are basically the characteristics of the problem, the characteristics of the solutions that I'm going to sketch.",
                    "label": 0
                },
                {
                    "sent": "They all have something in common, and the thread that's going to title together is some combination of kind of incremental or hierarchical learning, and I mean that in a very vague way, because what that means is going to vary from case to case.",
                    "label": 0
                },
                {
                    "sent": "But I find this to be a very important tying thread that you either can't or don't want to learn everything all at once.",
                    "label": 0
                },
                {
                    "sent": "This is also the solutions I'm going to sketch are going to involve kind of very careful and often iterative choice of what to model, and sometimes even more importantly, what not to model.",
                    "label": 1
                },
                {
                    "sent": "So we'll see a couple of cases where you can't model everything you don't want to model everything an one good way to make progress that's worked very well in our group is to kind of look at what your system is doing, build something simple and then kind of only introduced the variables that you absolutely.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Last model OK so from here on out I'm going to talk about several concrete examples.",
                    "label": 0
                },
                {
                    "sent": "We're going to 1st talk about the task of unsupervised grammar refinement.",
                    "label": 1
                },
                {
                    "sent": "This is related to but not exactly the same as grammar induction.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about this first, then we'll talk about coreference resolution and finally some work on translation.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cape.",
                    "label": 0
                },
                {
                    "sent": "So hopefully many of you were at Mikes invited talk earlier and he talked a little bit about syntactic analysis, syntactic parsing.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about the same, the same problem.",
                    "label": 0
                },
                {
                    "sent": "The goal here is to take sentence is an.",
                    "label": 0
                },
                {
                    "sent": "Remember, most senses are actually very complicated, so I'll show simple examples, but please don't take from this that the examples in practice are simple, so here is a basically an average length sentence from newswire.",
                    "label": 0
                },
                {
                    "sent": "Hurricane Emily held Tord Mexico's Caribbean coast on Sunday, packing 135 mile per hour winds and torrential rain.",
                    "label": 1
                },
                {
                    "sent": "And causing panic in Cancun were frightened tour squeezed into musty shelters.",
                    "label": 0
                },
                {
                    "sent": "OK, this is not.",
                    "label": 0
                },
                {
                    "sent": "He saw the cat, right?",
                    "label": 0
                },
                {
                    "sent": "If it were, he saw the cat, this would be easy.",
                    "label": 0
                },
                {
                    "sent": "This is hard because, for example, there's wins an rain and panic.",
                    "label": 0
                },
                {
                    "sent": "But it's not all the same.",
                    "label": 0
                },
                {
                    "sent": "It's not a conjunction of three things here, and you need to sort out what the various pieces of this sentence are and how they combine together.",
                    "label": 0
                },
                {
                    "sent": "And that's going to be the job of a syntactic parser.",
                    "label": 0
                },
                {
                    "sent": "Why do we do this?",
                    "label": 0
                },
                {
                    "sent": "We do this.",
                    "label": 0
                },
                {
                    "sent": "We do this because it's kind of an important intermediary step between surface.",
                    "label": 0
                },
                {
                    "sent": "Language and semantics.",
                    "label": 0
                },
                {
                    "sent": "And it's also these kinds of tree structures are very useful for lots of problems like math.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In translation.",
                    "label": 0
                },
                {
                    "sent": "OK so back to simple examples, but please if there's nothing else you remember from this about syntactic parsing, Please remember that the reality is very complicated and it's not always he was right.",
                    "label": 0
                },
                {
                    "sent": "So that's going to be the recurring example here.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you want to get a grammar and you would like to be parsing these structures well, you might look at them and say those look awful lot like context free grammars to me.",
                    "label": 1
                },
                {
                    "sent": "And then you think, well, where am I going to get a context free grammar?",
                    "label": 0
                },
                {
                    "sent": "Well, I could try to brainstorm one.",
                    "label": 0
                },
                {
                    "sent": "It turns out that doesn't work very well.",
                    "label": 1
                },
                {
                    "sent": "I could have a bunch of annotated sentences like this, where somebody has been kind enough to painstakingly mark out what the syntactic structures are like this to say.",
                    "label": 0
                },
                {
                    "sent": "Well, there's a noun phrase and it contains a preposition, and it contains a pronoun.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "OK, if we had a bunch of these and we do, these are called treebanks.",
                    "label": 0
                },
                {
                    "sent": "We could read off what the rules are from the grammar, so if I had this tree, I would read off.",
                    "label": 0
                },
                {
                    "sent": "Well, there's one instance of a route going to sentence.",
                    "label": 0
                },
                {
                    "sent": "There's one instance of a sentence going to a noun phrase, a verb phrase, followed by a period, and so on.",
                    "label": 0
                },
                {
                    "sent": "Now I'd like to emphasize that if you just take, for example, we have 40,000 parsed English sentences, and I look at just the noun phrases alone.",
                    "label": 0
                },
                {
                    "sent": "There aren't 30 or 40 kinds of noun phrase rules.",
                    "label": 0
                },
                {
                    "sent": "There are 10,000, and you find new ones as you look at new sentence is.",
                    "label": 0
                },
                {
                    "sent": "And this is because there's all kinds of phenomena interacting, punctuation and all kinds of modification, and all kinds of complexities that don't show up in these simple sentences.",
                    "label": 0
                },
                {
                    "sent": "And they all interact, kind of combinatorially.",
                    "label": 0
                },
                {
                    "sent": "OK, so but you could take this.",
                    "label": 1
                },
                {
                    "sent": "These 40,000 sentences you could read off the grammar and then take these counts and normalize them and kind of the obvious way to get a probabilistic context free grammar.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a generalization of a hidden Markov model, and if you did this and ignoring any efficiency questions about how you parse and you use this to infer most likely parses, you would have a parser that has what I'll call her baseline accuracy of 72%.",
                    "label": 0
                },
                {
                    "sent": "This F1 score is something like how many of your nodes are correct.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into the details.",
                    "label": 0
                },
                {
                    "sent": "OK, so this people have been doing this for awhile.",
                    "label": 0
                },
                {
                    "sent": "It turns out this isn't a very good number.",
                    "label": 0
                },
                {
                    "sent": "You don't really want your parser to make a miss.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1/4 of the nodes are wrong and you might ask, well, kind of our context.",
                    "label": 0
                },
                {
                    "sent": "Free grammars, probabilistic, context free grammars, somehow the wrong model class and the answer is they're not necessarily the wrong model class.",
                    "label": 0
                },
                {
                    "sent": "Just because your parser doesn't work very well, and one of the reasons is that the kind of the trees in the data that have symbols like noun phrase make overly strong conditional independence assertions when kind of viewed as probabilistic objects.",
                    "label": 1
                },
                {
                    "sent": "So for example, here's another sentence.",
                    "label": 0
                },
                {
                    "sent": "She heard the noise, and in the sentence there are the two green nodes.",
                    "label": 0
                },
                {
                    "sent": "They're both noun phrases.",
                    "label": 0
                },
                {
                    "sent": "So they're both noun phrases, linguistically, in terms of the roles they play in the sentence at some level.",
                    "label": 0
                },
                {
                    "sent": "But somehow, as a kind of a context free grammar, it's not right because we can't grammatically swap these, we can't have the noise heard, she it would be the noise heard her aside from it being semantically anomalous, it wouldn't be syntactically correct, so there's something these aren't.",
                    "label": 0
                },
                {
                    "sent": "Noun phrases aren't really the same, and if we build grammars that have such symbols, they're not going to be very good.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this brings us to the task of grammar refinement.",
                    "label": 1
                },
                {
                    "sent": "What's grammar refinement all about we're observing these trees, but they're kind of only the courses description of what's going on linguistically.",
                    "label": 0
                },
                {
                    "sent": "What's really going on is much finer grain, their subjects, their objects.",
                    "label": 0
                },
                {
                    "sent": "There's passivization all kinds of phenomena are happening, and they're all happening at once, and they're not labeled in these trees, so we'd like to have some description of what's really going on.",
                    "label": 0
                },
                {
                    "sent": "An one way to do this kind of one of the earliest ways was Mark Johnson's parent annotation, where we go along, saying, well.",
                    "label": 0
                },
                {
                    "sent": "Those MPs are different because one is a subjective ones.",
                    "label": 0
                },
                {
                    "sent": "An object we can tell that by their parents.",
                    "label": 0
                },
                {
                    "sent": "So you go around marking all the nodes with their parents.",
                    "label": 0
                },
                {
                    "sent": "Well, this is good.",
                    "label": 0
                },
                {
                    "sent": "We've got more symbols, so our context freedom assumptions are less wrong.",
                    "label": 0
                },
                {
                    "sent": "But now we also need more data to estimate anything and so you.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Getting sparsity problems.",
                    "label": 0
                },
                {
                    "sent": "Hey, what can you do?",
                    "label": 0
                },
                {
                    "sent": "You can take an even more extreme case and say actually this isn't just any noun phrase in object position, it's something about noise and I want to say this is a noun phrase that's about noise because, well, it's an object of the verb heard and somehow heard goes with noise and I can try to kind of get in a little bit of not only lexical syntax, but also some lexical semantics here as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what's called head lexicalization, where you go along annotating all of your nodes with actual words.",
                    "label": 1
                },
                {
                    "sent": "Well, this makes it even harder sparsity problems.",
                    "label": 0
                },
                {
                    "sent": "You can make good parsers in this way, but you have to work very, very hard and the whole game becomes overcoming the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sparsity problems.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to talk about here is an unsupervised approach to the refinement, so we're going to have the trees, but we're going to find them automatically and try to figure out in an automatic fashion what these true underlying syntactic processes are so.",
                    "label": 0
                },
                {
                    "sent": "In this case, we might say that there's an NP1 and NP2, but I don't know what that means and I don't know which ones are which, but I'm going to sort it out statistically.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What does this mean?",
                    "label": 0
                },
                {
                    "sent": "This means that where we used to have parses, we observe parses in our tree bank.",
                    "label": 0
                },
                {
                    "sent": "Are these processes that have sentences and noun phrases and verb phrases we observe them and at Test time we want to produce them.",
                    "label": 0
                },
                {
                    "sent": "But we know that underneath everything is some more complex phenomenon and this more complex phenomenon is going to be represented by grammars over water called derivations shown on the right.",
                    "label": 0
                },
                {
                    "sent": "So for example, here are two derivations on the right that both derive the same parse.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have these parses that'll be capital T. We have these derivations.",
                    "label": 0
                },
                {
                    "sent": "These are lower case T and what exactly?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are we trying to do well what we're trying to do here is?",
                    "label": 0
                },
                {
                    "sent": "We're trying to find some grammar over these derivation trees that best explains in some sense, and being deliberately vague here.",
                    "label": 0
                },
                {
                    "sent": "That best explains the observed parses OK. One option, one of the easiest options, and what I'll talk through here is to use maximum likelihood estimation with them.",
                    "label": 1
                },
                {
                    "sent": "I'll briefly mention some other options, including being more beige and about this and looking at doing this so you could either want to be more beige and or more discriminative, or maybe somehow both.",
                    "label": 0
                },
                {
                    "sent": "And we've looked at both of those things, but I'm going to mainly talk today about kind of the basic case of maximum likelihood in AM here.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do we want?",
                    "label": 0
                },
                {
                    "sent": "We want these derivation parameters that maximize the likelihood of the parses and you look at that and say, well, that's exactly what year was built for.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's take that a little bit further.",
                    "label": 0
                },
                {
                    "sent": "So what does M mean in this case?",
                    "label": 0
                },
                {
                    "sent": "It means we're going to see structures like what's shown here on the left, where we have sentences and noun phrases and verb phrases there going to be latent variables that represent the clusters.",
                    "label": 0
                },
                {
                    "sent": "At each of these points.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, this isn't like point clustering or mixture of Gaussians are selling.",
                    "label": 0
                },
                {
                    "sent": "These clusters are structured, they're interacting because the cluster at one point determines the probability of reaching the clusters at other points.",
                    "label": 0
                },
                {
                    "sent": "But there are bunch of latent variables in there kind of more or less on a chain.",
                    "label": 0
                },
                {
                    "sent": "Here it's a tree, but the brackets are known and so.",
                    "label": 1
                },
                {
                    "sent": "Essentially, learning here really isn't much more complicated than it is for Hmm's, and we can run a basic generalization of the forward backward that somewhere between forward, backward and the inside outside algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can use EM here to learn these sub categories.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what might we learn?",
                    "label": 0
                },
                {
                    "sent": "Well, if we did this, we'd be splitting stuff everywhere.",
                    "label": 0
                },
                {
                    "sent": "We have lots of kinds of sentences, lots of kinds of noun phrases, and lots of kinds of verb phrases.",
                    "label": 0
                },
                {
                    "sent": "In particular, we have lots of kinds of each of the parts of speech.",
                    "label": 1
                },
                {
                    "sent": "I'm going to show you lots of them because they're very easy to kind of.",
                    "label": 0
                },
                {
                    "sent": "Get your head around, but don't confuse that this is only about word clustering.",
                    "label": 0
                },
                {
                    "sent": "So here's the determiner category and the most common determiners in the tree Bank RVA and capital V cases preserved here.",
                    "label": 0
                },
                {
                    "sent": "Right, if we split that into four and learn those clusters with them, and remember we're not just learning, determine clusters were learning clusters across the whole grammar.",
                    "label": 0
                },
                {
                    "sent": "We would find in this case the algorithm produces four categories.",
                    "label": 0
                },
                {
                    "sent": "You get one which is more or less the indefinite determiners.",
                    "label": 0
                },
                {
                    "sent": "The second one, which is more or less definite, determiners, the third ones more or less demonstratives and the last ones more or less quantifiers.",
                    "label": 0
                },
                {
                    "sent": "They're not perfect, but this is basically what's what's kind of being captured by the distributional clustering.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that's great, but what if we wanted eight and in fact we're going to end up with something more like 64128 well?",
                    "label": 0
                },
                {
                    "sent": "Turns out it doesn't work very well.",
                    "label": 0
                },
                {
                    "sent": "You have kind of basically have lots of search problems and you have lots of model problems when you just try to.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually split like this, so here's something that worked very well in the speech literature and people have been doing in kind of a lot of different places, and that's don't learn all of your structure at once, learning hierarchically, so instead of just splitting it, all of my grammar everywhere, I'm going to maybe save that for every single observed symbol there's two underlying symbols.",
                    "label": 0
                },
                {
                    "sent": "We might get the proper determiners in the demonstratives.",
                    "label": 0
                },
                {
                    "sent": "Then we'll take those symbols and will further split them and we'll get this tree of incremental refinement.",
                    "label": 0
                },
                {
                    "sent": "OK again.",
                    "label": 0
                },
                {
                    "sent": "Now we're back to four, which is where we started.",
                    "label": 0
                },
                {
                    "sent": "But it turns out you can go much.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Farther without without hitting problems.",
                    "label": 0
                },
                {
                    "sent": "So if we do this, we're essentially going to get in ontogeny of grammars, so we'll have some basic underlying simple grammar that then gets split and split and split in a way that's kind of incrementally structurally clustered until finally we get is a grammar that's as refined as we want.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So remember, we're going to have this sequence of grammars, and in each kind of at each step the symbols get more and more refined, but they correspond to symbols of the level at one level up.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what does this give you well?",
                    "label": 0
                },
                {
                    "sent": "So the green line here, the black dots are.",
                    "label": 0
                },
                {
                    "sent": "Increasing rounds of this kind of flat training training with two categories for categories, eight categories and so on.",
                    "label": 0
                },
                {
                    "sent": "The Green round, the green is kind of doing this hierarchical training, and the first thing to note is were a lot higher than the baseline now, so we actually start in this game lower than the baseline because we started with an extremely simple grammar and we end up with basically reasonable results.",
                    "label": 0
                },
                {
                    "sent": "So now we're up to about 88% and just to give you a sense, kind of the range in which people report kind of state of the art parsing numbers is kind of in the.",
                    "label": 0
                },
                {
                    "sent": "High 80s to kind of very, very low 90s range.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can see that the kind of the further you split the better it is to have trained things hierarchically.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it turns out this is a problem.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because this graph doesn't keep going, it doesn't look like it's awesome to did, right.",
                    "label": 0
                },
                {
                    "sent": "You think we can keep splitting in one day?",
                    "label": 0
                },
                {
                    "sent": "Our accuracy will hit like 300%, right?",
                    "label": 0
                },
                {
                    "sent": "But it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Those are joke.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work because you run out of kind of computational resources.",
                    "label": 0
                },
                {
                    "sent": "This gets these rumors, get big, the number of rules grows.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Grows cubically in the number of States and it turns out that we're really wasting a lot splitting Everything Everywhere, and maybe this should be obvious, but took us awhile to realize this and in particular there's a part of speech called, OK.",
                    "label": 0
                },
                {
                    "sent": "The words in the part of speech, are.",
                    "label": 0
                },
                {
                    "sent": "The comma.",
                    "label": 0
                },
                {
                    "sent": "And so we split this into many, many clusters, each which contains the comma.",
                    "label": 0
                },
                {
                    "sent": "So this is clearly a waste, so we don't want to be doing this.",
                    "label": 0
                },
                {
                    "sent": "But even beyond that, what if we were just splitting determiners again?",
                    "label": 0
                },
                {
                    "sent": "And here's kind of further down the determiner split tree, and if you look over there on the left, you can see that we've got two categories that really aren't so different.",
                    "label": 0
                },
                {
                    "sent": "So are these two similar?",
                    "label": 0
                },
                {
                    "sent": "To keep this split, or do we want to kind of stop splitting at this point?",
                    "label": 0
                },
                {
                    "sent": "Well, I don't know.",
                    "label": 0
                },
                {
                    "sent": "This is why I kind of.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The computer sort this out and so I have to have some kind of test.",
                    "label": 0
                },
                {
                    "sent": "The test we used is.",
                    "label": 0
                },
                {
                    "sent": "We look at the likelihood with and without the split and if the gain in likelihood isn't enough then we reverse the split and we get these kind of ragged trees that only split where where it's useful.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now essentially we're splitting all the categories, but then we take half of half of the splits that are at least useful and we merge them back together.",
                    "label": 0
                },
                {
                    "sent": "What does this do?",
                    "label": 0
                },
                {
                    "sent": "It kind of shrinks the rate at which our grammars grow substantially and suddenly all of these points that we were achieving with very big grammars.",
                    "label": 0
                },
                {
                    "sent": "They all shift over to the left.",
                    "label": 0
                },
                {
                    "sent": "We get similar accuracies with much smaller grammars.",
                    "label": 0
                },
                {
                    "sent": "The accuracy is actually go up too, and we let us do more rounds of this without blowing up the grammar size, and now we can get a lot further.",
                    "label": 0
                },
                {
                    "sent": "So now if we do some kind of merging as well.",
                    "label": 0
                },
                {
                    "sent": "So that we're not allocating parameters everywhere, but only where they're needed, then we gain not only in size, but also in accuracy.",
                    "label": 0
                },
                {
                    "sent": "Here up to 89.5%.",
                    "label": 0
                },
                {
                    "sent": "Game.",
                    "label": 0
                },
                {
                    "sent": "And this is already starting to be a state of the art number.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I said we were allocating complexity where it's needed.",
                    "label": 0
                },
                {
                    "sent": "I'll return to this later, but for now we might kind of look and see what kinds of.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kinds of things we made complex, and it turns out noun phrases and verb phrases and prepositional phrases and other things that you've heard of are in fact very common, and they're also very complex, and we've captured that by kind of allocating.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out of parameters to them.",
                    "label": 0
                },
                {
                    "sent": "But then things that you haven't heard about an you trust me.",
                    "label": 0
                },
                {
                    "sent": "You really don't want to know about like the nada category and the X symbol with these are remained very simple, both because they aren't very frequent an because.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They're not very complex.",
                    "label": 0
                },
                {
                    "sent": "Same thing if you look at parts of speech, so nouns and adjectives.",
                    "label": 0
                },
                {
                    "sent": "These kinds of things are very complicated, they.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Split a lot, whereas our friend, the comma and the part of speech two which also only contains one word, don't get split at all because there's never any likelihood gain from it.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've kind of allocated parameters in an incremental way so that we learn reasonable things and we also stop when there's when there's kind of no more support.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For these splits.",
                    "label": 0
                },
                {
                    "sent": "What kinds of things do we learn?",
                    "label": 0
                },
                {
                    "sent": "Well, here's some candy.",
                    "label": 0
                },
                {
                    "sent": "Here are some of the proper nouns, so it's remember in the tree bank, there's just one kind of proper noun, but it turns out months are very different from people and their syntactic distribution in the system is learned that appropriately.",
                    "label": 0
                },
                {
                    "sent": "So months in particular, abbreviated months, because they happen to be used in a stylistically particular way.",
                    "label": 0
                },
                {
                    "sent": "Get one category, first names, middle names, last names.",
                    "label": 0
                },
                {
                    "sent": "I'll get their own category.",
                    "label": 0
                },
                {
                    "sent": "You can see the first word of a place in the second word of a place are syntactically different in their distribution, so they get their own categories in terms of pronouns.",
                    "label": 0
                },
                {
                    "sent": "There's only one pronoun, but it turns out that according to the statistics, there are.",
                    "label": 0
                },
                {
                    "sent": "There are three kinds of case.",
                    "label": 0
                },
                {
                    "sent": "In English, there's nominative case there's Accusatives case, and there's uppercase, and the system is learned.",
                    "label": 0
                },
                {
                    "sent": "This generalization, which never would have come kind of from a top down kind of linguistic notion where upper case is not a case.",
                    "label": 0
                },
                {
                    "sent": "That was a joke too.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's some other splits so you can see adverbs have been split.",
                    "label": 0
                },
                {
                    "sent": "I personally would have had trouble figuring out how to split the adverbs, but it turns out that there's various kinds of degree adverbs and they pattern differently.",
                    "label": 0
                },
                {
                    "sent": "Syntactically numbers.",
                    "label": 0
                },
                {
                    "sent": "There's a very big difference between numbers that mean years and numbers that mean stock prices, and so they get different categories because they kind of occur in different places in the tree.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No one.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a case where what we've learned.",
                    "label": 0
                },
                {
                    "sent": "We kind of this is an incremental learning regime where we're starting with only the coarsest observation of the course is kind of syntax, and we're starting to learn the complex phenomenon that underlies that in that kind of an incremental fashion.",
                    "label": 0
                },
                {
                    "sent": "And remember, we get this sequence of grammars, and so far I've motivated this sequence purely as being a way of kind of getting EM to do the right thing, that somehow if you try to learn everything all at once, it's not going to work.",
                    "label": 0
                },
                {
                    "sent": "And maybe this is nice because we can merge it every round.",
                    "label": 0
                },
                {
                    "sent": "If I really only wanted to tell you about unsupervised learning, I would stop here and move on to the next topic.",
                    "label": 0
                },
                {
                    "sent": "There's actually something else I want to talk about that has almost nothing to do with unsupervised learning is a small digression and that has to do with kind of course defined analysis.",
                    "label": 0
                },
                {
                    "sent": "This is essentially course define model learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're starting with the course model and we're gradually refining it, but it turns out we can do the same thing when it's time to do inference, so when it's time to actually figure out what the right parses are, remember we have this big grammar now that has all these symbols.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have one noun phrase with.",
                    "label": 0
                },
                {
                    "sent": "A gazillion rules it now has 32 or 128 or however many.",
                    "label": 0
                },
                {
                    "sent": "There are noun phrases with a gazillion rules and so the grammars are getting quite complex and to directly do inference with them would kind of scale.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very badly, but we can do something we can exploit.",
                    "label": 0
                },
                {
                    "sent": "These other grammars we have and we can do course defined analysis.",
                    "label": 0
                },
                {
                    "sent": "This is not something new, but it's something important.",
                    "label": 0
                },
                {
                    "sent": "So this goes back to kind of charniak's earlier parsers, and there's a multipath version of this, more like what I'm going to talk about here in Chiniak and Johnson and the way it basically works is well.",
                    "label": 0
                },
                {
                    "sent": "If we had a course grammar, it would only have symbols like quantifier phrase and noun phrases and verb phrase.",
                    "label": 0
                },
                {
                    "sent": "And if I was analyzing my dynamic program some span, say from word 5 toward 12.",
                    "label": 0
                },
                {
                    "sent": "I want to figure out what belonged here.",
                    "label": 0
                },
                {
                    "sent": "I could say maybe this grammar isn't very good, but it's good enough to rule out some of these things right?",
                    "label": 0
                },
                {
                    "sent": "It's so confident that some of these things aren't here.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can prune some stuff.",
                    "label": 0
                },
                {
                    "sent": "Why would I ever want to do this?",
                    "label": 0
                },
                {
                    "sent": "I'd want to do this because then when the grammar splits in two at the next stage, now I can maybe avoid doing any kind of inference about these elements in the dynamic programming chart, and maybe I can even print out some more stuff at this level.",
                    "label": 0
                },
                {
                    "sent": "'cause remember, this grammar is better.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I mean by pruning is kind of looking at model posteriors and Xing off things where the posteriors are too small, so then I might split in four and remember almost all of my charts already gone from previous grammars, and any new distinctions that can be made at this level that weren't available at previous levels kind of now can be pruned, and this basically transforms these grammars that I've been talking about, which take weeks to parse with to parse a large corpus.",
                    "label": 0
                },
                {
                    "sent": "With we can parse the same corpus in 15 minutes.",
                    "label": 0
                },
                {
                    "sent": "By doing this kind of course define analysis, it's a huge speedup.",
                    "label": 0
                },
                {
                    "sent": "And in particular, every single layer of analysis helps, so every kind of going every grammar rather than skipping some.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is very important.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So here's kind of a visualization of what happens, so here's kind of all of the possible nodes that could be in the tree representatives.",
                    "label": 0
                },
                {
                    "sent": "Kind of this dynamic programming triangle here.",
                    "label": 0
                },
                {
                    "sent": "So here are the posteriors in the coarsest model.",
                    "label": 0
                },
                {
                    "sent": "So the courses model isn't very sure, but there are certain areas that we can be pretty confident.",
                    "label": 0
                },
                {
                    "sent": "Have no kind of are not in the correct tree.",
                    "label": 0
                },
                {
                    "sent": "We can prune them out right and then we can look at the posteriors after the next grammar, which is even sparser.",
                    "label": 0
                },
                {
                    "sent": "We prune again.",
                    "label": 0
                },
                {
                    "sent": "We look at a better grammar.",
                    "label": 0
                },
                {
                    "sent": "It's able to make even more distinctions.",
                    "label": 0
                },
                {
                    "sent": "We prune again, and as we do this kind of we can eliminate about half half or more of the work at each level.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's kind of keeps going until all you're left with is the small number of ambiguities that are really there in the sentence, and it turns out in this sense there really are three or four parses that are very hard to tell.",
                    "label": 0
                },
                {
                    "sent": "The difference and only the finer grammar is going to make a decision about those things.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this kind of course define inference is really useful and a lot of people are already using this, But I encourage you to think in your problems whether this can be helpful.",
                    "label": 0
                },
                {
                    "sent": "I want to say one more thing about this kind of course.",
                    "label": 0
                },
                {
                    "sent": "Define inference where you have this sequence of models, each one finer but more expensive, and you kind of start with the course grammar and kind of zoom on in.",
                    "label": 0
                },
                {
                    "sent": "The first is that remember we had the sequence of grammars and they kind of all relate to each other, but they kind of don't write.",
                    "label": 0
                },
                {
                    "sent": "Each one is trained by me.",
                    "label": 0
                },
                {
                    "sent": "It's kind of based on the previous grammar, but they can drift around a little bit and what we're interested in is parsing in the grammar at the bottom.",
                    "label": 0
                },
                {
                    "sent": "The biggest grammar that we finally get, and this actually goes to a point that was raised in the question period from Mike Stock, and that is, is it possible to learn these kinds of these kinds of pruning grammars?",
                    "label": 0
                },
                {
                    "sent": "And the answer is it's possible and it's necessary, and it's really useful.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can say actually I don't care about the actual ontogeny of this grammar.",
                    "label": 0
                },
                {
                    "sent": "I don't care about what it was like when it was a kid.",
                    "label": 0
                },
                {
                    "sent": "What I care about is approximating the final state of the grammar.",
                    "label": 0
                },
                {
                    "sent": "And if what I care about is this, I can take this final grammar and project it back up to kind of simpler grammars.",
                    "label": 0
                },
                {
                    "sent": "And they're not going to be exactly the same as what was learned in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "These are going to be the grammars, not just trained at an earlier stage, but rather.",
                    "label": 0
                },
                {
                    "sent": "The grammars which are optimized in kind of KL projection to modeling the the kind of the final grammar, and if you do this you get different grammars.",
                    "label": 0
                },
                {
                    "sent": "You get much better pruning and you get huge speedups from actually fitting these things rather than just taking any grammars you have lying around, calling them course models and hoping for the best.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's some final results of this split merge stuff.",
                    "label": 1
                },
                {
                    "sent": "So what I'm comparing to is a vanishing fraction of the parsing literature.",
                    "label": 0
                },
                {
                    "sent": "Here I'm comparing to kind of somehow the relevant relevant competitors here of kind of these generative dynamic programming models, and this approach works quite well, and it works well cross linguistically.",
                    "label": 0
                },
                {
                    "sent": "It also works well on an axis that you might look at this access and think the main thing that's changing here is the languages are all very different, and that's true.",
                    "label": 0
                },
                {
                    "sent": "So one of the things that's nice about this approach is is kind of.",
                    "label": 0
                },
                {
                    "sent": "You learn one thing for German, and you learn another thing for Chinese and you learn another thing for English because the relevant syntactic patterns that need to be modeled are different in these languages.",
                    "label": 0
                },
                {
                    "sent": "But another thing that's different is the size of these corpora and the complexity and the style of annotation and kind of in all of these cases.",
                    "label": 0
                },
                {
                    "sent": "This approach seems to work as well or better than other approaches that have been kind of much more manually intensive.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can improve on these numbers by kind of taking the results and ranking them in various kind of less low.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Always.",
                    "label": 0
                },
                {
                    "sent": "Get one thing I should say, especially to this audience is you may have been a little bit worried about this whole split and merge thing where I'm kind of introducing structure and throwing away based on some kind of weird likelihood test and the beige and and you may feel like there's something nonparametric going on here.",
                    "label": 0
                },
                {
                    "sent": "You can reformulate all of this as as kind of kind of hierarchical Dirichlet process based generalization of a PC FG, and in that case we can kind of try to do some automatic more beige and control of the structure.",
                    "label": 0
                },
                {
                    "sent": "It turns out you can do this.",
                    "label": 0
                },
                {
                    "sent": "You can formulate it through and it does work, but you largely get similar results.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. One other thing I'd like to say is the split merge stuff has really worked great for us.",
                    "label": 0
                },
                {
                    "sent": "Basically, there have been a lot of cases where there's some hidden structure.",
                    "label": 0
                },
                {
                    "sent": "We only see the coarsest trace of it, but we know there's some kind of incredibly complicated pattern, and we're hoping that the model will just learn it and it really does, and so one other case for this work very well is in a sequence case looking at modeling phones and kind of basic phonetic phonology patterns.",
                    "label": 0
                },
                {
                    "sent": "Here, where this is the task, it's a speech recognition task, is kind of the baby.",
                    "label": 0
                },
                {
                    "sent": "Speech recognition tasks.",
                    "label": 0
                },
                {
                    "sent": "At recognizing at doing, doing phone recognition and kind of a kind of a good baseline.",
                    "label": 0
                },
                {
                    "sent": "Hmm, the kind of the standard approach circa maybe I don't know.",
                    "label": 0
                },
                {
                    "sent": "Five or ten years ago, maybe more like 10 years ago gives you about a 25% error rate.",
                    "label": 0
                },
                {
                    "sent": "And doing the same split and merge thing for five rounds improves on that substantially.",
                    "label": 0
                },
                {
                    "sent": "Now what's going on?",
                    "label": 0
                },
                {
                    "sent": "What's going on is your starting off saying, here's a sequence of phonemes.",
                    "label": 0
                },
                {
                    "sent": "I don't know what's going on beyond that, but I suspect there's some kind of tricky phone, a logical pattern, and there is.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of kind of phone, a logical classes where kind of some sounds are similar and they act the same contextually, like all kind of nasals may influence the next sound in the same way, and Moreover those kind of duration patterns were in certain contexts certain kind of sounds get longer or shorter, and all of this is kind of being learned automatically.",
                    "label": 0
                },
                {
                    "sent": "By the model.",
                    "label": 0
                },
                {
                    "sent": "So in this particular case I'm showing the kind of the simplest structure here for one phone, and what you what the model is learned here is that in this particular case kind of stops pattern in one way on one side and various kinds of Sonoran pattern.",
                    "label": 0
                },
                {
                    "sent": "And on the other side.",
                    "label": 0
                },
                {
                    "sent": "And it's kind of learning these clusters which we could have imposed based on our knowledge of phonology.",
                    "label": 0
                },
                {
                    "sent": "But we didn't have to.",
                    "label": 0
                },
                {
                    "sent": "This is another case of kind of linguistic structure that in some cases verifies what we suspected.",
                    "label": 0
                },
                {
                    "sent": "Another case is kind of improves on it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Works here too.",
                    "label": 0
                },
                {
                    "sent": "So in summary, so far I've been talking about this problem of latent variable grammar, refinement to take in kind of much more.",
                    "label": 0
                },
                {
                    "sent": "Generally this is.",
                    "label": 0
                },
                {
                    "sent": "I see I see a course trace of a complex process and I want to learn about that process in an unsupervised fashion and to do this with grammars.",
                    "label": 0
                },
                {
                    "sent": "You get kind of very, very good and interpretable grammar splits.",
                    "label": 1
                },
                {
                    "sent": "This gives rise to state of the art parsing.",
                    "label": 0
                },
                {
                    "sent": "I mean, this works as well as kind of any other class of techniques for parsing, and in my opinion, is substantially simpler.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, because you've got this ontology of ontogeny of grammars, you can kind of use that again at inference time, so you kind of get your inference algorithm along with your learning algorithm for free.",
                    "label": 0
                },
                {
                    "sent": "I suspect there's a lot of applications beyond parsing and the other linguistic cases we've looked at, so I encourage you to think about it.",
                    "label": 1
                },
                {
                    "sent": "And if you can think of any, if you can find a use, great.",
                    "label": 0
                },
                {
                    "sent": "If you can tell me of problems that you think have this character, I'd love to hear about.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So back to our outline.",
                    "label": 0
                },
                {
                    "sent": "The next thing I'm going to talk about is a very different kind of linguistic structure, but we're going to be using some of the same methods, so in this case, again, we're going to do some unsupervised learning, but we're going to be looking at the problem of coreference.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's this problem about?",
                    "label": 0
                },
                {
                    "sent": "So here's a short discourse.",
                    "label": 0
                },
                {
                    "sent": "The Ware group whose headquarters is in the US is a large specialized Corporation.",
                    "label": 1
                },
                {
                    "sent": "This power plant, which will be situated in Jangsu, has a large generation capacity.",
                    "label": 0
                },
                {
                    "sent": "So what's coreference about?",
                    "label": 0
                },
                {
                    "sent": "There's a lot going on here.",
                    "label": 0
                },
                {
                    "sent": "One thing that we could want to do, a discourse kind of break it up in some syntactic fashion with the parser we already talked about, that that was.",
                    "label": 0
                },
                {
                    "sent": "Last five minutes.",
                    "label": 0
                },
                {
                    "sent": "What I want to do in Co references.",
                    "label": 0
                },
                {
                    "sent": "I want to basically say what are the mentions here.",
                    "label": 0
                },
                {
                    "sent": "So what are the entities being talked about?",
                    "label": 0
                },
                {
                    "sent": "An which of them are the same right?",
                    "label": 0
                },
                {
                    "sent": "So each of these boxes is what I'm going to call mention, so it's some syntactic item which evokes some underlying entity and I want to know which of them are the same.",
                    "label": 0
                },
                {
                    "sent": "Maybe inside a document and also across documents.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be talking about models that basically ignore the rest of the structure, so the discourse looks like this.",
                    "label": 0
                },
                {
                    "sent": "It's not like the rest of the structure isn't important, but we can't model everything, so we're going to talk about models which are basically view a discourse as kind of a sequence of nominal mentions of various kinds.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that means we're going to have some kind of generative model, the generative model, and I'm being deliberately vague 'cause we're going to go through a lot of iterations of this fairly quickly.",
                    "label": 0
                },
                {
                    "sent": "The generative model is going to have this kind of flavor.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of slots in which I have to say something, and I'm going to draw a bunch of entities that I'm going to keep talking about.",
                    "label": 0
                },
                {
                    "sent": "For some reason, discourse is seem to be about.",
                    "label": 0
                },
                {
                    "sent": "Things OK so I have some sequence of entities that comes from some prior distribution and we'll talk about that and then I'm going to go 1 by 1 to each of these entities, figuring out in some linguistic way how to render it into a string.",
                    "label": 0
                },
                {
                    "sent": "So these underlying entities sometimes pop up as proper names.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they pop up as pronouns according to some other data process.",
                    "label": 0
                },
                {
                    "sent": "The time that we're also going to talk about and go through some iterations on.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK and inference time.",
                    "label": 0
                },
                {
                    "sent": "What we're going to observe is the mentions, and we're going to want to infer what the entities are and the reason we're going to want that is basically so we can figure out which of them are the same, and so on.",
                    "label": 0
                },
                {
                    "sent": "I want to repeat that this particular task is going to take as input a bunch of documents, each of which is basically kind of pre segmented into these kinds of lists of nouns.",
                    "label": 0
                },
                {
                    "sent": "OK, but we're never going to see an example of coreference.",
                    "label": 0
                },
                {
                    "sent": "This is going to be done entirely in an unsupervised fashion in a model based way.",
                    "label": 0
                },
                {
                    "sent": "This includes the classical anaphora problem.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "That's a good point.",
                    "label": 0
                },
                {
                    "sent": "This includes the anaphora problem, which is what are the pronouns refer to, but it also includes that Bush is the president, and that George Bush in this document is Mr Bush in that document.",
                    "label": 0
                },
                {
                    "sent": "So it is a generalization of the reference problem.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's talk about the simplest thing that could possibly work.",
                    "label": 0
                },
                {
                    "sent": "It will not, and then we want to fix it.",
                    "label": 0
                },
                {
                    "sent": "OK, the simplest thing you could imagine is saying, well, I have a bunch of slots to fill for each one.",
                    "label": 0
                },
                {
                    "sent": "I'm going to have a random variable which has whose value is among the space of entities.",
                    "label": 0
                },
                {
                    "sent": "So I'm writing things like where group, Anwhere headquarters.",
                    "label": 0
                },
                {
                    "sent": "But really these are probably just going to be integers or something like that.",
                    "label": 0
                },
                {
                    "sent": "And I could imagine that I have some prior distribution over which entities get used, and again it says where group, but that's probably entity #307.",
                    "label": 0
                },
                {
                    "sent": "Then each one of these slots independently gets transformed into a string.",
                    "label": 0
                },
                {
                    "sent": "OK, what is this parameterized by?",
                    "label": 0
                },
                {
                    "sent": "Well, the simplest thing that could possibly work and will not is that each entity comes along with the distribution over strings.",
                    "label": 0
                },
                {
                    "sent": "So this is where these would be the mention parameters.",
                    "label": 0
                },
                {
                    "sent": "These characterize what strings are appropriate for each entity.",
                    "label": 0
                },
                {
                    "sent": "We would have to learn both of these things from data in an unsupervised.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this kind of endeavour, I'm going to call this entity distribution, which kind of just the relative frequencies really of these entities.",
                    "label": 0
                },
                {
                    "sent": "This is going to be beta going to some vector.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Proportions and dimension parameter is.",
                    "label": 0
                },
                {
                    "sent": "There's going to be one distribution for every entity.",
                    "label": 0
                },
                {
                    "sent": "Which is here just a multinomial over strings.",
                    "label": 0
                },
                {
                    "sent": "We're going to call those fee.",
                    "label": 0
                },
                {
                    "sent": "And there's going to be a of those two were here.",
                    "label": 0
                },
                {
                    "sent": "There are K different entities.",
                    "label": 0
                },
                {
                    "sent": "And what this would do is at each position you choose according to beta and entity and then you look at the relevant multinomial and choose a string.",
                    "label": 0
                },
                {
                    "sent": "OK, if we happen to have these strings.",
                    "label": 0
                },
                {
                    "sent": "This might even work because then we know that Bush and the president are both strings for the same entity.",
                    "label": 0
                },
                {
                    "sent": "OK, but we don't have the strings.",
                    "label": 0
                },
                {
                    "sent": "This doesn't work very well, and in particular we don't even know the number of entities K. So this.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of the easiest answer is to just erase K right Infinity and call it a day.",
                    "label": 0
                },
                {
                    "sent": "Early process mixture model.",
                    "label": 0
                },
                {
                    "sent": "So that's what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "We're not going to determine the number in advance, there's going to be some inference that I'm not going to talk about here.",
                    "label": 0
                },
                {
                    "sent": "We use a sampler.",
                    "label": 0
                },
                {
                    "sent": "That allocates these.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how does this work?",
                    "label": 0
                },
                {
                    "sent": "I'm going to be reporting numbers.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of measures of how well you're doing in this task.",
                    "label": 0
                },
                {
                    "sent": "The measure I'm going to be reporting here is the muck F1 measure, and it turns out you get about a 54%.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This isn't a good number, but it may be higher than you think, and the reason is higher than you think is 'cause there are a lot of singletons in the world, and they're going to be in their own class with their own Singleton multinomial and you'll get them right, and there's a lot of times where you just save the President, the President, the President or him him him and you'll get those right too, so you won't get everything wrong.",
                    "label": 0
                },
                {
                    "sent": "But basically you'll be doing.",
                    "label": 0
                },
                {
                    "sent": "Quite badly, right?",
                    "label": 0
                },
                {
                    "sent": "What kinds of things do you do wrong?",
                    "label": 0
                },
                {
                    "sent": "Well, kind of.",
                    "label": 0
                },
                {
                    "sent": "The biggest thing that's going wrong here is every unique string is going to turn out to be its own entity, so there's the Ware Group great, but there's also who's which is some entity, even though that's a pronoun here and which won't be Co reference to anything.",
                    "label": 0
                },
                {
                    "sent": "And the basic thing that is missing from this model.",
                    "label": 0
                },
                {
                    "sent": "There are many things missing from this model.",
                    "label": 0
                },
                {
                    "sent": "The basic thing linguistically, that's missing from this model is the fact that some of these words are pronouns, and they work in a different way from all of those other strings.",
                    "label": 0
                },
                {
                    "sent": "I'm deliberately being vague.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recruit here will get more refined.",
                    "label": 0
                },
                {
                    "sent": "So what's the simplest thing we could do?",
                    "label": 0
                },
                {
                    "sent": "We could look at our mention model and say, well, that seems kind of naive, but what's the simplest thing?",
                    "label": 1
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We could do to fix it.",
                    "label": 0
                },
                {
                    "sent": "Well, I could say well, non pronouns work this way.",
                    "label": 0
                },
                {
                    "sent": "Non pronouns you pick an entity and choose a string for that entity according to its magic string distribution for pronouns somehow.",
                    "label": 0
                },
                {
                    "sent": "What's going on here with a pronoun is that entities, in addition to having their magic string distributions, have a number, singular or plural, which by the way, we will never observe.",
                    "label": 0
                },
                {
                    "sent": "A gender masculine feminine or newer in English which we will never observe and.",
                    "label": 0
                },
                {
                    "sent": "An entity, type, person, location, organization, miscellaneous.",
                    "label": 0
                },
                {
                    "sent": "This kind of thing which we will not observe even though in principle we have systems that could label this.",
                    "label": 0
                },
                {
                    "sent": "In this task we will in this particular model these experiments we do not observe these things.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have to learn these.",
                    "label": 0
                },
                {
                    "sent": "And then once we have these, somehow you when you're a pronoun, you select your actual string from these properties.",
                    "label": 0
                },
                {
                    "sent": "This is basically what's going on is still quite crude, but now it's kind of more or less recognized that pronouns exist.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is important for coreference.",
                    "label": 0
                },
                {
                    "sent": "OK, we now have more parameters than we used to because we've got information like, hey, what pronouns do you use for a single male person?",
                    "label": 0
                },
                {
                    "sent": "And it turns out he and him are the typical suspects.",
                    "label": 0
                },
                {
                    "sent": "OK, but we have to learn this, 'cause we're not going to tell the system these things and these parameters which we're going to learn.",
                    "label": 0
                },
                {
                    "sent": "There will be some parameters that are going to be global.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, have some value, so we have the pronoun way of Jenner.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Strings the non pronoun way.",
                    "label": 0
                },
                {
                    "sent": "We need some kind of gate to switch between them, and this will be the mention type.",
                    "label": 1
                },
                {
                    "sent": "This will be the one thing other than the words that we observe.",
                    "label": 0
                },
                {
                    "sent": "This is the only thing other than the words of this model observes is we know whether or not something is a pronoun, and that's easy because the list of pronouns are small and finite.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's going to be this gating function that determines which route you use and if you go the pronoun route, you are generated in accordance with these properties that we don't know, so we're going to have to learn that that entities may land.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ones plural and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, but somehow even though these are latent variables and we're going to marginalized them right out, it's still going to help help us do a better job.",
                    "label": 0
                },
                {
                    "sent": "So here's our old model at each kind of.",
                    "label": 0
                },
                {
                    "sent": "At each position, we're choosing an entity, and then each of those entities kind of gave rise directly to a string.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From its multinomial distribution.",
                    "label": 0
                },
                {
                    "sent": "Now we've got something more complicated here that says, well, maybe you're a pronoun and something special happens.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. How's this work?",
                    "label": 0
                },
                {
                    "sent": "Well, it works substantially better, but not well.",
                    "label": 0
                },
                {
                    "sent": "And adding this program model helps.",
                    "label": 0
                },
                {
                    "sent": "What do you get?",
                    "label": 0
                },
                {
                    "sent": "Well, in retrospect is kind of unsurprising what you get here is the Weir group example who's is now reference to the Weir Group.",
                    "label": 0
                },
                {
                    "sent": "That's good, which is also referenced to the Weir Group.",
                    "label": 1
                },
                {
                    "sent": "OK, so the basic problem here is which and the basic issue is that it's referenced to the biggest entity, not the most recent entities.",
                    "label": 0
                },
                {
                    "sent": "So the biggest thing we failed to model is that there's something about recency or salience that governs anaphora, and we don't have anything like that.",
                    "label": 0
                },
                {
                    "sent": "We've got an exchangeable model.",
                    "label": 0
                },
                {
                    "sent": "They just they can't even tell what precedes what.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to have to do something to sacrifice the exchangeability of our model and.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give rise to some kind of sequence and here is the simplest thing we could think of.",
                    "label": 0
                },
                {
                    "sent": "OK, you may be able to think of something simpler and I'm sure you can think of something more complex.",
                    "label": 0
                },
                {
                    "sent": "So the simplest thing is to say there's some new variable LL is going to represent in a very crude way, kind of what entities are alive in the discourse.",
                    "label": 0
                },
                {
                    "sent": "Kind of you can think you can think, for example, in the user's head, but kind of doesn't necessarily represent anything.",
                    "label": 0
                },
                {
                    "sent": "And so, for example, it will be a list of the entities that have been invoked and their activations, whatever that means.",
                    "label": 0
                },
                {
                    "sent": "It's just some number here, OK?",
                    "label": 0
                },
                {
                    "sent": "If we have some list L and we've got some particular entities, you which is again just going to be some integer number of an entity, we can kind of put them together and I can tell you that Z has this kind of salience in that list, and the values are it's going to be the most salient that stop.",
                    "label": 0
                },
                {
                    "sent": "It could be none, which means it's not.",
                    "label": 0
                },
                {
                    "sent": "It hasn't been mentioned yet, or it can be somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "So this bucket S which kind of tells you that entity, how active is it in the discourse here?",
                    "label": 0
                },
                {
                    "sent": "How salient is it is going to be used for one thing.",
                    "label": 0
                },
                {
                    "sent": "Only this will help govern this.",
                    "label": 0
                },
                {
                    "sent": "This will kind of be conditioned on when choosing that observed variable of whether or not it's a pronoun.",
                    "label": 0
                },
                {
                    "sent": "That's all that this is going to be used for.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's very simple.",
                    "label": 0
                },
                {
                    "sent": "So how is this going to work?",
                    "label": 0
                },
                {
                    "sent": "Well, you start off with kind of some some list where nothing has been mentioned.",
                    "label": 0
                },
                {
                    "sent": "Everything's activation 0 maybe for the first slot, my prior chooses that it wants to talk about entity one now.",
                    "label": 0
                },
                {
                    "sent": "OK, well that has salience value none and maybe the mention that's chosen in a good setting of the parameters that hopefully we will learn is that now we're going to introduce this with a proper noun, 'cause we've never mentioned this before.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a new L which evolves deterministically.",
                    "label": 0
                },
                {
                    "sent": "So the thing we just mentioned is more active than it was before.",
                    "label": 0
                },
                {
                    "sent": "Maybe now I'm going to talk about entity to its salience value is again none, so I might use a proper mention again.",
                    "label": 0
                },
                {
                    "sent": "KL evolves again.",
                    "label": 0
                },
                {
                    "sent": "The first entity decays and the second entity is evoked here in the previous step.",
                    "label": 0
                },
                {
                    "sent": "Maybe I decide to talk about entity two again, and in this case it would have the value top.",
                    "label": 0
                },
                {
                    "sent": "It's the most salient entity and maybe now choose to use a pronoun.",
                    "label": 0
                },
                {
                    "sent": "I'm telling the.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Best case story here.",
                    "label": 0
                },
                {
                    "sent": "So what does this do?",
                    "label": 0
                },
                {
                    "sent": "This now adds this kind of thin bit of structure that makes this model non exchangeable.",
                    "label": 0
                },
                {
                    "sent": "But now let's us say something about recency.",
                    "label": 0
                },
                {
                    "sent": "OK, there's some pressure now, or there's some capacity for this model to learn about pronouns and recency.",
                    "label": 0
                },
                {
                    "sent": "It could learn the opposite, right?",
                    "label": 0
                },
                {
                    "sent": "It could learn that pronouns like to refer to the least salient thing, though of course.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can kind of peek inside and look at kind of the parameters that are learned at some at some phase and you can see that it's kind of learned what you'd expect if you had to hand code something, which is that when you're introducing something new, it's typically a proper noun mention, whereas the most salient thing is typically referred to as a pronoun and then kind of in between you get various in between proportions.",
                    "label": 0
                },
                {
                    "sent": "OK, basic.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning the right kinds of patterns.",
                    "label": 0
                },
                {
                    "sent": "This gives you another pretty big boost.",
                    "label": 0
                },
                {
                    "sent": "Now you're up to 71.5, adding the salience, and in this particular example, all of the problems are gone.",
                    "label": 0
                },
                {
                    "sent": "So this particular example is now right, though of course you're making tons of errors still in various places.",
                    "label": 0
                },
                {
                    "sent": "OK, I'd like to point out that in general these are crude models like.",
                    "label": 0
                },
                {
                    "sent": "I mean, I could give you a story about very, very complicated rules of anaphora, right?",
                    "label": 0
                },
                {
                    "sent": "I know about these rules and I could pull them all in, but the question here is, what is the simplest thing that could work and kind of trying to model as little as possible to learn?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As much as possible, OK?",
                    "label": 0
                },
                {
                    "sent": "So the one thing that's missing here is that this model is just completely incapable of associating entities across documents as I've described it so far.",
                    "label": 0
                },
                {
                    "sent": "Each document has its own special entities that don't occur anywhere else.",
                    "label": 0
                },
                {
                    "sent": "The obvious thing to do is to add 1 layer of a hierarchy, and I'm sure you can see where this is going.",
                    "label": 0
                },
                {
                    "sent": "I would like to have some model where some of the entities occur in many documents and then documents of course can have their own weird entities that are unique to them, and there's some global set that each.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Documents is drawing from.",
                    "label": 0
                },
                {
                    "sent": "I can do this in kind of.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait forward way by taking what we had before sticking.",
                    "label": 0
                },
                {
                    "sent": "Then this kind of always used to live inside a document plate that I wasn't showing.",
                    "label": 0
                },
                {
                    "sent": "So there's all this truck.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inside a document I can say, well, actually there's a global set of these entities.",
                    "label": 0
                },
                {
                    "sent": "There's global entity proportions, beta, beta, not.",
                    "label": 1
                },
                {
                    "sent": "There's a global parameters which include both their magic string distributions and also things like male and female, and so on, and then each document simply subselects, and this is now a hierarchical Dirichlet.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This turns out to actually help, even though the whole point of this is to now be able to answer questions about cross Document Reference, which isn't reflected in the score.",
                    "label": 0
                },
                {
                    "sent": "This actually helps even in this within Document reference score, and you might wonder why we wondered why.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The reason why this helps is because there are a lot of cases like where you have documents like the one on the bottom where you have rice and then Bush and then sheet.",
                    "label": 0
                },
                {
                    "sent": "And if you're looking from a single document perspective, she is going to get linked up to Bush because it's the closest thing.",
                    "label": 0
                },
                {
                    "sent": "And if that's all you know that's all you're going to do.",
                    "label": 0
                },
                {
                    "sent": "But if you have the first document you already have good evidence.",
                    "label": 0
                },
                {
                    "sent": "That's pretty unambiguous that Bush is in fact got the other, got the other value for the gender parameter, and now you can have a much better shot at getting the second document right.",
                    "label": 0
                },
                {
                    "sent": "So you can kind of share information across documents that turns out to be useful.",
                    "label": 0
                },
                {
                    "sent": "'cause newswire really does talk about the same people over.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turned over.",
                    "label": 0
                },
                {
                    "sent": "OK, here's some scores just to kind of give you a sense of how this is going in an absolute sense.",
                    "label": 0
                },
                {
                    "sent": "What I have here is the closest fully unsupervised system.",
                    "label": 0
                },
                {
                    "sent": "Cardion Wagstaff, 99, that I know about, and this system is substantially better than that, right.",
                    "label": 0
                },
                {
                    "sent": "It works substantially better, and in fact, it's on par with a lot of somewhat recent supervised systems.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, there are supervised systems that do better.",
                    "label": 0
                },
                {
                    "sent": "I would be shocked if there were not, but many of them that have similar features really don't do very much better.",
                    "label": 0
                },
                {
                    "sent": "And the advantages here is you can kind of look at more data.",
                    "label": 0
                },
                {
                    "sent": "You can look at languages for which we don't have the supervision, which is actually one of the more expensive kinds.",
                    "label": 0
                },
                {
                    "sent": "The note here about supervised plus plus this 81 is the best number I could find on this measure on this data set.",
                    "label": 0
                },
                {
                    "sent": "They take as they have gold access to various kinds of information like the named entity type.",
                    "label": 0
                },
                {
                    "sent": "That is a huge clue that they get to observe the truth.",
                    "label": 0
                },
                {
                    "sent": "Whereas we just model it as a latent variable.",
                    "label": 0
                },
                {
                    "sent": "So that's a huge clue, and so it's not entirely comperable, but I want to give you the sense that the system, though fully unsupervised, is kind of in the pack with a lot of.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A lot of supervised systems.",
                    "label": 0
                },
                {
                    "sent": "OK, so in summary, what happened here OK?",
                    "label": 0
                },
                {
                    "sent": "Hear what I'm talking about is another thing that's working fairly well for us in terms of unsupervised learning, and that is starting with some very, very simple model and slowly adding kind of the minimal random variable that's necessary to fix the big problems, and this seems to work a lot better than just writing down every variable we could think of.",
                    "label": 0
                },
                {
                    "sent": "Kind of Opry, Ori and then trying to fit the whole mess.",
                    "label": 0
                },
                {
                    "sent": "I see this as another kind of incremental learning or what you're doing is adding complexity.",
                    "label": 0
                },
                {
                    "sent": "Kind of 1 modeled phenomenon at a time and seeing whether or not you're still able to learn.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a model of pronoun structure.",
                    "label": 1
                },
                {
                    "sent": "There was this kind of sequential model because that was very important.",
                    "label": 0
                },
                {
                    "sent": "We added this hierarchy because that was necessary for cross document.",
                    "label": 0
                },
                {
                    "sent": "You could imagine adding more things and I encourage anybody who's interested in this to do so.",
                    "label": 0
                },
                {
                    "sent": "I am definitely not claiming that this is somehow kind of maximal modeling here, but just that from some surprises from a surprisingly simple system.",
                    "label": 0
                },
                {
                    "sent": "I was extremely surprised at how well this does at this task in a fully unsupervised way.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, there's one more thing I'm going to talk about today.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to talk about very briefly.",
                    "label": 0
                },
                {
                    "sent": "This is the problem of unsupervised translation mind.",
                    "label": 1
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's this about so many of you may know something about the standard machine translation approach.",
                    "label": 0
                },
                {
                    "sent": "In this approach, you've got a bunch of documents that contain more or less literal translation sentence by sentence.",
                    "label": 0
                },
                {
                    "sent": "So you have examples of exact kind of literal translations, and when you have this, this is great.",
                    "label": 0
                },
                {
                    "sent": "Obviously, you can leverage this, even though there's.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complicated things that mediate OK.",
                    "label": 0
                },
                {
                    "sent": "I want to talk very briefly about the problem of getting translations from mono text.",
                    "label": 0
                },
                {
                    "sent": "So in this case you've got a bunch of source documents.",
                    "label": 0
                },
                {
                    "sent": "A bunch of target documents and you don't have any examples of translations yet somehow we'd still.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like to figure out what words are translations.",
                    "label": 0
                },
                {
                    "sent": "The task here is lexecon induction, so there's going to be a bunch of source words.",
                    "label": 1
                },
                {
                    "sent": "There's going to be a bunch of target words, and there's going to be some matching between them, and this we're going to look at matches that are at most one to one, and somehow we'd like to learn these in an unsupervised fashion, even though we don't see exam.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of them.",
                    "label": 0
                },
                {
                    "sent": "OK, how would I do this?",
                    "label": 0
                },
                {
                    "sent": "Here's kind of the simplest data representation that could work for every word I look at the source text.",
                    "label": 1
                },
                {
                    "sent": "I write down the kind of the letters in that word, so I write down various features that talk about its orthography.",
                    "label": 0
                },
                {
                    "sent": "So the various engrams I also write down what words this kind of what other words this occurs with in that language.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So all of these features are monolingual.",
                    "label": 0
                },
                {
                    "sent": "And I do that for the other language too.",
                    "label": 0
                },
                {
                    "sent": "All of these features are now target lingual.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I've got these pairs and The thing is really going to make all this work.",
                    "label": 0
                },
                {
                    "sent": "Is Canonical correlation analysis, which I'm sure is familiar to all of you.",
                    "label": 1
                },
                {
                    "sent": "You may or may not be aware of the probabilistic formulation that I'm sure most of you are there too, but I want to quickly illustrate why I'm talking about CCA here.",
                    "label": 0
                },
                {
                    "sent": "If I just had a bunch of vectors like this in a source space, right, I might use PCA to describe them to kind of represent them in some way.",
                    "label": 0
                },
                {
                    "sent": "If I had a bunch of word features in the target space, I might use.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "PCA to describe them to the problem would be if they actually had pairs.",
                    "label": 0
                },
                {
                    "sent": "If they came in pairs like they do in translation, the projections onto these subspaces would be kind of arbitrarily related.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unrelated OK, what I'd like to do is I'd like to somehow find representations for the data that correlate stem, and that's what CC does, as I'm sure most of you know.",
                    "label": 0
                },
                {
                    "sent": "Here what I now get is I get these subspaces where.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These things are correlated, and in particular in the probabilistic formulation of Bach and Jordan.",
                    "label": 0
                },
                {
                    "sent": "What I can do is talk about there being a single language independent latent space that gives rise to these language specific.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Concepts.",
                    "label": 0
                },
                {
                    "sent": "OK, so this basically means that those latent space that I can wiggle around concepts and out pop source and target.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Translations OK, so I need some model.",
                    "label": 0
                },
                {
                    "sent": "The model is very simple.",
                    "label": 0
                },
                {
                    "sent": "I generate source a bunch of slots for source words, I choose a number of target words and I choose a matching.",
                    "label": 1
                },
                {
                    "sent": "Now I've got some pairs and some.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paired words for each of the paired words.",
                    "label": 0
                },
                {
                    "sent": "I'm going to take a point in this Canonical space, which is language independent.",
                    "label": 1
                },
                {
                    "sent": "OK, I'm going to choose that according to some simple distribution I'm going to project it to the source and target space with some transformation vector that of course I have to learn plus noise.",
                    "label": 0
                },
                {
                    "sent": "Same on the target side transform.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plus noise.",
                    "label": 0
                },
                {
                    "sent": "And that's going to generate all of my parent items.",
                    "label": 0
                },
                {
                    "sent": "Separately, I'm going to have some noise model in the source that generates source items, and I'm going to have some noise model on the target.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generates target items.",
                    "label": 0
                },
                {
                    "sent": "OK, how would I learn this?",
                    "label": 0
                },
                {
                    "sent": "Well M is my friend so I might want to obtain some posterior matchings that turns out to be bad, 'cause that's intractable.",
                    "label": 0
                },
                {
                    "sent": "And then I might want to kind of maximize my parameters, but kind of even if I had those posteriors, I wouldn't know how to find those projections.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it turns out to be bad, but it turns out in this case Hardy M is my even better friend where finding the best matching is easy with fixed parameters.",
                    "label": 0
                },
                {
                    "sent": "I just look at some kind of joint likelihood with some with the noise models factored in some way.",
                    "label": 0
                },
                {
                    "sent": "And if I had a fixed matching then I would have paired data, even if it's wrongly paired, and then I can find these parameters of the.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Action using CCA.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do I do?",
                    "label": 0
                },
                {
                    "sent": "Quick experiment and then that will be the end here is I take the 2000 most frequent nouns.",
                    "label": 1
                },
                {
                    "sent": "I take some text from Wikipedia so they're not translations, but they do contain some of the same stuff.",
                    "label": 0
                },
                {
                    "sent": "And I might or might not take 100 translation pairs.",
                    "label": 1
                },
                {
                    "sent": "It turns out you don't need this, but these experiments have them, and the evaluation is going to be some kind of precision at some recall.",
                    "label": 0
                },
                {
                    "sent": "The reason why use a recall low recall like 33% is 'cause.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As good as the bass lines ever do.",
                    "label": 0
                },
                {
                    "sent": "So if I use edit distance on English Spanish, which is a particularly easy case, I get 61% because there are a lot of very similar words, especially technical words.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, If however I simply use features of the orthography, this kind of alternating learning with M and CCA and matchings gets me to 80%.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So does looking at the context without any information about the spelling of the words at all.",
                    "label": 0
                },
                {
                    "sent": "This is pure.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Context.",
                    "label": 0
                },
                {
                    "sent": "And if you put them together, they stack.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a huge improvement over previous work which was not able to beat the added distance baseline.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here's kind of precision and recall of edit distance.",
                    "label": 0
                },
                {
                    "sent": "It drops off very quickly.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As you get out of the space of cognates, precision stays much higher for much longer with this match.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "CCA model you might say that's not fair.",
                    "label": 0
                },
                {
                    "sent": "You started with some seed lexicons.",
                    "label": 0
                },
                {
                    "sent": "Well, what happens if we just take the lowest edit distance words as the seed?",
                    "label": 1
                },
                {
                    "sent": "Turns out that works almost as well this thing.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Somewhat robust in that case.",
                    "label": 0
                },
                {
                    "sent": "And the last things I want to show you are some examples of what's learned 'cause.",
                    "label": 0
                },
                {
                    "sent": "I think they're really neat.",
                    "label": 0
                },
                {
                    "sent": "So these are the most confident non identical matches.",
                    "label": 0
                },
                {
                    "sent": "Of course the most confident ones are the ones that occur in the same context, and they're also identical string.",
                    "label": 0
                },
                {
                    "sent": "Identical.",
                    "label": 0
                },
                {
                    "sent": "Here are some cases that are.",
                    "label": 0
                },
                {
                    "sent": "There are, of course, they're cognates.",
                    "label": 0
                },
                {
                    "sent": "Right for me?",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spanish, of course those are going to be at the top but you learn non Cognates 2.",
                    "label": 0
                },
                {
                    "sent": "Here are the top non cognates and you can see some of these are kind of semi cognates.",
                    "label": 0
                },
                {
                    "sent": "They aren't cognates, but you probably have an easy time learning them.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Here are some mistakes that are interesting, so the system can't tell the difference between action and reaction, but neither could I guess Newton.",
                    "label": 0
                },
                {
                    "sent": "And there's a couple other confusions here that are reasonable.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this works for French and you say OK, well, that's not too surprise.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, it works for Chinese.",
                    "label": 0
                },
                {
                    "sent": "Doesn't work as well, but that's not surprising either because.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You don't have as many queues.",
                    "label": 0
                },
                {
                    "sent": "OK. What's being learned well here is looking at features on one side and seeing what CCA pairs them with.",
                    "label": 0
                },
                {
                    "sent": "People have questions.",
                    "label": 0
                },
                {
                    "sent": "I can say what that means on the other side, so you learn something that edit distance.",
                    "label": 0
                },
                {
                    "sent": "Can't you learn regular morphological pairing?",
                    "label": 0
                },
                {
                    "sent": "So kind of some suffixes in English, matches some suffixes and Spanish regularly, but not string identically, and you learn those, and that's why you can do better than edit distance without context.",
                    "label": 0
                },
                {
                    "sent": "And if you look at context, you learn that kind of similar semantic fields.",
                    "label": 0
                },
                {
                    "sent": "So Democrat looks the same as socialist.",
                    "label": 0
                },
                {
                    "sent": "OK, well the system is a little bit confused.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Politically, but what can you do?",
                    "label": 0
                },
                {
                    "sent": "OK, so in summary, here's a model that learns bilingual lexicons from monolingual text.",
                    "label": 0
                },
                {
                    "sent": "It works much better than baselines and much better than kind of previous work in this.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Area.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is my conclusion.",
                    "label": 0
                },
                {
                    "sent": "I've talked through more kind of an argument by example and what I'm basically trying to argue here.",
                    "label": 0
                },
                {
                    "sent": "There's three cases of unsupervised learning.",
                    "label": 1
                },
                {
                    "sent": "We're kind of we've got nontrivial, interesting linguistic structure for some NLP problems that we really care about.",
                    "label": 0
                },
                {
                    "sent": "We've done this by kind of incremental learning, being very careful about what we do and don't model and what we do and don't learn at any given time.",
                    "label": 0
                },
                {
                    "sent": "In a lot of these cases, these unsupervised methods or unsupervised refinement methods work quite well.",
                    "label": 0
                },
                {
                    "sent": "In the case of parsing, actually better than a lot of kind of more manual techniques.",
                    "label": 0
                },
                {
                    "sent": "There's a lot left to do, so I don't want to give the impression that kind of we're ready to kind of pipe data at the system and suddenly all of language will be learned.",
                    "label": 0
                },
                {
                    "sent": "But I'm very excited about what unsupervised systems have been able to do recently, and I hope this will help get get this this audience excited too about unsupervised systems, both as an interesting problem to think about, and also.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for other domains, so that's it, thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "He did not make reference to quite a lot of work which falls under unsupervised learning like.",
                    "label": 0
                },
                {
                    "sent": "Leaving semantic analysis work for the solving general problems and many others right?",
                    "label": 0
                },
                {
                    "sent": "That's totally fair.",
                    "label": 0
                },
                {
                    "sent": "I should say that in all of this is not unique to that.",
                    "label": 0
                },
                {
                    "sent": "And all of these cases I talked about only a small fraction of the most related work, and I neither attempted.",
                    "label": 0
                },
                {
                    "sent": "That that is kind of as far as I'm concerned.",
                    "label": 0
                },
                {
                    "sent": "Truth in advertising what I said on the kind of direct comparison side.",
                    "label": 0
                },
                {
                    "sent": "I did not attempt to compare kind of all the relevant methods.",
                    "label": 0
                },
                {
                    "sent": "I did attempt to compare on kind of this data in these conditions.",
                    "label": 0
                },
                {
                    "sent": "The things people have tried that have worked the best of those methods an the representative there I chose was the Cardium Wagstaff paper if there if there's work from that field that does better than theirs that I should be comparing to.",
                    "label": 0
                },
                {
                    "sent": "I would love to know about it, but everything I know about that's fully unsupervised.",
                    "label": 0
                },
                {
                    "sent": "That I could find data did not work as well.",
                    "label": 0
                },
                {
                    "sent": "That's not, it does not mean that I do not think that the ideas there are interesting and in a kind of Fuller treatment of that space should not be discussed.",
                    "label": 0
                },
                {
                    "sent": "Nor do I think that my system somehow captures everything that those kinds of methods can.",
                    "label": 0
                },
                {
                    "sent": "It absolutely doesn't.",
                    "label": 0
                },
                {
                    "sent": "In particular, I'm not leveraging anywhere near as much text, though.",
                    "label": 0
                },
                {
                    "sent": "That's more limitation of the experiments in the model.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "He said that Hardy M was working better than than soft again.",
                    "label": 0
                },
                {
                    "sent": "In that I know how to run it.",
                    "label": 0
                },
                {
                    "sent": "Not that I ran both and hard work better.",
                    "label": 0
                },
                {
                    "sent": "I guess the question is, do you think there's have you experiment with hard?",
                    "label": 0
                },
                {
                    "sent": "Again?",
                    "label": 0
                },
                {
                    "sent": "The earlier problems think that that if there's any point in that experiment.",
                    "label": 0
                },
                {
                    "sent": "Oh, if I could do softeam I absolutely would.",
                    "label": 0
                },
                {
                    "sent": "I find that kind of every time we've done the comparison, soft DM is more stable and gives better answers.",
                    "label": 0
                },
                {
                    "sent": "The end result.",
                    "label": 0
                },
                {
                    "sent": "And then use the label data that's out there.",
                    "label": 0
                },
                {
                    "sent": "With the performance right, so this is a very kind of.",
                    "label": 0
                },
                {
                    "sent": "It's in the space of.",
                    "label": 0
                },
                {
                    "sent": "How do you look at?",
                    "label": 0
                },
                {
                    "sent": "Kind of the semi supervised world where you've got a lot of data and you want to do something and maybe we know all this about unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "We've got some labels to.",
                    "label": 0
                },
                {
                    "sent": "How do we combine them best?",
                    "label": 0
                },
                {
                    "sent": "I have no doubt that with lots of data and some labels you can do better than you can without those labels, especially if you're going to get judged on whatever task is represented by those labels.",
                    "label": 0
                },
                {
                    "sent": "So a lot of times with unsupervised system, the frustration is that it's something basically reasonable, but it didn't read your mind about the details of the task.",
                    "label": 0
                },
                {
                    "sent": "An no system is going to do that, so a lot of this kind of being very careful about how what model you choose to model and not does end up kind of interacting with what annotations you're trying to reproduce, which can be a little frustrating.",
                    "label": 0
                },
                {
                    "sent": "I'm wondering.",
                    "label": 0
                },
                {
                    "sent": "Radical.",
                    "label": 0
                },
                {
                    "sent": "Models which were you just kind of making small perturbations or perturbations in order to improve the performance, right?",
                    "label": 0
                },
                {
                    "sent": "So you're doing this manually, and I'm wondering.",
                    "label": 0
                },
                {
                    "sent": "You could imagine nothing automatically.",
                    "label": 0
                },
                {
                    "sent": "Looking at her.",
                    "label": 0
                },
                {
                    "sent": "Right, so there's there's kind of this.",
                    "label": 0
                },
                {
                    "sent": "You can imagine something where I have just enough labeled data that I can tell when I've made a good improvement to my model, which is actually usually the case, and then I could try to do some automatic search over the space of models.",
                    "label": 0
                },
                {
                    "sent": "I could absolutely imagine that, and I think that would be very interesting, I mean.",
                    "label": 0
                },
                {
                    "sent": "Like everything else, then there's the question of you know can you automate it as well as what we do, and then immediately the question is maybe you can automate it better.",
                    "label": 0
                },
                {
                    "sent": "I think it's interesting.",
                    "label": 0
                },
                {
                    "sent": "There's a question in the back.",
                    "label": 0
                },
                {
                    "sent": "I don't know how many we can take one more.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Brown stop.",
                    "label": 0
                },
                {
                    "sent": "Simple things.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that's another great is the question is about training from kind of easy examples to hard examples.",
                    "label": 0
                },
                {
                    "sent": "I think it's I think that's a great point.",
                    "label": 0
                },
                {
                    "sent": "I think it's another case of this kind of incremental learning being the key to learning good stuff.",
                    "label": 0
                },
                {
                    "sent": "There are a couple of cases where I think we've benefited from that.",
                    "label": 0
                },
                {
                    "sent": "Though we've never explicitly sorted out, so the cases we've benefited so one line of work I didn't talk about it all is any of the kind of learning with agreement where you have multiple models and they have to agree somehow when you learn and you get kind of a multiview kind of effect.",
                    "label": 0
                },
                {
                    "sent": "Another effect you get in the particular way we've done, we've done multiple models being forced to agree is it tends to be that kind of an simple examples they'll agree right away, and on the simple example is not necessarily short examples right in the way you're saying, but on the simple examples.",
                    "label": 0
                },
                {
                    "sent": "You kind of you learn immediately and then as the models get better, they agree on harder and harder cases.",
                    "label": 0
                },
                {
                    "sent": "And learning somehow proceeds out.",
                    "label": 0
                },
                {
                    "sent": "So although I can't quantify that, I really do have a sense that in some of our other work we are seeing exactly that effect where although we haven't expressly built a system for incremental learning in that way that we're benefiting from that, and that there's a lot of potential there.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}