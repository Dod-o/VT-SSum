{
    "id": "e2u3b3qfgsy7ajcpadwcvvjcxgcujf4o",
    "title": "A Novel Ensemble Method for Named Entity Recognition and Disambiguation based on Neural Network",
    "info": {
        "author": [
            "Pasquale Lisena, EURECOM"
        ],
        "published": "Nov. 22, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2018_lisena_ensemble_method_disambiguation/",
    "segmentation": [
        [
            "OK, thank you for the presentation, so I'm pascuala from Europe, I'm presenting a new way of doing named entity recognition and disambiguation with neural networks.",
            "I have to give a big credit to Lorenzo Canal for this work that has been conducted mostly by email during his master thesis under supervision of our filter and see and me."
        ],
        [
            "So I guess that most of you knows what are a name, entity recognition and named entity disambiguation.",
            "Basically, having attacks, the goal is to find a part of this text that to which we can assign a type like in this case personal emperor and general historical figure or or in better case and assign him an identify are that helps you to say OK, we're speaking.",
            "Exactly of this, Napoleon.",
            "So normally for the type assignment, extractors works with a taxonomy of types that can differ a lot between extractor and extractor.",
            "Normally you have always person, organization, location, but obviously you can have other kind of times that are quite different from one extra order for the knowledge base that are instead the target for the name entity disambiguation normally.",
            "The most popular are the Pedia, Wikidata and Wikipedia.",
            "This is a cool fact because thanks to the same answer link I can pass from one data data set to the other in are quite easy way.",
            "So."
        ],
        [
            "This is a list of interesting extractors that are currently available, some of them they perform a direct type recognition, so they have their own tip type set and they try to understand which type is the spot at the token in the.",
            "In the text, some other ones instead, that goes to perform entities immigration.",
            "That means that they try to spot the right entity and eventually came back by inferring the type from the disambiguation, some of them instead perform both task.",
            "Normally these extractor are black boxes, even if some of them can be trained with different parameters and data set, some of them are strong in a certain kind of information with the same data sets.",
            "Certain data sets with certain language with certain type of entities.",
            "Some other week, some model.",
            "Well, each one has its own strengths and weakness.",
            "So for this reason a lot of research."
        ],
        [
            "Has been conducting last year in order to perform ensemble strategies that wants to take the best from each of those extractor and give a final answer that is most near to the truth.",
            "I give you just two example.",
            "One is nerd and the 2nd is Fox.",
            "They work with a different number of extractor and with different extractor.",
            "Actually they have different strategy for performing again sample what they have in common is that they need a mapping from the input type set to their own internal typesetter that are the nerd ontology for nelda and this location, organization, person array for.",
            "Box so this mapping is expensive to realize.",
            "Sometimes it's manual activities and other times it is.",
            "Performed with the machine learning what we are proposing now."
        ],
        [
            "Is a new ensemble strategy that requires."
        ],
        [
            "Shopping at all, it automatically align with the type set of the ground truth, and the idea is that I want to not predict directly directly type and entity, but predict with the most trustworthy extractor.",
            "For doing this, we rely on for kind of features and to never allow networks I go now in detail."
        ],
        [
            "Explaining the four kinds of features for next slide exit will be extractor W word, an E entity."
        ],
        [
            "So the first kind of features are tax related features that are actually worth and bending serialized with the first X we use actually 2 fast X model.",
            "One is trained by us on the corpus that we want to manage and the second one is said there is a pre trained version that is available on the first X repository.",
            "Basically has been trained on a certain a localized version of Wikipedia.",
            "So what we do here actually is user fast X model and a fast X vector an boost it with pre trained version.",
            "Sick."
        ],
        [
            "Kind of features are actually the representation of the taxonomy of types over the single extractor.",
            "Or in case there is not of the class hierarchy from Wikidata.",
            "This is a very easy simple taxonomy.",
            "We have a place organization in person and then a city and mountain as subclasses of place and actor and musician as subclasses of a person.",
            "What we do is we look level by level and we perform one hot encoding."
        ],
        [
            "On the level 4 representing the type vector, we concatenate the path of surpassing level by level until arriving to the last one.",
            "So for musician we concatenate person and musician.",
            "When the prediction stops at one of Upper the upper level, we just add padding in order to complete the.",
            "A feature of our representation."
        ],
        [
            "So, uh III convented the feature is the entity features and actually represent the similarities between two entities.",
            "We identified the five kind of similarities.",
            "If they have the same UI.",
            "If they have similar labels.",
            "If we have similar abstract.",
            "If the two person of the same occupation, this is valid just for person actually, and if there is a such structural similarities, that is the shortest distance.",
            "Between two entities in DB Pedia in Wikidata, sorry."
        ],
        [
            "For now, we can realize this similarity.",
            "We computed similarities between each couple of extracted entities or the one excited by text Razor and 1 / D be spotlighted.",
            "One of the X Rays only one Dover, Del, etc and we concatenate everything in a single vector."
        ],
        [
            "Last kind of feature.",
            "Our score features that actually represented the scores that extractor giving output for indicated their confidence or the salience of that predicted entity.",
            "So use those features for for training and two neural networks, one for temple recognition, one for this immigration.",
            "So then."
        ],
        [
            "Cable network for type of recognition, or enter as."
        ],
        [
            "The goal of answering the question which of those types is the right one?",
            "We run this network one for each token.",
            "Now I'm going to describe that."
        ],
        [
            "Need to the structure on top.",
            "We have this alignment blocks that basically connects the input or two.",
            "This yellow set of nodes that have the same dimensionality of the typeset.",
            "We call them alignment block because if we cut the network at this point they basically do the mapping that we wanted to avoid.",
            "For us the good the difference of this approach is better of other mappings is that the network is.",
            "One, and so these mappings are.",
            "Well, go straight.",
            "There are optimized to answer the task over the entire network.",
            "Moreover, separating the different blocks helps us to not give a different importance biased by the dimensionality of each input, so that input with different dimensionality entering the network in exactly the same way.",
            "The."
        ],
        [
            "Yellow blocks are then concatenated and entering this dense layer that connects them to the output.",
            "The output has the same dimensionality of the final typeset.",
            "And then obviously those are basically scores of that helps us to understand which one is the strongest candidate for the type of recognition."
        ],
        [
            "Second network is for disambiguation.",
            "Use it together with a voting mechanism.",
            "As you can see, there is just one up to output liar."
        ],
        [
            "Be cause this because the question that we want to answer with this network is is this candidate entity the right one?",
            "So we run this network once for each candidate and candidate for us is each distinct entity that has been extracted by the different extractor for a specific token.",
            "The."
        ],
        [
            "So here we have two kind of features in input.",
            "One is the entity features that actually in this case are the similarities between the candidate and each extracted one, and the type features that have their input into the network with the same alignment block of enter.",
            "Then there are two dense layers that connects to the unique."
        ],
        [
            "Output node.",
            "So the voting mechanism is basically random after an network run, so every candidate has been processed and we choose the the output and error on value that have the maximum score, and then we further evaluate understanding if it's surpass a certain threshold.",
            "That means that is a good or bad candidate."
        ],
        [
            "Results we evaluated this strategies."
        ],
        [
            "Three different Grand Theft corpora with different languages with different disambiguating targets, and this corpora latassa to evaluate both nerd and type assignment with different type set.",
            "So."
        ],
        [
            "So our assemble method over outperforms all the single extractor in every metrics for timber recognition.",
            "We did also this experiment in switching off the some features to see what is the impact that each feature, as in the final score.",
            "And even if most information is contained in the type only.",
            "Features the other feature are still crucial to have results that are this good for."
        ],
        [
            "Something disambiguation we did the same comparison.",
            "We have a better results than the single extractor and also in this case we need both the entity and the type features in order to have the best results."
        ],
        [
            "We also compared them.",
            "We order a kind of extract or that we found in the jar Bill.",
            "Also in this case we have very good results even if there are some 0 scores that are a bit weird.",
            "Probably those extractor where just disconnected at the time we did win Aranda experiments so."
        ],
        [
            "I presented two neural networks with different strategies that outperforms the.",
            "A single extractor that they put it together in the ensemble.",
            "Moreover, this network proves that we need more than just a single type of features for predicting the very the very good.",
            "Entity and type."
        ],
        [
            "As future works we want to add new extractor to the assemble method.",
            "Specifically, I'm thinking about Elmo, an flair that actually are trending now.",
            "Also, we wanted to make more experiments by using a recurrent neural network and LTM, especially for a surface form features, and we want to try Wikidata embedding and also we plan to announce the next Gen TV corpus that has a corpus as being realized.",
            "For actually this experiment, by adding a new.",
            "The annotation."
        ],
        [
            "So thank you for attention.",
            "I leave you with the code for the repository linked to the repository of this experiment and the link to this slide.",
            "Thank you.",
            "I'm happy to answer your questions.",
            "Thank you Paula."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you for the presentation, so I'm pascuala from Europe, I'm presenting a new way of doing named entity recognition and disambiguation with neural networks.",
                    "label": 0
                },
                {
                    "sent": "I have to give a big credit to Lorenzo Canal for this work that has been conducted mostly by email during his master thesis under supervision of our filter and see and me.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I guess that most of you knows what are a name, entity recognition and named entity disambiguation.",
                    "label": 1
                },
                {
                    "sent": "Basically, having attacks, the goal is to find a part of this text that to which we can assign a type like in this case personal emperor and general historical figure or or in better case and assign him an identify are that helps you to say OK, we're speaking.",
                    "label": 0
                },
                {
                    "sent": "Exactly of this, Napoleon.",
                    "label": 0
                },
                {
                    "sent": "So normally for the type assignment, extractors works with a taxonomy of types that can differ a lot between extractor and extractor.",
                    "label": 1
                },
                {
                    "sent": "Normally you have always person, organization, location, but obviously you can have other kind of times that are quite different from one extra order for the knowledge base that are instead the target for the name entity disambiguation normally.",
                    "label": 0
                },
                {
                    "sent": "The most popular are the Pedia, Wikidata and Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "This is a cool fact because thanks to the same answer link I can pass from one data data set to the other in are quite easy way.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a list of interesting extractors that are currently available, some of them they perform a direct type recognition, so they have their own tip type set and they try to understand which type is the spot at the token in the.",
                    "label": 0
                },
                {
                    "sent": "In the text, some other ones instead, that goes to perform entities immigration.",
                    "label": 0
                },
                {
                    "sent": "That means that they try to spot the right entity and eventually came back by inferring the type from the disambiguation, some of them instead perform both task.",
                    "label": 0
                },
                {
                    "sent": "Normally these extractor are black boxes, even if some of them can be trained with different parameters and data set, some of them are strong in a certain kind of information with the same data sets.",
                    "label": 0
                },
                {
                    "sent": "Certain data sets with certain language with certain type of entities.",
                    "label": 0
                },
                {
                    "sent": "Some other week, some model.",
                    "label": 0
                },
                {
                    "sent": "Well, each one has its own strengths and weakness.",
                    "label": 0
                },
                {
                    "sent": "So for this reason a lot of research.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Has been conducting last year in order to perform ensemble strategies that wants to take the best from each of those extractor and give a final answer that is most near to the truth.",
                    "label": 1
                },
                {
                    "sent": "I give you just two example.",
                    "label": 0
                },
                {
                    "sent": "One is nerd and the 2nd is Fox.",
                    "label": 0
                },
                {
                    "sent": "They work with a different number of extractor and with different extractor.",
                    "label": 0
                },
                {
                    "sent": "Actually they have different strategy for performing again sample what they have in common is that they need a mapping from the input type set to their own internal typesetter that are the nerd ontology for nelda and this location, organization, person array for.",
                    "label": 0
                },
                {
                    "sent": "Box so this mapping is expensive to realize.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's manual activities and other times it is.",
                    "label": 0
                },
                {
                    "sent": "Performed with the machine learning what we are proposing now.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a new ensemble strategy that requires.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shopping at all, it automatically align with the type set of the ground truth, and the idea is that I want to not predict directly directly type and entity, but predict with the most trustworthy extractor.",
                    "label": 0
                },
                {
                    "sent": "For doing this, we rely on for kind of features and to never allow networks I go now in detail.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Explaining the four kinds of features for next slide exit will be extractor W word, an E entity.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first kind of features are tax related features that are actually worth and bending serialized with the first X we use actually 2 fast X model.",
                    "label": 0
                },
                {
                    "sent": "One is trained by us on the corpus that we want to manage and the second one is said there is a pre trained version that is available on the first X repository.",
                    "label": 0
                },
                {
                    "sent": "Basically has been trained on a certain a localized version of Wikipedia.",
                    "label": 1
                },
                {
                    "sent": "So what we do here actually is user fast X model and a fast X vector an boost it with pre trained version.",
                    "label": 0
                },
                {
                    "sent": "Sick.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kind of features are actually the representation of the taxonomy of types over the single extractor.",
                    "label": 0
                },
                {
                    "sent": "Or in case there is not of the class hierarchy from Wikidata.",
                    "label": 1
                },
                {
                    "sent": "This is a very easy simple taxonomy.",
                    "label": 0
                },
                {
                    "sent": "We have a place organization in person and then a city and mountain as subclasses of place and actor and musician as subclasses of a person.",
                    "label": 0
                },
                {
                    "sent": "What we do is we look level by level and we perform one hot encoding.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the level 4 representing the type vector, we concatenate the path of surpassing level by level until arriving to the last one.",
                    "label": 1
                },
                {
                    "sent": "So for musician we concatenate person and musician.",
                    "label": 0
                },
                {
                    "sent": "When the prediction stops at one of Upper the upper level, we just add padding in order to complete the.",
                    "label": 0
                },
                {
                    "sent": "A feature of our representation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, uh III convented the feature is the entity features and actually represent the similarities between two entities.",
                    "label": 0
                },
                {
                    "sent": "We identified the five kind of similarities.",
                    "label": 0
                },
                {
                    "sent": "If they have the same UI.",
                    "label": 0
                },
                {
                    "sent": "If they have similar labels.",
                    "label": 0
                },
                {
                    "sent": "If we have similar abstract.",
                    "label": 0
                },
                {
                    "sent": "If the two person of the same occupation, this is valid just for person actually, and if there is a such structural similarities, that is the shortest distance.",
                    "label": 0
                },
                {
                    "sent": "Between two entities in DB Pedia in Wikidata, sorry.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For now, we can realize this similarity.",
                    "label": 0
                },
                {
                    "sent": "We computed similarities between each couple of extracted entities or the one excited by text Razor and 1 / D be spotlighted.",
                    "label": 0
                },
                {
                    "sent": "One of the X Rays only one Dover, Del, etc and we concatenate everything in a single vector.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last kind of feature.",
                    "label": 0
                },
                {
                    "sent": "Our score features that actually represented the scores that extractor giving output for indicated their confidence or the salience of that predicted entity.",
                    "label": 0
                },
                {
                    "sent": "So use those features for for training and two neural networks, one for temple recognition, one for this immigration.",
                    "label": 0
                },
                {
                    "sent": "So then.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cable network for type of recognition, or enter as.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The goal of answering the question which of those types is the right one?",
                    "label": 1
                },
                {
                    "sent": "We run this network one for each token.",
                    "label": 1
                },
                {
                    "sent": "Now I'm going to describe that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Need to the structure on top.",
                    "label": 0
                },
                {
                    "sent": "We have this alignment blocks that basically connects the input or two.",
                    "label": 0
                },
                {
                    "sent": "This yellow set of nodes that have the same dimensionality of the typeset.",
                    "label": 0
                },
                {
                    "sent": "We call them alignment block because if we cut the network at this point they basically do the mapping that we wanted to avoid.",
                    "label": 0
                },
                {
                    "sent": "For us the good the difference of this approach is better of other mappings is that the network is.",
                    "label": 0
                },
                {
                    "sent": "One, and so these mappings are.",
                    "label": 0
                },
                {
                    "sent": "Well, go straight.",
                    "label": 0
                },
                {
                    "sent": "There are optimized to answer the task over the entire network.",
                    "label": 0
                },
                {
                    "sent": "Moreover, separating the different blocks helps us to not give a different importance biased by the dimensionality of each input, so that input with different dimensionality entering the network in exactly the same way.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yellow blocks are then concatenated and entering this dense layer that connects them to the output.",
                    "label": 0
                },
                {
                    "sent": "The output has the same dimensionality of the final typeset.",
                    "label": 0
                },
                {
                    "sent": "And then obviously those are basically scores of that helps us to understand which one is the strongest candidate for the type of recognition.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second network is for disambiguation.",
                    "label": 0
                },
                {
                    "sent": "Use it together with a voting mechanism.",
                    "label": 0
                },
                {
                    "sent": "As you can see, there is just one up to output liar.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be cause this because the question that we want to answer with this network is is this candidate entity the right one?",
                    "label": 0
                },
                {
                    "sent": "So we run this network once for each candidate and candidate for us is each distinct entity that has been extracted by the different extractor for a specific token.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we have two kind of features in input.",
                    "label": 0
                },
                {
                    "sent": "One is the entity features that actually in this case are the similarities between the candidate and each extracted one, and the type features that have their input into the network with the same alignment block of enter.",
                    "label": 1
                },
                {
                    "sent": "Then there are two dense layers that connects to the unique.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Output node.",
                    "label": 0
                },
                {
                    "sent": "So the voting mechanism is basically random after an network run, so every candidate has been processed and we choose the the output and error on value that have the maximum score, and then we further evaluate understanding if it's surpass a certain threshold.",
                    "label": 0
                },
                {
                    "sent": "That means that is a good or bad candidate.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results we evaluated this strategies.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three different Grand Theft corpora with different languages with different disambiguating targets, and this corpora latassa to evaluate both nerd and type assignment with different type set.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our assemble method over outperforms all the single extractor in every metrics for timber recognition.",
                    "label": 0
                },
                {
                    "sent": "We did also this experiment in switching off the some features to see what is the impact that each feature, as in the final score.",
                    "label": 0
                },
                {
                    "sent": "And even if most information is contained in the type only.",
                    "label": 0
                },
                {
                    "sent": "Features the other feature are still crucial to have results that are this good for.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something disambiguation we did the same comparison.",
                    "label": 0
                },
                {
                    "sent": "We have a better results than the single extractor and also in this case we need both the entity and the type features in order to have the best results.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also compared them.",
                    "label": 0
                },
                {
                    "sent": "We order a kind of extract or that we found in the jar Bill.",
                    "label": 0
                },
                {
                    "sent": "Also in this case we have very good results even if there are some 0 scores that are a bit weird.",
                    "label": 0
                },
                {
                    "sent": "Probably those extractor where just disconnected at the time we did win Aranda experiments so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I presented two neural networks with different strategies that outperforms the.",
                    "label": 0
                },
                {
                    "sent": "A single extractor that they put it together in the ensemble.",
                    "label": 0
                },
                {
                    "sent": "Moreover, this network proves that we need more than just a single type of features for predicting the very the very good.",
                    "label": 0
                },
                {
                    "sent": "Entity and type.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As future works we want to add new extractor to the assemble method.",
                    "label": 0
                },
                {
                    "sent": "Specifically, I'm thinking about Elmo, an flair that actually are trending now.",
                    "label": 0
                },
                {
                    "sent": "Also, we wanted to make more experiments by using a recurrent neural network and LTM, especially for a surface form features, and we want to try Wikidata embedding and also we plan to announce the next Gen TV corpus that has a corpus as being realized.",
                    "label": 0
                },
                {
                    "sent": "For actually this experiment, by adding a new.",
                    "label": 0
                },
                {
                    "sent": "The annotation.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you for attention.",
                    "label": 0
                },
                {
                    "sent": "I leave you with the code for the repository linked to the repository of this experiment and the link to this slide.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to answer your questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you Paula.",
                    "label": 0
                }
            ]
        }
    }
}