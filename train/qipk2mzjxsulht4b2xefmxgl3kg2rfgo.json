{
    "id": "qipk2mzjxsulht4b2xefmxgl3kg2rfgo",
    "title": "Aligning Sense Inventories in Wikipedia and Wordnet",
    "info": {
        "author": [
            "Elisabeth Wolf, Ubiquitous Knowledge Processing Lab, Darmstadt University of Technology"
        ],
        "published": "June 7, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Information Extraction"
        ]
    },
    "url": "http://videolectures.net/akbc2010_wolf_asiww/",
    "segmentation": [
        [
            "My name is Elizabeth and I am a PhD student at the UWP lab.",
            "At the Dumpster University in Germany, and while I'm very happy that I got the possibility to give a talk here in front of such professional audience, the title of my Talk is aligning since inventories in Wikipedia and Word Net.",
            "So as we already heard yesterday, many ahnapee task rely on sense or word sense information.",
            "So for example, in the task of food, since this immigration or the calculation of semantic relatedness or also fields like machine."
        ],
        [
            "Translation and semantic search, and for around for the last 25 years, Birdnet was often utilized to gather sense information.",
            "But a few years ago, Wikipedia turned also to a valuable resource in the NLP community.",
            "Um?",
            "And actually, it could be used to get comparable results in many tasks, or even sometimes better results."
        ],
        [
            "So what?",
            "Net is very famous for its precisely defined taxonomy, but?"
        ],
        [
            "But that contains only a few or a limited amount of textual information, so this textual information, I mean, for example, the clauses or the definition of each word sense in burtnett and also it somehow it limited in the size of since cover which.",
            "So on the other side, Wikipedia don't have."
        ],
        [
            "Precisely defined taxonomy.",
            "I mean, a lot of approaches use the category system to infer the taxonomical information, but the categories are rather sloppy defined.",
            "But on the other side, Wikipedia has huge amount of textual information and also covers at least in the English version, 3 million concepts.",
            "And in addition to that."
        ],
        [
            "Wikipedia also comes with multi lingual information, so when you see this picture here.",
            "Our motivation is here to."
        ],
        [
            "Bind both resources to research that actually provide all of these properties and."
        ],
        [
            "I am at doing this on the sensor on the word sense level.",
            "Because our motivation is to build a better word sense inventory that have actually 2 main benefits.",
            "So for sensors which occurs in both resources, we can get a better sense representation.",
            "And for sensors which only occurs in one of the.",
            "Or this fact that census occur only in one of the two resources increases suspense coverage."
        ],
        [
            "So just to give you an example for the known damper we have three sunsets of three senses or meanings in Word net."
        ],
        [
            "And the first 2 meanings can be also find in Wikipedia.",
            "So like.",
            "Now you can be the temper movable.",
            "I would've played in the other one which holds more.",
            "Could be connected to the article which is which have the article title muffler, but what we see here also is."
        ],
        [
            "That word, net.",
            "Contains another sunset or another sense which is not part of Wikipedia and on the other side Wikipedia has."
        ],
        [
            "Additional sensors which do not occur in Fortnite.",
            "So if you can combine both resources on the word sense level, we can actually increase the sense coverage."
        ],
        [
            "Of course there are.",
            "There are many approaches currently that aim at aligning both sense and birdnet in Wikipedia.",
            "But most of them actually focus on building an ontology.",
            "So for example the Argo.",
            "So they focus on the category system and infobox classes to match them to burtnett sunsets.",
            "And we are only about two other works that actually map directly sensor.",
            "So I mean Wikipedia are tickets to other worthless sunsets.",
            "So this is 1 approach by rules Casado, which map English or simple English Wikipedia toward.",
            "Net synsets and also maccia.",
            "Used Wikipedia as a source of sense annotated data to manually or they were mapped in their experience.",
            "Wikipedia articles toward net sunsets."
        ],
        [
            "So our approach here is to 1st analyze these sense coverage of a combined resource and to also analyze the sense complementarity issue and we generated the data sets, which is a human annotation in.",
            "This gave us some insights for the fully automatic alignment of both resources.",
            "So we."
        ],
        [
            "Proposal two step approach.",
            "In the first step is to if you have given a word net sense, since it is to expect all potential sense mapping pairs.",
            "So for example on the left side you have witnessed sunsets or one word net synsets.",
            "They talk about the doctor or medical physician and on the right side right side we expect all Wikipedia ticket that we can find for these sunset words.",
            "So we have a sense physician with.",
            "Actually would fit to the sense, but we have also have Wikipedia article about the title Doctor or the Doctor."
        ],
        [
            "Title so first we have to expect the sensors and in the second step we have to classify them on to some degree of filters.",
            "Sensors with sense peers actually."
        ],
        [
            "So just another slide.",
            "How to expect these candidates so we always consider all birdnet.",
            "All words that appear in the sunset.",
            "So in this example we have 3 words.",
            "Handwriting hands and script, and we check in Wikipedia if we have particles that have titles that match these words or articulus.",
            "However, we direct.",
            "So for example, for this, since it amongst many others we have we're tickled script, typefaces, script, typefaces, comics and also penmanship is.",
            "This article has redirect handwriting.",
            "Just to give you some examples based on.",
            "The 80,000 nonsense it's inverted.",
            "For 15% of them we could not find any Wikipedia tickle based on this expect method for 1/3 of these nonsense that we could find one Wikipedia article, but Now the interesting thing is, if the sensors of these pairs match and for half of them we could find more than one Wikipedia ticker.",
            "So."
        ],
        [
            "Um, in the disintegration step?",
            "Be just to get some insights for the future automatic alignment step.",
            "We randomly sampled 38 sunsets which leads to 300 potential sense pairs which have to be annotated.",
            "And on average person said we expected around 8 Wikipedia tickets.",
            "And for the annotation task, wait for human annotators and they had to label the.",
            "And both sense path either being the same sense or not.",
            "And the annotations are quite reliable as Internet annotator agreement is quite high, so the media."
        ],
        [
            "Party of Annotators decided that 21 sunsets.",
            "Um or they annotated that 21 sunsets share a sense in Wikipedia, so they were aligned as being the same sense in 17 sunsets were not or could not align to any Wikipedia article.",
            "We check these 17 sunsets manually and try to find them in Wikipedia, so too actually.",
            "And evaluate or exception method.",
            "So for sunsets could be found in a disambiguation page, but just as an entry in a disambiguation page, while those then an article and two sunsets could not be aligned as the appropriate Wikipedia article could not be extracted.",
            "So just."
        ],
        [
            "Comment on this point.",
            "For example, for the sunset bandwagon, popular trend that attracts growing support the appropriate article.",
            "In Wikipedia's bandwagon effect.",
            "So what we do in our extended projects that we also consider the link labels in Wikipedia.",
            "So every link in Wikipedia comes with the link labels.",
            "So the word that appears in the text here it would be benefiting, and the link anchor links to the actual article.",
            "So here we can of course, hopefully increase the recall of expected candidates."
        ],
        [
            "And the remaining sunsets are not in Wikipedia, so this actually shows that the fence coverage can be increased when combining both resources."
        ],
        [
            "So to conclude, we.",
            "Um could show that the Wikipedia article level and birdnet sunset level is a good level of kernel arity to combine both resources, and these combination has two main benefits.",
            "I already presented them and.",
            "Yeah, for the two step approach, the extraction method should also process disambiguation pages, but we are working on this as the disambiguation pages are not really well structured even in one language Wikipedia.",
            "And yeah, the disintegration study have shown that we have a high internal data agreement and that's actually motivates us to automatically perform the alignment on sense level so.",
            "For the future work."
        ],
        [
            "Actually, what we already did it we generated a larger data set to evaluate or experiments, or our alignments, and we started to perform automatic alignments.",
            "Right now we use simple text similarity measures, so we have the Wordnet Synset definition and public ipea text or the Wikipedia first paragraph and then we calculate similarity to infer if they actually.",
            "Talk about the same sense or not, and the long term goal is to integrate other resources like other, since inventories, dictionaries or for example other types of alignment like they are represented in the album.",
            "So thank you."
        ],
        [
            "So much and I think we have still one or two minutes for questions."
        ],
        [
            "Yep.",
            "So."
        ],
        [
            "If we want to find Wikipedia match for this sunset bandwagon.",
            "We can't just.",
            "Retrieve all articles that have a title, including the bandwagon, because then it would be a very high recall and it's difficult to do some great.",
            "So what we do is we check all link links in the HTML source files of Wikipedia so this link occurs with the label when checking on the text, those is actually fits exactly the words in this sunset.",
            "And this things thanks to the article, which is actually the right articular.",
            "Does.",
            "Sometimes major difference between labor and type.",
            "So that sometimes this.",
            "I felt that there was no relation at all.",
            "OK, I OK.",
            "I should say.",
            "I should say we had to to set a threshold so we only consider these link labor and clinker combinations say across at least three times and Wikipedia.",
            "And there's some how.",
            "The feeling that the quality is better.",
            "So of course, if you take all in claims and the combined link anchor.",
            "This many it's it's actually very noisy.",
            "OK, so thank you very much again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Elizabeth and I am a PhD student at the UWP lab.",
                    "label": 0
                },
                {
                    "sent": "At the Dumpster University in Germany, and while I'm very happy that I got the possibility to give a talk here in front of such professional audience, the title of my Talk is aligning since inventories in Wikipedia and Word Net.",
                    "label": 1
                },
                {
                    "sent": "So as we already heard yesterday, many ahnapee task rely on sense or word sense information.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the task of food, since this immigration or the calculation of semantic relatedness or also fields like machine.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Translation and semantic search, and for around for the last 25 years, Birdnet was often utilized to gather sense information.",
                    "label": 1
                },
                {
                    "sent": "But a few years ago, Wikipedia turned also to a valuable resource in the NLP community.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And actually, it could be used to get comparable results in many tasks, or even sometimes better results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "Net is very famous for its precisely defined taxonomy, but?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But that contains only a few or a limited amount of textual information, so this textual information, I mean, for example, the clauses or the definition of each word sense in burtnett and also it somehow it limited in the size of since cover which.",
                    "label": 0
                },
                {
                    "sent": "So on the other side, Wikipedia don't have.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Precisely defined taxonomy.",
                    "label": 0
                },
                {
                    "sent": "I mean, a lot of approaches use the category system to infer the taxonomical information, but the categories are rather sloppy defined.",
                    "label": 0
                },
                {
                    "sent": "But on the other side, Wikipedia has huge amount of textual information and also covers at least in the English version, 3 million concepts.",
                    "label": 0
                },
                {
                    "sent": "And in addition to that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wikipedia also comes with multi lingual information, so when you see this picture here.",
                    "label": 0
                },
                {
                    "sent": "Our motivation is here to.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bind both resources to research that actually provide all of these properties and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I am at doing this on the sensor on the word sense level.",
                    "label": 1
                },
                {
                    "sent": "Because our motivation is to build a better word sense inventory that have actually 2 main benefits.",
                    "label": 1
                },
                {
                    "sent": "So for sensors which occurs in both resources, we can get a better sense representation.",
                    "label": 0
                },
                {
                    "sent": "And for sensors which only occurs in one of the.",
                    "label": 0
                },
                {
                    "sent": "Or this fact that census occur only in one of the two resources increases suspense coverage.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to give you an example for the known damper we have three sunsets of three senses or meanings in Word net.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the first 2 meanings can be also find in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So like.",
                    "label": 0
                },
                {
                    "sent": "Now you can be the temper movable.",
                    "label": 0
                },
                {
                    "sent": "I would've played in the other one which holds more.",
                    "label": 0
                },
                {
                    "sent": "Could be connected to the article which is which have the article title muffler, but what we see here also is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That word, net.",
                    "label": 0
                },
                {
                    "sent": "Contains another sunset or another sense which is not part of Wikipedia and on the other side Wikipedia has.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Additional sensors which do not occur in Fortnite.",
                    "label": 0
                },
                {
                    "sent": "So if you can combine both resources on the word sense level, we can actually increase the sense coverage.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course there are.",
                    "label": 0
                },
                {
                    "sent": "There are many approaches currently that aim at aligning both sense and birdnet in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "But most of them actually focus on building an ontology.",
                    "label": 0
                },
                {
                    "sent": "So for example the Argo.",
                    "label": 0
                },
                {
                    "sent": "So they focus on the category system and infobox classes to match them to burtnett sunsets.",
                    "label": 1
                },
                {
                    "sent": "And we are only about two other works that actually map directly sensor.",
                    "label": 0
                },
                {
                    "sent": "So I mean Wikipedia are tickets to other worthless sunsets.",
                    "label": 1
                },
                {
                    "sent": "So this is 1 approach by rules Casado, which map English or simple English Wikipedia toward.",
                    "label": 0
                },
                {
                    "sent": "Net synsets and also maccia.",
                    "label": 0
                },
                {
                    "sent": "Used Wikipedia as a source of sense annotated data to manually or they were mapped in their experience.",
                    "label": 1
                },
                {
                    "sent": "Wikipedia articles toward net sunsets.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our approach here is to 1st analyze these sense coverage of a combined resource and to also analyze the sense complementarity issue and we generated the data sets, which is a human annotation in.",
                    "label": 0
                },
                {
                    "sent": "This gave us some insights for the fully automatic alignment of both resources.",
                    "label": 1
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Proposal two step approach.",
                    "label": 0
                },
                {
                    "sent": "In the first step is to if you have given a word net sense, since it is to expect all potential sense mapping pairs.",
                    "label": 0
                },
                {
                    "sent": "So for example on the left side you have witnessed sunsets or one word net synsets.",
                    "label": 0
                },
                {
                    "sent": "They talk about the doctor or medical physician and on the right side right side we expect all Wikipedia ticket that we can find for these sunset words.",
                    "label": 0
                },
                {
                    "sent": "So we have a sense physician with.",
                    "label": 0
                },
                {
                    "sent": "Actually would fit to the sense, but we have also have Wikipedia article about the title Doctor or the Doctor.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Title so first we have to expect the sensors and in the second step we have to classify them on to some degree of filters.",
                    "label": 0
                },
                {
                    "sent": "Sensors with sense peers actually.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just another slide.",
                    "label": 0
                },
                {
                    "sent": "How to expect these candidates so we always consider all birdnet.",
                    "label": 1
                },
                {
                    "sent": "All words that appear in the sunset.",
                    "label": 0
                },
                {
                    "sent": "So in this example we have 3 words.",
                    "label": 0
                },
                {
                    "sent": "Handwriting hands and script, and we check in Wikipedia if we have particles that have titles that match these words or articulus.",
                    "label": 0
                },
                {
                    "sent": "However, we direct.",
                    "label": 0
                },
                {
                    "sent": "So for example, for this, since it amongst many others we have we're tickled script, typefaces, script, typefaces, comics and also penmanship is.",
                    "label": 0
                },
                {
                    "sent": "This article has redirect handwriting.",
                    "label": 1
                },
                {
                    "sent": "Just to give you some examples based on.",
                    "label": 0
                },
                {
                    "sent": "The 80,000 nonsense it's inverted.",
                    "label": 0
                },
                {
                    "sent": "For 15% of them we could not find any Wikipedia tickle based on this expect method for 1/3 of these nonsense that we could find one Wikipedia article, but Now the interesting thing is, if the sensors of these pairs match and for half of them we could find more than one Wikipedia ticker.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um, in the disintegration step?",
                    "label": 0
                },
                {
                    "sent": "Be just to get some insights for the future automatic alignment step.",
                    "label": 0
                },
                {
                    "sent": "We randomly sampled 38 sunsets which leads to 300 potential sense pairs which have to be annotated.",
                    "label": 1
                },
                {
                    "sent": "And on average person said we expected around 8 Wikipedia tickets.",
                    "label": 0
                },
                {
                    "sent": "And for the annotation task, wait for human annotators and they had to label the.",
                    "label": 1
                },
                {
                    "sent": "And both sense path either being the same sense or not.",
                    "label": 1
                },
                {
                    "sent": "And the annotations are quite reliable as Internet annotator agreement is quite high, so the media.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Party of Annotators decided that 21 sunsets.",
                    "label": 0
                },
                {
                    "sent": "Um or they annotated that 21 sunsets share a sense in Wikipedia, so they were aligned as being the same sense in 17 sunsets were not or could not align to any Wikipedia article.",
                    "label": 0
                },
                {
                    "sent": "We check these 17 sunsets manually and try to find them in Wikipedia, so too actually.",
                    "label": 0
                },
                {
                    "sent": "And evaluate or exception method.",
                    "label": 0
                },
                {
                    "sent": "So for sunsets could be found in a disambiguation page, but just as an entry in a disambiguation page, while those then an article and two sunsets could not be aligned as the appropriate Wikipedia article could not be extracted.",
                    "label": 0
                },
                {
                    "sent": "So just.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comment on this point.",
                    "label": 0
                },
                {
                    "sent": "For example, for the sunset bandwagon, popular trend that attracts growing support the appropriate article.",
                    "label": 0
                },
                {
                    "sent": "In Wikipedia's bandwagon effect.",
                    "label": 0
                },
                {
                    "sent": "So what we do in our extended projects that we also consider the link labels in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So every link in Wikipedia comes with the link labels.",
                    "label": 0
                },
                {
                    "sent": "So the word that appears in the text here it would be benefiting, and the link anchor links to the actual article.",
                    "label": 0
                },
                {
                    "sent": "So here we can of course, hopefully increase the recall of expected candidates.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the remaining sunsets are not in Wikipedia, so this actually shows that the fence coverage can be increased when combining both resources.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, we.",
                    "label": 0
                },
                {
                    "sent": "Um could show that the Wikipedia article level and birdnet sunset level is a good level of kernel arity to combine both resources, and these combination has two main benefits.",
                    "label": 1
                },
                {
                    "sent": "I already presented them and.",
                    "label": 1
                },
                {
                    "sent": "Yeah, for the two step approach, the extraction method should also process disambiguation pages, but we are working on this as the disambiguation pages are not really well structured even in one language Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "And yeah, the disintegration study have shown that we have a high internal data agreement and that's actually motivates us to automatically perform the alignment on sense level so.",
                    "label": 0
                },
                {
                    "sent": "For the future work.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, what we already did it we generated a larger data set to evaluate or experiments, or our alignments, and we started to perform automatic alignments.",
                    "label": 0
                },
                {
                    "sent": "Right now we use simple text similarity measures, so we have the Wordnet Synset definition and public ipea text or the Wikipedia first paragraph and then we calculate similarity to infer if they actually.",
                    "label": 1
                },
                {
                    "sent": "Talk about the same sense or not, and the long term goal is to integrate other resources like other, since inventories, dictionaries or for example other types of alignment like they are represented in the album.",
                    "label": 1
                },
                {
                    "sent": "So thank you.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So much and I think we have still one or two minutes for questions.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we want to find Wikipedia match for this sunset bandwagon.",
                    "label": 0
                },
                {
                    "sent": "We can't just.",
                    "label": 0
                },
                {
                    "sent": "Retrieve all articles that have a title, including the bandwagon, because then it would be a very high recall and it's difficult to do some great.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we check all link links in the HTML source files of Wikipedia so this link occurs with the label when checking on the text, those is actually fits exactly the words in this sunset.",
                    "label": 0
                },
                {
                    "sent": "And this things thanks to the article, which is actually the right articular.",
                    "label": 0
                },
                {
                    "sent": "Does.",
                    "label": 0
                },
                {
                    "sent": "Sometimes major difference between labor and type.",
                    "label": 0
                },
                {
                    "sent": "So that sometimes this.",
                    "label": 0
                },
                {
                    "sent": "I felt that there was no relation at all.",
                    "label": 0
                },
                {
                    "sent": "OK, I OK.",
                    "label": 0
                },
                {
                    "sent": "I should say.",
                    "label": 0
                },
                {
                    "sent": "I should say we had to to set a threshold so we only consider these link labor and clinker combinations say across at least three times and Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "And there's some how.",
                    "label": 0
                },
                {
                    "sent": "The feeling that the quality is better.",
                    "label": 0
                },
                {
                    "sent": "So of course, if you take all in claims and the combined link anchor.",
                    "label": 0
                },
                {
                    "sent": "This many it's it's actually very noisy.",
                    "label": 0
                },
                {
                    "sent": "OK, so thank you very much again.",
                    "label": 0
                }
            ]
        }
    }
}