{
    "id": "7jddle3xsr4j6eob6jjsmnvsdqd5ld6c",
    "title": "Nonparametric Tests between Distributions",
    "info": {
        "author": [
            "Alex Smola, Amazon"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "October 2005",
        "category": [
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/mcslw04_smola_ntbd/",
    "segmentation": [
        [
            "Also maxi so so.",
            "A lot of things will look for media.",
            "We use it for slightly different tests so but that's pretty much your idea.",
            "Well, I guess that's part of the reason.",
            "Yeah, so this is joint work.",
            "The whole bunch of cards."
        ],
        [
            "Sorry, now what's the setting that we look at?",
            "Well, you might want to do something that well in one way you could just call property testing for distributions.",
            "In other words, well given some observations, there's a certain property hold for random variables like, well, are they independent?",
            "Are they distributed according to the same distribution?",
            "Does the support of the distribution have?",
            "Does it intersect or similar things like that?",
            "So for this joint support, I mean people are doing that a lot.",
            "I mean, this is people typically refer to it as the classification problem, so they would want to test with a.",
            "Classified data sets well.",
            "And of course, that also means that if you can classify with very low error or Syria, then there's a high chance that the distributions have disjoint support memory.",
            "Those of class one and class minus one for independence.",
            "Well, there are lots of criteria that you could use to check whether 2 random variables are independent.",
            "We have one that essentially what it does is it starts with a quite convenient linear criterion.",
            "And then we map all this into Hilbert space and then get necessary and sufficient conditions.",
            "And well, it actually works amazingly well.",
            "Be giving you the theoretical explanation on how to do this and then the end and this is.",
            "Not published yet?",
            "Well, if you want to check with the two sets of observations are drawn from the same distribution you want to have a nice nonparametric test, so probably one of the famous ones is the Kolmogorov Smirnov test and derivatives thereof, as it actually turns out this criterion is a contains the Kolmogorov Smirnov test is a special case, and we get the.",
            "Analysis Just the same way as we would have done what deal did were also for independence.",
            "So well, let's look at it in a bit more detail to see what I actually mean by that.",
            "So for the support, well given data sets well."
        ],
        [
            "13 exam drawn from X probability of distribution of X&Y one through Y.",
            "In going from this distribution of why well we might want to test with the supports.",
            "This section supports vanish is or for independence well, given pairs X&Y.",
            "Well, test whether the probability distribution in X&Y factorizes.",
            "Or for identity well drawn X one through XM from PR efix an corresponding whilst on from probability of why we want to check whether these two distributions are actually the same.",
            "Of course, you will never ever be able to.",
            "Really.",
            "Check with certainty whether that's the case.",
            "I mean, you could always construct distributions that will fool you into believing that the two distributions are the same, even though they are not.",
            "Or they're trying to.",
            "Variables are independent even though they're not.",
            "At least we want to get the hypothesis test, which as we get large sample size, we will be able to reject the null hypothesis of the distributions being independent random variables being independent or the distributions being the same in a sensible way.",
            "What's the difference to computer science style property testing?",
            "Well, there people are usually really fast about.",
            "Particular finite domains, they usually assume that generating new data is very cheap, and.",
            "Thing that the key issue is actually to, well, find an algorithm that runs really fast.",
            "So you can be wasteful with the data, but you have to be very greedy with the computations.",
            "In our case, it's the other way around.",
            "OK, well why would you want to use that well?"
        ],
        [
            "One application would be more independent component analysis.",
            "Will the usual thing like givenness set of in audio signals find independent sources of the recording?",
            "You might use that for EG data to denoise it in some way.",
            "You can apply this to neuroscience well, cosmology removing background radiation would ever for statistical modeling.",
            "While you can use such tests in graphical models, if you know which random variables are independent.",
            "While you can design your graph based on that, you can select variables like for an estimation problem, short estimate, why give mix.",
            "Well, if X if some coordinate exercise independent of why you might as well throw it out.",
            "For the distribution testing well, one application would be.",
            "You might want to merge databases, say the health insurance records for find hovinen, those of Amsterdam and maybe they're sitting on different servers.",
            "And of course you only want to merge them if they have the same statistical behavior.",
            "So you want to have a cheap test to find out about that.",
            "Or combine experiments from different sources.",
            "So if you're doing microarray measurements and you have one team of doctors in one city in a team of doctors in another and they have different equipment.",
            "And you want to actually use the combination of both for the estimates for you want to find out where that is sensible or fraud detection like somebody wants to pass off, well, home painting drawing as a Picasso.",
            "Well, you'd want to have some device to test that.",
            "So what's the key strategy in all those three examples of?"
        ],
        [
            "Mention and why am I mentioning all three at once?",
            "Well, essentially what we do is we have a simple linear criterion and linear is to be taken with a grain of salt.",
            "So we have some sort of linear witness of dependence of this agreement, which is easy to compute.",
            "But of course, since it's just a simple linear thing, I mean it will typically not give us necessary and sufficient implications.",
            "And the trick is thin to use suitable non linearisation.",
            "To extend this to linear function space is typically in a reproducing kernel space, but it could be any other backspace and then to find an efficient parameterisation for this test such that I can Even so I'm now looking at a whole class of nonlinear objects.",
            "Still, I'm still able to compute this efficiently.",
            "And then you go and find an empirical approximation.",
            "Then in the end, yes, you do the statistical analysis.",
            "So you look at the expectations you look at.",
            "The tails show that The thing is concentrated, give corresponding bounds and well that gives you the test.",
            "So that's the general strategy.",
            "And, well, let's just begin with one instance for that.",
            "You've definitely seen, namely the test for disjoint support.",
            "So let's say we've got X&Y drawn from 2 distributions.",
            "I want to check with the."
        ],
        [
            "Supports are disjoint.",
            "If this is on a finite domain.",
            "I mean, if you just look for enough that I mean as soon as you find the collusion, you know, well, game over.",
            "But if the domain is not finite.",
            "Thin.",
            "You have to do something slightly different because it's highly unlikely.",
            "If you have, say, alright and some.",
            "Non atomic distribution that you will find the same entrance instance twice.",
            "Now of course a linear witness, and that's quite a simple thing for would be that if I can find some linear function F of X being W transpose X such that F of X is less than F of Y for all X&Y pairs.",
            "Well, if I can find such a function, well, that is clearly a proof that the data is linearly separable.",
            "Remember, that's basically what you would do in a support vector machine.",
            "I need within, say, well, actually pick this threshold here to be 0, but actually I wouldn't really have to.",
            "Now a nonlinear witness.",
            "And that's the necessary and sufficient is, if there exists some 01 bounded function such that this condition holds.",
            "So actually Bioresource lemma if the closures of the supports are disjoint and you can find a.",
            "Continuous 01 bounded function that satisfies this.",
            "Now unfortunately, will we?",
            "Always have will always be able to find a nonlinear witness.",
            "If I have finite sample size, so too.",
            "Explain that point.",
            "Well, that data looks quite."
        ],
        [
            "Actually linearly separable.",
            "Tom."
        ],
        [
            "I forget something like this.",
            "Well, it's pretty much up to us to argue with.",
            "This is non linearly separable or not.",
            "I mean, because this could be just that the region around here is quite noisy, in which case it's not linearly separable, or it could be that, well, the separating.",
            "Surface is just really complicated."
        ],
        [
            "I mean, that's.",
            "But it could also be that this is the best thing you can do.",
            "And so then the question boils down to how reliable is this?"
        ],
        [
            "Yes, that we're producing.",
            "Now, this is actually a problem that people have studied a lot in machine learning, so I'm not going to go into details about rates and so on, because I mean this is just really re phrasing existing results in new address.",
            "So essentially what we end up doing is we solve noise free binary classification problem.",
            "And then, well, we just go and take our favorite uniform convergence bounds, depending on what estimator I take so I can have some bounded, we see that mentioned it would take some of development concerning KISS results I could use rather more averages localized rather more averages.",
            "What have you?",
            "And then I use reciting them to test the hypothesis of separability.",
            "So in other words, I would want to have a test which tells me that if I can separate.",
            "Well, perfectly on my training data, what's the probability that the error on the test set will exceed some threshold epsilon, and if I pick that threshold epsilon?",
            "Suitably I will get the test that at least if the support had measure larger than some epsilon, then well I would be able to pick it up.",
            "So for instance, a computational solution for that would be to use an SVM hard margin optimizer.",
            "So you need to run a quadratic program and it will do the trick.",
            "It's not the only.",
            "Device, but that would be quite convenient.",
            "I mean, so far this is.",
            "Just re phrasing things, but."
        ],
        [
            "The point that I wanted to drive home with this is, well, you have some problem, so some property.",
            "It's easy to find some linear criterion.",
            "It's not so easy to find the nonlinear criterion unless we do some normalization in between, which turns the problem into something convenient, and then in the end, while we apply our statistical bounds to that.",
            "Now this is where things get a bit more interesting.",
            "Let's say I have pairs of random variables X&Y.",
            "They want to test with a distribution factorizes.",
            "Now a linear witness for X&Y be independent.",
            "Is well if for some linear function F. Www.transpose XNGV transpose Y so covariance deviates from zero.",
            "That's the case.",
            "Well, I know the random variables cannot be independent.",
            "Now a non necessary and sufficient condition is clearly not justify linear functions, but if I also pick some 01 bounded functions, FNG and I say well if the covariance for those functions.",
            "Is greater than zero, then obviously.",
            "This tells me that the random variables are independent of each other.",
            "So really actually uses this as a test for independence.",
            "Why's that nice?",
            "Because it does not require us to actually build a model of the distributions of X&Y or the joint distribution in X&Y.",
            "All I have to do is I just have to check some reasonably simple criterion.",
            "Unfortunately, for finite sample sizes, we can almost always find some nonlinear witness such that the empirical covariance exceeds 0.",
            "In other words, you have to construct really pathological examples for which the empirical covariance will not exceed 0.",
            "So.",
            "These, for instance, could be."
        ],
        [
            "Independent random variables.",
            "Now these.",
            "I'm claiming our dependent random variables mean doesn't look much different.",
            "Or maybe we were just unlucky.",
            "So I mean, if you just eyeball it, it will be not so easy to see that these are in fact dependent random variables.",
            "Essentially what's happened is I've taken a uniform distribution and slanted it a little bit.",
            "See look at it.",
            "You can see some parallelogram here.",
            "OK but yeah."
        ],
        [
            "It's hard to tell, just.",
            "By yes.",
            "Yeah, so that would be X.",
            "That would be Y and I've just plotted the pairs of X&Y.",
            "And all these random variables are dependent.",
            "Slightly, but they are."
        ],
        [
            "In other words, you can see there's a little bit of a trend.",
            "In other words, large X is correspond to slightly larger vice.",
            "So we can do a little bit of a prediction here, and that of course means that the random variables are dependent."
        ],
        [
            "So how do we get our hands on that?",
            "And, well, let's pull out the trusty reproducing kernel spaces.",
            "So let's define kernels K of X&X, prime alavoine Y prime on X&Y respectively, so they don't need to be the same type, so for instance X could be texts and why could be sound?",
            "Or why could be images or whatever, so I mean, there's absolutely no requirement that these actually need to be of the same type.",
            "And Furthermore, well, this is a nice thing about representative spaces that the evaluation function is a linear operator.",
            "So in other words, a fix can be written as inner product between F&K of X and dot.",
            "Moreover, I will assume that K&L are bounded on the domain.",
            "That's not always the case, but for a lot of things, like RBF kernels, that's the that's true.",
            "And now I just need to define two more operators, namely, I find a mean operator.",
            "And this is a linear map which Maps if.",
            "Into its average on X and likewise G into this average and Y.",
            "You can clearly see these linear operators.",
            "And.",
            "So this way I can define mutex and new line.",
            "So basically what you do is you define.",
            "These means.",
            "Ask the corresponding through the corresponding linear functionals.",
            "Likewise, you can define bilinear forms.",
            "Namely the covariance operators.",
            "As follows that.",
            "The covariance operator applied to F&G.",
            "Is.",
            "This expression here.",
            "Now, if I had independent random variables, this covariance operator would have to vanish.",
            "Because the covariance sustain are zero for all FG.",
            "So now and this is the key trick about it.",
            "I can design a test.",
            "By simply looking.",
            "Four, well, by computing the norm of this covariance operator.",
            "And then I will approximate this value on my finite set of observations and it will show that this approximation has nice statistical properties.",
            "So the strategy is, take this nonlinear take this linear operator, compute its norm, and then find a.",
            "Well, find sample size approximation.",
            "OK, So what I've just said before is so you can show that this is actually necessary and sufficient condition.",
            "If K&L are you."
        ],
        [
            "Virtual kernels.",
            "So that means that.",
            "You can basically approximate any continuous any continuous function.",
            "Then, and only then, X&Y are independent if the covariance operator vanishes.",
            "Now the proofs Kate is well for.",
            "This is really you just say well, if the X&Y are dependent then there must be some functions F star Angie star for which the covariance is greater than 0.",
            "And then, well, you just pick functions from the reproducing kernel Hilbert space, which converge to a star in G star.",
            "And these are epsilon prime approximations.",
            "Such that again, the covariance of the approximation doesn't vanish.",
            "And then consequently, this operator must not be 0.",
            "So this is how you show it.",
            "OK, so let's just write this covariance operator again and the test statistic will then be.",
            "If we call it Hilbert Schmidt independence criterion as we will compute the Hilbert Schmidt norm of that.",
            "Off, namely, this object here.",
            "Now this denotes ascension outer product.",
            "So in order to apply this to some F&G you need to compute the inner product between F and this part and G and that part.",
            "OK, so this is slightly lax notation.",
            "OK.",
            "So how do you come?"
        ],
        [
            "With those norms.",
            "Well, for rank one operators, that's pretty easy.",
            "We know that the Hilbert Schmidt norm off the outer product between F&G.",
            "Squared is just the norm of F squared times the normal G ^2.",
            "Well, think of it like when you compute the Frobenius norm of a matrix which is rank one.",
            "So let's say some matrix is first, then.",
            "Which is just a trace of course of MM transposed, which is then given by the trace.",
            "Of F * G transposed then times G * F transposed.",
            "Now by the fact that this is the trace like it, that this is just G transpose G * F transposed, if which is just the norm of G squared times the norm of F ^2.",
            "And what I've done here for matrices the same thing just works.",
            "In Hilbert space as well.",
            "OK, so now let's write out what this really means.",
            "The normalcy of XY squared still product between those terms.",
            "Why then do is I plug in the expressions that we had for cxy up here?",
            "OK. And then after a little bit of algebra.",
            "I get this expression here where I have the inner product between K&L&K of X prime LY prime and the only difference being the three different types of expectations that I'll be taking.",
            "So yeah, I'm taking the expectation of that expression for the X&Y and experiment why pairs jointly?",
            "Here I'm taking the expectation overall four terms independently, and of course here I have one term where I take one expectation independently and another one jointly.",
            "Now since this.",
            "Just by that expression here gives me the inner product.",
            "I just get various expectations of CFX and ex prime alavoine.",
            "Why prime?",
            "Now that expression is well defined if kendel abounded.",
            "OK, so now we have the quantity that we really after and we know that if we could compute this quantity we could check with the random variables are independent.",
            "So how do we go and estimate this?",
            "What I'm claiming is that this quad."
        ],
        [
            "To hear is a good empirical estimate.",
            "Of that quantity over there.",
            "So now I need to explain what this actually means.",
            "So the recipe is compute the kernel matrix on the axis, compute the kernel matrix on the wise.",
            "And take out the wrong column means.",
            "So this is the projector onto.",
            "Basically, the romin or the column mean.",
            "OK.",
            "Multiply those centric matrices and compute the trace.",
            "An if this quantity is close to 0.",
            "To normalize suitably, then you know that your random variables are most likely independent.",
            "If it's larger than if it's significantly larger than zero, you know there.",
            "Dependant.",
            "OK.",
            "So what you can do is you can show that the expected value of this statistic here.",
            "Is exactly the criterion that we want plus a bias term which behaves like order of 1 / N. As we will see later on the statistical.",
            "Properties of this are like order of one over square root M, so this bias is negligible.",
            "Basically, where this bias comes from is the fact that this some here also can turn contains terms which are just sums in K of XR&XI&L of why I and why I so basically where you have the same argument in both terms.",
            "And so all you do is you really expand this and it's a little bit of a mess if you do it.",
            "In terms of all, all the expressions which don't have any duplicates and all the expressions which do have duplicates and for the latter you can show that they are of order one over him.",
            "And the constant here would be something like a three or four, so it's.",
            "Not very significant.",
            "OK.",
            "So.",
            "Now the last thing that we need to do is."
        ],
        [
            "Need to show that the random variable.",
            "This quantity here itself is also concentrated.",
            "So if dealt with the expectation, now we need to take care of the tail.",
            "And so we use drifting theorem in a slight modification for you statistics, so that should be clear from Shields talk before.",
            "Basically what you do is.",
            "Well this function U is a function.",
            "Is an average over some which is called the kernel of EU statistic, so not to be confused with the Mercer kernel, which depends on the whole on the subset of those random variables.",
            "So it's and this is just multiplying by the combinatorial number of possible combinations in which you can arrange these arguments.",
            "So we will get the standard lifting theorem if these functions G were just functions of 1 random variable, then we would just divide by the number of observations.",
            "Here in the other cases, well, we just need to do our part number of all pairs, triplets or whatever.",
            "And now the nice thing is that just like for independent random variables, also those terms, the average of those terms is concentrated in the same way.",
            "The probability that you exceed the expectation by more than T is bounded by E to the minus two T squared and then OK, I have to pay a price because I took well R tuples.",
            "So basically reduce my sample size.",
            "I have to divide it by R and this is just the range of this G function.",
            "So if you plug in the value one in here, you get the standard 15 theorem back, and otherwise I mean it's just 4.",
            "Larger subsets of random variables.",
            "So then what you do is you just count the individual terms.",
            "Like we in this previous expression.",
            "We have terms which depends on pairs, triplets and quadruples of random variables.",
            "Anne.",
            "You apply 15 theorem for each of them.",
            "Take the union bound over those three terms and after a little bit of algebra this is what you get.",
            "So in other words, what we now have is that.",
            "Our empirical criterion.",
            "Minus the Hilbert Schmidt norm of the covariance operator is bounded by one term, which behaves like log Delta over M ^2 of that and the bias term which behaves like 1 / M. And of course I can use that to derive an independent test for random variables.",
            "Ask, would you put 2 four came from?",
            "Fun.",
            "Well, it came from the fact that, well, you have to bound for the pairs triples and quadruples and what we did is we just been applied the Union bound.",
            "So I'm quite sure rather than 0.24 you could have a better constant.",
            "But you wouldn't be able to get a better rate.",
            "That is, unless you use things like local Bernstein type inequality's for use statistics.",
            "And in this case you should be able to get maybe something that behaves more like one over him rather than one over square root team.",
            "So in other words, since you can bound the variance, you should be able to get something that scales a little bit better for small sample size.",
            "OK, so of course in the end."
        ],
        [
            "For all of those criteria, the proof is in the pudding, so we compared a whole bunch of.",
            "Different independence criteria.",
            "So what we did is we.",
            "Well, synthetically generated a set of random variables, anything between.",
            "Just two different distributions, or 16 different ones.",
            "We looked at different sample sizes and repeated that many times.",
            "And then we compared a whole bunch of different independence criteria, so that's fast ICA of upheaval, and.",
            "Jad infomax.",
            "Radical, I think that's why typically at all.",
            "See if I I see I actually forgot which, which one Arthur compared against.",
            "These are some kernel constraint covariance other criteria which people have been using in the past.",
            "I think one of those is the one of Bach and Jordan.",
            "And these are the two new criteria, and the G&L mean that in one case we use the Gaussian, the other case Hello Plus kernel.",
            "Turns out the Laplacian electric performance is better.",
            "But what you can see is OK.",
            "The two strongest criteria are radical.",
            "Which is a criterion specifically tuned for linear ICA?",
            "And our Hilbert Schmidt independence criterion.",
            "We perform a little bit worse on one test, and otherwise we're always in the same.",
            "Ballparks are basically, you cannot distinguish it from radical.",
            "What we plot.",
            "OK, So what what those numbers are is the amerida vergence.",
            "So it's basically you take a whole bunch of known sources.",
            "You mix them and you check how well you are able to unmix them.",
            "So basically it's some sort of a matrix norm and that it's a quantity that's reasonably well respected in the ICA community.",
            "Yep.",
            "Colonel Myth is different from the approach that Doctor Jordan kept well.",
            "Fondren is actually one of the columns here.",
            "The key drawback in the original box in Jordan Paper is if you actually look at the theory there criterion is start independent.",
            "Now you might wonder why do they get away with it?",
            "Because actually in the implementation thing they talk about using some reduced rank expansion and some regularization.",
            "Well, and it's essentially those two tricks that make it work.",
            "If you drop those and compute exactly and I mean all you have to do is just sit down and look at the Canonical correlation expansion.",
            "You will see the criterion is data independent.",
            "This is one of those amazing cases where a broken theory leads to really good results.",
            "I'll show you on the next slide.",
            "Actually a little bit more of a comparison to those, but yes, we do beat them.",
            "I mean not very dramatically.",
            "I think this is.",
            "Barbara Jordan is one of those columns here.",
            "I forgot which one it is.",
            "I mean, yeah, we're better, but I mean it's not.",
            "Outrageously better, however, this is if you get the regularization constant for their method exactly right now, let me show you what happens if you need to scale a few of those things."
        ],
        [
            "I think.",
            "One of I think the.",
            "I think this is the Bakken Jordan method.",
            "As far as I remember.",
            "And what you can see is if you.",
            "Get the regularization right.",
            "It actually does pretty well.",
            "If you don't get it right, it can actually perform rather awfully.",
            "Whereas our criteria is the black line down here because it doesn't have any of those regularization constant, it doesn't require it well.",
            "It's actually quite nice all the time.",
            "Yes.",
            "So using your criteria.",
            "Find.",
            "Yes, So what you do is if you have an independence criterion, you then optimize overall rotations.",
            "And in the end world you will get a demixing matrix, some rotation, which then tells you.",
            "Well, so you basically use the independence criterion as a proxy.",
            "And your country lends itself well to this kind of information, yes?",
            "Then we, well, we it lends itself reasonably well to it.",
            "I mean, you need to do a few tricks in practice.",
            "I mean, you will use the incomplete Cholesky factorization and so on, in which case you get something that scales essentially linearly with the number of observations.",
            "I'll get to that in a moment.",
            "Well, the thing too.",
            "This.",
            "Should be another.",
            "Yep, the thing to note though, and this is."
        ],
        [
            "Actually, why I think our criterion is way better.",
            "I mean otherwise you could just use Mark in Jordan and say well beat so the theory is broken but.",
            "Our criterion is very resistant to outliers.",
            "So this dotted line here on top is radical.",
            "So that's the best performing ICA method.",
            "What happens though is if you add in some outliers, this method breaks down quite quite badly, whereas our criterion is very good.",
            "What's this came?",
            "Well, that's just the number that that's the I think the fraction of outliers.",
            "I think that's what it is.",
            "And it.",
            "One of the one of the reasons why this kernel based method is quite nice in terms of outliers I guess, is cause the kernel itself is bounded.",
            "And this and we have not done in any sort of theoretical analysis of this so far.",
            "Being, however, implies that the value of the independence criterion cannot be changed too much.",
            "If you change some data.",
            "So we haven't done any formal analysis of a breakdown point of the estimator, but I mean that would be.",
            "Something that I guess would be the key to the analysis.",
            "OK. Alright."
        ],
        [
            "So just getting back to how you actually."
        ],
        [
            "So and implemented well, you would not necessarily compute the full kernel matrix, because if you've got 100,000 observations, then computing 100 thousand 100,000 matrix is awful.",
            "What you would do in status you would compute some low rank approximation and for instance you could use.",
            "Like all the tricks for, say, sparse, greedy approximation or nice dream or whatever for that particular purpose.",
            "So you would use statistics in that case.",
            "Again, to show that this approximation is reasonably good.",
            "So Shields method from before would help.",
            "But the point is, this is essentially the.",
            "I'd say the best independence criterion you can get at the moment.",
            "And the key point is it was not designed specifically for linear ICA, but it works for it very well.",
            "So you could also think of nicely parameterized nonlinear ICA criteria, but I mean.",
            "You need to define the class of transformations rather sensibly."
        ],
        [
            "OK. Now this is quite new.",
            "It's in fact unpublished so far.",
            "Well, if we want to check whether to distributions are the same, how would you go about doing this?",
            "Well, there's a simple linear witness to check whether two distributions are not the same.",
            "Let's take some linear function, F of X is www.x.",
            "And check whether the means deviate even if they do, we clearly know those two functions are not the same.",
            "OK.",
            "So we compute the mean of that minus the mean of this.",
            "Now, a nonlinear witness will be, and that's now necessary and sufficient, and you can prove that.",
            "But well.",
            "Actually, there should be a square bracket here that the means for all those for any arbitrary function.",
            "If the if this always vanish, then I know those two functions are the same.",
            "Or Conversely, if I find some function for which the means deviate from zero.",
            "The two distributions are not the same.",
            "This is actually something that you would use typically in the context of weak convergence.",
            "So we convergence of of some distribution to another one means simply that.",
            "For all if from some class.",
            "The expected value with respect to some distribution PN of FX converges to expect value with respect to P of F. And once is that if this holds for N going to Infinity.",
            "That PN converges to P. And essentially all we've done is we've just used this criterion and turn it upside down to get the test for the distributions all the same.",
            "For finite sample size, however, the problem is we can almost always find a nonlinear witness.",
            "The only question is how meaningful is this nonlinear witness?",
            "Yeah, go back.",
            "Yeah.",
            "Is there an essential distinction between this and classification?",
            "So we can just we just try to customize in the Exeter the wife set.",
            "Well, OK, let me go to the next slide and you'll see why this is not new."
        ],
        [
            "Sorry, good idea.",
            "See, these two distributions are actually the same.",
            "Well trying to classify mean what would you exactly classify.",
            "Red versus blue.",
            "Well, all you would get through this is.",
            "Well, whether the supports are the same.",
            "Well, the only thing is you might want to try and classify and see how far away from .5 you will be able to get your classification error.",
            "Yeah, you could probably attack it like this.",
            "That's true, haven't thought of that yet and you might be able to get.",
            "Reasonable balance with that.",
            "However, what I'm going to present here does not require any optimization.",
            "So if you're trying to classify, you have to run a classifier into a whole bunch of optimization.",
            "When I'm going to show you here doesn't require that.",
            "So maybe the only advantage that what I'm presenting is is an algorithmic one.",
            "Actually."
        ],
        [
            "The algorithm is so simple that.",
            "I mean, you can code it up in a few lines.",
            "Well, these I'm claiming are actually drawn from two different distributions.",
            "And it's probably quite hard to spot, but essentially what happened is the means are slightly shifted.",
            "Or maybe we're just unlucky."
        ],
        [
            "You wouldn't be able to really tell so easily.",
            "So the theorem."
        ],
        [
            "We have is that for a pseudo unbounded range function.",
            "Well, these two distributions are the same.",
            "The expectations are the same for all F and.",
            "Well, basically again, what you do is you take distributions defined on the measure of the sets.",
            "The Sigma algebra thing considered deviations between such States and that's basically what you get.",
            "So what I'm now defining as my statistic is.",
            "Let's call it MMD.",
            "With respect to distributions, Anna function class, it's the soup of the functions from this function class that the expected value of F of X.",
            "Differs from the expected value from FY.",
            "Without loss of generality, I'm assuming that all those function classes are symmetric, otherwise I would have to take an absolute value somewhere, but.",
            "Yeah, just makes life a bit easier.",
            "OK."
        ],
        [
            "I.",
            "Now.",
            "Actually, there's a simple empirical criterion that you can fashion from this.",
            "We just replace all the expectations by empirical averages.",
            "So now instead of distributions I have sets of observations and I'm defining this empirical criterion to be the soup again over functions from this function class over the empirical averages for F of X and the empirical average for F of Y.",
            "Now this in general, well, yes, you could probably go and workout the statistical properties of that through rather more averages directly.",
            "But there is one special case where this derivation becomes rather nice.",
            "So let's say that F is the unit ball of functions in a bank space.",
            "In this case.",
            "Well.",
            "If I assume that F of X is given body inner product between F and some evaluation functional.",
            "So this I'm not only requiring it for the Hilbert space, but I can just state that in a backspace too, and afterwards we'll see that the Kolmogorov Smirnov test is actually specific case of that.",
            "I can see by the very fact that, well, this is a banner space.",
            "But this quantity up here is given by the dual norm.",
            "Of the difference of the means of evaluation functionals.",
            "Now, why is this so?",
            "Let me spell out the proof in a bit more detail, because that's actually one of the key tricks of of the entire idea.",
            "This patient is all these functions F yes, you're just saying you put it normal name to make it yes, yeah.",
            "So.",
            "Well, this quantity here can be written as the soup.",
            "Over Norm of F. Less equal than one.",
            "Off well.",
            "1 / M. Some over I going from one to M inner product between F. And Phi of XI.",
            "Minus one over in some of the same expression in the wise.",
            "OK, which can be written as the inner product between F and here I get 1 / M Some over I going from one to M fi of X I -- 1 over in some overriding from one to N Phi of Yi.",
            "You know product clothes and the soup, of course.",
            "Now, just by the very definition of the dual norm, this expression here is defined as the dual norm of this expression.",
            "Which is exactly what we have here.",
            "OK, So what does that mean?",
            "If I can compute this quantity efficiently?",
            "Then I will be able.",
            "To, well, get a convenient statistic.",
            "So now let's look at a special case.",
            "Which will give us the Kolmogorov Smirnov test.",
            "Let's define.",
            "Phi of XI to be the indicator functions.",
            "Going from minus Infinity to XI.",
            "Sorry OK. That's an indicator function.",
            "It says here and one.",
            "Yeah, OK.",
            "So it's basically one if you're in the interval between minus Infinity and XI, and otherwise it's 0.",
            "So this leaves will be elements in L Infinity.",
            "Furthermore, I take my space of functions if.",
            "To be that the.",
            "L1 norm of faith is less equal in one.",
            "In which case, well, what do I get?",
            "This expression here.",
            "Sorry, I should have written it as follows.",
            "Um?",
            "No, sorry starts from XI.",
            "And goes to Infinity.",
            "In this case, these functions here are the cumulative empirical cumulative distribution functions.",
            "For the X and the Y's.",
            "OK the.",
            "In a product is right before fun.",
            "What is your reputation, built space in my store?",
            "Well, in this case I'm not talking about reproducing kernel space, but about the backspace.",
            "Yeah.",
            "And, well, what now, if, if are, are the evaluations at those locations or linear combinations of the valuations?",
            "And I'm just picking those to be in L1.",
            "That's your space.",
            "Now.",
            "These expressions here will turn into the cumulative distribution functions.",
            "The empirical ones.",
            "So now the alien Infinity Norm of the difference between those is exactly the statistic that the Kolmogorov Smirnov test looks at.",
            "So if I had.",
            "Then we'll can you get one of those step functions here.",
            "Of course it will go to one and maybe here's another one.",
            "Sorry of course would have been too much.",
            "Maybe it continues like this.",
            "Then the L Infinity Norm would be this size.",
            "And for the living Cantelli theorem.",
            "You know that this random variable here behaves like order of one over square root M. And then you can use this to fashion a statistical test out of this quantity.",
            "And it's for univariate random variables.",
            "This is a pretty good thing that you can do.",
            "Trying to extend the Kolmogorov Smirnov tests to arbitrary domains is.",
            "Less than trivial.",
            "Because the CDF doesn't really exist so nicely anymore.",
            "So try defining a CDF on tests.",
            "And this is exactly.",
            "Where this general formulation comes to our rescue?",
            "Yep.",
            "Confused because this evaluation functionality.",
            "Specific.",
            "Now here you just select roughly alcohol with special functions.",
            "Yes, so all I've said all I'm saying is the Kolmogorov Smirnov test is a special case of this general strategy, and for that specific choice of Phi, and if I get this test out of it.",
            "But if I choose different files and will get completely different tests out of it, so it basically produces a large family of tests, what will actually also see if we have time?",
            "Is that the?",
            "The L2 distance between two kernel density estimates also is a special instance of this.",
            "Becausw well for most cases, actually working with Hilbert space is a bit more convenient than working with banner spaces.",
            "OK."
        ],
        [
            "So for Hilbert space in particular, well, we can exploit that for fix on 5X J.",
            "Just this is just the kernel and this is well defined.",
            "So if I square this quantity here, that's what I'm getting.",
            "So in other words, I can compute this statistic by just computing values on a bunch of entries of the kernel matrix.",
            "So now optimization required.",
            "You can actually get it even more cheaply by just some subsampling schemes.",
            "And well, they will have the same statistical properties.",
            "Now if this function K where the inner product between two parts in Windows kernel functions and this will be the L2 distance between the kernel density estimates.",
            "So in other words, if I approximate PX to be 1 / M sum over, I going from one to M. Kappa of well say X I -- X.",
            "And likewise for PN and I define K of X&X prime to be the integral of Kappa.",
            "Of X -- Z. Kappa of X prime minus seed dies it.",
            "Then I can see that the L2 distance between PXMPY.",
            "Squared is exactly the statistic here.",
            "So now we have another test that people might use for testing between distributions as a special case of this framework.",
            "But of course family, yes.",
            "Just there are many kernels, but there were so many of these kernels so yeah sure.",
            "So.",
            "But basically now we can use one machinery to analyze all of them, which is nice.",
            "Distribution free yes.",
            "Colonel bonus.",
            "Yes, but in the general case.",
            "You will.",
            "Well, OK, well our test is distribution free in the sense that.",
            "While the statistic vanish is if and only if the random variables are independent.",
            "However, what you're doing about the choice of the kernel and I see this as a feature rather than a drawback.",
            "You can prioritize which functions you're going to test for first, So what?",
            "Sometimes some practical statisticians complain about is they save all the Kolmogorov Smirnov test is very nice, but it doesn't really pick up the right dependencies that we want.",
            "They say it's not strong enough or whatever for their specific problem.",
            "Well, I'm saying here sure, that's all fine.",
            "I mean, you can just go and pick the right kernel where you believe the discrepancies will manifest themselves first.",
            "And by biasing it in the right way, you will get the test, which for the practical problems that you're trying to solve is more selective.",
            "Yes, Anne.",
            "Well, if I get a bit of time I can get to this in a bit more detail.",
            "Well, what you can show is if it's a universal kernel, then even if you screw up the bandwidth with large enough number of observations you will detect it.",
            "But yeah, this might not be quite as efficient thing.",
            "OK, let me first explain how the machinery actually works.",
            "So first."
        ],
        [
            "You go and get the large deviation bound.",
            "Let's just make the are made so all you're doing is you have the norm of a bunch of random variables and then you just argue that if you replace one of the observations by another one and if all the entries are bounded thing, well you get the hurting type rate, which is quite convenient.",
            "And so you get basically something which behaves like sqrt M + N / M times in.",
            "Now for.",
            "The norm itself.",
            "In order to get this expected value."
        ],
        [
            "Well, what we can do in the general case where we can bound all this with rather more averages quite easily directly into the center code sample.",
            "Tricking it works.",
            "Mailbox space case where a bit more fortunate, because what we can do is we can just.",
            "Show well by means inequality.",
            "The expectation of this term squared is less equal than the expected value of the squared term, which, after a little bit of algebra, just turns out to be again something that we have like order of square root N + N / M N. And here I have the expected value, the expected length, so the same feature space minus the expected overlap.",
            "And now let's get back to your question about the bandwidth.",
            "This is exactly where this shows up.",
            "So if I choose a kernel with a very narrow bandwidth, everything will look orthogonal.",
            "So in other words, this quantity here will pretty much be 0.",
            "OK.",
            "So that means if all the data are quite long.",
            "But I will get something that doesn't really vanish so quickly anymore.",
            "Ideally I want this right inside to be small.",
            "If it's small, that means I can have a very aggressive threshold.",
            "Now they make it really small.",
            "Well can make it really small by making this expression here pretty large.",
            "And this means choosing a wider bandwidth.",
            "But if you choose a bandwidth that's too wide, then this will be so small that the statistical term for the table bound will kick in.",
            "So we have those two effects.",
            "One is the tail bond which.",
            "Which came from Macdiarmid, which means that.",
            "The random variable itself is unreliable.",
            "The second term is that well in expectation, well just even if the two distributions were the same things.",
            "To deviate a little bit from each other just because I have a finite number of observations.",
            "And so it's those two effects that I need to control.",
            "That's exactly what I can find here.",
            "OK.",
            "But you can actually show if that the test is reasonably sensitive, so we can get order of square root in deviations from the identity.",
            "The right itself is tight and you can check that easily by just taking Gaussian random variables.",
            "And let's say they have both the same."
        ],
        [
            "Distribution say unit variance and different means you want to test whether the two distributions are the same.",
            "Then we will get something that looks like this, and that's exactly the quantity that we had before.",
            "This is basically something like the trace of the covariance matrix.",
            "And for exponential families, well under certain assumptions, this also bounced the Cal diversions.",
            "So what we've done now is we have.",
            "So basically we have a criterion which.",
            "Incorporates a couple of known distribution tests as special cases.",
            "Well, so I thought that was quite convenient.",
            "So summing it up I mean."
        ],
        [
            "What we have is.",
            "A simple linear test for checking the properties of some distribution.",
            "We have a simple algorithm that goes with it.",
            "The power of the test comes from using nonlinear functions.",
            "In Hilbert spaces, that's usually quite easy and well, yeah, we looked at supports of distributions, independence and entity.",
            "Not saying that's the only test for the identity.",
            "And yeah, you can use it for ICA verification and fraud detection merging databases.",
            "And yeah, OK, well that's the 30 seconds commercial now.",
            "So if you want to go to a machine learning summer school, there most likely will be one in February early next year.",
            "So.",
            "The site isn't up yet.",
            "2006 death.",
            "That's what it would be, yeah.",
            "Australia now."
        ],
        [
            "I know we're behind, but not gonna tell you.",
            "Well.",
            "Yeah, whatever and yeah.",
            "I mean if you want to get a good tan on sabbatical will come and visit us.",
            "So that's what I want to say.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also maxi so so.",
                    "label": 0
                },
                {
                    "sent": "A lot of things will look for media.",
                    "label": 0
                },
                {
                    "sent": "We use it for slightly different tests so but that's pretty much your idea.",
                    "label": 0
                },
                {
                    "sent": "Well, I guess that's part of the reason.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is joint work.",
                    "label": 0
                },
                {
                    "sent": "The whole bunch of cards.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry, now what's the setting that we look at?",
                    "label": 0
                },
                {
                    "sent": "Well, you might want to do something that well in one way you could just call property testing for distributions.",
                    "label": 1
                },
                {
                    "sent": "In other words, well given some observations, there's a certain property hold for random variables like, well, are they independent?",
                    "label": 0
                },
                {
                    "sent": "Are they distributed according to the same distribution?",
                    "label": 0
                },
                {
                    "sent": "Does the support of the distribution have?",
                    "label": 0
                },
                {
                    "sent": "Does it intersect or similar things like that?",
                    "label": 0
                },
                {
                    "sent": "So for this joint support, I mean people are doing that a lot.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is people typically refer to it as the classification problem, so they would want to test with a.",
                    "label": 0
                },
                {
                    "sent": "Classified data sets well.",
                    "label": 0
                },
                {
                    "sent": "And of course, that also means that if you can classify with very low error or Syria, then there's a high chance that the distributions have disjoint support memory.",
                    "label": 0
                },
                {
                    "sent": "Those of class one and class minus one for independence.",
                    "label": 0
                },
                {
                    "sent": "Well, there are lots of criteria that you could use to check whether 2 random variables are independent.",
                    "label": 0
                },
                {
                    "sent": "We have one that essentially what it does is it starts with a quite convenient linear criterion.",
                    "label": 0
                },
                {
                    "sent": "And then we map all this into Hilbert space and then get necessary and sufficient conditions.",
                    "label": 1
                },
                {
                    "sent": "And well, it actually works amazingly well.",
                    "label": 0
                },
                {
                    "sent": "Be giving you the theoretical explanation on how to do this and then the end and this is.",
                    "label": 0
                },
                {
                    "sent": "Not published yet?",
                    "label": 0
                },
                {
                    "sent": "Well, if you want to check with the two sets of observations are drawn from the same distribution you want to have a nice nonparametric test, so probably one of the famous ones is the Kolmogorov Smirnov test and derivatives thereof, as it actually turns out this criterion is a contains the Kolmogorov Smirnov test is a special case, and we get the.",
                    "label": 0
                },
                {
                    "sent": "Analysis Just the same way as we would have done what deal did were also for independence.",
                    "label": 0
                },
                {
                    "sent": "So well, let's look at it in a bit more detail to see what I actually mean by that.",
                    "label": 0
                },
                {
                    "sent": "So for the support, well given data sets well.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "13 exam drawn from X probability of distribution of X&Y one through Y.",
                    "label": 0
                },
                {
                    "sent": "In going from this distribution of why well we might want to test with the supports.",
                    "label": 0
                },
                {
                    "sent": "This section supports vanish is or for independence well, given pairs X&Y.",
                    "label": 0
                },
                {
                    "sent": "Well, test whether the probability distribution in X&Y factorizes.",
                    "label": 1
                },
                {
                    "sent": "Or for identity well drawn X one through XM from PR efix an corresponding whilst on from probability of why we want to check whether these two distributions are actually the same.",
                    "label": 0
                },
                {
                    "sent": "Of course, you will never ever be able to.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "Check with certainty whether that's the case.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could always construct distributions that will fool you into believing that the two distributions are the same, even though they are not.",
                    "label": 0
                },
                {
                    "sent": "Or they're trying to.",
                    "label": 0
                },
                {
                    "sent": "Variables are independent even though they're not.",
                    "label": 0
                },
                {
                    "sent": "At least we want to get the hypothesis test, which as we get large sample size, we will be able to reject the null hypothesis of the distributions being independent random variables being independent or the distributions being the same in a sensible way.",
                    "label": 0
                },
                {
                    "sent": "What's the difference to computer science style property testing?",
                    "label": 1
                },
                {
                    "sent": "Well, there people are usually really fast about.",
                    "label": 1
                },
                {
                    "sent": "Particular finite domains, they usually assume that generating new data is very cheap, and.",
                    "label": 0
                },
                {
                    "sent": "Thing that the key issue is actually to, well, find an algorithm that runs really fast.",
                    "label": 0
                },
                {
                    "sent": "So you can be wasteful with the data, but you have to be very greedy with the computations.",
                    "label": 0
                },
                {
                    "sent": "In our case, it's the other way around.",
                    "label": 0
                },
                {
                    "sent": "OK, well why would you want to use that well?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One application would be more independent component analysis.",
                    "label": 1
                },
                {
                    "sent": "Will the usual thing like givenness set of in audio signals find independent sources of the recording?",
                    "label": 1
                },
                {
                    "sent": "You might use that for EG data to denoise it in some way.",
                    "label": 0
                },
                {
                    "sent": "You can apply this to neuroscience well, cosmology removing background radiation would ever for statistical modeling.",
                    "label": 0
                },
                {
                    "sent": "While you can use such tests in graphical models, if you know which random variables are independent.",
                    "label": 0
                },
                {
                    "sent": "While you can design your graph based on that, you can select variables like for an estimation problem, short estimate, why give mix.",
                    "label": 0
                },
                {
                    "sent": "Well, if X if some coordinate exercise independent of why you might as well throw it out.",
                    "label": 0
                },
                {
                    "sent": "For the distribution testing well, one application would be.",
                    "label": 0
                },
                {
                    "sent": "You might want to merge databases, say the health insurance records for find hovinen, those of Amsterdam and maybe they're sitting on different servers.",
                    "label": 0
                },
                {
                    "sent": "And of course you only want to merge them if they have the same statistical behavior.",
                    "label": 0
                },
                {
                    "sent": "So you want to have a cheap test to find out about that.",
                    "label": 1
                },
                {
                    "sent": "Or combine experiments from different sources.",
                    "label": 0
                },
                {
                    "sent": "So if you're doing microarray measurements and you have one team of doctors in one city in a team of doctors in another and they have different equipment.",
                    "label": 0
                },
                {
                    "sent": "And you want to actually use the combination of both for the estimates for you want to find out where that is sensible or fraud detection like somebody wants to pass off, well, home painting drawing as a Picasso.",
                    "label": 0
                },
                {
                    "sent": "Well, you'd want to have some device to test that.",
                    "label": 0
                },
                {
                    "sent": "So what's the key strategy in all those three examples of?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mention and why am I mentioning all three at once?",
                    "label": 0
                },
                {
                    "sent": "Well, essentially what we do is we have a simple linear criterion and linear is to be taken with a grain of salt.",
                    "label": 0
                },
                {
                    "sent": "So we have some sort of linear witness of dependence of this agreement, which is easy to compute.",
                    "label": 1
                },
                {
                    "sent": "But of course, since it's just a simple linear thing, I mean it will typically not give us necessary and sufficient implications.",
                    "label": 0
                },
                {
                    "sent": "And the trick is thin to use suitable non linearisation.",
                    "label": 0
                },
                {
                    "sent": "To extend this to linear function space is typically in a reproducing kernel space, but it could be any other backspace and then to find an efficient parameterisation for this test such that I can Even so I'm now looking at a whole class of nonlinear objects.",
                    "label": 0
                },
                {
                    "sent": "Still, I'm still able to compute this efficiently.",
                    "label": 0
                },
                {
                    "sent": "And then you go and find an empirical approximation.",
                    "label": 0
                },
                {
                    "sent": "Then in the end, yes, you do the statistical analysis.",
                    "label": 0
                },
                {
                    "sent": "So you look at the expectations you look at.",
                    "label": 0
                },
                {
                    "sent": "The tails show that The thing is concentrated, give corresponding bounds and well that gives you the test.",
                    "label": 0
                },
                {
                    "sent": "So that's the general strategy.",
                    "label": 0
                },
                {
                    "sent": "And, well, let's just begin with one instance for that.",
                    "label": 0
                },
                {
                    "sent": "You've definitely seen, namely the test for disjoint support.",
                    "label": 0
                },
                {
                    "sent": "So let's say we've got X&Y drawn from 2 distributions.",
                    "label": 0
                },
                {
                    "sent": "I want to check with the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Supports are disjoint.",
                    "label": 0
                },
                {
                    "sent": "If this is on a finite domain.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you just look for enough that I mean as soon as you find the collusion, you know, well, game over.",
                    "label": 0
                },
                {
                    "sent": "But if the domain is not finite.",
                    "label": 0
                },
                {
                    "sent": "Thin.",
                    "label": 0
                },
                {
                    "sent": "You have to do something slightly different because it's highly unlikely.",
                    "label": 0
                },
                {
                    "sent": "If you have, say, alright and some.",
                    "label": 0
                },
                {
                    "sent": "Non atomic distribution that you will find the same entrance instance twice.",
                    "label": 0
                },
                {
                    "sent": "Now of course a linear witness, and that's quite a simple thing for would be that if I can find some linear function F of X being W transpose X such that F of X is less than F of Y for all X&Y pairs.",
                    "label": 0
                },
                {
                    "sent": "Well, if I can find such a function, well, that is clearly a proof that the data is linearly separable.",
                    "label": 0
                },
                {
                    "sent": "Remember, that's basically what you would do in a support vector machine.",
                    "label": 0
                },
                {
                    "sent": "I need within, say, well, actually pick this threshold here to be 0, but actually I wouldn't really have to.",
                    "label": 0
                },
                {
                    "sent": "Now a nonlinear witness.",
                    "label": 0
                },
                {
                    "sent": "And that's the necessary and sufficient is, if there exists some 01 bounded function such that this condition holds.",
                    "label": 0
                },
                {
                    "sent": "So actually Bioresource lemma if the closures of the supports are disjoint and you can find a.",
                    "label": 0
                },
                {
                    "sent": "Continuous 01 bounded function that satisfies this.",
                    "label": 0
                },
                {
                    "sent": "Now unfortunately, will we?",
                    "label": 0
                },
                {
                    "sent": "Always have will always be able to find a nonlinear witness.",
                    "label": 0
                },
                {
                    "sent": "If I have finite sample size, so too.",
                    "label": 0
                },
                {
                    "sent": "Explain that point.",
                    "label": 0
                },
                {
                    "sent": "Well, that data looks quite.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually linearly separable.",
                    "label": 0
                },
                {
                    "sent": "Tom.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I forget something like this.",
                    "label": 0
                },
                {
                    "sent": "Well, it's pretty much up to us to argue with.",
                    "label": 0
                },
                {
                    "sent": "This is non linearly separable or not.",
                    "label": 0
                },
                {
                    "sent": "I mean, because this could be just that the region around here is quite noisy, in which case it's not linearly separable, or it could be that, well, the separating.",
                    "label": 0
                },
                {
                    "sent": "Surface is just really complicated.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, that's.",
                    "label": 0
                },
                {
                    "sent": "But it could also be that this is the best thing you can do.",
                    "label": 0
                },
                {
                    "sent": "And so then the question boils down to how reliable is this?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, that we're producing.",
                    "label": 0
                },
                {
                    "sent": "Now, this is actually a problem that people have studied a lot in machine learning, so I'm not going to go into details about rates and so on, because I mean this is just really re phrasing existing results in new address.",
                    "label": 0
                },
                {
                    "sent": "So essentially what we end up doing is we solve noise free binary classification problem.",
                    "label": 1
                },
                {
                    "sent": "And then, well, we just go and take our favorite uniform convergence bounds, depending on what estimator I take so I can have some bounded, we see that mentioned it would take some of development concerning KISS results I could use rather more averages localized rather more averages.",
                    "label": 0
                },
                {
                    "sent": "What have you?",
                    "label": 0
                },
                {
                    "sent": "And then I use reciting them to test the hypothesis of separability.",
                    "label": 1
                },
                {
                    "sent": "So in other words, I would want to have a test which tells me that if I can separate.",
                    "label": 0
                },
                {
                    "sent": "Well, perfectly on my training data, what's the probability that the error on the test set will exceed some threshold epsilon, and if I pick that threshold epsilon?",
                    "label": 0
                },
                {
                    "sent": "Suitably I will get the test that at least if the support had measure larger than some epsilon, then well I would be able to pick it up.",
                    "label": 0
                },
                {
                    "sent": "So for instance, a computational solution for that would be to use an SVM hard margin optimizer.",
                    "label": 0
                },
                {
                    "sent": "So you need to run a quadratic program and it will do the trick.",
                    "label": 0
                },
                {
                    "sent": "It's not the only.",
                    "label": 0
                },
                {
                    "sent": "Device, but that would be quite convenient.",
                    "label": 0
                },
                {
                    "sent": "I mean, so far this is.",
                    "label": 0
                },
                {
                    "sent": "Just re phrasing things, but.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The point that I wanted to drive home with this is, well, you have some problem, so some property.",
                    "label": 0
                },
                {
                    "sent": "It's easy to find some linear criterion.",
                    "label": 0
                },
                {
                    "sent": "It's not so easy to find the nonlinear criterion unless we do some normalization in between, which turns the problem into something convenient, and then in the end, while we apply our statistical bounds to that.",
                    "label": 0
                },
                {
                    "sent": "Now this is where things get a bit more interesting.",
                    "label": 0
                },
                {
                    "sent": "Let's say I have pairs of random variables X&Y.",
                    "label": 0
                },
                {
                    "sent": "They want to test with a distribution factorizes.",
                    "label": 0
                },
                {
                    "sent": "Now a linear witness for X&Y be independent.",
                    "label": 1
                },
                {
                    "sent": "Is well if for some linear function F. Www.transpose XNGV transpose Y so covariance deviates from zero.",
                    "label": 1
                },
                {
                    "sent": "That's the case.",
                    "label": 0
                },
                {
                    "sent": "Well, I know the random variables cannot be independent.",
                    "label": 1
                },
                {
                    "sent": "Now a non necessary and sufficient condition is clearly not justify linear functions, but if I also pick some 01 bounded functions, FNG and I say well if the covariance for those functions.",
                    "label": 0
                },
                {
                    "sent": "Is greater than zero, then obviously.",
                    "label": 0
                },
                {
                    "sent": "This tells me that the random variables are independent of each other.",
                    "label": 1
                },
                {
                    "sent": "So really actually uses this as a test for independence.",
                    "label": 0
                },
                {
                    "sent": "Why's that nice?",
                    "label": 0
                },
                {
                    "sent": "Because it does not require us to actually build a model of the distributions of X&Y or the joint distribution in X&Y.",
                    "label": 0
                },
                {
                    "sent": "All I have to do is I just have to check some reasonably simple criterion.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, for finite sample sizes, we can almost always find some nonlinear witness such that the empirical covariance exceeds 0.",
                    "label": 1
                },
                {
                    "sent": "In other words, you have to construct really pathological examples for which the empirical covariance will not exceed 0.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "These, for instance, could be.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Independent random variables.",
                    "label": 0
                },
                {
                    "sent": "Now these.",
                    "label": 0
                },
                {
                    "sent": "I'm claiming our dependent random variables mean doesn't look much different.",
                    "label": 1
                },
                {
                    "sent": "Or maybe we were just unlucky.",
                    "label": 0
                },
                {
                    "sent": "So I mean, if you just eyeball it, it will be not so easy to see that these are in fact dependent random variables.",
                    "label": 0
                },
                {
                    "sent": "Essentially what's happened is I've taken a uniform distribution and slanted it a little bit.",
                    "label": 0
                },
                {
                    "sent": "See look at it.",
                    "label": 0
                },
                {
                    "sent": "You can see some parallelogram here.",
                    "label": 0
                },
                {
                    "sent": "OK but yeah.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's hard to tell, just.",
                    "label": 0
                },
                {
                    "sent": "By yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that would be X.",
                    "label": 0
                },
                {
                    "sent": "That would be Y and I've just plotted the pairs of X&Y.",
                    "label": 0
                },
                {
                    "sent": "And all these random variables are dependent.",
                    "label": 1
                },
                {
                    "sent": "Slightly, but they are.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In other words, you can see there's a little bit of a trend.",
                    "label": 0
                },
                {
                    "sent": "In other words, large X is correspond to slightly larger vice.",
                    "label": 0
                },
                {
                    "sent": "So we can do a little bit of a prediction here, and that of course means that the random variables are dependent.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we get our hands on that?",
                    "label": 0
                },
                {
                    "sent": "And, well, let's pull out the trusty reproducing kernel spaces.",
                    "label": 1
                },
                {
                    "sent": "So let's define kernels K of X&X, prime alavoine Y prime on X&Y respectively, so they don't need to be the same type, so for instance X could be texts and why could be sound?",
                    "label": 0
                },
                {
                    "sent": "Or why could be images or whatever, so I mean, there's absolutely no requirement that these actually need to be of the same type.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, well, this is a nice thing about representative spaces that the evaluation function is a linear operator.",
                    "label": 0
                },
                {
                    "sent": "So in other words, a fix can be written as inner product between F&K of X and dot.",
                    "label": 0
                },
                {
                    "sent": "Moreover, I will assume that K&L are bounded on the domain.",
                    "label": 0
                },
                {
                    "sent": "That's not always the case, but for a lot of things, like RBF kernels, that's the that's true.",
                    "label": 0
                },
                {
                    "sent": "And now I just need to define two more operators, namely, I find a mean operator.",
                    "label": 0
                },
                {
                    "sent": "And this is a linear map which Maps if.",
                    "label": 1
                },
                {
                    "sent": "Into its average on X and likewise G into this average and Y.",
                    "label": 0
                },
                {
                    "sent": "You can clearly see these linear operators.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So this way I can define mutex and new line.",
                    "label": 0
                },
                {
                    "sent": "So basically what you do is you define.",
                    "label": 0
                },
                {
                    "sent": "These means.",
                    "label": 0
                },
                {
                    "sent": "Ask the corresponding through the corresponding linear functionals.",
                    "label": 0
                },
                {
                    "sent": "Likewise, you can define bilinear forms.",
                    "label": 0
                },
                {
                    "sent": "Namely the covariance operators.",
                    "label": 0
                },
                {
                    "sent": "As follows that.",
                    "label": 0
                },
                {
                    "sent": "The covariance operator applied to F&G.",
                    "label": 1
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "This expression here.",
                    "label": 0
                },
                {
                    "sent": "Now, if I had independent random variables, this covariance operator would have to vanish.",
                    "label": 0
                },
                {
                    "sent": "Because the covariance sustain are zero for all FG.",
                    "label": 0
                },
                {
                    "sent": "So now and this is the key trick about it.",
                    "label": 0
                },
                {
                    "sent": "I can design a test.",
                    "label": 1
                },
                {
                    "sent": "By simply looking.",
                    "label": 0
                },
                {
                    "sent": "Four, well, by computing the norm of this covariance operator.",
                    "label": 0
                },
                {
                    "sent": "And then I will approximate this value on my finite set of observations and it will show that this approximation has nice statistical properties.",
                    "label": 0
                },
                {
                    "sent": "So the strategy is, take this nonlinear take this linear operator, compute its norm, and then find a.",
                    "label": 0
                },
                {
                    "sent": "Well, find sample size approximation.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I've just said before is so you can show that this is actually necessary and sufficient condition.",
                    "label": 0
                },
                {
                    "sent": "If K&L are you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Virtual kernels.",
                    "label": 0
                },
                {
                    "sent": "So that means that.",
                    "label": 0
                },
                {
                    "sent": "You can basically approximate any continuous any continuous function.",
                    "label": 0
                },
                {
                    "sent": "Then, and only then, X&Y are independent if the covariance operator vanishes.",
                    "label": 1
                },
                {
                    "sent": "Now the proofs Kate is well for.",
                    "label": 1
                },
                {
                    "sent": "This is really you just say well, if the X&Y are dependent then there must be some functions F star Angie star for which the covariance is greater than 0.",
                    "label": 0
                },
                {
                    "sent": "And then, well, you just pick functions from the reproducing kernel Hilbert space, which converge to a star in G star.",
                    "label": 1
                },
                {
                    "sent": "And these are epsilon prime approximations.",
                    "label": 0
                },
                {
                    "sent": "Such that again, the covariance of the approximation doesn't vanish.",
                    "label": 0
                },
                {
                    "sent": "And then consequently, this operator must not be 0.",
                    "label": 0
                },
                {
                    "sent": "So this is how you show it.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's just write this covariance operator again and the test statistic will then be.",
                    "label": 0
                },
                {
                    "sent": "If we call it Hilbert Schmidt independence criterion as we will compute the Hilbert Schmidt norm of that.",
                    "label": 0
                },
                {
                    "sent": "Off, namely, this object here.",
                    "label": 0
                },
                {
                    "sent": "Now this denotes ascension outer product.",
                    "label": 0
                },
                {
                    "sent": "So in order to apply this to some F&G you need to compute the inner product between F and this part and G and that part.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is slightly lax notation.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So how do you come?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With those norms.",
                    "label": 0
                },
                {
                    "sent": "Well, for rank one operators, that's pretty easy.",
                    "label": 0
                },
                {
                    "sent": "We know that the Hilbert Schmidt norm off the outer product between F&G.",
                    "label": 0
                },
                {
                    "sent": "Squared is just the norm of F squared times the normal G ^2.",
                    "label": 0
                },
                {
                    "sent": "Well, think of it like when you compute the Frobenius norm of a matrix which is rank one.",
                    "label": 0
                },
                {
                    "sent": "So let's say some matrix is first, then.",
                    "label": 0
                },
                {
                    "sent": "Which is just a trace of course of MM transposed, which is then given by the trace.",
                    "label": 0
                },
                {
                    "sent": "Of F * G transposed then times G * F transposed.",
                    "label": 0
                },
                {
                    "sent": "Now by the fact that this is the trace like it, that this is just G transpose G * F transposed, if which is just the norm of G squared times the norm of F ^2.",
                    "label": 0
                },
                {
                    "sent": "And what I've done here for matrices the same thing just works.",
                    "label": 0
                },
                {
                    "sent": "In Hilbert space as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's write out what this really means.",
                    "label": 0
                },
                {
                    "sent": "The normalcy of XY squared still product between those terms.",
                    "label": 0
                },
                {
                    "sent": "Why then do is I plug in the expressions that we had for cxy up here?",
                    "label": 0
                },
                {
                    "sent": "OK. And then after a little bit of algebra.",
                    "label": 0
                },
                {
                    "sent": "I get this expression here where I have the inner product between K&L&K of X prime LY prime and the only difference being the three different types of expectations that I'll be taking.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I'm taking the expectation of that expression for the X&Y and experiment why pairs jointly?",
                    "label": 0
                },
                {
                    "sent": "Here I'm taking the expectation overall four terms independently, and of course here I have one term where I take one expectation independently and another one jointly.",
                    "label": 0
                },
                {
                    "sent": "Now since this.",
                    "label": 0
                },
                {
                    "sent": "Just by that expression here gives me the inner product.",
                    "label": 0
                },
                {
                    "sent": "I just get various expectations of CFX and ex prime alavoine.",
                    "label": 0
                },
                {
                    "sent": "Why prime?",
                    "label": 0
                },
                {
                    "sent": "Now that expression is well defined if kendel abounded.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have the quantity that we really after and we know that if we could compute this quantity we could check with the random variables are independent.",
                    "label": 0
                },
                {
                    "sent": "So how do we go and estimate this?",
                    "label": 0
                },
                {
                    "sent": "What I'm claiming is that this quad.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To hear is a good empirical estimate.",
                    "label": 0
                },
                {
                    "sent": "Of that quantity over there.",
                    "label": 0
                },
                {
                    "sent": "So now I need to explain what this actually means.",
                    "label": 0
                },
                {
                    "sent": "So the recipe is compute the kernel matrix on the axis, compute the kernel matrix on the wise.",
                    "label": 0
                },
                {
                    "sent": "And take out the wrong column means.",
                    "label": 0
                },
                {
                    "sent": "So this is the projector onto.",
                    "label": 0
                },
                {
                    "sent": "Basically, the romin or the column mean.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Multiply those centric matrices and compute the trace.",
                    "label": 0
                },
                {
                    "sent": "An if this quantity is close to 0.",
                    "label": 0
                },
                {
                    "sent": "To normalize suitably, then you know that your random variables are most likely independent.",
                    "label": 0
                },
                {
                    "sent": "If it's larger than if it's significantly larger than zero, you know there.",
                    "label": 0
                },
                {
                    "sent": "Dependant.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is you can show that the expected value of this statistic here.",
                    "label": 0
                },
                {
                    "sent": "Is exactly the criterion that we want plus a bias term which behaves like order of 1 / N. As we will see later on the statistical.",
                    "label": 0
                },
                {
                    "sent": "Properties of this are like order of one over square root M, so this bias is negligible.",
                    "label": 0
                },
                {
                    "sent": "Basically, where this bias comes from is the fact that this some here also can turn contains terms which are just sums in K of XR&XI&L of why I and why I so basically where you have the same argument in both terms.",
                    "label": 0
                },
                {
                    "sent": "And so all you do is you really expand this and it's a little bit of a mess if you do it.",
                    "label": 0
                },
                {
                    "sent": "In terms of all, all the expressions which don't have any duplicates and all the expressions which do have duplicates and for the latter you can show that they are of order one over him.",
                    "label": 0
                },
                {
                    "sent": "And the constant here would be something like a three or four, so it's.",
                    "label": 0
                },
                {
                    "sent": "Not very significant.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now the last thing that we need to do is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Need to show that the random variable.",
                    "label": 0
                },
                {
                    "sent": "This quantity here itself is also concentrated.",
                    "label": 0
                },
                {
                    "sent": "So if dealt with the expectation, now we need to take care of the tail.",
                    "label": 0
                },
                {
                    "sent": "And so we use drifting theorem in a slight modification for you statistics, so that should be clear from Shields talk before.",
                    "label": 0
                },
                {
                    "sent": "Basically what you do is.",
                    "label": 0
                },
                {
                    "sent": "Well this function U is a function.",
                    "label": 0
                },
                {
                    "sent": "Is an average over some which is called the kernel of EU statistic, so not to be confused with the Mercer kernel, which depends on the whole on the subset of those random variables.",
                    "label": 0
                },
                {
                    "sent": "So it's and this is just multiplying by the combinatorial number of possible combinations in which you can arrange these arguments.",
                    "label": 0
                },
                {
                    "sent": "So we will get the standard lifting theorem if these functions G were just functions of 1 random variable, then we would just divide by the number of observations.",
                    "label": 0
                },
                {
                    "sent": "Here in the other cases, well, we just need to do our part number of all pairs, triplets or whatever.",
                    "label": 0
                },
                {
                    "sent": "And now the nice thing is that just like for independent random variables, also those terms, the average of those terms is concentrated in the same way.",
                    "label": 0
                },
                {
                    "sent": "The probability that you exceed the expectation by more than T is bounded by E to the minus two T squared and then OK, I have to pay a price because I took well R tuples.",
                    "label": 0
                },
                {
                    "sent": "So basically reduce my sample size.",
                    "label": 0
                },
                {
                    "sent": "I have to divide it by R and this is just the range of this G function.",
                    "label": 0
                },
                {
                    "sent": "So if you plug in the value one in here, you get the standard 15 theorem back, and otherwise I mean it's just 4.",
                    "label": 0
                },
                {
                    "sent": "Larger subsets of random variables.",
                    "label": 0
                },
                {
                    "sent": "So then what you do is you just count the individual terms.",
                    "label": 0
                },
                {
                    "sent": "Like we in this previous expression.",
                    "label": 0
                },
                {
                    "sent": "We have terms which depends on pairs, triplets and quadruples of random variables.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "You apply 15 theorem for each of them.",
                    "label": 0
                },
                {
                    "sent": "Take the union bound over those three terms and after a little bit of algebra this is what you get.",
                    "label": 0
                },
                {
                    "sent": "So in other words, what we now have is that.",
                    "label": 0
                },
                {
                    "sent": "Our empirical criterion.",
                    "label": 0
                },
                {
                    "sent": "Minus the Hilbert Schmidt norm of the covariance operator is bounded by one term, which behaves like log Delta over M ^2 of that and the bias term which behaves like 1 / M. And of course I can use that to derive an independent test for random variables.",
                    "label": 0
                },
                {
                    "sent": "Ask, would you put 2 four came from?",
                    "label": 0
                },
                {
                    "sent": "Fun.",
                    "label": 0
                },
                {
                    "sent": "Well, it came from the fact that, well, you have to bound for the pairs triples and quadruples and what we did is we just been applied the Union bound.",
                    "label": 0
                },
                {
                    "sent": "So I'm quite sure rather than 0.24 you could have a better constant.",
                    "label": 0
                },
                {
                    "sent": "But you wouldn't be able to get a better rate.",
                    "label": 0
                },
                {
                    "sent": "That is, unless you use things like local Bernstein type inequality's for use statistics.",
                    "label": 0
                },
                {
                    "sent": "And in this case you should be able to get maybe something that behaves more like one over him rather than one over square root team.",
                    "label": 0
                },
                {
                    "sent": "So in other words, since you can bound the variance, you should be able to get something that scales a little bit better for small sample size.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course in the end.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For all of those criteria, the proof is in the pudding, so we compared a whole bunch of.",
                    "label": 0
                },
                {
                    "sent": "Different independence criteria.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we.",
                    "label": 0
                },
                {
                    "sent": "Well, synthetically generated a set of random variables, anything between.",
                    "label": 0
                },
                {
                    "sent": "Just two different distributions, or 16 different ones.",
                    "label": 0
                },
                {
                    "sent": "We looked at different sample sizes and repeated that many times.",
                    "label": 0
                },
                {
                    "sent": "And then we compared a whole bunch of different independence criteria, so that's fast ICA of upheaval, and.",
                    "label": 0
                },
                {
                    "sent": "Jad infomax.",
                    "label": 0
                },
                {
                    "sent": "Radical, I think that's why typically at all.",
                    "label": 0
                },
                {
                    "sent": "See if I I see I actually forgot which, which one Arthur compared against.",
                    "label": 0
                },
                {
                    "sent": "These are some kernel constraint covariance other criteria which people have been using in the past.",
                    "label": 0
                },
                {
                    "sent": "I think one of those is the one of Bach and Jordan.",
                    "label": 0
                },
                {
                    "sent": "And these are the two new criteria, and the G&L mean that in one case we use the Gaussian, the other case Hello Plus kernel.",
                    "label": 0
                },
                {
                    "sent": "Turns out the Laplacian electric performance is better.",
                    "label": 0
                },
                {
                    "sent": "But what you can see is OK.",
                    "label": 0
                },
                {
                    "sent": "The two strongest criteria are radical.",
                    "label": 0
                },
                {
                    "sent": "Which is a criterion specifically tuned for linear ICA?",
                    "label": 0
                },
                {
                    "sent": "And our Hilbert Schmidt independence criterion.",
                    "label": 0
                },
                {
                    "sent": "We perform a little bit worse on one test, and otherwise we're always in the same.",
                    "label": 0
                },
                {
                    "sent": "Ballparks are basically, you cannot distinguish it from radical.",
                    "label": 0
                },
                {
                    "sent": "What we plot.",
                    "label": 0
                },
                {
                    "sent": "OK, So what what those numbers are is the amerida vergence.",
                    "label": 0
                },
                {
                    "sent": "So it's basically you take a whole bunch of known sources.",
                    "label": 0
                },
                {
                    "sent": "You mix them and you check how well you are able to unmix them.",
                    "label": 0
                },
                {
                    "sent": "So basically it's some sort of a matrix norm and that it's a quantity that's reasonably well respected in the ICA community.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Colonel Myth is different from the approach that Doctor Jordan kept well.",
                    "label": 0
                },
                {
                    "sent": "Fondren is actually one of the columns here.",
                    "label": 0
                },
                {
                    "sent": "The key drawback in the original box in Jordan Paper is if you actually look at the theory there criterion is start independent.",
                    "label": 0
                },
                {
                    "sent": "Now you might wonder why do they get away with it?",
                    "label": 0
                },
                {
                    "sent": "Because actually in the implementation thing they talk about using some reduced rank expansion and some regularization.",
                    "label": 0
                },
                {
                    "sent": "Well, and it's essentially those two tricks that make it work.",
                    "label": 0
                },
                {
                    "sent": "If you drop those and compute exactly and I mean all you have to do is just sit down and look at the Canonical correlation expansion.",
                    "label": 0
                },
                {
                    "sent": "You will see the criterion is data independent.",
                    "label": 0
                },
                {
                    "sent": "This is one of those amazing cases where a broken theory leads to really good results.",
                    "label": 0
                },
                {
                    "sent": "I'll show you on the next slide.",
                    "label": 0
                },
                {
                    "sent": "Actually a little bit more of a comparison to those, but yes, we do beat them.",
                    "label": 0
                },
                {
                    "sent": "I mean not very dramatically.",
                    "label": 0
                },
                {
                    "sent": "I think this is.",
                    "label": 0
                },
                {
                    "sent": "Barbara Jordan is one of those columns here.",
                    "label": 0
                },
                {
                    "sent": "I forgot which one it is.",
                    "label": 0
                },
                {
                    "sent": "I mean, yeah, we're better, but I mean it's not.",
                    "label": 0
                },
                {
                    "sent": "Outrageously better, however, this is if you get the regularization constant for their method exactly right now, let me show you what happens if you need to scale a few of those things.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "One of I think the.",
                    "label": 0
                },
                {
                    "sent": "I think this is the Bakken Jordan method.",
                    "label": 0
                },
                {
                    "sent": "As far as I remember.",
                    "label": 0
                },
                {
                    "sent": "And what you can see is if you.",
                    "label": 0
                },
                {
                    "sent": "Get the regularization right.",
                    "label": 0
                },
                {
                    "sent": "It actually does pretty well.",
                    "label": 0
                },
                {
                    "sent": "If you don't get it right, it can actually perform rather awfully.",
                    "label": 0
                },
                {
                    "sent": "Whereas our criteria is the black line down here because it doesn't have any of those regularization constant, it doesn't require it well.",
                    "label": 0
                },
                {
                    "sent": "It's actually quite nice all the time.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So using your criteria.",
                    "label": 0
                },
                {
                    "sent": "Find.",
                    "label": 0
                },
                {
                    "sent": "Yes, So what you do is if you have an independence criterion, you then optimize overall rotations.",
                    "label": 0
                },
                {
                    "sent": "And in the end world you will get a demixing matrix, some rotation, which then tells you.",
                    "label": 0
                },
                {
                    "sent": "Well, so you basically use the independence criterion as a proxy.",
                    "label": 0
                },
                {
                    "sent": "And your country lends itself well to this kind of information, yes?",
                    "label": 0
                },
                {
                    "sent": "Then we, well, we it lends itself reasonably well to it.",
                    "label": 0
                },
                {
                    "sent": "I mean, you need to do a few tricks in practice.",
                    "label": 0
                },
                {
                    "sent": "I mean, you will use the incomplete Cholesky factorization and so on, in which case you get something that scales essentially linearly with the number of observations.",
                    "label": 0
                },
                {
                    "sent": "I'll get to that in a moment.",
                    "label": 0
                },
                {
                    "sent": "Well, the thing too.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Should be another.",
                    "label": 0
                },
                {
                    "sent": "Yep, the thing to note though, and this is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, why I think our criterion is way better.",
                    "label": 0
                },
                {
                    "sent": "I mean otherwise you could just use Mark in Jordan and say well beat so the theory is broken but.",
                    "label": 0
                },
                {
                    "sent": "Our criterion is very resistant to outliers.",
                    "label": 0
                },
                {
                    "sent": "So this dotted line here on top is radical.",
                    "label": 0
                },
                {
                    "sent": "So that's the best performing ICA method.",
                    "label": 0
                },
                {
                    "sent": "What happens though is if you add in some outliers, this method breaks down quite quite badly, whereas our criterion is very good.",
                    "label": 0
                },
                {
                    "sent": "What's this came?",
                    "label": 0
                },
                {
                    "sent": "Well, that's just the number that that's the I think the fraction of outliers.",
                    "label": 0
                },
                {
                    "sent": "I think that's what it is.",
                    "label": 0
                },
                {
                    "sent": "And it.",
                    "label": 0
                },
                {
                    "sent": "One of the one of the reasons why this kernel based method is quite nice in terms of outliers I guess, is cause the kernel itself is bounded.",
                    "label": 0
                },
                {
                    "sent": "And this and we have not done in any sort of theoretical analysis of this so far.",
                    "label": 0
                },
                {
                    "sent": "Being, however, implies that the value of the independence criterion cannot be changed too much.",
                    "label": 0
                },
                {
                    "sent": "If you change some data.",
                    "label": 0
                },
                {
                    "sent": "So we haven't done any formal analysis of a breakdown point of the estimator, but I mean that would be.",
                    "label": 0
                },
                {
                    "sent": "Something that I guess would be the key to the analysis.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just getting back to how you actually.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and implemented well, you would not necessarily compute the full kernel matrix, because if you've got 100,000 observations, then computing 100 thousand 100,000 matrix is awful.",
                    "label": 0
                },
                {
                    "sent": "What you would do in status you would compute some low rank approximation and for instance you could use.",
                    "label": 0
                },
                {
                    "sent": "Like all the tricks for, say, sparse, greedy approximation or nice dream or whatever for that particular purpose.",
                    "label": 0
                },
                {
                    "sent": "So you would use statistics in that case.",
                    "label": 0
                },
                {
                    "sent": "Again, to show that this approximation is reasonably good.",
                    "label": 0
                },
                {
                    "sent": "So Shields method from before would help.",
                    "label": 0
                },
                {
                    "sent": "But the point is, this is essentially the.",
                    "label": 0
                },
                {
                    "sent": "I'd say the best independence criterion you can get at the moment.",
                    "label": 0
                },
                {
                    "sent": "And the key point is it was not designed specifically for linear ICA, but it works for it very well.",
                    "label": 0
                },
                {
                    "sent": "So you could also think of nicely parameterized nonlinear ICA criteria, but I mean.",
                    "label": 0
                },
                {
                    "sent": "You need to define the class of transformations rather sensibly.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Now this is quite new.",
                    "label": 0
                },
                {
                    "sent": "It's in fact unpublished so far.",
                    "label": 0
                },
                {
                    "sent": "Well, if we want to check whether to distributions are the same, how would you go about doing this?",
                    "label": 0
                },
                {
                    "sent": "Well, there's a simple linear witness to check whether two distributions are not the same.",
                    "label": 0
                },
                {
                    "sent": "Let's take some linear function, F of X is www.x.",
                    "label": 1
                },
                {
                    "sent": "And check whether the means deviate even if they do, we clearly know those two functions are not the same.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we compute the mean of that minus the mean of this.",
                    "label": 0
                },
                {
                    "sent": "Now, a nonlinear witness will be, and that's now necessary and sufficient, and you can prove that.",
                    "label": 1
                },
                {
                    "sent": "But well.",
                    "label": 0
                },
                {
                    "sent": "Actually, there should be a square bracket here that the means for all those for any arbitrary function.",
                    "label": 0
                },
                {
                    "sent": "If the if this always vanish, then I know those two functions are the same.",
                    "label": 0
                },
                {
                    "sent": "Or Conversely, if I find some function for which the means deviate from zero.",
                    "label": 0
                },
                {
                    "sent": "The two distributions are not the same.",
                    "label": 0
                },
                {
                    "sent": "This is actually something that you would use typically in the context of weak convergence.",
                    "label": 0
                },
                {
                    "sent": "So we convergence of of some distribution to another one means simply that.",
                    "label": 0
                },
                {
                    "sent": "For all if from some class.",
                    "label": 0
                },
                {
                    "sent": "The expected value with respect to some distribution PN of FX converges to expect value with respect to P of F. And once is that if this holds for N going to Infinity.",
                    "label": 0
                },
                {
                    "sent": "That PN converges to P. And essentially all we've done is we've just used this criterion and turn it upside down to get the test for the distributions all the same.",
                    "label": 0
                },
                {
                    "sent": "For finite sample size, however, the problem is we can almost always find a nonlinear witness.",
                    "label": 1
                },
                {
                    "sent": "The only question is how meaningful is this nonlinear witness?",
                    "label": 0
                },
                {
                    "sent": "Yeah, go back.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Is there an essential distinction between this and classification?",
                    "label": 0
                },
                {
                    "sent": "So we can just we just try to customize in the Exeter the wife set.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, let me go to the next slide and you'll see why this is not new.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry, good idea.",
                    "label": 0
                },
                {
                    "sent": "See, these two distributions are actually the same.",
                    "label": 0
                },
                {
                    "sent": "Well trying to classify mean what would you exactly classify.",
                    "label": 0
                },
                {
                    "sent": "Red versus blue.",
                    "label": 0
                },
                {
                    "sent": "Well, all you would get through this is.",
                    "label": 0
                },
                {
                    "sent": "Well, whether the supports are the same.",
                    "label": 0
                },
                {
                    "sent": "Well, the only thing is you might want to try and classify and see how far away from .5 you will be able to get your classification error.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you could probably attack it like this.",
                    "label": 0
                },
                {
                    "sent": "That's true, haven't thought of that yet and you might be able to get.",
                    "label": 0
                },
                {
                    "sent": "Reasonable balance with that.",
                    "label": 0
                },
                {
                    "sent": "However, what I'm going to present here does not require any optimization.",
                    "label": 0
                },
                {
                    "sent": "So if you're trying to classify, you have to run a classifier into a whole bunch of optimization.",
                    "label": 0
                },
                {
                    "sent": "When I'm going to show you here doesn't require that.",
                    "label": 0
                },
                {
                    "sent": "So maybe the only advantage that what I'm presenting is is an algorithmic one.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The algorithm is so simple that.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can code it up in a few lines.",
                    "label": 0
                },
                {
                    "sent": "Well, these I'm claiming are actually drawn from two different distributions.",
                    "label": 1
                },
                {
                    "sent": "And it's probably quite hard to spot, but essentially what happened is the means are slightly shifted.",
                    "label": 0
                },
                {
                    "sent": "Or maybe we're just unlucky.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You wouldn't be able to really tell so easily.",
                    "label": 0
                },
                {
                    "sent": "So the theorem.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have is that for a pseudo unbounded range function.",
                    "label": 0
                },
                {
                    "sent": "Well, these two distributions are the same.",
                    "label": 1
                },
                {
                    "sent": "The expectations are the same for all F and.",
                    "label": 1
                },
                {
                    "sent": "Well, basically again, what you do is you take distributions defined on the measure of the sets.",
                    "label": 1
                },
                {
                    "sent": "The Sigma algebra thing considered deviations between such States and that's basically what you get.",
                    "label": 0
                },
                {
                    "sent": "So what I'm now defining as my statistic is.",
                    "label": 0
                },
                {
                    "sent": "Let's call it MMD.",
                    "label": 0
                },
                {
                    "sent": "With respect to distributions, Anna function class, it's the soup of the functions from this function class that the expected value of F of X.",
                    "label": 0
                },
                {
                    "sent": "Differs from the expected value from FY.",
                    "label": 0
                },
                {
                    "sent": "Without loss of generality, I'm assuming that all those function classes are symmetric, otherwise I would have to take an absolute value somewhere, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just makes life a bit easier.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Actually, there's a simple empirical criterion that you can fashion from this.",
                    "label": 0
                },
                {
                    "sent": "We just replace all the expectations by empirical averages.",
                    "label": 0
                },
                {
                    "sent": "So now instead of distributions I have sets of observations and I'm defining this empirical criterion to be the soup again over functions from this function class over the empirical averages for F of X and the empirical average for F of Y.",
                    "label": 0
                },
                {
                    "sent": "Now this in general, well, yes, you could probably go and workout the statistical properties of that through rather more averages directly.",
                    "label": 0
                },
                {
                    "sent": "But there is one special case where this derivation becomes rather nice.",
                    "label": 0
                },
                {
                    "sent": "So let's say that F is the unit ball of functions in a bank space.",
                    "label": 1
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "If I assume that F of X is given body inner product between F and some evaluation functional.",
                    "label": 0
                },
                {
                    "sent": "So this I'm not only requiring it for the Hilbert space, but I can just state that in a backspace too, and afterwards we'll see that the Kolmogorov Smirnov test is actually specific case of that.",
                    "label": 0
                },
                {
                    "sent": "I can see by the very fact that, well, this is a banner space.",
                    "label": 0
                },
                {
                    "sent": "But this quantity up here is given by the dual norm.",
                    "label": 1
                },
                {
                    "sent": "Of the difference of the means of evaluation functionals.",
                    "label": 0
                },
                {
                    "sent": "Now, why is this so?",
                    "label": 0
                },
                {
                    "sent": "Let me spell out the proof in a bit more detail, because that's actually one of the key tricks of of the entire idea.",
                    "label": 0
                },
                {
                    "sent": "This patient is all these functions F yes, you're just saying you put it normal name to make it yes, yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, this quantity here can be written as the soup.",
                    "label": 0
                },
                {
                    "sent": "Over Norm of F. Less equal than one.",
                    "label": 0
                },
                {
                    "sent": "Off well.",
                    "label": 0
                },
                {
                    "sent": "1 / M. Some over I going from one to M inner product between F. And Phi of XI.",
                    "label": 0
                },
                {
                    "sent": "Minus one over in some of the same expression in the wise.",
                    "label": 0
                },
                {
                    "sent": "OK, which can be written as the inner product between F and here I get 1 / M Some over I going from one to M fi of X I -- 1 over in some overriding from one to N Phi of Yi.",
                    "label": 0
                },
                {
                    "sent": "You know product clothes and the soup, of course.",
                    "label": 0
                },
                {
                    "sent": "Now, just by the very definition of the dual norm, this expression here is defined as the dual norm of this expression.",
                    "label": 1
                },
                {
                    "sent": "Which is exactly what we have here.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "If I can compute this quantity efficiently?",
                    "label": 0
                },
                {
                    "sent": "Then I will be able.",
                    "label": 0
                },
                {
                    "sent": "To, well, get a convenient statistic.",
                    "label": 0
                },
                {
                    "sent": "So now let's look at a special case.",
                    "label": 0
                },
                {
                    "sent": "Which will give us the Kolmogorov Smirnov test.",
                    "label": 0
                },
                {
                    "sent": "Let's define.",
                    "label": 0
                },
                {
                    "sent": "Phi of XI to be the indicator functions.",
                    "label": 0
                },
                {
                    "sent": "Going from minus Infinity to XI.",
                    "label": 0
                },
                {
                    "sent": "Sorry OK. That's an indicator function.",
                    "label": 0
                },
                {
                    "sent": "It says here and one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                },
                {
                    "sent": "So it's basically one if you're in the interval between minus Infinity and XI, and otherwise it's 0.",
                    "label": 0
                },
                {
                    "sent": "So this leaves will be elements in L Infinity.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, I take my space of functions if.",
                    "label": 0
                },
                {
                    "sent": "To be that the.",
                    "label": 0
                },
                {
                    "sent": "L1 norm of faith is less equal in one.",
                    "label": 0
                },
                {
                    "sent": "In which case, well, what do I get?",
                    "label": 0
                },
                {
                    "sent": "This expression here.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I should have written it as follows.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "No, sorry starts from XI.",
                    "label": 0
                },
                {
                    "sent": "And goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "In this case, these functions here are the cumulative empirical cumulative distribution functions.",
                    "label": 1
                },
                {
                    "sent": "For the X and the Y's.",
                    "label": 0
                },
                {
                    "sent": "OK the.",
                    "label": 0
                },
                {
                    "sent": "In a product is right before fun.",
                    "label": 0
                },
                {
                    "sent": "What is your reputation, built space in my store?",
                    "label": 0
                },
                {
                    "sent": "Well, in this case I'm not talking about reproducing kernel space, but about the backspace.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And, well, what now, if, if are, are the evaluations at those locations or linear combinations of the valuations?",
                    "label": 0
                },
                {
                    "sent": "And I'm just picking those to be in L1.",
                    "label": 0
                },
                {
                    "sent": "That's your space.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "These expressions here will turn into the cumulative distribution functions.",
                    "label": 0
                },
                {
                    "sent": "The empirical ones.",
                    "label": 0
                },
                {
                    "sent": "So now the alien Infinity Norm of the difference between those is exactly the statistic that the Kolmogorov Smirnov test looks at.",
                    "label": 0
                },
                {
                    "sent": "So if I had.",
                    "label": 0
                },
                {
                    "sent": "Then we'll can you get one of those step functions here.",
                    "label": 0
                },
                {
                    "sent": "Of course it will go to one and maybe here's another one.",
                    "label": 0
                },
                {
                    "sent": "Sorry of course would have been too much.",
                    "label": 0
                },
                {
                    "sent": "Maybe it continues like this.",
                    "label": 0
                },
                {
                    "sent": "Then the L Infinity Norm would be this size.",
                    "label": 0
                },
                {
                    "sent": "And for the living Cantelli theorem.",
                    "label": 0
                },
                {
                    "sent": "You know that this random variable here behaves like order of one over square root M. And then you can use this to fashion a statistical test out of this quantity.",
                    "label": 0
                },
                {
                    "sent": "And it's for univariate random variables.",
                    "label": 0
                },
                {
                    "sent": "This is a pretty good thing that you can do.",
                    "label": 0
                },
                {
                    "sent": "Trying to extend the Kolmogorov Smirnov tests to arbitrary domains is.",
                    "label": 0
                },
                {
                    "sent": "Less than trivial.",
                    "label": 0
                },
                {
                    "sent": "Because the CDF doesn't really exist so nicely anymore.",
                    "label": 0
                },
                {
                    "sent": "So try defining a CDF on tests.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly.",
                    "label": 0
                },
                {
                    "sent": "Where this general formulation comes to our rescue?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Confused because this evaluation functionality.",
                    "label": 0
                },
                {
                    "sent": "Specific.",
                    "label": 0
                },
                {
                    "sent": "Now here you just select roughly alcohol with special functions.",
                    "label": 0
                },
                {
                    "sent": "Yes, so all I've said all I'm saying is the Kolmogorov Smirnov test is a special case of this general strategy, and for that specific choice of Phi, and if I get this test out of it.",
                    "label": 0
                },
                {
                    "sent": "But if I choose different files and will get completely different tests out of it, so it basically produces a large family of tests, what will actually also see if we have time?",
                    "label": 0
                },
                {
                    "sent": "Is that the?",
                    "label": 0
                },
                {
                    "sent": "The L2 distance between two kernel density estimates also is a special instance of this.",
                    "label": 0
                },
                {
                    "sent": "Becausw well for most cases, actually working with Hilbert space is a bit more convenient than working with banner spaces.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for Hilbert space in particular, well, we can exploit that for fix on 5X J.",
                    "label": 0
                },
                {
                    "sent": "Just this is just the kernel and this is well defined.",
                    "label": 1
                },
                {
                    "sent": "So if I square this quantity here, that's what I'm getting.",
                    "label": 0
                },
                {
                    "sent": "So in other words, I can compute this statistic by just computing values on a bunch of entries of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "So now optimization required.",
                    "label": 0
                },
                {
                    "sent": "You can actually get it even more cheaply by just some subsampling schemes.",
                    "label": 0
                },
                {
                    "sent": "And well, they will have the same statistical properties.",
                    "label": 0
                },
                {
                    "sent": "Now if this function K where the inner product between two parts in Windows kernel functions and this will be the L2 distance between the kernel density estimates.",
                    "label": 1
                },
                {
                    "sent": "So in other words, if I approximate PX to be 1 / M sum over, I going from one to M. Kappa of well say X I -- X.",
                    "label": 0
                },
                {
                    "sent": "And likewise for PN and I define K of X&X prime to be the integral of Kappa.",
                    "label": 1
                },
                {
                    "sent": "Of X -- Z. Kappa of X prime minus seed dies it.",
                    "label": 0
                },
                {
                    "sent": "Then I can see that the L2 distance between PXMPY.",
                    "label": 0
                },
                {
                    "sent": "Squared is exactly the statistic here.",
                    "label": 0
                },
                {
                    "sent": "So now we have another test that people might use for testing between distributions as a special case of this framework.",
                    "label": 0
                },
                {
                    "sent": "But of course family, yes.",
                    "label": 0
                },
                {
                    "sent": "Just there are many kernels, but there were so many of these kernels so yeah sure.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "But basically now we can use one machinery to analyze all of them, which is nice.",
                    "label": 0
                },
                {
                    "sent": "Distribution free yes.",
                    "label": 0
                },
                {
                    "sent": "Colonel bonus.",
                    "label": 0
                },
                {
                    "sent": "Yes, but in the general case.",
                    "label": 0
                },
                {
                    "sent": "You will.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, well our test is distribution free in the sense that.",
                    "label": 0
                },
                {
                    "sent": "While the statistic vanish is if and only if the random variables are independent.",
                    "label": 0
                },
                {
                    "sent": "However, what you're doing about the choice of the kernel and I see this as a feature rather than a drawback.",
                    "label": 0
                },
                {
                    "sent": "You can prioritize which functions you're going to test for first, So what?",
                    "label": 0
                },
                {
                    "sent": "Sometimes some practical statisticians complain about is they save all the Kolmogorov Smirnov test is very nice, but it doesn't really pick up the right dependencies that we want.",
                    "label": 0
                },
                {
                    "sent": "They say it's not strong enough or whatever for their specific problem.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm saying here sure, that's all fine.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can just go and pick the right kernel where you believe the discrepancies will manifest themselves first.",
                    "label": 0
                },
                {
                    "sent": "And by biasing it in the right way, you will get the test, which for the practical problems that you're trying to solve is more selective.",
                    "label": 0
                },
                {
                    "sent": "Yes, Anne.",
                    "label": 0
                },
                {
                    "sent": "Well, if I get a bit of time I can get to this in a bit more detail.",
                    "label": 0
                },
                {
                    "sent": "Well, what you can show is if it's a universal kernel, then even if you screw up the bandwidth with large enough number of observations you will detect it.",
                    "label": 0
                },
                {
                    "sent": "But yeah, this might not be quite as efficient thing.",
                    "label": 0
                },
                {
                    "sent": "OK, let me first explain how the machinery actually works.",
                    "label": 0
                },
                {
                    "sent": "So first.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You go and get the large deviation bound.",
                    "label": 1
                },
                {
                    "sent": "Let's just make the are made so all you're doing is you have the norm of a bunch of random variables and then you just argue that if you replace one of the observations by another one and if all the entries are bounded thing, well you get the hurting type rate, which is quite convenient.",
                    "label": 0
                },
                {
                    "sent": "And so you get basically something which behaves like sqrt M + N / M times in.",
                    "label": 0
                },
                {
                    "sent": "Now for.",
                    "label": 0
                },
                {
                    "sent": "The norm itself.",
                    "label": 0
                },
                {
                    "sent": "In order to get this expected value.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, what we can do in the general case where we can bound all this with rather more averages quite easily directly into the center code sample.",
                    "label": 0
                },
                {
                    "sent": "Tricking it works.",
                    "label": 0
                },
                {
                    "sent": "Mailbox space case where a bit more fortunate, because what we can do is we can just.",
                    "label": 0
                },
                {
                    "sent": "Show well by means inequality.",
                    "label": 0
                },
                {
                    "sent": "The expectation of this term squared is less equal than the expected value of the squared term, which, after a little bit of algebra, just turns out to be again something that we have like order of square root N + N / M N. And here I have the expected value, the expected length, so the same feature space minus the expected overlap.",
                    "label": 0
                },
                {
                    "sent": "And now let's get back to your question about the bandwidth.",
                    "label": 0
                },
                {
                    "sent": "This is exactly where this shows up.",
                    "label": 0
                },
                {
                    "sent": "So if I choose a kernel with a very narrow bandwidth, everything will look orthogonal.",
                    "label": 0
                },
                {
                    "sent": "So in other words, this quantity here will pretty much be 0.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that means if all the data are quite long.",
                    "label": 0
                },
                {
                    "sent": "But I will get something that doesn't really vanish so quickly anymore.",
                    "label": 0
                },
                {
                    "sent": "Ideally I want this right inside to be small.",
                    "label": 0
                },
                {
                    "sent": "If it's small, that means I can have a very aggressive threshold.",
                    "label": 0
                },
                {
                    "sent": "Now they make it really small.",
                    "label": 0
                },
                {
                    "sent": "Well can make it really small by making this expression here pretty large.",
                    "label": 0
                },
                {
                    "sent": "And this means choosing a wider bandwidth.",
                    "label": 0
                },
                {
                    "sent": "But if you choose a bandwidth that's too wide, then this will be so small that the statistical term for the table bound will kick in.",
                    "label": 0
                },
                {
                    "sent": "So we have those two effects.",
                    "label": 0
                },
                {
                    "sent": "One is the tail bond which.",
                    "label": 0
                },
                {
                    "sent": "Which came from Macdiarmid, which means that.",
                    "label": 0
                },
                {
                    "sent": "The random variable itself is unreliable.",
                    "label": 0
                },
                {
                    "sent": "The second term is that well in expectation, well just even if the two distributions were the same things.",
                    "label": 0
                },
                {
                    "sent": "To deviate a little bit from each other just because I have a finite number of observations.",
                    "label": 0
                },
                {
                    "sent": "And so it's those two effects that I need to control.",
                    "label": 0
                },
                {
                    "sent": "That's exactly what I can find here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But you can actually show if that the test is reasonably sensitive, so we can get order of square root in deviations from the identity.",
                    "label": 0
                },
                {
                    "sent": "The right itself is tight and you can check that easily by just taking Gaussian random variables.",
                    "label": 0
                },
                {
                    "sent": "And let's say they have both the same.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distribution say unit variance and different means you want to test whether the two distributions are the same.",
                    "label": 0
                },
                {
                    "sent": "Then we will get something that looks like this, and that's exactly the quantity that we had before.",
                    "label": 0
                },
                {
                    "sent": "This is basically something like the trace of the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And for exponential families, well under certain assumptions, this also bounced the Cal diversions.",
                    "label": 1
                },
                {
                    "sent": "So what we've done now is we have.",
                    "label": 0
                },
                {
                    "sent": "So basically we have a criterion which.",
                    "label": 0
                },
                {
                    "sent": "Incorporates a couple of known distribution tests as special cases.",
                    "label": 0
                },
                {
                    "sent": "Well, so I thought that was quite convenient.",
                    "label": 0
                },
                {
                    "sent": "So summing it up I mean.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we have is.",
                    "label": 0
                },
                {
                    "sent": "A simple linear test for checking the properties of some distribution.",
                    "label": 1
                },
                {
                    "sent": "We have a simple algorithm that goes with it.",
                    "label": 0
                },
                {
                    "sent": "The power of the test comes from using nonlinear functions.",
                    "label": 1
                },
                {
                    "sent": "In Hilbert spaces, that's usually quite easy and well, yeah, we looked at supports of distributions, independence and entity.",
                    "label": 0
                },
                {
                    "sent": "Not saying that's the only test for the identity.",
                    "label": 0
                },
                {
                    "sent": "And yeah, you can use it for ICA verification and fraud detection merging databases.",
                    "label": 1
                },
                {
                    "sent": "And yeah, OK, well that's the 30 seconds commercial now.",
                    "label": 0
                },
                {
                    "sent": "So if you want to go to a machine learning summer school, there most likely will be one in February early next year.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The site isn't up yet.",
                    "label": 0
                },
                {
                    "sent": "2006 death.",
                    "label": 0
                },
                {
                    "sent": "That's what it would be, yeah.",
                    "label": 0
                },
                {
                    "sent": "Australia now.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I know we're behind, but not gonna tell you.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, whatever and yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean if you want to get a good tan on sabbatical will come and visit us.",
                    "label": 0
                },
                {
                    "sent": "So that's what I want to say.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}