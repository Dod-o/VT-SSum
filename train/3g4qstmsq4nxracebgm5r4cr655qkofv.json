{
    "id": "3g4qstmsq4nxracebgm5r4cr655qkofv",
    "title": "Why Does Unsupervised Pre-training Help Deep Discriminant Learning?",
    "info": {
        "author": [
            "Dumitru Erhan, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "March 26, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_erhan_wduphddl/",
    "segmentation": [
        [
            "Thank you smell so."
        ],
        [
            "Yeah, Russ has done the perfect introduction for deep networks.",
            "I think he sort of thought that I would be introducing them or but I thought that he would be introducing them so at least he had.",
            "He told us about the successes and how they can be helpful in lots of fields.",
            "Apparently in politics as well so."
        ],
        [
            "So we you know there's the question that some of you might might think about is why do we want to do deep models or what's so interesting about them?",
            "So we like to think, you know, several kind of vague ideas on vague, but perhaps important ideas of why deep learning could be could be interesting.",
            "First of all, this is that the brains are sort of deep models of the world, and second of all, perhaps more importantly is that we, as humans think of ideas or concepts as usual.",
            "In kind of a hierarchical fashion, you know you can see this in many aspects of life, but especially more precise domains like mathematics or simply when we when we learn about things in life, we learn in a kind of hierarchical fashion.",
            "So that's good that provides some sort of interesting parallel to the way that we're going to be using deep networks for learning as well.",
            "So there's some arguments that you show Bengio.",
            "Provided with the yellow on the on the efficiency of having these hierarchical representations for certain kind of restricted domains, but nevertheless interesting domains about learning functions on binary inputs.",
            "How they could be more efficient to actually learn?",
            "Hierarchical models have a hierarchical model of that data of those functions of those binary inputs, as opposed to so called shallow model and kind of, I guess, more generally, this plays into this idea that having deep models exploit statistical statistical strengths that you get from having hierarchies of latent variables, and that you can conceive of having a combinatorial gain of statistical strength.",
            "So to say, if you use them as opposed to just using one layer of latent variables.",
            "So the the question you might ask yourself is why?",
            "Why now?",
            "Because the tools for doing that have been there for quite awhile.",
            "I mean before 2006 people had access to multilane neural networks.",
            "It's not like it was complicated to implement them, but they were not like at least the fully connected versions of them.",
            "They were not.",
            "Particularly popular, and it's so clear actually why, but I guess the my 'cause I was not really a big researcher before 1006.",
            "Neither am I now, but.",
            "But I can have a more informed opinion now, so I guess it's because they just didn't work.",
            "I mean, they didn't work better than just one layer neural networks, so in 2006 Geoff Hinton and collaborators they.",
            "Discovered this trick, I, well, I don't know if they discovered all they thought really hard and they came up with this with this really good idea of using unsupervised pretraining with restricted Boltzmann machines to initialize these models.",
            "These deep neural networks and to make them actually work.",
            "And as we saw it works for all sorts of things like like usually 28 by 28 images.",
            "But still it works for images, for digits, faces, stuff like that so.",
            "LP stuff just pretty cool.",
            "You know there's been a flurry of kind of papers on the subject, and a lot of them basically use the same kind of recipe.",
            "You know, take something that's hierarchical, use unsupervised pre training with some circular machines or some other Baltimore machines or autoencoders, or even kernel.",
            "PCA, as we've seen this this year, Snips, Loren Saul and Mitchell, if I pronounce it correctly.",
            "And the use this as initialization for for this hierarchical model, and then do discriminative training.",
            "And it works really well and it's widely applied, but it's actually not quite clear why it works really well so."
        ],
        [
            "The plan that we had and that we still have is is to understand what's going on and to propose several hypothesis and to test them in a kind of rigorous fashion.",
            "So yes, the first step is to propose hypothesis.",
            "The second step is, I guess, observe the effects of pre training and we're talking, you know, really large scale kind of experiments where we spend a lot of CPU power.",
            "The third step is try to infer the role of pre training or what it actually does and how it actually does it and how it helps the performance and infer the level of agreement with our hypothesis.",
            "It could turn out that neither of our hypothesis actually true or only combination of these hypothesis is actually true.",
            "So what I'm talking about when I'm saying hypothesis is you can think about these.",
            "You know, for instance I give 2 examples and these will be kind of the main running examples of this talk.",
            "The first one is the regularization hypothesis and it says.",
            "Basically that pretraining the unsupervised components of this of this training of deep neural networks is constraining the model to model P of X the input distribution.",
            "So this is the regularization aspect of.",
            "It is constraining it to do P of X and it assumes that representing P of X with this model is actually going to be good for representing P of Y given X.",
            "So another hypothesis that's you know this is a testable hypothesis, another one that's also testable, I guess, is optimization hypothesis.",
            "That's how we call it.",
            "And it says basically that unsupervised initialization using either of these PBM's auto encoders and stuff like that.",
            "Lands the parameters near some better local minimum of P of Y given X.",
            "So this our objective function.",
            "And was basically says that if using unsupervised initialization you're going to reach a better local minimum, lower local minimum, which is not achievable by typical random initialization that we use in neural networks."
        ],
        [
            "So.",
            "I'm going to just jump straight to the kind of experiments one of our first experiments, and I think the the I mean very direct application or the very direct test of the hypothesis is we're just going to measure training error versus testing testing error overtime for a bunch of different networks.",
            "We've selected the hyperparameters in some sort of way.",
            "All have the same hyperparameter and the only thing that they differ.",
            "I mean as in all supervised networks have the same parameters.",
            "All networks with pre training have the same hyperparameters.",
            "The only thing that they differ in within the clusters are the initialization seed and we're just going to look at the training error versus testing error overtime and observe what's going on.",
            "And the interesting thing is that well, a couple of observations that are interesting.",
            "One is that for the same training error.",
            "The generalization error of the pre trained networks is lower.",
            "So which is which is which is interesting, but.",
            "More maybe relevant observation for regularization hypothesis for our hypothesis general is this gap that you're going to get at the end of training.",
            "Even after even at convergence, the pre trained networks do not seem to achieve low training error.",
            "At least it's going to be higher than what you're going to get with random initialization, so this kind of fits with the regularization interpretation of the what's going on in this world, since it basically acts like a regularizer, it it increases your training error, but it gives you generalization."
        ],
        [
            "And another experiment that we did is we said, well, let's see what's going on.",
            "If we're going to start fiddling with other constraints in the network, constraints that affect.",
            "The capacity of the network.",
            "So if you think of regularization is something's playing with capacity, there are other ways of playing with the capacity of a neural network, and one of them is.",
            "Shrinking the size of the layers, so if you just plot the testing error versus the layer size and this black thing here corresponds to the testing error of the networks with pre training with these ones with pre training the two red and blue and the black one without you see that it's as if the layer sizes sufficiently big.",
            "There's actually a gap you know the pre trained networks actually perform better, but as soon as you get to like 100 and below 700 units which is.",
            "Evidently too small for a good performance.",
            "We see this.",
            "This is pretty big gap in between, so it looks as if pretraining acts as an additional kind of capacity constraint onto the network.",
            "So the other."
        ],
        [
            "I meant, which is this kind of the source of pretty pictures is we.",
            "We just did this kind of projection.",
            "It's a bit of a crazy projection, but as I said, it provides pretty pictures.",
            "So we just said we were going to do this function space.",
            "Approximation of projection of the network, so we're going to take all the outputs.",
            "Projects.",
            "I'm into 2 dimensions using two nonlinear dimensionality reduction techniques.",
            "T, Sne, and Isomap, and we're going to look at what happens to the network.",
            "So each point in here represents a network.",
            "This is the cluster of networks with pre training.",
            "This is a cluster of networks without pretraining.",
            "Here with Isomap without.",
            "With and they sort of.",
            "The color indicates time, you know, as we go as we do an epic of training.",
            "This start here go go, go and spread out so it's a couple of interesting observations from here.",
            "One is that the.",
            "That works with and without pretraining.",
            "They seem to explore different kinds of space.",
            "You know they don't kind of intersect each other, they go to each their own.",
            "Each of the network kind of goes to the local minimum.",
            "And I think one of the more interesting, more revealing observations from the Isomap plot is that from the point of view of networks, without pretraining, those with retraining look basically all the same, so you know they seem to occupy much smaller space compared to networks with free training.",
            "So."
        ],
        [
            "It seems, or at least from the experiment so far, that pretraining places the networks in this region of parameter space that is very different from the one that given by random initialization.",
            "I mean, it's not just from the kind of test errors, but from what we've seen in the in the in those pretty pictures, and it seems to be like kind of regularizer, but the important thing to realize here that it's a regularizer, but it only affects the starting point of the optimization so.",
            "The question that we posed ourselves as well since it only affects the starting point of the optimization.",
            "What if we had this source of very almost infinite data and we just initialized with pre training?",
            "But then we continued on continued to supervise training.",
            "Will it effect disappear?",
            "So and the answer is actually not."
        ],
        [
            "The effect does not disappear, and if you compare, this is with the.",
            "I think it's called Infinite I missed so the data set by Leon Bottou where he just generates kind of elastic deformations of M list.",
            "You can generate 2 to 32 of them.",
            "So if you plot the online classification error versus the number of samples we have, like here 10,000,000 examples, the black curve, one of them represents a network without free training and the red curve represents a network with pre training.",
            "You can see that they don't necessarily intersect.",
            "And as I said, it's the same kind of scenario.",
            "Network starts from.",
            "From some solution that is given by unsupervised initialization an it goes somewhere, but it doesn't.",
            "They don't seem to be able to be converging.",
            "We're talking of course here about really small errors, so it's you know, since it's somewhat infinite data set, we didn't actually run until completion.",
            "There's some sort of attention span that I have.",
            "So it's clearly the starting point of this non convex optimization matters.",
            "Even in a scenario with essentially unbounded trading it and it's kind of surprising result because it doesn't follow the standard interpretation of a regularizer.",
            "You would think of if you think of a standard regularizer, its effect should somewhat.",
            "Go away if you have alot alot.",
            "Alot of training data labeled training data.",
            "The other interesting."
        ],
        [
            "Experiment that we did is we said well if we can maybe quantify in which sense pre training affects the outcome of the of the supervised optimization.",
            "So what we mean by this is we're going to literally vary the examples during training.",
            "So as I said, we have 10,000,000 examples.",
            "Now we're going to be very.",
            "We're going to do the normal training to 2.5 million examples of unsupervised training followed by 7 and 1/2 million examples of supervised training.",
            "And now at each like, we're going to vary only the 1st million examples and keep the rest fixed.",
            "So we're going to do some sort of shuffling of them.",
            "Re sample, keep the rest of them fixed, and then measure the variance of the function that we learn at the end of training.",
            "So it's going to be some function on the variance of this function on a test set and plot this.",
            "You know these variants as a function of where we varied, at which point in time did we varied examples so it's interesting here is that.",
            "The variance is very high at the beginning, so both so the blue curve is a network with without pretraining and the red curve is a network with pre training.",
            "In both cases it's very susceptible, at least it seems so to the examples at the beginning of training.",
            "But the I'd say."
        ],
        [
            "The more surprising result is.",
            "This is where we start supervised training for the network with pre training.",
            "So this is where we before.",
            "Before here we did unsupervised learning after supervised learning.",
            "And if we compare this point with this point 4, so in both cases here we start supervised training.",
            "And the variance is much much lower, so pretraining seems to act as a sort of a.",
            "Survey variance reduction technique.",
            "So, as I said, there's two kind of two lessons to be learned here.",
            "There's early examples are change.",
            "A lot of things, and it's pre training access various reduction, which is again some sort of indication of."
        ],
        [
            "Of a of a regularizer, so to kind of wrap up a bit.",
            "Well, it seems that what's going on is then we have this kind of.",
            "Interpretation of more more more mechanistic explanation of what's going on is as the way it's become larger.",
            "During training, they become trapped in what we like to call basins of attraction and the initial updates.",
            "You know, the where you start from some sort of small weights, and then whatever the initial bids gets you in whatever quadrant they get you.",
            "This is where you're going to end up after after critical period, and these initial updates.",
            "This is the explain much more of the variance of the.",
            "Of the function that you're going to learn as opposed to updates at the end of training, so that's why it seems that pretraining seems to help.",
            "And it you know, it's it gets you to a basin of attraction with good generalization properties.",
            "It doesn't just get you to some random basis."
        ],
        [
            "Attraction, so that's good.",
            "So we had some early results that pointed towards the regularization hypothesis and our results with this online training.",
            "Give some sort of a more nuanced interpretation of what's going on.",
            "We explored this online setting and we saw that the pre training effect does not vanish.",
            "And we saw also that pretraining seems to be a variance kinda variance reduction technique.",
            "There's a couple of caveats to pre training as.",
            "It will have a positive effect and with as with basically almost any kind of semi supervised learning technique as long only as long as P of X is used, modeling P of X or representing P of X is useful for modeling P of Y given X Ann.",
            "It's interesting to note that this this influence of the high variance that we get from early examples could be troublesome because if we are truly in a very large scale online learning scenario.",
            "It might be worth it for algorithms to try to forget what happened in the beginning, you know and try to change to whatever is happening now, so it might be interesting to see how to escape from the basins of attraction that we are in.",
            "And the future work is to understand other semi supervised approaches.",
            "We've explored a couple but it would be nice to know what happens if you combine the two cost at the same time, supervised and unsupervised.",
            "And we have many more results in our upcoming accepted jail or paper.",
            "Thank you."
        ],
        [
            "Have you experimented with how the size of your pre training set affects how much?",
            "How much appreciated or so I think so I've tried."
        ],
        [
            "Here, so like in this kind of scenario, so it's sort of like a have.",
            "A budget of 10,000,000 examples.",
            "So if I allow myself 10,000,000 updates, 10,000,000 updates and then I sort of.",
            "Just do 1 million unsupervised and 9 million supervised or 2 million.",
            "It all boils down to the same thing, so it's very.",
            "In this case I think the lowest I've tried this 500,000 and it was basically the same kind of curve.",
            "So it seems that you don't need a lot sort of.",
            "I just chose because this is clear presentation, but it's almost any point in here I could have started with a gun to the same kind.",
            "Yes, yes.",
            "Yes yes.",
            "Well very small relative to the total number of samples that you have here.",
            "So the interesting thing is if you use very many, if you use seven 7 million or something, it's going to converge much faster to the to whatever minimum.",
            "So once you start supervised training, if you use.",
            "If you did alot alot alot of unsupervised learning.",
            "And then flip on the switch to the supervised training you're going to converge much faster, so at least in this particular scenario.",
            "From the publicity for all of like that, no more miles an hour.",
            "So saying you are maybe manifold which can be strictly popular.",
            "It seems to vary.",
            "Symmetry compression, provided you compress it later you are solving at the same coffee, so maybe there's some connection we can do without connected, applied for sensing.",
            "Here to say hi Superman.",
            "Perhaps you're right, I don't have enough knowledge about compressed sensing to to make a qualified state windows.",
            "I don't know.",
            "Yeah, it sounds sounds logically reasonable, though from what I know.",
            "One more yeah.",
            "So I tried this until 30,000,000 examples.",
            "We have no generate generate this.",
            "Synthetically it up there and what if your synthetic data generator isn't actually that good but busy?",
            "Just repeat back the original samples.",
            "Because then you would.",
            "This would be expected, so it could be an estimation for, well, I think the way they own both constructed it.",
            "I don't think it actually repeats the same example.",
            "From the given samples and sort of distortion, yeah, so basically it is the same samples just slightly distorted, so it's not really.",
            "Yeah yeah no.",
            "I mean it's it's yeah, it's of course it's yeah yeah it's limited by in an infinite list of this procedure.",
            "I wanted it, but it's hard to make you have.",
            "There isn't anything.",
            "There is one.",
            "It was nothing.",
            "Actually, I'm not really here by morning.",
            "Somehow?",
            "Oh yeah.",
            "That's what we're doing.",
            "So category in Central Park is loading, right?",
            "So in principle you would think that you have alot alot alot of things perhaps capable.",
            "Oh yeah.",
            "If anything is that reconfirmation.",
            "It's not just any random order.",
            "Getting the local meeting.",
            "Question.",
            "An upgrade on my priest like only spectacles featuring slightly output prices, yeah?",
            "On my Dick."
        ],
        [
            "Helps for the absolute training.",
            "Oh, it's it's it's.",
            "I mean it helps in the sense.",
            "I mean, in the same sense that it's your wherever you are.",
            "You know whatever pre training gets you.",
            "You're not like like like with the local minimum.",
            "You're not going to get away easily out of there.",
            "I guess this experiment was done exactly to show that.",
            "Variance reduction equals good performance.",
            "It's more variance.",
            "Reduction is a characteristic of regularization and pre training seems to have this characteristic.",
            "It's debatable where weather reducing variance actually, generally speaking improves performance, let's say.",
            "Nice to meet you again.",
            "Coffee break."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you smell so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, Russ has done the perfect introduction for deep networks.",
                    "label": 0
                },
                {
                    "sent": "I think he sort of thought that I would be introducing them or but I thought that he would be introducing them so at least he had.",
                    "label": 0
                },
                {
                    "sent": "He told us about the successes and how they can be helpful in lots of fields.",
                    "label": 0
                },
                {
                    "sent": "Apparently in politics as well so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we you know there's the question that some of you might might think about is why do we want to do deep models or what's so interesting about them?",
                    "label": 0
                },
                {
                    "sent": "So we like to think, you know, several kind of vague ideas on vague, but perhaps important ideas of why deep learning could be could be interesting.",
                    "label": 0
                },
                {
                    "sent": "First of all, this is that the brains are sort of deep models of the world, and second of all, perhaps more importantly is that we, as humans think of ideas or concepts as usual.",
                    "label": 0
                },
                {
                    "sent": "In kind of a hierarchical fashion, you know you can see this in many aspects of life, but especially more precise domains like mathematics or simply when we when we learn about things in life, we learn in a kind of hierarchical fashion.",
                    "label": 0
                },
                {
                    "sent": "So that's good that provides some sort of interesting parallel to the way that we're going to be using deep networks for learning as well.",
                    "label": 0
                },
                {
                    "sent": "So there's some arguments that you show Bengio.",
                    "label": 0
                },
                {
                    "sent": "Provided with the yellow on the on the efficiency of having these hierarchical representations for certain kind of restricted domains, but nevertheless interesting domains about learning functions on binary inputs.",
                    "label": 0
                },
                {
                    "sent": "How they could be more efficient to actually learn?",
                    "label": 0
                },
                {
                    "sent": "Hierarchical models have a hierarchical model of that data of those functions of those binary inputs, as opposed to so called shallow model and kind of, I guess, more generally, this plays into this idea that having deep models exploit statistical statistical strengths that you get from having hierarchies of latent variables, and that you can conceive of having a combinatorial gain of statistical strength.",
                    "label": 0
                },
                {
                    "sent": "So to say, if you use them as opposed to just using one layer of latent variables.",
                    "label": 0
                },
                {
                    "sent": "So the the question you might ask yourself is why?",
                    "label": 0
                },
                {
                    "sent": "Why now?",
                    "label": 0
                },
                {
                    "sent": "Because the tools for doing that have been there for quite awhile.",
                    "label": 0
                },
                {
                    "sent": "I mean before 2006 people had access to multilane neural networks.",
                    "label": 0
                },
                {
                    "sent": "It's not like it was complicated to implement them, but they were not like at least the fully connected versions of them.",
                    "label": 0
                },
                {
                    "sent": "They were not.",
                    "label": 0
                },
                {
                    "sent": "Particularly popular, and it's so clear actually why, but I guess the my 'cause I was not really a big researcher before 1006.",
                    "label": 0
                },
                {
                    "sent": "Neither am I now, but.",
                    "label": 0
                },
                {
                    "sent": "But I can have a more informed opinion now, so I guess it's because they just didn't work.",
                    "label": 0
                },
                {
                    "sent": "I mean, they didn't work better than just one layer neural networks, so in 2006 Geoff Hinton and collaborators they.",
                    "label": 0
                },
                {
                    "sent": "Discovered this trick, I, well, I don't know if they discovered all they thought really hard and they came up with this with this really good idea of using unsupervised pretraining with restricted Boltzmann machines to initialize these models.",
                    "label": 1
                },
                {
                    "sent": "These deep neural networks and to make them actually work.",
                    "label": 0
                },
                {
                    "sent": "And as we saw it works for all sorts of things like like usually 28 by 28 images.",
                    "label": 0
                },
                {
                    "sent": "But still it works for images, for digits, faces, stuff like that so.",
                    "label": 0
                },
                {
                    "sent": "LP stuff just pretty cool.",
                    "label": 0
                },
                {
                    "sent": "You know there's been a flurry of kind of papers on the subject, and a lot of them basically use the same kind of recipe.",
                    "label": 1
                },
                {
                    "sent": "You know, take something that's hierarchical, use unsupervised pre training with some circular machines or some other Baltimore machines or autoencoders, or even kernel.",
                    "label": 0
                },
                {
                    "sent": "PCA, as we've seen this this year, Snips, Loren Saul and Mitchell, if I pronounce it correctly.",
                    "label": 0
                },
                {
                    "sent": "And the use this as initialization for for this hierarchical model, and then do discriminative training.",
                    "label": 0
                },
                {
                    "sent": "And it works really well and it's widely applied, but it's actually not quite clear why it works really well so.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The plan that we had and that we still have is is to understand what's going on and to propose several hypothesis and to test them in a kind of rigorous fashion.",
                    "label": 0
                },
                {
                    "sent": "So yes, the first step is to propose hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The second step is, I guess, observe the effects of pre training and we're talking, you know, really large scale kind of experiments where we spend a lot of CPU power.",
                    "label": 0
                },
                {
                    "sent": "The third step is try to infer the role of pre training or what it actually does and how it actually does it and how it helps the performance and infer the level of agreement with our hypothesis.",
                    "label": 1
                },
                {
                    "sent": "It could turn out that neither of our hypothesis actually true or only combination of these hypothesis is actually true.",
                    "label": 0
                },
                {
                    "sent": "So what I'm talking about when I'm saying hypothesis is you can think about these.",
                    "label": 0
                },
                {
                    "sent": "You know, for instance I give 2 examples and these will be kind of the main running examples of this talk.",
                    "label": 1
                },
                {
                    "sent": "The first one is the regularization hypothesis and it says.",
                    "label": 0
                },
                {
                    "sent": "Basically that pretraining the unsupervised components of this of this training of deep neural networks is constraining the model to model P of X the input distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is the regularization aspect of.",
                    "label": 0
                },
                {
                    "sent": "It is constraining it to do P of X and it assumes that representing P of X with this model is actually going to be good for representing P of Y given X.",
                    "label": 0
                },
                {
                    "sent": "So another hypothesis that's you know this is a testable hypothesis, another one that's also testable, I guess, is optimization hypothesis.",
                    "label": 0
                },
                {
                    "sent": "That's how we call it.",
                    "label": 0
                },
                {
                    "sent": "And it says basically that unsupervised initialization using either of these PBM's auto encoders and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "Lands the parameters near some better local minimum of P of Y given X.",
                    "label": 0
                },
                {
                    "sent": "So this our objective function.",
                    "label": 0
                },
                {
                    "sent": "And was basically says that if using unsupervised initialization you're going to reach a better local minimum, lower local minimum, which is not achievable by typical random initialization that we use in neural networks.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm going to just jump straight to the kind of experiments one of our first experiments, and I think the the I mean very direct application or the very direct test of the hypothesis is we're just going to measure training error versus testing testing error overtime for a bunch of different networks.",
                    "label": 0
                },
                {
                    "sent": "We've selected the hyperparameters in some sort of way.",
                    "label": 0
                },
                {
                    "sent": "All have the same hyperparameter and the only thing that they differ.",
                    "label": 0
                },
                {
                    "sent": "I mean as in all supervised networks have the same parameters.",
                    "label": 0
                },
                {
                    "sent": "All networks with pre training have the same hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "The only thing that they differ in within the clusters are the initialization seed and we're just going to look at the training error versus testing error overtime and observe what's going on.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is that well, a couple of observations that are interesting.",
                    "label": 0
                },
                {
                    "sent": "One is that for the same training error.",
                    "label": 1
                },
                {
                    "sent": "The generalization error of the pre trained networks is lower.",
                    "label": 0
                },
                {
                    "sent": "So which is which is which is interesting, but.",
                    "label": 0
                },
                {
                    "sent": "More maybe relevant observation for regularization hypothesis for our hypothesis general is this gap that you're going to get at the end of training.",
                    "label": 1
                },
                {
                    "sent": "Even after even at convergence, the pre trained networks do not seem to achieve low training error.",
                    "label": 0
                },
                {
                    "sent": "At least it's going to be higher than what you're going to get with random initialization, so this kind of fits with the regularization interpretation of the what's going on in this world, since it basically acts like a regularizer, it it increases your training error, but it gives you generalization.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And another experiment that we did is we said, well, let's see what's going on.",
                    "label": 0
                },
                {
                    "sent": "If we're going to start fiddling with other constraints in the network, constraints that affect.",
                    "label": 0
                },
                {
                    "sent": "The capacity of the network.",
                    "label": 0
                },
                {
                    "sent": "So if you think of regularization is something's playing with capacity, there are other ways of playing with the capacity of a neural network, and one of them is.",
                    "label": 0
                },
                {
                    "sent": "Shrinking the size of the layers, so if you just plot the testing error versus the layer size and this black thing here corresponds to the testing error of the networks with pre training with these ones with pre training the two red and blue and the black one without you see that it's as if the layer sizes sufficiently big.",
                    "label": 0
                },
                {
                    "sent": "There's actually a gap you know the pre trained networks actually perform better, but as soon as you get to like 100 and below 700 units which is.",
                    "label": 0
                },
                {
                    "sent": "Evidently too small for a good performance.",
                    "label": 0
                },
                {
                    "sent": "We see this.",
                    "label": 0
                },
                {
                    "sent": "This is pretty big gap in between, so it looks as if pretraining acts as an additional kind of capacity constraint onto the network.",
                    "label": 0
                },
                {
                    "sent": "So the other.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I meant, which is this kind of the source of pretty pictures is we.",
                    "label": 0
                },
                {
                    "sent": "We just did this kind of projection.",
                    "label": 0
                },
                {
                    "sent": "It's a bit of a crazy projection, but as I said, it provides pretty pictures.",
                    "label": 0
                },
                {
                    "sent": "So we just said we were going to do this function space.",
                    "label": 1
                },
                {
                    "sent": "Approximation of projection of the network, so we're going to take all the outputs.",
                    "label": 0
                },
                {
                    "sent": "Projects.",
                    "label": 0
                },
                {
                    "sent": "I'm into 2 dimensions using two nonlinear dimensionality reduction techniques.",
                    "label": 0
                },
                {
                    "sent": "T, Sne, and Isomap, and we're going to look at what happens to the network.",
                    "label": 0
                },
                {
                    "sent": "So each point in here represents a network.",
                    "label": 0
                },
                {
                    "sent": "This is the cluster of networks with pre training.",
                    "label": 0
                },
                {
                    "sent": "This is a cluster of networks without pretraining.",
                    "label": 0
                },
                {
                    "sent": "Here with Isomap without.",
                    "label": 0
                },
                {
                    "sent": "With and they sort of.",
                    "label": 0
                },
                {
                    "sent": "The color indicates time, you know, as we go as we do an epic of training.",
                    "label": 0
                },
                {
                    "sent": "This start here go go, go and spread out so it's a couple of interesting observations from here.",
                    "label": 0
                },
                {
                    "sent": "One is that the.",
                    "label": 0
                },
                {
                    "sent": "That works with and without pretraining.",
                    "label": 0
                },
                {
                    "sent": "They seem to explore different kinds of space.",
                    "label": 1
                },
                {
                    "sent": "You know they don't kind of intersect each other, they go to each their own.",
                    "label": 0
                },
                {
                    "sent": "Each of the network kind of goes to the local minimum.",
                    "label": 0
                },
                {
                    "sent": "And I think one of the more interesting, more revealing observations from the Isomap plot is that from the point of view of networks, without pretraining, those with retraining look basically all the same, so you know they seem to occupy much smaller space compared to networks with free training.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It seems, or at least from the experiment so far, that pretraining places the networks in this region of parameter space that is very different from the one that given by random initialization.",
                    "label": 1
                },
                {
                    "sent": "I mean, it's not just from the kind of test errors, but from what we've seen in the in the in those pretty pictures, and it seems to be like kind of regularizer, but the important thing to realize here that it's a regularizer, but it only affects the starting point of the optimization so.",
                    "label": 1
                },
                {
                    "sent": "The question that we posed ourselves as well since it only affects the starting point of the optimization.",
                    "label": 0
                },
                {
                    "sent": "What if we had this source of very almost infinite data and we just initialized with pre training?",
                    "label": 0
                },
                {
                    "sent": "But then we continued on continued to supervise training.",
                    "label": 0
                },
                {
                    "sent": "Will it effect disappear?",
                    "label": 0
                },
                {
                    "sent": "So and the answer is actually not.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The effect does not disappear, and if you compare, this is with the.",
                    "label": 0
                },
                {
                    "sent": "I think it's called Infinite I missed so the data set by Leon Bottou where he just generates kind of elastic deformations of M list.",
                    "label": 0
                },
                {
                    "sent": "You can generate 2 to 32 of them.",
                    "label": 0
                },
                {
                    "sent": "So if you plot the online classification error versus the number of samples we have, like here 10,000,000 examples, the black curve, one of them represents a network without free training and the red curve represents a network with pre training.",
                    "label": 0
                },
                {
                    "sent": "You can see that they don't necessarily intersect.",
                    "label": 0
                },
                {
                    "sent": "And as I said, it's the same kind of scenario.",
                    "label": 0
                },
                {
                    "sent": "Network starts from.",
                    "label": 0
                },
                {
                    "sent": "From some solution that is given by unsupervised initialization an it goes somewhere, but it doesn't.",
                    "label": 0
                },
                {
                    "sent": "They don't seem to be able to be converging.",
                    "label": 0
                },
                {
                    "sent": "We're talking of course here about really small errors, so it's you know, since it's somewhat infinite data set, we didn't actually run until completion.",
                    "label": 0
                },
                {
                    "sent": "There's some sort of attention span that I have.",
                    "label": 0
                },
                {
                    "sent": "So it's clearly the starting point of this non convex optimization matters.",
                    "label": 0
                },
                {
                    "sent": "Even in a scenario with essentially unbounded trading it and it's kind of surprising result because it doesn't follow the standard interpretation of a regularizer.",
                    "label": 1
                },
                {
                    "sent": "You would think of if you think of a standard regularizer, its effect should somewhat.",
                    "label": 0
                },
                {
                    "sent": "Go away if you have alot alot.",
                    "label": 0
                },
                {
                    "sent": "Alot of training data labeled training data.",
                    "label": 0
                },
                {
                    "sent": "The other interesting.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiment that we did is we said well if we can maybe quantify in which sense pre training affects the outcome of the of the supervised optimization.",
                    "label": 0
                },
                {
                    "sent": "So what we mean by this is we're going to literally vary the examples during training.",
                    "label": 0
                },
                {
                    "sent": "So as I said, we have 10,000,000 examples.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to be very.",
                    "label": 0
                },
                {
                    "sent": "We're going to do the normal training to 2.5 million examples of unsupervised training followed by 7 and 1/2 million examples of supervised training.",
                    "label": 0
                },
                {
                    "sent": "And now at each like, we're going to vary only the 1st million examples and keep the rest fixed.",
                    "label": 1
                },
                {
                    "sent": "So we're going to do some sort of shuffling of them.",
                    "label": 1
                },
                {
                    "sent": "Re sample, keep the rest of them fixed, and then measure the variance of the function that we learn at the end of training.",
                    "label": 1
                },
                {
                    "sent": "So it's going to be some function on the variance of this function on a test set and plot this.",
                    "label": 0
                },
                {
                    "sent": "You know these variants as a function of where we varied, at which point in time did we varied examples so it's interesting here is that.",
                    "label": 0
                },
                {
                    "sent": "The variance is very high at the beginning, so both so the blue curve is a network with without pretraining and the red curve is a network with pre training.",
                    "label": 0
                },
                {
                    "sent": "In both cases it's very susceptible, at least it seems so to the examples at the beginning of training.",
                    "label": 0
                },
                {
                    "sent": "But the I'd say.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The more surprising result is.",
                    "label": 0
                },
                {
                    "sent": "This is where we start supervised training for the network with pre training.",
                    "label": 1
                },
                {
                    "sent": "So this is where we before.",
                    "label": 0
                },
                {
                    "sent": "Before here we did unsupervised learning after supervised learning.",
                    "label": 1
                },
                {
                    "sent": "And if we compare this point with this point 4, so in both cases here we start supervised training.",
                    "label": 1
                },
                {
                    "sent": "And the variance is much much lower, so pretraining seems to act as a sort of a.",
                    "label": 0
                },
                {
                    "sent": "Survey variance reduction technique.",
                    "label": 0
                },
                {
                    "sent": "So, as I said, there's two kind of two lessons to be learned here.",
                    "label": 1
                },
                {
                    "sent": "There's early examples are change.",
                    "label": 0
                },
                {
                    "sent": "A lot of things, and it's pre training access various reduction, which is again some sort of indication of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of a of a regularizer, so to kind of wrap up a bit.",
                    "label": 0
                },
                {
                    "sent": "Well, it seems that what's going on is then we have this kind of.",
                    "label": 0
                },
                {
                    "sent": "Interpretation of more more more mechanistic explanation of what's going on is as the way it's become larger.",
                    "label": 0
                },
                {
                    "sent": "During training, they become trapped in what we like to call basins of attraction and the initial updates.",
                    "label": 1
                },
                {
                    "sent": "You know, the where you start from some sort of small weights, and then whatever the initial bids gets you in whatever quadrant they get you.",
                    "label": 0
                },
                {
                    "sent": "This is where you're going to end up after after critical period, and these initial updates.",
                    "label": 0
                },
                {
                    "sent": "This is the explain much more of the variance of the.",
                    "label": 1
                },
                {
                    "sent": "Of the function that you're going to learn as opposed to updates at the end of training, so that's why it seems that pretraining seems to help.",
                    "label": 0
                },
                {
                    "sent": "And it you know, it's it gets you to a basin of attraction with good generalization properties.",
                    "label": 1
                },
                {
                    "sent": "It doesn't just get you to some random basis.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Attraction, so that's good.",
                    "label": 0
                },
                {
                    "sent": "So we had some early results that pointed towards the regularization hypothesis and our results with this online training.",
                    "label": 1
                },
                {
                    "sent": "Give some sort of a more nuanced interpretation of what's going on.",
                    "label": 1
                },
                {
                    "sent": "We explored this online setting and we saw that the pre training effect does not vanish.",
                    "label": 1
                },
                {
                    "sent": "And we saw also that pretraining seems to be a variance kinda variance reduction technique.",
                    "label": 0
                },
                {
                    "sent": "There's a couple of caveats to pre training as.",
                    "label": 1
                },
                {
                    "sent": "It will have a positive effect and with as with basically almost any kind of semi supervised learning technique as long only as long as P of X is used, modeling P of X or representing P of X is useful for modeling P of Y given X Ann.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to note that this this influence of the high variance that we get from early examples could be troublesome because if we are truly in a very large scale online learning scenario.",
                    "label": 0
                },
                {
                    "sent": "It might be worth it for algorithms to try to forget what happened in the beginning, you know and try to change to whatever is happening now, so it might be interesting to see how to escape from the basins of attraction that we are in.",
                    "label": 0
                },
                {
                    "sent": "And the future work is to understand other semi supervised approaches.",
                    "label": 1
                },
                {
                    "sent": "We've explored a couple but it would be nice to know what happens if you combine the two cost at the same time, supervised and unsupervised.",
                    "label": 0
                },
                {
                    "sent": "And we have many more results in our upcoming accepted jail or paper.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have you experimented with how the size of your pre training set affects how much?",
                    "label": 0
                },
                {
                    "sent": "How much appreciated or so I think so I've tried.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here, so like in this kind of scenario, so it's sort of like a have.",
                    "label": 0
                },
                {
                    "sent": "A budget of 10,000,000 examples.",
                    "label": 0
                },
                {
                    "sent": "So if I allow myself 10,000,000 updates, 10,000,000 updates and then I sort of.",
                    "label": 0
                },
                {
                    "sent": "Just do 1 million unsupervised and 9 million supervised or 2 million.",
                    "label": 0
                },
                {
                    "sent": "It all boils down to the same thing, so it's very.",
                    "label": 0
                },
                {
                    "sent": "In this case I think the lowest I've tried this 500,000 and it was basically the same kind of curve.",
                    "label": 0
                },
                {
                    "sent": "So it seems that you don't need a lot sort of.",
                    "label": 0
                },
                {
                    "sent": "I just chose because this is clear presentation, but it's almost any point in here I could have started with a gun to the same kind.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes yes.",
                    "label": 0
                },
                {
                    "sent": "Well very small relative to the total number of samples that you have here.",
                    "label": 1
                },
                {
                    "sent": "So the interesting thing is if you use very many, if you use seven 7 million or something, it's going to converge much faster to the to whatever minimum.",
                    "label": 0
                },
                {
                    "sent": "So once you start supervised training, if you use.",
                    "label": 0
                },
                {
                    "sent": "If you did alot alot alot of unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "And then flip on the switch to the supervised training you're going to converge much faster, so at least in this particular scenario.",
                    "label": 0
                },
                {
                    "sent": "From the publicity for all of like that, no more miles an hour.",
                    "label": 0
                },
                {
                    "sent": "So saying you are maybe manifold which can be strictly popular.",
                    "label": 0
                },
                {
                    "sent": "It seems to vary.",
                    "label": 0
                },
                {
                    "sent": "Symmetry compression, provided you compress it later you are solving at the same coffee, so maybe there's some connection we can do without connected, applied for sensing.",
                    "label": 0
                },
                {
                    "sent": "Here to say hi Superman.",
                    "label": 0
                },
                {
                    "sent": "Perhaps you're right, I don't have enough knowledge about compressed sensing to to make a qualified state windows.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it sounds sounds logically reasonable, though from what I know.",
                    "label": 0
                },
                {
                    "sent": "One more yeah.",
                    "label": 0
                },
                {
                    "sent": "So I tried this until 30,000,000 examples.",
                    "label": 0
                },
                {
                    "sent": "We have no generate generate this.",
                    "label": 0
                },
                {
                    "sent": "Synthetically it up there and what if your synthetic data generator isn't actually that good but busy?",
                    "label": 0
                },
                {
                    "sent": "Just repeat back the original samples.",
                    "label": 0
                },
                {
                    "sent": "Because then you would.",
                    "label": 0
                },
                {
                    "sent": "This would be expected, so it could be an estimation for, well, I think the way they own both constructed it.",
                    "label": 0
                },
                {
                    "sent": "I don't think it actually repeats the same example.",
                    "label": 0
                },
                {
                    "sent": "From the given samples and sort of distortion, yeah, so basically it is the same samples just slightly distorted, so it's not really.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah no.",
                    "label": 0
                },
                {
                    "sent": "I mean it's it's yeah, it's of course it's yeah yeah it's limited by in an infinite list of this procedure.",
                    "label": 0
                },
                {
                    "sent": "I wanted it, but it's hard to make you have.",
                    "label": 0
                },
                {
                    "sent": "There isn't anything.",
                    "label": 0
                },
                {
                    "sent": "There is one.",
                    "label": 0
                },
                {
                    "sent": "It was nothing.",
                    "label": 0
                },
                {
                    "sent": "Actually, I'm not really here by morning.",
                    "label": 0
                },
                {
                    "sent": "Somehow?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "That's what we're doing.",
                    "label": 0
                },
                {
                    "sent": "So category in Central Park is loading, right?",
                    "label": 0
                },
                {
                    "sent": "So in principle you would think that you have alot alot alot of things perhaps capable.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "If anything is that reconfirmation.",
                    "label": 0
                },
                {
                    "sent": "It's not just any random order.",
                    "label": 0
                },
                {
                    "sent": "Getting the local meeting.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "An upgrade on my priest like only spectacles featuring slightly output prices, yeah?",
                    "label": 0
                },
                {
                    "sent": "On my Dick.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Helps for the absolute training.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's it's it's.",
                    "label": 0
                },
                {
                    "sent": "I mean it helps in the sense.",
                    "label": 0
                },
                {
                    "sent": "I mean, in the same sense that it's your wherever you are.",
                    "label": 0
                },
                {
                    "sent": "You know whatever pre training gets you.",
                    "label": 0
                },
                {
                    "sent": "You're not like like like with the local minimum.",
                    "label": 0
                },
                {
                    "sent": "You're not going to get away easily out of there.",
                    "label": 0
                },
                {
                    "sent": "I guess this experiment was done exactly to show that.",
                    "label": 0
                },
                {
                    "sent": "Variance reduction equals good performance.",
                    "label": 0
                },
                {
                    "sent": "It's more variance.",
                    "label": 0
                },
                {
                    "sent": "Reduction is a characteristic of regularization and pre training seems to have this characteristic.",
                    "label": 0
                },
                {
                    "sent": "It's debatable where weather reducing variance actually, generally speaking improves performance, let's say.",
                    "label": 0
                },
                {
                    "sent": "Nice to meet you again.",
                    "label": 0
                },
                {
                    "sent": "Coffee break.",
                    "label": 0
                }
            ]
        }
    }
}