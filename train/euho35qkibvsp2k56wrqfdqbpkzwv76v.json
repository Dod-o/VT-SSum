{
    "id": "euho35qkibvsp2k56wrqfdqbpkzwv76v",
    "title": "Global RDF Vector Space Embeddings",
    "info": {
        "author": [
            "Michael Cochez, Fraunhofer FIT"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_cochez_space_embeddings/",
    "segmentation": [
        [
            "So."
        ],
        [
            "To work together with these people, to this Paris to ski season about onset and heparin from University of Mannheim and."
        ],
        [
            "Let's start with the build background.",
            "So this is something which I kind of after the keynote.",
            "Don't have to show anymore.",
            "I think in this study which most people would would know anyway, we do have a lot of knowledge graphs.",
            "OK, so I kind of group them like like completely open knowledge graph with a lot of different topics inside.",
            "There are more like let's say topic specific knowledge graphs around here.",
            "Then there are close knowledge graph which companies are using and then there is the Thomson Reuters Knowledge Graph which has been presenting this conference as well.",
            "As a kind of mix between open and closed knowledgeable.",
            "So the question which we were trying to look at now is what happens if we can take this knowledge graphs and use it for machine learning.",
            "So it's very much related to previous presentations as well."
        ],
        [
            "Kind of task which we can solve.",
            "We can look at things like using for classification, recommender systems, document modeling, alignment of knowledge base, is completing knowledge base is finding mistakes in knowledge base is doing all kinds of other things as well.",
            "Now we have we have a lot a lot of data mining algo."
        ],
        [
            "OK, So what we would like to do is we would like to have this algorithm which we know quite well.",
            "We want to take a knowledge graph and we want to combine these things."
        ],
        [
            "This this is not an easy task.",
            "OK, it's it's at."
        ],
        [
            "Really rather heart and the main."
        ],
        [
            "The main challenge is kind of a model mismatch, so we do it on the one side.",
            "We do have graphs OK on the other side we have some machine learning model and I just take a very small neural network here and the problem is now that graph or network for example has a certain dimension as input, certain dimensions output, which is a fixed dimension.",
            "So you have to take that graph and somehow stuff that into that dimension which is physically just not possible because the graph has a skin dimension which is as large as the number of nodes and you cannot really.",
            "Just put that into your model.",
            "This is not unique to networks, all kind of different models they use essentially vectors as image.",
            "So what are we going to do with?"
        ],
        [
            "To take this craft model.",
            "Convert the graph model to something like vectors and then feed that one to a model so.",
            "More."
        ],
        [
            "Great, we're going to do is we're going to create one vector for each entity in the graph, so we have this graph for each of the nodes in this graph we are going to create vector and that factor we're going to or these factors of 1 for each node we're going to fit into the machine learning or data mining model.",
            "So what we have done this first is compatibility, but we also want other things.",
            "For example to preserve information so all the information which is contained in the graph, we want to somehow if as well in this embedded model.",
            "Then we want this thing to be unsupervised, so we don't really know how this embeddings should be.",
            "We don't have examples of embeddings, so the only way we can essentially do this is unsupervised modes.",
            "The next thing is we want to have efficient computation so.",
            "There are some machine learning models which work directly on graphs, and there are other algorithms for graphs and there's one problem that is scalability.",
            "So graph algorithms are known to be be hard.",
            "Hardness of conventional complexity.",
            "And then so we want to have something which scales to reasonably large graphs.",
            "Finally want to have low dimensional representation, so these factors they should not be too large either.",
            "If they are extremely large and we have a large graph then we have representation which blows up as well.",
            "So we want to have it somehow small, whatever that means.",
            "So."
        ],
        [
            "So how so?",
            "There are several approach which were all represented before, so this is, let's say one more in this series.",
            "So we have in our algorithm with three steps.",
            "I need the first step.",
            "We're going to take the graph and assign certain weights.",
            "The second step we're going to create a Co occurrence matrix, so we're going to create a matrix which I will go into more detail about soon and then this matrix is going to be used for training a certain machine learning model.",
            "The machine learning model will give us give us vectors and these factors can then be used."
        ],
        [
            "So weighing a graph, so I took this this very small graph.",
            "So just as an example, so we have we have some countries here we have some cities and then we have languages which are one of the languages which is spoken in these countries.",
            "Um?",
            "Now what we see in this graph is or if you think about the graph, there are some relations which are more important than other ones.",
            "And for this example, I took it such that yeah, so that the language property is much more important as the type property you can think about it this way that there are only few countries which are speaking German and there are many.",
            "There are many countries, right?",
            "So the country property is happening much more often as this other property so.",
            "There are different ways of doing this way.",
            "We have actually 12 different strategies for doing away weighing on the graph in the paper, but one of them is exactly like this that the more frequent property happens, the less important you make it.",
            "And this this works this way works actually rather well.",
            "OK."
        ],
        [
            "Yes, so the next step is to go from this kind of structure to a Co occurrence matrix.",
            "Now Co occurrence matrix here has has kind of like on the top we see all the entities in the graph.",
            "Here on the site we sell them.",
            "At this end all the properties in the graph.",
            "OK, so if the predicates and we have entities, so it's not a not a symmetric matrix.",
            "How we're going to fill that?",
            "Well, we start from this graph.",
            "And what we what we started with?"
        ],
        [
            "Basically, a personalized page rank, so we have we have a note which we're looking at the moon, so this is going to be Jenna.",
            "What we do is basically we see how important are other notes for this note, so you can think about the person's patron case.",
            "You can put a.",
            "One way to think about this, like you put the bucket of paint inside one of the notes, like they're not where you're starting from Vienna.",
            "Some of this pain is going to stick on that note.",
            "It's going to remain there and some of that is going to flow out to the other, like through the outages to other nodes in the graph.",
            "So what would happen?",
            "Concrete is that.",
            "You put the paint here in Vienna.",
            "It flows out through this link to Austria.",
            "And then it's going to happen recursively.",
            "So also when paint arrives in Austria, it's going to flow out through the different links out.",
            "So you have language type in capital and it's going to flow to all these notes as well.",
            "This flow is going to happen depending on the importance of the links, or more importantly, it's going to look more pain through and less important, less pain through.",
            "So what we get in the end is.",
            "For Vienna, we're going to get numbers, so how much paint has arrived or dries up in a certain note?",
            "Um?",
            "And well, this is basically what the personalized page rank is giving us now.",
            "We did a couple more things, so we added to this.",
            "Basically the same procedure but with reversed edges.",
            "So we vote on edges in the graph, so you just turn them around and we see what we get out of the page rank again or personal page rank.",
            "And then we add these numbers together.",
            "We do a couple of normalization."
        ],
        [
            "Steps meaning we are removing the entry for the note itself."
        ],
        [
            "And I'm normalizing such that some of these factors is 1.",
            "The reason for this normalization step in this reversal of edges is meant that we noticed an experiment that this works better.",
            "OK, so some of them make sense.",
            "Some of them are a bit.",
            "We don't really know why they buy the improved results.",
            "OK, so.",
            "What we what we kind of end up with this is this large graph now.",
            "This computation, which are saying like like doing the person's Pagerank doing the inverse, something together it's.",
            "This is not going to work really, so the problem is if you take a normal person's patron algorithm, right?",
            "If you take the real thing, you have to compute this for each possible note in the graph.",
            "So remember that here on the top we have our entities.",
            "If you're going to do this person, especially for all these entities one by one, this is just not going to workout.",
            "So what we did we did."
        ],
        [
            "Optimizations on this computing personalization for all the possible nodes.",
            "So we start from one existing approximation algorithm, so it's a bookmark coloring algorithm, and then we improve that further to be able to handle, let's say, and I'll note personalized page rank.",
            "So we have this.",
            "The first contribution that paper, really, it's an approximation for all pairs.",
            "Personalized page rank.",
            "So thanks to this this algorithm, we end up with."
        ],
        [
            "Very large matrix with all entities on the top.",
            "I'll end.",
            "It is an all predicates on the underside.",
            "And this this metrics are going to be rather sparse thanks to the approximation.",
            "What we're doing with this matrix is we're feeding it basically to the existing law framework, which is which I will go into a bit sooner and we get embeddings out, so we get a vector for each entity in the graph.",
            "So glad he's going to optimize this formula and I'm not going to go into that, But anyway, so it's optimizing this formula, and anyway, I have a couple of things which it will try to do with this formula.",
            "It starts from the assumption that similar things will have similar vector in this in this matrix, right?",
            "So if you have something like in our graph, that little bit back here, something like Vienna and something like Berlin.",
            "If you look in the graph they have a bit of similar structure right?",
            "In this one, very similar that I just made a small difference here, but they're going to have very similar structure and as a result they're going to get very similar.",
            "Let's say very similar column in this core occurrence matrix as well."
        ],
        [
            "So.",
            "That assumption being made, then it will optimize for this kind of analogies which we signed out in previous presentations.",
            "Well, very shortly mentioned and one of the features of gloves that's going to capture complete context so it will do much quite quite large environment of the note into account when doing the embedding."
        ],
        [
            "So now we have this embedding.",
            "Now we have to do some kind of evaluation to see where this thing makes any sense or works at all.",
            "So instead we work on very large graph, so this is really embedded base to hold the pedia with this.",
            "So this was mentioned previously.",
            "It was one of the problems.",
            "So our approach scales reasonably well.",
            "So about 5 million instances 1004 under mappings or 400 properties.",
            "We use different weighting strategies and we compared to a.",
            "Kind of baselines so we have or business or other methods.",
            "Also, the ones which were mentioned in the previous talk are here."
        ],
        [
            "OK, so the first thing which we looked at is embedding time.",
            "So our approach takes something between 6 and 48 hours, about two days to embed the whole whole thing and it is not such that the longer it takes, the better it gets.",
            "OK, so this depends on the weighing method like how the wings done.",
            "You get different kinds of matrices and then it takes longer so other methods on the left of it will take about a day to compute on the similar machine.",
            "Distrans systems takes about a week and then there are many methods.",
            "We just don't scale and one of the reasons is that you need so much memory to just store, like if you.",
            "If you take all these notes, are these entities and you make a dense matrix out of it.",
            "You need like 10s of terabytes of memory just to store that matrix temporarily."
        ],
        [
            "OK, so we evaluated this thing on different machine learning tasks so we have we have different classification, regression and on document modeling tasks.",
            "And.",
            "Yeah, so I'm skipping that."
        ],
        [
            "Because of this, but.",
            "So what we find is the result is that our approach is kind of.",
            "On par or a bit worse, artist avec most of the times.",
            "So one of the reasons that are on the reason we still suspect is that we don't know how to put certain parameters.",
            "We have 12 weighing strategies.",
            "We have several other parameters and we did not do an exhaustive evaluation.",
            "All possible para meters here.",
            "So this is one thing which we still should do, but still.",
            "So we outperformed all the other baselines except for the artifact approach.",
            "Um?",
            "OK then yeah, similar and then OK. We have some tasks.",
            "Some specific task on which we will outperform artifact as well.",
            "So in the."
        ],
        [
            "Conclusion already so so we have some kind of approach for generating embeddings based on the graph.",
            "We explore exploit this kind of global pattern so we can go quite deep into the graph like this.",
            "Like we don't take just a couple of hops, it can really go up to like 20 hops deep in the graph when I'm doing embedding.",
            "It preserves some of the graph information.",
            "At least we are compatible with certain machine learning algorithms all at once, which basically take vectors as input.",
            "We compute very efficient embeddings compared to many of the other approaches, and then we have also noticed that we've kind of no one size fits all embedding problem, so the different weighting strategies are going to give different performance in different tasks.",
            "And if there is no kind of 1 winner, so there's not like this waiting strategies, I was going to work for classification so there is no kind of consistent good embedding.",
            "So, but we still have that up to now, so we did this task and it's at independent approach.",
            "And as I have in the next slide here, there's probably not."
        ],
        [
            "We should be doing in the future, so.",
            "Um?",
            "How to get forward from this so we have something which works.",
            "It gives some results there reasonable, but how to get forward is the last thing to say.",
            "So first one we need better evaluation metrics as in the previous talk was set like OK, we use these metrics.",
            "We use this metric so everyone is using a bit of different metrics.",
            "There are some standard ones, but they are very very small.",
            "It's a very small graphs which are being embedded.",
            "Very small test sets as well.",
            "Um?",
            "Now you can ask all kind of different questions about the embeddings, which you might want to answer.",
            "We also need the large scale evaluation, so having like a lot of lot of different, there's lots of different datasets.",
            "Another thing is we don't include literal settlement.",
            "It is possible.",
            "Same with the previous talks or basically was done in the previous talk can be contained inside this Co occurrence matrix as well, so we can do this kind of combined embeddings and then we're going to have embedding of all sorts of graph.",
            "So for example, we were discussing earlier or I was discussing earlier about, for example, embedding Markov chains.",
            "We can also have let's all kind of different graphs which can be embedded to see what happens with that.",
            "So with this match that I'm open to any."
        ],
        [
            "Questions or comments?",
            "Thank you very much Michael.",
            "Hey, thanks for your presentation.",
            "I was wondering if you look into the effect of imbalance graph in terms of knowledge on this.",
            "Like for example if for Vienna we had the name of the mayor and also historical places would it be still the case that it's going to be similar to billing?",
            "Or would it kind of have a negative effect on that?",
            "So we we did not like specifically look at it but we know that the glove embedding model is going to cancel out some of these things, like if you have large similarity.",
            "It will still be similar, of course.",
            "Any discrepancy between entity is going to push them further apart.",
            "But we did not.",
            "We did not like analyze exactly how much that would be, but it it it will happen.",
            "But you're saying yes, did you try to reduce the initial graph in order to actually make this more scalable, you could do some sampling or something like that.",
            "OK, always be directly start from the original graph.",
            "But one thing we do is we.",
            "So we removed the literal values which have a lot of information then that's why we will never in the future.",
            "So that is a sort of thing what you're saying, so we reduce the size of the graph by this quite a lot, right?",
            "Or maybe you care about a specific properties where specific things, and then you pick everything.",
            "Right, sorry.",
            "What is the sort of machine that you perform your tests on?",
            "So this is a large machine, so it's a. I think I have to recall it's more exactly what we used about.",
            "This has about 200 gigabytes of RAM, so that's one thing which which is maybe more of a bottleneck.",
            "Then the so the coherence computations just using single threats, but for the.",
            "Embedding cell I think it's using.",
            "Recall correctly 32 cores or something like that, but that is the same for the other methods as well, so we use the same ship setup.",
            "Cool, thanks.",
            "OK, I'm going to inject one of my own questions.",
            "Why?",
            "Why are you assuming RDF is directional?",
            "I found that rather curious why do we assume?",
            "Well, your personalized page rank only follows links forward and not backwards.",
            "Have you have you done any sensitivity analysis on this?",
            "Why does this make sense?",
            "No.",
            "So this is a very good question.",
            "Is something we should try.",
            "So we did the reverse linking as well.",
            "So we added them together.",
            "But still, you could just ignore the directionality of the links and do do a thing like that this possible.",
            "We didn't try it.",
            "Well, that's that's the only answer I can give to the next question.",
            "So you said some of the approaches don't scale to the size of DB pedia.",
            "Which one are those?",
            "Because I'm missing some of the latest related work there, I would have to check which one did which one didn't so.",
            "Did you check this holistic embeddings, or nor?",
            "But that I would have to check?",
            "Yeah, so we have.",
            "We have a list which we did in those scale because we did it, but it's not scaling me.",
            "Usually they didn't finish within two weeks or went out of memory in this 250 gigabyte machine so I can get back to you.",
            "OK, two more lightning questions.",
            "Thanks for your talk.",
            "So my question is, would it be possible to identify what is the exact similarities and differences for a given entities in your graph?",
            "As in your example, when in Berlin are similar, right?",
            "So of course you can see that they're working.",
            "Balance.",
            "Their vectors are similar.",
            "This is fine, but you know in some areas in particular, in biology it's it's really crucial to understand what is the difference.",
            "Give another knows.",
            "For example, in this particular example I would be really would be really fun to see that these are two capitals of German speaking countries.",
            "So to extract this kind of information within this possible to extract.",
            "Would it be possible that we hope so?",
            "So we're currently working on some medical cases exactly like like for example, like let's say disease drug interaction we are doing.",
            "And this is also a bit of a problem for link prediction, right?",
            "So what you're trying to do your trying to see where you have missing links and try to complete the graph and then you also see what the differences are between entities, so it's a bit of a hope.",
            "So the problem is really that the how to say that there is a step in between this embedding with the model which are currently using.",
            "It's somewhat easier to reverse it with you earlier models, they are very.",
            "Obfuscate, have no idea what this happening.",
            "Here we have more of a chance to see why certain things are happening.",
            "We're not there yet, so but the hope is at least there with previous one.",
            "There was no hope to do an obvious question about the direction, so if you go with an undirected graph, so you end up with the frequency distribution of the overall graph.",
            "So which doesn't makes really sense.",
            "So you need a direction, otherwise it doesn't work.",
            "So that is good thing about it.",
            "I agree that's right, that's right to his question.",
            "So I agree.",
            "But we notice that if we reverse the edges our quality improves.",
            "So we can.",
            "We can talk about it offline.",
            "So yeah, I was lying around so it's one of the things we all agree we need to talk.",
            "Thank you very much, Michael."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To work together with these people, to this Paris to ski season about onset and heparin from University of Mannheim and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's start with the build background.",
                    "label": 0
                },
                {
                    "sent": "So this is something which I kind of after the keynote.",
                    "label": 0
                },
                {
                    "sent": "Don't have to show anymore.",
                    "label": 0
                },
                {
                    "sent": "I think in this study which most people would would know anyway, we do have a lot of knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "OK, so I kind of group them like like completely open knowledge graph with a lot of different topics inside.",
                    "label": 0
                },
                {
                    "sent": "There are more like let's say topic specific knowledge graphs around here.",
                    "label": 1
                },
                {
                    "sent": "Then there are close knowledge graph which companies are using and then there is the Thomson Reuters Knowledge Graph which has been presenting this conference as well.",
                    "label": 0
                },
                {
                    "sent": "As a kind of mix between open and closed knowledgeable.",
                    "label": 0
                },
                {
                    "sent": "So the question which we were trying to look at now is what happens if we can take this knowledge graphs and use it for machine learning.",
                    "label": 0
                },
                {
                    "sent": "So it's very much related to previous presentations as well.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kind of task which we can solve.",
                    "label": 0
                },
                {
                    "sent": "We can look at things like using for classification, recommender systems, document modeling, alignment of knowledge base, is completing knowledge base is finding mistakes in knowledge base is doing all kinds of other things as well.",
                    "label": 1
                },
                {
                    "sent": "Now we have we have a lot a lot of data mining algo.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what we would like to do is we would like to have this algorithm which we know quite well.",
                    "label": 0
                },
                {
                    "sent": "We want to take a knowledge graph and we want to combine these things.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This this is not an easy task.",
                    "label": 0
                },
                {
                    "sent": "OK, it's it's at.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really rather heart and the main.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main challenge is kind of a model mismatch, so we do it on the one side.",
                    "label": 0
                },
                {
                    "sent": "We do have graphs OK on the other side we have some machine learning model and I just take a very small neural network here and the problem is now that graph or network for example has a certain dimension as input, certain dimensions output, which is a fixed dimension.",
                    "label": 0
                },
                {
                    "sent": "So you have to take that graph and somehow stuff that into that dimension which is physically just not possible because the graph has a skin dimension which is as large as the number of nodes and you cannot really.",
                    "label": 0
                },
                {
                    "sent": "Just put that into your model.",
                    "label": 0
                },
                {
                    "sent": "This is not unique to networks, all kind of different models they use essentially vectors as image.",
                    "label": 0
                },
                {
                    "sent": "So what are we going to do with?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To take this craft model.",
                    "label": 0
                },
                {
                    "sent": "Convert the graph model to something like vectors and then feed that one to a model so.",
                    "label": 0
                },
                {
                    "sent": "More.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great, we're going to do is we're going to create one vector for each entity in the graph, so we have this graph for each of the nodes in this graph we are going to create vector and that factor we're going to or these factors of 1 for each node we're going to fit into the machine learning or data mining model.",
                    "label": 1
                },
                {
                    "sent": "So what we have done this first is compatibility, but we also want other things.",
                    "label": 0
                },
                {
                    "sent": "For example to preserve information so all the information which is contained in the graph, we want to somehow if as well in this embedded model.",
                    "label": 0
                },
                {
                    "sent": "Then we want this thing to be unsupervised, so we don't really know how this embeddings should be.",
                    "label": 0
                },
                {
                    "sent": "We don't have examples of embeddings, so the only way we can essentially do this is unsupervised modes.",
                    "label": 1
                },
                {
                    "sent": "The next thing is we want to have efficient computation so.",
                    "label": 0
                },
                {
                    "sent": "There are some machine learning models which work directly on graphs, and there are other algorithms for graphs and there's one problem that is scalability.",
                    "label": 0
                },
                {
                    "sent": "So graph algorithms are known to be be hard.",
                    "label": 0
                },
                {
                    "sent": "Hardness of conventional complexity.",
                    "label": 0
                },
                {
                    "sent": "And then so we want to have something which scales to reasonably large graphs.",
                    "label": 1
                },
                {
                    "sent": "Finally want to have low dimensional representation, so these factors they should not be too large either.",
                    "label": 0
                },
                {
                    "sent": "If they are extremely large and we have a large graph then we have representation which blows up as well.",
                    "label": 0
                },
                {
                    "sent": "So we want to have it somehow small, whatever that means.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how so?",
                    "label": 0
                },
                {
                    "sent": "There are several approach which were all represented before, so this is, let's say one more in this series.",
                    "label": 0
                },
                {
                    "sent": "So we have in our algorithm with three steps.",
                    "label": 0
                },
                {
                    "sent": "I need the first step.",
                    "label": 0
                },
                {
                    "sent": "We're going to take the graph and assign certain weights.",
                    "label": 0
                },
                {
                    "sent": "The second step we're going to create a Co occurrence matrix, so we're going to create a matrix which I will go into more detail about soon and then this matrix is going to be used for training a certain machine learning model.",
                    "label": 0
                },
                {
                    "sent": "The machine learning model will give us give us vectors and these factors can then be used.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So weighing a graph, so I took this this very small graph.",
                    "label": 0
                },
                {
                    "sent": "So just as an example, so we have we have some countries here we have some cities and then we have languages which are one of the languages which is spoken in these countries.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now what we see in this graph is or if you think about the graph, there are some relations which are more important than other ones.",
                    "label": 0
                },
                {
                    "sent": "And for this example, I took it such that yeah, so that the language property is much more important as the type property you can think about it this way that there are only few countries which are speaking German and there are many.",
                    "label": 0
                },
                {
                    "sent": "There are many countries, right?",
                    "label": 0
                },
                {
                    "sent": "So the country property is happening much more often as this other property so.",
                    "label": 0
                },
                {
                    "sent": "There are different ways of doing this way.",
                    "label": 0
                },
                {
                    "sent": "We have actually 12 different strategies for doing away weighing on the graph in the paper, but one of them is exactly like this that the more frequent property happens, the less important you make it.",
                    "label": 0
                },
                {
                    "sent": "And this this works this way works actually rather well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, so the next step is to go from this kind of structure to a Co occurrence matrix.",
                    "label": 0
                },
                {
                    "sent": "Now Co occurrence matrix here has has kind of like on the top we see all the entities in the graph.",
                    "label": 0
                },
                {
                    "sent": "Here on the site we sell them.",
                    "label": 0
                },
                {
                    "sent": "At this end all the properties in the graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so if the predicates and we have entities, so it's not a not a symmetric matrix.",
                    "label": 0
                },
                {
                    "sent": "How we're going to fill that?",
                    "label": 0
                },
                {
                    "sent": "Well, we start from this graph.",
                    "label": 0
                },
                {
                    "sent": "And what we what we started with?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, a personalized page rank, so we have we have a note which we're looking at the moon, so this is going to be Jenna.",
                    "label": 0
                },
                {
                    "sent": "What we do is basically we see how important are other notes for this note, so you can think about the person's patron case.",
                    "label": 0
                },
                {
                    "sent": "You can put a.",
                    "label": 0
                },
                {
                    "sent": "One way to think about this, like you put the bucket of paint inside one of the notes, like they're not where you're starting from Vienna.",
                    "label": 0
                },
                {
                    "sent": "Some of this pain is going to stick on that note.",
                    "label": 0
                },
                {
                    "sent": "It's going to remain there and some of that is going to flow out to the other, like through the outages to other nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "So what would happen?",
                    "label": 0
                },
                {
                    "sent": "Concrete is that.",
                    "label": 0
                },
                {
                    "sent": "You put the paint here in Vienna.",
                    "label": 0
                },
                {
                    "sent": "It flows out through this link to Austria.",
                    "label": 0
                },
                {
                    "sent": "And then it's going to happen recursively.",
                    "label": 0
                },
                {
                    "sent": "So also when paint arrives in Austria, it's going to flow out through the different links out.",
                    "label": 0
                },
                {
                    "sent": "So you have language type in capital and it's going to flow to all these notes as well.",
                    "label": 0
                },
                {
                    "sent": "This flow is going to happen depending on the importance of the links, or more importantly, it's going to look more pain through and less important, less pain through.",
                    "label": 0
                },
                {
                    "sent": "So what we get in the end is.",
                    "label": 0
                },
                {
                    "sent": "For Vienna, we're going to get numbers, so how much paint has arrived or dries up in a certain note?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And well, this is basically what the personalized page rank is giving us now.",
                    "label": 0
                },
                {
                    "sent": "We did a couple more things, so we added to this.",
                    "label": 0
                },
                {
                    "sent": "Basically the same procedure but with reversed edges.",
                    "label": 0
                },
                {
                    "sent": "So we vote on edges in the graph, so you just turn them around and we see what we get out of the page rank again or personal page rank.",
                    "label": 0
                },
                {
                    "sent": "And then we add these numbers together.",
                    "label": 0
                },
                {
                    "sent": "We do a couple of normalization.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Steps meaning we are removing the entry for the note itself.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm normalizing such that some of these factors is 1.",
                    "label": 0
                },
                {
                    "sent": "The reason for this normalization step in this reversal of edges is meant that we noticed an experiment that this works better.",
                    "label": 0
                },
                {
                    "sent": "OK, so some of them make sense.",
                    "label": 0
                },
                {
                    "sent": "Some of them are a bit.",
                    "label": 0
                },
                {
                    "sent": "We don't really know why they buy the improved results.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What we what we kind of end up with this is this large graph now.",
                    "label": 0
                },
                {
                    "sent": "This computation, which are saying like like doing the person's Pagerank doing the inverse, something together it's.",
                    "label": 0
                },
                {
                    "sent": "This is not going to work really, so the problem is if you take a normal person's patron algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "If you take the real thing, you have to compute this for each possible note in the graph.",
                    "label": 0
                },
                {
                    "sent": "So remember that here on the top we have our entities.",
                    "label": 0
                },
                {
                    "sent": "If you're going to do this person, especially for all these entities one by one, this is just not going to workout.",
                    "label": 0
                },
                {
                    "sent": "So what we did we did.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimizations on this computing personalization for all the possible nodes.",
                    "label": 1
                },
                {
                    "sent": "So we start from one existing approximation algorithm, so it's a bookmark coloring algorithm, and then we improve that further to be able to handle, let's say, and I'll note personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "So we have this.",
                    "label": 0
                },
                {
                    "sent": "The first contribution that paper, really, it's an approximation for all pairs.",
                    "label": 1
                },
                {
                    "sent": "Personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "So thanks to this this algorithm, we end up with.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very large matrix with all entities on the top.",
                    "label": 0
                },
                {
                    "sent": "I'll end.",
                    "label": 0
                },
                {
                    "sent": "It is an all predicates on the underside.",
                    "label": 0
                },
                {
                    "sent": "And this this metrics are going to be rather sparse thanks to the approximation.",
                    "label": 0
                },
                {
                    "sent": "What we're doing with this matrix is we're feeding it basically to the existing law framework, which is which I will go into a bit sooner and we get embeddings out, so we get a vector for each entity in the graph.",
                    "label": 0
                },
                {
                    "sent": "So glad he's going to optimize this formula and I'm not going to go into that, But anyway, so it's optimizing this formula, and anyway, I have a couple of things which it will try to do with this formula.",
                    "label": 0
                },
                {
                    "sent": "It starts from the assumption that similar things will have similar vector in this in this matrix, right?",
                    "label": 0
                },
                {
                    "sent": "So if you have something like in our graph, that little bit back here, something like Vienna and something like Berlin.",
                    "label": 0
                },
                {
                    "sent": "If you look in the graph they have a bit of similar structure right?",
                    "label": 0
                },
                {
                    "sent": "In this one, very similar that I just made a small difference here, but they're going to have very similar structure and as a result they're going to get very similar.",
                    "label": 0
                },
                {
                    "sent": "Let's say very similar column in this core occurrence matrix as well.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That assumption being made, then it will optimize for this kind of analogies which we signed out in previous presentations.",
                    "label": 0
                },
                {
                    "sent": "Well, very shortly mentioned and one of the features of gloves that's going to capture complete context so it will do much quite quite large environment of the note into account when doing the embedding.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we have this embedding.",
                    "label": 0
                },
                {
                    "sent": "Now we have to do some kind of evaluation to see where this thing makes any sense or works at all.",
                    "label": 0
                },
                {
                    "sent": "So instead we work on very large graph, so this is really embedded base to hold the pedia with this.",
                    "label": 0
                },
                {
                    "sent": "So this was mentioned previously.",
                    "label": 0
                },
                {
                    "sent": "It was one of the problems.",
                    "label": 0
                },
                {
                    "sent": "So our approach scales reasonably well.",
                    "label": 0
                },
                {
                    "sent": "So about 5 million instances 1004 under mappings or 400 properties.",
                    "label": 0
                },
                {
                    "sent": "We use different weighting strategies and we compared to a.",
                    "label": 0
                },
                {
                    "sent": "Kind of baselines so we have or business or other methods.",
                    "label": 1
                },
                {
                    "sent": "Also, the ones which were mentioned in the previous talk are here.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the first thing which we looked at is embedding time.",
                    "label": 1
                },
                {
                    "sent": "So our approach takes something between 6 and 48 hours, about two days to embed the whole whole thing and it is not such that the longer it takes, the better it gets.",
                    "label": 1
                },
                {
                    "sent": "OK, so this depends on the weighing method like how the wings done.",
                    "label": 0
                },
                {
                    "sent": "You get different kinds of matrices and then it takes longer so other methods on the left of it will take about a day to compute on the similar machine.",
                    "label": 1
                },
                {
                    "sent": "Distrans systems takes about a week and then there are many methods.",
                    "label": 0
                },
                {
                    "sent": "We just don't scale and one of the reasons is that you need so much memory to just store, like if you.",
                    "label": 0
                },
                {
                    "sent": "If you take all these notes, are these entities and you make a dense matrix out of it.",
                    "label": 0
                },
                {
                    "sent": "You need like 10s of terabytes of memory just to store that matrix temporarily.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we evaluated this thing on different machine learning tasks so we have we have different classification, regression and on document modeling tasks.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'm skipping that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because of this, but.",
                    "label": 0
                },
                {
                    "sent": "So what we find is the result is that our approach is kind of.",
                    "label": 0
                },
                {
                    "sent": "On par or a bit worse, artist avec most of the times.",
                    "label": 0
                },
                {
                    "sent": "So one of the reasons that are on the reason we still suspect is that we don't know how to put certain parameters.",
                    "label": 0
                },
                {
                    "sent": "We have 12 weighing strategies.",
                    "label": 0
                },
                {
                    "sent": "We have several other parameters and we did not do an exhaustive evaluation.",
                    "label": 0
                },
                {
                    "sent": "All possible para meters here.",
                    "label": 0
                },
                {
                    "sent": "So this is one thing which we still should do, but still.",
                    "label": 0
                },
                {
                    "sent": "So we outperformed all the other baselines except for the artifact approach.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK then yeah, similar and then OK. We have some tasks.",
                    "label": 0
                },
                {
                    "sent": "Some specific task on which we will outperform artifact as well.",
                    "label": 0
                },
                {
                    "sent": "So in the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclusion already so so we have some kind of approach for generating embeddings based on the graph.",
                    "label": 1
                },
                {
                    "sent": "We explore exploit this kind of global pattern so we can go quite deep into the graph like this.",
                    "label": 0
                },
                {
                    "sent": "Like we don't take just a couple of hops, it can really go up to like 20 hops deep in the graph when I'm doing embedding.",
                    "label": 1
                },
                {
                    "sent": "It preserves some of the graph information.",
                    "label": 1
                },
                {
                    "sent": "At least we are compatible with certain machine learning algorithms all at once, which basically take vectors as input.",
                    "label": 1
                },
                {
                    "sent": "We compute very efficient embeddings compared to many of the other approaches, and then we have also noticed that we've kind of no one size fits all embedding problem, so the different weighting strategies are going to give different performance in different tasks.",
                    "label": 0
                },
                {
                    "sent": "And if there is no kind of 1 winner, so there's not like this waiting strategies, I was going to work for classification so there is no kind of consistent good embedding.",
                    "label": 0
                },
                {
                    "sent": "So, but we still have that up to now, so we did this task and it's at independent approach.",
                    "label": 0
                },
                {
                    "sent": "And as I have in the next slide here, there's probably not.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We should be doing in the future, so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "How to get forward from this so we have something which works.",
                    "label": 0
                },
                {
                    "sent": "It gives some results there reasonable, but how to get forward is the last thing to say.",
                    "label": 0
                },
                {
                    "sent": "So first one we need better evaluation metrics as in the previous talk was set like OK, we use these metrics.",
                    "label": 1
                },
                {
                    "sent": "We use this metric so everyone is using a bit of different metrics.",
                    "label": 0
                },
                {
                    "sent": "There are some standard ones, but they are very very small.",
                    "label": 0
                },
                {
                    "sent": "It's a very small graphs which are being embedded.",
                    "label": 0
                },
                {
                    "sent": "Very small test sets as well.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now you can ask all kind of different questions about the embeddings, which you might want to answer.",
                    "label": 1
                },
                {
                    "sent": "We also need the large scale evaluation, so having like a lot of lot of different, there's lots of different datasets.",
                    "label": 0
                },
                {
                    "sent": "Another thing is we don't include literal settlement.",
                    "label": 0
                },
                {
                    "sent": "It is possible.",
                    "label": 0
                },
                {
                    "sent": "Same with the previous talks or basically was done in the previous talk can be contained inside this Co occurrence matrix as well, so we can do this kind of combined embeddings and then we're going to have embedding of all sorts of graph.",
                    "label": 0
                },
                {
                    "sent": "So for example, we were discussing earlier or I was discussing earlier about, for example, embedding Markov chains.",
                    "label": 0
                },
                {
                    "sent": "We can also have let's all kind of different graphs which can be embedded to see what happens with that.",
                    "label": 0
                },
                {
                    "sent": "So with this match that I'm open to any.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Questions or comments?",
                    "label": 0
                },
                {
                    "sent": "Thank you very much Michael.",
                    "label": 0
                },
                {
                    "sent": "Hey, thanks for your presentation.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if you look into the effect of imbalance graph in terms of knowledge on this.",
                    "label": 0
                },
                {
                    "sent": "Like for example if for Vienna we had the name of the mayor and also historical places would it be still the case that it's going to be similar to billing?",
                    "label": 0
                },
                {
                    "sent": "Or would it kind of have a negative effect on that?",
                    "label": 0
                },
                {
                    "sent": "So we we did not like specifically look at it but we know that the glove embedding model is going to cancel out some of these things, like if you have large similarity.",
                    "label": 0
                },
                {
                    "sent": "It will still be similar, of course.",
                    "label": 0
                },
                {
                    "sent": "Any discrepancy between entity is going to push them further apart.",
                    "label": 0
                },
                {
                    "sent": "But we did not.",
                    "label": 0
                },
                {
                    "sent": "We did not like analyze exactly how much that would be, but it it it will happen.",
                    "label": 0
                },
                {
                    "sent": "But you're saying yes, did you try to reduce the initial graph in order to actually make this more scalable, you could do some sampling or something like that.",
                    "label": 0
                },
                {
                    "sent": "OK, always be directly start from the original graph.",
                    "label": 0
                },
                {
                    "sent": "But one thing we do is we.",
                    "label": 0
                },
                {
                    "sent": "So we removed the literal values which have a lot of information then that's why we will never in the future.",
                    "label": 0
                },
                {
                    "sent": "So that is a sort of thing what you're saying, so we reduce the size of the graph by this quite a lot, right?",
                    "label": 0
                },
                {
                    "sent": "Or maybe you care about a specific properties where specific things, and then you pick everything.",
                    "label": 0
                },
                {
                    "sent": "Right, sorry.",
                    "label": 0
                },
                {
                    "sent": "What is the sort of machine that you perform your tests on?",
                    "label": 0
                },
                {
                    "sent": "So this is a large machine, so it's a. I think I have to recall it's more exactly what we used about.",
                    "label": 0
                },
                {
                    "sent": "This has about 200 gigabytes of RAM, so that's one thing which which is maybe more of a bottleneck.",
                    "label": 0
                },
                {
                    "sent": "Then the so the coherence computations just using single threats, but for the.",
                    "label": 0
                },
                {
                    "sent": "Embedding cell I think it's using.",
                    "label": 0
                },
                {
                    "sent": "Recall correctly 32 cores or something like that, but that is the same for the other methods as well, so we use the same ship setup.",
                    "label": 0
                },
                {
                    "sent": "Cool, thanks.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to inject one of my own questions.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Why are you assuming RDF is directional?",
                    "label": 0
                },
                {
                    "sent": "I found that rather curious why do we assume?",
                    "label": 0
                },
                {
                    "sent": "Well, your personalized page rank only follows links forward and not backwards.",
                    "label": 0
                },
                {
                    "sent": "Have you have you done any sensitivity analysis on this?",
                    "label": 0
                },
                {
                    "sent": "Why does this make sense?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So this is a very good question.",
                    "label": 0
                },
                {
                    "sent": "Is something we should try.",
                    "label": 0
                },
                {
                    "sent": "So we did the reverse linking as well.",
                    "label": 0
                },
                {
                    "sent": "So we added them together.",
                    "label": 0
                },
                {
                    "sent": "But still, you could just ignore the directionality of the links and do do a thing like that this possible.",
                    "label": 0
                },
                {
                    "sent": "We didn't try it.",
                    "label": 0
                },
                {
                    "sent": "Well, that's that's the only answer I can give to the next question.",
                    "label": 0
                },
                {
                    "sent": "So you said some of the approaches don't scale to the size of DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Which one are those?",
                    "label": 0
                },
                {
                    "sent": "Because I'm missing some of the latest related work there, I would have to check which one did which one didn't so.",
                    "label": 0
                },
                {
                    "sent": "Did you check this holistic embeddings, or nor?",
                    "label": 0
                },
                {
                    "sent": "But that I would have to check?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we have.",
                    "label": 0
                },
                {
                    "sent": "We have a list which we did in those scale because we did it, but it's not scaling me.",
                    "label": 0
                },
                {
                    "sent": "Usually they didn't finish within two weeks or went out of memory in this 250 gigabyte machine so I can get back to you.",
                    "label": 0
                },
                {
                    "sent": "OK, two more lightning questions.",
                    "label": 0
                },
                {
                    "sent": "Thanks for your talk.",
                    "label": 0
                },
                {
                    "sent": "So my question is, would it be possible to identify what is the exact similarities and differences for a given entities in your graph?",
                    "label": 0
                },
                {
                    "sent": "As in your example, when in Berlin are similar, right?",
                    "label": 0
                },
                {
                    "sent": "So of course you can see that they're working.",
                    "label": 1
                },
                {
                    "sent": "Balance.",
                    "label": 0
                },
                {
                    "sent": "Their vectors are similar.",
                    "label": 0
                },
                {
                    "sent": "This is fine, but you know in some areas in particular, in biology it's it's really crucial to understand what is the difference.",
                    "label": 0
                },
                {
                    "sent": "Give another knows.",
                    "label": 0
                },
                {
                    "sent": "For example, in this particular example I would be really would be really fun to see that these are two capitals of German speaking countries.",
                    "label": 0
                },
                {
                    "sent": "So to extract this kind of information within this possible to extract.",
                    "label": 0
                },
                {
                    "sent": "Would it be possible that we hope so?",
                    "label": 0
                },
                {
                    "sent": "So we're currently working on some medical cases exactly like like for example, like let's say disease drug interaction we are doing.",
                    "label": 0
                },
                {
                    "sent": "And this is also a bit of a problem for link prediction, right?",
                    "label": 0
                },
                {
                    "sent": "So what you're trying to do your trying to see where you have missing links and try to complete the graph and then you also see what the differences are between entities, so it's a bit of a hope.",
                    "label": 0
                },
                {
                    "sent": "So the problem is really that the how to say that there is a step in between this embedding with the model which are currently using.",
                    "label": 0
                },
                {
                    "sent": "It's somewhat easier to reverse it with you earlier models, they are very.",
                    "label": 0
                },
                {
                    "sent": "Obfuscate, have no idea what this happening.",
                    "label": 0
                },
                {
                    "sent": "Here we have more of a chance to see why certain things are happening.",
                    "label": 0
                },
                {
                    "sent": "We're not there yet, so but the hope is at least there with previous one.",
                    "label": 0
                },
                {
                    "sent": "There was no hope to do an obvious question about the direction, so if you go with an undirected graph, so you end up with the frequency distribution of the overall graph.",
                    "label": 0
                },
                {
                    "sent": "So which doesn't makes really sense.",
                    "label": 0
                },
                {
                    "sent": "So you need a direction, otherwise it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "So that is good thing about it.",
                    "label": 0
                },
                {
                    "sent": "I agree that's right, that's right to his question.",
                    "label": 0
                },
                {
                    "sent": "So I agree.",
                    "label": 0
                },
                {
                    "sent": "But we notice that if we reverse the edges our quality improves.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "We can talk about it offline.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I was lying around so it's one of the things we all agree we need to talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much, Michael.",
                    "label": 0
                }
            ]
        }
    }
}