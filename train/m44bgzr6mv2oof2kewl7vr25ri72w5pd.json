{
    "id": "m44bgzr6mv2oof2kewl7vr25ri72w5pd",
    "title": "Stochastic Subgradient Approach for Solving Linear Support Vector Machines",
    "info": {
        "author": [
            "Jan Rupnik, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Nov. 7, 2008",
        "recorded": "October 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/sikdd08_rupnik_ssa/",
    "segmentation": [
        [
            "But he's going little bit more into into inside of SVN.",
            "So it should be more machine learning package presentation.",
            "As we heard about April or on the on Rupnick and I'll be talking about this quite a quite recent approach to solving linear support vector machines.",
            "And it's based on some stochastic subgradient technique."
        ],
        [
            "So the outline of this presentation will be first.",
            "I'll give an introduction, then I'll talk about support vector machines.",
            "Then I'll continue with some details with this stochastic subgradient descent approach.",
            "This is an algorithm called Pegasus, which I will describe in more in details, and then I'll conclude with some experiments and final notes."
        ],
        [
            "So for the introduction, so support vector machines are one of the most popular classification tools for solving so binary classification problems.",
            "And just the some straightforward implementations are not very tractable because they're there.",
            "Computational complexity is super linear in the number of training examples.",
            "So a lot of different approaches existed to to some different optimization techniques to get around that problem, and in the recent year some new techniques emerged that had that have linear computational complexity and the algorithm that will present today is one of those.",
            "And what's nice?",
            "Nice about it.",
            "Why I chose it is because it's a.",
            "It's very easy to understand and easy to implement."
        ],
        [
            "So first about the support vector machines.",
            "So this is just the full picture to give you a flavor.",
            "So we have this positive and negative examples that the red and blue dots.",
            "And so we're trying to find the linear separator, and as you can see, there are many possible classifiers that would perfectly separate these two sets.",
            "For instance, one of those is the red line.",
            "But there's only one one classifier that has the maximal margin to this data set, so the distance to the closest points is a is the maximum, so this is only the one with the blue line, so that hyperplane is the ideal in that sense, and so optimizing for that criterion.",
            "The margin criterion turns out to be to have some very nice theoretical properties, so one can show that.",
            "Discuss this is connected with the generalization ability of the classifier.",
            "So and so.",
            "This is the heart."
        ],
        [
            "Margin.",
            "The so input in most cases the data isn't perfectly linearly separable and that were classified classifier wouldn't even exist.",
            "So this is a soft margin extension which allows which allows some data points to be misclassified, but still and still to find the large margin.",
            "So this is robust to noise.",
            "Um San Juan one.",
            "Another important thing about SVM's or the support vectors.",
            "So these are the dots that lie on the border of this margin.",
            "And also these two points are support vectors as well.",
            "But not not in a not proper support vector, so these are the bounded support vectors, so they're called.",
            "So some points can lie within the margin, but still because so correctly classified.",
            "But this red point, for example, lies on the other side, so it would be misclassified and the soft soft margin.",
            "SVM provides us this tradeoff between this classification accuracy on the training set and the size of the margin, which is usually connected with the generalization ability."
        ],
        [
            "So the problem setting this is we have a.",
            "We have a finite sample of data points that are pairs of input and output.",
            "Vectors, so input or inputs are vectors.",
            "Outputs are just numbers plus or minus one, so this these represent the classes.",
            "So in the, the goal is to find the hyperplane with the normal vector W which is of the same dimensionality as the data.",
            "And the offset vector B, which is a real number.",
            "Which define hyper play so that and that hyper hyperplane has to have high classification accuracy on the training set S and the large margin.",
            "And the new example using this hyperplane would be classified with this formula.",
            "So we just multiply it with W. This is the inner product, deduct the.",
            "B and just take the sign of it."
        ],
        [
            "So, and this is in in in more, more detailed.",
            "This is a formal optimization problem of the soft soft margin SVM, but this particular form is maybe less known that the one that where you are solving this quadratic program in the dual view with Alpha variables.",
            "This one this is a design constraint.",
            "Optimization problem is maybe less frequently directly solved, But the approach that I'll be presenting is.",
            "Is dealing with this formula and so just to give you an interpretation of the terms here.",
            "Further, so we're minimizing this quantity and over over different W vectors and different be numbers.",
            "So the 1st the 1st first number Lambda over 2 is a is just a trade off parameter between the.",
            "This size of the margin and the this loss function.",
            "So there so as I said, what there are two.",
            "There are two objectives here.",
            "One is to be to have a low loss on the training set.",
            "This is the second part and the other one is to have a large margin.",
            "So this is it.",
            "Turns out the margin can be can be.",
            "That is equivalent to looking at the norm of the W. Or this is the square of the norm?",
            "And if we look at this second term.",
            "This is the.",
            "This factor is the how we classify.",
            "So the absolute value of the XI sample when we classified so we don't take the sign, we just leave this as a this number.",
            "And then if we multiply it with Yi.",
            "Here we gather a positive number.",
            "If the example was correctly classified in the negative number.",
            "If it was misclassified.",
            "So and if we're deducing a negative number here, this means that the loss is positive.",
            "So this plus just means that we're taking the positive.",
            "If if the number is larger than zero, we leave it as it is, but if it's smaller, we set it to 0.",
            "So yeah, question.",
            "You said the best is the tradeoff between margin loss, so so that's usually.",
            "But sometimes this is, I think, most of the times this is referred to as C. In SVM, yes.",
            "So this is.",
            "Actually so divided by or, so this so this Lambda, large values of Lambda will emphasize the margin and low values of Lambda will emphasize the loss.",
            "Um?",
            "So and OK.",
            "So now I will."
        ],
        [
            "I'll give you another this analogy with the perceptron algorithm.",
            "That is all that is a very well known algorithm.",
            "And because the the the algorithm for solving our problem will be very similar to Perceptron, so that's all that's very nice about it.",
            "So we see that our our regularised hinge loss OK First thing we set B20 we ignore it.",
            "This offset parameter and after that if we look at the optimization problem of SVM it's yeah this norm of W squared plus that hinge loss function on the training data.",
            "And if you look at the this.",
            "This last function that we're optimizing when we're solving perception, we see that they are quite similar.",
            "So we don't have any regularization term like the square of the norm.",
            "W. So the perceptron doesn't doesn't do anything about this model selection becausw in SVM, this norm of W actually represents model complexity.",
            "So, so Perceptron just tries to find a classifier that will optimize this loss function.",
            "So it's just trying to separate the training set and you see that these two loss functions are very similar."
        ],
        [
            "Except for the only difference that the the hinge loss is a plus one in the."
        ],
        [
            "In this part, the part the positive only part."
        ],
        [
            "So in the if we look at this loss functions, we see that even if we classify appoint correctly in the SVM, if it's if it's within the margin but still correctly classified, we give it we penalize it.",
            "So we give it some loss and that's the main difference between the SVM in the perception because in perceptron, if we're very close to the margin.",
            "Very close to the hyperplane, but correctly classified we don't penalize the the classifier.",
            "And the third one is just the standard 01 loss, which is, which is problematic to work with, and so most most of the algorithms assume this linear linear particle instead of a constant part here.",
            "Um?"
        ],
        [
            "So this is about the loss functions and then now to the to the core of the algorithm so."
        ],
        [
            "And this function is the the regularised hinge loss function is a piecewise quadratic function.",
            "This is a quadratic part and these are piecewise linear functions because it's a sum of piecewise linear fun."
        ],
        [
            "So how this looks like is it's composed of different segments of nice differentiable quadratic functions.",
            "So since it's not smooth, it's not even differentiable.",
            "We can't use standard techniques like gradient descent, but we can use something very similar.",
            "It's called the subgradient descent algorithm.",
            "So the main point is that if if the function is differentiable, we can compute gradient in every point, which is uniquely defined.",
            "So it's just a hyperplane that start that touches the point and lies beneath the.",
            "This graph of the function.",
            "But if the function is not differentiable, then we have lots of candidates.",
            "For instance, all these three lines set this all these three lines are considered as subgradients, and they're all equally valid.",
            "And the the main point is that the gradient approach can be generalized to subgradient descent techniques.",
            "To solving this optimization problems."
        ],
        [
            "So the subgradient in the perceptron algorithm is just 1 / M times the sum of minus YIXI for all the misclassified examples.",
            "And this is the.",
            "This is the well known thing about the perception.",
            "So what we're doing there is.",
            "We're we're scanning through the data and once we find the misclassified example, we add a portion of that example to our hyperplane.",
            "If it's a positive, or we deduce the that example if it's a negative.",
            "So this is the why I hear that tells us this.",
            "So in this sub gradient in the SVM is very similar except for we have this one here and Lambda times W. So, um and.",
            "So four and another point is that.",
            "Let's say if we have 1 million points, computing a subgradient in one point will include.",
            "We will include one million sums of the vectors, so we have to sum all the training points that were misclassified in that step, and this can be really time consuming.",
            "So the the stochastic part of the algorithm the the main idea here is that we just subsample a set of misclassified points and then use those points too.",
            "To estimate the subgradient.",
            "So this this formula is is, it is.",
            "It's the same as this formula, except for.",
            "We're not summing over the whole training set, but only over K examples for.",
            "So this is the subsample.",
            "How?"
        ],
        [
            "OK, so and this is the algorithm.",
            "As you can see it's it's a very simple algorithm, just 4 lines, 5 lines in the in the main code.",
            "So the input is the training set S The thread this optimization trade off parameter Lambda.",
            "Then the capital T, which is the number of iterations that we will allow and the the parameter K which is the size of the subsample when we're estimating the subgradients.",
            "So in the first step we just choose W1 randomly, but but normalize it so that it lies within a bowl of with radius one over square root of Lambda.",
            "The reason for that is that the authors have proved that the solution will always lie in the bowl of that size, so this this.",
            "This will always just improve the convergence if you if you keep projecting it into the bowl.",
            "If, let's say the solution goes out of the one moment.",
            "So now we iterate with the with T from 1, two Cup capital T and in each step we choose a random subset of the training examples of size K. Then we set the set BT which is a set of pairs from this subsample which were misclassified.",
            "So this is what this formula says.",
            "Then we set the step size so we know that in gradient descent approaches we have a our current solution, the gradient, and we add the gradient, but only a proportion of gradient.",
            "So we need and we define that proportion with the step size.",
            "So we see that the step size her, so how how the gradient will influence the solution keeps decreasing as T grows, so if T is very large this parameter will be almost zero.",
            "And when will be adding the gradient, we won't be changing the solution much.",
            "So this this this is also referred to as the learning rate parameter.",
            "So the the third, the 4th step, is the most important one.",
            "This is just taking the W and moving into the direction of the subgradient with stepsize muti.",
            "And this is the.",
            "So this this is the formula that they showed just W plus the subgradient that I showed you in the previous slide in this final step is just projection onto a bowl of the radius one over square root of Lambda.",
            "And this projection is basically just a rescaling, so if the if the W is outside the ball, we just divide it with the appropriate number so that it falls within the ball.",
            "And this is all there is to the algorithm, and the most difficult part in in this with what the authors had to face, was to prove that this this algorithm really converges.",
            "But once you have that, implementing is.",
            "It is very simple, so it's it's almost identical to the purse."
        ],
        [
            "That's an algorithm.",
            "And so the question, what's new in bigger sources since gradient descent approach is very old, subgradient descent technique is 50 years old.",
            "Soft margin SVM is 14 years old.",
            "So, so what's new here?",
            "Is that the typical gradient descent methods they suffered from slow convergence and the point is that previously.",
            "Authors couldn't prove that the that's taking really aggressive decrease in learning rate still leads to convergence, so they had to use slow learning rate.",
            "For instance, 1 / sqrt T. So it was dropping slower than one over Lambda T. So this is one of the main.",
            "The main advantages of the biggest is algorithm that you know that you will still converge even though you're slowing the progress very rapidly.",
            "And the second one is that they prove that the solution lies in the ball of radius one over square root of Lambda, which is also essentially proving the convergence convergence of the algorithm."
        ],
        [
            "So and I'll just.",
            "I'll just assume go go onto the experiments and this is just a slide about SVM light, which is one of the most popular SVM solvers.",
            "But with super linear contained complexity.",
            "But still, I guess many people still use the SVM light, and so the experiments, the point of the experiments will be to show you that you can really surpass the performance of this SVM light.",
            "So it's based on solving a large quadratic program and it's using the fact that the solution of the SVM can be expressed in terms of just a small number of data points.",
            "The training points which are called the support vectors which you saw on the figure that are showed in the beginning of the presentation, and it's based on an active set method that's literally trying to find this true subset of support vectors so.",
            "It keeps adding vectors into the active set and removing vectors by by using some criteriums from the LaGrange multiplier theory.",
            "So one point is that the solutions are in.",
            "It solves a series of smaller quadratic problems in the process, so it's it's memory requirements aren't also demand demanding anymore, and so it's possible to deal with large datasets with limited memory and still get a solution.",
            "So the solutions that it finds are very highly accurate.",
            "And the algorithm is in with the device by Torsten your Kims.",
            "And here is just the link to that article."
        ],
        [
            "OK, so onto the experiments.",
            "So I was experimenting with text classification task, binary text classification on Reuters, RCA V2 data set which consists of rough, roughly 800,000 news articles.",
            "And so they already pre processed the data set with bag of words.",
            "Vectors is already publicly available, so I just use that data and there we have roughly five 50,000 features for every.",
            "Every vector that represents this news document.",
            "And one thing is that these vectors are very sparse, which is a good thing because the this Pegasos algorithm can take sparsity into account, so it can take advantage and get a computational speedup from that.",
            "And the point and why?",
            "Why it can do that is that the the main effort in computational effort lies in estimating the subgradient, which is when you're doing that.",
            "You're just operating with this large matrix of all the documents.",
            "And you have to sum, sum, sum those documents together and if they have a very low number of non zero elements you can easily implement that and and so speed the the summing part considerably.",
            "So to make this problem binary classification problem, we use the CC scatter category category which consists of 380 roughly 380,000 news and all the other news are considered as negative examples."
        ],
        [
            "So, so the, uh, the main advantage of Pegasus algorithm is that it really rapidly converges to suboptimal solutions.",
            "For instance, in 200 iterations, when I took on this whole data set of 800,000 documents, when I fixed the subsample size to 8000, I got within zero point 3%.",
            "From the objective value of the real objective value in nine point 2 computer seconds.",
            "Where is Alexa SVM?",
            "Light took roughly 4 hours of computational time to get a solution.",
            "Quick question.",
            "So, did you use SVM right or SVM?",
            "No, so that was a SVM light because that was.",
            "OK. OK."
        ],
        [
            "Then about the test set error, we can notice that the generalization performance stops becomes quite constant really early on in the optimization, so it's actually not.",
            "You can see that in this case it's not very reasonable to optimize over that.",
            "That number of iterations, for instance 30, because the there is no difference in the performance on the test set.",
            "So this is the point.",
            "The point is that suboptimal solutions can work equivalently well on GNU unseen data."
        ],
        [
            "And this is just about parameters K&T, so if we fix them, it's a it's actually and it doesn't matter what we set K to.",
            "This is the this large long constant.",
            "Curve here and if we increase the number of K * T It the objective value obviously becomes lower, so it this means that actually means that the algorithm converges.",
            "The more effort we put in, the closer we are."
        ],
        [
            "So these are just the conclusions.",
            "The Pegasos is a sub optimal.",
            "Solver for SVN and it's very easy to implement.",
            "It's based on subgradient stochastic subgradient approach.",
            "It's a linear solver, and so nonlinear extensions exist, but they're really not that efficient yet, so we lose this speed.",
            "And it can also take advantage of sparsity, which is very convenient, and let's."
        ],
        [
            "Text mining thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But he's going little bit more into into inside of SVN.",
                    "label": 0
                },
                {
                    "sent": "So it should be more machine learning package presentation.",
                    "label": 0
                },
                {
                    "sent": "As we heard about April or on the on Rupnick and I'll be talking about this quite a quite recent approach to solving linear support vector machines.",
                    "label": 1
                },
                {
                    "sent": "And it's based on some stochastic subgradient technique.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the outline of this presentation will be first.",
                    "label": 0
                },
                {
                    "sent": "I'll give an introduction, then I'll talk about support vector machines.",
                    "label": 1
                },
                {
                    "sent": "Then I'll continue with some details with this stochastic subgradient descent approach.",
                    "label": 0
                },
                {
                    "sent": "This is an algorithm called Pegasus, which I will describe in more in details, and then I'll conclude with some experiments and final notes.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the introduction, so support vector machines are one of the most popular classification tools for solving so binary classification problems.",
                    "label": 1
                },
                {
                    "sent": "And just the some straightforward implementations are not very tractable because they're there.",
                    "label": 1
                },
                {
                    "sent": "Computational complexity is super linear in the number of training examples.",
                    "label": 0
                },
                {
                    "sent": "So a lot of different approaches existed to to some different optimization techniques to get around that problem, and in the recent year some new techniques emerged that had that have linear computational complexity and the algorithm that will present today is one of those.",
                    "label": 0
                },
                {
                    "sent": "And what's nice?",
                    "label": 0
                },
                {
                    "sent": "Nice about it.",
                    "label": 0
                },
                {
                    "sent": "Why I chose it is because it's a.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to understand and easy to implement.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first about the support vector machines.",
                    "label": 0
                },
                {
                    "sent": "So this is just the full picture to give you a flavor.",
                    "label": 0
                },
                {
                    "sent": "So we have this positive and negative examples that the red and blue dots.",
                    "label": 0
                },
                {
                    "sent": "And so we're trying to find the linear separator, and as you can see, there are many possible classifiers that would perfectly separate these two sets.",
                    "label": 1
                },
                {
                    "sent": "For instance, one of those is the red line.",
                    "label": 1
                },
                {
                    "sent": "But there's only one one classifier that has the maximal margin to this data set, so the distance to the closest points is a is the maximum, so this is only the one with the blue line, so that hyperplane is the ideal in that sense, and so optimizing for that criterion.",
                    "label": 1
                },
                {
                    "sent": "The margin criterion turns out to be to have some very nice theoretical properties, so one can show that.",
                    "label": 0
                },
                {
                    "sent": "Discuss this is connected with the generalization ability of the classifier.",
                    "label": 0
                },
                {
                    "sent": "So and so.",
                    "label": 0
                },
                {
                    "sent": "This is the heart.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Margin.",
                    "label": 0
                },
                {
                    "sent": "The so input in most cases the data isn't perfectly linearly separable and that were classified classifier wouldn't even exist.",
                    "label": 0
                },
                {
                    "sent": "So this is a soft margin extension which allows which allows some data points to be misclassified, but still and still to find the large margin.",
                    "label": 1
                },
                {
                    "sent": "So this is robust to noise.",
                    "label": 0
                },
                {
                    "sent": "Um San Juan one.",
                    "label": 0
                },
                {
                    "sent": "Another important thing about SVM's or the support vectors.",
                    "label": 0
                },
                {
                    "sent": "So these are the dots that lie on the border of this margin.",
                    "label": 0
                },
                {
                    "sent": "And also these two points are support vectors as well.",
                    "label": 0
                },
                {
                    "sent": "But not not in a not proper support vector, so these are the bounded support vectors, so they're called.",
                    "label": 0
                },
                {
                    "sent": "So some points can lie within the margin, but still because so correctly classified.",
                    "label": 0
                },
                {
                    "sent": "But this red point, for example, lies on the other side, so it would be misclassified and the soft soft margin.",
                    "label": 0
                },
                {
                    "sent": "SVM provides us this tradeoff between this classification accuracy on the training set and the size of the margin, which is usually connected with the generalization ability.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem setting this is we have a.",
                    "label": 0
                },
                {
                    "sent": "We have a finite sample of data points that are pairs of input and output.",
                    "label": 0
                },
                {
                    "sent": "Vectors, so input or inputs are vectors.",
                    "label": 0
                },
                {
                    "sent": "Outputs are just numbers plus or minus one, so this these represent the classes.",
                    "label": 0
                },
                {
                    "sent": "So in the, the goal is to find the hyperplane with the normal vector W which is of the same dimensionality as the data.",
                    "label": 1
                },
                {
                    "sent": "And the offset vector B, which is a real number.",
                    "label": 0
                },
                {
                    "sent": "Which define hyper play so that and that hyper hyperplane has to have high classification accuracy on the training set S and the large margin.",
                    "label": 1
                },
                {
                    "sent": "And the new example using this hyperplane would be classified with this formula.",
                    "label": 0
                },
                {
                    "sent": "So we just multiply it with W. This is the inner product, deduct the.",
                    "label": 0
                },
                {
                    "sent": "B and just take the sign of it.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, and this is in in in more, more detailed.",
                    "label": 0
                },
                {
                    "sent": "This is a formal optimization problem of the soft soft margin SVM, but this particular form is maybe less known that the one that where you are solving this quadratic program in the dual view with Alpha variables.",
                    "label": 0
                },
                {
                    "sent": "This one this is a design constraint.",
                    "label": 1
                },
                {
                    "sent": "Optimization problem is maybe less frequently directly solved, But the approach that I'll be presenting is.",
                    "label": 0
                },
                {
                    "sent": "Is dealing with this formula and so just to give you an interpretation of the terms here.",
                    "label": 0
                },
                {
                    "sent": "Further, so we're minimizing this quantity and over over different W vectors and different be numbers.",
                    "label": 0
                },
                {
                    "sent": "So the 1st the 1st first number Lambda over 2 is a is just a trade off parameter between the.",
                    "label": 0
                },
                {
                    "sent": "This size of the margin and the this loss function.",
                    "label": 1
                },
                {
                    "sent": "So there so as I said, what there are two.",
                    "label": 0
                },
                {
                    "sent": "There are two objectives here.",
                    "label": 1
                },
                {
                    "sent": "One is to be to have a low loss on the training set.",
                    "label": 0
                },
                {
                    "sent": "This is the second part and the other one is to have a large margin.",
                    "label": 0
                },
                {
                    "sent": "So this is it.",
                    "label": 0
                },
                {
                    "sent": "Turns out the margin can be can be.",
                    "label": 0
                },
                {
                    "sent": "That is equivalent to looking at the norm of the W. Or this is the square of the norm?",
                    "label": 0
                },
                {
                    "sent": "And if we look at this second term.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This factor is the how we classify.",
                    "label": 0
                },
                {
                    "sent": "So the absolute value of the XI sample when we classified so we don't take the sign, we just leave this as a this number.",
                    "label": 0
                },
                {
                    "sent": "And then if we multiply it with Yi.",
                    "label": 0
                },
                {
                    "sent": "Here we gather a positive number.",
                    "label": 0
                },
                {
                    "sent": "If the example was correctly classified in the negative number.",
                    "label": 0
                },
                {
                    "sent": "If it was misclassified.",
                    "label": 0
                },
                {
                    "sent": "So and if we're deducing a negative number here, this means that the loss is positive.",
                    "label": 0
                },
                {
                    "sent": "So this plus just means that we're taking the positive.",
                    "label": 0
                },
                {
                    "sent": "If if the number is larger than zero, we leave it as it is, but if it's smaller, we set it to 0.",
                    "label": 0
                },
                {
                    "sent": "So yeah, question.",
                    "label": 0
                },
                {
                    "sent": "You said the best is the tradeoff between margin loss, so so that's usually.",
                    "label": 0
                },
                {
                    "sent": "But sometimes this is, I think, most of the times this is referred to as C. In SVM, yes.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "Actually so divided by or, so this so this Lambda, large values of Lambda will emphasize the margin and low values of Lambda will emphasize the loss.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So and OK.",
                    "label": 0
                },
                {
                    "sent": "So now I will.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll give you another this analogy with the perceptron algorithm.",
                    "label": 0
                },
                {
                    "sent": "That is all that is a very well known algorithm.",
                    "label": 0
                },
                {
                    "sent": "And because the the the algorithm for solving our problem will be very similar to Perceptron, so that's all that's very nice about it.",
                    "label": 0
                },
                {
                    "sent": "So we see that our our regularised hinge loss OK First thing we set B20 we ignore it.",
                    "label": 1
                },
                {
                    "sent": "This offset parameter and after that if we look at the optimization problem of SVM it's yeah this norm of W squared plus that hinge loss function on the training data.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the this.",
                    "label": 0
                },
                {
                    "sent": "This last function that we're optimizing when we're solving perception, we see that they are quite similar.",
                    "label": 0
                },
                {
                    "sent": "So we don't have any regularization term like the square of the norm.",
                    "label": 0
                },
                {
                    "sent": "W. So the perceptron doesn't doesn't do anything about this model selection becausw in SVM, this norm of W actually represents model complexity.",
                    "label": 0
                },
                {
                    "sent": "So, so Perceptron just tries to find a classifier that will optimize this loss function.",
                    "label": 0
                },
                {
                    "sent": "So it's just trying to separate the training set and you see that these two loss functions are very similar.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Except for the only difference that the the hinge loss is a plus one in the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this part, the part the positive only part.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the if we look at this loss functions, we see that even if we classify appoint correctly in the SVM, if it's if it's within the margin but still correctly classified, we give it we penalize it.",
                    "label": 1
                },
                {
                    "sent": "So we give it some loss and that's the main difference between the SVM in the perception because in perceptron, if we're very close to the margin.",
                    "label": 1
                },
                {
                    "sent": "Very close to the hyperplane, but correctly classified we don't penalize the the classifier.",
                    "label": 0
                },
                {
                    "sent": "And the third one is just the standard 01 loss, which is, which is problematic to work with, and so most most of the algorithms assume this linear linear particle instead of a constant part here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is about the loss functions and then now to the to the core of the algorithm so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this function is the the regularised hinge loss function is a piecewise quadratic function.",
                    "label": 0
                },
                {
                    "sent": "This is a quadratic part and these are piecewise linear functions because it's a sum of piecewise linear fun.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how this looks like is it's composed of different segments of nice differentiable quadratic functions.",
                    "label": 0
                },
                {
                    "sent": "So since it's not smooth, it's not even differentiable.",
                    "label": 0
                },
                {
                    "sent": "We can't use standard techniques like gradient descent, but we can use something very similar.",
                    "label": 1
                },
                {
                    "sent": "It's called the subgradient descent algorithm.",
                    "label": 1
                },
                {
                    "sent": "So the main point is that if if the function is differentiable, we can compute gradient in every point, which is uniquely defined.",
                    "label": 0
                },
                {
                    "sent": "So it's just a hyperplane that start that touches the point and lies beneath the.",
                    "label": 0
                },
                {
                    "sent": "This graph of the function.",
                    "label": 0
                },
                {
                    "sent": "But if the function is not differentiable, then we have lots of candidates.",
                    "label": 0
                },
                {
                    "sent": "For instance, all these three lines set this all these three lines are considered as subgradients, and they're all equally valid.",
                    "label": 1
                },
                {
                    "sent": "And the the main point is that the gradient approach can be generalized to subgradient descent techniques.",
                    "label": 0
                },
                {
                    "sent": "To solving this optimization problems.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the subgradient in the perceptron algorithm is just 1 / M times the sum of minus YIXI for all the misclassified examples.",
                    "label": 1
                },
                {
                    "sent": "And this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the well known thing about the perception.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing there is.",
                    "label": 0
                },
                {
                    "sent": "We're we're scanning through the data and once we find the misclassified example, we add a portion of that example to our hyperplane.",
                    "label": 0
                },
                {
                    "sent": "If it's a positive, or we deduce the that example if it's a negative.",
                    "label": 0
                },
                {
                    "sent": "So this is the why I hear that tells us this.",
                    "label": 0
                },
                {
                    "sent": "So in this sub gradient in the SVM is very similar except for we have this one here and Lambda times W. So, um and.",
                    "label": 0
                },
                {
                    "sent": "So four and another point is that.",
                    "label": 0
                },
                {
                    "sent": "Let's say if we have 1 million points, computing a subgradient in one point will include.",
                    "label": 0
                },
                {
                    "sent": "We will include one million sums of the vectors, so we have to sum all the training points that were misclassified in that step, and this can be really time consuming.",
                    "label": 0
                },
                {
                    "sent": "So the the stochastic part of the algorithm the the main idea here is that we just subsample a set of misclassified points and then use those points too.",
                    "label": 0
                },
                {
                    "sent": "To estimate the subgradient.",
                    "label": 0
                },
                {
                    "sent": "So this this formula is is, it is.",
                    "label": 0
                },
                {
                    "sent": "It's the same as this formula, except for.",
                    "label": 0
                },
                {
                    "sent": "We're not summing over the whole training set, but only over K examples for.",
                    "label": 0
                },
                {
                    "sent": "So this is the subsample.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so and this is the algorithm.",
                    "label": 1
                },
                {
                    "sent": "As you can see it's it's a very simple algorithm, just 4 lines, 5 lines in the in the main code.",
                    "label": 0
                },
                {
                    "sent": "So the input is the training set S The thread this optimization trade off parameter Lambda.",
                    "label": 0
                },
                {
                    "sent": "Then the capital T, which is the number of iterations that we will allow and the the parameter K which is the size of the subsample when we're estimating the subgradients.",
                    "label": 0
                },
                {
                    "sent": "So in the first step we just choose W1 randomly, but but normalize it so that it lies within a bowl of with radius one over square root of Lambda.",
                    "label": 0
                },
                {
                    "sent": "The reason for that is that the authors have proved that the solution will always lie in the bowl of that size, so this this.",
                    "label": 0
                },
                {
                    "sent": "This will always just improve the convergence if you if you keep projecting it into the bowl.",
                    "label": 0
                },
                {
                    "sent": "If, let's say the solution goes out of the one moment.",
                    "label": 0
                },
                {
                    "sent": "So now we iterate with the with T from 1, two Cup capital T and in each step we choose a random subset of the training examples of size K. Then we set the set BT which is a set of pairs from this subsample which were misclassified.",
                    "label": 0
                },
                {
                    "sent": "So this is what this formula says.",
                    "label": 0
                },
                {
                    "sent": "Then we set the step size so we know that in gradient descent approaches we have a our current solution, the gradient, and we add the gradient, but only a proportion of gradient.",
                    "label": 0
                },
                {
                    "sent": "So we need and we define that proportion with the step size.",
                    "label": 0
                },
                {
                    "sent": "So we see that the step size her, so how how the gradient will influence the solution keeps decreasing as T grows, so if T is very large this parameter will be almost zero.",
                    "label": 0
                },
                {
                    "sent": "And when will be adding the gradient, we won't be changing the solution much.",
                    "label": 1
                },
                {
                    "sent": "So this this this is also referred to as the learning rate parameter.",
                    "label": 0
                },
                {
                    "sent": "So the the third, the 4th step, is the most important one.",
                    "label": 0
                },
                {
                    "sent": "This is just taking the W and moving into the direction of the subgradient with stepsize muti.",
                    "label": 0
                },
                {
                    "sent": "And this is the.",
                    "label": 0
                },
                {
                    "sent": "So this this is the formula that they showed just W plus the subgradient that I showed you in the previous slide in this final step is just projection onto a bowl of the radius one over square root of Lambda.",
                    "label": 0
                },
                {
                    "sent": "And this projection is basically just a rescaling, so if the if the W is outside the ball, we just divide it with the appropriate number so that it falls within the ball.",
                    "label": 0
                },
                {
                    "sent": "And this is all there is to the algorithm, and the most difficult part in in this with what the authors had to face, was to prove that this this algorithm really converges.",
                    "label": 0
                },
                {
                    "sent": "But once you have that, implementing is.",
                    "label": 0
                },
                {
                    "sent": "It is very simple, so it's it's almost identical to the purse.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's an algorithm.",
                    "label": 0
                },
                {
                    "sent": "And so the question, what's new in bigger sources since gradient descent approach is very old, subgradient descent technique is 50 years old.",
                    "label": 1
                },
                {
                    "sent": "Soft margin SVM is 14 years old.",
                    "label": 0
                },
                {
                    "sent": "So, so what's new here?",
                    "label": 0
                },
                {
                    "sent": "Is that the typical gradient descent methods they suffered from slow convergence and the point is that previously.",
                    "label": 0
                },
                {
                    "sent": "Authors couldn't prove that the that's taking really aggressive decrease in learning rate still leads to convergence, so they had to use slow learning rate.",
                    "label": 1
                },
                {
                    "sent": "For instance, 1 / sqrt T. So it was dropping slower than one over Lambda T. So this is one of the main.",
                    "label": 0
                },
                {
                    "sent": "The main advantages of the biggest is algorithm that you know that you will still converge even though you're slowing the progress very rapidly.",
                    "label": 0
                },
                {
                    "sent": "And the second one is that they prove that the solution lies in the ball of radius one over square root of Lambda, which is also essentially proving the convergence convergence of the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So and I'll just.",
                    "label": 0
                },
                {
                    "sent": "I'll just assume go go onto the experiments and this is just a slide about SVM light, which is one of the most popular SVM solvers.",
                    "label": 0
                },
                {
                    "sent": "But with super linear contained complexity.",
                    "label": 0
                },
                {
                    "sent": "But still, I guess many people still use the SVM light, and so the experiments, the point of the experiments will be to show you that you can really surpass the performance of this SVM light.",
                    "label": 0
                },
                {
                    "sent": "So it's based on solving a large quadratic program and it's using the fact that the solution of the SVM can be expressed in terms of just a small number of data points.",
                    "label": 1
                },
                {
                    "sent": "The training points which are called the support vectors which you saw on the figure that are showed in the beginning of the presentation, and it's based on an active set method that's literally trying to find this true subset of support vectors so.",
                    "label": 0
                },
                {
                    "sent": "It keeps adding vectors into the active set and removing vectors by by using some criteriums from the LaGrange multiplier theory.",
                    "label": 1
                },
                {
                    "sent": "So one point is that the solutions are in.",
                    "label": 0
                },
                {
                    "sent": "It solves a series of smaller quadratic problems in the process, so it's it's memory requirements aren't also demand demanding anymore, and so it's possible to deal with large datasets with limited memory and still get a solution.",
                    "label": 0
                },
                {
                    "sent": "So the solutions that it finds are very highly accurate.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm is in with the device by Torsten your Kims.",
                    "label": 0
                },
                {
                    "sent": "And here is just the link to that article.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so onto the experiments.",
                    "label": 0
                },
                {
                    "sent": "So I was experimenting with text classification task, binary text classification on Reuters, RCA V2 data set which consists of rough, roughly 800,000 news articles.",
                    "label": 1
                },
                {
                    "sent": "And so they already pre processed the data set with bag of words.",
                    "label": 0
                },
                {
                    "sent": "Vectors is already publicly available, so I just use that data and there we have roughly five 50,000 features for every.",
                    "label": 0
                },
                {
                    "sent": "Every vector that represents this news document.",
                    "label": 0
                },
                {
                    "sent": "And one thing is that these vectors are very sparse, which is a good thing because the this Pegasos algorithm can take sparsity into account, so it can take advantage and get a computational speedup from that.",
                    "label": 0
                },
                {
                    "sent": "And the point and why?",
                    "label": 0
                },
                {
                    "sent": "Why it can do that is that the the main effort in computational effort lies in estimating the subgradient, which is when you're doing that.",
                    "label": 0
                },
                {
                    "sent": "You're just operating with this large matrix of all the documents.",
                    "label": 0
                },
                {
                    "sent": "And you have to sum, sum, sum those documents together and if they have a very low number of non zero elements you can easily implement that and and so speed the the summing part considerably.",
                    "label": 0
                },
                {
                    "sent": "So to make this problem binary classification problem, we use the CC scatter category category which consists of 380 roughly 380,000 news and all the other news are considered as negative examples.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so the, uh, the main advantage of Pegasus algorithm is that it really rapidly converges to suboptimal solutions.",
                    "label": 0
                },
                {
                    "sent": "For instance, in 200 iterations, when I took on this whole data set of 800,000 documents, when I fixed the subsample size to 8000, I got within zero point 3%.",
                    "label": 0
                },
                {
                    "sent": "From the objective value of the real objective value in nine point 2 computer seconds.",
                    "label": 1
                },
                {
                    "sent": "Where is Alexa SVM?",
                    "label": 0
                },
                {
                    "sent": "Light took roughly 4 hours of computational time to get a solution.",
                    "label": 1
                },
                {
                    "sent": "Quick question.",
                    "label": 0
                },
                {
                    "sent": "So, did you use SVM right or SVM?",
                    "label": 0
                },
                {
                    "sent": "No, so that was a SVM light because that was.",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then about the test set error, we can notice that the generalization performance stops becomes quite constant really early on in the optimization, so it's actually not.",
                    "label": 1
                },
                {
                    "sent": "You can see that in this case it's not very reasonable to optimize over that.",
                    "label": 0
                },
                {
                    "sent": "That number of iterations, for instance 30, because the there is no difference in the performance on the test set.",
                    "label": 1
                },
                {
                    "sent": "So this is the point.",
                    "label": 0
                },
                {
                    "sent": "The point is that suboptimal solutions can work equivalently well on GNU unseen data.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is just about parameters K&T, so if we fix them, it's a it's actually and it doesn't matter what we set K to.",
                    "label": 0
                },
                {
                    "sent": "This is the this large long constant.",
                    "label": 0
                },
                {
                    "sent": "Curve here and if we increase the number of K * T It the objective value obviously becomes lower, so it this means that actually means that the algorithm converges.",
                    "label": 0
                },
                {
                    "sent": "The more effort we put in, the closer we are.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are just the conclusions.",
                    "label": 0
                },
                {
                    "sent": "The Pegasos is a sub optimal.",
                    "label": 0
                },
                {
                    "sent": "Solver for SVN and it's very easy to implement.",
                    "label": 0
                },
                {
                    "sent": "It's based on subgradient stochastic subgradient approach.",
                    "label": 0
                },
                {
                    "sent": "It's a linear solver, and so nonlinear extensions exist, but they're really not that efficient yet, so we lose this speed.",
                    "label": 1
                },
                {
                    "sent": "And it can also take advantage of sparsity, which is very convenient, and let's.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Text mining thank you.",
                    "label": 0
                }
            ]
        }
    }
}