{
    "id": "n4y4fx5qsom5hp4zb5bojmsyeumqkete",
    "title": "Learning Patterns of the Brain: Machine Learning Challenges of fMRI Analysis",
    "info": {
        "author": [
            "Mark Palatucci, Robotics Institute, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Oct. 21, 2008",
        "recorded": "May 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/cmulls08_palatucci_lpb/",
    "segmentation": [
        [
            "Hello everyone.",
            "Thanks a lot for coming to the machine learning lunch on Mark Palatucci and so today I'm going to talk about learning patterns of the brain.",
            "They spent some time talking about the brain or spend some time talking about functional magnetic resonance imaging, which is a new tool for studying the brain.",
            "And it presents a lot of challenges for the machine learning and statistics."
        ],
        [
            "Community, so I'm going to talk about some of those today, so a lot of the work I'm going to discuss is actually a collaboration with the neuro semantics research team.",
            "Probably a lot of people you recognize a lot of these people are in the room, right?"
        ],
        [
            "No.",
            "So.",
            "With magnetic resonance imaging, you're probably used to seeing this image here on the left, and that's called a structural magnetic resonance image.",
            "And you're probably not as familiar, maybe with what's called functional MRI.",
            "Now functional MRI is actually a way to study the underlying neural activity of a person, so you can imagine taking an MRI machine and taking a snapshot of a persons like neural activity, say like every second.",
            "So in some sense you get like a movie of what's happening in the person's brain."
        ],
        [
            "So what are the applications of magnetic resonance imaging?",
            "So one is functional brain mapping an what you do here is basically trying to determine what parts of the brain respond to different types of stimuli.",
            "So in this study from the Indiana University School of Medicine, they basically played, spoke.",
            "They played a speech in a scanner and they found that men one side of the brain responds an in women two sides of the brain response."
        ],
        [
            "So.",
            "Another application is called cognitive State classification and sometimes this is known and you've seen this in the newspapers things people call in mind reading.",
            "But basically if we have some image of a person's neural activation, we want to know what is the person thinking about is that person you know, maybe thinking about food, sports, maybe music Britney?"
        ],
        [
            "Anne.",
            "If we think about each functional magnetic resonance images, it has about 15,000 voxels and wanna voxel is if you think about an image, a 2D image of pixels, it's picture element.",
            "If you think about taking a 3 dimensional image where one little tiny cube that's called the volume element and we call it a voxel.",
            "So we take one of those images roughly once a second an A typical experiment may last 30 to 60 seconds.",
            "Repeat that several times, so at the end of the day we have hundreds of thousands of data points and we have very limited training data, and there's a lot of noise.",
            "So if I repeat the experiment but do the same experiment with the same person in the same machine, I'm going to get a lot of variance on that data.",
            "So how can we actually learn?"
        ],
        [
            "Turn in this type of domain.",
            "So first challenge, I want to talk about is how?"
        ],
        [
            "Build a cognitive state classifier.",
            "So how these experiments work will usually take some participant and will put them into this.",
            "fMRI scanner will show them various stimuli, could be pictures of things we could be, sound music, and then we record their record, an image of their other neural activity."
        ],
        [
            "So a kind of state classifier.",
            "What we're trying to do is we're trying to learn this function that Maps neural images to the state of what they're doing.",
            "So for example, if we show people pictures of.",
            "Of dwelling, say like buildings and houses, and then we show them pictures of tools like Hammers, screwdrivers, things like that, can we actually build a classifier to distinguish between those two?",
            "Pat."
        ],
        [
            "So what other cognitive states can we classify an over the last like four or five years or so?",
            "There's actually been a really large literature that's been generated for people trying to figure out, like what they can distinguish.",
            "So one funding was a political affiliation, so we can determine just by showing you different pictures of political candidates.",
            "If you're a Democrat or Republican, we can actually determine we can show you pictures of different individuals, and we can tell you like which one is your.",
            "Your spouse, your girlfriend.",
            "And more recently people live.",
            "There are two companies actually that have started to try to use fMRI to do lie detection as a replacement for the polygraph."
        ],
        [
            "So one classifiers that people typically use here, so support vector machines, logistic regression if you do some sort of sparsity regularization, K nearest neighbor methods, naive Bayes, and all of these methods.",
            "Actually the exception of, I guess, naive Bayes and.",
            "The regularizer just aggression.",
            "You have to do some sort of prior dimensionality reduction step.",
            "So you have this very large time series with hundreds of thousands of features.",
            "Usually people would average over space, they would average overtime to try and reduce that dimension.",
            "So naive Bayes is quite popular 'cause it's very easy to implement.",
            "It's robust and you don't actually need to.",
            "Do this prior dimensionality reduction step and you can be successful on problems with only 40 examples.",
            "But Emma running our experiments is expensive.",
            "People, you know, it's uncomfortable for people to sit in these scanners for more than 20 minutes or so.",
            "So we'd like to use less data.",
            "So how can we build a class?"
        ],
        [
            "Our family had two examples.",
            "An so when you have little data, you know one way you can actually learn is to use what's called like hierarchical Bayesian modeling.",
            "An some of you might remember this from intermediate statistics.",
            "You know there's a lot of people who took that here an.",
            "There are some related work from injury Standi.",
            "Applying this to fMRI and the problem that he was trying to address is when, how do you learn with multiple subjects?",
            "So if I scan five peoples brains, can I use information from one person's scans to help me when I'm learning say classifier for another person, so to give you an example of how this works, I just want to discuss this model A little bit.",
            "We see here if we have some data X. OK an I don't know if you can see this is actually.",
            "This isn't superscript S for subject.",
            "We have some.",
            "This is for index of data for subjects for feature J and then we have some example I and we imagine that each subject has some parameter for a feature that we're trying to learn and you can imagine collecting an examples for each subject.",
            "Anne.",
            "If we do this or say like 3 subjects, you know the brains between different people actually operate in a similar manner.",
            "So you can imagine that these parameters that we're trying to learn, Theta one Theta, two data, three or actually related somehow, and if we had an idea of how they're related, maybe that could help us.",
            "Actually, we're trying to get an estimate, maybe for subject 4, so in Subject 4 here we collect less samples.",
            "We don't collect the full end, but if we know that these parameters are related somehow, say.",
            "They are distributed from this or call hyper distribution, right?",
            "Then if we have knowledge of the parameters of this hyper distribution then we can use that to influence our estimate for this data and that can allow us to get a better estimate with less data over here and one of the ways that people.",
            "Estimate these hyperparameters is to use what's called like Empirical Bayes.",
            "And there you basically take point estimates of.",
            "Your estimates of these parameters.",
            "This is basically just taking a mean of means, and similarly for the variance."
        ],
        [
            "So.",
            "In a hierarchical model, it's kind of intuitive what we're doing, what we're looking for is a map estimate an we're trying to basically say what's the best estimate of a parameter, Theta.",
            "Given not only the data that we have, but also the hyperparameters.",
            "And if we compute the map estimate, we actually get something that's pretty intuitive.",
            "Normally we would just get.",
            "We would just have a sample mean and what the map estimate is.",
            "It's actually a weighted average between the sample mean an our prior belief are hyper mean and the weights.",
            "Actually here are given by the precisions and the precision is just the inverse of the variances and what's really happening here.",
            "So if we have very a small number of samples.",
            "The variance here is going to be actually very high.",
            "OK, and that's going to put very little weight on the sample mean.",
            "We're going to put more weight on this hyper mean, but as we collect more and more samples will put more weight here on the sample mean and we'll kind of ignore the hyper mean here.",
            "An one so inderstand it used this method to build a Gaussian naive Bayes classifier using these hierarchical parameter estimates, and it was a nice piece of work for a combined."
        ],
        [
            "The multiple subjects.",
            "OK, so.",
            "What this image is here, do you imagine?",
            "A 3 dimensional image.",
            "This is 1 two dimensional slice of that 3 dimensional image and then each of these little boxes here are going to be voxels and if we zoom in this is a picture of the visual cortex.",
            "What we actually have here is this is basically the time series of 1 individual voxel OK and what we have here is the neural activation that we're measuring overtime.",
            "OK, so.",
            "Whereas in Ender's Work Anderson his work, he was trying to combine data from multiple subjects and where the data is related.",
            "But if we actually look at this Norma generating data, we see that there are many regions, many actual voxels that are related and you see like these neighborhoods where there's very very strong correlation between features.",
            "So we're trying to figure out a way to apply these hierarchical Bayesian methods to the process of estimating features for single subject.",
            "And then there's some related work from nicolescu that basically showed that when he assumed that there was this linear relationship between between voxels, he could learn he could."
        ],
        [
            "Learn better models so.",
            "One of the problems with hierarchical Bayesian models that you assume, at least in the normal model that we had before, that these parameters related by this common normal distribution so.",
            "If you imagine in this image here, we have, say, two fMRI signals.",
            "Now these signals, the red and the blue are actually perfectly correlated.",
            "OK, so if we knew one that could, probably we want to, it could tell us something about the other, but.",
            "If we end up estimating our hyperparameters in the usual way, like taking a mean of means that would end up basically drawing, we would estimate our hyperparameter be here and that would end up making both of these estimates look worse, so.",
            "What we're going to do is we're going to transform.",
            "These signals, first, we're going to try and find a linear mapping between this signal through the other one first, and then we're going to calculate.",
            "High parameters and I'll step through."
        ],
        [
            "That in just a second here so.",
            "We developed this feature sharing classifier an.",
            "Basically, you can imagine doing a normal naive Bayes an.",
            "We're going to get estimates of our parameters in with just a few steps, so assume that you have some.",
            "You have some training examples, in particular voxel, and the first thing we're going to do is we're just going to calculate sample means.",
            "We're going to calculate the average over those examples.",
            "And then we're going to repeat that for every single voxel.",
            "So in our data, voxel at a particular time point is actually a feature, and so compute our sample means for each feature.",
            "And then what we're going to do is we're going to perform a regression from.",
            "So we're interested in calculating.",
            "The parameters for this particular voxel here, and what we're going to do is we're going to try and predict this voxel using its neighbors, so we're going to perform a linear regression just between a voxel and each of its neighbors.",
            "And what that's going to do is it's going to give us some scaling.",
            "Constant beta.",
            "OK, that Maps some neighbor V prime.",
            "TV.",
            "OK."
        ],
        [
            "So.",
            "What we're essentially doing is we're trying if we have these two signals, we're trying to learn the best beta.",
            "The best scaling of this signal here to the other signal in blue.",
            "Right, so we do that.",
            "For each of the voxels neighbors OK and then what we then do is using those those scaled data.",
            "We then take the mean over that.",
            "So for each neighbor we're basically taking the mean of that were then using that to calculate our hyperparameters question.",
            "This this data actually here is a simulation that I did just to try to make it a little bit more intuitive, but question.",
            "Trying to find trying to reconstruct the center from.",
            "I'm calculating independently, so each from each individual neighbor.",
            "I'm trying to find that best beta, so each neighbor is going to have a beta.",
            "OK, so.",
            "Once we compute our hyperparameters we then this is the equation we just saw before and we just take those high parameters we plug in and we get a smooth smooth estimate of a parameter.",
            "At a voxel at a particular time point, that's what this vieti means.",
            "For some class K. So we do that for every single feature for each class that we're trying to compute.",
            "And that's how we."
        ],
        [
            "So that's how we build our classifier so.",
            "We use this to in an fMRI classification task an.",
            "The task that we're trying to classify here is, you know, is a subject reading a sentence, or are they viewing a picture?",
            "And here are the images.",
            "Associated that so.",
            "In this experiment we have 5000 voxels at 16 time points, so that gives us 80,000 features and we're only going to use two training examples per class.",
            "Yes, yeah, so right now we're going to show you some results that are four across 13 different subjects."
        ],
        [
            "That's what this is, but we train each subject independently, so we're not actually considering any other subjects data when we're trying to estimate for single subject.",
            "No, it's it's the same session, so you imagine like.",
            "Imagine just repeat like you make them view a picture and then you do that.",
            "You do that twice.",
            "OK, so in these results here a standard naive Bayes classifier would give you basically this blue line.",
            "Here is this random accuracy and we get pretty much random accuracy for.",
            "All 13 subjects that we tried in this experiment, where is the feature sharing classifier which is red gives you.",
            "You know a pretty big boost.",
            "Across all 13."
        ],
        [
            "13 subjects so.",
            "In this experiment here what we did is we know that the visual cortex actually contains a lot of information for this task.",
            "So what we did is we just looked at the voxels in the visual cortex and we threw out, say, the 95% of the other box.",
            "Also, there's three about 300 voxels, 16 time points that gives us 5000 features.",
            "And what's interesting here is that.",
            "The standard Gaussian naive Bayes classifier, even though we're throwing away a lot of noisy features, it still can't actually.",
            "It still can't classify and it still gives you random accuracy, whereas in this feature sharing classifier that we've developed in 2/3 of the subjects, we get very large improvements.",
            "You know about 20% or more."
        ],
        [
            "OK, so some of the takeaway points from this section, so we developed this empirical Bayes classifier.",
            "Some modification in the standard Bayesian model hierarchical Bayesian model.",
            "We learn our parameters through these related features.",
            "It's computationally inefficient and it's useful when you have a large number of features.",
            "Small number of examples, and you have some knowledge of the relationships between features ahead of time, and there's a paper that if you're interested.",
            "We've generalized this to high dimensional problems."
        ],
        [
            "And you can see that OK.",
            "So challenge 2.",
            "How can we actually find like good features from fMRI data?",
            "So we've talked about how to build a classifier where we don't need to do feature selection, but still useful to think about.",
            "How can we actually pick."
        ],
        [
            "Good fixtures out of the raw data so.",
            "There are lots of ways that people have been doing this, an usually when you talk about feature selection often you'll see this terms embedded filter and rapper and typically feature selection methods can be grouped in only, so if you imagine doing some sort of logistic regression with L1 regularization to some sort of sparsity constraint, that's it.",
            "That's an example of an embedded method where you're actually combining the feature selection part is in the optimization itself.",
            "An another example that is the support vector decomposition machine from Francisco Pereira, an Jeff Gordon an.",
            "So filter methods you imagine it doesn't depend what the learner is, so you can imagine just taking your data, averaging overtime, averaging over space, maybe running some sort of PCA.",
            "So these are all independent, and then a wrapper method basically uses some sort of induction method.",
            "An typical example that is doing some sort of discriminative feature selection where you would train, take some validation data.",
            "You train a classifier to see, like which features discriminate well, and then.",
            "Pick some high performing.",
            "Features an throw the rest away so."
        ],
        [
            "So we did is.",
            "Imagine that task that classification tasks were talking bout before where we had 80,000 features.",
            "Imagine taking each feature and training it on our trading a classifier with an individual feature.",
            "So we have 80,000 classifiers at this point, and then we're going to evaluate those classifiers on some validation data.",
            "OK, so if we do that and then we plot a histogram of the accuracies of these classifiers, we get this curve that looks like this.",
            "And we have 40 examples here that we're testing, so 50% accuracy would be about 20%.",
            "OK, so when we looked at this data?",
            "We notice something interesting.",
            "We notice that there were classifiers that would report that they had like 5% accuracy and 10% accuracy, and we ask the question alright.",
            "Well, when it why would a classifier get just 5% accuracy if it's not?",
            "If the feature is noisy?",
            "Ann, you're not learning anything.",
            "You'd think that you would get maybe just 50% accuracy, right?",
            "So what we realize is that when you have such a large number.",
            "A feature some of them are actually just performing.",
            "Poorly just by random chance, right?",
            "'cause you have a very very small number of labels.",
            "So similarly you might get some that are performing very very well.",
            "Maybe like 80 percent, 90% just by random chance.",
            "So we ask the question, you know if we have a whole bunch of classifiers out putting labels randomly, you know how well.",
            "Could the best do?"
        ],
        [
            "And we can state that formally, and we can say, given M classifiers, that each produce labels randomly.",
            "For an examples, what is the expected accuracy?",
            "Of the best one."
        ],
        [
            "And we went and we used order statistics actually to go and calculate.",
            "That expression an we're able to drive this theorem here, which is called the highest chance accuracy an.",
            "It's actually pretty simple and it just uses this incomplete beta function here, which you can implement in one line of Matlab.",
            "And basically if you take the number of examples N, the number of classifiers M. You just plug in.",
            "Anne.",
            "You can."
        ],
        [
            "And accuracy and let me show you an example of how this works.",
            "So you consider in an office football pool when you have 200 people.",
            "Betting at random on the outcome of 20 games, so assume that no one year knowing your office actually knows anything about football and they're basically just flipping coins.",
            "To figure out what the outcome is going to be, someone is going to win the football pool and how well would we expect that.",
            "That winners perform an in this particular case, the accuracy of the best particular participant is going to be 80%, which is much higher than you would intuitively think.",
            "Think it might be.",
            "So what's interesting here is that.",
            "You know 1 / 2 to the 20th right is basically one in.",
            "About 1,000,000 so we'd expect that we need about a million participants before we get one that has a perfect labeling.",
            "An I take out point here is going to be the chance of obtaining a very good labeling.",
            "It's going to be very high even if the chance of obtaining a perfect labeling is very low."
        ],
        [
            "So if we take if we look at a plot of that function that I just showed you, you can see what's going on, and we expect the accuracy to be 50% for each individual classifier, but.",
            "When we have a collection of classifiers, someone is going to perform pretty well.",
            "So if you imagine if I have just 100 classifiers.",
            "Betting on 10 examples, you know, just by random chance you're going to get someone that gets.",
            "Over 85% accuracy.",
            "And as we increase the number of examples, we see that this gap here closes and we get closer to the 50% accuracy."
        ],
        [
            "OK, so.",
            "If we model the number of errors that each classifier makes is a binomial where the chance of making an error here say .5.",
            "For example coin flip.",
            "We just get a bunch of random samples from binomial and we consider the classifier that has the smallest number of errors and we use this notation here.",
            "One to indicate that it's just the min.",
            "Over these variables.",
            "OK, so we define something here called the multiplicity gap, which is.",
            "Basically this is the expected value of.",
            "An individual classifier.",
            "The number of errors individual classifier would make, and that's just the normal expectation of a binomial.",
            "The number of examples times the probability of the error, and then what's interesting is that this men here.",
            "This is also a random variable, so we can talk about taking the expectation of that.",
            "OK, so that's the expected minimum number of errors an we define this multiplicity gap in terms of the number of examples and a number of classifiers M as just the difference between these two values."
        ],
        [
            "OK, so.",
            "We asked you know, how do we use this?",
            "Can we use this as a feature selection method and?",
            "One thing that we feel is that this."
        ],
        [
            "This value here in some sense gives you this kind of natural feature selection threshold because.",
            "It only depends on the number of examples and number of classifiers.",
            "We wouldn't expect anyone if they're just flipping flipping coins randomly to actually perform better than that."
        ],
        [
            "So.",
            "We would conjecture that.",
            "If we kind of look at all possible.",
            "Discriminative thresholds that the optimal threshold is going to fall within this multiplicity gap right so in this graph.",
            "Here what we're doing is.",
            "We're doing this fMRI classification task.",
            "We're using a validation set of say, 20 examples, and we're allowing.",
            "This is the number of errors that were allowing, so 10 would be, say, 50% OK and.",
            "This line here is just the expected value of 1 classifier of an individual classifier and this is the expected value of the min right?",
            "So this gives us this gap, so we actually and then what we did here is we tested the test accuracy for every possible threshold here.",
            "So 12345 mistakes or so on.",
            "Ann, this is the plot we saw, so we actually did this for 13 different subjects and we also evaluated a microarray data set that where the goal was to predict whether or not you had cancer.",
            "If the patient was going to cancel or not an we found that.",
            "That was true in all 14 of those experiments.",
            "Yeah, So what happens?",
            "Yeah, there's 20 training examples and then there's 20 validation examples in that set."
        ],
        [
            "OK, so.",
            "We have some experimental results of trying to use this expected value as a feature selection threshold, but there's an assumption here that doesn't exactly hold in real problems, so one is that all these features are independent and also that all the features are noisy and then a sparse high dimensional problem that's not going to be.",
            "That's not going to be true, right?",
            "But it does kind of give us this useful upper bound.",
            "So what we did is we evaluated.",
            "Um?",
            "Two experiments, here's this cancer data set where we had 2000 features and 60 examples.",
            "This is using its classification using microarrays and then this is the fMRI task that I was telling you about earlier, and so we tested 5 methods here, so this blue over here is no feature selection.",
            "OK, this light blue here is just this very strict threshold that we just talked about.",
            "OK, so we get a little boost here and then.",
            "This green is.",
            "Kind of a standard method.",
            "What you might use in statistics, right?",
            "So it's a binomial hypothesis test using a false discovery rate correction, and we're using a significance level of Alpha equals 0.5.",
            "And this orange line here is a relaxation of the above method, which we call the multiplicity gap midpoint.",
            "So if you look."
        ],
        [
            "Back here as our heuristic.",
            "Or just taking the midpoint between these two extremes that we calculate."
        ],
        [
            "OK, so and then the yellow line is actually the Oracle Eric accuracy.",
            "So some Oracle told us what the optimal threshold would have been.",
            "This is the highest test accuracy we could have gotten.",
            "An I one thing actually worth noting here is that.",
            "This false discovery rate method.",
            "In this friv task we had 80,000 features and one thing we found is that.",
            "These methods, the standard you know.",
            "Bonferroni, as well as false discovery rate corrections are actually two conservative when you have very, very high numbers of examples and you have I'm sorry, numbers of features and with just a very small number of examples."
        ],
        [
            "OK, so takeaway here is.",
            "We've.",
            "We've developed this highest chance accuracy theorem that gives this kind of intuitive accuracy number.",
            "And we found that the classical hypothesis tests with these corrections are too conservative.",
            "We developed this midpoint heuristic that we found is actually useful for very sparse high dimensional problems, and it's very simple to implement in one line of Matlab.",
            "And there's a paper at this years ICL if you're interested."
        ],
        [
            "OK, so.",
            "Challenge 3 is.",
            "A generative theory of neural activation, so we spent the last two sections talking about classification.",
            "So given that I have some.",
            "Normal image you know can we tell you what the cognitive state is?",
            "But now we want to actually talk about a generative model.",
            "So if I'm thinking.",
            "About the word act, Apple, can I actually produce a model to predict?",
            "Specifically, what my neural activation is going to be.",
            "Anne.",
            "So the question we're trying to answer here is can we predict the neural activation of any word in English?",
            "From just a very small number of training examples."
        ],
        [
            "Anne.",
            "We're going to do this, and we're going to take an idea.",
            "Actually from computational linguistics and the idea that we're going to use here is that the meaning of a word can actually be captured by its Co occurrence statistics in a large text corpus.",
            "So to give you an example of that, if I see in a very large body of text that the word Apple Co.",
            "Occurs with the word eat.",
            "And I see that the word Orange Co occurs with a lot with the word eat.",
            "Then I might think that alright.",
            "Well, maybe Apple and oranges are somehow semantically related.",
            "So what we're going to do is we're actually going to compute these statistical cooccurrence features, right?",
            "So we're going to represent each word in English by.",
            "Some statistical features from a large body of text try to capture its meaning, and then we're going to learn a mapping from those features to some fMRI data with a small number of."
        ],
        [
            "Samples OK so.",
            "We're going to borrow another idea from psychology, and that idea is that the brain represents meaning of words in sensory motor areas of the cortex, so.",
            "What we're going to do on our first model is we're going to try to encode each word using the Co occurrence statistics with 25 sensory action words, and these are the words we see.",
            "So we have eat here.",
            "In the example I just talked about, will have words like ride drive.",
            "And what each feature value is, it's going to be the frequency that the word Co occurs in this trillion word corpus, and I don't know how many people have seen this, but Google actually released this.",
            "They went over their entire repository web pages and produced this trillion word text corpus, and you can compute very interesting statistics from that."
        ],
        [
            "So to give you an example.",
            "If we look in this corpus of texts and we look, we look for the word celery.",
            "We see what words the celery Co occur a lot with.",
            "We see that Co occurs.",
            "What you might expect goes with eat and taste, but you don't really ride celery.",
            "So like you.",
            "You see very basically zero wait here, but in the case of an airplane, right?",
            "You do write an airplane, right?",
            "So you see that Co occurs quite a lot.",
            "With that you could see an airplane.",
            "So essentially what our training data is going to be is so for the word celery, we're going to record a persons neural image here.",
            "An one thing to keep in mind is that in this in this model I'm going to talk about, we've removed the temporal dimension.",
            "We've averaged overtime, so we're just considering a spatial image, OK?",
            "So we have an F MRI image and then we also have this vector of Co occurrences.",
            "OK, Ann."
        ],
        [
            "In our model.",
            "The first thing we tried was just basically a simple linear model where we're modeling this Y.",
            "Here is the fMRI output for a particular voxel.",
            "OK, this is just a standard linear model where this X is going to be our data matrix of text Co occurrences, so that mentions that we have, say N training examples, and then we picked out 25 words OK, and then we have error.",
            "And then what we're trying to do is we're trying to learn await a beta, wait for each voxel over those 25 features.",
            "An if we actually look at.",
            "If we look at each voxel gets a beta tone beta and if we look at, say, the betas for the word eat.",
            "That's what this image is.",
            "So what we're doing is we're actually.",
            "Combining linearly.",
            "To get a final output image here."
        ],
        [
            "OK.",
            "So.",
            "Here's an example of how we do this.",
            "What we did is we have 60 training words an we trained on 58 of them and we left out celery in airplane.",
            "And what we did is we looked at.",
            "We learn the model and then we took the cooccurrence accounts for seller in airplane, pumped into the model, and then saw the image that we get.",
            "And this is the predicted image.",
            "And this is actually what the true images that we observed.",
            "And you see that.",
            "It captures a lot of that an airplane here in particular.",
            "You see that so red here means that there's a very strong activation.",
            "We see that the model actually pulls out.",
            "A lot of that really high activating area.",
            "OK, so."
        ],
        [
            "To evaluate this theory, what we did is we ran a leave to cross validation out and.",
            "So 60 choose two.",
            "We basically get 17170 test pairs.",
            "Random guessing is going to give us 50% accuracy.",
            "Using a permutation test, we found that .62 is significant.",
            "And we tested this independently over 9 subjects, and these are the accuracies that we got.",
            "The mean accuracy .77 and when we did this, we actually we didn't look at the whole brain.",
            "We looked at just 500 of the most stable voxels."
        ],
        [
            "An so.",
            "One question I want to ask is.",
            "You know, we kind of picked these 25 words.",
            "These 25 sensory motor words based on the psychological literature and.",
            "We'd like to find out can we actually train a model to automatically determine a good basis of words?",
            "So how do we know that the 25 is is?"
        ],
        [
            "It or not, so one way we've been doing this is using like regularize neural networks an what we're doing here is.",
            "Imagine the input layer to this neural network is basically those cooccurrence counts, but instead of actually using 25 features, we actually have data for all 50,000 words in English, so our input.",
            "To this thing to this network is basically 50,000 input units.",
            "And then the output image is going to be our F MRI activation.",
            "So in this case we have 20,000 outputs an we tested models using various numbers hidden units.",
            "What I'm going to do here is just a model trained with five hidden units.",
            "And what we can do is we can look at the learned weights.",
            "Of the inputs as well as the outputs, so the inputs were putting high weight on.",
            "In particular words, so the models regularising sense that we put a prior to basically kind of make all the weights be close to 0, and then we want to see what what words actually have very strongly weight that we learn.",
            "So if we do that, this is the output.",
            "Wait for the first.",
            "These are three of the hidden units OK, and we see here will see activation in this area and it seems to be there's some sort of religious theme going on here we.",
            "Find that puts high weight on Catholic Baptist Christ White in this second hidden unit.",
            "Here we see activation this other region and there seems to be some sort of food theme.",
            "Chopped up carrots but, and this is actually.",
            "The highest highest overall weight.",
            "And.",
            "In this unit here.",
            "It's not clear actually, what if there's any sort of.",
            "Pattern coming out here you'll see foot and fetish in the same same thing, but other than that it's hard to infer.",
            "What the category is."
        ],
        [
            "Here so.",
            "Some of the future work that we're looking at right now is how to automatically discover semantic basis.",
            "So there are a number of methods in using simultaneous sparse approximation.",
            "There's something called the simultaneous lasso.",
            "There's also a method called simultaneous orthogonal matching pursuit, and we're looking at these right now.",
            "An another study that we want to figure do later in the year is to try to figure out.",
            "How does your activation change when there are adjectives involved?",
            "So for example, you know how does the brain image change.",
            "When I have time considering a slow runner versus considering like a fast runner and we want to build a model that can actually trying to learn that difference."
        ],
        [
            "And make predictions.",
            "So some take out points for this section, right?",
            "We've built the first computational model that predicts fMRI image for previously unseen words.",
            "The model works by encoding words according to their Co occurrence statistics with 25 sensory motor words an we're trying to.",
            "Now learn.",
            "Learn models that will automatically discover the semantic basis if you're interested.",
            "This paper will be out very soon.",
            "And you can see that."
        ],
        [
            "OK, so.",
            "I want to give you kind of a global summary of the challenge here.",
            "So first we talked about this feature sharing classifier, which is useful for domains with very very high dimensional data and very very small numbers of examples.",
            "We've talked about this highest chance accuracy theorem and how to use that to do feature selection an.",
            "We just talked about this generative model of neural activation for nouns in English."
        ],
        [
            "So.",
            "That's all I have for you today, and I figured I'd take.",
            "Discovery rates.",
            "Yeah, very surprising.",
            "Did you try different tuning?",
            "The different alphas?",
            "Yeah so.",
            "Can you bring up a great point, right?",
            "So you could take you could tune right?",
            "And I could lower that Alpha threshold to make it less conservative.",
            "And I would admit more features and performance would go up.",
            "So the question is, how do I?",
            "How do I tune that Alpha right?",
            "I need to find run some sort of cross validation step, right?",
            "So I'm going to run cross validation.",
            "Then I could just test for a different threshold anyway, so.",
            "I guess what we're arguing here is that the midpoint heuristic gives you kind of a good default regardless of the dimensionality of the problem.",
            "If that makes that makes sense.",
            "But yeah, you're totally right.",
            "You could.",
            "You could just tune your parameter, find something that works.",
            "And the question is, how do you?",
            "How do you find a good one and not so 1A value?",
            "That might work really well for one problem.",
            "Given some dimensionality would then not be a good Alpha, then for a problem with a very different dimensionality or number of examples.",
            "For which model for the?",
            "I mean which classifier?",
            "So in the first section where we talked about the feature."
        ],
        [
            "Sharing that was using a.",
            "Modification to a Gaussian I base.",
            "Where we use this hierarchal models to get our parameter estimates for.",
            "The classifier.",
            "Tailored to.",
            "What do you mean by that?",
            "Account.",
            "Data.",
            "Customization or what?",
            "Why did you choose that?",
            "Yeah, so if we can actually go back a little bit.",
            "When we've looked at, we have looked at a lot of this data."
        ],
        [
            "And.",
            "Yeah, basically we can see that we see when we look at the data and we plot images like this.",
            "We see these regions that have very high correlations where typically what you might see is some voxel and then the immediate region around it will have basically a lower amplitude.",
            "So when I was talking to Francisco Pereira about this at one point, and he's mentioned that data does also have phase shifts as well, but.",
            "Or not, we're not modeling that here.",
            "Can you estimate the model?",
            "Loop.",
            "No, actually we don't need to do that here.",
            "So you're talking about the hyperparameters or."
        ],
        [
            "Let's see wearing.",
            "Using all eight neighbors, yeah, so let's see here.",
            "Or essentially doing is we're using all of our spatial neighbors right to try and predict, so our neighbors are going to be basically very highly correlated versions of.",
            "Of what an interval boxell is doing.",
            "OK, so we use our neighbors to try and predict.",
            "What?",
            "What is a neighbor?",
            "Think its value is going to be here so the neighbor is related according to some correlation, some linear map.",
            "And then we try to we get it.",
            "We basically let each neighbor predict what it thinks the value of this box is going to be.",
            "And then we use those predictions to estimate."
        ],
        [
            "These hyperparameters.",
            "And that's we're doing here, so we're taking this neighbor, right?",
            "So imagine this line.",
            "Here is a neighbor.",
            "And then we're scaling this guy according to that regression parameter that we learned.",
            "And then we're doing that for each neighbor and they were taking those scaled estimates and using that to calculate the hyperparameter and then the hyperparameters, then what?",
            "What we smooth we used to smooth in the map estimate."
        ],
        [
            "Any other questions?",
            "Is High Commissioner problems but happens if you directly feed all these features to SPM without any feature selection.",
            "So with two examples I mean I have I haven't run this personally with two examples, but pretty much guarantee with that I would highly bet that with two examples it's not going to work.",
            "Francisco well, with more than two, but it was.",
            "Not as good as selecting.",
            "Oh, that is.",
            "That's an excellent question.",
            "I have it written down an if I don't remember offhand, but if you I think it came from Princeton.",
            "So if you stop up afterwards, I can look at the paper where we have a reference to it and I can tell you which one it is.",
            "You had mentioned that at the end you were predicting the activation for unseen word.",
            "Can you actually use that to feed it back into a classifier and predict whether that's the one person talking about?",
            "So you're saying that we predict?",
            "We predicted image prediction accurate enough that you could use it as the training data for what you did in the first step.",
            "That's a good question.",
            "I'm not sure, actually, I'd imagine that.",
            "For certain words, maybe.",
            "To do that, but.",
            "I don't haven't tried it, but that's definitely good, good question.",
            "Andy.",
            "Think about using.",
            "In combination with feature selection, so once you've selected features that may be used for sharing, yes, yeah, that's that's a good.",
            "That's a good point.",
            "Yeah, there's no reason why you couldn't do that, and that would probably would probably help.",
            "'cause of the voxels that you so you throw away?",
            "A lot of them?",
            "And actually if you look at this."
        ],
        [
            "Second experiment here, that's in some sense what we're doing.",
            "Right, we're not.",
            "We're not using the feature selection method that we talked about in Section 2, but we are kind of using domain knowledge to throw away everything that's not in the visual cortex, and we saw that we did get that.",
            "That added boost here.",
            "So yeah, I can imagine you could do that and.",
            "It would probably be useful.",
            "Last section, how many boxes where you at?",
            "So.",
            "In the experimental results that we presented, there was five.",
            "There were 500."
        ],
        [
            "Where is it?",
            "Yeah, so here.",
            "We're doing it 500.",
            "We also trained the model actually with all the voxels and there were 20,000 and in that case the accuracy wasn't as good as it was here, but I think in.",
            "Four so at the subjects it was statistically significant in the highest, I think was maybe 77%.",
            "But for sharing, probably between people.",
            "Personally, since you need some image registration algorithm because yeah, yeah.",
            "Yeah, you bring up you bring up a great point.",
            "So in the work that I mentioned from introduced Andy.",
            "Typically what people do is they project the brain onto.",
            "There are these kind of Canonical brains right that have the same number of features ones called.",
            "I think Tyler Rackspace and there's another one called MINI Space and I think that Alarak spaces there's this very lovely French woman and her brain was kind of modeled for everyone else but.",
            "So yeah, so basically in each of those cases you project onto this common spatial dimension and you lose some.",
            "You definitely lose some accuracy.",
            "There was some data, but so that's where he was, what he was doing.",
            "Alright, well thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone.",
                    "label": 0
                },
                {
                    "sent": "Thanks a lot for coming to the machine learning lunch on Mark Palatucci and so today I'm going to talk about learning patterns of the brain.",
                    "label": 1
                },
                {
                    "sent": "They spent some time talking about the brain or spend some time talking about functional magnetic resonance imaging, which is a new tool for studying the brain.",
                    "label": 0
                },
                {
                    "sent": "And it presents a lot of challenges for the machine learning and statistics.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Community, so I'm going to talk about some of those today, so a lot of the work I'm going to discuss is actually a collaboration with the neuro semantics research team.",
                    "label": 0
                },
                {
                    "sent": "Probably a lot of people you recognize a lot of these people are in the room, right?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "With magnetic resonance imaging, you're probably used to seeing this image here on the left, and that's called a structural magnetic resonance image.",
                    "label": 1
                },
                {
                    "sent": "And you're probably not as familiar, maybe with what's called functional MRI.",
                    "label": 0
                },
                {
                    "sent": "Now functional MRI is actually a way to study the underlying neural activity of a person, so you can imagine taking an MRI machine and taking a snapshot of a persons like neural activity, say like every second.",
                    "label": 1
                },
                {
                    "sent": "So in some sense you get like a movie of what's happening in the person's brain.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are the applications of magnetic resonance imaging?",
                    "label": 0
                },
                {
                    "sent": "So one is functional brain mapping an what you do here is basically trying to determine what parts of the brain respond to different types of stimuli.",
                    "label": 0
                },
                {
                    "sent": "So in this study from the Indiana University School of Medicine, they basically played, spoke.",
                    "label": 1
                },
                {
                    "sent": "They played a speech in a scanner and they found that men one side of the brain responds an in women two sides of the brain response.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Another application is called cognitive State classification and sometimes this is known and you've seen this in the newspapers things people call in mind reading.",
                    "label": 0
                },
                {
                    "sent": "But basically if we have some image of a person's neural activation, we want to know what is the person thinking about is that person you know, maybe thinking about food, sports, maybe music Britney?",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "If we think about each functional magnetic resonance images, it has about 15,000 voxels and wanna voxel is if you think about an image, a 2D image of pixels, it's picture element.",
                    "label": 0
                },
                {
                    "sent": "If you think about taking a 3 dimensional image where one little tiny cube that's called the volume element and we call it a voxel.",
                    "label": 0
                },
                {
                    "sent": "So we take one of those images roughly once a second an A typical experiment may last 30 to 60 seconds.",
                    "label": 0
                },
                {
                    "sent": "Repeat that several times, so at the end of the day we have hundreds of thousands of data points and we have very limited training data, and there's a lot of noise.",
                    "label": 1
                },
                {
                    "sent": "So if I repeat the experiment but do the same experiment with the same person in the same machine, I'm going to get a lot of variance on that data.",
                    "label": 1
                },
                {
                    "sent": "So how can we actually learn?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turn in this type of domain.",
                    "label": 0
                },
                {
                    "sent": "So first challenge, I want to talk about is how?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Build a cognitive state classifier.",
                    "label": 0
                },
                {
                    "sent": "So how these experiments work will usually take some participant and will put them into this.",
                    "label": 0
                },
                {
                    "sent": "fMRI scanner will show them various stimuli, could be pictures of things we could be, sound music, and then we record their record, an image of their other neural activity.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a kind of state classifier.",
                    "label": 1
                },
                {
                    "sent": "What we're trying to do is we're trying to learn this function that Maps neural images to the state of what they're doing.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we show people pictures of.",
                    "label": 0
                },
                {
                    "sent": "Of dwelling, say like buildings and houses, and then we show them pictures of tools like Hammers, screwdrivers, things like that, can we actually build a classifier to distinguish between those two?",
                    "label": 1
                },
                {
                    "sent": "Pat.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what other cognitive states can we classify an over the last like four or five years or so?",
                    "label": 1
                },
                {
                    "sent": "There's actually been a really large literature that's been generated for people trying to figure out, like what they can distinguish.",
                    "label": 0
                },
                {
                    "sent": "So one funding was a political affiliation, so we can determine just by showing you different pictures of political candidates.",
                    "label": 0
                },
                {
                    "sent": "If you're a Democrat or Republican, we can actually determine we can show you pictures of different individuals, and we can tell you like which one is your.",
                    "label": 0
                },
                {
                    "sent": "Your spouse, your girlfriend.",
                    "label": 0
                },
                {
                    "sent": "And more recently people live.",
                    "label": 0
                },
                {
                    "sent": "There are two companies actually that have started to try to use fMRI to do lie detection as a replacement for the polygraph.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one classifiers that people typically use here, so support vector machines, logistic regression if you do some sort of sparsity regularization, K nearest neighbor methods, naive Bayes, and all of these methods.",
                    "label": 0
                },
                {
                    "sent": "Actually the exception of, I guess, naive Bayes and.",
                    "label": 0
                },
                {
                    "sent": "The regularizer just aggression.",
                    "label": 0
                },
                {
                    "sent": "You have to do some sort of prior dimensionality reduction step.",
                    "label": 0
                },
                {
                    "sent": "So you have this very large time series with hundreds of thousands of features.",
                    "label": 0
                },
                {
                    "sent": "Usually people would average over space, they would average overtime to try and reduce that dimension.",
                    "label": 0
                },
                {
                    "sent": "So naive Bayes is quite popular 'cause it's very easy to implement.",
                    "label": 1
                },
                {
                    "sent": "It's robust and you don't actually need to.",
                    "label": 0
                },
                {
                    "sent": "Do this prior dimensionality reduction step and you can be successful on problems with only 40 examples.",
                    "label": 1
                },
                {
                    "sent": "But Emma running our experiments is expensive.",
                    "label": 0
                },
                {
                    "sent": "People, you know, it's uncomfortable for people to sit in these scanners for more than 20 minutes or so.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to use less data.",
                    "label": 0
                },
                {
                    "sent": "So how can we build a class?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our family had two examples.",
                    "label": 0
                },
                {
                    "sent": "An so when you have little data, you know one way you can actually learn is to use what's called like hierarchical Bayesian modeling.",
                    "label": 0
                },
                {
                    "sent": "An some of you might remember this from intermediate statistics.",
                    "label": 0
                },
                {
                    "sent": "You know there's a lot of people who took that here an.",
                    "label": 0
                },
                {
                    "sent": "There are some related work from injury Standi.",
                    "label": 0
                },
                {
                    "sent": "Applying this to fMRI and the problem that he was trying to address is when, how do you learn with multiple subjects?",
                    "label": 0
                },
                {
                    "sent": "So if I scan five peoples brains, can I use information from one person's scans to help me when I'm learning say classifier for another person, so to give you an example of how this works, I just want to discuss this model A little bit.",
                    "label": 0
                },
                {
                    "sent": "We see here if we have some data X. OK an I don't know if you can see this is actually.",
                    "label": 0
                },
                {
                    "sent": "This isn't superscript S for subject.",
                    "label": 0
                },
                {
                    "sent": "We have some.",
                    "label": 0
                },
                {
                    "sent": "This is for index of data for subjects for feature J and then we have some example I and we imagine that each subject has some parameter for a feature that we're trying to learn and you can imagine collecting an examples for each subject.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "If we do this or say like 3 subjects, you know the brains between different people actually operate in a similar manner.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine that these parameters that we're trying to learn, Theta one Theta, two data, three or actually related somehow, and if we had an idea of how they're related, maybe that could help us.",
                    "label": 0
                },
                {
                    "sent": "Actually, we're trying to get an estimate, maybe for subject 4, so in Subject 4 here we collect less samples.",
                    "label": 0
                },
                {
                    "sent": "We don't collect the full end, but if we know that these parameters are related somehow, say.",
                    "label": 0
                },
                {
                    "sent": "They are distributed from this or call hyper distribution, right?",
                    "label": 0
                },
                {
                    "sent": "Then if we have knowledge of the parameters of this hyper distribution then we can use that to influence our estimate for this data and that can allow us to get a better estimate with less data over here and one of the ways that people.",
                    "label": 0
                },
                {
                    "sent": "Estimate these hyperparameters is to use what's called like Empirical Bayes.",
                    "label": 0
                },
                {
                    "sent": "And there you basically take point estimates of.",
                    "label": 0
                },
                {
                    "sent": "Your estimates of these parameters.",
                    "label": 0
                },
                {
                    "sent": "This is basically just taking a mean of means, and similarly for the variance.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In a hierarchical model, it's kind of intuitive what we're doing, what we're looking for is a map estimate an we're trying to basically say what's the best estimate of a parameter, Theta.",
                    "label": 0
                },
                {
                    "sent": "Given not only the data that we have, but also the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "And if we compute the map estimate, we actually get something that's pretty intuitive.",
                    "label": 0
                },
                {
                    "sent": "Normally we would just get.",
                    "label": 0
                },
                {
                    "sent": "We would just have a sample mean and what the map estimate is.",
                    "label": 0
                },
                {
                    "sent": "It's actually a weighted average between the sample mean an our prior belief are hyper mean and the weights.",
                    "label": 1
                },
                {
                    "sent": "Actually here are given by the precisions and the precision is just the inverse of the variances and what's really happening here.",
                    "label": 0
                },
                {
                    "sent": "So if we have very a small number of samples.",
                    "label": 0
                },
                {
                    "sent": "The variance here is going to be actually very high.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's going to put very little weight on the sample mean.",
                    "label": 0
                },
                {
                    "sent": "We're going to put more weight on this hyper mean, but as we collect more and more samples will put more weight here on the sample mean and we'll kind of ignore the hyper mean here.",
                    "label": 1
                },
                {
                    "sent": "An one so inderstand it used this method to build a Gaussian naive Bayes classifier using these hierarchical parameter estimates, and it was a nice piece of work for a combined.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The multiple subjects.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What this image is here, do you imagine?",
                    "label": 0
                },
                {
                    "sent": "A 3 dimensional image.",
                    "label": 0
                },
                {
                    "sent": "This is 1 two dimensional slice of that 3 dimensional image and then each of these little boxes here are going to be voxels and if we zoom in this is a picture of the visual cortex.",
                    "label": 0
                },
                {
                    "sent": "What we actually have here is this is basically the time series of 1 individual voxel OK and what we have here is the neural activation that we're measuring overtime.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Whereas in Ender's Work Anderson his work, he was trying to combine data from multiple subjects and where the data is related.",
                    "label": 0
                },
                {
                    "sent": "But if we actually look at this Norma generating data, we see that there are many regions, many actual voxels that are related and you see like these neighborhoods where there's very very strong correlation between features.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to figure out a way to apply these hierarchical Bayesian methods to the process of estimating features for single subject.",
                    "label": 0
                },
                {
                    "sent": "And then there's some related work from nicolescu that basically showed that when he assumed that there was this linear relationship between between voxels, he could learn he could.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learn better models so.",
                    "label": 0
                },
                {
                    "sent": "One of the problems with hierarchical Bayesian models that you assume, at least in the normal model that we had before, that these parameters related by this common normal distribution so.",
                    "label": 1
                },
                {
                    "sent": "If you imagine in this image here, we have, say, two fMRI signals.",
                    "label": 0
                },
                {
                    "sent": "Now these signals, the red and the blue are actually perfectly correlated.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we knew one that could, probably we want to, it could tell us something about the other, but.",
                    "label": 0
                },
                {
                    "sent": "If we end up estimating our hyperparameters in the usual way, like taking a mean of means that would end up basically drawing, we would estimate our hyperparameter be here and that would end up making both of these estimates look worse, so.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is we're going to transform.",
                    "label": 0
                },
                {
                    "sent": "These signals, first, we're going to try and find a linear mapping between this signal through the other one first, and then we're going to calculate.",
                    "label": 0
                },
                {
                    "sent": "High parameters and I'll step through.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That in just a second here so.",
                    "label": 0
                },
                {
                    "sent": "We developed this feature sharing classifier an.",
                    "label": 1
                },
                {
                    "sent": "Basically, you can imagine doing a normal naive Bayes an.",
                    "label": 0
                },
                {
                    "sent": "We're going to get estimates of our parameters in with just a few steps, so assume that you have some.",
                    "label": 0
                },
                {
                    "sent": "You have some training examples, in particular voxel, and the first thing we're going to do is we're just going to calculate sample means.",
                    "label": 1
                },
                {
                    "sent": "We're going to calculate the average over those examples.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to repeat that for every single voxel.",
                    "label": 0
                },
                {
                    "sent": "So in our data, voxel at a particular time point is actually a feature, and so compute our sample means for each feature.",
                    "label": 0
                },
                {
                    "sent": "And then what we're going to do is we're going to perform a regression from.",
                    "label": 0
                },
                {
                    "sent": "So we're interested in calculating.",
                    "label": 0
                },
                {
                    "sent": "The parameters for this particular voxel here, and what we're going to do is we're going to try and predict this voxel using its neighbors, so we're going to perform a linear regression just between a voxel and each of its neighbors.",
                    "label": 1
                },
                {
                    "sent": "And what that's going to do is it's going to give us some scaling.",
                    "label": 0
                },
                {
                    "sent": "Constant beta.",
                    "label": 0
                },
                {
                    "sent": "OK, that Maps some neighbor V prime.",
                    "label": 0
                },
                {
                    "sent": "TV.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we're essentially doing is we're trying if we have these two signals, we're trying to learn the best beta.",
                    "label": 0
                },
                {
                    "sent": "The best scaling of this signal here to the other signal in blue.",
                    "label": 0
                },
                {
                    "sent": "Right, so we do that.",
                    "label": 0
                },
                {
                    "sent": "For each of the voxels neighbors OK and then what we then do is using those those scaled data.",
                    "label": 0
                },
                {
                    "sent": "We then take the mean over that.",
                    "label": 0
                },
                {
                    "sent": "So for each neighbor we're basically taking the mean of that were then using that to calculate our hyperparameters question.",
                    "label": 0
                },
                {
                    "sent": "This this data actually here is a simulation that I did just to try to make it a little bit more intuitive, but question.",
                    "label": 0
                },
                {
                    "sent": "Trying to find trying to reconstruct the center from.",
                    "label": 0
                },
                {
                    "sent": "I'm calculating independently, so each from each individual neighbor.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to find that best beta, so each neighbor is going to have a beta.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Once we compute our hyperparameters we then this is the equation we just saw before and we just take those high parameters we plug in and we get a smooth smooth estimate of a parameter.",
                    "label": 0
                },
                {
                    "sent": "At a voxel at a particular time point, that's what this vieti means.",
                    "label": 0
                },
                {
                    "sent": "For some class K. So we do that for every single feature for each class that we're trying to compute.",
                    "label": 0
                },
                {
                    "sent": "And that's how we.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's how we build our classifier so.",
                    "label": 0
                },
                {
                    "sent": "We use this to in an fMRI classification task an.",
                    "label": 1
                },
                {
                    "sent": "The task that we're trying to classify here is, you know, is a subject reading a sentence, or are they viewing a picture?",
                    "label": 1
                },
                {
                    "sent": "And here are the images.",
                    "label": 0
                },
                {
                    "sent": "Associated that so.",
                    "label": 1
                },
                {
                    "sent": "In this experiment we have 5000 voxels at 16 time points, so that gives us 80,000 features and we're only going to use two training examples per class.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah, so right now we're going to show you some results that are four across 13 different subjects.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's what this is, but we train each subject independently, so we're not actually considering any other subjects data when we're trying to estimate for single subject.",
                    "label": 0
                },
                {
                    "sent": "No, it's it's the same session, so you imagine like.",
                    "label": 0
                },
                {
                    "sent": "Imagine just repeat like you make them view a picture and then you do that.",
                    "label": 0
                },
                {
                    "sent": "You do that twice.",
                    "label": 0
                },
                {
                    "sent": "OK, so in these results here a standard naive Bayes classifier would give you basically this blue line.",
                    "label": 0
                },
                {
                    "sent": "Here is this random accuracy and we get pretty much random accuracy for.",
                    "label": 0
                },
                {
                    "sent": "All 13 subjects that we tried in this experiment, where is the feature sharing classifier which is red gives you.",
                    "label": 0
                },
                {
                    "sent": "You know a pretty big boost.",
                    "label": 0
                },
                {
                    "sent": "Across all 13.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "13 subjects so.",
                    "label": 0
                },
                {
                    "sent": "In this experiment here what we did is we know that the visual cortex actually contains a lot of information for this task.",
                    "label": 1
                },
                {
                    "sent": "So what we did is we just looked at the voxels in the visual cortex and we threw out, say, the 95% of the other box.",
                    "label": 0
                },
                {
                    "sent": "Also, there's three about 300 voxels, 16 time points that gives us 5000 features.",
                    "label": 1
                },
                {
                    "sent": "And what's interesting here is that.",
                    "label": 0
                },
                {
                    "sent": "The standard Gaussian naive Bayes classifier, even though we're throwing away a lot of noisy features, it still can't actually.",
                    "label": 0
                },
                {
                    "sent": "It still can't classify and it still gives you random accuracy, whereas in this feature sharing classifier that we've developed in 2/3 of the subjects, we get very large improvements.",
                    "label": 0
                },
                {
                    "sent": "You know about 20% or more.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so some of the takeaway points from this section, so we developed this empirical Bayes classifier.",
                    "label": 0
                },
                {
                    "sent": "Some modification in the standard Bayesian model hierarchical Bayesian model.",
                    "label": 1
                },
                {
                    "sent": "We learn our parameters through these related features.",
                    "label": 0
                },
                {
                    "sent": "It's computationally inefficient and it's useful when you have a large number of features.",
                    "label": 1
                },
                {
                    "sent": "Small number of examples, and you have some knowledge of the relationships between features ahead of time, and there's a paper that if you're interested.",
                    "label": 1
                },
                {
                    "sent": "We've generalized this to high dimensional problems.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you can see that OK.",
                    "label": 0
                },
                {
                    "sent": "So challenge 2.",
                    "label": 0
                },
                {
                    "sent": "How can we actually find like good features from fMRI data?",
                    "label": 1
                },
                {
                    "sent": "So we've talked about how to build a classifier where we don't need to do feature selection, but still useful to think about.",
                    "label": 0
                },
                {
                    "sent": "How can we actually pick.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good fixtures out of the raw data so.",
                    "label": 0
                },
                {
                    "sent": "There are lots of ways that people have been doing this, an usually when you talk about feature selection often you'll see this terms embedded filter and rapper and typically feature selection methods can be grouped in only, so if you imagine doing some sort of logistic regression with L1 regularization to some sort of sparsity constraint, that's it.",
                    "label": 1
                },
                {
                    "sent": "That's an example of an embedded method where you're actually combining the feature selection part is in the optimization itself.",
                    "label": 0
                },
                {
                    "sent": "An another example that is the support vector decomposition machine from Francisco Pereira, an Jeff Gordon an.",
                    "label": 1
                },
                {
                    "sent": "So filter methods you imagine it doesn't depend what the learner is, so you can imagine just taking your data, averaging overtime, averaging over space, maybe running some sort of PCA.",
                    "label": 0
                },
                {
                    "sent": "So these are all independent, and then a wrapper method basically uses some sort of induction method.",
                    "label": 0
                },
                {
                    "sent": "An typical example that is doing some sort of discriminative feature selection where you would train, take some validation data.",
                    "label": 0
                },
                {
                    "sent": "You train a classifier to see, like which features discriminate well, and then.",
                    "label": 0
                },
                {
                    "sent": "Pick some high performing.",
                    "label": 0
                },
                {
                    "sent": "Features an throw the rest away so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we did is.",
                    "label": 0
                },
                {
                    "sent": "Imagine that task that classification tasks were talking bout before where we had 80,000 features.",
                    "label": 0
                },
                {
                    "sent": "Imagine taking each feature and training it on our trading a classifier with an individual feature.",
                    "label": 1
                },
                {
                    "sent": "So we have 80,000 classifiers at this point, and then we're going to evaluate those classifiers on some validation data.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we do that and then we plot a histogram of the accuracies of these classifiers, we get this curve that looks like this.",
                    "label": 0
                },
                {
                    "sent": "And we have 40 examples here that we're testing, so 50% accuracy would be about 20%.",
                    "label": 0
                },
                {
                    "sent": "OK, so when we looked at this data?",
                    "label": 0
                },
                {
                    "sent": "We notice something interesting.",
                    "label": 0
                },
                {
                    "sent": "We notice that there were classifiers that would report that they had like 5% accuracy and 10% accuracy, and we ask the question alright.",
                    "label": 0
                },
                {
                    "sent": "Well, when it why would a classifier get just 5% accuracy if it's not?",
                    "label": 1
                },
                {
                    "sent": "If the feature is noisy?",
                    "label": 0
                },
                {
                    "sent": "Ann, you're not learning anything.",
                    "label": 0
                },
                {
                    "sent": "You'd think that you would get maybe just 50% accuracy, right?",
                    "label": 0
                },
                {
                    "sent": "So what we realize is that when you have such a large number.",
                    "label": 0
                },
                {
                    "sent": "A feature some of them are actually just performing.",
                    "label": 0
                },
                {
                    "sent": "Poorly just by random chance, right?",
                    "label": 0
                },
                {
                    "sent": "'cause you have a very very small number of labels.",
                    "label": 0
                },
                {
                    "sent": "So similarly you might get some that are performing very very well.",
                    "label": 1
                },
                {
                    "sent": "Maybe like 80 percent, 90% just by random chance.",
                    "label": 0
                },
                {
                    "sent": "So we ask the question, you know if we have a whole bunch of classifiers out putting labels randomly, you know how well.",
                    "label": 0
                },
                {
                    "sent": "Could the best do?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can state that formally, and we can say, given M classifiers, that each produce labels randomly.",
                    "label": 1
                },
                {
                    "sent": "For an examples, what is the expected accuracy?",
                    "label": 0
                },
                {
                    "sent": "Of the best one.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we went and we used order statistics actually to go and calculate.",
                    "label": 0
                },
                {
                    "sent": "That expression an we're able to drive this theorem here, which is called the highest chance accuracy an.",
                    "label": 1
                },
                {
                    "sent": "It's actually pretty simple and it just uses this incomplete beta function here, which you can implement in one line of Matlab.",
                    "label": 0
                },
                {
                    "sent": "And basically if you take the number of examples N, the number of classifiers M. You just plug in.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And accuracy and let me show you an example of how this works.",
                    "label": 0
                },
                {
                    "sent": "So you consider in an office football pool when you have 200 people.",
                    "label": 0
                },
                {
                    "sent": "Betting at random on the outcome of 20 games, so assume that no one year knowing your office actually knows anything about football and they're basically just flipping coins.",
                    "label": 1
                },
                {
                    "sent": "To figure out what the outcome is going to be, someone is going to win the football pool and how well would we expect that.",
                    "label": 0
                },
                {
                    "sent": "That winners perform an in this particular case, the accuracy of the best particular participant is going to be 80%, which is much higher than you would intuitively think.",
                    "label": 0
                },
                {
                    "sent": "Think it might be.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting here is that.",
                    "label": 0
                },
                {
                    "sent": "You know 1 / 2 to the 20th right is basically one in.",
                    "label": 0
                },
                {
                    "sent": "About 1,000,000 so we'd expect that we need about a million participants before we get one that has a perfect labeling.",
                    "label": 0
                },
                {
                    "sent": "An I take out point here is going to be the chance of obtaining a very good labeling.",
                    "label": 1
                },
                {
                    "sent": "It's going to be very high even if the chance of obtaining a perfect labeling is very low.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we take if we look at a plot of that function that I just showed you, you can see what's going on, and we expect the accuracy to be 50% for each individual classifier, but.",
                    "label": 0
                },
                {
                    "sent": "When we have a collection of classifiers, someone is going to perform pretty well.",
                    "label": 0
                },
                {
                    "sent": "So if you imagine if I have just 100 classifiers.",
                    "label": 0
                },
                {
                    "sent": "Betting on 10 examples, you know, just by random chance you're going to get someone that gets.",
                    "label": 1
                },
                {
                    "sent": "Over 85% accuracy.",
                    "label": 0
                },
                {
                    "sent": "And as we increase the number of examples, we see that this gap here closes and we get closer to the 50% accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "If we model the number of errors that each classifier makes is a binomial where the chance of making an error here say .5.",
                    "label": 0
                },
                {
                    "sent": "For example coin flip.",
                    "label": 0
                },
                {
                    "sent": "We just get a bunch of random samples from binomial and we consider the classifier that has the smallest number of errors and we use this notation here.",
                    "label": 1
                },
                {
                    "sent": "One to indicate that it's just the min.",
                    "label": 0
                },
                {
                    "sent": "Over these variables.",
                    "label": 1
                },
                {
                    "sent": "OK, so we define something here called the multiplicity gap, which is.",
                    "label": 0
                },
                {
                    "sent": "Basically this is the expected value of.",
                    "label": 0
                },
                {
                    "sent": "An individual classifier.",
                    "label": 0
                },
                {
                    "sent": "The number of errors individual classifier would make, and that's just the normal expectation of a binomial.",
                    "label": 0
                },
                {
                    "sent": "The number of examples times the probability of the error, and then what's interesting is that this men here.",
                    "label": 0
                },
                {
                    "sent": "This is also a random variable, so we can talk about taking the expectation of that.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the expected minimum number of errors an we define this multiplicity gap in terms of the number of examples and a number of classifiers M as just the difference between these two values.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We asked you know, how do we use this?",
                    "label": 0
                },
                {
                    "sent": "Can we use this as a feature selection method and?",
                    "label": 1
                },
                {
                    "sent": "One thing that we feel is that this.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This value here in some sense gives you this kind of natural feature selection threshold because.",
                    "label": 1
                },
                {
                    "sent": "It only depends on the number of examples and number of classifiers.",
                    "label": 1
                },
                {
                    "sent": "We wouldn't expect anyone if they're just flipping flipping coins randomly to actually perform better than that.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We would conjecture that.",
                    "label": 0
                },
                {
                    "sent": "If we kind of look at all possible.",
                    "label": 0
                },
                {
                    "sent": "Discriminative thresholds that the optimal threshold is going to fall within this multiplicity gap right so in this graph.",
                    "label": 1
                },
                {
                    "sent": "Here what we're doing is.",
                    "label": 0
                },
                {
                    "sent": "We're doing this fMRI classification task.",
                    "label": 0
                },
                {
                    "sent": "We're using a validation set of say, 20 examples, and we're allowing.",
                    "label": 0
                },
                {
                    "sent": "This is the number of errors that were allowing, so 10 would be, say, 50% OK and.",
                    "label": 0
                },
                {
                    "sent": "This line here is just the expected value of 1 classifier of an individual classifier and this is the expected value of the min right?",
                    "label": 0
                },
                {
                    "sent": "So this gives us this gap, so we actually and then what we did here is we tested the test accuracy for every possible threshold here.",
                    "label": 0
                },
                {
                    "sent": "So 12345 mistakes or so on.",
                    "label": 0
                },
                {
                    "sent": "Ann, this is the plot we saw, so we actually did this for 13 different subjects and we also evaluated a microarray data set that where the goal was to predict whether or not you had cancer.",
                    "label": 0
                },
                {
                    "sent": "If the patient was going to cancel or not an we found that.",
                    "label": 0
                },
                {
                    "sent": "That was true in all 14 of those experiments.",
                    "label": 0
                },
                {
                    "sent": "Yeah, So what happens?",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's 20 training examples and then there's 20 validation examples in that set.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We have some experimental results of trying to use this expected value as a feature selection threshold, but there's an assumption here that doesn't exactly hold in real problems, so one is that all these features are independent and also that all the features are noisy and then a sparse high dimensional problem that's not going to be.",
                    "label": 1
                },
                {
                    "sent": "That's not going to be true, right?",
                    "label": 0
                },
                {
                    "sent": "But it does kind of give us this useful upper bound.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we evaluated.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Two experiments, here's this cancer data set where we had 2000 features and 60 examples.",
                    "label": 0
                },
                {
                    "sent": "This is using its classification using microarrays and then this is the fMRI task that I was telling you about earlier, and so we tested 5 methods here, so this blue over here is no feature selection.",
                    "label": 0
                },
                {
                    "sent": "OK, this light blue here is just this very strict threshold that we just talked about.",
                    "label": 0
                },
                {
                    "sent": "OK, so we get a little boost here and then.",
                    "label": 0
                },
                {
                    "sent": "This green is.",
                    "label": 0
                },
                {
                    "sent": "Kind of a standard method.",
                    "label": 0
                },
                {
                    "sent": "What you might use in statistics, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a binomial hypothesis test using a false discovery rate correction, and we're using a significance level of Alpha equals 0.5.",
                    "label": 1
                },
                {
                    "sent": "And this orange line here is a relaxation of the above method, which we call the multiplicity gap midpoint.",
                    "label": 0
                },
                {
                    "sent": "So if you look.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back here as our heuristic.",
                    "label": 0
                },
                {
                    "sent": "Or just taking the midpoint between these two extremes that we calculate.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so and then the yellow line is actually the Oracle Eric accuracy.",
                    "label": 0
                },
                {
                    "sent": "So some Oracle told us what the optimal threshold would have been.",
                    "label": 0
                },
                {
                    "sent": "This is the highest test accuracy we could have gotten.",
                    "label": 0
                },
                {
                    "sent": "An I one thing actually worth noting here is that.",
                    "label": 0
                },
                {
                    "sent": "This false discovery rate method.",
                    "label": 0
                },
                {
                    "sent": "In this friv task we had 80,000 features and one thing we found is that.",
                    "label": 0
                },
                {
                    "sent": "These methods, the standard you know.",
                    "label": 0
                },
                {
                    "sent": "Bonferroni, as well as false discovery rate corrections are actually two conservative when you have very, very high numbers of examples and you have I'm sorry, numbers of features and with just a very small number of examples.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so takeaway here is.",
                    "label": 0
                },
                {
                    "sent": "We've.",
                    "label": 0
                },
                {
                    "sent": "We've developed this highest chance accuracy theorem that gives this kind of intuitive accuracy number.",
                    "label": 1
                },
                {
                    "sent": "And we found that the classical hypothesis tests with these corrections are too conservative.",
                    "label": 1
                },
                {
                    "sent": "We developed this midpoint heuristic that we found is actually useful for very sparse high dimensional problems, and it's very simple to implement in one line of Matlab.",
                    "label": 1
                },
                {
                    "sent": "And there's a paper at this years ICL if you're interested.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Challenge 3 is.",
                    "label": 0
                },
                {
                    "sent": "A generative theory of neural activation, so we spent the last two sections talking about classification.",
                    "label": 1
                },
                {
                    "sent": "So given that I have some.",
                    "label": 0
                },
                {
                    "sent": "Normal image you know can we tell you what the cognitive state is?",
                    "label": 0
                },
                {
                    "sent": "But now we want to actually talk about a generative model.",
                    "label": 0
                },
                {
                    "sent": "So if I'm thinking.",
                    "label": 0
                },
                {
                    "sent": "About the word act, Apple, can I actually produce a model to predict?",
                    "label": 0
                },
                {
                    "sent": "Specifically, what my neural activation is going to be.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So the question we're trying to answer here is can we predict the neural activation of any word in English?",
                    "label": 1
                },
                {
                    "sent": "From just a very small number of training examples.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We're going to do this, and we're going to take an idea.",
                    "label": 0
                },
                {
                    "sent": "Actually from computational linguistics and the idea that we're going to use here is that the meaning of a word can actually be captured by its Co occurrence statistics in a large text corpus.",
                    "label": 1
                },
                {
                    "sent": "So to give you an example of that, if I see in a very large body of text that the word Apple Co.",
                    "label": 0
                },
                {
                    "sent": "Occurs with the word eat.",
                    "label": 0
                },
                {
                    "sent": "And I see that the word Orange Co occurs with a lot with the word eat.",
                    "label": 0
                },
                {
                    "sent": "Then I might think that alright.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe Apple and oranges are somehow semantically related.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're actually going to compute these statistical cooccurrence features, right?",
                    "label": 0
                },
                {
                    "sent": "So we're going to represent each word in English by.",
                    "label": 0
                },
                {
                    "sent": "Some statistical features from a large body of text try to capture its meaning, and then we're going to learn a mapping from those features to some fMRI data with a small number of.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Samples OK so.",
                    "label": 0
                },
                {
                    "sent": "We're going to borrow another idea from psychology, and that idea is that the brain represents meaning of words in sensory motor areas of the cortex, so.",
                    "label": 1
                },
                {
                    "sent": "What we're going to do on our first model is we're going to try to encode each word using the Co occurrence statistics with 25 sensory action words, and these are the words we see.",
                    "label": 0
                },
                {
                    "sent": "So we have eat here.",
                    "label": 1
                },
                {
                    "sent": "In the example I just talked about, will have words like ride drive.",
                    "label": 0
                },
                {
                    "sent": "And what each feature value is, it's going to be the frequency that the word Co occurs in this trillion word corpus, and I don't know how many people have seen this, but Google actually released this.",
                    "label": 0
                },
                {
                    "sent": "They went over their entire repository web pages and produced this trillion word text corpus, and you can compute very interesting statistics from that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to give you an example.",
                    "label": 0
                },
                {
                    "sent": "If we look in this corpus of texts and we look, we look for the word celery.",
                    "label": 0
                },
                {
                    "sent": "We see what words the celery Co occur a lot with.",
                    "label": 0
                },
                {
                    "sent": "We see that Co occurs.",
                    "label": 0
                },
                {
                    "sent": "What you might expect goes with eat and taste, but you don't really ride celery.",
                    "label": 0
                },
                {
                    "sent": "So like you.",
                    "label": 0
                },
                {
                    "sent": "You see very basically zero wait here, but in the case of an airplane, right?",
                    "label": 0
                },
                {
                    "sent": "You do write an airplane, right?",
                    "label": 0
                },
                {
                    "sent": "So you see that Co occurs quite a lot.",
                    "label": 0
                },
                {
                    "sent": "With that you could see an airplane.",
                    "label": 0
                },
                {
                    "sent": "So essentially what our training data is going to be is so for the word celery, we're going to record a persons neural image here.",
                    "label": 0
                },
                {
                    "sent": "An one thing to keep in mind is that in this in this model I'm going to talk about, we've removed the temporal dimension.",
                    "label": 0
                },
                {
                    "sent": "We've averaged overtime, so we're just considering a spatial image, OK?",
                    "label": 0
                },
                {
                    "sent": "So we have an F MRI image and then we also have this vector of Co occurrences.",
                    "label": 0
                },
                {
                    "sent": "OK, Ann.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our model.",
                    "label": 0
                },
                {
                    "sent": "The first thing we tried was just basically a simple linear model where we're modeling this Y.",
                    "label": 0
                },
                {
                    "sent": "Here is the fMRI output for a particular voxel.",
                    "label": 0
                },
                {
                    "sent": "OK, this is just a standard linear model where this X is going to be our data matrix of text Co occurrences, so that mentions that we have, say N training examples, and then we picked out 25 words OK, and then we have error.",
                    "label": 1
                },
                {
                    "sent": "And then what we're trying to do is we're trying to learn await a beta, wait for each voxel over those 25 features.",
                    "label": 0
                },
                {
                    "sent": "An if we actually look at.",
                    "label": 0
                },
                {
                    "sent": "If we look at each voxel gets a beta tone beta and if we look at, say, the betas for the word eat.",
                    "label": 0
                },
                {
                    "sent": "That's what this image is.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is we're actually.",
                    "label": 0
                },
                {
                    "sent": "Combining linearly.",
                    "label": 0
                },
                {
                    "sent": "To get a final output image here.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of how we do this.",
                    "label": 0
                },
                {
                    "sent": "What we did is we have 60 training words an we trained on 58 of them and we left out celery in airplane.",
                    "label": 0
                },
                {
                    "sent": "And what we did is we looked at.",
                    "label": 0
                },
                {
                    "sent": "We learn the model and then we took the cooccurrence accounts for seller in airplane, pumped into the model, and then saw the image that we get.",
                    "label": 0
                },
                {
                    "sent": "And this is the predicted image.",
                    "label": 0
                },
                {
                    "sent": "And this is actually what the true images that we observed.",
                    "label": 0
                },
                {
                    "sent": "And you see that.",
                    "label": 0
                },
                {
                    "sent": "It captures a lot of that an airplane here in particular.",
                    "label": 0
                },
                {
                    "sent": "You see that so red here means that there's a very strong activation.",
                    "label": 0
                },
                {
                    "sent": "We see that the model actually pulls out.",
                    "label": 0
                },
                {
                    "sent": "A lot of that really high activating area.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To evaluate this theory, what we did is we ran a leave to cross validation out and.",
                    "label": 0
                },
                {
                    "sent": "So 60 choose two.",
                    "label": 0
                },
                {
                    "sent": "We basically get 17170 test pairs.",
                    "label": 1
                },
                {
                    "sent": "Random guessing is going to give us 50% accuracy.",
                    "label": 0
                },
                {
                    "sent": "Using a permutation test, we found that .62 is significant.",
                    "label": 1
                },
                {
                    "sent": "And we tested this independently over 9 subjects, and these are the accuracies that we got.",
                    "label": 0
                },
                {
                    "sent": "The mean accuracy .77 and when we did this, we actually we didn't look at the whole brain.",
                    "label": 0
                },
                {
                    "sent": "We looked at just 500 of the most stable voxels.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An so.",
                    "label": 0
                },
                {
                    "sent": "One question I want to ask is.",
                    "label": 0
                },
                {
                    "sent": "You know, we kind of picked these 25 words.",
                    "label": 0
                },
                {
                    "sent": "These 25 sensory motor words based on the psychological literature and.",
                    "label": 0
                },
                {
                    "sent": "We'd like to find out can we actually train a model to automatically determine a good basis of words?",
                    "label": 1
                },
                {
                    "sent": "So how do we know that the 25 is is?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It or not, so one way we've been doing this is using like regularize neural networks an what we're doing here is.",
                    "label": 0
                },
                {
                    "sent": "Imagine the input layer to this neural network is basically those cooccurrence counts, but instead of actually using 25 features, we actually have data for all 50,000 words in English, so our input.",
                    "label": 0
                },
                {
                    "sent": "To this thing to this network is basically 50,000 input units.",
                    "label": 0
                },
                {
                    "sent": "And then the output image is going to be our F MRI activation.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have 20,000 outputs an we tested models using various numbers hidden units.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do here is just a model trained with five hidden units.",
                    "label": 0
                },
                {
                    "sent": "And what we can do is we can look at the learned weights.",
                    "label": 0
                },
                {
                    "sent": "Of the inputs as well as the outputs, so the inputs were putting high weight on.",
                    "label": 0
                },
                {
                    "sent": "In particular words, so the models regularising sense that we put a prior to basically kind of make all the weights be close to 0, and then we want to see what what words actually have very strongly weight that we learn.",
                    "label": 0
                },
                {
                    "sent": "So if we do that, this is the output.",
                    "label": 0
                },
                {
                    "sent": "Wait for the first.",
                    "label": 0
                },
                {
                    "sent": "These are three of the hidden units OK, and we see here will see activation in this area and it seems to be there's some sort of religious theme going on here we.",
                    "label": 0
                },
                {
                    "sent": "Find that puts high weight on Catholic Baptist Christ White in this second hidden unit.",
                    "label": 1
                },
                {
                    "sent": "Here we see activation this other region and there seems to be some sort of food theme.",
                    "label": 0
                },
                {
                    "sent": "Chopped up carrots but, and this is actually.",
                    "label": 0
                },
                {
                    "sent": "The highest highest overall weight.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In this unit here.",
                    "label": 0
                },
                {
                    "sent": "It's not clear actually, what if there's any sort of.",
                    "label": 0
                },
                {
                    "sent": "Pattern coming out here you'll see foot and fetish in the same same thing, but other than that it's hard to infer.",
                    "label": 0
                },
                {
                    "sent": "What the category is.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here so.",
                    "label": 0
                },
                {
                    "sent": "Some of the future work that we're looking at right now is how to automatically discover semantic basis.",
                    "label": 1
                },
                {
                    "sent": "So there are a number of methods in using simultaneous sparse approximation.",
                    "label": 1
                },
                {
                    "sent": "There's something called the simultaneous lasso.",
                    "label": 0
                },
                {
                    "sent": "There's also a method called simultaneous orthogonal matching pursuit, and we're looking at these right now.",
                    "label": 1
                },
                {
                    "sent": "An another study that we want to figure do later in the year is to try to figure out.",
                    "label": 0
                },
                {
                    "sent": "How does your activation change when there are adjectives involved?",
                    "label": 1
                },
                {
                    "sent": "So for example, you know how does the brain image change.",
                    "label": 0
                },
                {
                    "sent": "When I have time considering a slow runner versus considering like a fast runner and we want to build a model that can actually trying to learn that difference.",
                    "label": 1
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And make predictions.",
                    "label": 0
                },
                {
                    "sent": "So some take out points for this section, right?",
                    "label": 0
                },
                {
                    "sent": "We've built the first computational model that predicts fMRI image for previously unseen words.",
                    "label": 1
                },
                {
                    "sent": "The model works by encoding words according to their Co occurrence statistics with 25 sensory motor words an we're trying to.",
                    "label": 1
                },
                {
                    "sent": "Now learn.",
                    "label": 0
                },
                {
                    "sent": "Learn models that will automatically discover the semantic basis if you're interested.",
                    "label": 0
                },
                {
                    "sent": "This paper will be out very soon.",
                    "label": 0
                },
                {
                    "sent": "And you can see that.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I want to give you kind of a global summary of the challenge here.",
                    "label": 0
                },
                {
                    "sent": "So first we talked about this feature sharing classifier, which is useful for domains with very very high dimensional data and very very small numbers of examples.",
                    "label": 1
                },
                {
                    "sent": "We've talked about this highest chance accuracy theorem and how to use that to do feature selection an.",
                    "label": 0
                },
                {
                    "sent": "We just talked about this generative model of neural activation for nouns in English.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's all I have for you today, and I figured I'd take.",
                    "label": 0
                },
                {
                    "sent": "Discovery rates.",
                    "label": 0
                },
                {
                    "sent": "Yeah, very surprising.",
                    "label": 0
                },
                {
                    "sent": "Did you try different tuning?",
                    "label": 0
                },
                {
                    "sent": "The different alphas?",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Can you bring up a great point, right?",
                    "label": 0
                },
                {
                    "sent": "So you could take you could tune right?",
                    "label": 0
                },
                {
                    "sent": "And I could lower that Alpha threshold to make it less conservative.",
                    "label": 0
                },
                {
                    "sent": "And I would admit more features and performance would go up.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how do I?",
                    "label": 0
                },
                {
                    "sent": "How do I tune that Alpha right?",
                    "label": 0
                },
                {
                    "sent": "I need to find run some sort of cross validation step, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to run cross validation.",
                    "label": 0
                },
                {
                    "sent": "Then I could just test for a different threshold anyway, so.",
                    "label": 0
                },
                {
                    "sent": "I guess what we're arguing here is that the midpoint heuristic gives you kind of a good default regardless of the dimensionality of the problem.",
                    "label": 0
                },
                {
                    "sent": "If that makes that makes sense.",
                    "label": 0
                },
                {
                    "sent": "But yeah, you're totally right.",
                    "label": 0
                },
                {
                    "sent": "You could.",
                    "label": 0
                },
                {
                    "sent": "You could just tune your parameter, find something that works.",
                    "label": 0
                },
                {
                    "sent": "And the question is, how do you?",
                    "label": 0
                },
                {
                    "sent": "How do you find a good one and not so 1A value?",
                    "label": 0
                },
                {
                    "sent": "That might work really well for one problem.",
                    "label": 0
                },
                {
                    "sent": "Given some dimensionality would then not be a good Alpha, then for a problem with a very different dimensionality or number of examples.",
                    "label": 0
                },
                {
                    "sent": "For which model for the?",
                    "label": 0
                },
                {
                    "sent": "I mean which classifier?",
                    "label": 0
                },
                {
                    "sent": "So in the first section where we talked about the feature.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sharing that was using a.",
                    "label": 0
                },
                {
                    "sent": "Modification to a Gaussian I base.",
                    "label": 0
                },
                {
                    "sent": "Where we use this hierarchal models to get our parameter estimates for.",
                    "label": 0
                },
                {
                    "sent": "The classifier.",
                    "label": 0
                },
                {
                    "sent": "Tailored to.",
                    "label": 0
                },
                {
                    "sent": "What do you mean by that?",
                    "label": 0
                },
                {
                    "sent": "Account.",
                    "label": 0
                },
                {
                    "sent": "Data.",
                    "label": 0
                },
                {
                    "sent": "Customization or what?",
                    "label": 0
                },
                {
                    "sent": "Why did you choose that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if we can actually go back a little bit.",
                    "label": 0
                },
                {
                    "sent": "When we've looked at, we have looked at a lot of this data.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, basically we can see that we see when we look at the data and we plot images like this.",
                    "label": 0
                },
                {
                    "sent": "We see these regions that have very high correlations where typically what you might see is some voxel and then the immediate region around it will have basically a lower amplitude.",
                    "label": 0
                },
                {
                    "sent": "So when I was talking to Francisco Pereira about this at one point, and he's mentioned that data does also have phase shifts as well, but.",
                    "label": 0
                },
                {
                    "sent": "Or not, we're not modeling that here.",
                    "label": 0
                },
                {
                    "sent": "Can you estimate the model?",
                    "label": 0
                },
                {
                    "sent": "Loop.",
                    "label": 0
                },
                {
                    "sent": "No, actually we don't need to do that here.",
                    "label": 0
                },
                {
                    "sent": "So you're talking about the hyperparameters or.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see wearing.",
                    "label": 0
                },
                {
                    "sent": "Using all eight neighbors, yeah, so let's see here.",
                    "label": 0
                },
                {
                    "sent": "Or essentially doing is we're using all of our spatial neighbors right to try and predict, so our neighbors are going to be basically very highly correlated versions of.",
                    "label": 0
                },
                {
                    "sent": "Of what an interval boxell is doing.",
                    "label": 0
                },
                {
                    "sent": "OK, so we use our neighbors to try and predict.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "What is a neighbor?",
                    "label": 0
                },
                {
                    "sent": "Think its value is going to be here so the neighbor is related according to some correlation, some linear map.",
                    "label": 0
                },
                {
                    "sent": "And then we try to we get it.",
                    "label": 0
                },
                {
                    "sent": "We basically let each neighbor predict what it thinks the value of this box is going to be.",
                    "label": 0
                },
                {
                    "sent": "And then we use those predictions to estimate.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "And that's we're doing here, so we're taking this neighbor, right?",
                    "label": 0
                },
                {
                    "sent": "So imagine this line.",
                    "label": 0
                },
                {
                    "sent": "Here is a neighbor.",
                    "label": 0
                },
                {
                    "sent": "And then we're scaling this guy according to that regression parameter that we learned.",
                    "label": 0
                },
                {
                    "sent": "And then we're doing that for each neighbor and they were taking those scaled estimates and using that to calculate the hyperparameter and then the hyperparameters, then what?",
                    "label": 0
                },
                {
                    "sent": "What we smooth we used to smooth in the map estimate.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Is High Commissioner problems but happens if you directly feed all these features to SPM without any feature selection.",
                    "label": 0
                },
                {
                    "sent": "So with two examples I mean I have I haven't run this personally with two examples, but pretty much guarantee with that I would highly bet that with two examples it's not going to work.",
                    "label": 0
                },
                {
                    "sent": "Francisco well, with more than two, but it was.",
                    "label": 0
                },
                {
                    "sent": "Not as good as selecting.",
                    "label": 0
                },
                {
                    "sent": "Oh, that is.",
                    "label": 0
                },
                {
                    "sent": "That's an excellent question.",
                    "label": 0
                },
                {
                    "sent": "I have it written down an if I don't remember offhand, but if you I think it came from Princeton.",
                    "label": 0
                },
                {
                    "sent": "So if you stop up afterwards, I can look at the paper where we have a reference to it and I can tell you which one it is.",
                    "label": 0
                },
                {
                    "sent": "You had mentioned that at the end you were predicting the activation for unseen word.",
                    "label": 0
                },
                {
                    "sent": "Can you actually use that to feed it back into a classifier and predict whether that's the one person talking about?",
                    "label": 0
                },
                {
                    "sent": "So you're saying that we predict?",
                    "label": 0
                },
                {
                    "sent": "We predicted image prediction accurate enough that you could use it as the training data for what you did in the first step.",
                    "label": 0
                },
                {
                    "sent": "That's a good question.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure, actually, I'd imagine that.",
                    "label": 0
                },
                {
                    "sent": "For certain words, maybe.",
                    "label": 0
                },
                {
                    "sent": "To do that, but.",
                    "label": 0
                },
                {
                    "sent": "I don't haven't tried it, but that's definitely good, good question.",
                    "label": 0
                },
                {
                    "sent": "Andy.",
                    "label": 0
                },
                {
                    "sent": "Think about using.",
                    "label": 0
                },
                {
                    "sent": "In combination with feature selection, so once you've selected features that may be used for sharing, yes, yeah, that's that's a good.",
                    "label": 0
                },
                {
                    "sent": "That's a good point.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's no reason why you couldn't do that, and that would probably would probably help.",
                    "label": 0
                },
                {
                    "sent": "'cause of the voxels that you so you throw away?",
                    "label": 0
                },
                {
                    "sent": "A lot of them?",
                    "label": 0
                },
                {
                    "sent": "And actually if you look at this.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second experiment here, that's in some sense what we're doing.",
                    "label": 0
                },
                {
                    "sent": "Right, we're not.",
                    "label": 0
                },
                {
                    "sent": "We're not using the feature selection method that we talked about in Section 2, but we are kind of using domain knowledge to throw away everything that's not in the visual cortex, and we saw that we did get that.",
                    "label": 0
                },
                {
                    "sent": "That added boost here.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I can imagine you could do that and.",
                    "label": 0
                },
                {
                    "sent": "It would probably be useful.",
                    "label": 0
                },
                {
                    "sent": "Last section, how many boxes where you at?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In the experimental results that we presented, there was five.",
                    "label": 0
                },
                {
                    "sent": "There were 500.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where is it?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so here.",
                    "label": 0
                },
                {
                    "sent": "We're doing it 500.",
                    "label": 0
                },
                {
                    "sent": "We also trained the model actually with all the voxels and there were 20,000 and in that case the accuracy wasn't as good as it was here, but I think in.",
                    "label": 0
                },
                {
                    "sent": "Four so at the subjects it was statistically significant in the highest, I think was maybe 77%.",
                    "label": 0
                },
                {
                    "sent": "But for sharing, probably between people.",
                    "label": 0
                },
                {
                    "sent": "Personally, since you need some image registration algorithm because yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you bring up you bring up a great point.",
                    "label": 0
                },
                {
                    "sent": "So in the work that I mentioned from introduced Andy.",
                    "label": 0
                },
                {
                    "sent": "Typically what people do is they project the brain onto.",
                    "label": 0
                },
                {
                    "sent": "There are these kind of Canonical brains right that have the same number of features ones called.",
                    "label": 0
                },
                {
                    "sent": "I think Tyler Rackspace and there's another one called MINI Space and I think that Alarak spaces there's this very lovely French woman and her brain was kind of modeled for everyone else but.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so basically in each of those cases you project onto this common spatial dimension and you lose some.",
                    "label": 0
                },
                {
                    "sent": "You definitely lose some accuracy.",
                    "label": 0
                },
                {
                    "sent": "There was some data, but so that's where he was, what he was doing.",
                    "label": 0
                },
                {
                    "sent": "Alright, well thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}