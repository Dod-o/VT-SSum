{
    "id": "dbzhbw5pqtxggj6jo4jlcp2bjsvedham",
    "title": "A Simpler, Intuitive Approach to Morpheme Induction",
    "info": {
        "coauthor": [
            "Samarth Keshava, Yale University"
        ],
        "author": [
            "Emily Pitler, Yale University"
        ],
        "published": "April 15, 2007",
        "recorded": "April 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Human Language Technology"
        ]
    },
    "url": "http://videolectures.net/pcw06_pitler_atm/",
    "segmentation": [
        [
            "Simpler intuitive approach to morpheme induction.",
            "We originally took computational linguistics class, where our final project was to write a program to find the most common prefixes and suffixes in English, and we then expanded this program into what you."
        ],
        [
            "See it today.",
            "So our main goal was to design a simple algorithm that minimized assumptions and parameters that made the program work.",
            "So some examples of assumptions that we did not make as we did not limit the total number of morphemes or program could identify.",
            "We no limitations on the lengths of morphemes, prefixes and suffixes can be of any length, and we made no limitations on the number of morphemes per word.",
            "So as you'll see later in our presentation, we really just have one parameter."
        ],
        [
            "So because this came out of a project that sought to identify prefixes and suffixes, our main approaches to 1st identify common morphemes in the language and then use these lists to segment the test words.",
            "So I put prefix and suffix in quotation marks because not everything in the prefix list is actually a prefix, not everything in suffix list is actually a suffix.",
            "So, for example, morphemes like man, that it that's not actually a suffix, but if it appears in a compound word, it's likely to appear at the end, like Fireman or Chairman, whereas if it appears at the beginning, it like in the word manatee, it's not likely to be a true morpheme.",
            "So morphemes that are truly should be characterized in stems we divide into the prefix and suffix list."
        ],
        [
            "There are two main intuitions that really guide our program.",
            "The first intuition is that if you have a word and you remove a potential morpheme, the rest of the word should also be a word in the corpus.",
            "So for example, in the word training, if you peel off the suffix, ING train is still a word.",
            "If you tried peeling off Ng then train I is not a word.",
            "If you tried pulling off an ING then tray is not award.",
            "This also works for compound words like Chairman.",
            "If you peel off man chair is still award.",
            "However it doesn't work in all cases.",
            "For example, insufferable in suffer is not a word and so we would never use this approach to actually segment the words because there are so many misses like in this example.",
            "However, in a large enough corpus will be able to find the suffix able in another corpus word, and so because the corpus is large enough, it's OK.",
            "If we have Mrs as long as we don't get false positives, so we wanted to air on the side."
        ],
        [
            "Strychnos our other main intuition is to use fluctuations in the transitional probabilities.",
            "This isn't new, we've seen this many times today and our intuition is that more few internal boundary.",
            "For example, the probability of T given report is going to be approximately 1, whereas the probability of S given report which is a morpheme boundary, is going to be less than one 'cause they're going to be other morphemes that could have been on the end of report like reported.",
            "Reporting report."
        ],
        [
            "Keystring so our program consists of four steps.",
            "First step is sort of a preprocessing step.",
            "We build what we call the lexicographic trees, which will explain in a bit.",
            "The second step is that we use these trees to score all potential substrings in language to determine what are likely to be morphemes.",
            "We then have a preliminary prefix listener preliminary suffix list.",
            "We then prune the morpheme lists for spurious morphemes, and then finally we use these lists and the original trees to actually segment the test words."
        ],
        [
            "So in order to explain the trees, I think it would be easiest to show you an example.",
            "So here's a hypothetical section in the forward tree at first.",
            "Like to note that we do not actually store these as a tree structure, we really use a more efficient data structure.",
            "However, we always conceptualise the algorithm in terms of these trees, so I'm going to explain it as if we were using the trees today.",
            "So create create both forward tree and a backward."
        ],
        [
            "In the forward tree, each node stores the a substring and an associated frequency count.",
            "So in the forward tree the frequency count is the sum of all the frequency counts in the corpus of words that have that node substring as the starting fragment of the word.",
            "So in this example, there would be a total of 2000 frequency count is the sum of all the frequency counts of words that begin with R. In a backward tree, would store the counts where the substring is the ending word.",
            "So, for example, we might store the number of the total frequency count of words that begin with the letter G. And so once we have this, it's very easy to calculate transitional probabilities.",
            "So for example, if we wanted to store to find the conditional probability of S given report, we would take the.",
            "Frequency Counter reports, which is 20 and divide by the frequency count of its parent, which is 100 and we get a probability of 20%.",
            "So we can use this to easily check those intuitions that I mentioned earlier.",
            "So."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simpler intuitive approach to morpheme induction.",
                    "label": 0
                },
                {
                    "sent": "We originally took computational linguistics class, where our final project was to write a program to find the most common prefixes and suffixes in English, and we then expanded this program into what you.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See it today.",
                    "label": 0
                },
                {
                    "sent": "So our main goal was to design a simple algorithm that minimized assumptions and parameters that made the program work.",
                    "label": 1
                },
                {
                    "sent": "So some examples of assumptions that we did not make as we did not limit the total number of morphemes or program could identify.",
                    "label": 0
                },
                {
                    "sent": "We no limitations on the lengths of morphemes, prefixes and suffixes can be of any length, and we made no limitations on the number of morphemes per word.",
                    "label": 0
                },
                {
                    "sent": "So as you'll see later in our presentation, we really just have one parameter.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So because this came out of a project that sought to identify prefixes and suffixes, our main approaches to 1st identify common morphemes in the language and then use these lists to segment the test words.",
                    "label": 1
                },
                {
                    "sent": "So I put prefix and suffix in quotation marks because not everything in the prefix list is actually a prefix, not everything in suffix list is actually a suffix.",
                    "label": 0
                },
                {
                    "sent": "So, for example, morphemes like man, that it that's not actually a suffix, but if it appears in a compound word, it's likely to appear at the end, like Fireman or Chairman, whereas if it appears at the beginning, it like in the word manatee, it's not likely to be a true morpheme.",
                    "label": 0
                },
                {
                    "sent": "So morphemes that are truly should be characterized in stems we divide into the prefix and suffix list.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are two main intuitions that really guide our program.",
                    "label": 0
                },
                {
                    "sent": "The first intuition is that if you have a word and you remove a potential morpheme, the rest of the word should also be a word in the corpus.",
                    "label": 1
                },
                {
                    "sent": "So for example, in the word training, if you peel off the suffix, ING train is still a word.",
                    "label": 0
                },
                {
                    "sent": "If you tried peeling off Ng then train I is not a word.",
                    "label": 0
                },
                {
                    "sent": "If you tried pulling off an ING then tray is not award.",
                    "label": 0
                },
                {
                    "sent": "This also works for compound words like Chairman.",
                    "label": 0
                },
                {
                    "sent": "If you peel off man chair is still award.",
                    "label": 0
                },
                {
                    "sent": "However it doesn't work in all cases.",
                    "label": 0
                },
                {
                    "sent": "For example, insufferable in suffer is not a word and so we would never use this approach to actually segment the words because there are so many misses like in this example.",
                    "label": 0
                },
                {
                    "sent": "However, in a large enough corpus will be able to find the suffix able in another corpus word, and so because the corpus is large enough, it's OK.",
                    "label": 0
                },
                {
                    "sent": "If we have Mrs as long as we don't get false positives, so we wanted to air on the side.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Strychnos our other main intuition is to use fluctuations in the transitional probabilities.",
                    "label": 1
                },
                {
                    "sent": "This isn't new, we've seen this many times today and our intuition is that more few internal boundary.",
                    "label": 0
                },
                {
                    "sent": "For example, the probability of T given report is going to be approximately 1, whereas the probability of S given report which is a morpheme boundary, is going to be less than one 'cause they're going to be other morphemes that could have been on the end of report like reported.",
                    "label": 0
                },
                {
                    "sent": "Reporting report.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Keystring so our program consists of four steps.",
                    "label": 1
                },
                {
                    "sent": "First step is sort of a preprocessing step.",
                    "label": 0
                },
                {
                    "sent": "We build what we call the lexicographic trees, which will explain in a bit.",
                    "label": 1
                },
                {
                    "sent": "The second step is that we use these trees to score all potential substrings in language to determine what are likely to be morphemes.",
                    "label": 0
                },
                {
                    "sent": "We then have a preliminary prefix listener preliminary suffix list.",
                    "label": 0
                },
                {
                    "sent": "We then prune the morpheme lists for spurious morphemes, and then finally we use these lists and the original trees to actually segment the test words.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to explain the trees, I think it would be easiest to show you an example.",
                    "label": 0
                },
                {
                    "sent": "So here's a hypothetical section in the forward tree at first.",
                    "label": 0
                },
                {
                    "sent": "Like to note that we do not actually store these as a tree structure, we really use a more efficient data structure.",
                    "label": 0
                },
                {
                    "sent": "However, we always conceptualise the algorithm in terms of these trees, so I'm going to explain it as if we were using the trees today.",
                    "label": 0
                },
                {
                    "sent": "So create create both forward tree and a backward.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the forward tree, each node stores the a substring and an associated frequency count.",
                    "label": 1
                },
                {
                    "sent": "So in the forward tree the frequency count is the sum of all the frequency counts in the corpus of words that have that node substring as the starting fragment of the word.",
                    "label": 0
                },
                {
                    "sent": "So in this example, there would be a total of 2000 frequency count is the sum of all the frequency counts of words that begin with R. In a backward tree, would store the counts where the substring is the ending word.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we might store the number of the total frequency count of words that begin with the letter G. And so once we have this, it's very easy to calculate transitional probabilities.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we wanted to store to find the conditional probability of S given report, we would take the.",
                    "label": 0
                },
                {
                    "sent": "Frequency Counter reports, which is 20 and divide by the frequency count of its parent, which is 100 and we get a probability of 20%.",
                    "label": 0
                },
                {
                    "sent": "So we can use this to easily check those intuitions that I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        }
    }
}