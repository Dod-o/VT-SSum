{
    "id": "bftwyamxoso4lc4xw5lyeqpyh63am727",
    "title": "Introduction To Statistical Machine Learning",
    "info": {
        "author": [
            "Marcus Hutter, Australian National University"
        ],
        "published": "April 1, 2009",
        "recorded": "January 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/ssll09_hutter_isml/",
    "segmentation": [
        [
            "Statistical machine learning.",
            "So you're right here, if you're logicians, because that's sort of a crossover course this time.",
            "First year we have an AI course, I mean a mini course.",
            "In the second week, and that hopefully gives you a.",
            "Back and the required background to understand his lectures.",
            "And for the machine learners.",
            "That hopefully is a nice summary of what you have learned and what you will learn in the next week.",
            "So I try to really cover machine learning as broadly as possible.",
            "That means, of course, I will not go into any death, but then at least you have heard you know the terminology and some words and maybe over the weekend you can look up something.",
            "In the Internet which you want to.",
            "Understand more deeply.",
            "Um?"
        ],
        [
            "Now we're here.",
            "Differently.",
            "OK. Peter, do you have your laser pointer with you?",
            "That was not yours.",
            "Anybody have a laser pointer?",
            "Are there some?",
            "OK, so modern technology.",
            "OK, let's start.",
            "There's a lot to to learn."
        ],
        [
            "OK, first I will give you an introduction and then I will quite in quite detail talk about linear methods for regression.",
            "That sounds totally boring.",
            "And sort of is that's good old fashion statistics from I don't know.",
            "Probably 100 years ago most of this stuff here, but if you don't understand the linear things I mean, don't try something more complex, so it's really good together.",
            "Grasp on that.",
            "And there are some fields in machine learning where they tried fancy stuff, neural networks and these things.",
            "And later they found you know just doing linear regression you know is better than all these fancy stuff.",
            "Yeah, so first try it simple and then go to the nonlinear stuff which is then in Section 3.",
            "Then in Section 4 I will talk about model assessment and selection, which becomes important if you have complex problems.",
            "And complex models and not enough data.",
            "Then you get the problem of overfitting.",
            "Section 5 continues that what to do with large problems.",
            "In general you combine all sorts of methods and you do some other things.",
            "That's largely so far for supervised learning.",
            "There's also unsupervised learning, which I briefly discussed, and reinforcement learning or reactive learning in Section 7 and then the three hours are.",
            "Over."
        ],
        [
            "OK, so first, what is my?"
        ],
        [
            "In learning so one definition would be machine learning is concerned with the development of algorithms and techniques that allow computers to learn.",
            "It's not very deep.",
            "I mean there's machine learning, so it has something to do with machines that learn.",
            "I mean you all.",
            "You all know what machines are that are computers and learning in this context is the process of gaining understanding by constructing models, models an important part of observed data with the intention to use them for prediction.",
            "So important keywords are models.",
            "Beta and you're what you want to do with it.",
            "Typically it's for prediction.",
            "Ultimately.",
            "I mean, often you get stuck in the middle and say the models are.",
            "You know your goal and then you give the model to somebody else.",
            "Yeah, but ultimately you want to do something with your model, and that's often good to keep in mind because you see all these techniques.",
            "Trying to find good in quotation Mark models.",
            "And sometimes you have.",
            "You know competing ideas or competing algorithms and then you ask yourself.",
            "I mean which is better?",
            "How do I evaluate them?",
            "What is the ultimate purpose?",
            "So related fields are artificial intelligence.",
            "Both are trying to find smart algorithms processing data.",
            "A machine learning draws a lot from statistics.",
            "So there's a huge overlap, so at least 50%, but statistics is more concerned with really the estimators and the models and machine learning is more concerned with really using it for prediction.",
            "I mean, that's not completely fair, but you know it's a rough characterization.",
            "And there's also data mining.",
            "Which searches through large volumes of data.",
            "I mean computer science in general.",
            "I mean you want efficient algorithms for all that, and to deal with complex models you need computers of course."
        ],
        [
            "So why should we learn it all, I mean.",
            "Logicians in logic.",
            "You typically don't learn.",
            "I mean, you learn of course stuff here, but I mean what you do is you deduction.",
            "You have your axioms, and you did you something and you don't create in this sense extra knowledge at all.",
            "Your knowledge is implicitly already in the axioms.",
            "And also if you have say payroll, I mean there's.",
            "You don't need to learn online.",
            "I mean, once you have learned how to add, you know that's more or less it.",
            "So when is learning used when the human expertise does not exist?",
            "For instance, navigating on Mars?",
            "I mean you cannot put describe perfectly your environment or all the axioms.",
            "And that's not possible.",
            "Or if it's very difficult for humans to explain their expertise.",
            "For instance, speech recognition.",
            "I mean, we all know how to recognize speech, but it's extremely hard to impossible to formally write that down in logical axioms.",
            "So what you do is you take spoken speech and the corresponding text, and then you try to learn the relation.",
            "If the solution change in time.",
            "So.",
            "I mean, for computer networks, maybe you know the network, but tomorrow there's another computer and everything changes, so it's better to have a system which automatically adapts, and adapting closely related to learning.",
            "And that's the last point.",
            "OK, so for instance, one of the first successes in the I was the Checkers program from summer in 1956 or something.",
            "He didn't roll the program which played checkers well, but a checkers program which was quite stupid.",
            "Just knowing the rules and then by self play, learn to play better and better.",
            "And later came TD Begeman, so TD Gammon was the best beginning player for a large.",
            "So."
        ],
        [
            "So here's another example, handwritten character recognition that's actually very important for Mail.",
            "All your zip codes are automatically scanned by a computer, and your handwriting is detected as the reason why they ask you to write nicely.",
            "Are there still?",
            "I don't know the success rate maybe 95% and then these 5% or get get sorted out and human has to recognize the digits and one approach would be OK. Let's try to characterize a four.",
            "I mean this has these three lines in this.",
            "Um, relative orientation and positions.",
            "And you know there are lots of variations and you could try to explicitly come up with a description list which is.",
            "In this case, maybe even possible, you have just 10 digits and you can do it.",
            "So the learning approach is.",
            "You have this handwritten digits.",
            "You have a training example where you give the system the digits orders 7210 and the system should figure out itself what is the relation and then generalize to future digits.",
            "Which is much easier than trying to find."
        ],
        [
            "System by hand.",
            "So here is a list of applications which, with quite successful applications, natural language processing is.",
            "Maybe one of the most famous ones which works quite well already, so you have three criteria.",
            "First, it should be speaker independent.",
            "Second, you want to have a large vocabulary and 3rd it should work on continuous speech and not on separate words.",
            "And if you only demand two of the three conditions, whatever the two are, you get really, really good systems.",
            "There are no systems yet available which have all three properties, but that will take maybe 5 or 10 years more and then.",
            "This problem is solved and this is.",
            "There's heavy machine learning going on.",
            "This Markov process is and all kinds of things which, yes.",
            "The third one is detecting continuous speech, not making a break between each word.",
            "Um?",
            "OK, search engines Google One of some of the best machine learners work at Google.",
            "Medical diagnosis credit.",
            "Card fraud, stock market analysis.",
            "Bioinformatics is popular field in last 10 years where a lot of machine learning is going on.",
            "Um?",
            "Computer vision, more and more algorithms learn from experience rather than pre programming everything.",
            "Gameplaying, obviously.",
            "I think not so much.",
            "In chess there's not so much learning going on there.",
            "I mean, you just have, I mean the games, the end games, the beginning, the openings and and that research.",
            "But in other games, especially the more probabilistic ones like Gammons.",
            "And go this is done with machine learning techniques and robot locomotion.",
            "OK.",
            "So isn't everything."
        ],
        [
            "Field, you can characterize different learning approaches or problem.",
            "And possibly the most fundamental categorization is in these three categories.",
            "One is supervised learning, which are.",
            "Will talk mostly about and probably 80% of machine learning is about supervised learning.",
            "That's why you have a teacher which tells you the answer on the training examples, like in the handwritten character recognition case you know the answer for the training set.",
            "Then there's unsupervised learning.",
            "If there's no teacher telling you what the truth is.",
            "One interesting question is, I mean, what's really the goal?",
            "Can you well define this problem?",
            "And in a certain sense you can.",
            "So what you do is you learn associations or clustering density estimation.",
            "So you try to find patterns in your data.",
            "In supervised learning of classification and regression.",
            "And then there's reinforcement learning in between.",
            "Where?",
            "The environment only provides a reward signal telling you whether your action or decision or whatever, and was good or bad, not what you should have done, but whether it was good or bad and you should learn from that.",
            "So this characteristic.",
            "And in my second lecture next week on Friday evening, I will talk a lot about this case about reinforcement learning.",
            "And there are others which are mostly mixtures of this.",
            "OK, so the supervised learning.",
            "In supervised learning, the task is to predict the output from."
        ],
        [
            "Put.",
            "Or first I mean to detect the rule for that.",
            "Or yeah, that was the second point.",
            "I'm the rule is either used for prediction or sometimes used for compressing the data.",
            "I mean, if you have a rule you can code your data more shortly and you don't have to keep the data around.",
            "I'm another application.",
            "A more category in supervised learning is outlier detection, which is sort of a classification where one class where you have only very few instances.",
            "So you have, you know a lot of emails which are nowadays most emails or spam or a few emails are good and all the good ones are the outliers or the other way around.",
            "The extreme cases.",
            "So."
        ],
        [
            "Our classification it's a simple example.",
            "For instance, say you want to determine whether a certain customer is worth credit and you know his income and the savings.",
            "And if he has a high income and say a lot of savings, then there's low risk to giving him alone and otherwise if there's a low income or low savings then there's a high risk.",
            "If you just have, you know this binary decision giving him alone or not.",
            "And in this case you could write down the rule.",
            "So if the income is larger than the other one and the savings large, and there are two then low risk else high risk.",
            "So that's a classification problem.",
            "And what you would like, rather than writing down this rules and that ever wanted it to, you want to learn from past experience from your customers?",
            "You have the income and savings of your customers and better they in the past were able to pay back the loan or not."
        ],
        [
            "So in regression, so in classification your output is.",
            "Discrete often binary, so minus and plus.",
            "And in regression, what you have is you have an input variable X and an output variable Y and you have a functional relation.",
            "Y is equal to F of X.",
            "In nearly all cases you have noise.",
            "I want to find a function which nicely fits through the data points.",
            "But not perfectly, because, um, you have the noise, and if you would, I mean, if you would take a perfect curve.",
            "Would look like this, but this is probably not the real functional relationship.",
            "I come to that later."
        ],
        [
            "Pectolite later.",
            "So unsupervised learning.",
            "One example is clustering.",
            "You have data points here.",
            "Data points here and you want to just, you know, identify the groups.",
            "So for example customer segmentation.",
            "I mean you have customers there, young customers, male customers, female customers and but you don't know this information to just have your data points that behavior say how they click in the Internet and you want to group them into these categories and then send specific ads for instance.",
            "But, and you also don't know apriori what is a good grouping, I mean male versus female, maybe good, maybe not young versus old, but you don't know that our priority, so you want to learn that from your data yourself itself.",
            "Find reinforcement learning.",
            "I mentioned it already up."
        ],
        [
            "Apart from that, you only have very scarce feedback, namely, teacher tells you whether it was good or bad.",
            "You have many other additional complications.",
            "Normally you have temporal dependence.",
            "Either enforcement learning is used when you have an agent which interacts with environment in cycles, and occasionally you get the rewards they assume you play chess and at the end of the chess game the agent gets a reward positive reward.",
            "If it wins Anna negatively, but if it loses so, then the problem is.",
            "Which moves in the game were responsible for winning or losing?",
            "So the agent has to find out.",
            "By himself also this.",
            "So that's the.",
            "That's.",
            "The credit assignment problem.",
            "So which action in the past?",
            "Oops.",
            "Which action in the past or action sequence is responsible for the reward later?",
            "Additional difficulties you have if you have multiple agents or two agents playing against each other for reinforcement learning is mostly used or developed for the case you have one learning agents playing against the static.",
            "Environment, I mean static means it can be probabilistic.",
            "It can change overtime, but you more or less know it and you can prove a lot of things about it.",
            "But then in game playing you have two learning agents, possibly playing against each other.",
            "That makes it gives an additional complication.",
            "So, so this was the course characterization, supervised learning, reinforcement learning, unsupervised learning.",
            "And if you have a problem, you should first check in which class it falls.",
            "So.",
            "But there are a lot of other."
        ],
        [
            "The bottom is for instance.",
            "I mean the most other most important.",
            "Other one is probably the machine learning statistical side versus the logic.",
            "Good old patient AI side.",
            "Then the distinction whether you want to do induction that is just learning your model and then that's it, and you give the model to somebody else, or you really want to do use the model for prediction.",
            "Or you could just skip this step and predict directly.",
            "That's transductive learning.",
            "But then you can ask your question, why do you want to predict what is a good prediction actually?",
            "And this is also not so trivial.",
            "It depends on your loss function.",
            "So what you ultimately the ultimate reason for wanting to predict something is that you want to make some decisions.",
            "So you predict the weather for tomorrow 'cause you want to decide in the morning whether you take an umbrella or sunglasses.",
            "And then you have some loss function.",
            "I mean taking an umbrella.",
            "It's sunny is probably still better than you know, taking your sunglasses it rains, so you have a loss matrix and depending on that and this influences your decision.",
            "And then ultimately I made this decision leads to an action.",
            "Which may influence the environment, so it's also good to know you know which of these problems you're actually attacking.",
            "There's regression versus classification already mentioned.",
            "Most of machine learning is about identically independent distributed data.",
            "That means you get a data point and then you get another data point in their sample from the distributions and these are independent.",
            "Made identically distributed, probably.",
            "Maybe 95% of machine learning is about this here.",
            "I'm sequence prediction or the agent setup is non IID.",
            "There's also online versus offline offline is you train your system on your training data you have.",
            "Then you fix everything in you sell your product and it doesn't learn anymore.",
            "Online means that it learns over the lifetime.",
            "So it makes maybe a prediction and then it receives the true label.",
            "For instance of weather forecasting system.",
            "And it makes a prediction at the next day.",
            "It knows the weather at the previous day and it could use this information to improve itself.",
            "I guess most commercial weather forecasting systems are not like that.",
            "It's developed by machine learning techniques at the University and then sold and statically fixed.",
            "So this would be an offline approach.",
            "I'm.",
            "This is related passive prediction.",
            "Active learning to this year above.",
            "Passive prediction in this context means that your prediction does not influence the environment.",
            "Jamaica Weather forecasting prediction.",
            "You take an umbrella, your sunglass, but this will not affect the weather tomorrow.",
            "Except for the butterfly effect, so let's ignore that.",
            "Where does active learning is where you have an agent which makes a prediction than an action like in chess playing.",
            "But this influences what your opponent does.",
            "And this active learning, I would say.",
            "I mean, if you want to quantify this, at least you know.",
            "10 or 100 times more complex than the passive case, so it makes things really much more complicated.",
            "Parametric versus nonparametric parametric is also the domain of statistics.",
            "You have some model with a few parameters and you want to identify the parameters.",
            "Nonparametric is say you want to fit a function for data points, but a pro you have no functional class or any function is a pretty good or considered.",
            "What is the best function?",
            "Then of course, as always, you could consider more than mathematic conceptual issues or the computational issues.",
            "That's also.",
            "I mean, statistics is more on this side and machine learning is more this side.",
            "Also, exactly this principled.",
            "So probably machine learning is also more on this side and but, but the fields are merging and OK.",
            "This picture to me I had before.",
            "OK, now I have."
        ],
        [
            "Two more slides before I go into more detail.",
            "That's about probability basics.",
            "I.",
            "Is there anyone who has not had a probability or statistics lecture in the past?",
            "OK, so.",
            "OK one OK, this slide is good, at least for one machine.",
            "Learning is heavily based on uncertainty and uncertainty is characterized by probabilities, so that just gives you a.",
            "Brief reminder about the terminology.",
            "So what you have to say, take a die which you roll.",
            "The sample space is the possible outcomes one to six events are subsets of the sample space.",
            "For instance, the event that an even number turns up.",
            "So that would be 2 for sex or odd number would be 135.",
            "Then the probability of six is.",
            "I mean in this case obviously 16.",
            "This point is annoying.",
            "The probability of even is OK. Three out of six is 1/2.",
            "Six would be the outcome and conditional probabilities are computed by ratios of probabilities or probability of a given.",
            "B is a probably with A&B divided by the probability of B.",
            "So 6 and even OK, it's just six, so this is 1, six and even is 1/2.",
            "So this is 1/3.",
            "And here's the general probability axioms.",
            "And the probability is between zero and one.",
            "If, say, A&B are disjunct.",
            "Then the probability of a Union B is the sum of probabilities and this is the general form, and this is the definition of conditional probability I just mentioned.",
            "OK, that's the very."
        ],
        [
            "Basics and he is more probability jargon.",
            "Which may be new to some of the logicians.",
            "So let's consider calling here.",
            "Make it even simpler.",
            "So coin can have tail or hat.",
            "And this is to be identified with zero and one, and assume it's an unfair coin.",
            "In this probability of head is Theta.",
            "So Taylor can be anything between zero and one before a fair coin is of course 1/2.",
            "So the likelihood of observing, say 11014 coin which has buyers Petta.",
            "Is that OK?",
            "The probability of one is status or your theater times Theta times.",
            "The probability of zero is 1 minus Theta and then another one is tighter, so the probability is Theta cubed times 1 minus Theta and that's called the likelihood.",
            "The maximum likelihood estimate is, as the name says, takes that at a which maximizes the likelihood, and this would be here.",
            "In this case.",
            "I mean, if you look at this function theater 3 * 1 -- 10 to the maximum is at 3 / 4.",
            "Which is just the relative frequency.",
            "So you count the number of heads divided by the sequence length.",
            "So this is 3 four.",
            "So if you don't know the buyers of the coin are reasonable, estimate is to look at the relative frequency.",
            "OK, for Beijing prediction you also need a, so that's the frequentist approach.",
            "What?",
            "How so?",
            "So what I take this function here?",
            "This is this function which looks like.",
            "This one this is P of 1101 given Theta which is Theta to the three, 1 minus Theta.",
            "So it's a cubic function here.",
            "And it's a linear function here, so it.",
            "Looks like this.",
            "And the maximum.",
            "You can show.",
            "Is at .75.",
            "And you take this as an estimate, and that's I mean you can ask wise is a cost estimate and you can argue for that, improve various things.",
            "Or you can just take it as a principle.",
            "So the maximum likelihood principle tells you to take this as an estimate.",
            "I'm.",
            "So if you do Bayesian statistics, you also need a prior over Theta, so assume.",
            "You take a physical coin.",
            "And you throw and you get 3 ones and 10.",
            "Would you say or would you estimate the probability of her to be three 4th?",
            "Probably not, you would say, OK, I mean that's just bad luck.",
            "Or I mean this is quite normal.",
            "It's not even very exotic 'cause you have a strong bias towards fair coins.",
            "I mean you can look at the coin and you know that's fair or even if you don't look typically a coin is quite fair.",
            "So what you have is before you throw the coin.",
            "Before you throw the coin, you have a prior knowledge and.",
            "With hype prior belief you coin is fair and unfair coins are quite unlikely.",
            "So this is, I mean this is not a Delta P for something which is query I mean.",
            "Very high, but you know maybe 100, maybe 99 out of 100 coins is fair.",
            "Yeah, I mean it's a density so.",
            "Forget about the scale.",
            "OK.",
            "So in this prior knowledge should be taken into account and there is one disadvantage of the maximum likelihood approach.",
            "It does not take this prior knowledge into account.",
            "So if we are indifferent, then we can choose P of Theta equal to constant and if not we choose it.",
            "Maybe you know like this or whatever.",
            "My prior knowledge is.",
            "Then you can compute the evidence which is integrating out the data so you have the probability of a given B times the probability of B, summed over all possible bees, give you the probability of a.",
            "So that's just normal some rule, and if you do that you get 1 / 20.",
            "Actually, since status continues this summer is integral in the probabilities of densities and so on.",
            "But forget about these technicalities.",
            "This evidence is on the one hand very important.",
            "But on the other hand, it's very hard to get the real meaning out of it, so don't try.",
            "Now at least OK, but from the evidence you can compute the posterior.",
            "What is the probability of Theta after you have seen 1101 and you just apply two times the definition of conditional probability.",
            "So you have the probability of a given B.",
            "Times the probability of B was the probability of A&BI mean.",
            "That's the definition of the conditional probability, and then you would use it in reverse form.",
            "Like this?",
            "And divide.",
            "You divide by P of B. PFB and that's it.",
            "And that flips around the data from this side.",
            "So the A was the day was the data.",
            "And now they is on the left hand side.",
            "Is that its base rule?",
            "I will come back to that in more detail on our next Friday.",
            "So now you can compute your posterior belief.",
            "Of Theta given your sequence.",
            "If you have a uniform prior like I assumed here, you get exactly the same functional and the maximum posteriori estimates now maximize this posterior rather than the likely to maximize the posterior.",
            "So you see, if you have the privacy uniform, you get the same answer, but if you plug in a prior like this.",
            "You would get a different answer and you can do that if you want to, and if you take a prior like this, one characterization would be.",
            "One nice way to characterize these prices cater to the A1 minus Theta to the better.",
            "So take P of Theta is proportional tool and you play around with Alpha and beta until you get a function which you like.",
            "I mean, which you know corresponds roughly to your prior knowledge, because if you use these, you can compute the integrals nicely and then check what you get and you get something which is much closer to 1/2.",
            "Of course, if you then have more evidence, a lot along sequence, then it will start to deviate from 1/2 because I mean if you throw a coin 1000 times and you have 900 times or one and only 100 times a zero, then you get suspicious.",
            "In the Bayesian updating I mean correctly.",
            "And catchers this.",
            "Belief.",
            "OK, the predictive distribution is.",
            "What you really?",
            "Ultimately, what you want to predict, the probability of the next coin or the next flip.",
            "So what is appropriate one given?",
            "The previous fourth rows, which you can compute as this ratio here.",
            "It's again the definition of conditional probability, which would be 2 third.",
            "For instance here expected values is just, you know, by definition this here the expected value of a function.",
            "Is F of Theta times the probability test or something over it?",
            "Which would be here also to 3rd?",
            "The variance is.",
            "The myth.",
            "Of this distribution with squared, which compute in this way and probability densities are fine like this.",
            "I mean you have a very short interval.",
            "You ask what is the probability divided by the length of the interval.",
            "Disk issues, again city.",
            "If that or if some of these things are new to you, then I recommend that you just look it up in Wikipedia over the weekend.",
            "Because I know, I mean, that was very quick.",
            "So let's come to Linea."
        ],
        [
            "Methods of regression.",
            "So are there any further questions so far?"
        ],
        [
            "OK.",
            "So linear regression.",
            "What you have is you have data points in some.",
            "High dimensional or low dimensional space and you want to fit the plane through the data points?",
            "That's linear regression.",
            "What you do is you typically have features called feature vector, say X0X1 up to HD.",
            "So here you have X1 and X2, so maybe X one is the H and X2 is the height of a person and in the Y axis you have the response variable which is often noisy.",
            "For instance that could be the person's weight which is a function of H and height and you want to.",
            "Detect the dependency and assume hopefully that it's linear.",
            "Then you make the following on.",
            "And that's so.",
            "And this is a nice German word which is used in physics but not outside physics.",
            "And that is an equation.",
            "Which or model which contains some free parameters which you want to determine later?",
            "So there's no good English word for that, so that's an answer.",
            "Here you have this linear function, so you have a weighted average over your coordinates X0 up to XD which gives you Y and you want to determine the coefficients here.",
            "So it's very convenient to introduce an extra coordinate X0 which is identically one which gives you the upset.",
            "So this is just one here, so that's the offset.",
            "So now you have your data points X1Y one up to X&YN.",
            "These are these red points.",
            "When you do regression.",
            "And you want to best fit your data.",
            "You have to define what best means you into characterization.",
            "And you do that with the loss function.",
            "So the most simple loss function of quadratic loss you compare the true value Y with what this function would predict.",
            "So you plug in the XY, you compute the estimate and you compare to the true value and take the square and you sum it over all data points.",
            "So that's the square loss.",
            "And it's reasonable to try to find the function.",
            "So that means to try to find the parameters in such a way that this loss is minimized.",
            "So that's here.",
            "So this least square regression.",
            "And this you can compute this just gives you a linear equation and this results in this case.",
            "Pretty boring, isn't it?",
            "But yeah, unfortunately or Fortunately very useful."
        ],
        [
            "OK, so let's make it more interesting.",
            "Let's assume you have.",
            "A lot of dimensions, so for instance.",
            "Now I'm in spam classifications, more our classification problem, but it doesn't really matter for this purpose.",
            "So for instance, you have the bag of words model, so each word is a coordinate.",
            "An accordion can be present or not.",
            "Yeah, then the dimension would be the number of possible words you have.",
            "Or maybe a better example.",
            "In bioinformatics you have these microarrays which can probe.",
            "I think the best chips.",
            "Now I have 500,000.",
            "I also don't know the details this base sequences they can prop so you have a feature vector 500,000.",
            "So these 500,000 on the other hand, your data sizes maybe especially by Infomatics 100 or at most 300.",
            "So you have 500,000 dimensions but just three in our data points.",
            "And as you can easily see if you have more data and more dimensions and data points, you can always fit the perfect plane through it.",
            "And if you think about it, it can't be good.",
            "That's the problem of overfitting.",
            "OK, why is it bad to fit perfectly?",
            "OK, perfect fit.",
            "Sounds good, but if you try to use this model for prediction you will realize it will make very poor predictions.",
            "Um, OK one way.",
            "To overcome this problem would be to identify a smaller subset of these features.",
            "So what could try to fix this on K?",
            "Say three more data points.",
            "Then you say fix or assume that there are 10 important features or 10 important dimensions.",
            "You try all you try, I mean 10 out of 500,000.",
            "Features all combinations you compute the least square and you take the one which is minimal.",
            "So it would be the most simple coefficient subset selection.",
            "But you have to fix Ki mean if you also allow varying K then of course you get cake will be because it always gives a perfect fit.",
            "Yeah, yeah, I will tell you later.",
            "That's the model selection in I'll come to in Section 4."
        ],
        [
            "OK, another possibility would be.",
            "So you have these weights so, so each dimension is associated weight.",
            "What you could do is you had a penalty term and say if the rates are two too many rates are too large, your loss gets penalized, so you add it to the loss so you get a bias towards smaller weights.",
            "In rich regression, you use a quadratic loss, which effectively shrinks all coefficients, but no coefficient really get 0.",
            "If you use a one norm that's called Lasso.",
            "Then what you will see is that many of these coefficients will be zero and only a few will be non 0 but then have different values.",
            "So that's so lost lives in between coefficient subset selection and Ridge regression, it shrinks the coefficients but many will be shrunken to 0, which is effectively the same as throwing dimensions out.",
            "Invasion regression.",
            "What you do is you take a prior over your weights, so maybe you know that some features are proven more important than others, or even if not, you have noninformative priors.",
            "And then you compute the posterior.",
            "So this is the likelihood times the prior divided by normalization constant.",
            "And then you take the maximum over W. And what you have.",
            "Here is that.",
            "You cannot give.",
            "A high prior to always, so most of these weights will have a small prior and if you multiply the privacy likelihood, this gives you something which is small.",
            "Which.",
            "Penalize these weights because you want to find a maximum so all this also gives you a.",
            "A bias towards those weights which appear or more likely, and if you choose the prior appropriately, these other smaller weights.",
            "So just remember there are three important.",
            "I mean there are many more ways, but these are the three most important regression, Lasso invasion, linear regression to overcome the problem of.",
            "Too many dimensions."
        ],
        [
            "So that was linear methods for regression.",
            "So what we do with classification?",
            "The one thing we could do is reduce it to regression, so classification, for instance, we want to classify emails with respect to spam or not spam.",
            "And again, let's use minus one or one or sometimes 01 for the labels.",
            "And you just regard minus one and one as a real number.",
            "I mean there are real numbers and then you do regression and you're done.",
            "So.",
            "It's hard to draw in many dimensions, but in one easy to draw.",
            "So assume you have the data points like this, so this is.",
            "I'll make it.",
            "This is 1.",
            "This is minus one.",
            "And you just have one dimension and.",
            "So if it looks like this.",
            "And you regard it as a regression problem.",
            "Then the best linear line through these data points is possibly.",
            "This.",
            "And it doesn't fit very well, but OK, that's the best you can do with linear regression.",
            "And then you say the function is positive.",
            "You classify it plus one.",
            "And if the function is negative, you classify as minus one, and that's not too bad.",
            "I mean all these right ones are classified correctly and here on the left side you have.",
            "Two wrong once.",
            "The problem with this knife approach is that you know if it extends very far to the side.",
            "And you use the square error.",
            "This is a huge error.",
            "And because it squared.",
            "You get a large loss, So what happens is if you minimize this loss, these points at the very end.",
            "The influence line mostly so the linear regression tries to push the line down here and here at the very end where it's obvious anyway how to classify.",
            "While the points and the critical boundary.",
            "They are more or less ignored because I mean, whether you're one away or you know, zero, what is doesn't make big difference.",
            "If it's here 1000 or something, so it just concentrated their effort on the wrong data points.",
            "And.",
            "Yeah, things can go really wrong.",
            "But OK, it's the 1st.",
            "It's worth a first trial.",
            "So OK, so for binary classification function is spent solely to linear regression, the functions positive you classify as one in negative is minus one.",
            "I mean, if you already I mean.",
            "I have a function now which is not just plus or minus.",
            "One gives you a real number you could think of.",
            "Maybe I should output a probability of the label being one or minus one, which you could easily do.",
            "You just interpret.",
            "The value of this function is a probability, since probabilities are between zero and one.",
            "You take some simple transform.",
            "So for instance, the lock of the probability of 1 divided by probability zero.",
            "You say this is the function value and so you can solve this equation easily with respect to P of Y equal to 1.",
            "So you which is effectively what you do is.",
            "So if this is your function value.",
            "You say the probability is this.",
            "So if the function value is very large to say, the probability is very high that the label is 1.",
            "If the function value is very small, it said probably that's minus.",
            "Sorry, probabilities, of course here.",
            "So that's the probability that the label wise equal to 1.",
            "I'm giving you data OK. Meet you compute from this function.",
            "It functions very negative.",
            "You say that it's likely to be 0, the label and if the function is close to zero, you assign a probability which is roughly 50%.",
            "OK, given that this method has certain problems, there are a lot of other methods.",
            "To do regression, there's linear discriminant analysis, which, as the name says, is still linear, but a little bit smarter, and all these others here.",
            "Logistic, regression, perceptron, an support vector machines.",
            "I will explain them later in more detail are nonlinear in nature.",
            "And you can generalize to nonbinary by in several ways.",
            "I mean, if you have three categories, you could classify one against the other two, and then one against the other two, and so on in three ways.",
            "And then you also have.",
            "So you have to reduce it in this way to binary classification.",
            "So here's a 2 dimensional example done with a linear discriminant analysis.",
            "So you add labels 1, two and three.",
            "And these are the.",
            "Still, you do something like regression and then you threshold at zero and these are then there.",
            "Lines where it passes.",
            "I'm zero and it's a reasonable output."
        ],
        [
            "OK, so now.",
            "This slide tells you why linear regression is really, really, really powerful.",
            "Becaused you can.",
            "Use it in the following way, which is called linear basis function regression.",
            "So even if you have a problem which doesn't look linear at all, it is often possible to reduce it to linear regression.",
            "The standard example is.",
            "Fitting a polynomial through data points.",
            "So that may be a cubic polynomial fits nicely for this data point, and a cubic polynomial doesn't look particularly linear, but actually it's very easy to reduce that to linear regression.",
            "What you do is the following.",
            "In general you have your feature vector X.",
            "You simply transform it to some new feature vector file of X.",
            "So from a D dimensional space to run, possibly different dimensional space, and then you assume or hope that the response variable Y is now linear file.",
            "That's the trick.",
            "So linear in linear regression before the feature function is just the components of XI mean you just X one X2X3.",
            "These are the feature phones, so it's identity transformation so nothing happens.",
            "In polynomial regression, what you do is you say that Phi.",
            "One of X is constant.",
            "5 two of X is X5.",
            "Three of X is X squared and say if you want to cubic.",
            "Regression you take up 2X cube.",
            "So also you have only One X, so here D was one.",
            "So it just have 1X now P is equal to 4.",
            "I can't plot that so.",
            "So now I'm for each data point you have a feature vector which looks like.",
            "You know this, you know 4 dimensional space.",
            "And you have the Y value.",
            "And now.",
            "You fit a linear hyperplane through this points.",
            "And project back.",
            "To this one dimensional.",
            "Original feature vector and what you get get is this cubic line.",
            "So other examples is, for instance piecewise constant regression.",
            "In this case, the feature functional just step functions.",
            "So what you have is if you want piecewise constant regression.",
            "I mean this is.",
            "Since 2 trivial you take as your feature functions.",
            "You take step functions.",
            "Which more laser indicator functions is my data?",
            "Would you get in?",
            "Think it is an indicator vector, say 001000, so that means that your data point X does not lie in the first blog, not lie in the second block, it lies in the third block does not lie in the photo and so on.",
            "So your feature vectors are just binary and then you make a linear regression.",
            "In this case it is.",
            "I mean just averaging the data points so it's trivial.",
            "But you can extend it to piecewise polynomials into splines, which I will do in the next slides.",
            "So look at here."
        ],
        [
            "So that is the piecewise constant regression and with these base functions you can do piecewise linear.",
            "You can do continuous.",
            "Piecewise linear, which gives you.",
            "I mean the points, obviously data points and the curve is the.",
            "True function where the data points were sampled from this noise and the linear ones are there.",
            "Regression curves.",
            "And for the piecewise linear basis function, for instance, looks like this, you have this triangular functions."
        ],
        [
            "So wavelets.",
            "Bayflite regression is also linear regressions, linear basis, function regression, and the basis functions.",
            "Are these here now?",
            "So you have to interpret it to follow, I mean the following value.",
            "The original space is 1 dimensional here, so that's your X.",
            "And what you get is for each of these feature functions safer feature function 8 or something 1234568 you get a vector which consists of numbers 000000000 and negative numbers 0 positive numbers and so on.",
            "So huge vector.",
            "So this is a huge dimensional space.",
            "Then you do linear regression.",
            "Which or another way to view it, is just taking a weighted.",
            "Mixture over all these wavelets and you fit it through your original function.",
            "And which gives you answer.",
            "And and he is a 2 dimensional case, so the original space, these two dimensional and the wavelet spaces is large.",
            "As the number of wave lifts you use.",
            "So still or linear.",
            "So yet now it gets a little bit more."
        ],
        [
            "Exciting.",
            "Very simple way.",
            "I'm to do regression which is also linear is local smoothing.",
            "So you have your data points here.",
            "Just take a window from here to here.",
            "Take the average of the Y values and this gives you.",
            "You know your regression point and then you move the window around.",
            "Which gives you the green this green function.",
            "Becaused you have here.",
            "So you average the Y values of the data points in a local neighborhood, and if you move the Windows data points go in and go out abruptly dysfunction still.",
            "Wiggles alot you could improve that by taking a kernel.",
            "Say you take a weighted average of these data points and you waited once feature in the middle more than on the on this side.",
            "So if you move this window in the data board gets in its first rated.",
            "Not very much and then more and more and more.",
            "And then it fades out so you get a curve which is much more smoother as you see here.",
            "So formally what you do is.",
            "You estimate.",
            "The function at Point X as a weighted average of the Y values and the weight.",
            "Function.",
            "Depends on the location of XI.",
            "And the point X you want to estimate.",
            "So in the in the nearest neighbor case you just say, I mean this kernel function is 1 if the point is close and 0 otherwise.",
            "So you take all points and take a weighted average.",
            "Or in this case, taking average, which is, I mean not weighted.",
            "Or here in this case you have quadratic kernel, so zero if.",
            "Your XI is farther than if you X is farther than a away then.",
            "From XI and otherwise, you have this quadratic shape.",
            "And if you look at it, I mean here you obviously see this linear, so you have a weighted average over the wise, and it's very easy.",
            "I mean, the coefficient weights are directly given, you don't even have a regression problem."
        ],
        [
            "I'm.",
            "So here's a nonparametric but still linear regression.",
            "Problem.",
            "So we still want to minimize the square loss, but now we take all functions, no restriction.",
            "Or maybe you you assume that it's.",
            "I'm differentiable or something, but I mean it's it's non parametric.",
            "If you would do that without any correction here, I explained it later.",
            "What would you get is of course a perfect fit, fit through the data.",
            "I mean, there's always a function.",
            "I mean even a C Infinity function or 10 times continuous function which perfectly goes through your data points.",
            "Easy to draw data.",
            "2 problems you have red points and blue points, and it's direct recursion curve and the blue one, so.",
            "I'm just a examples.",
            "OK, so this is not really a useful answer, so I assume you have, you know.",
            "Look at, look at this, I mean obviously you see here here it's large and here small you want something which looks like I mean these curves, but if you would perfectly fit through the data points.",
            "Give you a justice wiggly curve which is quite useless.",
            "So what you want to do is you want to regularize.",
            "Your problem, so you want to have a bias towards smooth functions.",
            "Smooth and then it I mean not mathematical sense now, but.",
            "Intuitive sense, So what you could do is you could say OK I want the curvature of this function so the second derivative is the curvature to be not too large.",
            "So you add a term which takes the curvature at point X and integrates over all X, so it's the average curvature of the whole domain.",
            "And you penalize your loss.",
            "So if your function with large curvature, you know you say the loss is larger than if you have a small curvature and you put a regularization parameter Lambda in there.",
            "So for Lambda equals 0 L explained, you would get a function which perfectly fits through the data.",
            "Is the other extreme tablang the evil to Infinity?",
            "So then you can forget about this term Infinity.",
            "I know you saw you can't forget you have an Infinity here, so ideally this should be 0.",
            "And you can make the zero by taking a linear function, because the second error is 0.",
            "And then this term gets you the least square fit.",
            "So you would just get returned to linear regression.",
            "So for Lambda Infinity you have linear regression.",
            "I mean the most smooth and simple function.",
            "For Lambda equals zero, you get this really function and for Lambda in between.",
            "What you get is you get a piece wise you can show you get a piecewise cubic function with continuous derivative.",
            "And that's just spline interpolation.",
            "If you've heard about that.",
            "And the Lambda controls sort of the wiggliness.",
            "Sorry.",
            "It's it's cubic, yeah, it's.",
            "I'm.",
            "The reason is the fault.",
            "Let me see intuitively, if you're a cubic polynomial, the second derivative is a constant.",
            "If you have a quartic or higher order polynomial would not be a constant.",
            "And.",
            "You can show that.",
            "I mean, in order to make this fit, I mean.",
            "You need at least a constant there and you can show it's better.",
            "You know you have the.",
            "You have a second derivative which is piecewise constant rather than piecewise linear or something.",
            "In order to minimize this expression.",
            "If you want higher order polynomials then you need higher order derivatives, so if you put the force derivative here then you would get piecewise 5th order.",
            "Yet mathematically follows.",
            "And indeed, I think I have a slide on that.",
            "I mean, you can reduce this functional optimization problem to a finite dimensional optimization problem, which is linear.",
            "So which just means solving some linear equations.",
            "OK, there was linear regression and I think that's exactly right for break."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Statistical machine learning.",
                    "label": 0
                },
                {
                    "sent": "So you're right here, if you're logicians, because that's sort of a crossover course this time.",
                    "label": 0
                },
                {
                    "sent": "First year we have an AI course, I mean a mini course.",
                    "label": 0
                },
                {
                    "sent": "In the second week, and that hopefully gives you a.",
                    "label": 0
                },
                {
                    "sent": "Back and the required background to understand his lectures.",
                    "label": 0
                },
                {
                    "sent": "And for the machine learners.",
                    "label": 0
                },
                {
                    "sent": "That hopefully is a nice summary of what you have learned and what you will learn in the next week.",
                    "label": 0
                },
                {
                    "sent": "So I try to really cover machine learning as broadly as possible.",
                    "label": 0
                },
                {
                    "sent": "That means, of course, I will not go into any death, but then at least you have heard you know the terminology and some words and maybe over the weekend you can look up something.",
                    "label": 0
                },
                {
                    "sent": "In the Internet which you want to.",
                    "label": 0
                },
                {
                    "sent": "Understand more deeply.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we're here.",
                    "label": 0
                },
                {
                    "sent": "Differently.",
                    "label": 0
                },
                {
                    "sent": "OK. Peter, do you have your laser pointer with you?",
                    "label": 0
                },
                {
                    "sent": "That was not yours.",
                    "label": 0
                },
                {
                    "sent": "Anybody have a laser pointer?",
                    "label": 0
                },
                {
                    "sent": "Are there some?",
                    "label": 0
                },
                {
                    "sent": "OK, so modern technology.",
                    "label": 0
                },
                {
                    "sent": "OK, let's start.",
                    "label": 0
                },
                {
                    "sent": "There's a lot to to learn.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, first I will give you an introduction and then I will quite in quite detail talk about linear methods for regression.",
                    "label": 1
                },
                {
                    "sent": "That sounds totally boring.",
                    "label": 0
                },
                {
                    "sent": "And sort of is that's good old fashion statistics from I don't know.",
                    "label": 0
                },
                {
                    "sent": "Probably 100 years ago most of this stuff here, but if you don't understand the linear things I mean, don't try something more complex, so it's really good together.",
                    "label": 0
                },
                {
                    "sent": "Grasp on that.",
                    "label": 0
                },
                {
                    "sent": "And there are some fields in machine learning where they tried fancy stuff, neural networks and these things.",
                    "label": 0
                },
                {
                    "sent": "And later they found you know just doing linear regression you know is better than all these fancy stuff.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so first try it simple and then go to the nonlinear stuff which is then in Section 3.",
                    "label": 0
                },
                {
                    "sent": "Then in Section 4 I will talk about model assessment and selection, which becomes important if you have complex problems.",
                    "label": 0
                },
                {
                    "sent": "And complex models and not enough data.",
                    "label": 0
                },
                {
                    "sent": "Then you get the problem of overfitting.",
                    "label": 1
                },
                {
                    "sent": "Section 5 continues that what to do with large problems.",
                    "label": 0
                },
                {
                    "sent": "In general you combine all sorts of methods and you do some other things.",
                    "label": 0
                },
                {
                    "sent": "That's largely so far for supervised learning.",
                    "label": 0
                },
                {
                    "sent": "There's also unsupervised learning, which I briefly discussed, and reinforcement learning or reactive learning in Section 7 and then the three hours are.",
                    "label": 0
                },
                {
                    "sent": "Over.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so first, what is my?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In learning so one definition would be machine learning is concerned with the development of algorithms and techniques that allow computers to learn.",
                    "label": 1
                },
                {
                    "sent": "It's not very deep.",
                    "label": 0
                },
                {
                    "sent": "I mean there's machine learning, so it has something to do with machines that learn.",
                    "label": 0
                },
                {
                    "sent": "I mean you all.",
                    "label": 0
                },
                {
                    "sent": "You all know what machines are that are computers and learning in this context is the process of gaining understanding by constructing models, models an important part of observed data with the intention to use them for prediction.",
                    "label": 1
                },
                {
                    "sent": "So important keywords are models.",
                    "label": 0
                },
                {
                    "sent": "Beta and you're what you want to do with it.",
                    "label": 0
                },
                {
                    "sent": "Typically it's for prediction.",
                    "label": 0
                },
                {
                    "sent": "Ultimately.",
                    "label": 0
                },
                {
                    "sent": "I mean, often you get stuck in the middle and say the models are.",
                    "label": 0
                },
                {
                    "sent": "You know your goal and then you give the model to somebody else.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but ultimately you want to do something with your model, and that's often good to keep in mind because you see all these techniques.",
                    "label": 0
                },
                {
                    "sent": "Trying to find good in quotation Mark models.",
                    "label": 0
                },
                {
                    "sent": "And sometimes you have.",
                    "label": 0
                },
                {
                    "sent": "You know competing ideas or competing algorithms and then you ask yourself.",
                    "label": 0
                },
                {
                    "sent": "I mean which is better?",
                    "label": 0
                },
                {
                    "sent": "How do I evaluate them?",
                    "label": 1
                },
                {
                    "sent": "What is the ultimate purpose?",
                    "label": 0
                },
                {
                    "sent": "So related fields are artificial intelligence.",
                    "label": 0
                },
                {
                    "sent": "Both are trying to find smart algorithms processing data.",
                    "label": 0
                },
                {
                    "sent": "A machine learning draws a lot from statistics.",
                    "label": 0
                },
                {
                    "sent": "So there's a huge overlap, so at least 50%, but statistics is more concerned with really the estimators and the models and machine learning is more concerned with really using it for prediction.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's not completely fair, but you know it's a rough characterization.",
                    "label": 1
                },
                {
                    "sent": "And there's also data mining.",
                    "label": 0
                },
                {
                    "sent": "Which searches through large volumes of data.",
                    "label": 0
                },
                {
                    "sent": "I mean computer science in general.",
                    "label": 0
                },
                {
                    "sent": "I mean you want efficient algorithms for all that, and to deal with complex models you need computers of course.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why should we learn it all, I mean.",
                    "label": 0
                },
                {
                    "sent": "Logicians in logic.",
                    "label": 0
                },
                {
                    "sent": "You typically don't learn.",
                    "label": 0
                },
                {
                    "sent": "I mean, you learn of course stuff here, but I mean what you do is you deduction.",
                    "label": 0
                },
                {
                    "sent": "You have your axioms, and you did you something and you don't create in this sense extra knowledge at all.",
                    "label": 0
                },
                {
                    "sent": "Your knowledge is implicitly already in the axioms.",
                    "label": 0
                },
                {
                    "sent": "And also if you have say payroll, I mean there's.",
                    "label": 0
                },
                {
                    "sent": "You don't need to learn online.",
                    "label": 1
                },
                {
                    "sent": "I mean, once you have learned how to add, you know that's more or less it.",
                    "label": 0
                },
                {
                    "sent": "So when is learning used when the human expertise does not exist?",
                    "label": 1
                },
                {
                    "sent": "For instance, navigating on Mars?",
                    "label": 0
                },
                {
                    "sent": "I mean you cannot put describe perfectly your environment or all the axioms.",
                    "label": 0
                },
                {
                    "sent": "And that's not possible.",
                    "label": 1
                },
                {
                    "sent": "Or if it's very difficult for humans to explain their expertise.",
                    "label": 1
                },
                {
                    "sent": "For instance, speech recognition.",
                    "label": 0
                },
                {
                    "sent": "I mean, we all know how to recognize speech, but it's extremely hard to impossible to formally write that down in logical axioms.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you take spoken speech and the corresponding text, and then you try to learn the relation.",
                    "label": 0
                },
                {
                    "sent": "If the solution change in time.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I mean, for computer networks, maybe you know the network, but tomorrow there's another computer and everything changes, so it's better to have a system which automatically adapts, and adapting closely related to learning.",
                    "label": 1
                },
                {
                    "sent": "And that's the last point.",
                    "label": 0
                },
                {
                    "sent": "OK, so for instance, one of the first successes in the I was the Checkers program from summer in 1956 or something.",
                    "label": 0
                },
                {
                    "sent": "He didn't roll the program which played checkers well, but a checkers program which was quite stupid.",
                    "label": 0
                },
                {
                    "sent": "Just knowing the rules and then by self play, learn to play better and better.",
                    "label": 0
                },
                {
                    "sent": "And later came TD Begeman, so TD Gammon was the best beginning player for a large.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's another example, handwritten character recognition that's actually very important for Mail.",
                    "label": 1
                },
                {
                    "sent": "All your zip codes are automatically scanned by a computer, and your handwriting is detected as the reason why they ask you to write nicely.",
                    "label": 0
                },
                {
                    "sent": "Are there still?",
                    "label": 0
                },
                {
                    "sent": "I don't know the success rate maybe 95% and then these 5% or get get sorted out and human has to recognize the digits and one approach would be OK. Let's try to characterize a four.",
                    "label": 0
                },
                {
                    "sent": "I mean this has these three lines in this.",
                    "label": 0
                },
                {
                    "sent": "Um, relative orientation and positions.",
                    "label": 0
                },
                {
                    "sent": "And you know there are lots of variations and you could try to explicitly come up with a description list which is.",
                    "label": 0
                },
                {
                    "sent": "In this case, maybe even possible, you have just 10 digits and you can do it.",
                    "label": 0
                },
                {
                    "sent": "So the learning approach is.",
                    "label": 0
                },
                {
                    "sent": "You have this handwritten digits.",
                    "label": 0
                },
                {
                    "sent": "You have a training example where you give the system the digits orders 7210 and the system should figure out itself what is the relation and then generalize to future digits.",
                    "label": 0
                },
                {
                    "sent": "Which is much easier than trying to find.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "System by hand.",
                    "label": 0
                },
                {
                    "sent": "So here is a list of applications which, with quite successful applications, natural language processing is.",
                    "label": 1
                },
                {
                    "sent": "Maybe one of the most famous ones which works quite well already, so you have three criteria.",
                    "label": 0
                },
                {
                    "sent": "First, it should be speaker independent.",
                    "label": 0
                },
                {
                    "sent": "Second, you want to have a large vocabulary and 3rd it should work on continuous speech and not on separate words.",
                    "label": 0
                },
                {
                    "sent": "And if you only demand two of the three conditions, whatever the two are, you get really, really good systems.",
                    "label": 0
                },
                {
                    "sent": "There are no systems yet available which have all three properties, but that will take maybe 5 or 10 years more and then.",
                    "label": 0
                },
                {
                    "sent": "This problem is solved and this is.",
                    "label": 1
                },
                {
                    "sent": "There's heavy machine learning going on.",
                    "label": 0
                },
                {
                    "sent": "This Markov process is and all kinds of things which, yes.",
                    "label": 0
                },
                {
                    "sent": "The third one is detecting continuous speech, not making a break between each word.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, search engines Google One of some of the best machine learners work at Google.",
                    "label": 0
                },
                {
                    "sent": "Medical diagnosis credit.",
                    "label": 0
                },
                {
                    "sent": "Card fraud, stock market analysis.",
                    "label": 1
                },
                {
                    "sent": "Bioinformatics is popular field in last 10 years where a lot of machine learning is going on.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Computer vision, more and more algorithms learn from experience rather than pre programming everything.",
                    "label": 0
                },
                {
                    "sent": "Gameplaying, obviously.",
                    "label": 0
                },
                {
                    "sent": "I think not so much.",
                    "label": 0
                },
                {
                    "sent": "In chess there's not so much learning going on there.",
                    "label": 0
                },
                {
                    "sent": "I mean, you just have, I mean the games, the end games, the beginning, the openings and and that research.",
                    "label": 1
                },
                {
                    "sent": "But in other games, especially the more probabilistic ones like Gammons.",
                    "label": 0
                },
                {
                    "sent": "And go this is done with machine learning techniques and robot locomotion.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So isn't everything.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Field, you can characterize different learning approaches or problem.",
                    "label": 0
                },
                {
                    "sent": "And possibly the most fundamental categorization is in these three categories.",
                    "label": 0
                },
                {
                    "sent": "One is supervised learning, which are.",
                    "label": 0
                },
                {
                    "sent": "Will talk mostly about and probably 80% of machine learning is about supervised learning.",
                    "label": 0
                },
                {
                    "sent": "That's why you have a teacher which tells you the answer on the training examples, like in the handwritten character recognition case you know the answer for the training set.",
                    "label": 0
                },
                {
                    "sent": "Then there's unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "If there's no teacher telling you what the truth is.",
                    "label": 0
                },
                {
                    "sent": "One interesting question is, I mean, what's really the goal?",
                    "label": 0
                },
                {
                    "sent": "Can you well define this problem?",
                    "label": 0
                },
                {
                    "sent": "And in a certain sense you can.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you learn associations or clustering density estimation.",
                    "label": 1
                },
                {
                    "sent": "So you try to find patterns in your data.",
                    "label": 0
                },
                {
                    "sent": "In supervised learning of classification and regression.",
                    "label": 1
                },
                {
                    "sent": "And then there's reinforcement learning in between.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "The environment only provides a reward signal telling you whether your action or decision or whatever, and was good or bad, not what you should have done, but whether it was good or bad and you should learn from that.",
                    "label": 0
                },
                {
                    "sent": "So this characteristic.",
                    "label": 0
                },
                {
                    "sent": "And in my second lecture next week on Friday evening, I will talk a lot about this case about reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And there are others which are mostly mixtures of this.",
                    "label": 1
                },
                {
                    "sent": "OK, so the supervised learning.",
                    "label": 0
                },
                {
                    "sent": "In supervised learning, the task is to predict the output from.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Put.",
                    "label": 0
                },
                {
                    "sent": "Or first I mean to detect the rule for that.",
                    "label": 1
                },
                {
                    "sent": "Or yeah, that was the second point.",
                    "label": 0
                },
                {
                    "sent": "I'm the rule is either used for prediction or sometimes used for compressing the data.",
                    "label": 1
                },
                {
                    "sent": "I mean, if you have a rule you can code your data more shortly and you don't have to keep the data around.",
                    "label": 0
                },
                {
                    "sent": "I'm another application.",
                    "label": 1
                },
                {
                    "sent": "A more category in supervised learning is outlier detection, which is sort of a classification where one class where you have only very few instances.",
                    "label": 0
                },
                {
                    "sent": "So you have, you know a lot of emails which are nowadays most emails or spam or a few emails are good and all the good ones are the outliers or the other way around.",
                    "label": 0
                },
                {
                    "sent": "The extreme cases.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our classification it's a simple example.",
                    "label": 0
                },
                {
                    "sent": "For instance, say you want to determine whether a certain customer is worth credit and you know his income and the savings.",
                    "label": 0
                },
                {
                    "sent": "And if he has a high income and say a lot of savings, then there's low risk to giving him alone and otherwise if there's a low income or low savings then there's a high risk.",
                    "label": 0
                },
                {
                    "sent": "If you just have, you know this binary decision giving him alone or not.",
                    "label": 0
                },
                {
                    "sent": "And in this case you could write down the rule.",
                    "label": 0
                },
                {
                    "sent": "So if the income is larger than the other one and the savings large, and there are two then low risk else high risk.",
                    "label": 0
                },
                {
                    "sent": "So that's a classification problem.",
                    "label": 0
                },
                {
                    "sent": "And what you would like, rather than writing down this rules and that ever wanted it to, you want to learn from past experience from your customers?",
                    "label": 0
                },
                {
                    "sent": "You have the income and savings of your customers and better they in the past were able to pay back the loan or not.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in regression, so in classification your output is.",
                    "label": 0
                },
                {
                    "sent": "Discrete often binary, so minus and plus.",
                    "label": 0
                },
                {
                    "sent": "And in regression, what you have is you have an input variable X and an output variable Y and you have a functional relation.",
                    "label": 0
                },
                {
                    "sent": "Y is equal to F of X.",
                    "label": 0
                },
                {
                    "sent": "In nearly all cases you have noise.",
                    "label": 0
                },
                {
                    "sent": "I want to find a function which nicely fits through the data points.",
                    "label": 0
                },
                {
                    "sent": "But not perfectly, because, um, you have the noise, and if you would, I mean, if you would take a perfect curve.",
                    "label": 0
                },
                {
                    "sent": "Would look like this, but this is probably not the real functional relationship.",
                    "label": 0
                },
                {
                    "sent": "I come to that later.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pectolite later.",
                    "label": 0
                },
                {
                    "sent": "So unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "One example is clustering.",
                    "label": 0
                },
                {
                    "sent": "You have data points here.",
                    "label": 0
                },
                {
                    "sent": "Data points here and you want to just, you know, identify the groups.",
                    "label": 0
                },
                {
                    "sent": "So for example customer segmentation.",
                    "label": 1
                },
                {
                    "sent": "I mean you have customers there, young customers, male customers, female customers and but you don't know this information to just have your data points that behavior say how they click in the Internet and you want to group them into these categories and then send specific ads for instance.",
                    "label": 0
                },
                {
                    "sent": "But, and you also don't know apriori what is a good grouping, I mean male versus female, maybe good, maybe not young versus old, but you don't know that our priority, so you want to learn that from your data yourself itself.",
                    "label": 0
                },
                {
                    "sent": "Find reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "I mentioned it already up.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apart from that, you only have very scarce feedback, namely, teacher tells you whether it was good or bad.",
                    "label": 0
                },
                {
                    "sent": "You have many other additional complications.",
                    "label": 0
                },
                {
                    "sent": "Normally you have temporal dependence.",
                    "label": 0
                },
                {
                    "sent": "Either enforcement learning is used when you have an agent which interacts with environment in cycles, and occasionally you get the rewards they assume you play chess and at the end of the chess game the agent gets a reward positive reward.",
                    "label": 0
                },
                {
                    "sent": "If it wins Anna negatively, but if it loses so, then the problem is.",
                    "label": 0
                },
                {
                    "sent": "Which moves in the game were responsible for winning or losing?",
                    "label": 0
                },
                {
                    "sent": "So the agent has to find out.",
                    "label": 0
                },
                {
                    "sent": "By himself also this.",
                    "label": 0
                },
                {
                    "sent": "So that's the.",
                    "label": 0
                },
                {
                    "sent": "That's.",
                    "label": 0
                },
                {
                    "sent": "The credit assignment problem.",
                    "label": 0
                },
                {
                    "sent": "So which action in the past?",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "Which action in the past or action sequence is responsible for the reward later?",
                    "label": 0
                },
                {
                    "sent": "Additional difficulties you have if you have multiple agents or two agents playing against each other for reinforcement learning is mostly used or developed for the case you have one learning agents playing against the static.",
                    "label": 1
                },
                {
                    "sent": "Environment, I mean static means it can be probabilistic.",
                    "label": 0
                },
                {
                    "sent": "It can change overtime, but you more or less know it and you can prove a lot of things about it.",
                    "label": 1
                },
                {
                    "sent": "But then in game playing you have two learning agents, possibly playing against each other.",
                    "label": 0
                },
                {
                    "sent": "That makes it gives an additional complication.",
                    "label": 0
                },
                {
                    "sent": "So, so this was the course characterization, supervised learning, reinforcement learning, unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "And if you have a problem, you should first check in which class it falls.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "But there are a lot of other.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The bottom is for instance.",
                    "label": 0
                },
                {
                    "sent": "I mean the most other most important.",
                    "label": 0
                },
                {
                    "sent": "Other one is probably the machine learning statistical side versus the logic.",
                    "label": 1
                },
                {
                    "sent": "Good old patient AI side.",
                    "label": 0
                },
                {
                    "sent": "Then the distinction whether you want to do induction that is just learning your model and then that's it, and you give the model to somebody else, or you really want to do use the model for prediction.",
                    "label": 0
                },
                {
                    "sent": "Or you could just skip this step and predict directly.",
                    "label": 0
                },
                {
                    "sent": "That's transductive learning.",
                    "label": 0
                },
                {
                    "sent": "But then you can ask your question, why do you want to predict what is a good prediction actually?",
                    "label": 0
                },
                {
                    "sent": "And this is also not so trivial.",
                    "label": 0
                },
                {
                    "sent": "It depends on your loss function.",
                    "label": 0
                },
                {
                    "sent": "So what you ultimately the ultimate reason for wanting to predict something is that you want to make some decisions.",
                    "label": 0
                },
                {
                    "sent": "So you predict the weather for tomorrow 'cause you want to decide in the morning whether you take an umbrella or sunglasses.",
                    "label": 0
                },
                {
                    "sent": "And then you have some loss function.",
                    "label": 0
                },
                {
                    "sent": "I mean taking an umbrella.",
                    "label": 0
                },
                {
                    "sent": "It's sunny is probably still better than you know, taking your sunglasses it rains, so you have a loss matrix and depending on that and this influences your decision.",
                    "label": 0
                },
                {
                    "sent": "And then ultimately I made this decision leads to an action.",
                    "label": 0
                },
                {
                    "sent": "Which may influence the environment, so it's also good to know you know which of these problems you're actually attacking.",
                    "label": 0
                },
                {
                    "sent": "There's regression versus classification already mentioned.",
                    "label": 0
                },
                {
                    "sent": "Most of machine learning is about identically independent distributed data.",
                    "label": 0
                },
                {
                    "sent": "That means you get a data point and then you get another data point in their sample from the distributions and these are independent.",
                    "label": 0
                },
                {
                    "sent": "Made identically distributed, probably.",
                    "label": 0
                },
                {
                    "sent": "Maybe 95% of machine learning is about this here.",
                    "label": 0
                },
                {
                    "sent": "I'm sequence prediction or the agent setup is non IID.",
                    "label": 0
                },
                {
                    "sent": "There's also online versus offline offline is you train your system on your training data you have.",
                    "label": 0
                },
                {
                    "sent": "Then you fix everything in you sell your product and it doesn't learn anymore.",
                    "label": 0
                },
                {
                    "sent": "Online means that it learns over the lifetime.",
                    "label": 0
                },
                {
                    "sent": "So it makes maybe a prediction and then it receives the true label.",
                    "label": 0
                },
                {
                    "sent": "For instance of weather forecasting system.",
                    "label": 0
                },
                {
                    "sent": "And it makes a prediction at the next day.",
                    "label": 0
                },
                {
                    "sent": "It knows the weather at the previous day and it could use this information to improve itself.",
                    "label": 0
                },
                {
                    "sent": "I guess most commercial weather forecasting systems are not like that.",
                    "label": 0
                },
                {
                    "sent": "It's developed by machine learning techniques at the University and then sold and statically fixed.",
                    "label": 0
                },
                {
                    "sent": "So this would be an offline approach.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 1
                },
                {
                    "sent": "This is related passive prediction.",
                    "label": 0
                },
                {
                    "sent": "Active learning to this year above.",
                    "label": 0
                },
                {
                    "sent": "Passive prediction in this context means that your prediction does not influence the environment.",
                    "label": 0
                },
                {
                    "sent": "Jamaica Weather forecasting prediction.",
                    "label": 0
                },
                {
                    "sent": "You take an umbrella, your sunglass, but this will not affect the weather tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Except for the butterfly effect, so let's ignore that.",
                    "label": 0
                },
                {
                    "sent": "Where does active learning is where you have an agent which makes a prediction than an action like in chess playing.",
                    "label": 0
                },
                {
                    "sent": "But this influences what your opponent does.",
                    "label": 0
                },
                {
                    "sent": "And this active learning, I would say.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you want to quantify this, at least you know.",
                    "label": 0
                },
                {
                    "sent": "10 or 100 times more complex than the passive case, so it makes things really much more complicated.",
                    "label": 0
                },
                {
                    "sent": "Parametric versus nonparametric parametric is also the domain of statistics.",
                    "label": 0
                },
                {
                    "sent": "You have some model with a few parameters and you want to identify the parameters.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric is say you want to fit a function for data points, but a pro you have no functional class or any function is a pretty good or considered.",
                    "label": 0
                },
                {
                    "sent": "What is the best function?",
                    "label": 0
                },
                {
                    "sent": "Then of course, as always, you could consider more than mathematic conceptual issues or the computational issues.",
                    "label": 0
                },
                {
                    "sent": "That's also.",
                    "label": 0
                },
                {
                    "sent": "I mean, statistics is more on this side and machine learning is more this side.",
                    "label": 0
                },
                {
                    "sent": "Also, exactly this principled.",
                    "label": 0
                },
                {
                    "sent": "So probably machine learning is also more on this side and but, but the fields are merging and OK.",
                    "label": 0
                },
                {
                    "sent": "This picture to me I had before.",
                    "label": 0
                },
                {
                    "sent": "OK, now I have.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two more slides before I go into more detail.",
                    "label": 0
                },
                {
                    "sent": "That's about probability basics.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Is there anyone who has not had a probability or statistics lecture in the past?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK one OK, this slide is good, at least for one machine.",
                    "label": 0
                },
                {
                    "sent": "Learning is heavily based on uncertainty and uncertainty is characterized by probabilities, so that just gives you a.",
                    "label": 0
                },
                {
                    "sent": "Brief reminder about the terminology.",
                    "label": 0
                },
                {
                    "sent": "So what you have to say, take a die which you roll.",
                    "label": 0
                },
                {
                    "sent": "The sample space is the possible outcomes one to six events are subsets of the sample space.",
                    "label": 0
                },
                {
                    "sent": "For instance, the event that an even number turns up.",
                    "label": 0
                },
                {
                    "sent": "So that would be 2 for sex or odd number would be 135.",
                    "label": 0
                },
                {
                    "sent": "Then the probability of six is.",
                    "label": 0
                },
                {
                    "sent": "I mean in this case obviously 16.",
                    "label": 0
                },
                {
                    "sent": "This point is annoying.",
                    "label": 0
                },
                {
                    "sent": "The probability of even is OK. Three out of six is 1/2.",
                    "label": 0
                },
                {
                    "sent": "Six would be the outcome and conditional probabilities are computed by ratios of probabilities or probability of a given.",
                    "label": 0
                },
                {
                    "sent": "B is a probably with A&B divided by the probability of B.",
                    "label": 0
                },
                {
                    "sent": "So 6 and even OK, it's just six, so this is 1, six and even is 1/2.",
                    "label": 0
                },
                {
                    "sent": "So this is 1/3.",
                    "label": 0
                },
                {
                    "sent": "And here's the general probability axioms.",
                    "label": 0
                },
                {
                    "sent": "And the probability is between zero and one.",
                    "label": 0
                },
                {
                    "sent": "If, say, A&B are disjunct.",
                    "label": 0
                },
                {
                    "sent": "Then the probability of a Union B is the sum of probabilities and this is the general form, and this is the definition of conditional probability I just mentioned.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the very.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basics and he is more probability jargon.",
                    "label": 1
                },
                {
                    "sent": "Which may be new to some of the logicians.",
                    "label": 0
                },
                {
                    "sent": "So let's consider calling here.",
                    "label": 0
                },
                {
                    "sent": "Make it even simpler.",
                    "label": 0
                },
                {
                    "sent": "So coin can have tail or hat.",
                    "label": 1
                },
                {
                    "sent": "And this is to be identified with zero and one, and assume it's an unfair coin.",
                    "label": 0
                },
                {
                    "sent": "In this probability of head is Theta.",
                    "label": 0
                },
                {
                    "sent": "So Taylor can be anything between zero and one before a fair coin is of course 1/2.",
                    "label": 0
                },
                {
                    "sent": "So the likelihood of observing, say 11014 coin which has buyers Petta.",
                    "label": 0
                },
                {
                    "sent": "Is that OK?",
                    "label": 0
                },
                {
                    "sent": "The probability of one is status or your theater times Theta times.",
                    "label": 0
                },
                {
                    "sent": "The probability of zero is 1 minus Theta and then another one is tighter, so the probability is Theta cubed times 1 minus Theta and that's called the likelihood.",
                    "label": 0
                },
                {
                    "sent": "The maximum likelihood estimate is, as the name says, takes that at a which maximizes the likelihood, and this would be here.",
                    "label": 1
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you look at this function theater 3 * 1 -- 10 to the maximum is at 3 / 4.",
                    "label": 0
                },
                {
                    "sent": "Which is just the relative frequency.",
                    "label": 0
                },
                {
                    "sent": "So you count the number of heads divided by the sequence length.",
                    "label": 0
                },
                {
                    "sent": "So this is 3 four.",
                    "label": 0
                },
                {
                    "sent": "So if you don't know the buyers of the coin are reasonable, estimate is to look at the relative frequency.",
                    "label": 0
                },
                {
                    "sent": "OK, for Beijing prediction you also need a, so that's the frequentist approach.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "How so?",
                    "label": 0
                },
                {
                    "sent": "So what I take this function here?",
                    "label": 0
                },
                {
                    "sent": "This is this function which looks like.",
                    "label": 0
                },
                {
                    "sent": "This one this is P of 1101 given Theta which is Theta to the three, 1 minus Theta.",
                    "label": 0
                },
                {
                    "sent": "So it's a cubic function here.",
                    "label": 0
                },
                {
                    "sent": "And it's a linear function here, so it.",
                    "label": 0
                },
                {
                    "sent": "Looks like this.",
                    "label": 0
                },
                {
                    "sent": "And the maximum.",
                    "label": 0
                },
                {
                    "sent": "You can show.",
                    "label": 0
                },
                {
                    "sent": "Is at .75.",
                    "label": 0
                },
                {
                    "sent": "And you take this as an estimate, and that's I mean you can ask wise is a cost estimate and you can argue for that, improve various things.",
                    "label": 0
                },
                {
                    "sent": "Or you can just take it as a principle.",
                    "label": 1
                },
                {
                    "sent": "So the maximum likelihood principle tells you to take this as an estimate.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So if you do Bayesian statistics, you also need a prior over Theta, so assume.",
                    "label": 0
                },
                {
                    "sent": "You take a physical coin.",
                    "label": 0
                },
                {
                    "sent": "And you throw and you get 3 ones and 10.",
                    "label": 0
                },
                {
                    "sent": "Would you say or would you estimate the probability of her to be three 4th?",
                    "label": 0
                },
                {
                    "sent": "Probably not, you would say, OK, I mean that's just bad luck.",
                    "label": 0
                },
                {
                    "sent": "Or I mean this is quite normal.",
                    "label": 0
                },
                {
                    "sent": "It's not even very exotic 'cause you have a strong bias towards fair coins.",
                    "label": 0
                },
                {
                    "sent": "I mean you can look at the coin and you know that's fair or even if you don't look typically a coin is quite fair.",
                    "label": 0
                },
                {
                    "sent": "So what you have is before you throw the coin.",
                    "label": 0
                },
                {
                    "sent": "Before you throw the coin, you have a prior knowledge and.",
                    "label": 0
                },
                {
                    "sent": "With hype prior belief you coin is fair and unfair coins are quite unlikely.",
                    "label": 0
                },
                {
                    "sent": "So this is, I mean this is not a Delta P for something which is query I mean.",
                    "label": 0
                },
                {
                    "sent": "Very high, but you know maybe 100, maybe 99 out of 100 coins is fair.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean it's a density so.",
                    "label": 0
                },
                {
                    "sent": "Forget about the scale.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in this prior knowledge should be taken into account and there is one disadvantage of the maximum likelihood approach.",
                    "label": 0
                },
                {
                    "sent": "It does not take this prior knowledge into account.",
                    "label": 0
                },
                {
                    "sent": "So if we are indifferent, then we can choose P of Theta equal to constant and if not we choose it.",
                    "label": 1
                },
                {
                    "sent": "Maybe you know like this or whatever.",
                    "label": 1
                },
                {
                    "sent": "My prior knowledge is.",
                    "label": 0
                },
                {
                    "sent": "Then you can compute the evidence which is integrating out the data so you have the probability of a given B times the probability of B, summed over all possible bees, give you the probability of a.",
                    "label": 0
                },
                {
                    "sent": "So that's just normal some rule, and if you do that you get 1 / 20.",
                    "label": 0
                },
                {
                    "sent": "Actually, since status continues this summer is integral in the probabilities of densities and so on.",
                    "label": 0
                },
                {
                    "sent": "But forget about these technicalities.",
                    "label": 0
                },
                {
                    "sent": "This evidence is on the one hand very important.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, it's very hard to get the real meaning out of it, so don't try.",
                    "label": 0
                },
                {
                    "sent": "Now at least OK, but from the evidence you can compute the posterior.",
                    "label": 0
                },
                {
                    "sent": "What is the probability of Theta after you have seen 1101 and you just apply two times the definition of conditional probability.",
                    "label": 0
                },
                {
                    "sent": "So you have the probability of a given B.",
                    "label": 0
                },
                {
                    "sent": "Times the probability of B was the probability of A&BI mean.",
                    "label": 0
                },
                {
                    "sent": "That's the definition of the conditional probability, and then you would use it in reverse form.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "And divide.",
                    "label": 0
                },
                {
                    "sent": "You divide by P of B. PFB and that's it.",
                    "label": 0
                },
                {
                    "sent": "And that flips around the data from this side.",
                    "label": 0
                },
                {
                    "sent": "So the A was the day was the data.",
                    "label": 0
                },
                {
                    "sent": "And now they is on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "Is that its base rule?",
                    "label": 0
                },
                {
                    "sent": "I will come back to that in more detail on our next Friday.",
                    "label": 0
                },
                {
                    "sent": "So now you can compute your posterior belief.",
                    "label": 0
                },
                {
                    "sent": "Of Theta given your sequence.",
                    "label": 0
                },
                {
                    "sent": "If you have a uniform prior like I assumed here, you get exactly the same functional and the maximum posteriori estimates now maximize this posterior rather than the likely to maximize the posterior.",
                    "label": 0
                },
                {
                    "sent": "So you see, if you have the privacy uniform, you get the same answer, but if you plug in a prior like this.",
                    "label": 0
                },
                {
                    "sent": "You would get a different answer and you can do that if you want to, and if you take a prior like this, one characterization would be.",
                    "label": 0
                },
                {
                    "sent": "One nice way to characterize these prices cater to the A1 minus Theta to the better.",
                    "label": 0
                },
                {
                    "sent": "So take P of Theta is proportional tool and you play around with Alpha and beta until you get a function which you like.",
                    "label": 0
                },
                {
                    "sent": "I mean, which you know corresponds roughly to your prior knowledge, because if you use these, you can compute the integrals nicely and then check what you get and you get something which is much closer to 1/2.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you then have more evidence, a lot along sequence, then it will start to deviate from 1/2 because I mean if you throw a coin 1000 times and you have 900 times or one and only 100 times a zero, then you get suspicious.",
                    "label": 0
                },
                {
                    "sent": "In the Bayesian updating I mean correctly.",
                    "label": 0
                },
                {
                    "sent": "And catchers this.",
                    "label": 0
                },
                {
                    "sent": "Belief.",
                    "label": 1
                },
                {
                    "sent": "OK, the predictive distribution is.",
                    "label": 0
                },
                {
                    "sent": "What you really?",
                    "label": 0
                },
                {
                    "sent": "Ultimately, what you want to predict, the probability of the next coin or the next flip.",
                    "label": 0
                },
                {
                    "sent": "So what is appropriate one given?",
                    "label": 0
                },
                {
                    "sent": "The previous fourth rows, which you can compute as this ratio here.",
                    "label": 0
                },
                {
                    "sent": "It's again the definition of conditional probability, which would be 2 third.",
                    "label": 0
                },
                {
                    "sent": "For instance here expected values is just, you know, by definition this here the expected value of a function.",
                    "label": 0
                },
                {
                    "sent": "Is F of Theta times the probability test or something over it?",
                    "label": 0
                },
                {
                    "sent": "Which would be here also to 3rd?",
                    "label": 0
                },
                {
                    "sent": "The variance is.",
                    "label": 0
                },
                {
                    "sent": "The myth.",
                    "label": 0
                },
                {
                    "sent": "Of this distribution with squared, which compute in this way and probability densities are fine like this.",
                    "label": 0
                },
                {
                    "sent": "I mean you have a very short interval.",
                    "label": 0
                },
                {
                    "sent": "You ask what is the probability divided by the length of the interval.",
                    "label": 0
                },
                {
                    "sent": "Disk issues, again city.",
                    "label": 0
                },
                {
                    "sent": "If that or if some of these things are new to you, then I recommend that you just look it up in Wikipedia over the weekend.",
                    "label": 0
                },
                {
                    "sent": "Because I know, I mean, that was very quick.",
                    "label": 0
                },
                {
                    "sent": "So let's come to Linea.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Methods of regression.",
                    "label": 0
                },
                {
                    "sent": "So are there any further questions so far?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So linear regression.",
                    "label": 0
                },
                {
                    "sent": "What you have is you have data points in some.",
                    "label": 0
                },
                {
                    "sent": "High dimensional or low dimensional space and you want to fit the plane through the data points?",
                    "label": 1
                },
                {
                    "sent": "That's linear regression.",
                    "label": 0
                },
                {
                    "sent": "What you do is you typically have features called feature vector, say X0X1 up to HD.",
                    "label": 1
                },
                {
                    "sent": "So here you have X1 and X2, so maybe X one is the H and X2 is the height of a person and in the Y axis you have the response variable which is often noisy.",
                    "label": 0
                },
                {
                    "sent": "For instance that could be the person's weight which is a function of H and height and you want to.",
                    "label": 1
                },
                {
                    "sent": "Detect the dependency and assume hopefully that it's linear.",
                    "label": 0
                },
                {
                    "sent": "Then you make the following on.",
                    "label": 0
                },
                {
                    "sent": "And that's so.",
                    "label": 0
                },
                {
                    "sent": "And this is a nice German word which is used in physics but not outside physics.",
                    "label": 0
                },
                {
                    "sent": "And that is an equation.",
                    "label": 0
                },
                {
                    "sent": "Which or model which contains some free parameters which you want to determine later?",
                    "label": 0
                },
                {
                    "sent": "So there's no good English word for that, so that's an answer.",
                    "label": 0
                },
                {
                    "sent": "Here you have this linear function, so you have a weighted average over your coordinates X0 up to XD which gives you Y and you want to determine the coefficients here.",
                    "label": 0
                },
                {
                    "sent": "So it's very convenient to introduce an extra coordinate X0 which is identically one which gives you the upset.",
                    "label": 0
                },
                {
                    "sent": "So this is just one here, so that's the offset.",
                    "label": 0
                },
                {
                    "sent": "So now you have your data points X1Y one up to X&YN.",
                    "label": 0
                },
                {
                    "sent": "These are these red points.",
                    "label": 0
                },
                {
                    "sent": "When you do regression.",
                    "label": 0
                },
                {
                    "sent": "And you want to best fit your data.",
                    "label": 0
                },
                {
                    "sent": "You have to define what best means you into characterization.",
                    "label": 1
                },
                {
                    "sent": "And you do that with the loss function.",
                    "label": 0
                },
                {
                    "sent": "So the most simple loss function of quadratic loss you compare the true value Y with what this function would predict.",
                    "label": 0
                },
                {
                    "sent": "So you plug in the XY, you compute the estimate and you compare to the true value and take the square and you sum it over all data points.",
                    "label": 0
                },
                {
                    "sent": "So that's the square loss.",
                    "label": 0
                },
                {
                    "sent": "And it's reasonable to try to find the function.",
                    "label": 1
                },
                {
                    "sent": "So that means to try to find the parameters in such a way that this loss is minimized.",
                    "label": 0
                },
                {
                    "sent": "So that's here.",
                    "label": 0
                },
                {
                    "sent": "So this least square regression.",
                    "label": 0
                },
                {
                    "sent": "And this you can compute this just gives you a linear equation and this results in this case.",
                    "label": 0
                },
                {
                    "sent": "Pretty boring, isn't it?",
                    "label": 0
                },
                {
                    "sent": "But yeah, unfortunately or Fortunately very useful.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's make it more interesting.",
                    "label": 0
                },
                {
                    "sent": "Let's assume you have.",
                    "label": 0
                },
                {
                    "sent": "A lot of dimensions, so for instance.",
                    "label": 0
                },
                {
                    "sent": "Now I'm in spam classifications, more our classification problem, but it doesn't really matter for this purpose.",
                    "label": 0
                },
                {
                    "sent": "So for instance, you have the bag of words model, so each word is a coordinate.",
                    "label": 0
                },
                {
                    "sent": "An accordion can be present or not.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then the dimension would be the number of possible words you have.",
                    "label": 0
                },
                {
                    "sent": "Or maybe a better example.",
                    "label": 0
                },
                {
                    "sent": "In bioinformatics you have these microarrays which can probe.",
                    "label": 0
                },
                {
                    "sent": "I think the best chips.",
                    "label": 0
                },
                {
                    "sent": "Now I have 500,000.",
                    "label": 0
                },
                {
                    "sent": "I also don't know the details this base sequences they can prop so you have a feature vector 500,000.",
                    "label": 0
                },
                {
                    "sent": "So these 500,000 on the other hand, your data sizes maybe especially by Infomatics 100 or at most 300.",
                    "label": 0
                },
                {
                    "sent": "So you have 500,000 dimensions but just three in our data points.",
                    "label": 0
                },
                {
                    "sent": "And as you can easily see if you have more data and more dimensions and data points, you can always fit the perfect plane through it.",
                    "label": 0
                },
                {
                    "sent": "And if you think about it, it can't be good.",
                    "label": 0
                },
                {
                    "sent": "That's the problem of overfitting.",
                    "label": 0
                },
                {
                    "sent": "OK, why is it bad to fit perfectly?",
                    "label": 0
                },
                {
                    "sent": "OK, perfect fit.",
                    "label": 0
                },
                {
                    "sent": "Sounds good, but if you try to use this model for prediction you will realize it will make very poor predictions.",
                    "label": 0
                },
                {
                    "sent": "Um, OK one way.",
                    "label": 0
                },
                {
                    "sent": "To overcome this problem would be to identify a smaller subset of these features.",
                    "label": 1
                },
                {
                    "sent": "So what could try to fix this on K?",
                    "label": 0
                },
                {
                    "sent": "Say three more data points.",
                    "label": 0
                },
                {
                    "sent": "Then you say fix or assume that there are 10 important features or 10 important dimensions.",
                    "label": 1
                },
                {
                    "sent": "You try all you try, I mean 10 out of 500,000.",
                    "label": 0
                },
                {
                    "sent": "Features all combinations you compute the least square and you take the one which is minimal.",
                    "label": 1
                },
                {
                    "sent": "So it would be the most simple coefficient subset selection.",
                    "label": 0
                },
                {
                    "sent": "But you have to fix Ki mean if you also allow varying K then of course you get cake will be because it always gives a perfect fit.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, I will tell you later.",
                    "label": 0
                },
                {
                    "sent": "That's the model selection in I'll come to in Section 4.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, another possibility would be.",
                    "label": 0
                },
                {
                    "sent": "So you have these weights so, so each dimension is associated weight.",
                    "label": 0
                },
                {
                    "sent": "What you could do is you had a penalty term and say if the rates are two too many rates are too large, your loss gets penalized, so you add it to the loss so you get a bias towards smaller weights.",
                    "label": 0
                },
                {
                    "sent": "In rich regression, you use a quadratic loss, which effectively shrinks all coefficients, but no coefficient really get 0.",
                    "label": 0
                },
                {
                    "sent": "If you use a one norm that's called Lasso.",
                    "label": 0
                },
                {
                    "sent": "Then what you will see is that many of these coefficients will be zero and only a few will be non 0 but then have different values.",
                    "label": 0
                },
                {
                    "sent": "So that's so lost lives in between coefficient subset selection and Ridge regression, it shrinks the coefficients but many will be shrunken to 0, which is effectively the same as throwing dimensions out.",
                    "label": 0
                },
                {
                    "sent": "Invasion regression.",
                    "label": 0
                },
                {
                    "sent": "What you do is you take a prior over your weights, so maybe you know that some features are proven more important than others, or even if not, you have noninformative priors.",
                    "label": 0
                },
                {
                    "sent": "And then you compute the posterior.",
                    "label": 0
                },
                {
                    "sent": "So this is the likelihood times the prior divided by normalization constant.",
                    "label": 0
                },
                {
                    "sent": "And then you take the maximum over W. And what you have.",
                    "label": 0
                },
                {
                    "sent": "Here is that.",
                    "label": 0
                },
                {
                    "sent": "You cannot give.",
                    "label": 0
                },
                {
                    "sent": "A high prior to always, so most of these weights will have a small prior and if you multiply the privacy likelihood, this gives you something which is small.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Penalize these weights because you want to find a maximum so all this also gives you a.",
                    "label": 0
                },
                {
                    "sent": "A bias towards those weights which appear or more likely, and if you choose the prior appropriately, these other smaller weights.",
                    "label": 0
                },
                {
                    "sent": "So just remember there are three important.",
                    "label": 0
                },
                {
                    "sent": "I mean there are many more ways, but these are the three most important regression, Lasso invasion, linear regression to overcome the problem of.",
                    "label": 0
                },
                {
                    "sent": "Too many dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that was linear methods for regression.",
                    "label": 1
                },
                {
                    "sent": "So what we do with classification?",
                    "label": 0
                },
                {
                    "sent": "The one thing we could do is reduce it to regression, so classification, for instance, we want to classify emails with respect to spam or not spam.",
                    "label": 0
                },
                {
                    "sent": "And again, let's use minus one or one or sometimes 01 for the labels.",
                    "label": 0
                },
                {
                    "sent": "And you just regard minus one and one as a real number.",
                    "label": 0
                },
                {
                    "sent": "I mean there are real numbers and then you do regression and you're done.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's hard to draw in many dimensions, but in one easy to draw.",
                    "label": 0
                },
                {
                    "sent": "So assume you have the data points like this, so this is.",
                    "label": 0
                },
                {
                    "sent": "I'll make it.",
                    "label": 0
                },
                {
                    "sent": "This is 1.",
                    "label": 0
                },
                {
                    "sent": "This is minus one.",
                    "label": 0
                },
                {
                    "sent": "And you just have one dimension and.",
                    "label": 0
                },
                {
                    "sent": "So if it looks like this.",
                    "label": 0
                },
                {
                    "sent": "And you regard it as a regression problem.",
                    "label": 0
                },
                {
                    "sent": "Then the best linear line through these data points is possibly.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "And it doesn't fit very well, but OK, that's the best you can do with linear regression.",
                    "label": 0
                },
                {
                    "sent": "And then you say the function is positive.",
                    "label": 0
                },
                {
                    "sent": "You classify it plus one.",
                    "label": 0
                },
                {
                    "sent": "And if the function is negative, you classify as minus one, and that's not too bad.",
                    "label": 0
                },
                {
                    "sent": "I mean all these right ones are classified correctly and here on the left side you have.",
                    "label": 0
                },
                {
                    "sent": "Two wrong once.",
                    "label": 0
                },
                {
                    "sent": "The problem with this knife approach is that you know if it extends very far to the side.",
                    "label": 0
                },
                {
                    "sent": "And you use the square error.",
                    "label": 0
                },
                {
                    "sent": "This is a huge error.",
                    "label": 0
                },
                {
                    "sent": "And because it squared.",
                    "label": 0
                },
                {
                    "sent": "You get a large loss, So what happens is if you minimize this loss, these points at the very end.",
                    "label": 0
                },
                {
                    "sent": "The influence line mostly so the linear regression tries to push the line down here and here at the very end where it's obvious anyway how to classify.",
                    "label": 0
                },
                {
                    "sent": "While the points and the critical boundary.",
                    "label": 0
                },
                {
                    "sent": "They are more or less ignored because I mean, whether you're one away or you know, zero, what is doesn't make big difference.",
                    "label": 0
                },
                {
                    "sent": "If it's here 1000 or something, so it just concentrated their effort on the wrong data points.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, things can go really wrong.",
                    "label": 0
                },
                {
                    "sent": "But OK, it's the 1st.",
                    "label": 0
                },
                {
                    "sent": "It's worth a first trial.",
                    "label": 1
                },
                {
                    "sent": "So OK, so for binary classification function is spent solely to linear regression, the functions positive you classify as one in negative is minus one.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you already I mean.",
                    "label": 0
                },
                {
                    "sent": "I have a function now which is not just plus or minus.",
                    "label": 0
                },
                {
                    "sent": "One gives you a real number you could think of.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should output a probability of the label being one or minus one, which you could easily do.",
                    "label": 0
                },
                {
                    "sent": "You just interpret.",
                    "label": 0
                },
                {
                    "sent": "The value of this function is a probability, since probabilities are between zero and one.",
                    "label": 0
                },
                {
                    "sent": "You take some simple transform.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the lock of the probability of 1 divided by probability zero.",
                    "label": 0
                },
                {
                    "sent": "You say this is the function value and so you can solve this equation easily with respect to P of Y equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So you which is effectively what you do is.",
                    "label": 0
                },
                {
                    "sent": "So if this is your function value.",
                    "label": 0
                },
                {
                    "sent": "You say the probability is this.",
                    "label": 0
                },
                {
                    "sent": "So if the function value is very large to say, the probability is very high that the label is 1.",
                    "label": 0
                },
                {
                    "sent": "If the function value is very small, it said probably that's minus.",
                    "label": 0
                },
                {
                    "sent": "Sorry, probabilities, of course here.",
                    "label": 1
                },
                {
                    "sent": "So that's the probability that the label wise equal to 1.",
                    "label": 0
                },
                {
                    "sent": "I'm giving you data OK. Meet you compute from this function.",
                    "label": 0
                },
                {
                    "sent": "It functions very negative.",
                    "label": 0
                },
                {
                    "sent": "You say that it's likely to be 0, the label and if the function is close to zero, you assign a probability which is roughly 50%.",
                    "label": 0
                },
                {
                    "sent": "OK, given that this method has certain problems, there are a lot of other methods.",
                    "label": 0
                },
                {
                    "sent": "To do regression, there's linear discriminant analysis, which, as the name says, is still linear, but a little bit smarter, and all these others here.",
                    "label": 1
                },
                {
                    "sent": "Logistic, regression, perceptron, an support vector machines.",
                    "label": 1
                },
                {
                    "sent": "I will explain them later in more detail are nonlinear in nature.",
                    "label": 0
                },
                {
                    "sent": "And you can generalize to nonbinary by in several ways.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you have three categories, you could classify one against the other two, and then one against the other two, and so on in three ways.",
                    "label": 0
                },
                {
                    "sent": "And then you also have.",
                    "label": 1
                },
                {
                    "sent": "So you have to reduce it in this way to binary classification.",
                    "label": 0
                },
                {
                    "sent": "So here's a 2 dimensional example done with a linear discriminant analysis.",
                    "label": 0
                },
                {
                    "sent": "So you add labels 1, two and three.",
                    "label": 0
                },
                {
                    "sent": "And these are the.",
                    "label": 0
                },
                {
                    "sent": "Still, you do something like regression and then you threshold at zero and these are then there.",
                    "label": 0
                },
                {
                    "sent": "Lines where it passes.",
                    "label": 0
                },
                {
                    "sent": "I'm zero and it's a reasonable output.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now.",
                    "label": 0
                },
                {
                    "sent": "This slide tells you why linear regression is really, really, really powerful.",
                    "label": 0
                },
                {
                    "sent": "Becaused you can.",
                    "label": 0
                },
                {
                    "sent": "Use it in the following way, which is called linear basis function regression.",
                    "label": 1
                },
                {
                    "sent": "So even if you have a problem which doesn't look linear at all, it is often possible to reduce it to linear regression.",
                    "label": 0
                },
                {
                    "sent": "The standard example is.",
                    "label": 0
                },
                {
                    "sent": "Fitting a polynomial through data points.",
                    "label": 0
                },
                {
                    "sent": "So that may be a cubic polynomial fits nicely for this data point, and a cubic polynomial doesn't look particularly linear, but actually it's very easy to reduce that to linear regression.",
                    "label": 0
                },
                {
                    "sent": "What you do is the following.",
                    "label": 0
                },
                {
                    "sent": "In general you have your feature vector X.",
                    "label": 1
                },
                {
                    "sent": "You simply transform it to some new feature vector file of X.",
                    "label": 0
                },
                {
                    "sent": "So from a D dimensional space to run, possibly different dimensional space, and then you assume or hope that the response variable Y is now linear file.",
                    "label": 1
                },
                {
                    "sent": "That's the trick.",
                    "label": 1
                },
                {
                    "sent": "So linear in linear regression before the feature function is just the components of XI mean you just X one X2X3.",
                    "label": 0
                },
                {
                    "sent": "These are the feature phones, so it's identity transformation so nothing happens.",
                    "label": 0
                },
                {
                    "sent": "In polynomial regression, what you do is you say that Phi.",
                    "label": 0
                },
                {
                    "sent": "One of X is constant.",
                    "label": 0
                },
                {
                    "sent": "5 two of X is X5.",
                    "label": 0
                },
                {
                    "sent": "Three of X is X squared and say if you want to cubic.",
                    "label": 0
                },
                {
                    "sent": "Regression you take up 2X cube.",
                    "label": 0
                },
                {
                    "sent": "So also you have only One X, so here D was one.",
                    "label": 0
                },
                {
                    "sent": "So it just have 1X now P is equal to 4.",
                    "label": 0
                },
                {
                    "sent": "I can't plot that so.",
                    "label": 0
                },
                {
                    "sent": "So now I'm for each data point you have a feature vector which looks like.",
                    "label": 0
                },
                {
                    "sent": "You know this, you know 4 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And you have the Y value.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "You fit a linear hyperplane through this points.",
                    "label": 0
                },
                {
                    "sent": "And project back.",
                    "label": 0
                },
                {
                    "sent": "To this one dimensional.",
                    "label": 1
                },
                {
                    "sent": "Original feature vector and what you get get is this cubic line.",
                    "label": 0
                },
                {
                    "sent": "So other examples is, for instance piecewise constant regression.",
                    "label": 0
                },
                {
                    "sent": "In this case, the feature functional just step functions.",
                    "label": 0
                },
                {
                    "sent": "So what you have is if you want piecewise constant regression.",
                    "label": 0
                },
                {
                    "sent": "I mean this is.",
                    "label": 0
                },
                {
                    "sent": "Since 2 trivial you take as your feature functions.",
                    "label": 0
                },
                {
                    "sent": "You take step functions.",
                    "label": 0
                },
                {
                    "sent": "Which more laser indicator functions is my data?",
                    "label": 0
                },
                {
                    "sent": "Would you get in?",
                    "label": 0
                },
                {
                    "sent": "Think it is an indicator vector, say 001000, so that means that your data point X does not lie in the first blog, not lie in the second block, it lies in the third block does not lie in the photo and so on.",
                    "label": 0
                },
                {
                    "sent": "So your feature vectors are just binary and then you make a linear regression.",
                    "label": 0
                },
                {
                    "sent": "In this case it is.",
                    "label": 0
                },
                {
                    "sent": "I mean just averaging the data points so it's trivial.",
                    "label": 0
                },
                {
                    "sent": "But you can extend it to piecewise polynomials into splines, which I will do in the next slides.",
                    "label": 0
                },
                {
                    "sent": "So look at here.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that is the piecewise constant regression and with these base functions you can do piecewise linear.",
                    "label": 0
                },
                {
                    "sent": "You can do continuous.",
                    "label": 0
                },
                {
                    "sent": "Piecewise linear, which gives you.",
                    "label": 0
                },
                {
                    "sent": "I mean the points, obviously data points and the curve is the.",
                    "label": 0
                },
                {
                    "sent": "True function where the data points were sampled from this noise and the linear ones are there.",
                    "label": 0
                },
                {
                    "sent": "Regression curves.",
                    "label": 0
                },
                {
                    "sent": "And for the piecewise linear basis function, for instance, looks like this, you have this triangular functions.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So wavelets.",
                    "label": 0
                },
                {
                    "sent": "Bayflite regression is also linear regressions, linear basis, function regression, and the basis functions.",
                    "label": 0
                },
                {
                    "sent": "Are these here now?",
                    "label": 0
                },
                {
                    "sent": "So you have to interpret it to follow, I mean the following value.",
                    "label": 0
                },
                {
                    "sent": "The original space is 1 dimensional here, so that's your X.",
                    "label": 0
                },
                {
                    "sent": "And what you get is for each of these feature functions safer feature function 8 or something 1234568 you get a vector which consists of numbers 000000000 and negative numbers 0 positive numbers and so on.",
                    "label": 0
                },
                {
                    "sent": "So huge vector.",
                    "label": 0
                },
                {
                    "sent": "So this is a huge dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Then you do linear regression.",
                    "label": 0
                },
                {
                    "sent": "Which or another way to view it, is just taking a weighted.",
                    "label": 0
                },
                {
                    "sent": "Mixture over all these wavelets and you fit it through your original function.",
                    "label": 0
                },
                {
                    "sent": "And which gives you answer.",
                    "label": 0
                },
                {
                    "sent": "And and he is a 2 dimensional case, so the original space, these two dimensional and the wavelet spaces is large.",
                    "label": 0
                },
                {
                    "sent": "As the number of wave lifts you use.",
                    "label": 0
                },
                {
                    "sent": "So still or linear.",
                    "label": 0
                },
                {
                    "sent": "So yet now it gets a little bit more.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Exciting.",
                    "label": 0
                },
                {
                    "sent": "Very simple way.",
                    "label": 0
                },
                {
                    "sent": "I'm to do regression which is also linear is local smoothing.",
                    "label": 1
                },
                {
                    "sent": "So you have your data points here.",
                    "label": 0
                },
                {
                    "sent": "Just take a window from here to here.",
                    "label": 0
                },
                {
                    "sent": "Take the average of the Y values and this gives you.",
                    "label": 0
                },
                {
                    "sent": "You know your regression point and then you move the window around.",
                    "label": 0
                },
                {
                    "sent": "Which gives you the green this green function.",
                    "label": 0
                },
                {
                    "sent": "Becaused you have here.",
                    "label": 0
                },
                {
                    "sent": "So you average the Y values of the data points in a local neighborhood, and if you move the Windows data points go in and go out abruptly dysfunction still.",
                    "label": 0
                },
                {
                    "sent": "Wiggles alot you could improve that by taking a kernel.",
                    "label": 0
                },
                {
                    "sent": "Say you take a weighted average of these data points and you waited once feature in the middle more than on the on this side.",
                    "label": 0
                },
                {
                    "sent": "So if you move this window in the data board gets in its first rated.",
                    "label": 0
                },
                {
                    "sent": "Not very much and then more and more and more.",
                    "label": 0
                },
                {
                    "sent": "And then it fades out so you get a curve which is much more smoother as you see here.",
                    "label": 0
                },
                {
                    "sent": "So formally what you do is.",
                    "label": 0
                },
                {
                    "sent": "You estimate.",
                    "label": 0
                },
                {
                    "sent": "The function at Point X as a weighted average of the Y values and the weight.",
                    "label": 0
                },
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "Depends on the location of XI.",
                    "label": 0
                },
                {
                    "sent": "And the point X you want to estimate.",
                    "label": 0
                },
                {
                    "sent": "So in the in the nearest neighbor case you just say, I mean this kernel function is 1 if the point is close and 0 otherwise.",
                    "label": 1
                },
                {
                    "sent": "So you take all points and take a weighted average.",
                    "label": 0
                },
                {
                    "sent": "Or in this case, taking average, which is, I mean not weighted.",
                    "label": 0
                },
                {
                    "sent": "Or here in this case you have quadratic kernel, so zero if.",
                    "label": 0
                },
                {
                    "sent": "Your XI is farther than if you X is farther than a away then.",
                    "label": 0
                },
                {
                    "sent": "From XI and otherwise, you have this quadratic shape.",
                    "label": 0
                },
                {
                    "sent": "And if you look at it, I mean here you obviously see this linear, so you have a weighted average over the wise, and it's very easy.",
                    "label": 0
                },
                {
                    "sent": "I mean, the coefficient weights are directly given, you don't even have a regression problem.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So here's a nonparametric but still linear regression.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "So we still want to minimize the square loss, but now we take all functions, no restriction.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you you assume that it's.",
                    "label": 0
                },
                {
                    "sent": "I'm differentiable or something, but I mean it's it's non parametric.",
                    "label": 0
                },
                {
                    "sent": "If you would do that without any correction here, I explained it later.",
                    "label": 0
                },
                {
                    "sent": "What would you get is of course a perfect fit, fit through the data.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's always a function.",
                    "label": 0
                },
                {
                    "sent": "I mean even a C Infinity function or 10 times continuous function which perfectly goes through your data points.",
                    "label": 0
                },
                {
                    "sent": "Easy to draw data.",
                    "label": 0
                },
                {
                    "sent": "2 problems you have red points and blue points, and it's direct recursion curve and the blue one, so.",
                    "label": 0
                },
                {
                    "sent": "I'm just a examples.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is not really a useful answer, so I assume you have, you know.",
                    "label": 0
                },
                {
                    "sent": "Look at, look at this, I mean obviously you see here here it's large and here small you want something which looks like I mean these curves, but if you would perfectly fit through the data points.",
                    "label": 0
                },
                {
                    "sent": "Give you a justice wiggly curve which is quite useless.",
                    "label": 0
                },
                {
                    "sent": "So what you want to do is you want to regularize.",
                    "label": 0
                },
                {
                    "sent": "Your problem, so you want to have a bias towards smooth functions.",
                    "label": 0
                },
                {
                    "sent": "Smooth and then it I mean not mathematical sense now, but.",
                    "label": 0
                },
                {
                    "sent": "Intuitive sense, So what you could do is you could say OK I want the curvature of this function so the second derivative is the curvature to be not too large.",
                    "label": 0
                },
                {
                    "sent": "So you add a term which takes the curvature at point X and integrates over all X, so it's the average curvature of the whole domain.",
                    "label": 0
                },
                {
                    "sent": "And you penalize your loss.",
                    "label": 0
                },
                {
                    "sent": "So if your function with large curvature, you know you say the loss is larger than if you have a small curvature and you put a regularization parameter Lambda in there.",
                    "label": 0
                },
                {
                    "sent": "So for Lambda equals 0 L explained, you would get a function which perfectly fits through the data.",
                    "label": 0
                },
                {
                    "sent": "Is the other extreme tablang the evil to Infinity?",
                    "label": 0
                },
                {
                    "sent": "So then you can forget about this term Infinity.",
                    "label": 0
                },
                {
                    "sent": "I know you saw you can't forget you have an Infinity here, so ideally this should be 0.",
                    "label": 0
                },
                {
                    "sent": "And you can make the zero by taking a linear function, because the second error is 0.",
                    "label": 0
                },
                {
                    "sent": "And then this term gets you the least square fit.",
                    "label": 0
                },
                {
                    "sent": "So you would just get returned to linear regression.",
                    "label": 0
                },
                {
                    "sent": "So for Lambda Infinity you have linear regression.",
                    "label": 0
                },
                {
                    "sent": "I mean the most smooth and simple function.",
                    "label": 0
                },
                {
                    "sent": "For Lambda equals zero, you get this really function and for Lambda in between.",
                    "label": 0
                },
                {
                    "sent": "What you get is you get a piece wise you can show you get a piecewise cubic function with continuous derivative.",
                    "label": 1
                },
                {
                    "sent": "And that's just spline interpolation.",
                    "label": 0
                },
                {
                    "sent": "If you've heard about that.",
                    "label": 0
                },
                {
                    "sent": "And the Lambda controls sort of the wiggliness.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "It's it's cubic, yeah, it's.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "The reason is the fault.",
                    "label": 0
                },
                {
                    "sent": "Let me see intuitively, if you're a cubic polynomial, the second derivative is a constant.",
                    "label": 0
                },
                {
                    "sent": "If you have a quartic or higher order polynomial would not be a constant.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You can show that.",
                    "label": 0
                },
                {
                    "sent": "I mean, in order to make this fit, I mean.",
                    "label": 0
                },
                {
                    "sent": "You need at least a constant there and you can show it's better.",
                    "label": 0
                },
                {
                    "sent": "You know you have the.",
                    "label": 0
                },
                {
                    "sent": "You have a second derivative which is piecewise constant rather than piecewise linear or something.",
                    "label": 0
                },
                {
                    "sent": "In order to minimize this expression.",
                    "label": 0
                },
                {
                    "sent": "If you want higher order polynomials then you need higher order derivatives, so if you put the force derivative here then you would get piecewise 5th order.",
                    "label": 0
                },
                {
                    "sent": "Yet mathematically follows.",
                    "label": 0
                },
                {
                    "sent": "And indeed, I think I have a slide on that.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can reduce this functional optimization problem to a finite dimensional optimization problem, which is linear.",
                    "label": 0
                },
                {
                    "sent": "So which just means solving some linear equations.",
                    "label": 0
                },
                {
                    "sent": "OK, there was linear regression and I think that's exactly right for break.",
                    "label": 0
                }
            ]
        }
    }
}