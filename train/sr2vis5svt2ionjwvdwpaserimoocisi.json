{
    "id": "sr2vis5svt2ionjwvdwpaserimoocisi",
    "title": "Learning the Bayesian Network Structure: Dirichlet Prior versus Data",
    "info": {
        "author": [
            "Harald Steck, Bell Labs, Alcatel-Lucent"
        ],
        "published": "July 30, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Machine Learning->Bayesian Learning->Dirichlet Processes"
        ]
    },
    "url": "http://videolectures.net/uai08_steck_lbns/",
    "segmentation": [
        [
            "For this talk with on the tradeoff between the Dirichlet prior and the actual training data.",
            "When you learn Bayesian network structure and it's late in the day, so let's start with."
        ],
        [
            "Informal example.",
            "So let's assume we have some data concerning 2 random variables, dogs and persons, and the data actually tells you that they are independent, indicated by the missing leash and now taking a Bayesian approach, you also have a Dirichlet prior, and you're also assume in literally prior that the dogs are independent of the persons.",
            "So what would you get now if actually even have a very strong prior?",
            "So the result of this duration of this Bayesian approach is now not actually you have Alicia there, and the dogs are dependent on the person's, so that's the typical result.",
            "So what I want to talk about is now, how come and under what conditions exactly you get this behavior, which might be a little bit surprising.",
            "OK, before I."
        ],
        [
            "Give you the explanation in two parts.",
            "I first want to give you now more formal problem statement and introduce the relevant Note 8."
        ],
        [
            "OK so I have a little problem with this crawling here, so we assume that we have discrete random variables that follow a multinomial distribution and the patient network model, as you know, describes.",
            "Then our joint distribution over the variables X one through North.",
            "An as a product of conditional probabilities for each variable given its parents.",
            "And I went to when you want to learn the graph structure using a Bayesian approach, what you do is typically your apply a directly prior over the model parameters, just because it's a conjugate prior, so it's convenient, but also because it ensures likelihood equivalence, which means the resulting scoring function actually assigns the same score to Markov equivalent graphs.",
            "That's very desirable property, and only the Dirichlet prior actually gives you that.",
            "But seriously, Pryor has now some.",
            "Parameters where you can actually plug in your prior knowledge and those are these Alpha XI Pi I for each joint state of a variable and its parents.",
            "Moreover, you can decompose them the Alpha XI Pi into two parts and normalized distribution Q and some scale parameter Alpha and this Alpha is a constant.",
            "You have to pick it at one point and it's also called the equivalent Sam."
        ],
        [
            "Size.",
            "So now in order to get your scoring function for the graph, what you then typically do is you integrate, integrate out the parameters and what you obtain is then.",
            "OK, the marginal likelihood for your graph structure we don't have to bother with the exact details here.",
            "Just note that the alphas now behave like the.",
            "Actual counts from the data, so the capital ends are the counts.",
            "Based on the data and the Alphas, are the hyperparameters from the Dirichlet prior and as you can see, they behave like a virtual counts representing your prior knowledge.",
            "OK, let's just.",
            "Do the weather.",
            "Can I have a problem?",
            "I guess working for some back.",
            "OK, so as a special case and so simplify things, one assumes that.",
            "This distribution queue is uniform, and then one obtains the so-called BDE, you score.",
            "Thank you.",
            "And when you assume that Q is uniform, you have to simplification simplification at all.",
            "These Alpha, XI, Pi.",
            "I then just become a function of the scale parameter or equivalent sample size Alpha divided by the number of joint states of each variable and its parents.",
            "So the only free parameter is now these equivalent sample size Alpha and the question now is how does the value you pick for this?",
            "Parameter affect the graph structure that you learn, so the graph structure that optimizes your matching."
        ],
        [
            "Likelihood.",
            "OK, works and there have been a lot of experiments presented at UI last year by Tommy Sealander and coworkers and what they showed is as you change the equivalent sample size from small values to large values.",
            "You can get any number of edges in the graph from zero to the complete graph, so you get the empty graph complete graph in between somewhere the optimal graph.",
            "And they got those results.",
            "Pretty much for all the other 20 UCI datasets on which they ran the experiments.",
            "Now basically the question is, can we better understand what's going on here?",
            "So concerning the empty graph that has been earlier work with Tommy Achola at NIPS 2002.",
            "And now in this talk I want to cover the other two cases.",
            "The case of large equivalent sample sizes understand why do we get the complete graph?",
            "Almost complete graph in many cases and then also how can we actually then get the optimal graph which is somewhere in between?"
        ],
        [
            "OK, so let's start with a large liquid."
        ],
        [
            "Sample size let's consider two graphs that are identical except for the precious presence of a single edge between A&B.",
            "Which is pressing the left graph and up sending the right one, and then we can just look at marginal likelihood ratio between those two graphs.",
            "And the nice thing is then that this score now simplifies.",
            "It's only a function of the variables involved of a B and the parents pie and all the other variables in the graph cancel out in this log based vector.",
            "So now basically we don't have to consider all the edges in the graph, we can just look at one edge at the time.",
            "If we use lock based vector and as you can see now if the lock based factor is positive, it favors the left graph.",
            "It means that the presence, not the edge AB is present, which means that there's a conditional dependence between A&B given pie."
        ],
        [
            "And if it's.",
            "Negative, then the edge is favored to be absent and we have found a conditional independence."
        ],
        [
            "OK, now let's look at the case of large equivalent sample sizes.",
            "To understand what's going on and the natural thing to do is.",
            "Noah Taylor expansion and keep the leading order, and now we can see two things in front of the bracket.",
            "We have the ratio of N divided by Alpha, which means that as Alpha goes to Infinity, we see that the log base factor goes to zero and it's basically inconclusive.",
            "So all graphs have basically then the same score, so this is not very interesting.",
            "Interesting is now if you have large but finite values of Alpha and then this red term.",
            "Determines the sign of the lock based factor.",
            "If it's positive or negative and this red term has now.",
            "A few components and the number of data points in your training set.",
            "U is a GNU uniformity measure.",
            "I will show you the details in a second and the DF is just the standard degrees of freedom.",
            "And so basically what we have now, if this expression N * U minus DF is positive, then the edge between A&B is present for sufficiently large Alpha.",
            "OK, So what does this uniformity match?"
        ],
        [
            "Sure, you look like it has the following shape.",
            "It has basically four terms and in each term the probability is multiplied by the number of joint states of the variables involved.",
            "And if you want to compare it, for example to the mutual information you see, if you now replace the number of joint states with a lock, then you would just get the mutual information.",
            "So what properties does this new uniformity measure have?",
            "Because that determines now the behavior of the lock based vector.",
            "It's easy to see that it is symmetric in the variables A&B.",
            "It's also non negative.",
            "And the interesting property is the minimality.",
            "And it's interesting that it's zero if and only if the.",
            "Variables A&B are conditionally independent.",
            "And Additionally.",
            "For each parent state pie that has a positive probability, at least one of the marginal distributions.",
            "So P of a given pie.",
            "RPSB given Pie has to be uniform.",
            "Which in turn means that if both of these distributions are not uniform, but you still have independence, then you is not zero anymore.",
            "And that's exactly then what gives rise to all these edges as we."
        ],
        [
            "Oh see OK.",
            "So let's go back quickly.",
            "Now to this original expression that we had.",
            "So this.",
            "Tells us now.",
            "So what does this?",
            "Basically, this degrees of freedom mean?",
            "We can compare now this expression with something that looks very similar with the Archaic information criteria.",
            "So if we replace the.",
            "Uniformity measure you.",
            "There's a mutual information I.",
            "Then we get the Akaike information criterion for that one.",
            "We know that if it's positive, it means that.",
            "Which information is notably larger than zero?",
            "In some sense?",
            "This degrees of freedom is a threshold that is similar to some threshold you would use for significance tests, but because this is not really a significant test in a statistical sense, I just call it notable instead of significant, but pretty much it's just.",
            "I think it's all practical reasons to pretty much the same.",
            "OK, so basically now we can combine this one with the previous slide to get some.",
            "Conclusion In works and it basically now means that for sufficiently large equipment sample sizes, the edge between A&B is present.",
            "If the empirical distribution implies that there's a notable dependence between A&B given pie.",
            "Or that's interesting part.",
            "There is a notable skewness of both distributions.",
            "A given B&B given.",
            "Hi.",
            "And so that shows now that even if you only have skewness.",
            "You can already get a non 0.",
            "Or unnotable.",
            "Salt here and the edges present.",
            "So this.",
            "Pertains not to every edge in the network, right?",
            "And so if these conditions are fulfilled now for every edge, then immediately you get the result that the complete graph has now the highest score.",
            "In practice, if you have small datasets, it may happen due to very small cell due to 0 cell counts.",
            "Because you have maybe very large tables.",
            "If you have many parents then these conditions may not be fulfilled.",
            "In the case of very many parents, that means you don't get to the complete graph, but something very close to complete graph.",
            "So some very dense graph.",
            "Um?",
            "Which is indeed observed.",
            "OK, so now there's also very simple intuitive."
        ],
        [
            "Bination for that.",
            "As you remember, we integrated out the.",
            "Model parameters to get our marginal likelihood as a scoring function for the graph, which means implicitly we're actually using as a parameter estimate.",
            "The average over the posteriors, which has this well known form, and as you can see, it's a weighted sum.",
            "Of the uniform prior Q weighted by liquid and sample size Alpha.",
            "And the empirical distribution.",
            "He had waited by.",
            "The sample size N. And obviously, if you look now at this, if you have independence implied by Q and also by P. Hat.",
            "Then you have a sum of.",
            "Products and obviously that's in general not product anymore, and so any independence anymore.",
            "So that's a very simple, intuitive explanation.",
            "Why if one distribution is cute and the other is uniform, or as long as they are somehow different, you Lucinda."
        ],
        [
            "Balance.",
            "OK, so now let's go to the second part.",
            "Now that we know that if you have a very logical and sample size, you may get something unexpected.",
            "If you have a small equivalent sample size, you get something unexpected.",
            "So you somehow have to find the middle where you have the optimal equivalent sample size and questions.",
            "Now, what properties of the data actually determine the value of the equivalent sample?"
        ],
        [
            "Size.",
            "So we treat it now as an additional parameter that we want to learn, and among all the different.",
            "Oh criteria that we could optimize.",
            "We just follow the objective function of Tommy Sealander and coworkers from last year that we want to maximize the.",
            "Marginal likelihood again and find the Alpha and the graph that maximizes this.",
            "And that can be done easily in a coordinate wise ascent approach.",
            "You just iterate 2 steps in the first step you optimize the graph for a fixed.",
            "Equivalent sample size value that can be done with any standard structure learning algorithm.",
            "And the second step is now optimized the equivalent.",
            "A sample size value for a fixed graph, and that's not the interesting part.",
            "How can we do that?"
        ],
        [
            "And what I do is now I do a lot of very crude approximations.",
            "Because I just want to find out what are the main effects or main properties of the data that affect the equivalent sample size value.",
            "So let's start out again with the marginal likelihood.",
            "And as Peter Grunwald mentioned yesterday, you can understand it as a measure of predictive accuracy in the prevencial set.",
            "That means that you assume that data arrives sequentially and then at every step you predict that point given all the previous data points.",
            "And now I approximate that by the frequentist test error, which is very crude, I admit it.",
            "So this is now the test error that you typically get when you do cross validation.",
            "Now as a next step I can approximate this one at.",
            "On the one hand, with the Akaike information criteria, so I.",
            "You can just compute it from the training error and the penalty and dark information criteria is independent of Alpha, obviously, so that's the one thing and then I can do a second approximation.",
            "Where I assume now that in this test relates my.",
            "Distribution described by the model with a true test distribution that now this.",
            "True distribution takes the form as implied by the basin approach.",
            "Where I don't know the optimal value alphastar.",
            "And now I can just recreate both of those.",
            "I."
        ],
        [
            "Approximations and get a leading order approximation for my equivalence for my optimal equivalent sample size.",
            "And as we can see, it's a ratio of the effective number of parameters.",
            "Innovation Network divided by.",
            "Basically, the negative entropy of the empirical distribution or the maximum likelihood divided by N. And the other term is basically the cross likelihood of the uniform prior Q and the empirical distribution.",
            "Divided by N again.",
            "As a sanity check, we can now see that denominator denominator is indeed a positive number.",
            "If Q is uniform, because it can be rewritten in terms of entropy zancai elder versions.",
            "And so we actually get that the.",
            "Alphastar is indeed positive."
        ],
        [
            "As it has to be now, we can also derive a few properties of this optimal equivalent sample size.",
            "1 property is that.",
            "If the data implies a large skewness or dependencies.",
            "Then it means that this.",
            "A term in the denominator becomes larger because the maximum likelihood basically increases, and so in this case the optimal equation sample size is small.",
            "Now let's look at the sample size N. It doesn't occur at all in this equation, so there's no explicit impact by that one, but there might be a little bit of implicit effects there, but one can expect because of that that for large sample sizes they the optimal value Alpha star is approximately a constant, so it does."
        ],
        [
            "Increase with then.",
            "And Lastly, the number of nodes in the graph also turns out that it doesn't really have a big effect cause the numerator and the denominator both are additive in the number of nodes.",
            "So both actually grow approximately in a similar way.",
            "One can say in a handwaving manner, and so the ratio should be approx."
        ],
        [
            "Constant, so also to check now if all these crude approximations are not too bad, I compare it with the exact results of Tommy Sealander and coworkers and so you can see for these different datasets in the first column is a sample size ranges from very small to quite large values.",
            "Also the number of nodes changes quite a bit and then the next two columns show you the exact results that they got.",
            "And what's interesting is that there are three datasets that have an equivalent sample size of about 50 that is optimal, and the other ones have much smaller values, about 10 or less, and our approximation Alpha star in the.",
            "Very right column actually agrees very well with that, so we can capture the main effects with our crude approximation.",
            "So that's."
        ],
        [
            "Not too bad.",
            "OK in conclusion, so I've shown now for the large equivalent sample size it can happen.",
            "But even though your data implies independence and your prior implies independence, the result can actually be the complete graph.",
            "I also discussed the exact conditions when that happens and also concerning now, the optimal equivalent sample size I've showed that it's approximately independent.",
            "The sample size and the number of the nodes.",
            "And it tends to be small.",
            "If there's strong skewness or dependencies in the data, like for example we had in the alarm network that used to be the benchmark in the old days, and they couldn't sample size is large if small.",
            "Size is large.",
            "If there's a small skewness and weak dependencies in the data.",
            "OK, thank you.",
            "Basically the analytical derivation or in the first part for logical and sample size depends a lot or not, so if it's not uniform you get some equations that basically you cannot do it analytically anymore, so only for the U1 you have.",
            "The nice property that you can analytically go basically all the way to the end and then basically get."
        ],
        [
            "Get a nice expression for this uniformity measure here, so if you so basically for the uniform Q you get that then in front of these pieces you have the number of joint states.",
            "And if it's not uniform, then you get something very ugly there.",
            "But I think qualitatively it wouldn't make a big difference if you don't have a uniform queue anymore, but it's just that it doesn't work out analytically anymore.",
            "Any other questions?",
            "This is to say that DDU is broken.",
            "One way to see it, yeah.",
            "So we seem to be in a dilemma.",
            "Yes, that's true yes.",
            "So basically I think there are two ways out.",
            "So if you want to use it then you should do a very careful sensitivity analysis and see what the effect of the value of Alpha is.",
            "And if you want to do a very lazy approach, you could just say OK I do by CRA I see something that doesn't involve directly prior.",
            "Right?",
            "I don't know.",
            "I mean, my experience is that they're not.",
            "Right, I mean my experience is that there is not such a huge difference, I mean.",
            "Maybe a few edges different or so, but.",
            "Thinking last.",
            "OK, my experiments.",
            "Typically it was not too bad actually, but it depends on the datasets I guess."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this talk with on the tradeoff between the Dirichlet prior and the actual training data.",
                    "label": 0
                },
                {
                    "sent": "When you learn Bayesian network structure and it's late in the day, so let's start with.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Informal example.",
                    "label": 0
                },
                {
                    "sent": "So let's assume we have some data concerning 2 random variables, dogs and persons, and the data actually tells you that they are independent, indicated by the missing leash and now taking a Bayesian approach, you also have a Dirichlet prior, and you're also assume in literally prior that the dogs are independent of the persons.",
                    "label": 0
                },
                {
                    "sent": "So what would you get now if actually even have a very strong prior?",
                    "label": 0
                },
                {
                    "sent": "So the result of this duration of this Bayesian approach is now not actually you have Alicia there, and the dogs are dependent on the person's, so that's the typical result.",
                    "label": 0
                },
                {
                    "sent": "So what I want to talk about is now, how come and under what conditions exactly you get this behavior, which might be a little bit surprising.",
                    "label": 0
                },
                {
                    "sent": "OK, before I.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give you the explanation in two parts.",
                    "label": 0
                },
                {
                    "sent": "I first want to give you now more formal problem statement and introduce the relevant Note 8.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I have a little problem with this crawling here, so we assume that we have discrete random variables that follow a multinomial distribution and the patient network model, as you know, describes.",
                    "label": 1
                },
                {
                    "sent": "Then our joint distribution over the variables X one through North.",
                    "label": 0
                },
                {
                    "sent": "An as a product of conditional probabilities for each variable given its parents.",
                    "label": 1
                },
                {
                    "sent": "And I went to when you want to learn the graph structure using a Bayesian approach, what you do is typically your apply a directly prior over the model parameters, just because it's a conjugate prior, so it's convenient, but also because it ensures likelihood equivalence, which means the resulting scoring function actually assigns the same score to Markov equivalent graphs.",
                    "label": 1
                },
                {
                    "sent": "That's very desirable property, and only the Dirichlet prior actually gives you that.",
                    "label": 0
                },
                {
                    "sent": "But seriously, Pryor has now some.",
                    "label": 0
                },
                {
                    "sent": "Parameters where you can actually plug in your prior knowledge and those are these Alpha XI Pi I for each joint state of a variable and its parents.",
                    "label": 0
                },
                {
                    "sent": "Moreover, you can decompose them the Alpha XI Pi into two parts and normalized distribution Q and some scale parameter Alpha and this Alpha is a constant.",
                    "label": 0
                },
                {
                    "sent": "You have to pick it at one point and it's also called the equivalent Sam.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Size.",
                    "label": 0
                },
                {
                    "sent": "So now in order to get your scoring function for the graph, what you then typically do is you integrate, integrate out the parameters and what you obtain is then.",
                    "label": 1
                },
                {
                    "sent": "OK, the marginal likelihood for your graph structure we don't have to bother with the exact details here.",
                    "label": 0
                },
                {
                    "sent": "Just note that the alphas now behave like the.",
                    "label": 0
                },
                {
                    "sent": "Actual counts from the data, so the capital ends are the counts.",
                    "label": 0
                },
                {
                    "sent": "Based on the data and the Alphas, are the hyperparameters from the Dirichlet prior and as you can see, they behave like a virtual counts representing your prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "OK, let's just.",
                    "label": 0
                },
                {
                    "sent": "Do the weather.",
                    "label": 0
                },
                {
                    "sent": "Can I have a problem?",
                    "label": 0
                },
                {
                    "sent": "I guess working for some back.",
                    "label": 0
                },
                {
                    "sent": "OK, so as a special case and so simplify things, one assumes that.",
                    "label": 0
                },
                {
                    "sent": "This distribution queue is uniform, and then one obtains the so-called BDE, you score.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "And when you assume that Q is uniform, you have to simplification simplification at all.",
                    "label": 0
                },
                {
                    "sent": "These Alpha, XI, Pi.",
                    "label": 0
                },
                {
                    "sent": "I then just become a function of the scale parameter or equivalent sample size Alpha divided by the number of joint states of each variable and its parents.",
                    "label": 0
                },
                {
                    "sent": "So the only free parameter is now these equivalent sample size Alpha and the question now is how does the value you pick for this?",
                    "label": 1
                },
                {
                    "sent": "Parameter affect the graph structure that you learn, so the graph structure that optimizes your matching.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK, works and there have been a lot of experiments presented at UI last year by Tommy Sealander and coworkers and what they showed is as you change the equivalent sample size from small values to large values.",
                    "label": 0
                },
                {
                    "sent": "You can get any number of edges in the graph from zero to the complete graph, so you get the empty graph complete graph in between somewhere the optimal graph.",
                    "label": 1
                },
                {
                    "sent": "And they got those results.",
                    "label": 0
                },
                {
                    "sent": "Pretty much for all the other 20 UCI datasets on which they ran the experiments.",
                    "label": 0
                },
                {
                    "sent": "Now basically the question is, can we better understand what's going on here?",
                    "label": 1
                },
                {
                    "sent": "So concerning the empty graph that has been earlier work with Tommy Achola at NIPS 2002.",
                    "label": 0
                },
                {
                    "sent": "And now in this talk I want to cover the other two cases.",
                    "label": 0
                },
                {
                    "sent": "The case of large equivalent sample sizes understand why do we get the complete graph?",
                    "label": 0
                },
                {
                    "sent": "Almost complete graph in many cases and then also how can we actually then get the optimal graph which is somewhere in between?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's start with a large liquid.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample size let's consider two graphs that are identical except for the precious presence of a single edge between A&B.",
                    "label": 0
                },
                {
                    "sent": "Which is pressing the left graph and up sending the right one, and then we can just look at marginal likelihood ratio between those two graphs.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing is then that this score now simplifies.",
                    "label": 0
                },
                {
                    "sent": "It's only a function of the variables involved of a B and the parents pie and all the other variables in the graph cancel out in this log based vector.",
                    "label": 0
                },
                {
                    "sent": "So now basically we don't have to consider all the edges in the graph, we can just look at one edge at the time.",
                    "label": 0
                },
                {
                    "sent": "If we use lock based vector and as you can see now if the lock based factor is positive, it favors the left graph.",
                    "label": 0
                },
                {
                    "sent": "It means that the presence, not the edge AB is present, which means that there's a conditional dependence between A&B given pie.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if it's.",
                    "label": 0
                },
                {
                    "sent": "Negative, then the edge is favored to be absent and we have found a conditional independence.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now let's look at the case of large equivalent sample sizes.",
                    "label": 0
                },
                {
                    "sent": "To understand what's going on and the natural thing to do is.",
                    "label": 0
                },
                {
                    "sent": "Noah Taylor expansion and keep the leading order, and now we can see two things in front of the bracket.",
                    "label": 0
                },
                {
                    "sent": "We have the ratio of N divided by Alpha, which means that as Alpha goes to Infinity, we see that the log base factor goes to zero and it's basically inconclusive.",
                    "label": 0
                },
                {
                    "sent": "So all graphs have basically then the same score, so this is not very interesting.",
                    "label": 0
                },
                {
                    "sent": "Interesting is now if you have large but finite values of Alpha and then this red term.",
                    "label": 0
                },
                {
                    "sent": "Determines the sign of the lock based factor.",
                    "label": 0
                },
                {
                    "sent": "If it's positive or negative and this red term has now.",
                    "label": 0
                },
                {
                    "sent": "A few components and the number of data points in your training set.",
                    "label": 0
                },
                {
                    "sent": "U is a GNU uniformity measure.",
                    "label": 0
                },
                {
                    "sent": "I will show you the details in a second and the DF is just the standard degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "And so basically what we have now, if this expression N * U minus DF is positive, then the edge between A&B is present for sufficiently large Alpha.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does this uniformity match?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure, you look like it has the following shape.",
                    "label": 0
                },
                {
                    "sent": "It has basically four terms and in each term the probability is multiplied by the number of joint states of the variables involved.",
                    "label": 0
                },
                {
                    "sent": "And if you want to compare it, for example to the mutual information you see, if you now replace the number of joint states with a lock, then you would just get the mutual information.",
                    "label": 0
                },
                {
                    "sent": "So what properties does this new uniformity measure have?",
                    "label": 1
                },
                {
                    "sent": "Because that determines now the behavior of the lock based vector.",
                    "label": 0
                },
                {
                    "sent": "It's easy to see that it is symmetric in the variables A&B.",
                    "label": 0
                },
                {
                    "sent": "It's also non negative.",
                    "label": 0
                },
                {
                    "sent": "And the interesting property is the minimality.",
                    "label": 0
                },
                {
                    "sent": "And it's interesting that it's zero if and only if the.",
                    "label": 0
                },
                {
                    "sent": "Variables A&B are conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "And Additionally.",
                    "label": 0
                },
                {
                    "sent": "For each parent state pie that has a positive probability, at least one of the marginal distributions.",
                    "label": 0
                },
                {
                    "sent": "So P of a given pie.",
                    "label": 0
                },
                {
                    "sent": "RPSB given Pie has to be uniform.",
                    "label": 0
                },
                {
                    "sent": "Which in turn means that if both of these distributions are not uniform, but you still have independence, then you is not zero anymore.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly then what gives rise to all these edges as we.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh see OK.",
                    "label": 0
                },
                {
                    "sent": "So let's go back quickly.",
                    "label": 0
                },
                {
                    "sent": "Now to this original expression that we had.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Tells us now.",
                    "label": 0
                },
                {
                    "sent": "So what does this?",
                    "label": 0
                },
                {
                    "sent": "Basically, this degrees of freedom mean?",
                    "label": 0
                },
                {
                    "sent": "We can compare now this expression with something that looks very similar with the Archaic information criteria.",
                    "label": 0
                },
                {
                    "sent": "So if we replace the.",
                    "label": 0
                },
                {
                    "sent": "Uniformity measure you.",
                    "label": 0
                },
                {
                    "sent": "There's a mutual information I.",
                    "label": 0
                },
                {
                    "sent": "Then we get the Akaike information criterion for that one.",
                    "label": 0
                },
                {
                    "sent": "We know that if it's positive, it means that.",
                    "label": 0
                },
                {
                    "sent": "Which information is notably larger than zero?",
                    "label": 1
                },
                {
                    "sent": "In some sense?",
                    "label": 0
                },
                {
                    "sent": "This degrees of freedom is a threshold that is similar to some threshold you would use for significance tests, but because this is not really a significant test in a statistical sense, I just call it notable instead of significant, but pretty much it's just.",
                    "label": 0
                },
                {
                    "sent": "I think it's all practical reasons to pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically now we can combine this one with the previous slide to get some.",
                    "label": 1
                },
                {
                    "sent": "Conclusion In works and it basically now means that for sufficiently large equipment sample sizes, the edge between A&B is present.",
                    "label": 1
                },
                {
                    "sent": "If the empirical distribution implies that there's a notable dependence between A&B given pie.",
                    "label": 0
                },
                {
                    "sent": "Or that's interesting part.",
                    "label": 0
                },
                {
                    "sent": "There is a notable skewness of both distributions.",
                    "label": 1
                },
                {
                    "sent": "A given B&B given.",
                    "label": 0
                },
                {
                    "sent": "Hi.",
                    "label": 0
                },
                {
                    "sent": "And so that shows now that even if you only have skewness.",
                    "label": 0
                },
                {
                    "sent": "You can already get a non 0.",
                    "label": 0
                },
                {
                    "sent": "Or unnotable.",
                    "label": 0
                },
                {
                    "sent": "Salt here and the edges present.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Pertains not to every edge in the network, right?",
                    "label": 1
                },
                {
                    "sent": "And so if these conditions are fulfilled now for every edge, then immediately you get the result that the complete graph has now the highest score.",
                    "label": 0
                },
                {
                    "sent": "In practice, if you have small datasets, it may happen due to very small cell due to 0 cell counts.",
                    "label": 0
                },
                {
                    "sent": "Because you have maybe very large tables.",
                    "label": 0
                },
                {
                    "sent": "If you have many parents then these conditions may not be fulfilled.",
                    "label": 0
                },
                {
                    "sent": "In the case of very many parents, that means you don't get to the complete graph, but something very close to complete graph.",
                    "label": 0
                },
                {
                    "sent": "So some very dense graph.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Which is indeed observed.",
                    "label": 0
                },
                {
                    "sent": "OK, so now there's also very simple intuitive.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bination for that.",
                    "label": 0
                },
                {
                    "sent": "As you remember, we integrated out the.",
                    "label": 0
                },
                {
                    "sent": "Model parameters to get our marginal likelihood as a scoring function for the graph, which means implicitly we're actually using as a parameter estimate.",
                    "label": 0
                },
                {
                    "sent": "The average over the posteriors, which has this well known form, and as you can see, it's a weighted sum.",
                    "label": 0
                },
                {
                    "sent": "Of the uniform prior Q weighted by liquid and sample size Alpha.",
                    "label": 1
                },
                {
                    "sent": "And the empirical distribution.",
                    "label": 0
                },
                {
                    "sent": "He had waited by.",
                    "label": 0
                },
                {
                    "sent": "The sample size N. And obviously, if you look now at this, if you have independence implied by Q and also by P. Hat.",
                    "label": 0
                },
                {
                    "sent": "Then you have a sum of.",
                    "label": 0
                },
                {
                    "sent": "Products and obviously that's in general not product anymore, and so any independence anymore.",
                    "label": 0
                },
                {
                    "sent": "So that's a very simple, intuitive explanation.",
                    "label": 0
                },
                {
                    "sent": "Why if one distribution is cute and the other is uniform, or as long as they are somehow different, you Lucinda.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Balance.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's go to the second part.",
                    "label": 0
                },
                {
                    "sent": "Now that we know that if you have a very logical and sample size, you may get something unexpected.",
                    "label": 0
                },
                {
                    "sent": "If you have a small equivalent sample size, you get something unexpected.",
                    "label": 1
                },
                {
                    "sent": "So you somehow have to find the middle where you have the optimal equivalent sample size and questions.",
                    "label": 1
                },
                {
                    "sent": "Now, what properties of the data actually determine the value of the equivalent sample?",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Size.",
                    "label": 0
                },
                {
                    "sent": "So we treat it now as an additional parameter that we want to learn, and among all the different.",
                    "label": 1
                },
                {
                    "sent": "Oh criteria that we could optimize.",
                    "label": 0
                },
                {
                    "sent": "We just follow the objective function of Tommy Sealander and coworkers from last year that we want to maximize the.",
                    "label": 0
                },
                {
                    "sent": "Marginal likelihood again and find the Alpha and the graph that maximizes this.",
                    "label": 0
                },
                {
                    "sent": "And that can be done easily in a coordinate wise ascent approach.",
                    "label": 0
                },
                {
                    "sent": "You just iterate 2 steps in the first step you optimize the graph for a fixed.",
                    "label": 0
                },
                {
                    "sent": "Equivalent sample size value that can be done with any standard structure learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "And the second step is now optimized the equivalent.",
                    "label": 0
                },
                {
                    "sent": "A sample size value for a fixed graph, and that's not the interesting part.",
                    "label": 0
                },
                {
                    "sent": "How can we do that?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what I do is now I do a lot of very crude approximations.",
                    "label": 0
                },
                {
                    "sent": "Because I just want to find out what are the main effects or main properties of the data that affect the equivalent sample size value.",
                    "label": 0
                },
                {
                    "sent": "So let's start out again with the marginal likelihood.",
                    "label": 1
                },
                {
                    "sent": "And as Peter Grunwald mentioned yesterday, you can understand it as a measure of predictive accuracy in the prevencial set.",
                    "label": 1
                },
                {
                    "sent": "That means that you assume that data arrives sequentially and then at every step you predict that point given all the previous data points.",
                    "label": 0
                },
                {
                    "sent": "And now I approximate that by the frequentist test error, which is very crude, I admit it.",
                    "label": 1
                },
                {
                    "sent": "So this is now the test error that you typically get when you do cross validation.",
                    "label": 0
                },
                {
                    "sent": "Now as a next step I can approximate this one at.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, with the Akaike information criteria, so I.",
                    "label": 0
                },
                {
                    "sent": "You can just compute it from the training error and the penalty and dark information criteria is independent of Alpha, obviously, so that's the one thing and then I can do a second approximation.",
                    "label": 1
                },
                {
                    "sent": "Where I assume now that in this test relates my.",
                    "label": 0
                },
                {
                    "sent": "Distribution described by the model with a true test distribution that now this.",
                    "label": 0
                },
                {
                    "sent": "True distribution takes the form as implied by the basin approach.",
                    "label": 0
                },
                {
                    "sent": "Where I don't know the optimal value alphastar.",
                    "label": 0
                },
                {
                    "sent": "And now I can just recreate both of those.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approximations and get a leading order approximation for my equivalence for my optimal equivalent sample size.",
                    "label": 1
                },
                {
                    "sent": "And as we can see, it's a ratio of the effective number of parameters.",
                    "label": 0
                },
                {
                    "sent": "Innovation Network divided by.",
                    "label": 0
                },
                {
                    "sent": "Basically, the negative entropy of the empirical distribution or the maximum likelihood divided by N. And the other term is basically the cross likelihood of the uniform prior Q and the empirical distribution.",
                    "label": 0
                },
                {
                    "sent": "Divided by N again.",
                    "label": 1
                },
                {
                    "sent": "As a sanity check, we can now see that denominator denominator is indeed a positive number.",
                    "label": 0
                },
                {
                    "sent": "If Q is uniform, because it can be rewritten in terms of entropy zancai elder versions.",
                    "label": 0
                },
                {
                    "sent": "And so we actually get that the.",
                    "label": 0
                },
                {
                    "sent": "Alphastar is indeed positive.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As it has to be now, we can also derive a few properties of this optimal equivalent sample size.",
                    "label": 1
                },
                {
                    "sent": "1 property is that.",
                    "label": 0
                },
                {
                    "sent": "If the data implies a large skewness or dependencies.",
                    "label": 1
                },
                {
                    "sent": "Then it means that this.",
                    "label": 0
                },
                {
                    "sent": "A term in the denominator becomes larger because the maximum likelihood basically increases, and so in this case the optimal equation sample size is small.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at the sample size N. It doesn't occur at all in this equation, so there's no explicit impact by that one, but there might be a little bit of implicit effects there, but one can expect because of that that for large sample sizes they the optimal value Alpha star is approximately a constant, so it does.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Increase with then.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, the number of nodes in the graph also turns out that it doesn't really have a big effect cause the numerator and the denominator both are additive in the number of nodes.",
                    "label": 0
                },
                {
                    "sent": "So both actually grow approximately in a similar way.",
                    "label": 0
                },
                {
                    "sent": "One can say in a handwaving manner, and so the ratio should be approx.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Constant, so also to check now if all these crude approximations are not too bad, I compare it with the exact results of Tommy Sealander and coworkers and so you can see for these different datasets in the first column is a sample size ranges from very small to quite large values.",
                    "label": 0
                },
                {
                    "sent": "Also the number of nodes changes quite a bit and then the next two columns show you the exact results that they got.",
                    "label": 1
                },
                {
                    "sent": "And what's interesting is that there are three datasets that have an equivalent sample size of about 50 that is optimal, and the other ones have much smaller values, about 10 or less, and our approximation Alpha star in the.",
                    "label": 0
                },
                {
                    "sent": "Very right column actually agrees very well with that, so we can capture the main effects with our crude approximation.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not too bad.",
                    "label": 0
                },
                {
                    "sent": "OK in conclusion, so I've shown now for the large equivalent sample size it can happen.",
                    "label": 0
                },
                {
                    "sent": "But even though your data implies independence and your prior implies independence, the result can actually be the complete graph.",
                    "label": 0
                },
                {
                    "sent": "I also discussed the exact conditions when that happens and also concerning now, the optimal equivalent sample size I've showed that it's approximately independent.",
                    "label": 0
                },
                {
                    "sent": "The sample size and the number of the nodes.",
                    "label": 0
                },
                {
                    "sent": "And it tends to be small.",
                    "label": 0
                },
                {
                    "sent": "If there's strong skewness or dependencies in the data, like for example we had in the alarm network that used to be the benchmark in the old days, and they couldn't sample size is large if small.",
                    "label": 0
                },
                {
                    "sent": "Size is large.",
                    "label": 0
                },
                {
                    "sent": "If there's a small skewness and weak dependencies in the data.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Basically the analytical derivation or in the first part for logical and sample size depends a lot or not, so if it's not uniform you get some equations that basically you cannot do it analytically anymore, so only for the U1 you have.",
                    "label": 0
                },
                {
                    "sent": "The nice property that you can analytically go basically all the way to the end and then basically get.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get a nice expression for this uniformity measure here, so if you so basically for the uniform Q you get that then in front of these pieces you have the number of joint states.",
                    "label": 0
                },
                {
                    "sent": "And if it's not uniform, then you get something very ugly there.",
                    "label": 0
                },
                {
                    "sent": "But I think qualitatively it wouldn't make a big difference if you don't have a uniform queue anymore, but it's just that it doesn't work out analytically anymore.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "This is to say that DDU is broken.",
                    "label": 0
                },
                {
                    "sent": "One way to see it, yeah.",
                    "label": 0
                },
                {
                    "sent": "So we seem to be in a dilemma.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's true yes.",
                    "label": 0
                },
                {
                    "sent": "So basically I think there are two ways out.",
                    "label": 0
                },
                {
                    "sent": "So if you want to use it then you should do a very careful sensitivity analysis and see what the effect of the value of Alpha is.",
                    "label": 0
                },
                {
                    "sent": "And if you want to do a very lazy approach, you could just say OK I do by CRA I see something that doesn't involve directly prior.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, my experience is that they're not.",
                    "label": 0
                },
                {
                    "sent": "Right, I mean my experience is that there is not such a huge difference, I mean.",
                    "label": 0
                },
                {
                    "sent": "Maybe a few edges different or so, but.",
                    "label": 0
                },
                {
                    "sent": "Thinking last.",
                    "label": 0
                },
                {
                    "sent": "OK, my experiments.",
                    "label": 0
                },
                {
                    "sent": "Typically it was not too bad actually, but it depends on the datasets I guess.",
                    "label": 0
                }
            ]
        }
    }
}