{
    "id": "hh37uuwxdwrpxlzbo3vz2qpnykmz3zhb",
    "title": "Structured sparsity-inducing norms through submodular functions",
    "info": {
        "author": [
            "Francis R. Bach, INRIA - SIERRA project-team"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_bach_ssi/",
    "segmentation": [
        [
            "Hi good afternoon.",
            "So what I will."
        ],
        [
            "Sun today is a structured sparsity, and this could be achieved through a similar functions.",
            "And so at the beginning of the talk I will start to motivate why we need a structured sparsity, and essentially it will mean going going beyond the L1 norm and its relationship between the Academy cardinality function.",
            "So this will be done for similar functions and the tool I will need from that type of work is alove edge extension, so I will describe what this means and they will describe the actual content, the novel contributions of this work and show allocation of explicit.",
            "In addition, so supports and show a unified algorithms and analysis."
        ],
        [
            "So a bit a small recap on sparsity, so we consider some data SII so N pairs where XYZ in RP and why I is in R and we consider the regularised empirical risk minimization when we have a new minimized with respect to our linear predictor data fitting term here plus penalty.",
            "And here's the penalty is here to promote sparsity and the main example is the norm, and this has many names depending on where you come from.",
            "Baseball suits in signal processing and less so in statistics and machine learning, so why would you want to use sparsity?",
            "So I see many reasons, but two important ones are the interpretability.",
            "Essentially, when you have many zeros is like a proxy for interpretability, and the second reason is as it was well explained yesterday.",
            "Tutorial Video is a possibility of doing high dimensional inference.",
            "So here if P the number of variables and end the number of observations, P can be a lot a lot larger than N as long as P is less than exponential N. So it's just to mention it this is not sparsity is not specific to supervise learning.",
            "It can be also applied to unsupervised learning.",
            "Essentially, when you only have observations and you want to build the design matrix so that it is well adapted to your entire set of observations, essentially it is doing PCA and sparse versions of PCA, which are often called Dictionary dictionary learning."
        ],
        [
            "So why do you want structured sparsity?",
            "First one to be my main reason is that it's OK to have a lot of zeros, but in many cases you want to zero to organize in a special way.",
            "And we have done some work with hydrogen Atom and Gramma belinski to try to in the PCA context to learn principle components with a given structure.",
            "So this is the first aspect.",
            "Second aspect is when you want to learn decompositions onto those components which structure.",
            "So here you assume that traditional elements they'll even though given the topology and motivating example, is taken from topic."
        ],
        [
            "Models quite similar to the previous spotlight, so in topic models you might want to be the Yaqui of topics where you want to decompose every document as a decomposition of topics where you start with the root topic and go down the tree by building your documents to the topics.",
            "Longer paths of the tree so the main one of the main themes of what we're doing with positive using norms is to be able to do that kind of thing by simply changing one line of code in a standard algorithm for dictionary.",
            "So just replace the L1 norm by the norm that I'm going to present and you can have that type of behavior."
        ],
        [
            "So of course the portability is not the only the only thing I want to do a bit more so.",
            "Of course, if you know more about your data and yuanyu prior fits matches that knowledge, you should predict better and also you can use this structure sparsity to be numerically efficient.",
            "So I have some works.",
            "Two years ago when I was doing non general viable selection and essentially I had to search over all possible to the P subsets where peas and number of variables.",
            "So if P is 100.",
            "Makes a lot of subsets and by using structures pacitti you can design polynomial time algorithms to do that."
        ],
        [
            "So let's go down to to design ways of inducing a structured sparsity.",
            "But before doing that, let's go back a bit to see what how you would justify the L1 norm.",
            "So the way I like is to justify the L1 norm as a convex approximation of the LO Casino.",
            "So just for notations W in RP and throughout the talk we called Capital V, this discrete set one 2P and the support of W will be the set of nonzero indices, so the L0 penalty, essentially the number of the cardinality of the support.",
            "And one way of defending the element norm is at the convex envelope of the L0 norm.",
            "So what is a convex envelope?",
            "It is the largest convex lower bound of the given function.",
            "So here this is the example in 1D.",
            "When you have the LOL0 norm, so it's zero at zero and one everywhere else.",
            "And if you consider that interval minus one one, if you want to be convex to be below the red curve between minus one and one, and to be the largest possible is clearly the.",
            "OK, so this is the traditional way of defending the L1 norm.",
            "Essentially we're going to do in.",
            "This talk is to consider other functions, then the cardinality and play the same trick trained to derive up.",
            "Sorry, trying to derive the largest.",
            "Convex lower bound and this will be done for a particular set of functions which are similar function."
        ],
        [
            "So what are those functions?",
            "So before I describe them that we point out the nice book by fujishige on the topic, which is quite comprehensive, but which is a tough read.",
            "So it is.",
            "This is why I wrote a tutorial which is directed to a machine learning audience where I assume are basic knowledge connection.",
            "Alesis and we derive all results which are useful for working with similar functions.",
            "So let's go down to the definition.",
            "So we define.",
            "This is a set functions which go from sets.",
            "So 2002 V is a set of subsets, and for each set you give a value and you're still modular.",
            "If you take any two subsets A&BF of air plus FAB is larger than FA intersect B&F Union B.",
            "So if F is the is the cardinality function or any measure, then this you have equality over there.",
            "So it's well known and to be submodular you just need one inequality.",
            "Another way of defining similarity, which is often easier to understand, is this one, where you take any set A and you add one more, one more element of the set and you see how much you have gained by using that element.",
            "So F of a non K -- F of a you are similar.",
            "If this function is non increasing.",
            "So this is a notion of diminishing return and this leads to the first intuition about similar functions."
        ],
        [
            "Which is there in fact concave function and one example of that.",
            "If you take functions of the cardinality G of cardinality of a, it is similar.",
            "As soon as G is concave.",
            "So this shows also that the commonality is a similar function.",
            "Although it is a defined almost like a concave function, it behaves like a convex."
        ],
        [
            "Function OK, why?",
            "Because you have a polynomial time algorithms to minimize them.",
            "You have a nice conjugacy theory and you have links with the low back extension, which I will describe in the next line."
        ],
        [
            "So those have been used in several areas in signal processing, computer vision or machine learning, so there's a whole series of work about graph cuts in computer vision and also nice work in optimal design and other things by house and get string."
        ],
        [
            "So let's not defined the main tool that I'm going to use in this talk, which is a large extension, so this can be defined, But for any any set functions.",
            "So how do you compute that?",
            "You take a W in RP you order in its components in decreasing order, so WG One WG 2 up to WJP and then you take a linear combinations linear combination of WJC with that weight.",
            "So the formulas are not so important.",
            "What is important is the following property that return below.",
            "Which is the fact that if W is the indicator vector of the set a, so one is the component belongs to A and 0 otherwise, then the value of the love at extension is essentially the value of the function.",
            "So this means that you can since you can identify subsets of V with the set 01 to the P, you essentially extended the function capital F from zero one to appeal to the whole RP.",
            "Moreover, F is piecewise.",
            "Define and positively imaginaries.",
            "So this is always true for all possible set functions and a nice nice thing is that if F is similar capital F is similar if you only if it's love at extension is convex.",
            "So this is really an important aspect of similar analysis, and this list for example to this nice property that minimizing love at extension is equivalent to minimizing the function itself."
        ],
        [
            "So now let's go down to the contributions of of my work.",
            "So here we are considering similar function F, which is similar and it will be nondecreasing.",
            "So why not decreasing?",
            "Because I want to favor small subsets, so I want to penalize more larger subsets so the main proposition is if you consider the function that takes the value you take is support and you penalize, and you can compute the set function that for those people W. Then you get the functions from RP to R and it turns out that the convex envelope of that function, so the lowest the largest convex lower bound is exactly the large lower extension computed as the vector composed of the absolute values of the components of W."
        ],
        [
            "So yeah, just as a safety check, if you take F being the cardinality, the large extension is just a sum of components and you get back the fact that the L1 norm is a convex envelope of the cognitive function.",
            "So now I've defined Norm, Omega and the rest of the talk will be dedicated to analyze this number.",
            "So first, since F here F is piecewise affine, my norm is a polyhedral, so it's unit ball as a polytope, and it's traditional in this setting to draw that ball.",
            "OK, so here I've drawn the boarding 2D, so here was the important aspect.",
            "Is the fact that all the extreme points of the bowl are always on the same directions you have once along the axis and once along the main diagonal's and the main action of the lack of adding different values for F is the fact that you will have some extreme points disappear and some which won't disappear.",
            "And so for example the one norm is a case where essentially those this points this point.",
            "This extreme points disappear the L Infinity norm, which just so sad case.",
            "It's a case where that point disappears and so on.",
            "OK, so at the end this will define the set of corners and this will you can define so-called stable sets which are sets so that all larger subsets are they strictly larger function value and you can prove that stable set or stable.",
            "Essentially those are those are the only one which can be obtained when you regularize by that name.",
            "So."
        ],
        [
            "I can go to three dimension and show other balls, but I want describe them.",
            "You can see then in the poster and try to go to examples in model."
        ],
        [
            "Mentions.",
            "And I believe we give two types of examples.",
            "The first one will go from a norm which is known to us to a set function, and for the other ones that we got from the set function to a new norm.",
            "So to go from an unknown to a set function considering the group norm with overlapping groups.",
            "So the first 2 talks before.",
            "There was a norm with the novel."
        ],
        [
            "Group, which is a special case of that one.",
            "So you take W and use some of our certain set of roots.",
            "So here Kitty Graphic G is a set of groups and you penalize W restricted to that group text Infinity norm and you sum all of that.",
            "So this is an instance of L1L Infinity norm and it is well known it will induce sparsity at the group level.",
            "But here since the groups."
        ],
        [
            "Overlapping is a bit different from simply group selection, So what you can show that you take some groups G and you said the corresponding GG200 at the end.",
            "If you take those groups and let's call H the set of those groups, you have put 2 zeros or the values which are in the Union.",
            "The Union of all groups G 4G plug into edge.",
            "So essentially the zeros we build that form, meaning that the complement of the support will be a platform.",
            "So here we see that this is how it was justified the beginning.",
            "We have a norm that we create a certain set of zeros.",
            "It turns out that it corresponds to a specific similar function, which is the following.",
            "You simply count the number of groups.",
            "So given a set a, you count the number of groups which, with nonempty intersection with your visual set and this as I show in the paper, which you can see in the poster.",
            "This provider justification, which is not only limited to specific patterns, basically provide a full prior on subsets."
        ],
        [
            "They need a second wait a second 2nd.",
            "The second possibility when you go from a similar function to a norm so special you can create new norms by looking at this as a list of all possible submodular functions.",
            "So the first one is the credibility based function, so we take a function G of the continuity of the upside of A and here you can show that Omega.",
            "So the norm is a combination linear combination of other statistics.",
            "So you take constant time to the largest value of W, then a constant times the second largest value of the value, and so on.",
            "So these have not been used so much machine learning, but at least now we have unified algorithms analysis for that, the ones which I prefer more is nonfatal priors.",
            "So usually in the supervised learning context, when you learn from the design matrix X and then people will be penalized by the L1 norm which correspond to the continuity.",
            "But it turns out that you can design better regularizer, which depends also on the design matrix.",
            "So if your frequent."
        ],
        [
            "Just you know, Malos Malos penalty, which depends on the degrees of freedom and essentially of that form.",
            "Not luck is not very lucky.",
            "This is not a similar function, but now if your version, the marginal likelihood, which is the traditional way of limited selection, leads to leads to leads to similar functions.",
            "So we can use that to do a supervised learning and is a poster gives more examples about that."
        ],
        [
            "Then let's go to the unified algorithms.",
            "So what we would like to do is to have a unique way of optimizing with all those norms.",
            "So the first thing you might want to try, you have a political norm, so you have a set of extreme points, so why not just use a linear programming?",
            "This is not possible because you have exponentially many constraints.",
            "We could use a gradient, but is too slow and instead I."
        ],
        [
            "Use approximate methods which will describe yesterday in Steven Wright's tutorial and those are well adapted to situations where you have the sum of differentiable term.",
            "So here it's my loss and a non differentiable term.",
            "This here this is my norm and those methods are efficient if you able to solve those problems efficiently, minimizing penalized problem, but with the quadratic term with diagonal Hessian and it turns out that for all similar functions you can.",
            "Minimizes I think, is equivalent to minimizing or similar function which is Lambda times F of a minus this modular function with the specific algorithms.",
            "So at the end we can have efficient algorithm to solve.",
            "The circle possible problem, so this has no complexity bounds, but it's empirically between PP squared depending on the other problem.",
            "So of course it's generated growth and it always works, but it never.",
            "It never is the most fastest algorithm and we have a companion paper by mayoral Jeanette and Anna Baginski.",
            "Myself, when we consider the case of overlapping groups and we show that can be we have a dedicated algorithm to be a lot faster than that.",
            "So I will skip that."
        ],
        [
            "So now let's go to the unified analysis.",
            "So here the very nice paper from last NIPS is the paper bag balm, which showed that if you have some decomposability property for the regularizer, you automatically get some nice some nice analysis.",
            "It doesn't hold in our case, but for all possible similar functions you can design the modified version of the composability, which allows us to derive general quizlet for support recovery, which.",
            "Exactly extend the known ones for the L1 norm.",
            "The L1L Infinity Norm, as well as a resource about high dimensional inference where again we extend known results for the L1 norm at the L1L Infinity norm, and if you want to know more about those results you can come to the poster."
        ],
        [
            "So to conclude, I presented you away to achieve total sparsity through similar functions, so have not focused to merge in this talk on the applications, but we have a lot of applications going on in different fields where structure is highly relevant, such as image, audio and text.",
            "I've shown that unifies set of analysis and algorithm for those problems and of course this is not limited."
        ],
        [
            "To a similar functions or nondecreasing nondecreasing?",
            "Similar functions?",
            "We're currently working on extending these two symmetric similar functions, so symmetric, so it's not increasing, but it goes up and down.",
            "And in fact, in 3rd out that you replace putting a prior on the support.",
            "So what is a support?",
            "This is a level set double equals 0, so replace only acting this report by trying to act on all possible level sets of your function an over the same type of analysis can be carried carry through with like unified algorithms and analysis.",
            "Of course lies there is life beyond convexity and similar functions, so we're considering other extensions as well as well as links with greedy methods.",
            "OK so here are present in the convex framework, but of course greedy methods could also be considered.",
            "And finally there is a nice line of work that's been going on in machine learning is to extend sparsity in vector to sparsity matrices.",
            "And of course we could consider that as well.",
            "So thank you for your attention.",
            "So I'm sure there are a couple of questions.",
            "Let me just start with one you so you presented.",
            "Basically something like an optimization approach.",
            "You could take it up to minus debt and try to do marginalization.",
            "Have you thought about whether that would be possible?",
            "And that's a very good questions.",
            "That's not my typical line of work, but I think it's doable.",
            "Maybe not for the higher on subsets, but I guess it would be doable for the low back extension if you take E to the minus F of the value then I will tell you can integrate that out with a decent amount of time.",
            "But I have not done it yet.",
            "It's doable, but not try for one.",
            "Question over there.",
            "So if someone were to show a lower bound for submodular function minimization of something like end of the fifth, wouldn't that mean that your proximal regularization for your structured norms also would be that difficult?",
            "Yeah, so here, good question.",
            "So here essentially.",
            "So here are the.",
            "This is empirical complexity.",
            "OK if I just took my simulations and took a leverage of the running time true, the official time for similar function minimization is owed to the peer to peer.",
            "Six to six.",
            "So it's very slow.",
            "But it is also known that community that the fastest algorithm I'll, not the ones with the complexity bound.",
            "So here for sure, I can't say in advance that is going to be over the square.",
            "OK, but this being said, there are lot of special cases.",
            "This one is.",
            "This is one of them, where you can.",
            "You have exact complexity bounds, which is a lot slower than P to 6 smoke like P square are a bit more.",
            "Do we have any further questions?",
            "Otherwise, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi good afternoon.",
                    "label": 0
                },
                {
                    "sent": "So what I will.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sun today is a structured sparsity, and this could be achieved through a similar functions.",
                    "label": 0
                },
                {
                    "sent": "And so at the beginning of the talk I will start to motivate why we need a structured sparsity, and essentially it will mean going going beyond the L1 norm and its relationship between the Academy cardinality function.",
                    "label": 1
                },
                {
                    "sent": "So this will be done for similar functions and the tool I will need from that type of work is alove edge extension, so I will describe what this means and they will describe the actual content, the novel contributions of this work and show allocation of explicit.",
                    "label": 0
                },
                {
                    "sent": "In addition, so supports and show a unified algorithms and analysis.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a bit a small recap on sparsity, so we consider some data SII so N pairs where XYZ in RP and why I is in R and we consider the regularised empirical risk minimization when we have a new minimized with respect to our linear predictor data fitting term here plus penalty.",
                    "label": 0
                },
                {
                    "sent": "And here's the penalty is here to promote sparsity and the main example is the norm, and this has many names depending on where you come from.",
                    "label": 1
                },
                {
                    "sent": "Baseball suits in signal processing and less so in statistics and machine learning, so why would you want to use sparsity?",
                    "label": 1
                },
                {
                    "sent": "So I see many reasons, but two important ones are the interpretability.",
                    "label": 1
                },
                {
                    "sent": "Essentially, when you have many zeros is like a proxy for interpretability, and the second reason is as it was well explained yesterday.",
                    "label": 0
                },
                {
                    "sent": "Tutorial Video is a possibility of doing high dimensional inference.",
                    "label": 0
                },
                {
                    "sent": "So here if P the number of variables and end the number of observations, P can be a lot a lot larger than N as long as P is less than exponential N. So it's just to mention it this is not sparsity is not specific to supervise learning.",
                    "label": 1
                },
                {
                    "sent": "It can be also applied to unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "Essentially, when you only have observations and you want to build the design matrix so that it is well adapted to your entire set of observations, essentially it is doing PCA and sparse versions of PCA, which are often called Dictionary dictionary learning.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why do you want structured sparsity?",
                    "label": 0
                },
                {
                    "sent": "First one to be my main reason is that it's OK to have a lot of zeros, but in many cases you want to zero to organize in a special way.",
                    "label": 0
                },
                {
                    "sent": "And we have done some work with hydrogen Atom and Gramma belinski to try to in the PCA context to learn principle components with a given structure.",
                    "label": 0
                },
                {
                    "sent": "So this is the first aspect.",
                    "label": 0
                },
                {
                    "sent": "Second aspect is when you want to learn decompositions onto those components which structure.",
                    "label": 0
                },
                {
                    "sent": "So here you assume that traditional elements they'll even though given the topology and motivating example, is taken from topic.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Models quite similar to the previous spotlight, so in topic models you might want to be the Yaqui of topics where you want to decompose every document as a decomposition of topics where you start with the root topic and go down the tree by building your documents to the topics.",
                    "label": 0
                },
                {
                    "sent": "Longer paths of the tree so the main one of the main themes of what we're doing with positive using norms is to be able to do that kind of thing by simply changing one line of code in a standard algorithm for dictionary.",
                    "label": 0
                },
                {
                    "sent": "So just replace the L1 norm by the norm that I'm going to present and you can have that type of behavior.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course the portability is not the only the only thing I want to do a bit more so.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you know more about your data and yuanyu prior fits matches that knowledge, you should predict better and also you can use this structure sparsity to be numerically efficient.",
                    "label": 0
                },
                {
                    "sent": "So I have some works.",
                    "label": 0
                },
                {
                    "sent": "Two years ago when I was doing non general viable selection and essentially I had to search over all possible to the P subsets where peas and number of variables.",
                    "label": 0
                },
                {
                    "sent": "So if P is 100.",
                    "label": 0
                },
                {
                    "sent": "Makes a lot of subsets and by using structures pacitti you can design polynomial time algorithms to do that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's go down to to design ways of inducing a structured sparsity.",
                    "label": 0
                },
                {
                    "sent": "But before doing that, let's go back a bit to see what how you would justify the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "So the way I like is to justify the L1 norm as a convex approximation of the LO Casino.",
                    "label": 0
                },
                {
                    "sent": "So just for notations W in RP and throughout the talk we called Capital V, this discrete set one 2P and the support of W will be the set of nonzero indices, so the L0 penalty, essentially the number of the cardinality of the support.",
                    "label": 0
                },
                {
                    "sent": "And one way of defending the element norm is at the convex envelope of the L0 norm.",
                    "label": 0
                },
                {
                    "sent": "So what is a convex envelope?",
                    "label": 0
                },
                {
                    "sent": "It is the largest convex lower bound of the given function.",
                    "label": 0
                },
                {
                    "sent": "So here this is the example in 1D.",
                    "label": 0
                },
                {
                    "sent": "When you have the LOL0 norm, so it's zero at zero and one everywhere else.",
                    "label": 0
                },
                {
                    "sent": "And if you consider that interval minus one one, if you want to be convex to be below the red curve between minus one and one, and to be the largest possible is clearly the.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the traditional way of defending the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "Essentially we're going to do in.",
                    "label": 0
                },
                {
                    "sent": "This talk is to consider other functions, then the cardinality and play the same trick trained to derive up.",
                    "label": 0
                },
                {
                    "sent": "Sorry, trying to derive the largest.",
                    "label": 0
                },
                {
                    "sent": "Convex lower bound and this will be done for a particular set of functions which are similar function.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what are those functions?",
                    "label": 0
                },
                {
                    "sent": "So before I describe them that we point out the nice book by fujishige on the topic, which is quite comprehensive, but which is a tough read.",
                    "label": 0
                },
                {
                    "sent": "So it is.",
                    "label": 0
                },
                {
                    "sent": "This is why I wrote a tutorial which is directed to a machine learning audience where I assume are basic knowledge connection.",
                    "label": 0
                },
                {
                    "sent": "Alesis and we derive all results which are useful for working with similar functions.",
                    "label": 0
                },
                {
                    "sent": "So let's go down to the definition.",
                    "label": 0
                },
                {
                    "sent": "So we define.",
                    "label": 0
                },
                {
                    "sent": "This is a set functions which go from sets.",
                    "label": 0
                },
                {
                    "sent": "So 2002 V is a set of subsets, and for each set you give a value and you're still modular.",
                    "label": 0
                },
                {
                    "sent": "If you take any two subsets A&BF of air plus FAB is larger than FA intersect B&F Union B.",
                    "label": 0
                },
                {
                    "sent": "So if F is the is the cardinality function or any measure, then this you have equality over there.",
                    "label": 0
                },
                {
                    "sent": "So it's well known and to be submodular you just need one inequality.",
                    "label": 0
                },
                {
                    "sent": "Another way of defining similarity, which is often easier to understand, is this one, where you take any set A and you add one more, one more element of the set and you see how much you have gained by using that element.",
                    "label": 0
                },
                {
                    "sent": "So F of a non K -- F of a you are similar.",
                    "label": 0
                },
                {
                    "sent": "If this function is non increasing.",
                    "label": 0
                },
                {
                    "sent": "So this is a notion of diminishing return and this leads to the first intuition about similar functions.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is there in fact concave function and one example of that.",
                    "label": 0
                },
                {
                    "sent": "If you take functions of the cardinality G of cardinality of a, it is similar.",
                    "label": 0
                },
                {
                    "sent": "As soon as G is concave.",
                    "label": 1
                },
                {
                    "sent": "So this shows also that the commonality is a similar function.",
                    "label": 0
                },
                {
                    "sent": "Although it is a defined almost like a concave function, it behaves like a convex.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function OK, why?",
                    "label": 0
                },
                {
                    "sent": "Because you have a polynomial time algorithms to minimize them.",
                    "label": 0
                },
                {
                    "sent": "You have a nice conjugacy theory and you have links with the low back extension, which I will describe in the next line.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So those have been used in several areas in signal processing, computer vision or machine learning, so there's a whole series of work about graph cuts in computer vision and also nice work in optimal design and other things by house and get string.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's not defined the main tool that I'm going to use in this talk, which is a large extension, so this can be defined, But for any any set functions.",
                    "label": 0
                },
                {
                    "sent": "So how do you compute that?",
                    "label": 0
                },
                {
                    "sent": "You take a W in RP you order in its components in decreasing order, so WG One WG 2 up to WJP and then you take a linear combinations linear combination of WJC with that weight.",
                    "label": 0
                },
                {
                    "sent": "So the formulas are not so important.",
                    "label": 0
                },
                {
                    "sent": "What is important is the following property that return below.",
                    "label": 0
                },
                {
                    "sent": "Which is the fact that if W is the indicator vector of the set a, so one is the component belongs to A and 0 otherwise, then the value of the love at extension is essentially the value of the function.",
                    "label": 0
                },
                {
                    "sent": "So this means that you can since you can identify subsets of V with the set 01 to the P, you essentially extended the function capital F from zero one to appeal to the whole RP.",
                    "label": 0
                },
                {
                    "sent": "Moreover, F is piecewise.",
                    "label": 0
                },
                {
                    "sent": "Define and positively imaginaries.",
                    "label": 0
                },
                {
                    "sent": "So this is always true for all possible set functions and a nice nice thing is that if F is similar capital F is similar if you only if it's love at extension is convex.",
                    "label": 1
                },
                {
                    "sent": "So this is really an important aspect of similar analysis, and this list for example to this nice property that minimizing love at extension is equivalent to minimizing the function itself.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's go down to the contributions of of my work.",
                    "label": 0
                },
                {
                    "sent": "So here we are considering similar function F, which is similar and it will be nondecreasing.",
                    "label": 0
                },
                {
                    "sent": "So why not decreasing?",
                    "label": 0
                },
                {
                    "sent": "Because I want to favor small subsets, so I want to penalize more larger subsets so the main proposition is if you consider the function that takes the value you take is support and you penalize, and you can compute the set function that for those people W. Then you get the functions from RP to R and it turns out that the convex envelope of that function, so the lowest the largest convex lower bound is exactly the large lower extension computed as the vector composed of the absolute values of the components of W.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, just as a safety check, if you take F being the cardinality, the large extension is just a sum of components and you get back the fact that the L1 norm is a convex envelope of the cognitive function.",
                    "label": 0
                },
                {
                    "sent": "So now I've defined Norm, Omega and the rest of the talk will be dedicated to analyze this number.",
                    "label": 0
                },
                {
                    "sent": "So first, since F here F is piecewise affine, my norm is a polyhedral, so it's unit ball as a polytope, and it's traditional in this setting to draw that ball.",
                    "label": 1
                },
                {
                    "sent": "OK, so here I've drawn the boarding 2D, so here was the important aspect.",
                    "label": 1
                },
                {
                    "sent": "Is the fact that all the extreme points of the bowl are always on the same directions you have once along the axis and once along the main diagonal's and the main action of the lack of adding different values for F is the fact that you will have some extreme points disappear and some which won't disappear.",
                    "label": 0
                },
                {
                    "sent": "And so for example the one norm is a case where essentially those this points this point.",
                    "label": 0
                },
                {
                    "sent": "This extreme points disappear the L Infinity norm, which just so sad case.",
                    "label": 0
                },
                {
                    "sent": "It's a case where that point disappears and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so at the end this will define the set of corners and this will you can define so-called stable sets which are sets so that all larger subsets are they strictly larger function value and you can prove that stable set or stable.",
                    "label": 1
                },
                {
                    "sent": "Essentially those are those are the only one which can be obtained when you regularize by that name.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can go to three dimension and show other balls, but I want describe them.",
                    "label": 0
                },
                {
                    "sent": "You can see then in the poster and try to go to examples in model.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mentions.",
                    "label": 0
                },
                {
                    "sent": "And I believe we give two types of examples.",
                    "label": 0
                },
                {
                    "sent": "The first one will go from a norm which is known to us to a set function, and for the other ones that we got from the set function to a new norm.",
                    "label": 0
                },
                {
                    "sent": "So to go from an unknown to a set function considering the group norm with overlapping groups.",
                    "label": 1
                },
                {
                    "sent": "So the first 2 talks before.",
                    "label": 0
                },
                {
                    "sent": "There was a norm with the novel.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Group, which is a special case of that one.",
                    "label": 0
                },
                {
                    "sent": "So you take W and use some of our certain set of roots.",
                    "label": 0
                },
                {
                    "sent": "So here Kitty Graphic G is a set of groups and you penalize W restricted to that group text Infinity norm and you sum all of that.",
                    "label": 0
                },
                {
                    "sent": "So this is an instance of L1L Infinity norm and it is well known it will induce sparsity at the group level.",
                    "label": 1
                },
                {
                    "sent": "But here since the groups.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overlapping is a bit different from simply group selection, So what you can show that you take some groups G and you said the corresponding GG200 at the end.",
                    "label": 0
                },
                {
                    "sent": "If you take those groups and let's call H the set of those groups, you have put 2 zeros or the values which are in the Union.",
                    "label": 0
                },
                {
                    "sent": "The Union of all groups G 4G plug into edge.",
                    "label": 0
                },
                {
                    "sent": "So essentially the zeros we build that form, meaning that the complement of the support will be a platform.",
                    "label": 0
                },
                {
                    "sent": "So here we see that this is how it was justified the beginning.",
                    "label": 0
                },
                {
                    "sent": "We have a norm that we create a certain set of zeros.",
                    "label": 0
                },
                {
                    "sent": "It turns out that it corresponds to a specific similar function, which is the following.",
                    "label": 0
                },
                {
                    "sent": "You simply count the number of groups.",
                    "label": 0
                },
                {
                    "sent": "So given a set a, you count the number of groups which, with nonempty intersection with your visual set and this as I show in the paper, which you can see in the poster.",
                    "label": 0
                },
                {
                    "sent": "This provider justification, which is not only limited to specific patterns, basically provide a full prior on subsets.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They need a second wait a second 2nd.",
                    "label": 0
                },
                {
                    "sent": "The second possibility when you go from a similar function to a norm so special you can create new norms by looking at this as a list of all possible submodular functions.",
                    "label": 0
                },
                {
                    "sent": "So the first one is the credibility based function, so we take a function G of the continuity of the upside of A and here you can show that Omega.",
                    "label": 0
                },
                {
                    "sent": "So the norm is a combination linear combination of other statistics.",
                    "label": 1
                },
                {
                    "sent": "So you take constant time to the largest value of W, then a constant times the second largest value of the value, and so on.",
                    "label": 0
                },
                {
                    "sent": "So these have not been used so much machine learning, but at least now we have unified algorithms analysis for that, the ones which I prefer more is nonfatal priors.",
                    "label": 0
                },
                {
                    "sent": "So usually in the supervised learning context, when you learn from the design matrix X and then people will be penalized by the L1 norm which correspond to the continuity.",
                    "label": 1
                },
                {
                    "sent": "But it turns out that you can design better regularizer, which depends also on the design matrix.",
                    "label": 0
                },
                {
                    "sent": "So if your frequent.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just you know, Malos Malos penalty, which depends on the degrees of freedom and essentially of that form.",
                    "label": 0
                },
                {
                    "sent": "Not luck is not very lucky.",
                    "label": 0
                },
                {
                    "sent": "This is not a similar function, but now if your version, the marginal likelihood, which is the traditional way of limited selection, leads to leads to leads to similar functions.",
                    "label": 0
                },
                {
                    "sent": "So we can use that to do a supervised learning and is a poster gives more examples about that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then let's go to the unified algorithms.",
                    "label": 0
                },
                {
                    "sent": "So what we would like to do is to have a unique way of optimizing with all those norms.",
                    "label": 0
                },
                {
                    "sent": "So the first thing you might want to try, you have a political norm, so you have a set of extreme points, so why not just use a linear programming?",
                    "label": 1
                },
                {
                    "sent": "This is not possible because you have exponentially many constraints.",
                    "label": 1
                },
                {
                    "sent": "We could use a gradient, but is too slow and instead I.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use approximate methods which will describe yesterday in Steven Wright's tutorial and those are well adapted to situations where you have the sum of differentiable term.",
                    "label": 0
                },
                {
                    "sent": "So here it's my loss and a non differentiable term.",
                    "label": 0
                },
                {
                    "sent": "This here this is my norm and those methods are efficient if you able to solve those problems efficiently, minimizing penalized problem, but with the quadratic term with diagonal Hessian and it turns out that for all similar functions you can.",
                    "label": 0
                },
                {
                    "sent": "Minimizes I think, is equivalent to minimizing or similar function which is Lambda times F of a minus this modular function with the specific algorithms.",
                    "label": 1
                },
                {
                    "sent": "So at the end we can have efficient algorithm to solve.",
                    "label": 0
                },
                {
                    "sent": "The circle possible problem, so this has no complexity bounds, but it's empirically between PP squared depending on the other problem.",
                    "label": 1
                },
                {
                    "sent": "So of course it's generated growth and it always works, but it never.",
                    "label": 0
                },
                {
                    "sent": "It never is the most fastest algorithm and we have a companion paper by mayoral Jeanette and Anna Baginski.",
                    "label": 0
                },
                {
                    "sent": "Myself, when we consider the case of overlapping groups and we show that can be we have a dedicated algorithm to be a lot faster than that.",
                    "label": 0
                },
                {
                    "sent": "So I will skip that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's go to the unified analysis.",
                    "label": 0
                },
                {
                    "sent": "So here the very nice paper from last NIPS is the paper bag balm, which showed that if you have some decomposability property for the regularizer, you automatically get some nice some nice analysis.",
                    "label": 0
                },
                {
                    "sent": "It doesn't hold in our case, but for all possible similar functions you can design the modified version of the composability, which allows us to derive general quizlet for support recovery, which.",
                    "label": 0
                },
                {
                    "sent": "Exactly extend the known ones for the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "The L1L Infinity Norm, as well as a resource about high dimensional inference where again we extend known results for the L1 norm at the L1L Infinity norm, and if you want to know more about those results you can come to the poster.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to conclude, I presented you away to achieve total sparsity through similar functions, so have not focused to merge in this talk on the applications, but we have a lot of applications going on in different fields where structure is highly relevant, such as image, audio and text.",
                    "label": 0
                },
                {
                    "sent": "I've shown that unifies set of analysis and algorithm for those problems and of course this is not limited.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To a similar functions or nondecreasing nondecreasing?",
                    "label": 0
                },
                {
                    "sent": "Similar functions?",
                    "label": 0
                },
                {
                    "sent": "We're currently working on extending these two symmetric similar functions, so symmetric, so it's not increasing, but it goes up and down.",
                    "label": 0
                },
                {
                    "sent": "And in fact, in 3rd out that you replace putting a prior on the support.",
                    "label": 0
                },
                {
                    "sent": "So what is a support?",
                    "label": 0
                },
                {
                    "sent": "This is a level set double equals 0, so replace only acting this report by trying to act on all possible level sets of your function an over the same type of analysis can be carried carry through with like unified algorithms and analysis.",
                    "label": 0
                },
                {
                    "sent": "Of course lies there is life beyond convexity and similar functions, so we're considering other extensions as well as well as links with greedy methods.",
                    "label": 1
                },
                {
                    "sent": "OK so here are present in the convex framework, but of course greedy methods could also be considered.",
                    "label": 0
                },
                {
                    "sent": "And finally there is a nice line of work that's been going on in machine learning is to extend sparsity in vector to sparsity matrices.",
                    "label": 0
                },
                {
                    "sent": "And of course we could consider that as well.",
                    "label": 0
                },
                {
                    "sent": "So thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "So I'm sure there are a couple of questions.",
                    "label": 0
                },
                {
                    "sent": "Let me just start with one you so you presented.",
                    "label": 0
                },
                {
                    "sent": "Basically something like an optimization approach.",
                    "label": 0
                },
                {
                    "sent": "You could take it up to minus debt and try to do marginalization.",
                    "label": 0
                },
                {
                    "sent": "Have you thought about whether that would be possible?",
                    "label": 0
                },
                {
                    "sent": "And that's a very good questions.",
                    "label": 0
                },
                {
                    "sent": "That's not my typical line of work, but I think it's doable.",
                    "label": 0
                },
                {
                    "sent": "Maybe not for the higher on subsets, but I guess it would be doable for the low back extension if you take E to the minus F of the value then I will tell you can integrate that out with a decent amount of time.",
                    "label": 0
                },
                {
                    "sent": "But I have not done it yet.",
                    "label": 0
                },
                {
                    "sent": "It's doable, but not try for one.",
                    "label": 0
                },
                {
                    "sent": "Question over there.",
                    "label": 0
                },
                {
                    "sent": "So if someone were to show a lower bound for submodular function minimization of something like end of the fifth, wouldn't that mean that your proximal regularization for your structured norms also would be that difficult?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so here, good question.",
                    "label": 0
                },
                {
                    "sent": "So here essentially.",
                    "label": 0
                },
                {
                    "sent": "So here are the.",
                    "label": 0
                },
                {
                    "sent": "This is empirical complexity.",
                    "label": 0
                },
                {
                    "sent": "OK if I just took my simulations and took a leverage of the running time true, the official time for similar function minimization is owed to the peer to peer.",
                    "label": 0
                },
                {
                    "sent": "Six to six.",
                    "label": 0
                },
                {
                    "sent": "So it's very slow.",
                    "label": 0
                },
                {
                    "sent": "But it is also known that community that the fastest algorithm I'll, not the ones with the complexity bound.",
                    "label": 0
                },
                {
                    "sent": "So here for sure, I can't say in advance that is going to be over the square.",
                    "label": 0
                },
                {
                    "sent": "OK, but this being said, there are lot of special cases.",
                    "label": 0
                },
                {
                    "sent": "This one is.",
                    "label": 0
                },
                {
                    "sent": "This is one of them, where you can.",
                    "label": 0
                },
                {
                    "sent": "You have exact complexity bounds, which is a lot slower than P to 6 smoke like P square are a bit more.",
                    "label": 0
                },
                {
                    "sent": "Do we have any further questions?",
                    "label": 0
                },
                {
                    "sent": "Otherwise, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}