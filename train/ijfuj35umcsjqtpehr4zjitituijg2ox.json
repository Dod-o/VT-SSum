{
    "id": "ijfuj35umcsjqtpehr4zjitituijg2ox",
    "title": "Spectral Learning of General Weighted Automata via Constrained Matrix Completion",
    "info": {
        "author": [
            "Borja Balle, Departament de Llenguatges i Sistemes Inform\u00e0tics, Technical University of Catalonia"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/nips2012_balle_spectral_learning/",
    "segmentation": [
        [
            "That's joint work with Mary Amorian, the title spectral learning of general weighted Automata be a constraint matrix completion that may be a bit scary, but the problem we're solving is or trying to solve this easy to state.",
            "So the problem."
        ],
        [
            "Basically, regression on strings.",
            "That means that we have a data which comes in the form of an IID sample an as that's generated from some distribution that contains example labeled pairs.",
            "And here's, the examples are just finding strings over some finite alphabet that we call Sigma, and the strings are I nthe labels are just a real numbers, so the goal is given these sample is to run a regressor that we call F hot, which is basically a function from strings to numbers.",
            "So Sigma Star is the annotation for just the set of all strings over this finite alphabet Sigma, and we want this regressor to have a small generalization error.",
            "So the task is just a regression task and there are many examples of these tasking applications.",
            "So for example, if you can see the reinforcement learning, you may have that you may observe several traces over of a model.",
            "These traces are strings of actions and observations, and for each of these traces you may have a reward.",
            "And you may want to learn this this reward function.",
            "In biology, for example, you may have individuals.",
            "Characterized by a sequence of DNA or amino acids an you may have biological measurement that's this real number for each of these individuals, and you may want to do a regression task on this biological measurement or even natural language processing.",
            "You may have like sentences, desire strings which have been labeled by some expert according to some criteria, and you want to infer a model for this criteria.",
            "So in order to solve this problem, what we propose to do is to find hypothesis that functions from string to numbers in some."
        ],
        [
            "Prothesis class, which is a prothesis class of weighted finite outdoor it.",
            "So weighted final automata is basically a finite state machine.",
            "So for example you can say it has 10 States and these finite state machine will consume a string and will output a real number.",
            "So it's effectively computing one of these functions that we were interested in, and it does that by at each point it has a has a vector and dimensional vector that represents the current state of the machine and consumes a symbol and it transforms this vector by applying a linear operator.",
            "These are these transition matrices A, so the initial of the machine is initialized using this Alpha vector an you apply the linear operator corresponding to each of the symbols in this string, and you end up computing.",
            "The function here, like that at the end you take the actor, you have a new computer dot product with the final weights, which is this bitter.",
            "And you obtain a real number.",
            "So in order to parameterized, one of these functions from Sync 2 numbers, you see that you just need roughly and squared times the size of the alphabet parameters, and there are basically two ways to represent this, which automata one is the algebraic representation where you give explicitly these transition matrices and initial and final weights, and the other is to give a directed graph, or like an automaton representation where the states are represented by nodes and the transition weights are represented by the.",
            "The arts in the FA.",
            "So the main question that one may want to ask is why do we choose weighted finite automata?",
            "And there are several reasons for that.",
            "The first reason is is that it's an expressive class and it has been well studied in automata theory community.",
            "There's also a rich family of algorithms for manipulating this kind of automata, so there's a minimization determinization removal product and all sorts of efficient algorithms for combining this kind of automata's.",
            "And it turns out that also this kind of automata's have been useful in many applications.",
            "For example, speech recognition, image processing, optical character recognition, and system testing.",
            "So in."
        ],
        [
            "Just one line.",
            "Our result is basically a learning algorithm that can solve this regression problem that's given a data set of strings and relabels produces a weighted finite automata, and the main ingredients for this algorithm one is this spectral learning and the other is mattix completion.",
            "So what I will do in the rest of the talk is basically do three things.",
            "The first is I will recall a little bit about the spectral method we're using and see why it's useful in this setting.",
            "Then I will describe a family of learning algorithms.",
            "That we propose on you.",
            "Hopefully you'll see why matrix completion fits in there.",
            "And the third thing is I'm going to give a generalization bound for one particular algorithm in this family."
        ],
        [
            "So this is going to be just a very brief description of what spectral learning is, and The thing is going to be.",
            "I'm going to tell you what the spectral method does.",
            "I'm not going to tell you how it does it, I don't want to get into all the formulas, but there's going to be a workshop on Friday on this kind of method.",
            "So if you want to know more, you can stop by.",
            "So what the spectral learning is about?",
            "From my point of view, it can be summarized in this diagram.",
            "That says that if you have your target model F, so from F you can define a matrix M. This is a matrix of observable quantities that are related to your model F. On the learning is comes from the fact that if you sample F, so if you get the sample from F. You can compute the matrix M hot, which hopefully will approximate this.",
            "This or this M defined in terms of the model and then you apply the spectral learning method to this M hat, which is basically a set of linear algebra operations that the more important is the nasb decomposition.",
            "Hence the spectral name and from this am hat you will produce a model F hat and nice things that the theory says that if M hot is close to M then the model you produce F hat.",
            "Will be close to F. So this kind of methods have been."
        ],
        [
            "I did recently too many models so there can be applied to sequential models where the data are strings like we do here.",
            "You can also apply them to tree like structures and other kinds of graphical models, so there's been a lot of work recently on this stuff, starting with Hmm's on stochastic rational languages, and it turns out that when you take this method and see what happens with weighted automata, these metrics of observables, this M is a well known matrix, at least in automata."
        ],
        [
            "So it turns out that it is a Hankel matrix.",
            "So what is a Hankel matrix?",
            "How cool matrix for a function F from strings to numbers is basically a matrix whose rows are indexed by strings, and we call this trinks prefixes and the columns are indexed again by strings.",
            "But we call this string suffixes.",
            "So on the entries in this matrix adjust evaluations of F. So in particularly we have a prefix U and suffix B which define and entering the matrix.",
            "The number we have to put in there is the evaluation of F on the string you obtain by concatenating the prefix you with the suffix B.",
            "So for example, if I consider the finite alphabet with symbols AMB, I can define the function that given some string this F of X computes how many days does this string half.",
            "And in this case this hunkele.",
            "This matrix here is Hankel matrix for this function and you can see that, for example, if I take the first row which correspond to prefix A and I take the second column which correspond to suffix A.",
            "That means that in here I have to find the evaluation of the function on the string obtained by concatenating A&A.",
            "So just ring a which has two ways, so that's why you have an A in here and you can see immediately from the definition of Hankel matrix that this Hankel matrix is going to have some redundancies by definition, because if I have two prefixes and suffixes that give me the same string with strings, have paper can be partitioned in prefixes and suffixes in several ways, then I must have the same entry in the two corresponding places.",
            "So that's what happens here with these two, which again, if I have the prefix AA and suffix empty string which is denoted by epsilon I.",
            "This string also has 2A, so these two and these two represent the same evaluation of the function."
        ],
        [
            "So when you go and try to spectral learning with weighted finite automata, you will see is that this entry wise redundancies will play a role.",
            "But there's also wrote a role to be played by another kind of redundancy in the Matrix, which is the rank.",
            "So if you observe in this example I've just gave you the rank of this Hankel matrix is too because the 1st and 3rd columns are equal.",
            "So these are redundancies, will also play a role and this is a nice theoretical fact that.",
            "So suppose that you have a function from strings to numbers.",
            "You can take many, many Hankel matrices for this function.",
            "In fact, you can take arbitrary infinitely many Hankel matrices, so if you keep taking larger and larger and larger matrices, either two things can happen.",
            "The rank of this of these matrices will go to Infinity, or the rank of these matrices will stay bounded if the rank stays bounded, it will reach the top.",
            "So these these number this this rank this find number is the rank of the function and it turns out that the rank of the function is the number of states that you need in a weighted finite automata to compute such a function.",
            "So you will see that this will also play a role in our algorithm.",
            "So now that we know that we need a Hankel matrix to apply the spectral method to weighted finite automata, where do we get the Hankel matrix from?"
        ],
        [
            "And the problem is that in the typical application that we have, since in previous works.",
            "The hung the function you want to learn.",
            "The model is a probabilistic function, so for example it's F of X is the probability of strings, and in this case it turns out that the entries in your matrix are probabilities, which means that if you have a sample, you can compute an approximation of this matrix just by taking the empirical frequencies.",
            "And of course if there's some string that doesn't appear in your sample, then you can put a zero there, because that's the relative.",
            "That's the empirical frequency of that string.",
            "But in our case, the entries in the Hankel matrix don't satisfy this constraint.",
            "They're not relative and relative.",
            "Frequencies, they're just labels we observe in the sample, so it turns out that if you have a sample you can fill in this Hankel matrix, but it you may have missing entries, so and that missing entries don't give you an approximation of Hankel matrix, you can fit into the spectral method.",
            "So what we propose to do is to fill in this matrix.",
            "This entry is using."
        ],
        [
            "Matrix completion so matrix completion has been studied recently in many contexts and machine learning as well an on the important thing here is that matrix completion can usually be be solved using convex optimization problem, But it turns out that these algorithms that has been proposed.",
            "Don't take into account one particular thing of our problem, which is that the spectral method expects Hankel matrix.",
            "That means that the matrix has to satisfy some entrywise constraints.",
            "So Luckily for us, it turns out that you can also propose a convex optimization problem to find a Hankel matrix like you usually do in matrix completion, that is, minimizing some loss function that has a term that controls that your matrix agrees with what you have seen in the sample, and there's a regularizer term that controls the complexity of your matrix.",
            "But Furthermore, there's a third ingredient in this optimization is.",
            "That result must be Hankel matrix, but since this this constraint can be posed as just equalities between entries, this is a convex constraint, so you can still use convex optimization to solve the problem.",
            "And it turns out that here in this particular application of matrix completion, regularising plays a double role.",
            "The first is the usual role in Mathis completion.",
            "That's to solve the yield posters of completing a matrix.",
            "But the second is that regularising on on the Hankel matrix.",
            "If you obtain less complex Hankel matrices, this will lead to simpler weighted finite automata, like for example.",
            "As I said in the rank, if you try to minimize the nuclear norm, you hopefully will get a low rank matrix and that will give you a way to automata with fewer states, so that's that's regularising for the spectral method to come.",
            "So now we have all the angry."
        ],
        [
            "And so we can propose our family of algorithms.",
            "So this is basically.",
            "And you take your sample.",
            "You feeling this?",
            "This Hankel matrix that you have defined some rows and columns.",
            "You will have some missing entries.",
            "You apply constraint matrix completion you get a Hankel matrix that you can fit into a spectral learning and you get a weighted finite automata.",
            "So this is this is in fact the whole family of algorithms because it can be parameterized by.",
            "How do you choose the rows and columns?",
            "How?",
            "What kind of mattress completion algorithm you're going to use and what your regular sized regularising parameters are going to be so?",
            "The question is, once you have this family is OK.",
            "This seems to work, but can we give some bounds?",
            "Can we prove something about this algorithm?",
            "It's going to work."
        ],
        [
            "So it turns out that for one particular instance of this family, you making three hypothesis, you can prove a generalization bound.",
            "And the processes that you need some reasonable assumption on the distribution that generates these strings are real numbers.",
            "Then you take the completion loss to be absolute loss.",
            "And you take the regularizer to be from the Frobenius norm squared, and in this case it's interesting because this is.",
            "This regularizer is differentiable.",
            "So once you have that, you can prove that with high probability over the choice of a sample that has M examples.",
            "The output of your algorithm, the generalization error of the output of your algorithm.",
            "This quantity, here defining expected absolute loss.",
            "Will be less or equal than the empirical error plus a term that goes to zero with a number of examples in this way.",
            "And to prove that it turns out that you cannot use the typical bombing children, keys arguments or Rademacher complexity arguments, because we don't know how to compute these quantities for weighted finite automata.",
            "So I think that this is a an interesting open problem, But what we do is we do a stability analysis.",
            "So what we do is a joint stability analysis of the matrix completion and spectral learning all together to get this generalization bound.",
            "And it turns out that this kind of stability analysis has been also used for other regression tasks so.",
            "It's it's quite useful, inconvenient in these settings, so that's all."
        ],
        [
            "I had to say today so if you want to know more."
        ],
        [
            "You can stop by the poster this evening, which is D 47.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's joint work with Mary Amorian, the title spectral learning of general weighted Automata be a constraint matrix completion that may be a bit scary, but the problem we're solving is or trying to solve this easy to state.",
                    "label": 0
                },
                {
                    "sent": "So the problem.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, regression on strings.",
                    "label": 0
                },
                {
                    "sent": "That means that we have a data which comes in the form of an IID sample an as that's generated from some distribution that contains example labeled pairs.",
                    "label": 0
                },
                {
                    "sent": "And here's, the examples are just finding strings over some finite alphabet that we call Sigma, and the strings are I nthe labels are just a real numbers, so the goal is given these sample is to run a regressor that we call F hot, which is basically a function from strings to numbers.",
                    "label": 0
                },
                {
                    "sent": "So Sigma Star is the annotation for just the set of all strings over this finite alphabet Sigma, and we want this regressor to have a small generalization error.",
                    "label": 1
                },
                {
                    "sent": "So the task is just a regression task and there are many examples of these tasking applications.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you can see the reinforcement learning, you may have that you may observe several traces over of a model.",
                    "label": 0
                },
                {
                    "sent": "These traces are strings of actions and observations, and for each of these traces you may have a reward.",
                    "label": 0
                },
                {
                    "sent": "And you may want to learn this this reward function.",
                    "label": 0
                },
                {
                    "sent": "In biology, for example, you may have individuals.",
                    "label": 0
                },
                {
                    "sent": "Characterized by a sequence of DNA or amino acids an you may have biological measurement that's this real number for each of these individuals, and you may want to do a regression task on this biological measurement or even natural language processing.",
                    "label": 1
                },
                {
                    "sent": "You may have like sentences, desire strings which have been labeled by some expert according to some criteria, and you want to infer a model for this criteria.",
                    "label": 0
                },
                {
                    "sent": "So in order to solve this problem, what we propose to do is to find hypothesis that functions from string to numbers in some.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prothesis class, which is a prothesis class of weighted finite outdoor it.",
                    "label": 0
                },
                {
                    "sent": "So weighted final automata is basically a finite state machine.",
                    "label": 0
                },
                {
                    "sent": "So for example you can say it has 10 States and these finite state machine will consume a string and will output a real number.",
                    "label": 0
                },
                {
                    "sent": "So it's effectively computing one of these functions that we were interested in, and it does that by at each point it has a has a vector and dimensional vector that represents the current state of the machine and consumes a symbol and it transforms this vector by applying a linear operator.",
                    "label": 0
                },
                {
                    "sent": "These are these transition matrices A, so the initial of the machine is initialized using this Alpha vector an you apply the linear operator corresponding to each of the symbols in this string, and you end up computing.",
                    "label": 0
                },
                {
                    "sent": "The function here, like that at the end you take the actor, you have a new computer dot product with the final weights, which is this bitter.",
                    "label": 0
                },
                {
                    "sent": "And you obtain a real number.",
                    "label": 0
                },
                {
                    "sent": "So in order to parameterized, one of these functions from Sync 2 numbers, you see that you just need roughly and squared times the size of the alphabet parameters, and there are basically two ways to represent this, which automata one is the algebraic representation where you give explicitly these transition matrices and initial and final weights, and the other is to give a directed graph, or like an automaton representation where the states are represented by nodes and the transition weights are represented by the.",
                    "label": 0
                },
                {
                    "sent": "The arts in the FA.",
                    "label": 0
                },
                {
                    "sent": "So the main question that one may want to ask is why do we choose weighted finite automata?",
                    "label": 1
                },
                {
                    "sent": "And there are several reasons for that.",
                    "label": 0
                },
                {
                    "sent": "The first reason is is that it's an expressive class and it has been well studied in automata theory community.",
                    "label": 0
                },
                {
                    "sent": "There's also a rich family of algorithms for manipulating this kind of automata, so there's a minimization determinization removal product and all sorts of efficient algorithms for combining this kind of automata's.",
                    "label": 1
                },
                {
                    "sent": "And it turns out that also this kind of automata's have been useful in many applications.",
                    "label": 0
                },
                {
                    "sent": "For example, speech recognition, image processing, optical character recognition, and system testing.",
                    "label": 1
                },
                {
                    "sent": "So in.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just one line.",
                    "label": 0
                },
                {
                    "sent": "Our result is basically a learning algorithm that can solve this regression problem that's given a data set of strings and relabels produces a weighted finite automata, and the main ingredients for this algorithm one is this spectral learning and the other is mattix completion.",
                    "label": 0
                },
                {
                    "sent": "So what I will do in the rest of the talk is basically do three things.",
                    "label": 1
                },
                {
                    "sent": "The first is I will recall a little bit about the spectral method we're using and see why it's useful in this setting.",
                    "label": 0
                },
                {
                    "sent": "Then I will describe a family of learning algorithms.",
                    "label": 1
                },
                {
                    "sent": "That we propose on you.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you'll see why matrix completion fits in there.",
                    "label": 0
                },
                {
                    "sent": "And the third thing is I'm going to give a generalization bound for one particular algorithm in this family.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is going to be just a very brief description of what spectral learning is, and The thing is going to be.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you what the spectral method does.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to tell you how it does it, I don't want to get into all the formulas, but there's going to be a workshop on Friday on this kind of method.",
                    "label": 1
                },
                {
                    "sent": "So if you want to know more, you can stop by.",
                    "label": 0
                },
                {
                    "sent": "So what the spectral learning is about?",
                    "label": 0
                },
                {
                    "sent": "From my point of view, it can be summarized in this diagram.",
                    "label": 0
                },
                {
                    "sent": "That says that if you have your target model F, so from F you can define a matrix M. This is a matrix of observable quantities that are related to your model F. On the learning is comes from the fact that if you sample F, so if you get the sample from F. You can compute the matrix M hot, which hopefully will approximate this.",
                    "label": 0
                },
                {
                    "sent": "This or this M defined in terms of the model and then you apply the spectral learning method to this M hat, which is basically a set of linear algebra operations that the more important is the nasb decomposition.",
                    "label": 0
                },
                {
                    "sent": "Hence the spectral name and from this am hat you will produce a model F hat and nice things that the theory says that if M hot is close to M then the model you produce F hat.",
                    "label": 0
                },
                {
                    "sent": "Will be close to F. So this kind of methods have been.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I did recently too many models so there can be applied to sequential models where the data are strings like we do here.",
                    "label": 0
                },
                {
                    "sent": "You can also apply them to tree like structures and other kinds of graphical models, so there's been a lot of work recently on this stuff, starting with Hmm's on stochastic rational languages, and it turns out that when you take this method and see what happens with weighted automata, these metrics of observables, this M is a well known matrix, at least in automata.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out that it is a Hankel matrix.",
                    "label": 0
                },
                {
                    "sent": "So what is a Hankel matrix?",
                    "label": 0
                },
                {
                    "sent": "How cool matrix for a function F from strings to numbers is basically a matrix whose rows are indexed by strings, and we call this trinks prefixes and the columns are indexed again by strings.",
                    "label": 1
                },
                {
                    "sent": "But we call this string suffixes.",
                    "label": 0
                },
                {
                    "sent": "So on the entries in this matrix adjust evaluations of F. So in particularly we have a prefix U and suffix B which define and entering the matrix.",
                    "label": 0
                },
                {
                    "sent": "The number we have to put in there is the evaluation of F on the string you obtain by concatenating the prefix you with the suffix B.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I consider the finite alphabet with symbols AMB, I can define the function that given some string this F of X computes how many days does this string half.",
                    "label": 0
                },
                {
                    "sent": "And in this case this hunkele.",
                    "label": 0
                },
                {
                    "sent": "This matrix here is Hankel matrix for this function and you can see that, for example, if I take the first row which correspond to prefix A and I take the second column which correspond to suffix A.",
                    "label": 0
                },
                {
                    "sent": "That means that in here I have to find the evaluation of the function on the string obtained by concatenating A&A.",
                    "label": 0
                },
                {
                    "sent": "So just ring a which has two ways, so that's why you have an A in here and you can see immediately from the definition of Hankel matrix that this Hankel matrix is going to have some redundancies by definition, because if I have two prefixes and suffixes that give me the same string with strings, have paper can be partitioned in prefixes and suffixes in several ways, then I must have the same entry in the two corresponding places.",
                    "label": 0
                },
                {
                    "sent": "So that's what happens here with these two, which again, if I have the prefix AA and suffix empty string which is denoted by epsilon I.",
                    "label": 0
                },
                {
                    "sent": "This string also has 2A, so these two and these two represent the same evaluation of the function.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when you go and try to spectral learning with weighted finite automata, you will see is that this entry wise redundancies will play a role.",
                    "label": 0
                },
                {
                    "sent": "But there's also wrote a role to be played by another kind of redundancy in the Matrix, which is the rank.",
                    "label": 0
                },
                {
                    "sent": "So if you observe in this example I've just gave you the rank of this Hankel matrix is too because the 1st and 3rd columns are equal.",
                    "label": 0
                },
                {
                    "sent": "So these are redundancies, will also play a role and this is a nice theoretical fact that.",
                    "label": 0
                },
                {
                    "sent": "So suppose that you have a function from strings to numbers.",
                    "label": 0
                },
                {
                    "sent": "You can take many, many Hankel matrices for this function.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can take arbitrary infinitely many Hankel matrices, so if you keep taking larger and larger and larger matrices, either two things can happen.",
                    "label": 0
                },
                {
                    "sent": "The rank of this of these matrices will go to Infinity, or the rank of these matrices will stay bounded if the rank stays bounded, it will reach the top.",
                    "label": 0
                },
                {
                    "sent": "So these these number this this rank this find number is the rank of the function and it turns out that the rank of the function is the number of states that you need in a weighted finite automata to compute such a function.",
                    "label": 0
                },
                {
                    "sent": "So you will see that this will also play a role in our algorithm.",
                    "label": 0
                },
                {
                    "sent": "So now that we know that we need a Hankel matrix to apply the spectral method to weighted finite automata, where do we get the Hankel matrix from?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the problem is that in the typical application that we have, since in previous works.",
                    "label": 0
                },
                {
                    "sent": "The hung the function you want to learn.",
                    "label": 0
                },
                {
                    "sent": "The model is a probabilistic function, so for example it's F of X is the probability of strings, and in this case it turns out that the entries in your matrix are probabilities, which means that if you have a sample, you can compute an approximation of this matrix just by taking the empirical frequencies.",
                    "label": 1
                },
                {
                    "sent": "And of course if there's some string that doesn't appear in your sample, then you can put a zero there, because that's the relative.",
                    "label": 0
                },
                {
                    "sent": "That's the empirical frequency of that string.",
                    "label": 0
                },
                {
                    "sent": "But in our case, the entries in the Hankel matrix don't satisfy this constraint.",
                    "label": 1
                },
                {
                    "sent": "They're not relative and relative.",
                    "label": 1
                },
                {
                    "sent": "Frequencies, they're just labels we observe in the sample, so it turns out that if you have a sample you can fill in this Hankel matrix, but it you may have missing entries, so and that missing entries don't give you an approximation of Hankel matrix, you can fit into the spectral method.",
                    "label": 0
                },
                {
                    "sent": "So what we propose to do is to fill in this matrix.",
                    "label": 0
                },
                {
                    "sent": "This entry is using.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matrix completion so matrix completion has been studied recently in many contexts and machine learning as well an on the important thing here is that matrix completion can usually be be solved using convex optimization problem, But it turns out that these algorithms that has been proposed.",
                    "label": 0
                },
                {
                    "sent": "Don't take into account one particular thing of our problem, which is that the spectral method expects Hankel matrix.",
                    "label": 0
                },
                {
                    "sent": "That means that the matrix has to satisfy some entrywise constraints.",
                    "label": 0
                },
                {
                    "sent": "So Luckily for us, it turns out that you can also propose a convex optimization problem to find a Hankel matrix like you usually do in matrix completion, that is, minimizing some loss function that has a term that controls that your matrix agrees with what you have seen in the sample, and there's a regularizer term that controls the complexity of your matrix.",
                    "label": 0
                },
                {
                    "sent": "But Furthermore, there's a third ingredient in this optimization is.",
                    "label": 0
                },
                {
                    "sent": "That result must be Hankel matrix, but since this this constraint can be posed as just equalities between entries, this is a convex constraint, so you can still use convex optimization to solve the problem.",
                    "label": 1
                },
                {
                    "sent": "And it turns out that here in this particular application of matrix completion, regularising plays a double role.",
                    "label": 0
                },
                {
                    "sent": "The first is the usual role in Mathis completion.",
                    "label": 0
                },
                {
                    "sent": "That's to solve the yield posters of completing a matrix.",
                    "label": 0
                },
                {
                    "sent": "But the second is that regularising on on the Hankel matrix.",
                    "label": 1
                },
                {
                    "sent": "If you obtain less complex Hankel matrices, this will lead to simpler weighted finite automata, like for example.",
                    "label": 0
                },
                {
                    "sent": "As I said in the rank, if you try to minimize the nuclear norm, you hopefully will get a low rank matrix and that will give you a way to automata with fewer states, so that's that's regularising for the spectral method to come.",
                    "label": 0
                },
                {
                    "sent": "So now we have all the angry.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we can propose our family of algorithms.",
                    "label": 1
                },
                {
                    "sent": "So this is basically.",
                    "label": 0
                },
                {
                    "sent": "And you take your sample.",
                    "label": 0
                },
                {
                    "sent": "You feeling this?",
                    "label": 0
                },
                {
                    "sent": "This Hankel matrix that you have defined some rows and columns.",
                    "label": 0
                },
                {
                    "sent": "You will have some missing entries.",
                    "label": 0
                },
                {
                    "sent": "You apply constraint matrix completion you get a Hankel matrix that you can fit into a spectral learning and you get a weighted finite automata.",
                    "label": 1
                },
                {
                    "sent": "So this is this is in fact the whole family of algorithms because it can be parameterized by.",
                    "label": 0
                },
                {
                    "sent": "How do you choose the rows and columns?",
                    "label": 1
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "What kind of mattress completion algorithm you're going to use and what your regular sized regularising parameters are going to be so?",
                    "label": 0
                },
                {
                    "sent": "The question is, once you have this family is OK.",
                    "label": 0
                },
                {
                    "sent": "This seems to work, but can we give some bounds?",
                    "label": 0
                },
                {
                    "sent": "Can we prove something about this algorithm?",
                    "label": 0
                },
                {
                    "sent": "It's going to work.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out that for one particular instance of this family, you making three hypothesis, you can prove a generalization bound.",
                    "label": 0
                },
                {
                    "sent": "And the processes that you need some reasonable assumption on the distribution that generates these strings are real numbers.",
                    "label": 0
                },
                {
                    "sent": "Then you take the completion loss to be absolute loss.",
                    "label": 0
                },
                {
                    "sent": "And you take the regularizer to be from the Frobenius norm squared, and in this case it's interesting because this is.",
                    "label": 0
                },
                {
                    "sent": "This regularizer is differentiable.",
                    "label": 0
                },
                {
                    "sent": "So once you have that, you can prove that with high probability over the choice of a sample that has M examples.",
                    "label": 1
                },
                {
                    "sent": "The output of your algorithm, the generalization error of the output of your algorithm.",
                    "label": 0
                },
                {
                    "sent": "This quantity, here defining expected absolute loss.",
                    "label": 0
                },
                {
                    "sent": "Will be less or equal than the empirical error plus a term that goes to zero with a number of examples in this way.",
                    "label": 0
                },
                {
                    "sent": "And to prove that it turns out that you cannot use the typical bombing children, keys arguments or Rademacher complexity arguments, because we don't know how to compute these quantities for weighted finite automata.",
                    "label": 0
                },
                {
                    "sent": "So I think that this is a an interesting open problem, But what we do is we do a stability analysis.",
                    "label": 0
                },
                {
                    "sent": "So what we do is a joint stability analysis of the matrix completion and spectral learning all together to get this generalization bound.",
                    "label": 1
                },
                {
                    "sent": "And it turns out that this kind of stability analysis has been also used for other regression tasks so.",
                    "label": 0
                },
                {
                    "sent": "It's it's quite useful, inconvenient in these settings, so that's all.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I had to say today so if you want to know more.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can stop by the poster this evening, which is D 47.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}