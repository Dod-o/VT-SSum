{
    "id": "cohdfuowaav5sl2szpxi45oy5rymtmf3",
    "title": "Learning Time-Series Shapelets",
    "info": {
        "author": [
            "Josif Grabocka, University of Hildesheim"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_grabocka_shapelets/",
    "segmentation": [
        [
            "I'd like to present your work that we have conducted jointly with my colleagues at the University of Hildas.",
            "I'm in Germany, titled Learning Time series shape let's."
        ],
        [
            "The problem of this paper is to classify time series data, which is which is the data that has an globally assigned label for the time series and there is a recent trend in time series classification to extract discriminant if patterns from the time series so.",
            "Those discriminative patterns are called shape.",
            "It's in the literature of Time series and they are defined to be patterns with minimum distance to time series produced discriminated predictors.",
            "Therefore, the problem we're trying to solve is how to learn kamenny discriminate if shape, let's which we denote by S in our paper from time series data set denoted by T to."
        ],
        [
            "Provide a bit more intuition.",
            "The distance between a shape Letender time series, which we denoted by the letter M is defined to be the minimum distance between a shape that and every segment of a time series in a sliding window approach.",
            "The illustration below provides a little bit more intuition.",
            "We have on the left side two shape let's and in the middle plots for time series instances from two different classes and we show the alignment of the shape led to the most similar segments of the time series and the distance between the shape.",
            "Let's end the closest or the most similar segment can be thought as a sort of degree of membership of how strongly does this pattern belong to the time series and?",
            "At those distances, can project the time series data to a new representation or dimensionality that we call shape transform data.",
            "And such transformation has been shown to be both acurite in classifying the time series data on the new representation, and in addition, the extracted shape, let's provide interpretable features for domain experts on the differences between the classes as well as where do the discriminative patterns occur on the on the training instances."
        ],
        [
            "Up the related work tries to find the discriminant if shapeless, by considering candidates from that I'm serious instances.",
            "So basically every segment of the time series could be a candidate for being discriminate.",
            "If so, the baselines try to exhaustively enumerate all possible segments, measure the distances of time series two those candidates segments, and then those minimum distance features are compared.",
            "For their curacy on the target variable and in the end of the process, the top K performing shapeless are kept and Furthermore, on top of those derived features, a series of standard classifiers have been applied anschel to achieve high classification accuracy.",
            "Obviously, an exhaustive search is an expensive procedure.",
            "Therefore, speedups have been proposed."
        ],
        [
            "In contrast to the to the previous work, instead of searching shape leads by trying candidates from the training instance, we propose to use direct optimization method to learn the shapeless so that they can directly optimize the objective function of the classification.",
            "So in principle, would start with an initial guess for the shape.",
            "Let's and iteratively learn them to directly optimize the objective function.",
            "Our minimum distances of shapely to time series, which we said were predictors in the in the latent space can be used together with some linear classification weights W. For instance, to estimate the target variable that we denote by yhat, the objective.",
            "The aim of the problem is to minimize the specific classification loss between the estimated target Whitehead and the true target, Y.",
            "In our case, we use the logistic loss for the.",
            "For the statistical risk and overall, the final goal is how can we find the shapeless and the classification weights that minimize a regularize loss function?"
        ],
        [
            "And obviously one approaches to is to use numerical optimization.",
            "For instance, we use stochastic gradient descent.",
            "And therefore we decompose the objective function into per instance smaller objective functions and then compute the day.",
            "But if of the objective functions with respect to the shape, let's, however the minimum function is not differentiable, so you cannot learn directly.",
            "Shapeless, to optimize the original function or original objective.",
            "Therefore, we propose to use smooth approximation to the minimum function, which is known as the soft minimum, that is an approximation.",
            "Parametric approximation with the Parimeter Alpha, which resembles the true minimum for Alpha going to minus Infinity and semantically speaking the soft minimum is a sum of weighted Gaussian distances between shape let's and every segment of time series."
        ],
        [
            "That soft minimum function."
        ],
        [
            "As can be shown from the illustration provided at the topmost plot, we have a time series along time series and the shape it, and if we slide the shape read through all the segments of the of the time series.",
            "For instance with, we see that their kleidion distance approaches zero around time.",
            ".50 becausw there there is a pattern that is very close to the times to the shape, letting green and in the two.",
            "Plaza below we show how the soft minimum operates.",
            "Actually, the area under the curve is the minimum and we see that the spikes are the segments which contribute to the minimum, meaning that are closer to the minimum.",
            "And if we reduce the parimeter Alpha and make it even more smaller, we see that only the true minimum segment is allowed to contribute."
        ],
        [
            "And after we have all the pieces of the puzzle differentiable, we can derive an update rule for the optimization, namely the partial derivative of the per instance classification objective with respect to every point of a shape that shape.",
            "It can be learned using the chain rule of derivation, we can start with with the loss function who has the estimated target as a parameter.",
            "Then the estimated target has the minimum distances.",
            "As para meters and then the minimum distances have alot of per segment distances between shapeless and every segment an inside those distances shape that secures or through a chain rule of derivation we can compute the update rule for shapeless and do the same for classification weights and all the components of the day.",
            "Votives are are computable as shown below.",
            "Once the update rules are defined we can use the stochastic optimization to learn the optimal, shapeless and the classification weights."
        ],
        [
            "In a series of iterations of all the training set with.",
            "Learning rate ITA."
        ],
        [
            "To provide a bit more of an insight into how the method work, I can offer you the illustration shown and on the leftmost plot we see the initial status of of the learning algorithm with initial shape.",
            "Let values with the centroids of the segments as initial values for the shape.",
            "Let's and as can be seen in the upper plot, the minimum distances of time series to those two shapeless are shown on every axis.",
            "And as the algorithm progresses, we see that the shapeless on bottom are updated or changed, and they are changed in a way such that the minimum distances becomes linearly separable, which is an artifact of the logistic loss that we use and the rightmost plots show that after 800 iterations, updating the shapeless achieves a very good discrimination of the target variable.",
            "In this case, a binary problem.",
            "Over the training instances."
        ],
        [
            "Such an approach has at least two advantages over over previous work which extract candidates in an exhaustive manner from the training instances.",
            "The first is we can discover hidden or latent shape.",
            "Let's which, if we use the exhaustive search, we are limited to segments that explicitly appear in the data, and we cannot find latent or hidden ones, and the second advantages.",
            "If we check the accuracy of every candidate.",
            "One at a time.",
            "We cannot capture the interactions in terms of prediction quality by the resulting minimum distances, and that is a well known problem in data mining.",
            "Named variable subset selection.",
            "For instance, 2 features can be individually non discriminated with respect to the target.",
            "However, a simple 2D interaction of those two features can provide a perfect classification accuracy or none of the axis we have.",
            "Linear discrimination, however, just combining them we can achieve a perfect classification accuracy."
        ],
        [
            "End up in order to further validate the claims using empirical evidence is we run experimental comparisons against a set of baselines which extract shape.",
            "Let's and basically we can categorize the baseline says as those we extract shapeless, using several prediction quality criteria like information gain, Kruskal, Wallis, F, starts, and.",
            "Then they build decision trees on top of them.",
            "In addition, another set of baseline uses the derived predictors in the shape that transformed space to build standard classifiers on top.",
            "And also we have included speed up version of classifying time series using shape, it and dynamic time warping as being an historical paper method in the area.",
            "We use 28 datasets from diverse real life domains.",
            "From the collections of the University of California, Riverside and University of East Anglia, and we complied to the established benchmark of the provided training test splits and and Lastly, our hyperparameters are learned in a grid search approach by testing them over the training data in validation split."
        ],
        [
            "And the results compared to the 13 baselines.",
            "Provide an insight that learning shape let's directly to optimize objective function has significantly larger curacy than than guessing them by the segments of the time series and our method has both more wins more, one to one wins and significantly better rank than the baselines over those 28 datasets.",
            "In addition, the differences are statistically significant, according to a. Wilcoxon signed rank test, which is 2 tailed, uses A2 tailed hypothesis with a 5% significance level."
        ],
        [
            "And that naturally brings me to the to the final part of my talk.",
            "I'd like to remind you that I presented an optimization method to learn shapeless, so that they can directly optimize the classification objective, and such an approach of jointly learning shape let's is more accurate than exhaustively discovering shape.",
            "Let's from from.",
            "The training instances becausw they were not restricted to series segments and they can capture interactions among the derived minimum distance features and and Furthermore, results against a set of baselines and real life datasets validate the claim.",
            "I'd like to thank you and welcome questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd like to present your work that we have conducted jointly with my colleagues at the University of Hildas.",
                    "label": 0
                },
                {
                    "sent": "I'm in Germany, titled Learning Time series shape let's.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem of this paper is to classify time series data, which is which is the data that has an globally assigned label for the time series and there is a recent trend in time series classification to extract discriminant if patterns from the time series so.",
                    "label": 0
                },
                {
                    "sent": "Those discriminative patterns are called shape.",
                    "label": 0
                },
                {
                    "sent": "It's in the literature of Time series and they are defined to be patterns with minimum distance to time series produced discriminated predictors.",
                    "label": 0
                },
                {
                    "sent": "Therefore, the problem we're trying to solve is how to learn kamenny discriminate if shape, let's which we denote by S in our paper from time series data set denoted by T to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Provide a bit more intuition.",
                    "label": 0
                },
                {
                    "sent": "The distance between a shape Letender time series, which we denoted by the letter M is defined to be the minimum distance between a shape that and every segment of a time series in a sliding window approach.",
                    "label": 0
                },
                {
                    "sent": "The illustration below provides a little bit more intuition.",
                    "label": 0
                },
                {
                    "sent": "We have on the left side two shape let's and in the middle plots for time series instances from two different classes and we show the alignment of the shape led to the most similar segments of the time series and the distance between the shape.",
                    "label": 0
                },
                {
                    "sent": "Let's end the closest or the most similar segment can be thought as a sort of degree of membership of how strongly does this pattern belong to the time series and?",
                    "label": 0
                },
                {
                    "sent": "At those distances, can project the time series data to a new representation or dimensionality that we call shape transform data.",
                    "label": 0
                },
                {
                    "sent": "And such transformation has been shown to be both acurite in classifying the time series data on the new representation, and in addition, the extracted shape, let's provide interpretable features for domain experts on the differences between the classes as well as where do the discriminative patterns occur on the on the training instances.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Up the related work tries to find the discriminant if shapeless, by considering candidates from that I'm serious instances.",
                    "label": 0
                },
                {
                    "sent": "So basically every segment of the time series could be a candidate for being discriminate.",
                    "label": 0
                },
                {
                    "sent": "If so, the baselines try to exhaustively enumerate all possible segments, measure the distances of time series two those candidates segments, and then those minimum distance features are compared.",
                    "label": 1
                },
                {
                    "sent": "For their curacy on the target variable and in the end of the process, the top K performing shapeless are kept and Furthermore, on top of those derived features, a series of standard classifiers have been applied anschel to achieve high classification accuracy.",
                    "label": 1
                },
                {
                    "sent": "Obviously, an exhaustive search is an expensive procedure.",
                    "label": 0
                },
                {
                    "sent": "Therefore, speedups have been proposed.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In contrast to the to the previous work, instead of searching shape leads by trying candidates from the training instance, we propose to use direct optimization method to learn the shapeless so that they can directly optimize the objective function of the classification.",
                    "label": 0
                },
                {
                    "sent": "So in principle, would start with an initial guess for the shape.",
                    "label": 0
                },
                {
                    "sent": "Let's and iteratively learn them to directly optimize the objective function.",
                    "label": 0
                },
                {
                    "sent": "Our minimum distances of shapely to time series, which we said were predictors in the in the latent space can be used together with some linear classification weights W. For instance, to estimate the target variable that we denote by yhat, the objective.",
                    "label": 1
                },
                {
                    "sent": "The aim of the problem is to minimize the specific classification loss between the estimated target Whitehead and the true target, Y.",
                    "label": 0
                },
                {
                    "sent": "In our case, we use the logistic loss for the.",
                    "label": 0
                },
                {
                    "sent": "For the statistical risk and overall, the final goal is how can we find the shapeless and the classification weights that minimize a regularize loss function?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And obviously one approaches to is to use numerical optimization.",
                    "label": 0
                },
                {
                    "sent": "For instance, we use stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And therefore we decompose the objective function into per instance smaller objective functions and then compute the day.",
                    "label": 1
                },
                {
                    "sent": "But if of the objective functions with respect to the shape, let's, however the minimum function is not differentiable, so you cannot learn directly.",
                    "label": 0
                },
                {
                    "sent": "Shapeless, to optimize the original function or original objective.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we propose to use smooth approximation to the minimum function, which is known as the soft minimum, that is an approximation.",
                    "label": 0
                },
                {
                    "sent": "Parametric approximation with the Parimeter Alpha, which resembles the true minimum for Alpha going to minus Infinity and semantically speaking the soft minimum is a sum of weighted Gaussian distances between shape let's and every segment of time series.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That soft minimum function.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As can be shown from the illustration provided at the topmost plot, we have a time series along time series and the shape it, and if we slide the shape read through all the segments of the of the time series.",
                    "label": 1
                },
                {
                    "sent": "For instance with, we see that their kleidion distance approaches zero around time.",
                    "label": 0
                },
                {
                    "sent": ".50 becausw there there is a pattern that is very close to the times to the shape, letting green and in the two.",
                    "label": 1
                },
                {
                    "sent": "Plaza below we show how the soft minimum operates.",
                    "label": 0
                },
                {
                    "sent": "Actually, the area under the curve is the minimum and we see that the spikes are the segments which contribute to the minimum, meaning that are closer to the minimum.",
                    "label": 1
                },
                {
                    "sent": "And if we reduce the parimeter Alpha and make it even more smaller, we see that only the true minimum segment is allowed to contribute.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And after we have all the pieces of the puzzle differentiable, we can derive an update rule for the optimization, namely the partial derivative of the per instance classification objective with respect to every point of a shape that shape.",
                    "label": 1
                },
                {
                    "sent": "It can be learned using the chain rule of derivation, we can start with with the loss function who has the estimated target as a parameter.",
                    "label": 0
                },
                {
                    "sent": "Then the estimated target has the minimum distances.",
                    "label": 1
                },
                {
                    "sent": "As para meters and then the minimum distances have alot of per segment distances between shapeless and every segment an inside those distances shape that secures or through a chain rule of derivation we can compute the update rule for shapeless and do the same for classification weights and all the components of the day.",
                    "label": 0
                },
                {
                    "sent": "Votives are are computable as shown below.",
                    "label": 0
                },
                {
                    "sent": "Once the update rules are defined we can use the stochastic optimization to learn the optimal, shapeless and the classification weights.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a series of iterations of all the training set with.",
                    "label": 0
                },
                {
                    "sent": "Learning rate ITA.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To provide a bit more of an insight into how the method work, I can offer you the illustration shown and on the leftmost plot we see the initial status of of the learning algorithm with initial shape.",
                    "label": 0
                },
                {
                    "sent": "Let values with the centroids of the segments as initial values for the shape.",
                    "label": 0
                },
                {
                    "sent": "Let's and as can be seen in the upper plot, the minimum distances of time series to those two shapeless are shown on every axis.",
                    "label": 0
                },
                {
                    "sent": "And as the algorithm progresses, we see that the shapeless on bottom are updated or changed, and they are changed in a way such that the minimum distances becomes linearly separable, which is an artifact of the logistic loss that we use and the rightmost plots show that after 800 iterations, updating the shapeless achieves a very good discrimination of the target variable.",
                    "label": 0
                },
                {
                    "sent": "In this case, a binary problem.",
                    "label": 0
                },
                {
                    "sent": "Over the training instances.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such an approach has at least two advantages over over previous work which extract candidates in an exhaustive manner from the training instances.",
                    "label": 0
                },
                {
                    "sent": "The first is we can discover hidden or latent shape.",
                    "label": 0
                },
                {
                    "sent": "Let's which, if we use the exhaustive search, we are limited to segments that explicitly appear in the data, and we cannot find latent or hidden ones, and the second advantages.",
                    "label": 0
                },
                {
                    "sent": "If we check the accuracy of every candidate.",
                    "label": 0
                },
                {
                    "sent": "One at a time.",
                    "label": 0
                },
                {
                    "sent": "We cannot capture the interactions in terms of prediction quality by the resulting minimum distances, and that is a well known problem in data mining.",
                    "label": 0
                },
                {
                    "sent": "Named variable subset selection.",
                    "label": 0
                },
                {
                    "sent": "For instance, 2 features can be individually non discriminated with respect to the target.",
                    "label": 0
                },
                {
                    "sent": "However, a simple 2D interaction of those two features can provide a perfect classification accuracy or none of the axis we have.",
                    "label": 0
                },
                {
                    "sent": "Linear discrimination, however, just combining them we can achieve a perfect classification accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "End up in order to further validate the claims using empirical evidence is we run experimental comparisons against a set of baselines which extract shape.",
                    "label": 0
                },
                {
                    "sent": "Let's and basically we can categorize the baseline says as those we extract shapeless, using several prediction quality criteria like information gain, Kruskal, Wallis, F, starts, and.",
                    "label": 0
                },
                {
                    "sent": "Then they build decision trees on top of them.",
                    "label": 0
                },
                {
                    "sent": "In addition, another set of baseline uses the derived predictors in the shape that transformed space to build standard classifiers on top.",
                    "label": 0
                },
                {
                    "sent": "And also we have included speed up version of classifying time series using shape, it and dynamic time warping as being an historical paper method in the area.",
                    "label": 0
                },
                {
                    "sent": "We use 28 datasets from diverse real life domains.",
                    "label": 0
                },
                {
                    "sent": "From the collections of the University of California, Riverside and University of East Anglia, and we complied to the established benchmark of the provided training test splits and and Lastly, our hyperparameters are learned in a grid search approach by testing them over the training data in validation split.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the results compared to the 13 baselines.",
                    "label": 0
                },
                {
                    "sent": "Provide an insight that learning shape let's directly to optimize objective function has significantly larger curacy than than guessing them by the segments of the time series and our method has both more wins more, one to one wins and significantly better rank than the baselines over those 28 datasets.",
                    "label": 0
                },
                {
                    "sent": "In addition, the differences are statistically significant, according to a. Wilcoxon signed rank test, which is 2 tailed, uses A2 tailed hypothesis with a 5% significance level.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that naturally brings me to the to the final part of my talk.",
                    "label": 0
                },
                {
                    "sent": "I'd like to remind you that I presented an optimization method to learn shapeless, so that they can directly optimize the classification objective, and such an approach of jointly learning shape let's is more accurate than exhaustively discovering shape.",
                    "label": 1
                },
                {
                    "sent": "Let's from from.",
                    "label": 0
                },
                {
                    "sent": "The training instances becausw they were not restricted to series segments and they can capture interactions among the derived minimum distance features and and Furthermore, results against a set of baselines and real life datasets validate the claim.",
                    "label": 1
                },
                {
                    "sent": "I'd like to thank you and welcome questions.",
                    "label": 0
                }
            ]
        }
    }
}