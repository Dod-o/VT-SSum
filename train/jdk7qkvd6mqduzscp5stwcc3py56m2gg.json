{
    "id": "jdk7qkvd6mqduzscp5stwcc3py56m2gg",
    "title": "Some PAC-Bayesian Theorems",
    "info": {
        "author": [
            "David McAllester, Toyota Technological Institute at Chicago"
        ],
        "published": "April 14, 2010",
        "recorded": "March 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/pacbayesian_mcallester_spbt/",
    "segmentation": [
        [
            "It's wonderful to be able to attend a workshop dedicated to a phrase I invented.",
            "My talk is going to be sort of two parts.",
            "The first part is about a textbook I'm trying to write.",
            "I've been writing.",
            "I've been teaching introduction to machine learning and as part of teaching that course I've been trying to write a book and write.",
            "I think writing a book is like running a marathon.",
            "If you tell people you're going to do it, then you sort of have to do it.",
            "So I'm making a commitment.",
            "Now I'm telling people I'm writing this book, so if you think about the If PAC Bayes is going to survive.",
            "If that phrase is going to survive and stay current, it's going to have to be in the introductory textbooks.",
            "It would be really nice if it was there in here.",
            "So the question is how do we make this happen?",
            "If you think about the problem of at TTI Chicago, there's this sort of fight over the soul of the machine learning class.",
            "Nati Srebro is very interested in in teaching a lot of theory in intro machine learning, but if you look at the main competition, Chris Bishop's book on Machine Learning, there's almost no frequentist theory matter fact.",
            "There are a couple passages in there where he basically dismisses it.",
            "He says the bounds are useless.",
            "You know, we don't want to talk about it and it's all Bayesian.",
            "So the real challenge.",
            "OK, So what do people want in an introductory textbook for introductory class on machine learning?",
            "I've been talking to a few publishers what they want and what most people feel they have to teach, and I think it's an appropriate thing that you have to teach is sort of the recipe's right.",
            "You have to teach SVM.",
            "You have to teach kernel methods.",
            "You have to teach PCA.",
            "There's a long list of things that are sort of the standard recipes of machine learning and what people really want to learn, especially undergraduates, or, you know, first year graduate students or people working in other fields is how to do it, what software packages they can download, how to, how to pull the levers, make things happen.",
            "The challenge is to integrate a course that has that stuff.",
            "The recipes, and also the theory, and I think the big challenge in.",
            "Machine learning theory is is this whole challenge of integrating the existing recipes, the methods of machine learning with the theory."
        ],
        [
            "So I'm going to pick up where Francoists sort of.",
            "Sort of near the end of Francoise talk.",
            "This is a fundamental recipe of machine learning.",
            "And I have this discussion with naughty, you know what you're going to have to teach this.",
            "This is SVM, and it's just this is just you're looking for a linear classifier.",
            "You've got a feature vector here.",
            "You're looking for a weight vector that you can multiply by the feature vector to get a score.",
            "Your optimum weight vector is going to be up is solved by an optimization problem that minimizes the sum of a hinge loss Anna penalty that goes like the norm of the weight vector squared.",
            "OK, you're going to have to teach this.",
            "What is machine learning theory have to do with this?",
            "Well, the most fundamental thing that the that the students are going to do is they're going to play with Lambda, right?",
            "They're going to SPM light, they're going to download SVM light.",
            "SVM light is going is going to allow them to play with Lambda and they're gonna play with Lambda.",
            "So what's what's the tension between the practice in the theory here?",
            "The tension is that the theory, as Francois said, that at least the night theory doesn't have this right has a battens abound here?",
            "An Lambda is predicted in some sense.",
            "Lambda is dictated by the generalization bound.",
            "So the question is, can we understand the defaults?",
            "Let me go back here.",
            "If you download SVM light, there are defaults for this, and there are people in the machine learning Community Tom Mitchell's top Dietrich rather, who will say just use the defaults, they work.",
            "And it's a big deal because I've been doing a lot of computer vision and computer vision.",
            "It takes us about a week to train our object detection model.",
            "Overall, 20 classes in the Pascal BOC challenge, and if it takes you a week to test one of these lambdas.",
            "You can't.",
            "You sort of would be good to have guidance about what that Lambda is, and we don't.",
            "We're not at all confident that we have the right landing, because with the week of training time, we can't.",
            "Do much in the way of cross validation.",
            "So the idea is that the default.",
            "What are the defaults work?",
            "Well, what are the defaults?",
            "The defaults if we assume this feature vector has unit norm.",
            "The default is 1 basically.",
            "And what's weird is that if you look at.",
            "If you look so, I just want to point out that if you're a Bayesian, right?",
            "If you're Chris Bishop and you're writing a Bayesian book, this is."
        ],
        [
            "And the problem because you write some Bayesian formula.",
            "This is just the trivial Bayesian formula to maximize a conditional likelihood whenever you use Bayes, the relative weight of these two terms, you just maximizing the probability is just they are equally weighted right, so there's no Lambda.",
            "So the Bayesians say, well, the default works right?"
        ],
        [
            "What's weird is if you take the classical bounds for SVM, right, the classical square root bounds for SVM.",
            "They have this square root term and this is I keep fighting at TTI about whether the square root term is meaningful or not, and I keep saying this is worthless.",
            "It's not as bad as you might think if you if you take a bound like this, forget the big O.",
            "Suppose you've got some exact expression and you go to minimize the bound.",
            "Right, so you find the W minimizing this bound.",
            "There will be some.",
            "This bound is a function of the norm square, so there will be some regularizer Lambda which is expressing the sort of the locally linear dependence of this expression.",
            "On this norm there will be some expression Lambda that gives you the same minimizer.",
            "So you can always take this bound right since it's a function of this saying what's the tradeoff between the normal W squared in this term?",
            "The bound is at the optimum.",
            "There's going to be some tradeoff, which is going to give you Lambda.",
            "What is the Lambda that Disbound predicts?",
            "And let's compare it with the defaults in SVM light, right?",
            "Because the default spam lighter at least a decade of experience with many, many problems.",
            "And you can work it through and you get something like this that Lambda is order this thing.",
            "Now this thing is not as bad as it looks.",
            "Because if you take some kind of thermodynamic limit, you take an appropriate limit.",
            "Here you sort of expect the complexity of your model to be proportional to your sample size.",
            "So you expect this ratio to be essentially constant.",
            "But it gets the answer wrong or it gets the answer wrong is if you've got a problem in our vision problem.",
            "We've got datasets, we reduce it to a binary classification problem.",
            "Our datasets are mostly a few positive instances were looking for a car, say so there we had a few positive in some cars we've got a huge number of images that don't contain cars, so there's a huge label bias in the data.",
            "Most of the images are background there negative there, easily classified as negative.",
            "What happens in this kind of data?",
            "A detection problem where the positives are rare and most negatives are easily illuminated is that most is that the number of support vectors is vastly smaller than the size of your data.",
            "And if that's true, what happens in the default?"
        ],
        [
            "Is that?",
            "If you go back to this, this the penalty in the hinge loss is going like the number of support vectors.",
            "And what happens in a real problem is that the size of your weight vector is proportional to the number of support vectors, and that's what fails to be reproduced in the square root bound is.",
            "This is that Lambda should be one even in, even when the number of support vectors is a vanishing fraction of your training data, right?",
            "What matters is not end.",
            "But the number of support vectors?",
            "OK, we're talking about an introductory textbook, so we're talking about things that we want.",
            "We want to make the theory relevant to students without much work.",
            "We want to make this as accessible as possible, as direct as possible, so I'm going."
        ],
        [
            "Use my favorite version, actually Schwartz.",
            "Beat me over the head.",
            "He said you have to use a simpler bound.",
            "So I've decided this is the right way to do this.",
            "Is the right PAC Bayes bound?",
            "At least for my textbook, is going to focus on this version of the PAC Bayes bound, so the pacbase down says you consider a prior distribution on weight vectors.",
            "You consider a posterior Q.",
            "And the theorem says that the generalization error of the Gibbs sampler is less than or equal to some bound.",
            "Bounded queue, I'm gonna write this bound this way.",
            "You can you can derive this bound from the KL inverse bound that John Langford's been pushing for a decade probably.",
            "This bound says that the bound is written as the empirical risk of Q plus this thing where this C of Q is.",
            "The regularization part is the part that depends on the KL divergent's.",
            "Right, and it appears twice here.",
            "Once inside a square inside the square root term and once outside the square root term.",
            "Now, if you're a computer scientist, if this was a runtime bound, you would say, well, what's the order of runtime here?",
            "It's bound on risk.",
            "You would say this square root terms, not there.",
            "Because what's happening is this square root.",
            "This is a geometric mean.",
            "The geometric mean is bounded by the arithmetic mean.",
            "Right, So what you get is this expression.",
            "Here that the bound is bracketed by just the linear.",
            "Some of those two terms and three halves.",
            "Linear sum of those two terms.",
            "So I'm going to be very intuitive.",
            "There's another thing I've decided that for me personally now that I have tenure and don't have to worry about impressing people with fancy mathematics.",
            "I'm going to be intuitive and loosey Goosey sort of like a physicist.",
            "You know in the physics community, the people who try to be rigorous always end up losing the race.",
            "So.",
            "I'm going to, so I'm going to say look this.",
            "I mean, there's a precise approximation theorem here, which is if I optimize this thing because it's bracketed.",
            "Optimum of just this linear sum is not going to be worse than a factor of 2/3 off the optimum of the actual bound, so I'm just going to throw the bound away now and say look what I care about.",
            "Is this linear sum.",
            "Right, and that we immediately get this is immediately giving us that lambdas constant.",
            "Right, independent of an independent of the number of support vectors, right?",
            "Because now I'm optimizing just the sum of these two things and this is going to turn out to be the norm squared of the feature vectors.",
            "So now we get now we do.",
            "Via this analysis we get a match between the theoretical Lambda and the defaults of SVM light.",
            "OK, yeah.",
            "Right?",
            "Girls.",
            "And then you say that.",
            "I mean for me."
        ],
        [
            "But the non members, what kernel kind of the unstable?",
            "Making something really different from the RBF.",
            "So another way to say the same thing I think, is that the feature is all about the feature design.",
            "Phone number, but it's not right.",
            "So I mean when you download SVM light, it's going to let you specify the kernel too, but you're also going to specify the Lambda.",
            "Right, so you get some, you get to define your own.",
            "Yeah, I agree that the feature design.",
            "Perhaps swamps this nonetheless.",
            "In my work, I know that Lambda still does matter, and I'm pretty confident that we don't know empirically in our work whether we're using the right Lambda or not, because it takes 2 weeks to train all the classes.",
            "But I agree, the feature design.",
            "You have to.",
            "You have to worry about the feature design.",
            "I'm not going to talk about that."
        ],
        [
            "OK, the other thing that I want to get back to some things Francois said this is sort of a. Overlaps with his talk.",
            "The other thing about PAC Bayes is the whole idea is that it lets you think relate Bayesian thinking to frequentist thinking.",
            "So when we talk about L2 regularization, many students any any undergraduate will probably recognize that L2 regularization is intuitively like a Gaussian prior.",
            "So what the PAC Bayes framework lets you do is say yes, that's right, the L2 regularization is modeling a Gaussian prior, and here I'll prove it in a frequentist way, right?",
            "So this is the this is Langford, Shaw Taylor really sort of set this up.",
            "I wrote a paper that simplified the results, but the basic idea is we say we're being PAC Bayesian, so we can take a Gaussian prior.",
            "And we're being we're being a little sloppy so we can take a Gaussian posterior.",
            "And that just is to make the analysis simple.",
            "And then the main or the main insight is that then the empirical risk of this Gaussian posterior can be written as a probate loss.",
            "Right, so you just take the margin on.",
            "Here I've divided by the size of the feature vector so that this is a unit norm feature vector.",
            "You take the margin defined by unit norm, feature vector and the profit loss is a sigmoid loss.",
            "But the theory this this equation becomes exact.",
            "That's not a sloppy quality.",
            "This is a sloppy posterior, but that's an exact equality.",
            "So you get you get a theoretical justification for the probit loss.",
            "This nice, smooth sigmoid loss Princewill had a graph of it.",
            "And it's there's a definition here.",
            "The probate loss of Z is just the probability that a unit variant, unit variance, normal normal random variable exceeds Y.",
            "So it goes to zero as as this gets large compared to one.",
            "And this is also an exact equality.",
            "The KL Divergent has this really nice closed form.",
            "That's this.",
            "This exact equality motivated that sloppiness that the KL divergent becomes trivial, OK, and then you get.",
            "Then because I'm because I'm thinking of the PAC, Bayes bound is just being the sum of these two terms.",
            "Right now because I've been living in a simplified world, I'm just taking a linear sum of these two terms or this term plus twice that term.",
            "I'm going to ignore the log dependence on one over Delta.",
            "Then you get this.",
            "Formula.",
            "Um, throughout here just to get this formula throughout here I've said there's some variance, some bandwidth parameter here I can fix that to be one over the average feature vector here at one over the average norm of the feature vector here.",
            "And this then gives me this formula.",
            "OK, so this is almost again this is Francois stock.",
            "This is almost an SVM.",
            "Right, the difference between this and SVM?",
            "Is that is the probit loss versus the hinge loss.",
            "Right, and again, we've got a particular.",
            "This also corresponds to the default and SBM light.",
            "In that first version, I had said the feature vectors would normalize if the feature vectors are not normalized.",
            "Do not have unit norm.",
            "This is the default in SVM light, so we're doing, well, we've got our.",
            "We've got our regularization term behaving like the defaults, but we've got a sigmoid loss.",
            "Now I should say that sigmoid loss, being loosey Goosey, it doesn't really matter exactly what kind of signal you know.",
            "It's not going to matter in practice.",
            "What kind of sigmoid you put there right?",
            "Whether it's probate or is standard sigmoid Aurora.",
            "Piecewise linear sigmoid.",
            "It's not going to matter.",
            "The idea of using a non convex sigmoid.",
            "A loss here is very old.",
            "It's occurred to many, many people over the years to replace the hinge loss with a sigmoid loss, and there's a lot of experimental experience with it.",
            "None of it gets published because none of it gets a significant.",
            "Boost."
        ],
        [
            "OK.",
            "But these days, there's a huge amount of interest in L1 regularization.",
            "So what we really so now for writing a book through textbook on machine learning, introduction machine learning, you have to talk about L1 regularization and you probably have to talk about.",
            "You might have to talk about PQ norms.",
            "I'm not.",
            "I wouldn't.",
            "Lot of people would want PQ norms.",
            "Least you have to do L2L1 and L0 at the very least.",
            "Right, so here's so, let's do L1 so we can take.",
            "We can do L1 the same way.",
            "Pretty much the same way any students going to say.",
            "Oh, if IL one regularize as my regularization, that's going to correspond with Laplace prior.",
            "OK, so here's a question.",
            "Suppose I take a Laplace prior.",
            "We just get it with Gaussian.",
            "Let's just do it plus.",
            "Take a Laplace prior.",
            "Do the almost exact kind of same kind of analysis.",
            "What kind of we expect to get one regularization?",
            "What happens?",
            "So you can take the prior to be the Laplace prior, take the posterior tibial plus posterior center.",
            "Did some mean just like we did before?",
            "OK, this is this is loosey Goosey now.",
            "What is the error of Q?",
            "The error queue is when I draw from Q at random.",
            "What's the probability that I make a mistake?",
            "Right, that's defined to be this.",
            "The empirical risk of Q on the training data, right?",
            "So do the standard thing look at one training point.",
            "Imagine drawing from my posterior and saying what's the probability of error?",
            "Will drawing from my posterior is going to involve independent draws from the various components of my feature vector right this Laplace prior?",
            "This L1 bound is a sum over components.",
            "So when I draw from this posterior, I'm making an independent choice at each component of the feature and I'm adding scores from those independent choices together.",
            "So loosey goosey.",
            "The distribution is still going to be normal.",
            "The distribution of scores when I sample from Q is still going to be normal because it's an IID some and IID sums are rapidly normal.",
            "So the idea is if the posterior you can fix this, you can turn it into a precise bound, but you're going to throw away a lot of intuition.",
            "You can get a precise bound in terms of margin, but I think you're going to throw away a lot of intuition.",
            "The intuition is that when you draw from this, you're getting a Gaussian score, and if you're getting a Gaussian score, this is still a probate loss and the profit loss is determined by the variance of this Gaussian variable.",
            "The scale in here you're going to take.",
            "You want to take the random value and divide by the variance to pump it through the probit loss.",
            "So you can.",
            "You can analyze the variance and you can do that and then you can crank through, crank through the rest of the argument.",
            "You get this.",
            "OK, so again it's got a probit loss.",
            "It now has an L1 regularizer.",
            "Should have some scape or we just in there, right?",
            "It doesn't mean Paramita is 1, so it should really be.",
            "It should be a Gaussian scale para meters and some of them should be like like in tipping to work right when all the approximately.",
            "I don't see why the posterior should be allowed plus, which is kind of also not really distinguishing between the features.",
            "I mean the L1 is kind of switching off some features and other features survived, so yeah, this posterior have all the features are treated the same again.",
            "What's treated you so what's happening is if I pick a feature so I'm learning.",
            "I'm learning a feature vector, let me make sure I understand the question right.",
            "I'm learning a feature vector.",
            "The feature vector I'm learning is the mean mu.",
            "Here in this formula.",
            "Right, so I'm learning you is the thing I'm learning and in this equation it's mustar.",
            "I'm governing you star.",
            "This is the vector I'm learning.",
            "And the only issue in the posterior is if this is the vector I'm learning in order to make the KL divergent is finite, I have to have a little ball around that thing I'm learning.",
            "Right to make because of the appointment.",
            "So it's sitting so the prior 60 the posterior sits at mu and the only issue is what's the volume of the ball around mu and it just?",
            "I don't know.",
            "I guess I'd have to.",
            "We have to talk about whether we believe that's a significant issue.",
            "Right, right?",
            "Just like we did the same thing in the previous in the L2 case where we took up the posterior had the same variance, it was not.",
            "That's not optimal, but we know that better posteriors exist than this or that.",
            "But ultimately, what I want is.",
            "I mean ultimately what I'm after is this formula, right is to say, look what you're going to do at the end of the days in L1 regular is what they're going to do then, L1 regularize hinge loss?",
            "Can we understand what happens in an L1 regularizer, hinge loss and and now I'm asking for something very modest.",
            "I just want to understand the regularization parameter the Lambda.",
            "Um?"
        ],
        [
            "Now this analysis.",
            "Now we can also ask you at least two L2L 1L0.",
            "This is the L0 case.",
            "This analysis goes way back to boosting the margin 98.",
            "This and John Langford and Matthias.",
            "Sort of integrated PAC Bayes ideas, so this analysis is old predates the analysis.",
            "I don't know of a good reference for analysis, I just showed you.",
            "This predates all of this stuff.",
            "At least, this choice of prior, so this is the right way to think about it.",
            "Now we're going to take kind of L0 prior.",
            "We're going to take an IID sample of an independent draws from the features.",
            "And then we're going to take a uniformly weighted predictor, and that's our prior.",
            "From those, an independent draws.",
            "It's not really an L0 prior, but it's certainly a sparse prior.",
            "I'm calling it in L0 prior, so the prior is this parameter end of the prior, which is I draw in features.",
            "And then I wake them uniformly.",
            "And that's my prior with parameter N. My posterior is centered.",
            "My posterior involves a weight vector mu.",
            "And there's a trick here where you is an easy trick that allows you only to consider positive weights for every feature you can add in a feature that gives you the negative version of that feature, so I can think of mu as defining under an L1 regularization is defining a probability distribution.",
            "So I'm thinking the feature vector now is defining a probability distribution, the feature vector the mu gives me a distribution over the features and my posteriors to draw end times from the distribution mu.",
            "So this is not it's not as it doesn't fit quite as cleanly into the two previous priors and posteriors, but it does have a nice clean analysis.",
            "And again, because this is an N independent draws the posterior.",
            "Involves an independent, draws the score of the posterior is again going to be Gaussian.",
            "So again, you want it again you to really understand what you're doing, you should put a probate loss in there.",
            "OK, the bottom line is you get this formula.",
            "Now that what's interesting is, this formula also has an L1 regularization.",
            "So if you go way back to boosting the margin it was derived in L1 regularization.",
            "This formula has an L1 regularization.",
            "They're different, they are in fact incomparable."
        ],
        [
            "This is linear in the one norm."
        ],
        [
            "This is quadratic in the one norm.",
            "So linear in the one norm is better than quadratic in the one norm, because in order to get a good margin you're going to get a big feature vector."
        ],
        [
            "This involves the two norm of the feature vector."
        ],
        [
            "Oops.",
            "This involves the Infinity norm of the feature vector.",
            "So intuitively this is.",
            "This is a kind of dual norm bound.",
            "We used an L0 prior and we're getting an L Infinity property of the feature vector.",
            "So there are lots of these dual norm bounds where you say you've got one norm of the weight vector and the dual norm of the feature vector.",
            "So LL Infinity is better than L2, but it's a square and the bounds end up being incomparable, so there's a great.",
            "Theoretically there's great empirical question.",
            "I don't think there's nearly as much empirical experience with L1 regularization to say what the right defaults for an L1 support vector machine R. So I don't know, you know, if we had a decade of experience with with SVN packages.",
            "You know which?",
            "Which default value of Lambda would be better?",
            "Presumably we can work with.",
            "The minimum of the two."
        ],
        [
            "At the minimum of the two should always supply.",
            "I'm gonna switch topics a little bit.",
            "There's this sigmoidal loss and the idea is that all the generalization theory, at least the PAC Bayesian generalization theory, says you should be working with this non convex sigmoid loss.",
            "This elpro, but everybody in practice works with hinge loss.",
            "The hinge loss can be viewed as a convex relaxation of the profit loss.",
            "Now we also have at TTI Chicago.",
            "We have a large stock Fox theory.",
            "Group they do approximation algorithms in the stock Fox Community.",
            "The bread and butter of approximation algorithms.",
            "It takes some NP hard problem, some nonconvex problem.",
            "You know graph.",
            "Multi commodity flow problem or something.",
            "Or a facility location problem and you define a convex relaxation and typically defining some LP relaxation of a 01.",
            "Problem is trivial, right?",
            "It's easy.",
            "It's the first thing you can do.",
            "And then what's hard is proving that if I solve the LP relaxation, there's an approximation ratio between the thing I care about and the convex relaxation.",
            "So I think it's a horrible embarrassment to the machine learning community that SVM, the fundamental workhorse, at least a fundamental workhorse in machine learning is derived as a convex relaxation with no theorem.",
            "There's no approximate.",
            "I don't know of any approximation theorem that says that these are related by some approximation ratio.",
            "There are results that say.",
            "If you want to minimize the 01 loss of a weight vector, there is no approximation scheme.",
            "There's recent stock box papers that say if you want to minimize exactly 01 loss.",
            "It's there's no approximation scheme.",
            "There's no, there's no good approximation at all.",
            "But this but the margins involved get very get exponentially small.",
            "So here was the approximation.",
            "So after some closer W closing performance.",
            "Here I'm just interested in the difference in value between the minimizer of this and the minimizer of that.",
            "Objective so the objective function.",
            "The idea is that this is a surrogate for the generalization loss right?",
            "If we want to minimize the bound.",
            "Pose we're trying to minimize this.",
            "Then if we minimize this, do we get close in that value?",
            "The problem with the stock Fox communities.",
            "They're going to say, well, is that really an interesting problem to approximate, and I from a machine learning perspective, I think the machine learning community should care about this approximation ratio, and this is going to come up again."
        ],
        [
            "OK, so.",
            "I'm going to shift gears now and stop talking about textbook issues and start talking more about real research issues.",
            "So I'm these days I've been very into structured predictions or prediction problems for structured outputs.",
            "And just, you know, in terms of textbooks or talking to people who aren't necessarily in machine learning when it comes to structured outputs, I like to use machine translation as a paradigmatic example.",
            "So you know you're doing English to French.",
            "So imagine you know that X is an English sentence.",
            "And why is a French sentence?",
            "And the idea here is really.",
            "This is really almost a statement about computer science in general, not just about machine learning.",
            "Machine translation system is a software system, right?",
            "It takes an input.",
            "It has an output.",
            "How many software systems?",
            "Ultimately as we look into the future, are going to compute their output by optimizing?",
            "So the claim is optimization is absolutely ubiquitous in computer science.",
            "You know it's not just about machine learning.",
            "The theory, community studies optimization.",
            "Consider a software.",
            "Any software system that goes from an input to an output through an optimization.",
            "OK, and here we're making this optimization linear.",
            "We're introducing a feature vector that involves the input and output.",
            "We're taking a weight vector, but we all know as machine learning people that this linearity assumption is very weak, right?",
            "Almost any help with some kernel trick.",
            "Anything could be put in there, and Furthermore everything we're going to do could be done.",
            "If this was some nonlinear scoring function.",
            "OK, so we can just apply a PAC Bayes.",
            "So suppose we got it.",
            "OK, let me back up again.",
            "What the stock Fox community misses I think, and part of the reason that we're becoming more relevant to computer science generally is that in real software applications that compute by optimizing.",
            "A fundamental issue is what is the objective function.",
            "You can't sit down and write down an objective function for machine translation and expect it to work right, or genome annotation or speech recognition or computer vision scene understanding in computer vision.",
            "These software systems are going to work by optimization, but the objective function has to come from somewhere, and so machine learning is about getting the objective function that the software system optimizes when it runs.",
            "OK, so we're about learning W here.",
            "That's determining the objective function that the software system is going to use so we can take.",
            "So we're considering.",
            "So now I can apply a PAC Bayesian approach.",
            "I can say there's a prior, I'm just going to going to the simple thing.",
            "The trivial thing.",
            "The first thing is to take a Gaussian prior on W. Take the Gaussian posterior on W. And then write down and then do the same analysis that I did and I'm going to get this a similar expression, except that this thing.",
            "Is no longer a probit loss.",
            "I'm just leaving the PAC Bayes expression in here and not trying to analyze it yet, so this is just writing down the pack Bayesian analysis.",
            "RKL term comes out clean from this posterior from this prior in this posterior, but the empirical loss.",
            "I'm just writing down as the empirical risk of the posterior, and this is.",
            "This becomes a big problem in the structured case.",
            "Right?",
            "What is?",
            "How do we deal with this empirical list of the empirical risk of the posterior?",
            "It's not a clean, it's not a clean probate loss of the margin."
        ],
        [
            "OK, so I don't know if you're how many people are into structured output learning, but there's a standard by now.",
            "A very widely cited Standard method, a generalization of SVM's to the structured case.",
            "So there's something called a structured, structured SVM based on something called a structured hinge loss.",
            "This is a generalization of hinge loss to the structured case, and it's often motivated.",
            "Not always, but often motivated by taking it to be a convex relaxation of the loss itself, right?",
            "So one, so if we think about hinge loss, we go back to hinge loss.",
            "Hinge loss is the step function.",
            "There are people who motivate hinge loss by saying, well look, the hinge loss is an upper bound on the step loss.",
            "Forget about regularization.",
            "Just look at the 01 loss.",
            "The hinge loss upper bounds.",
            "It's a convex relaxation, so we should optimize.",
            "We can optimize that convex relaxation of the hinge loss.",
            "I can see that people are tired and I don't want to go through.",
            "I don't want to push too many technical formulas, but one of the problems with the structured case.",
            "As you have many possible outputs, not just one or minus one, and for each possible output.",
            "There's also a target output in labeled training data.",
            "Some reference translation, say, and for each possible output each candidate translation there's a different margin is a different difference in score between the thing you're trying to produce the reference translation, and the candidate.",
            "So your margin is a function of the margin being the difference in score between what you want the reference translation and what you're producing depends on.",
            "Which output you're doing.",
            "Um?",
            "I guess I don't want to go through this too carefully, but this margin if this is the best scoring value you're taking the best scoring value to be your output.",
            "This margins always negative, right?",
            "Or at least not.",
            "It can't be positive because the best scoring value.",
            "If this is the best scoring value, it has to be at least as large as the reference translation.",
            "So the margin of the best scoring value is never positive.",
            "So if I subtract something that's never positive, I get an upper bound.",
            "So the score itself.",
            "The loss itself is upper bounded by the loss minus the margin.",
            "OK then if it's upper bounded by the loss minus the margin, then I can take a Max over the candidate outputs and it only gets bigger.",
            "Right there, existing output that's up that's bigger than this.",
            "If I take a Max over the outputs, then it's still bigger.",
            "And this, then, is the structured hinge loss, the Max over possible outputs of this difference, and this margin is linear in W, so this is a Max of things linear in W, so it's convex.",
            "A Max of linear functions is convex, so this gives you a convex piecewise linear upper bound on the loss itself, ignoring ignoring generalization.",
            "And this is the way the structured hinge loss is usually motivated.",
            "Is just a convex relaxation.",
            "Um?",
            "The.",
            "Mice in mice.",
            "When I yeah, so you know this is this is a French sentence.",
            "Oh, I'm sorry.",
            "This last line is just saying that the structured hinge loss generalizes the binary hinge loss.",
            "If Y is in minus one one, then this Max Max over Y hat reduces to a Max over 0 and 1 minus.",
            "So the Max this Max turns into the Max of zero and one minus the margin.",
            "So this is the way I originally understood the structured hinge loss, being a generalization of the 01 hinge loss.",
            "But Ben Tasker beat me up and there's a much more insightful relationship between the structured hinge loss and the 01 hinge loss.",
            "Which is this?",
            "Let me just say it in words.",
            "I don't think she's like the slide.",
            "Take a take a binary classification problem.",
            "So I've got training pairs.",
            "XYY is either one or minus one group your binary data into groups.",
            "Sync."
        ],
        [
            "Of each group as a structured problem?",
            "Right?",
            "Because now it's got a bit string input.",
            "All the X is and it's got a bit string.",
            "Output all the wise.",
            "Think of it as a structured problem an apply the structured SVM optimization problem to it, right?",
            "So take a binary problem group it.",
            "Think of it as a collection of structured problems.",
            "Apply the structured hinge loss.",
            "The structured hinge loss on the group binary problem gets back to the ordinary binary hinge loss on the original independent data right?",
            "And that's a much.",
            "Stronger and better intuition for the relationship between the structured hinge loss and the binary hinge loss, I'm just going to skip over the."
        ],
        [
            "Now the motivation of just going to a relaxation.",
            "Is in the jochims Hoffman, Alton paper, Ben Tasker paper on this subject with Daphne, Kohler and Kastron proved a generalization bound.",
            "So there were thinking about generalization and it turns out that if you consider the realizable case.",
            "Meaning that at train time you're going to get all your things separated by a margin.",
            "All your data points are going to be correctly classified an separated by a margin.",
            "In the structured case, which is asking a lot.",
            "So this would be crazy for machine translation.",
            "Quite that you're going to get every translation word for word identical to the reference translation and separated by a margin from every other translation.",
            "It's not going to happen right, but suppose it did happen.",
            "Then you can prove a generalization bound.",
            "And what's interesting about that generalization bound is that the margin requirement is different for different labels.",
            "Now, in an ordinary SVM that one you take 1 minus the margin, that one is the margin requirement.",
            "As soon as your margin reaches one, you don't have to worry about the data point, it's not contributing to your loss anymore.",
            "In the structured case, you have many forgiven input.",
            "You have many different outputs each of your different outputs can have a different margin requirement, right?",
            "The margin you ask for for that are.",
            "Output can be different and if you look at.",
            "Um?",
            "Sorry."
        ],
        [
            "If you look at the structured bound.",
            "You can interpret this L as a margin requirement.",
            "Right, if every I.",
            "If every for every label the margin is at least the loss.",
            "Then your loss is 0.",
            "Right, so if every label meets its margin requirement, your losses 0 where this is the margin requirement, and jochims and Hoffner all called this a margin, rescaled structural SVM.",
            "They also have a slack, rescaled structural SVM that I'm not talking bout, which is a different convex relaxation.",
            "Now what Tasker proved was that from a generalization theory point of view you can take the margin requirement to be Hamming distance.",
            "And so motivating a Hamming distance.",
            "Here you can motivate a Hamming distance here from generalization theory.",
            "So they have a generalization bound that actually gets much tighter than than.",
            "Like standard multiclass bounds, by using a Hamming loss margin requirement there.",
            "But I think this is confusing because it confuses."
        ],
        [
            "The loss function with the margin requirement.",
            "There's a confusion in the literature.",
            "I think the field is still confused about in structural SVM's.",
            "What's the margin requirement and what's the loss function and how is it related to generalization?",
            "And how's the structured hinge loss motivated all of these things are very confused.",
            "I think in the community right now in an ordinary SVM you would say look at the fraction of the data that fails to meet its margin requirement.",
            "Your loss is going to be proportional to the fraction of data that fails to meet its margin requirement plus a quadratic penalty.",
            "This is the generalization of that to the structured case, except we're using a different margin requirement at every different label, and the margin requirement is a Hamming loss.",
            "However, for something that doesn't satisfy its margin requirement, we plug the value we plug in is not one, but the loss of that thing, and the important point about this bound is that it separates the margin requirement from the loss function."
        ],
        [
            "How am I doing on time?",
            "OK, I'll try not to put everybody to sleep.",
            "I'm going to shift gears a little bit again.",
            "So this now I'm going to start talking about very concrete machine learning recipes.",
            "And what's happening is something that's actually happening in the machine translation community.",
            "So.",
            "When you go to learn a structured label, there are various perceptron like learning rules.",
            "So we've got some training data.",
            "We're going to our training data, one training point at a time.",
            "For every training point, we're going to update our weight vector.",
            "So you can think of this as a gradient descent method or a subgradient descent method.",
            "But it but it goes back to Perceptron.",
            "Which you can also think of as a subgradient descent method.",
            "So the classical multiclass perceptron, this was championed in a paper by Michael Collins.",
            "Is you're looking at a data point XY pair.",
            "This is a training point, OK, an input and output input sentence.",
            "Output reference translation.",
            "I'm going to assume I have a feature vector on that pair.",
            "This is the reference translation.",
            "This is the label my system actually produces.",
            "The best scoring label.",
            "I should have written it as YW, Avex.",
            "You say this is what you want.",
            "This is what you've got.",
            "Move your weight vector away from what you've gotten Tord, what you want.",
            "And this is a generalization of the ordinary Perceptron update rule to the multiclass case, and you can prove a multi perceptron convergence to multiclass perceptron convergence theorem for this rule.",
            "Direct generalization of the standard binary one.",
            "OK, take the here's another update rule.",
            "Take the structured SVM and do a subgradient algorithm for it.",
            "So what is a subgradient algorithm means it means I look at a point.",
            "I look at the hinge loss.",
            "I'm trying to optimize.",
            "I compute a subgradient.",
            "Then I move in that direction.",
            "Now the hinge loss has a Max in it.",
            "Let me go back.",
            "Whoops forward."
        ],
        [
            "This is the thing I'm going to try to minimize.",
            "This is the generalization of Hingeless.",
            "When I do a subgradient, what I'm going to do is find this optimum and then take the gradient of this with respect to my weight vector, right?",
            "That's the subgradient that gives me a subgradient of this objective function is to find this Max.",
            "Now if I do that.",
            "I get this update.",
            "I take it just works out that I take the reference translation, which is what I want, minus the optimum of the hingeless.",
            "Minus the feature vector of the bottom of the hinge loss.",
            "Now the feature vector of the outcome of the hinge loss.",
            "So here's here's the ordinary.",
            "Best scoring value.",
            "This is the best scoring value where instead of just maximizing the score, I'm also maximizing.",
            "A component of my objective is the loss function itself, so I'm finding something that's both scoring well and has high loss.",
            "So when I do subgradient of the hinge loss, this is called a loss adjusted inference.",
            "So the idea of loss adjusted inference is.",
            "Central is really central to this structured SVM.",
            "So I do.",
            "I add in a lot.",
            "So what's the intuition here?",
            "The intuition is I've got the reference translation, which is what I want, and I've got something that scores well, but that's really bad.",
            "Not only does it score well, but it's bad and the idea is that among the things that score well, you know there's a lot of uncertainty.",
            "If I was doing PAC Bayesian sampling, I wouldn't be.",
            "I'd be sampling many different outputs.",
            "I care about the output that that's scores well and is bad.",
            "That's the one I want to get away from.",
            "That's the intuition.",
            "OK, but there's this problem.",
            "The structured hinge loss itself is flaky.",
            "It's derived as a convex relaxation with no theorem associated with it.",
            "It confuses the."
        ],
        [
            "Margin requirement.",
            "With the loss function.",
            "So it's kind of flaky, so this is a great opportunity for Pacbase.",
            "The theory is a mess.",
            "So let's let's go for it.",
            "So the first thing so so now this is research.",
            "The first thing I did was I said well, I can't thinking about this.",
            "Regularization is just too hard.",
            "It's just too hard.",
            "I'm going to throw away the regularization first, so suppose we were doing on line.",
            "You never batch.",
            "Let's do this online.",
            "Let's sample and do an update with every fresh sample.",
            "And if you do this online, you can just try and let's not try to regularize.",
            "Let's try to minimize our loss.",
            "So here's an intuitive idea for a perceptron like update.",
            "And when I started working on this the this intuitive idea came to me long before there's any theorem associated with it.",
            "But we're going to theorem.",
            "Intuitively, if I want to change my weight vector in a way that makes my translations better.",
            "I don't really care so much about the reference translation.",
            "What I care about is the neighborhood of the translations that my system is producing there going to be many translations that all score reasonably well.",
            "Some of them are closer to my reference translation than others.",
            "I would like to move my weight vector to shift my translator to be producing better scoring translations and the way I'm going to do that is by looking at, not the reference translation, but different translations that my translator produces.",
            "So now what I'm going to do is I'm going to compute the label that I would output.",
            "This is just the argmax I'm going to compute a loss adjusted.",
            "Inference.",
            "For for pragmatic reasons, instead of adding the loss, I'm going to subtract the loss.",
            "So this is what the system produces a translation.",
            "This is a better translation that scores well.",
            "It's better translation 'cause it has lower loss, but it also scores well.",
            "And what I'm going to do is I'm going to move away from the current translation and toward the better translation.",
            "OK, so I take my so the difference between this and the structural SVM subgradient is that these are both in Ferd labels.",
            "Neither one of them is the reference translation.",
            "The reference translation is used in this adjusted inference, because if its adjusted to be in the direction of the reference translation, so the reference translation is used in the loss adjusted inference.",
            "But neither of these are the reference translation.",
            "These are both inferred labels.",
            "OK, so it turns out so I had this intuition that this was a good idea.",
            "And then we can.",
            "We did some theory.",
            "I did some theory.",
            "Which I'll show you in a moment, but then I went to nips just a few months ago, and Michael Collins is, well, you know, the machine learning trend, the machine translation community has moved over to this already.",
            "Any which you would really like to do the theory and then get people to follow you, but it's.",
            "Good, at least when you're you know.",
            "You discover that they're doing it already.",
            "So not exactly this equation, but what they do is they produce N best lists of the translation so they get a neighborhood of translations they might produce, and then they do this kind of update by picking the.",
            "Maybe the best and the worst of their end.",
            "Best list as defined by the reference translation so they look at their end best list.",
            "See which ones near the reference translation and move their weight vector toward that.",
            "Um?",
            "And it's it's.",
            "I think it's sort of taken over, and a lot of people you know feel that it's a cheat.",
            "Somehow these machines, these trends, machine translation people are just hacks, right?",
            "Because this is a really ugly thing to do, there's no theory."
        ],
        [
            "OK, here's the theory.",
            "So here's a theorem.",
            "This is what we care about.",
            "This is the generalization loss, and let's suppose we just want to do gradient descent on it.",
            "So we want the gradient and W of this.",
            "It's a theorem that this gradient is equal to the limit as epsilon goes to zero of this feature difference.",
            "So what this says is in the limit as epsilon goes to 0, sorry.",
            "In the limit, is this epsilon in the loss adjustment goes to zero.",
            "The expectation of this update becomes the gradient direction of the loss I care about."
        ],
        [
            "And this paper is under review at ICML.",
            "And the difficulty with it with the paper and with this talk at this moment is that this proof is impenetrable.",
            "Um?",
            "The intuition let me just give you, let me just say some intuitive things.",
            "I'm going, I thought I would try to take you through."
        ],
        [
            "A bit of this proof, but let me say some intuitive things at least first.",
            "This loss.",
            "This label as defined by W is discrete object, right?",
            "It's like a sentence.",
            "So if I look at the at the translation being currently produced by the system and try to differentiate this loss by the blue score, which is what these people use with respect to W, it doesn't have a derivative.",
            "Right, because we've got a system that's producing discrete outputs and there's going to be some range of W that all produce the same translation.",
            "So you can't just differentiate the loss in here with respect to W, it's not differentiable.",
            "How is it possible that this gradient then even exists?",
            "So.",
            "What's happening in this theorem is?",
            "I'm assuming that the input is from a continuous space.",
            "The output can be from a discrete space, but this theorem, this analysis assumes the input is from a continuous space.",
            "Now that continuous space has, the translator is going to divide that continuous space into cells, where each cell is the range of continuous values that produce a given output.",
            "OK, so you can even think of this in the binary case.",
            "Suppose the labels were binary.",
            "This works perfectly well in the binary case.",
            "Supposed labels were binary.",
            "There's a region of input that gets labeled minus one.",
            "There's a region of input that gets labeled one.",
            "There's a boundary between those two regions.",
            "As I change the parameter vector W, the decision boundary moves.",
            "The decision boundary moves in a way that's continuous in W. And assuming you've got some smooth density over the over these, assuming this row is some smooth density over the input output pairs, the fact that the decision boundary is changing continuously in W means that this gradient is going to exist.",
            "Like there's going to be some gradient and W of the error rate.",
            "But that gradient only occurs at the decision boundary.",
            "Um?",
            "And if you think about this, update this update.",
            "If these two labels are the same, this update is 0.",
            "So what's happening is if epsilon is small, the loss adjustment is small.",
            "Then the two labels are very likely to be the same, specially in the binary case.",
            "Think about the binary case.",
            "If the loss adjustment is small in most of the X region, you're going to produce the same label in the update is 0.",
            "So this update is only going to happen if I pick a point near the decision boundary.",
            "So this expectation as epsilon goes to 0 is all about what happens right at the decision boundary, 'cause the gradient is all about what happens right at the decision boundary.",
            "And it turns out that if you analyze.",
            "If you analyze these two expressions.",
            "Both of them by by sort of working out for a finite epsilon probability that so for this one, for example, I say, well consider a change in W, right?",
            "The grading is all about what happens when I change you consider changing W. Under what conditions does the loss change?",
            "And that's all about picking a change that's all about looking at X is which are near the decision boundary.",
            "Anyway, it works out that if you analyze these two from that perspective, you get the same expression.",
            "And this is the analysis."
        ],
        [
            "And what happens is, in both analysis you get an indicator function which is integrating over the decision boundary.",
            "This is this can be viewed as an integral over a decision boundary.",
            "The decision boundary is where the wait times the Delta feature vector between two labels is 0.",
            "The two labels are tide.",
            "I think they have the same score.",
            "This is saying 2 labels are tide.",
            "To say that this is 0.",
            "But instead of saying it's zero, I'm going to say it's in some little interval.",
            "And when you integrate over the decision boundary, there's two things.",
            "There's the width of the boundary layer, and there's the density along the boundary.",
            "And in one analysis you get a width times the width times a Delta loss, and in the other analysis you get a width times changing feature vector.",
            "In both cases you take the width out and you get the same expression.",
            "So the problem with the paper that's been submitted by CML is the reviewers have say that proof is impenetrable.",
            "Sorry.",
            "Future work."
        ],
        [
            "We have a continuous version."
        ],
        [
            "People like so the part of the problem is there are discrete labels, so you get a lot of combinatorics.",
            "We have a version that says suppose everything insight is continuous, all the labels are continuous.",
            "That proof is not.",
            "It's also not so easy.",
            "Not so easy to read, but at least everything is continuous in it and it did validates the other theorem.",
            "Oh, I'm going."
        ],
        [
            "Skip this.",
            "Skip this experiment."
        ],
        [
            "Oh"
        ],
        [
            "I'm asking this, I think."
        ],
        [
            "Done right, I'm out of time.",
            "OK, so there's a summary.",
            "I'll stop there, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's wonderful to be able to attend a workshop dedicated to a phrase I invented.",
                    "label": 0
                },
                {
                    "sent": "My talk is going to be sort of two parts.",
                    "label": 0
                },
                {
                    "sent": "The first part is about a textbook I'm trying to write.",
                    "label": 0
                },
                {
                    "sent": "I've been writing.",
                    "label": 0
                },
                {
                    "sent": "I've been teaching introduction to machine learning and as part of teaching that course I've been trying to write a book and write.",
                    "label": 0
                },
                {
                    "sent": "I think writing a book is like running a marathon.",
                    "label": 0
                },
                {
                    "sent": "If you tell people you're going to do it, then you sort of have to do it.",
                    "label": 0
                },
                {
                    "sent": "So I'm making a commitment.",
                    "label": 0
                },
                {
                    "sent": "Now I'm telling people I'm writing this book, so if you think about the If PAC Bayes is going to survive.",
                    "label": 0
                },
                {
                    "sent": "If that phrase is going to survive and stay current, it's going to have to be in the introductory textbooks.",
                    "label": 0
                },
                {
                    "sent": "It would be really nice if it was there in here.",
                    "label": 0
                },
                {
                    "sent": "So the question is how do we make this happen?",
                    "label": 0
                },
                {
                    "sent": "If you think about the problem of at TTI Chicago, there's this sort of fight over the soul of the machine learning class.",
                    "label": 0
                },
                {
                    "sent": "Nati Srebro is very interested in in teaching a lot of theory in intro machine learning, but if you look at the main competition, Chris Bishop's book on Machine Learning, there's almost no frequentist theory matter fact.",
                    "label": 0
                },
                {
                    "sent": "There are a couple passages in there where he basically dismisses it.",
                    "label": 0
                },
                {
                    "sent": "He says the bounds are useless.",
                    "label": 0
                },
                {
                    "sent": "You know, we don't want to talk about it and it's all Bayesian.",
                    "label": 0
                },
                {
                    "sent": "So the real challenge.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do people want in an introductory textbook for introductory class on machine learning?",
                    "label": 0
                },
                {
                    "sent": "I've been talking to a few publishers what they want and what most people feel they have to teach, and I think it's an appropriate thing that you have to teach is sort of the recipe's right.",
                    "label": 0
                },
                {
                    "sent": "You have to teach SVM.",
                    "label": 0
                },
                {
                    "sent": "You have to teach kernel methods.",
                    "label": 0
                },
                {
                    "sent": "You have to teach PCA.",
                    "label": 0
                },
                {
                    "sent": "There's a long list of things that are sort of the standard recipes of machine learning and what people really want to learn, especially undergraduates, or, you know, first year graduate students or people working in other fields is how to do it, what software packages they can download, how to, how to pull the levers, make things happen.",
                    "label": 0
                },
                {
                    "sent": "The challenge is to integrate a course that has that stuff.",
                    "label": 0
                },
                {
                    "sent": "The recipes, and also the theory, and I think the big challenge in.",
                    "label": 0
                },
                {
                    "sent": "Machine learning theory is is this whole challenge of integrating the existing recipes, the methods of machine learning with the theory.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to pick up where Francoists sort of.",
                    "label": 0
                },
                {
                    "sent": "Sort of near the end of Francoise talk.",
                    "label": 0
                },
                {
                    "sent": "This is a fundamental recipe of machine learning.",
                    "label": 0
                },
                {
                    "sent": "And I have this discussion with naughty, you know what you're going to have to teach this.",
                    "label": 0
                },
                {
                    "sent": "This is SVM, and it's just this is just you're looking for a linear classifier.",
                    "label": 0
                },
                {
                    "sent": "You've got a feature vector here.",
                    "label": 0
                },
                {
                    "sent": "You're looking for a weight vector that you can multiply by the feature vector to get a score.",
                    "label": 0
                },
                {
                    "sent": "Your optimum weight vector is going to be up is solved by an optimization problem that minimizes the sum of a hinge loss Anna penalty that goes like the norm of the weight vector squared.",
                    "label": 0
                },
                {
                    "sent": "OK, you're going to have to teach this.",
                    "label": 0
                },
                {
                    "sent": "What is machine learning theory have to do with this?",
                    "label": 0
                },
                {
                    "sent": "Well, the most fundamental thing that the that the students are going to do is they're going to play with Lambda, right?",
                    "label": 0
                },
                {
                    "sent": "They're going to SPM light, they're going to download SVM light.",
                    "label": 0
                },
                {
                    "sent": "SVM light is going is going to allow them to play with Lambda and they're gonna play with Lambda.",
                    "label": 0
                },
                {
                    "sent": "So what's what's the tension between the practice in the theory here?",
                    "label": 0
                },
                {
                    "sent": "The tension is that the theory, as Francois said, that at least the night theory doesn't have this right has a battens abound here?",
                    "label": 0
                },
                {
                    "sent": "An Lambda is predicted in some sense.",
                    "label": 0
                },
                {
                    "sent": "Lambda is dictated by the generalization bound.",
                    "label": 0
                },
                {
                    "sent": "So the question is, can we understand the defaults?",
                    "label": 0
                },
                {
                    "sent": "Let me go back here.",
                    "label": 0
                },
                {
                    "sent": "If you download SVM light, there are defaults for this, and there are people in the machine learning Community Tom Mitchell's top Dietrich rather, who will say just use the defaults, they work.",
                    "label": 0
                },
                {
                    "sent": "And it's a big deal because I've been doing a lot of computer vision and computer vision.",
                    "label": 0
                },
                {
                    "sent": "It takes us about a week to train our object detection model.",
                    "label": 0
                },
                {
                    "sent": "Overall, 20 classes in the Pascal BOC challenge, and if it takes you a week to test one of these lambdas.",
                    "label": 0
                },
                {
                    "sent": "You can't.",
                    "label": 0
                },
                {
                    "sent": "You sort of would be good to have guidance about what that Lambda is, and we don't.",
                    "label": 0
                },
                {
                    "sent": "We're not at all confident that we have the right landing, because with the week of training time, we can't.",
                    "label": 0
                },
                {
                    "sent": "Do much in the way of cross validation.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that the default.",
                    "label": 0
                },
                {
                    "sent": "What are the defaults work?",
                    "label": 0
                },
                {
                    "sent": "Well, what are the defaults?",
                    "label": 0
                },
                {
                    "sent": "The defaults if we assume this feature vector has unit norm.",
                    "label": 0
                },
                {
                    "sent": "The default is 1 basically.",
                    "label": 1
                },
                {
                    "sent": "And what's weird is that if you look at.",
                    "label": 0
                },
                {
                    "sent": "If you look so, I just want to point out that if you're a Bayesian, right?",
                    "label": 0
                },
                {
                    "sent": "If you're Chris Bishop and you're writing a Bayesian book, this is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the problem because you write some Bayesian formula.",
                    "label": 0
                },
                {
                    "sent": "This is just the trivial Bayesian formula to maximize a conditional likelihood whenever you use Bayes, the relative weight of these two terms, you just maximizing the probability is just they are equally weighted right, so there's no Lambda.",
                    "label": 0
                },
                {
                    "sent": "So the Bayesians say, well, the default works right?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's weird is if you take the classical bounds for SVM, right, the classical square root bounds for SVM.",
                    "label": 1
                },
                {
                    "sent": "They have this square root term and this is I keep fighting at TTI about whether the square root term is meaningful or not, and I keep saying this is worthless.",
                    "label": 0
                },
                {
                    "sent": "It's not as bad as you might think if you if you take a bound like this, forget the big O.",
                    "label": 0
                },
                {
                    "sent": "Suppose you've got some exact expression and you go to minimize the bound.",
                    "label": 0
                },
                {
                    "sent": "Right, so you find the W minimizing this bound.",
                    "label": 0
                },
                {
                    "sent": "There will be some.",
                    "label": 0
                },
                {
                    "sent": "This bound is a function of the norm square, so there will be some regularizer Lambda which is expressing the sort of the locally linear dependence of this expression.",
                    "label": 0
                },
                {
                    "sent": "On this norm there will be some expression Lambda that gives you the same minimizer.",
                    "label": 0
                },
                {
                    "sent": "So you can always take this bound right since it's a function of this saying what's the tradeoff between the normal W squared in this term?",
                    "label": 0
                },
                {
                    "sent": "The bound is at the optimum.",
                    "label": 0
                },
                {
                    "sent": "There's going to be some tradeoff, which is going to give you Lambda.",
                    "label": 0
                },
                {
                    "sent": "What is the Lambda that Disbound predicts?",
                    "label": 0
                },
                {
                    "sent": "And let's compare it with the defaults in SVM light, right?",
                    "label": 0
                },
                {
                    "sent": "Because the default spam lighter at least a decade of experience with many, many problems.",
                    "label": 0
                },
                {
                    "sent": "And you can work it through and you get something like this that Lambda is order this thing.",
                    "label": 0
                },
                {
                    "sent": "Now this thing is not as bad as it looks.",
                    "label": 0
                },
                {
                    "sent": "Because if you take some kind of thermodynamic limit, you take an appropriate limit.",
                    "label": 0
                },
                {
                    "sent": "Here you sort of expect the complexity of your model to be proportional to your sample size.",
                    "label": 0
                },
                {
                    "sent": "So you expect this ratio to be essentially constant.",
                    "label": 0
                },
                {
                    "sent": "But it gets the answer wrong or it gets the answer wrong is if you've got a problem in our vision problem.",
                    "label": 0
                },
                {
                    "sent": "We've got datasets, we reduce it to a binary classification problem.",
                    "label": 0
                },
                {
                    "sent": "Our datasets are mostly a few positive instances were looking for a car, say so there we had a few positive in some cars we've got a huge number of images that don't contain cars, so there's a huge label bias in the data.",
                    "label": 0
                },
                {
                    "sent": "Most of the images are background there negative there, easily classified as negative.",
                    "label": 0
                },
                {
                    "sent": "What happens in this kind of data?",
                    "label": 0
                },
                {
                    "sent": "A detection problem where the positives are rare and most negatives are easily illuminated is that most is that the number of support vectors is vastly smaller than the size of your data.",
                    "label": 0
                },
                {
                    "sent": "And if that's true, what happens in the default?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "If you go back to this, this the penalty in the hinge loss is going like the number of support vectors.",
                    "label": 0
                },
                {
                    "sent": "And what happens in a real problem is that the size of your weight vector is proportional to the number of support vectors, and that's what fails to be reproduced in the square root bound is.",
                    "label": 0
                },
                {
                    "sent": "This is that Lambda should be one even in, even when the number of support vectors is a vanishing fraction of your training data, right?",
                    "label": 0
                },
                {
                    "sent": "What matters is not end.",
                    "label": 0
                },
                {
                    "sent": "But the number of support vectors?",
                    "label": 1
                },
                {
                    "sent": "OK, we're talking about an introductory textbook, so we're talking about things that we want.",
                    "label": 0
                },
                {
                    "sent": "We want to make the theory relevant to students without much work.",
                    "label": 0
                },
                {
                    "sent": "We want to make this as accessible as possible, as direct as possible, so I'm going.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use my favorite version, actually Schwartz.",
                    "label": 0
                },
                {
                    "sent": "Beat me over the head.",
                    "label": 0
                },
                {
                    "sent": "He said you have to use a simpler bound.",
                    "label": 0
                },
                {
                    "sent": "So I've decided this is the right way to do this.",
                    "label": 0
                },
                {
                    "sent": "Is the right PAC Bayes bound?",
                    "label": 0
                },
                {
                    "sent": "At least for my textbook, is going to focus on this version of the PAC Bayes bound, so the pacbase down says you consider a prior distribution on weight vectors.",
                    "label": 0
                },
                {
                    "sent": "You consider a posterior Q.",
                    "label": 0
                },
                {
                    "sent": "And the theorem says that the generalization error of the Gibbs sampler is less than or equal to some bound.",
                    "label": 0
                },
                {
                    "sent": "Bounded queue, I'm gonna write this bound this way.",
                    "label": 0
                },
                {
                    "sent": "You can you can derive this bound from the KL inverse bound that John Langford's been pushing for a decade probably.",
                    "label": 0
                },
                {
                    "sent": "This bound says that the bound is written as the empirical risk of Q plus this thing where this C of Q is.",
                    "label": 0
                },
                {
                    "sent": "The regularization part is the part that depends on the KL divergent's.",
                    "label": 0
                },
                {
                    "sent": "Right, and it appears twice here.",
                    "label": 0
                },
                {
                    "sent": "Once inside a square inside the square root term and once outside the square root term.",
                    "label": 0
                },
                {
                    "sent": "Now, if you're a computer scientist, if this was a runtime bound, you would say, well, what's the order of runtime here?",
                    "label": 0
                },
                {
                    "sent": "It's bound on risk.",
                    "label": 0
                },
                {
                    "sent": "You would say this square root terms, not there.",
                    "label": 0
                },
                {
                    "sent": "Because what's happening is this square root.",
                    "label": 0
                },
                {
                    "sent": "This is a geometric mean.",
                    "label": 0
                },
                {
                    "sent": "The geometric mean is bounded by the arithmetic mean.",
                    "label": 0
                },
                {
                    "sent": "Right, So what you get is this expression.",
                    "label": 0
                },
                {
                    "sent": "Here that the bound is bracketed by just the linear.",
                    "label": 0
                },
                {
                    "sent": "Some of those two terms and three halves.",
                    "label": 0
                },
                {
                    "sent": "Linear sum of those two terms.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to be very intuitive.",
                    "label": 0
                },
                {
                    "sent": "There's another thing I've decided that for me personally now that I have tenure and don't have to worry about impressing people with fancy mathematics.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be intuitive and loosey Goosey sort of like a physicist.",
                    "label": 0
                },
                {
                    "sent": "You know in the physics community, the people who try to be rigorous always end up losing the race.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm going to, so I'm going to say look this.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's a precise approximation theorem here, which is if I optimize this thing because it's bracketed.",
                    "label": 0
                },
                {
                    "sent": "Optimum of just this linear sum is not going to be worse than a factor of 2/3 off the optimum of the actual bound, so I'm just going to throw the bound away now and say look what I care about.",
                    "label": 0
                },
                {
                    "sent": "Is this linear sum.",
                    "label": 0
                },
                {
                    "sent": "Right, and that we immediately get this is immediately giving us that lambdas constant.",
                    "label": 0
                },
                {
                    "sent": "Right, independent of an independent of the number of support vectors, right?",
                    "label": 0
                },
                {
                    "sent": "Because now I'm optimizing just the sum of these two things and this is going to turn out to be the norm squared of the feature vectors.",
                    "label": 0
                },
                {
                    "sent": "So now we get now we do.",
                    "label": 0
                },
                {
                    "sent": "Via this analysis we get a match between the theoretical Lambda and the defaults of SVM light.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Girls.",
                    "label": 0
                },
                {
                    "sent": "And then you say that.",
                    "label": 0
                },
                {
                    "sent": "I mean for me.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the non members, what kernel kind of the unstable?",
                    "label": 0
                },
                {
                    "sent": "Making something really different from the RBF.",
                    "label": 0
                },
                {
                    "sent": "So another way to say the same thing I think, is that the feature is all about the feature design.",
                    "label": 0
                },
                {
                    "sent": "Phone number, but it's not right.",
                    "label": 0
                },
                {
                    "sent": "So I mean when you download SVM light, it's going to let you specify the kernel too, but you're also going to specify the Lambda.",
                    "label": 0
                },
                {
                    "sent": "Right, so you get some, you get to define your own.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I agree that the feature design.",
                    "label": 0
                },
                {
                    "sent": "Perhaps swamps this nonetheless.",
                    "label": 0
                },
                {
                    "sent": "In my work, I know that Lambda still does matter, and I'm pretty confident that we don't know empirically in our work whether we're using the right Lambda or not, because it takes 2 weeks to train all the classes.",
                    "label": 0
                },
                {
                    "sent": "But I agree, the feature design.",
                    "label": 0
                },
                {
                    "sent": "You have to.",
                    "label": 0
                },
                {
                    "sent": "You have to worry about the feature design.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the other thing that I want to get back to some things Francois said this is sort of a. Overlaps with his talk.",
                    "label": 0
                },
                {
                    "sent": "The other thing about PAC Bayes is the whole idea is that it lets you think relate Bayesian thinking to frequentist thinking.",
                    "label": 0
                },
                {
                    "sent": "So when we talk about L2 regularization, many students any any undergraduate will probably recognize that L2 regularization is intuitively like a Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "So what the PAC Bayes framework lets you do is say yes, that's right, the L2 regularization is modeling a Gaussian prior, and here I'll prove it in a frequentist way, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the this is Langford, Shaw Taylor really sort of set this up.",
                    "label": 0
                },
                {
                    "sent": "I wrote a paper that simplified the results, but the basic idea is we say we're being PAC Bayesian, so we can take a Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "And we're being we're being a little sloppy so we can take a Gaussian posterior.",
                    "label": 0
                },
                {
                    "sent": "And that just is to make the analysis simple.",
                    "label": 0
                },
                {
                    "sent": "And then the main or the main insight is that then the empirical risk of this Gaussian posterior can be written as a probate loss.",
                    "label": 0
                },
                {
                    "sent": "Right, so you just take the margin on.",
                    "label": 0
                },
                {
                    "sent": "Here I've divided by the size of the feature vector so that this is a unit norm feature vector.",
                    "label": 0
                },
                {
                    "sent": "You take the margin defined by unit norm, feature vector and the profit loss is a sigmoid loss.",
                    "label": 0
                },
                {
                    "sent": "But the theory this this equation becomes exact.",
                    "label": 0
                },
                {
                    "sent": "That's not a sloppy quality.",
                    "label": 0
                },
                {
                    "sent": "This is a sloppy posterior, but that's an exact equality.",
                    "label": 0
                },
                {
                    "sent": "So you get you get a theoretical justification for the probit loss.",
                    "label": 0
                },
                {
                    "sent": "This nice, smooth sigmoid loss Princewill had a graph of it.",
                    "label": 0
                },
                {
                    "sent": "And it's there's a definition here.",
                    "label": 0
                },
                {
                    "sent": "The probate loss of Z is just the probability that a unit variant, unit variance, normal normal random variable exceeds Y.",
                    "label": 0
                },
                {
                    "sent": "So it goes to zero as as this gets large compared to one.",
                    "label": 0
                },
                {
                    "sent": "And this is also an exact equality.",
                    "label": 0
                },
                {
                    "sent": "The KL Divergent has this really nice closed form.",
                    "label": 0
                },
                {
                    "sent": "That's this.",
                    "label": 0
                },
                {
                    "sent": "This exact equality motivated that sloppiness that the KL divergent becomes trivial, OK, and then you get.",
                    "label": 0
                },
                {
                    "sent": "Then because I'm because I'm thinking of the PAC, Bayes bound is just being the sum of these two terms.",
                    "label": 0
                },
                {
                    "sent": "Right now because I've been living in a simplified world, I'm just taking a linear sum of these two terms or this term plus twice that term.",
                    "label": 0
                },
                {
                    "sent": "I'm going to ignore the log dependence on one over Delta.",
                    "label": 0
                },
                {
                    "sent": "Then you get this.",
                    "label": 0
                },
                {
                    "sent": "Formula.",
                    "label": 0
                },
                {
                    "sent": "Um, throughout here just to get this formula throughout here I've said there's some variance, some bandwidth parameter here I can fix that to be one over the average feature vector here at one over the average norm of the feature vector here.",
                    "label": 0
                },
                {
                    "sent": "And this then gives me this formula.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is almost again this is Francois stock.",
                    "label": 0
                },
                {
                    "sent": "This is almost an SVM.",
                    "label": 0
                },
                {
                    "sent": "Right, the difference between this and SVM?",
                    "label": 0
                },
                {
                    "sent": "Is that is the probit loss versus the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Right, and again, we've got a particular.",
                    "label": 0
                },
                {
                    "sent": "This also corresponds to the default and SBM light.",
                    "label": 0
                },
                {
                    "sent": "In that first version, I had said the feature vectors would normalize if the feature vectors are not normalized.",
                    "label": 0
                },
                {
                    "sent": "Do not have unit norm.",
                    "label": 0
                },
                {
                    "sent": "This is the default in SVM light, so we're doing, well, we've got our.",
                    "label": 0
                },
                {
                    "sent": "We've got our regularization term behaving like the defaults, but we've got a sigmoid loss.",
                    "label": 0
                },
                {
                    "sent": "Now I should say that sigmoid loss, being loosey Goosey, it doesn't really matter exactly what kind of signal you know.",
                    "label": 0
                },
                {
                    "sent": "It's not going to matter in practice.",
                    "label": 0
                },
                {
                    "sent": "What kind of sigmoid you put there right?",
                    "label": 0
                },
                {
                    "sent": "Whether it's probate or is standard sigmoid Aurora.",
                    "label": 0
                },
                {
                    "sent": "Piecewise linear sigmoid.",
                    "label": 0
                },
                {
                    "sent": "It's not going to matter.",
                    "label": 0
                },
                {
                    "sent": "The idea of using a non convex sigmoid.",
                    "label": 0
                },
                {
                    "sent": "A loss here is very old.",
                    "label": 0
                },
                {
                    "sent": "It's occurred to many, many people over the years to replace the hinge loss with a sigmoid loss, and there's a lot of experimental experience with it.",
                    "label": 0
                },
                {
                    "sent": "None of it gets published because none of it gets a significant.",
                    "label": 0
                },
                {
                    "sent": "Boost.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But these days, there's a huge amount of interest in L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "So what we really so now for writing a book through textbook on machine learning, introduction machine learning, you have to talk about L1 regularization and you probably have to talk about.",
                    "label": 0
                },
                {
                    "sent": "You might have to talk about PQ norms.",
                    "label": 0
                },
                {
                    "sent": "I'm not.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't.",
                    "label": 0
                },
                {
                    "sent": "Lot of people would want PQ norms.",
                    "label": 0
                },
                {
                    "sent": "Least you have to do L2L1 and L0 at the very least.",
                    "label": 0
                },
                {
                    "sent": "Right, so here's so, let's do L1 so we can take.",
                    "label": 0
                },
                {
                    "sent": "We can do L1 the same way.",
                    "label": 0
                },
                {
                    "sent": "Pretty much the same way any students going to say.",
                    "label": 0
                },
                {
                    "sent": "Oh, if IL one regularize as my regularization, that's going to correspond with Laplace prior.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's a question.",
                    "label": 0
                },
                {
                    "sent": "Suppose I take a Laplace prior.",
                    "label": 0
                },
                {
                    "sent": "We just get it with Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Let's just do it plus.",
                    "label": 0
                },
                {
                    "sent": "Take a Laplace prior.",
                    "label": 0
                },
                {
                    "sent": "Do the almost exact kind of same kind of analysis.",
                    "label": 0
                },
                {
                    "sent": "What kind of we expect to get one regularization?",
                    "label": 0
                },
                {
                    "sent": "What happens?",
                    "label": 0
                },
                {
                    "sent": "So you can take the prior to be the Laplace prior, take the posterior tibial plus posterior center.",
                    "label": 0
                },
                {
                    "sent": "Did some mean just like we did before?",
                    "label": 0
                },
                {
                    "sent": "OK, this is this is loosey Goosey now.",
                    "label": 0
                },
                {
                    "sent": "What is the error of Q?",
                    "label": 0
                },
                {
                    "sent": "The error queue is when I draw from Q at random.",
                    "label": 0
                },
                {
                    "sent": "What's the probability that I make a mistake?",
                    "label": 0
                },
                {
                    "sent": "Right, that's defined to be this.",
                    "label": 0
                },
                {
                    "sent": "The empirical risk of Q on the training data, right?",
                    "label": 0
                },
                {
                    "sent": "So do the standard thing look at one training point.",
                    "label": 0
                },
                {
                    "sent": "Imagine drawing from my posterior and saying what's the probability of error?",
                    "label": 0
                },
                {
                    "sent": "Will drawing from my posterior is going to involve independent draws from the various components of my feature vector right this Laplace prior?",
                    "label": 0
                },
                {
                    "sent": "This L1 bound is a sum over components.",
                    "label": 0
                },
                {
                    "sent": "So when I draw from this posterior, I'm making an independent choice at each component of the feature and I'm adding scores from those independent choices together.",
                    "label": 0
                },
                {
                    "sent": "So loosey goosey.",
                    "label": 0
                },
                {
                    "sent": "The distribution is still going to be normal.",
                    "label": 0
                },
                {
                    "sent": "The distribution of scores when I sample from Q is still going to be normal because it's an IID some and IID sums are rapidly normal.",
                    "label": 0
                },
                {
                    "sent": "So the idea is if the posterior you can fix this, you can turn it into a precise bound, but you're going to throw away a lot of intuition.",
                    "label": 0
                },
                {
                    "sent": "You can get a precise bound in terms of margin, but I think you're going to throw away a lot of intuition.",
                    "label": 0
                },
                {
                    "sent": "The intuition is that when you draw from this, you're getting a Gaussian score, and if you're getting a Gaussian score, this is still a probate loss and the profit loss is determined by the variance of this Gaussian variable.",
                    "label": 0
                },
                {
                    "sent": "The scale in here you're going to take.",
                    "label": 0
                },
                {
                    "sent": "You want to take the random value and divide by the variance to pump it through the probit loss.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "You can analyze the variance and you can do that and then you can crank through, crank through the rest of the argument.",
                    "label": 0
                },
                {
                    "sent": "You get this.",
                    "label": 0
                },
                {
                    "sent": "OK, so again it's got a probit loss.",
                    "label": 0
                },
                {
                    "sent": "It now has an L1 regularizer.",
                    "label": 0
                },
                {
                    "sent": "Should have some scape or we just in there, right?",
                    "label": 0
                },
                {
                    "sent": "It doesn't mean Paramita is 1, so it should really be.",
                    "label": 0
                },
                {
                    "sent": "It should be a Gaussian scale para meters and some of them should be like like in tipping to work right when all the approximately.",
                    "label": 0
                },
                {
                    "sent": "I don't see why the posterior should be allowed plus, which is kind of also not really distinguishing between the features.",
                    "label": 0
                },
                {
                    "sent": "I mean the L1 is kind of switching off some features and other features survived, so yeah, this posterior have all the features are treated the same again.",
                    "label": 0
                },
                {
                    "sent": "What's treated you so what's happening is if I pick a feature so I'm learning.",
                    "label": 0
                },
                {
                    "sent": "I'm learning a feature vector, let me make sure I understand the question right.",
                    "label": 0
                },
                {
                    "sent": "I'm learning a feature vector.",
                    "label": 0
                },
                {
                    "sent": "The feature vector I'm learning is the mean mu.",
                    "label": 0
                },
                {
                    "sent": "Here in this formula.",
                    "label": 0
                },
                {
                    "sent": "Right, so I'm learning you is the thing I'm learning and in this equation it's mustar.",
                    "label": 0
                },
                {
                    "sent": "I'm governing you star.",
                    "label": 0
                },
                {
                    "sent": "This is the vector I'm learning.",
                    "label": 0
                },
                {
                    "sent": "And the only issue in the posterior is if this is the vector I'm learning in order to make the KL divergent is finite, I have to have a little ball around that thing I'm learning.",
                    "label": 0
                },
                {
                    "sent": "Right to make because of the appointment.",
                    "label": 0
                },
                {
                    "sent": "So it's sitting so the prior 60 the posterior sits at mu and the only issue is what's the volume of the ball around mu and it just?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I guess I'd have to.",
                    "label": 0
                },
                {
                    "sent": "We have to talk about whether we believe that's a significant issue.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "Just like we did the same thing in the previous in the L2 case where we took up the posterior had the same variance, it was not.",
                    "label": 0
                },
                {
                    "sent": "That's not optimal, but we know that better posteriors exist than this or that.",
                    "label": 0
                },
                {
                    "sent": "But ultimately, what I want is.",
                    "label": 0
                },
                {
                    "sent": "I mean ultimately what I'm after is this formula, right is to say, look what you're going to do at the end of the days in L1 regular is what they're going to do then, L1 regularize hinge loss?",
                    "label": 0
                },
                {
                    "sent": "Can we understand what happens in an L1 regularizer, hinge loss and and now I'm asking for something very modest.",
                    "label": 0
                },
                {
                    "sent": "I just want to understand the regularization parameter the Lambda.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this analysis.",
                    "label": 0
                },
                {
                    "sent": "Now we can also ask you at least two L2L 1L0.",
                    "label": 0
                },
                {
                    "sent": "This is the L0 case.",
                    "label": 0
                },
                {
                    "sent": "This analysis goes way back to boosting the margin 98.",
                    "label": 0
                },
                {
                    "sent": "This and John Langford and Matthias.",
                    "label": 0
                },
                {
                    "sent": "Sort of integrated PAC Bayes ideas, so this analysis is old predates the analysis.",
                    "label": 0
                },
                {
                    "sent": "I don't know of a good reference for analysis, I just showed you.",
                    "label": 0
                },
                {
                    "sent": "This predates all of this stuff.",
                    "label": 0
                },
                {
                    "sent": "At least, this choice of prior, so this is the right way to think about it.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to take kind of L0 prior.",
                    "label": 0
                },
                {
                    "sent": "We're going to take an IID sample of an independent draws from the features.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to take a uniformly weighted predictor, and that's our prior.",
                    "label": 0
                },
                {
                    "sent": "From those, an independent draws.",
                    "label": 0
                },
                {
                    "sent": "It's not really an L0 prior, but it's certainly a sparse prior.",
                    "label": 0
                },
                {
                    "sent": "I'm calling it in L0 prior, so the prior is this parameter end of the prior, which is I draw in features.",
                    "label": 0
                },
                {
                    "sent": "And then I wake them uniformly.",
                    "label": 0
                },
                {
                    "sent": "And that's my prior with parameter N. My posterior is centered.",
                    "label": 0
                },
                {
                    "sent": "My posterior involves a weight vector mu.",
                    "label": 0
                },
                {
                    "sent": "And there's a trick here where you is an easy trick that allows you only to consider positive weights for every feature you can add in a feature that gives you the negative version of that feature, so I can think of mu as defining under an L1 regularization is defining a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So I'm thinking the feature vector now is defining a probability distribution, the feature vector the mu gives me a distribution over the features and my posteriors to draw end times from the distribution mu.",
                    "label": 0
                },
                {
                    "sent": "So this is not it's not as it doesn't fit quite as cleanly into the two previous priors and posteriors, but it does have a nice clean analysis.",
                    "label": 0
                },
                {
                    "sent": "And again, because this is an N independent draws the posterior.",
                    "label": 0
                },
                {
                    "sent": "Involves an independent, draws the score of the posterior is again going to be Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So again, you want it again you to really understand what you're doing, you should put a probate loss in there.",
                    "label": 0
                },
                {
                    "sent": "OK, the bottom line is you get this formula.",
                    "label": 0
                },
                {
                    "sent": "Now that what's interesting is, this formula also has an L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "So if you go way back to boosting the margin it was derived in L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "This formula has an L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "They're different, they are in fact incomparable.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is linear in the one norm.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is quadratic in the one norm.",
                    "label": 0
                },
                {
                    "sent": "So linear in the one norm is better than quadratic in the one norm, because in order to get a good margin you're going to get a big feature vector.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This involves the two norm of the feature vector.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "This involves the Infinity norm of the feature vector.",
                    "label": 0
                },
                {
                    "sent": "So intuitively this is.",
                    "label": 0
                },
                {
                    "sent": "This is a kind of dual norm bound.",
                    "label": 0
                },
                {
                    "sent": "We used an L0 prior and we're getting an L Infinity property of the feature vector.",
                    "label": 0
                },
                {
                    "sent": "So there are lots of these dual norm bounds where you say you've got one norm of the weight vector and the dual norm of the feature vector.",
                    "label": 0
                },
                {
                    "sent": "So LL Infinity is better than L2, but it's a square and the bounds end up being incomparable, so there's a great.",
                    "label": 0
                },
                {
                    "sent": "Theoretically there's great empirical question.",
                    "label": 0
                },
                {
                    "sent": "I don't think there's nearly as much empirical experience with L1 regularization to say what the right defaults for an L1 support vector machine R. So I don't know, you know, if we had a decade of experience with with SVN packages.",
                    "label": 0
                },
                {
                    "sent": "You know which?",
                    "label": 0
                },
                {
                    "sent": "Which default value of Lambda would be better?",
                    "label": 0
                },
                {
                    "sent": "Presumably we can work with.",
                    "label": 0
                },
                {
                    "sent": "The minimum of the two.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the minimum of the two should always supply.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna switch topics a little bit.",
                    "label": 0
                },
                {
                    "sent": "There's this sigmoidal loss and the idea is that all the generalization theory, at least the PAC Bayesian generalization theory, says you should be working with this non convex sigmoid loss.",
                    "label": 0
                },
                {
                    "sent": "This elpro, but everybody in practice works with hinge loss.",
                    "label": 0
                },
                {
                    "sent": "The hinge loss can be viewed as a convex relaxation of the profit loss.",
                    "label": 1
                },
                {
                    "sent": "Now we also have at TTI Chicago.",
                    "label": 0
                },
                {
                    "sent": "We have a large stock Fox theory.",
                    "label": 0
                },
                {
                    "sent": "Group they do approximation algorithms in the stock Fox Community.",
                    "label": 0
                },
                {
                    "sent": "The bread and butter of approximation algorithms.",
                    "label": 0
                },
                {
                    "sent": "It takes some NP hard problem, some nonconvex problem.",
                    "label": 0
                },
                {
                    "sent": "You know graph.",
                    "label": 0
                },
                {
                    "sent": "Multi commodity flow problem or something.",
                    "label": 0
                },
                {
                    "sent": "Or a facility location problem and you define a convex relaxation and typically defining some LP relaxation of a 01.",
                    "label": 0
                },
                {
                    "sent": "Problem is trivial, right?",
                    "label": 0
                },
                {
                    "sent": "It's easy.",
                    "label": 0
                },
                {
                    "sent": "It's the first thing you can do.",
                    "label": 0
                },
                {
                    "sent": "And then what's hard is proving that if I solve the LP relaxation, there's an approximation ratio between the thing I care about and the convex relaxation.",
                    "label": 0
                },
                {
                    "sent": "So I think it's a horrible embarrassment to the machine learning community that SVM, the fundamental workhorse, at least a fundamental workhorse in machine learning is derived as a convex relaxation with no theorem.",
                    "label": 0
                },
                {
                    "sent": "There's no approximate.",
                    "label": 0
                },
                {
                    "sent": "I don't know of any approximation theorem that says that these are related by some approximation ratio.",
                    "label": 0
                },
                {
                    "sent": "There are results that say.",
                    "label": 0
                },
                {
                    "sent": "If you want to minimize the 01 loss of a weight vector, there is no approximation scheme.",
                    "label": 0
                },
                {
                    "sent": "There's recent stock box papers that say if you want to minimize exactly 01 loss.",
                    "label": 0
                },
                {
                    "sent": "It's there's no approximation scheme.",
                    "label": 0
                },
                {
                    "sent": "There's no, there's no good approximation at all.",
                    "label": 0
                },
                {
                    "sent": "But this but the margins involved get very get exponentially small.",
                    "label": 0
                },
                {
                    "sent": "So here was the approximation.",
                    "label": 0
                },
                {
                    "sent": "So after some closer W closing performance.",
                    "label": 0
                },
                {
                    "sent": "Here I'm just interested in the difference in value between the minimizer of this and the minimizer of that.",
                    "label": 0
                },
                {
                    "sent": "Objective so the objective function.",
                    "label": 0
                },
                {
                    "sent": "The idea is that this is a surrogate for the generalization loss right?",
                    "label": 0
                },
                {
                    "sent": "If we want to minimize the bound.",
                    "label": 0
                },
                {
                    "sent": "Pose we're trying to minimize this.",
                    "label": 0
                },
                {
                    "sent": "Then if we minimize this, do we get close in that value?",
                    "label": 0
                },
                {
                    "sent": "The problem with the stock Fox communities.",
                    "label": 0
                },
                {
                    "sent": "They're going to say, well, is that really an interesting problem to approximate, and I from a machine learning perspective, I think the machine learning community should care about this approximation ratio, and this is going to come up again.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to shift gears now and stop talking about textbook issues and start talking more about real research issues.",
                    "label": 0
                },
                {
                    "sent": "So I'm these days I've been very into structured predictions or prediction problems for structured outputs.",
                    "label": 0
                },
                {
                    "sent": "And just, you know, in terms of textbooks or talking to people who aren't necessarily in machine learning when it comes to structured outputs, I like to use machine translation as a paradigmatic example.",
                    "label": 0
                },
                {
                    "sent": "So you know you're doing English to French.",
                    "label": 0
                },
                {
                    "sent": "So imagine you know that X is an English sentence.",
                    "label": 1
                },
                {
                    "sent": "And why is a French sentence?",
                    "label": 0
                },
                {
                    "sent": "And the idea here is really.",
                    "label": 0
                },
                {
                    "sent": "This is really almost a statement about computer science in general, not just about machine learning.",
                    "label": 0
                },
                {
                    "sent": "Machine translation system is a software system, right?",
                    "label": 0
                },
                {
                    "sent": "It takes an input.",
                    "label": 0
                },
                {
                    "sent": "It has an output.",
                    "label": 0
                },
                {
                    "sent": "How many software systems?",
                    "label": 0
                },
                {
                    "sent": "Ultimately as we look into the future, are going to compute their output by optimizing?",
                    "label": 0
                },
                {
                    "sent": "So the claim is optimization is absolutely ubiquitous in computer science.",
                    "label": 0
                },
                {
                    "sent": "You know it's not just about machine learning.",
                    "label": 0
                },
                {
                    "sent": "The theory, community studies optimization.",
                    "label": 0
                },
                {
                    "sent": "Consider a software.",
                    "label": 0
                },
                {
                    "sent": "Any software system that goes from an input to an output through an optimization.",
                    "label": 0
                },
                {
                    "sent": "OK, and here we're making this optimization linear.",
                    "label": 0
                },
                {
                    "sent": "We're introducing a feature vector that involves the input and output.",
                    "label": 0
                },
                {
                    "sent": "We're taking a weight vector, but we all know as machine learning people that this linearity assumption is very weak, right?",
                    "label": 0
                },
                {
                    "sent": "Almost any help with some kernel trick.",
                    "label": 0
                },
                {
                    "sent": "Anything could be put in there, and Furthermore everything we're going to do could be done.",
                    "label": 0
                },
                {
                    "sent": "If this was some nonlinear scoring function.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can just apply a PAC Bayes.",
                    "label": 0
                },
                {
                    "sent": "So suppose we got it.",
                    "label": 0
                },
                {
                    "sent": "OK, let me back up again.",
                    "label": 0
                },
                {
                    "sent": "What the stock Fox community misses I think, and part of the reason that we're becoming more relevant to computer science generally is that in real software applications that compute by optimizing.",
                    "label": 0
                },
                {
                    "sent": "A fundamental issue is what is the objective function.",
                    "label": 0
                },
                {
                    "sent": "You can't sit down and write down an objective function for machine translation and expect it to work right, or genome annotation or speech recognition or computer vision scene understanding in computer vision.",
                    "label": 0
                },
                {
                    "sent": "These software systems are going to work by optimization, but the objective function has to come from somewhere, and so machine learning is about getting the objective function that the software system optimizes when it runs.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're about learning W here.",
                    "label": 0
                },
                {
                    "sent": "That's determining the objective function that the software system is going to use so we can take.",
                    "label": 0
                },
                {
                    "sent": "So we're considering.",
                    "label": 0
                },
                {
                    "sent": "So now I can apply a PAC Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "I can say there's a prior, I'm just going to going to the simple thing.",
                    "label": 0
                },
                {
                    "sent": "The trivial thing.",
                    "label": 0
                },
                {
                    "sent": "The first thing is to take a Gaussian prior on W. Take the Gaussian posterior on W. And then write down and then do the same analysis that I did and I'm going to get this a similar expression, except that this thing.",
                    "label": 0
                },
                {
                    "sent": "Is no longer a probit loss.",
                    "label": 0
                },
                {
                    "sent": "I'm just leaving the PAC Bayes expression in here and not trying to analyze it yet, so this is just writing down the pack Bayesian analysis.",
                    "label": 0
                },
                {
                    "sent": "RKL term comes out clean from this posterior from this prior in this posterior, but the empirical loss.",
                    "label": 0
                },
                {
                    "sent": "I'm just writing down as the empirical risk of the posterior, and this is.",
                    "label": 0
                },
                {
                    "sent": "This becomes a big problem in the structured case.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "What is?",
                    "label": 0
                },
                {
                    "sent": "How do we deal with this empirical list of the empirical risk of the posterior?",
                    "label": 0
                },
                {
                    "sent": "It's not a clean, it's not a clean probate loss of the margin.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I don't know if you're how many people are into structured output learning, but there's a standard by now.",
                    "label": 0
                },
                {
                    "sent": "A very widely cited Standard method, a generalization of SVM's to the structured case.",
                    "label": 0
                },
                {
                    "sent": "So there's something called a structured, structured SVM based on something called a structured hinge loss.",
                    "label": 0
                },
                {
                    "sent": "This is a generalization of hinge loss to the structured case, and it's often motivated.",
                    "label": 0
                },
                {
                    "sent": "Not always, but often motivated by taking it to be a convex relaxation of the loss itself, right?",
                    "label": 0
                },
                {
                    "sent": "So one, so if we think about hinge loss, we go back to hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Hinge loss is the step function.",
                    "label": 0
                },
                {
                    "sent": "There are people who motivate hinge loss by saying, well look, the hinge loss is an upper bound on the step loss.",
                    "label": 0
                },
                {
                    "sent": "Forget about regularization.",
                    "label": 0
                },
                {
                    "sent": "Just look at the 01 loss.",
                    "label": 0
                },
                {
                    "sent": "The hinge loss upper bounds.",
                    "label": 0
                },
                {
                    "sent": "It's a convex relaxation, so we should optimize.",
                    "label": 0
                },
                {
                    "sent": "We can optimize that convex relaxation of the hinge loss.",
                    "label": 1
                },
                {
                    "sent": "I can see that people are tired and I don't want to go through.",
                    "label": 1
                },
                {
                    "sent": "I don't want to push too many technical formulas, but one of the problems with the structured case.",
                    "label": 0
                },
                {
                    "sent": "As you have many possible outputs, not just one or minus one, and for each possible output.",
                    "label": 0
                },
                {
                    "sent": "There's also a target output in labeled training data.",
                    "label": 0
                },
                {
                    "sent": "Some reference translation, say, and for each possible output each candidate translation there's a different margin is a different difference in score between the thing you're trying to produce the reference translation, and the candidate.",
                    "label": 0
                },
                {
                    "sent": "So your margin is a function of the margin being the difference in score between what you want the reference translation and what you're producing depends on.",
                    "label": 0
                },
                {
                    "sent": "Which output you're doing.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I guess I don't want to go through this too carefully, but this margin if this is the best scoring value you're taking the best scoring value to be your output.",
                    "label": 0
                },
                {
                    "sent": "This margins always negative, right?",
                    "label": 0
                },
                {
                    "sent": "Or at least not.",
                    "label": 0
                },
                {
                    "sent": "It can't be positive because the best scoring value.",
                    "label": 0
                },
                {
                    "sent": "If this is the best scoring value, it has to be at least as large as the reference translation.",
                    "label": 0
                },
                {
                    "sent": "So the margin of the best scoring value is never positive.",
                    "label": 0
                },
                {
                    "sent": "So if I subtract something that's never positive, I get an upper bound.",
                    "label": 0
                },
                {
                    "sent": "So the score itself.",
                    "label": 0
                },
                {
                    "sent": "The loss itself is upper bounded by the loss minus the margin.",
                    "label": 0
                },
                {
                    "sent": "OK then if it's upper bounded by the loss minus the margin, then I can take a Max over the candidate outputs and it only gets bigger.",
                    "label": 0
                },
                {
                    "sent": "Right there, existing output that's up that's bigger than this.",
                    "label": 0
                },
                {
                    "sent": "If I take a Max over the outputs, then it's still bigger.",
                    "label": 0
                },
                {
                    "sent": "And this, then, is the structured hinge loss, the Max over possible outputs of this difference, and this margin is linear in W, so this is a Max of things linear in W, so it's convex.",
                    "label": 0
                },
                {
                    "sent": "A Max of linear functions is convex, so this gives you a convex piecewise linear upper bound on the loss itself, ignoring ignoring generalization.",
                    "label": 1
                },
                {
                    "sent": "And this is the way the structured hinge loss is usually motivated.",
                    "label": 0
                },
                {
                    "sent": "Is just a convex relaxation.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Mice in mice.",
                    "label": 0
                },
                {
                    "sent": "When I yeah, so you know this is this is a French sentence.",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "This last line is just saying that the structured hinge loss generalizes the binary hinge loss.",
                    "label": 0
                },
                {
                    "sent": "If Y is in minus one one, then this Max Max over Y hat reduces to a Max over 0 and 1 minus.",
                    "label": 0
                },
                {
                    "sent": "So the Max this Max turns into the Max of zero and one minus the margin.",
                    "label": 0
                },
                {
                    "sent": "So this is the way I originally understood the structured hinge loss, being a generalization of the 01 hinge loss.",
                    "label": 0
                },
                {
                    "sent": "But Ben Tasker beat me up and there's a much more insightful relationship between the structured hinge loss and the 01 hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Which is this?",
                    "label": 0
                },
                {
                    "sent": "Let me just say it in words.",
                    "label": 0
                },
                {
                    "sent": "I don't think she's like the slide.",
                    "label": 0
                },
                {
                    "sent": "Take a take a binary classification problem.",
                    "label": 0
                },
                {
                    "sent": "So I've got training pairs.",
                    "label": 0
                },
                {
                    "sent": "XYY is either one or minus one group your binary data into groups.",
                    "label": 0
                },
                {
                    "sent": "Sync.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of each group as a structured problem?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Because now it's got a bit string input.",
                    "label": 0
                },
                {
                    "sent": "All the X is and it's got a bit string.",
                    "label": 0
                },
                {
                    "sent": "Output all the wise.",
                    "label": 0
                },
                {
                    "sent": "Think of it as a structured problem an apply the structured SVM optimization problem to it, right?",
                    "label": 0
                },
                {
                    "sent": "So take a binary problem group it.",
                    "label": 0
                },
                {
                    "sent": "Think of it as a collection of structured problems.",
                    "label": 0
                },
                {
                    "sent": "Apply the structured hinge loss.",
                    "label": 1
                },
                {
                    "sent": "The structured hinge loss on the group binary problem gets back to the ordinary binary hinge loss on the original independent data right?",
                    "label": 1
                },
                {
                    "sent": "And that's a much.",
                    "label": 0
                },
                {
                    "sent": "Stronger and better intuition for the relationship between the structured hinge loss and the binary hinge loss, I'm just going to skip over the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the motivation of just going to a relaxation.",
                    "label": 0
                },
                {
                    "sent": "Is in the jochims Hoffman, Alton paper, Ben Tasker paper on this subject with Daphne, Kohler and Kastron proved a generalization bound.",
                    "label": 0
                },
                {
                    "sent": "So there were thinking about generalization and it turns out that if you consider the realizable case.",
                    "label": 0
                },
                {
                    "sent": "Meaning that at train time you're going to get all your things separated by a margin.",
                    "label": 0
                },
                {
                    "sent": "All your data points are going to be correctly classified an separated by a margin.",
                    "label": 1
                },
                {
                    "sent": "In the structured case, which is asking a lot.",
                    "label": 0
                },
                {
                    "sent": "So this would be crazy for machine translation.",
                    "label": 0
                },
                {
                    "sent": "Quite that you're going to get every translation word for word identical to the reference translation and separated by a margin from every other translation.",
                    "label": 0
                },
                {
                    "sent": "It's not going to happen right, but suppose it did happen.",
                    "label": 0
                },
                {
                    "sent": "Then you can prove a generalization bound.",
                    "label": 1
                },
                {
                    "sent": "And what's interesting about that generalization bound is that the margin requirement is different for different labels.",
                    "label": 0
                },
                {
                    "sent": "Now, in an ordinary SVM that one you take 1 minus the margin, that one is the margin requirement.",
                    "label": 0
                },
                {
                    "sent": "As soon as your margin reaches one, you don't have to worry about the data point, it's not contributing to your loss anymore.",
                    "label": 0
                },
                {
                    "sent": "In the structured case, you have many forgiven input.",
                    "label": 0
                },
                {
                    "sent": "You have many different outputs each of your different outputs can have a different margin requirement, right?",
                    "label": 0
                },
                {
                    "sent": "The margin you ask for for that are.",
                    "label": 0
                },
                {
                    "sent": "Output can be different and if you look at.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the structured bound.",
                    "label": 0
                },
                {
                    "sent": "You can interpret this L as a margin requirement.",
                    "label": 0
                },
                {
                    "sent": "Right, if every I.",
                    "label": 0
                },
                {
                    "sent": "If every for every label the margin is at least the loss.",
                    "label": 0
                },
                {
                    "sent": "Then your loss is 0.",
                    "label": 0
                },
                {
                    "sent": "Right, so if every label meets its margin requirement, your losses 0 where this is the margin requirement, and jochims and Hoffner all called this a margin, rescaled structural SVM.",
                    "label": 0
                },
                {
                    "sent": "They also have a slack, rescaled structural SVM that I'm not talking bout, which is a different convex relaxation.",
                    "label": 0
                },
                {
                    "sent": "Now what Tasker proved was that from a generalization theory point of view you can take the margin requirement to be Hamming distance.",
                    "label": 0
                },
                {
                    "sent": "And so motivating a Hamming distance.",
                    "label": 0
                },
                {
                    "sent": "Here you can motivate a Hamming distance here from generalization theory.",
                    "label": 0
                },
                {
                    "sent": "So they have a generalization bound that actually gets much tighter than than.",
                    "label": 0
                },
                {
                    "sent": "Like standard multiclass bounds, by using a Hamming loss margin requirement there.",
                    "label": 0
                },
                {
                    "sent": "But I think this is confusing because it confuses.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The loss function with the margin requirement.",
                    "label": 0
                },
                {
                    "sent": "There's a confusion in the literature.",
                    "label": 0
                },
                {
                    "sent": "I think the field is still confused about in structural SVM's.",
                    "label": 0
                },
                {
                    "sent": "What's the margin requirement and what's the loss function and how is it related to generalization?",
                    "label": 1
                },
                {
                    "sent": "And how's the structured hinge loss motivated all of these things are very confused.",
                    "label": 0
                },
                {
                    "sent": "I think in the community right now in an ordinary SVM you would say look at the fraction of the data that fails to meet its margin requirement.",
                    "label": 0
                },
                {
                    "sent": "Your loss is going to be proportional to the fraction of data that fails to meet its margin requirement plus a quadratic penalty.",
                    "label": 0
                },
                {
                    "sent": "This is the generalization of that to the structured case, except we're using a different margin requirement at every different label, and the margin requirement is a Hamming loss.",
                    "label": 0
                },
                {
                    "sent": "However, for something that doesn't satisfy its margin requirement, we plug the value we plug in is not one, but the loss of that thing, and the important point about this bound is that it separates the margin requirement from the loss function.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How am I doing on time?",
                    "label": 0
                },
                {
                    "sent": "OK, I'll try not to put everybody to sleep.",
                    "label": 0
                },
                {
                    "sent": "I'm going to shift gears a little bit again.",
                    "label": 0
                },
                {
                    "sent": "So this now I'm going to start talking about very concrete machine learning recipes.",
                    "label": 0
                },
                {
                    "sent": "And what's happening is something that's actually happening in the machine translation community.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "When you go to learn a structured label, there are various perceptron like learning rules.",
                    "label": 0
                },
                {
                    "sent": "So we've got some training data.",
                    "label": 0
                },
                {
                    "sent": "We're going to our training data, one training point at a time.",
                    "label": 0
                },
                {
                    "sent": "For every training point, we're going to update our weight vector.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this as a gradient descent method or a subgradient descent method.",
                    "label": 0
                },
                {
                    "sent": "But it but it goes back to Perceptron.",
                    "label": 0
                },
                {
                    "sent": "Which you can also think of as a subgradient descent method.",
                    "label": 0
                },
                {
                    "sent": "So the classical multiclass perceptron, this was championed in a paper by Michael Collins.",
                    "label": 1
                },
                {
                    "sent": "Is you're looking at a data point XY pair.",
                    "label": 0
                },
                {
                    "sent": "This is a training point, OK, an input and output input sentence.",
                    "label": 1
                },
                {
                    "sent": "Output reference translation.",
                    "label": 0
                },
                {
                    "sent": "I'm going to assume I have a feature vector on that pair.",
                    "label": 0
                },
                {
                    "sent": "This is the reference translation.",
                    "label": 0
                },
                {
                    "sent": "This is the label my system actually produces.",
                    "label": 0
                },
                {
                    "sent": "The best scoring label.",
                    "label": 0
                },
                {
                    "sent": "I should have written it as YW, Avex.",
                    "label": 0
                },
                {
                    "sent": "You say this is what you want.",
                    "label": 0
                },
                {
                    "sent": "This is what you've got.",
                    "label": 0
                },
                {
                    "sent": "Move your weight vector away from what you've gotten Tord, what you want.",
                    "label": 0
                },
                {
                    "sent": "And this is a generalization of the ordinary Perceptron update rule to the multiclass case, and you can prove a multi perceptron convergence to multiclass perceptron convergence theorem for this rule.",
                    "label": 0
                },
                {
                    "sent": "Direct generalization of the standard binary one.",
                    "label": 0
                },
                {
                    "sent": "OK, take the here's another update rule.",
                    "label": 0
                },
                {
                    "sent": "Take the structured SVM and do a subgradient algorithm for it.",
                    "label": 0
                },
                {
                    "sent": "So what is a subgradient algorithm means it means I look at a point.",
                    "label": 0
                },
                {
                    "sent": "I look at the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to optimize.",
                    "label": 0
                },
                {
                    "sent": "I compute a subgradient.",
                    "label": 0
                },
                {
                    "sent": "Then I move in that direction.",
                    "label": 0
                },
                {
                    "sent": "Now the hinge loss has a Max in it.",
                    "label": 0
                },
                {
                    "sent": "Let me go back.",
                    "label": 0
                },
                {
                    "sent": "Whoops forward.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the thing I'm going to try to minimize.",
                    "label": 0
                },
                {
                    "sent": "This is the generalization of Hingeless.",
                    "label": 0
                },
                {
                    "sent": "When I do a subgradient, what I'm going to do is find this optimum and then take the gradient of this with respect to my weight vector, right?",
                    "label": 0
                },
                {
                    "sent": "That's the subgradient that gives me a subgradient of this objective function is to find this Max.",
                    "label": 0
                },
                {
                    "sent": "Now if I do that.",
                    "label": 0
                },
                {
                    "sent": "I get this update.",
                    "label": 0
                },
                {
                    "sent": "I take it just works out that I take the reference translation, which is what I want, minus the optimum of the hingeless.",
                    "label": 0
                },
                {
                    "sent": "Minus the feature vector of the bottom of the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Now the feature vector of the outcome of the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "So here's here's the ordinary.",
                    "label": 0
                },
                {
                    "sent": "Best scoring value.",
                    "label": 0
                },
                {
                    "sent": "This is the best scoring value where instead of just maximizing the score, I'm also maximizing.",
                    "label": 0
                },
                {
                    "sent": "A component of my objective is the loss function itself, so I'm finding something that's both scoring well and has high loss.",
                    "label": 0
                },
                {
                    "sent": "So when I do subgradient of the hinge loss, this is called a loss adjusted inference.",
                    "label": 0
                },
                {
                    "sent": "So the idea of loss adjusted inference is.",
                    "label": 0
                },
                {
                    "sent": "Central is really central to this structured SVM.",
                    "label": 0
                },
                {
                    "sent": "So I do.",
                    "label": 0
                },
                {
                    "sent": "I add in a lot.",
                    "label": 0
                },
                {
                    "sent": "So what's the intuition here?",
                    "label": 0
                },
                {
                    "sent": "The intuition is I've got the reference translation, which is what I want, and I've got something that scores well, but that's really bad.",
                    "label": 0
                },
                {
                    "sent": "Not only does it score well, but it's bad and the idea is that among the things that score well, you know there's a lot of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "If I was doing PAC Bayesian sampling, I wouldn't be.",
                    "label": 0
                },
                {
                    "sent": "I'd be sampling many different outputs.",
                    "label": 0
                },
                {
                    "sent": "I care about the output that that's scores well and is bad.",
                    "label": 0
                },
                {
                    "sent": "That's the one I want to get away from.",
                    "label": 0
                },
                {
                    "sent": "That's the intuition.",
                    "label": 0
                },
                {
                    "sent": "OK, but there's this problem.",
                    "label": 0
                },
                {
                    "sent": "The structured hinge loss itself is flaky.",
                    "label": 1
                },
                {
                    "sent": "It's derived as a convex relaxation with no theorem associated with it.",
                    "label": 0
                },
                {
                    "sent": "It confuses the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Margin requirement.",
                    "label": 0
                },
                {
                    "sent": "With the loss function.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of flaky, so this is a great opportunity for Pacbase.",
                    "label": 0
                },
                {
                    "sent": "The theory is a mess.",
                    "label": 0
                },
                {
                    "sent": "So let's let's go for it.",
                    "label": 0
                },
                {
                    "sent": "So the first thing so so now this is research.",
                    "label": 0
                },
                {
                    "sent": "The first thing I did was I said well, I can't thinking about this.",
                    "label": 0
                },
                {
                    "sent": "Regularization is just too hard.",
                    "label": 0
                },
                {
                    "sent": "It's just too hard.",
                    "label": 0
                },
                {
                    "sent": "I'm going to throw away the regularization first, so suppose we were doing on line.",
                    "label": 0
                },
                {
                    "sent": "You never batch.",
                    "label": 0
                },
                {
                    "sent": "Let's do this online.",
                    "label": 0
                },
                {
                    "sent": "Let's sample and do an update with every fresh sample.",
                    "label": 0
                },
                {
                    "sent": "And if you do this online, you can just try and let's not try to regularize.",
                    "label": 0
                },
                {
                    "sent": "Let's try to minimize our loss.",
                    "label": 0
                },
                {
                    "sent": "So here's an intuitive idea for a perceptron like update.",
                    "label": 1
                },
                {
                    "sent": "And when I started working on this the this intuitive idea came to me long before there's any theorem associated with it.",
                    "label": 0
                },
                {
                    "sent": "But we're going to theorem.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, if I want to change my weight vector in a way that makes my translations better.",
                    "label": 0
                },
                {
                    "sent": "I don't really care so much about the reference translation.",
                    "label": 0
                },
                {
                    "sent": "What I care about is the neighborhood of the translations that my system is producing there going to be many translations that all score reasonably well.",
                    "label": 0
                },
                {
                    "sent": "Some of them are closer to my reference translation than others.",
                    "label": 0
                },
                {
                    "sent": "I would like to move my weight vector to shift my translator to be producing better scoring translations and the way I'm going to do that is by looking at, not the reference translation, but different translations that my translator produces.",
                    "label": 0
                },
                {
                    "sent": "So now what I'm going to do is I'm going to compute the label that I would output.",
                    "label": 0
                },
                {
                    "sent": "This is just the argmax I'm going to compute a loss adjusted.",
                    "label": 0
                },
                {
                    "sent": "Inference.",
                    "label": 1
                },
                {
                    "sent": "For for pragmatic reasons, instead of adding the loss, I'm going to subtract the loss.",
                    "label": 0
                },
                {
                    "sent": "So this is what the system produces a translation.",
                    "label": 0
                },
                {
                    "sent": "This is a better translation that scores well.",
                    "label": 0
                },
                {
                    "sent": "It's better translation 'cause it has lower loss, but it also scores well.",
                    "label": 0
                },
                {
                    "sent": "And what I'm going to do is I'm going to move away from the current translation and toward the better translation.",
                    "label": 0
                },
                {
                    "sent": "OK, so I take my so the difference between this and the structural SVM subgradient is that these are both in Ferd labels.",
                    "label": 0
                },
                {
                    "sent": "Neither one of them is the reference translation.",
                    "label": 0
                },
                {
                    "sent": "The reference translation is used in this adjusted inference, because if its adjusted to be in the direction of the reference translation, so the reference translation is used in the loss adjusted inference.",
                    "label": 0
                },
                {
                    "sent": "But neither of these are the reference translation.",
                    "label": 0
                },
                {
                    "sent": "These are both inferred labels.",
                    "label": 0
                },
                {
                    "sent": "OK, so it turns out so I had this intuition that this was a good idea.",
                    "label": 0
                },
                {
                    "sent": "And then we can.",
                    "label": 0
                },
                {
                    "sent": "We did some theory.",
                    "label": 0
                },
                {
                    "sent": "I did some theory.",
                    "label": 0
                },
                {
                    "sent": "Which I'll show you in a moment, but then I went to nips just a few months ago, and Michael Collins is, well, you know, the machine learning trend, the machine translation community has moved over to this already.",
                    "label": 0
                },
                {
                    "sent": "Any which you would really like to do the theory and then get people to follow you, but it's.",
                    "label": 0
                },
                {
                    "sent": "Good, at least when you're you know.",
                    "label": 0
                },
                {
                    "sent": "You discover that they're doing it already.",
                    "label": 0
                },
                {
                    "sent": "So not exactly this equation, but what they do is they produce N best lists of the translation so they get a neighborhood of translations they might produce, and then they do this kind of update by picking the.",
                    "label": 0
                },
                {
                    "sent": "Maybe the best and the worst of their end.",
                    "label": 0
                },
                {
                    "sent": "Best list as defined by the reference translation so they look at their end best list.",
                    "label": 0
                },
                {
                    "sent": "See which ones near the reference translation and move their weight vector toward that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And it's it's.",
                    "label": 0
                },
                {
                    "sent": "I think it's sort of taken over, and a lot of people you know feel that it's a cheat.",
                    "label": 0
                },
                {
                    "sent": "Somehow these machines, these trends, machine translation people are just hacks, right?",
                    "label": 0
                },
                {
                    "sent": "Because this is a really ugly thing to do, there's no theory.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here's the theory.",
                    "label": 0
                },
                {
                    "sent": "So here's a theorem.",
                    "label": 0
                },
                {
                    "sent": "This is what we care about.",
                    "label": 0
                },
                {
                    "sent": "This is the generalization loss, and let's suppose we just want to do gradient descent on it.",
                    "label": 0
                },
                {
                    "sent": "So we want the gradient and W of this.",
                    "label": 0
                },
                {
                    "sent": "It's a theorem that this gradient is equal to the limit as epsilon goes to zero of this feature difference.",
                    "label": 0
                },
                {
                    "sent": "So what this says is in the limit as epsilon goes to 0, sorry.",
                    "label": 0
                },
                {
                    "sent": "In the limit, is this epsilon in the loss adjustment goes to zero.",
                    "label": 0
                },
                {
                    "sent": "The expectation of this update becomes the gradient direction of the loss I care about.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this paper is under review at ICML.",
                    "label": 0
                },
                {
                    "sent": "And the difficulty with it with the paper and with this talk at this moment is that this proof is impenetrable.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The intuition let me just give you, let me just say some intuitive things.",
                    "label": 0
                },
                {
                    "sent": "I'm going, I thought I would try to take you through.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bit of this proof, but let me say some intuitive things at least first.",
                    "label": 0
                },
                {
                    "sent": "This loss.",
                    "label": 0
                },
                {
                    "sent": "This label as defined by W is discrete object, right?",
                    "label": 0
                },
                {
                    "sent": "It's like a sentence.",
                    "label": 0
                },
                {
                    "sent": "So if I look at the at the translation being currently produced by the system and try to differentiate this loss by the blue score, which is what these people use with respect to W, it doesn't have a derivative.",
                    "label": 0
                },
                {
                    "sent": "Right, because we've got a system that's producing discrete outputs and there's going to be some range of W that all produce the same translation.",
                    "label": 0
                },
                {
                    "sent": "So you can't just differentiate the loss in here with respect to W, it's not differentiable.",
                    "label": 0
                },
                {
                    "sent": "How is it possible that this gradient then even exists?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What's happening in this theorem is?",
                    "label": 0
                },
                {
                    "sent": "I'm assuming that the input is from a continuous space.",
                    "label": 0
                },
                {
                    "sent": "The output can be from a discrete space, but this theorem, this analysis assumes the input is from a continuous space.",
                    "label": 0
                },
                {
                    "sent": "Now that continuous space has, the translator is going to divide that continuous space into cells, where each cell is the range of continuous values that produce a given output.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can even think of this in the binary case.",
                    "label": 0
                },
                {
                    "sent": "Suppose the labels were binary.",
                    "label": 0
                },
                {
                    "sent": "This works perfectly well in the binary case.",
                    "label": 0
                },
                {
                    "sent": "Supposed labels were binary.",
                    "label": 0
                },
                {
                    "sent": "There's a region of input that gets labeled minus one.",
                    "label": 0
                },
                {
                    "sent": "There's a region of input that gets labeled one.",
                    "label": 0
                },
                {
                    "sent": "There's a boundary between those two regions.",
                    "label": 0
                },
                {
                    "sent": "As I change the parameter vector W, the decision boundary moves.",
                    "label": 0
                },
                {
                    "sent": "The decision boundary moves in a way that's continuous in W. And assuming you've got some smooth density over the over these, assuming this row is some smooth density over the input output pairs, the fact that the decision boundary is changing continuously in W means that this gradient is going to exist.",
                    "label": 0
                },
                {
                    "sent": "Like there's going to be some gradient and W of the error rate.",
                    "label": 0
                },
                {
                    "sent": "But that gradient only occurs at the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And if you think about this, update this update.",
                    "label": 0
                },
                {
                    "sent": "If these two labels are the same, this update is 0.",
                    "label": 0
                },
                {
                    "sent": "So what's happening is if epsilon is small, the loss adjustment is small.",
                    "label": 0
                },
                {
                    "sent": "Then the two labels are very likely to be the same, specially in the binary case.",
                    "label": 0
                },
                {
                    "sent": "Think about the binary case.",
                    "label": 0
                },
                {
                    "sent": "If the loss adjustment is small in most of the X region, you're going to produce the same label in the update is 0.",
                    "label": 0
                },
                {
                    "sent": "So this update is only going to happen if I pick a point near the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "So this expectation as epsilon goes to 0 is all about what happens right at the decision boundary, 'cause the gradient is all about what happens right at the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that if you analyze.",
                    "label": 0
                },
                {
                    "sent": "If you analyze these two expressions.",
                    "label": 0
                },
                {
                    "sent": "Both of them by by sort of working out for a finite epsilon probability that so for this one, for example, I say, well consider a change in W, right?",
                    "label": 0
                },
                {
                    "sent": "The grading is all about what happens when I change you consider changing W. Under what conditions does the loss change?",
                    "label": 0
                },
                {
                    "sent": "And that's all about picking a change that's all about looking at X is which are near the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "Anyway, it works out that if you analyze these two from that perspective, you get the same expression.",
                    "label": 0
                },
                {
                    "sent": "And this is the analysis.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what happens is, in both analysis you get an indicator function which is integrating over the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "This is this can be viewed as an integral over a decision boundary.",
                    "label": 0
                },
                {
                    "sent": "The decision boundary is where the wait times the Delta feature vector between two labels is 0.",
                    "label": 0
                },
                {
                    "sent": "The two labels are tide.",
                    "label": 0
                },
                {
                    "sent": "I think they have the same score.",
                    "label": 0
                },
                {
                    "sent": "This is saying 2 labels are tide.",
                    "label": 0
                },
                {
                    "sent": "To say that this is 0.",
                    "label": 0
                },
                {
                    "sent": "But instead of saying it's zero, I'm going to say it's in some little interval.",
                    "label": 0
                },
                {
                    "sent": "And when you integrate over the decision boundary, there's two things.",
                    "label": 0
                },
                {
                    "sent": "There's the width of the boundary layer, and there's the density along the boundary.",
                    "label": 0
                },
                {
                    "sent": "And in one analysis you get a width times the width times a Delta loss, and in the other analysis you get a width times changing feature vector.",
                    "label": 0
                },
                {
                    "sent": "In both cases you take the width out and you get the same expression.",
                    "label": 0
                },
                {
                    "sent": "So the problem with the paper that's been submitted by CML is the reviewers have say that proof is impenetrable.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Future work.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a continuous version.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People like so the part of the problem is there are discrete labels, so you get a lot of combinatorics.",
                    "label": 0
                },
                {
                    "sent": "We have a version that says suppose everything insight is continuous, all the labels are continuous.",
                    "label": 0
                },
                {
                    "sent": "That proof is not.",
                    "label": 0
                },
                {
                    "sent": "It's also not so easy.",
                    "label": 0
                },
                {
                    "sent": "Not so easy to read, but at least everything is continuous in it and it did validates the other theorem.",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm going.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Skip this.",
                    "label": 0
                },
                {
                    "sent": "Skip this experiment.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm asking this, I think.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Done right, I'm out of time.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a summary.",
                    "label": 0
                },
                {
                    "sent": "I'll stop there, thanks.",
                    "label": 0
                }
            ]
        }
    }
}