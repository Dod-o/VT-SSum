{
    "id": "2clkop2mg2uat5ei4qxe2pu6acmpqav5",
    "title": "Deep multi-view representation learning of brain responses to natural stimuli",
    "info": {
        "author": [
            "Leila Wehbe, Helen Wills Neuroscience Institute, UC Berkeley"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_wehbe_deep_multiview/",
    "segmentation": [
        [
            "Hi, I'm a postdoc at UC Berkeley and this is joint work with other people in my lab and were Alex Fatima and Natalia and our advisor Jack Allen.",
            "So I'm interested."
        ],
        [
            "And how the brain represents information in general and what I use in my research is machine learning and noninvasive brain imaging.",
            "So for example, fMRI, or also emoji and EG.",
            "And."
        ],
        [
            "A typical fMRI experiment is just a subject line down the scanner and doing something, and the scanner generates a series of volumes, so every image takes about one to two seconds to acquire.",
            "And this is a 3D image, and so every pixel here is a volume pixel, so we called it a voxel.",
            "And basically that's one image after the other being generated."
        ],
        [
            "So typically fMRI experiments are done in a kind of a simple fashion, so you isolate one thing that you want to study.",
            "For example, what is the difference between abstract and concrete nouns?",
            "And you find a set of concrete nouns and a set of abstract nouns, and you randomize them somehow, and then you do a sort of a 2 sample tests where you see how is the brain activity affected differently, where in the brain is there is a brain activity different along these two conditions, so.",
            "In order to do that, you need to design a very good controlled stimulus that only varies along this one dimension, and that's like ends up having some problems because you might end up with artificial experiments that might not generate generalize very well to real life conditions.",
            "Another problem is that if you want to study something like semantic representation like imagine you have to actually do this binary comparison for every semantic concept.",
            "It will become like very expensive and impossible.",
            "And so."
        ],
        [
            "So what I'm interested in is to actually use naturalistic experiments, and by that I mean somebody just gets into, scan the scanner and does something that is similar to what they do in real life like Watch a movie or listen to a story or read a book.",
            "Well before I talk more, I just want to show you an example of what the experiment looks like if you can just look at the screen."
        ],
        [
            "So this is this is a reconstruction of the head of the subject from high dimensional scan, and this grid is actually those voxels.",
            "So those voxels are kind of a little bit low dimension should not compare to the actual surface scans that we can get, and so there's a software that reconstructs.",
            "So the software reconstructs the head of the subject, and we can also inflate the head of the subject through instruction, and then we can also just cut it and."
        ],
        [
            "Separated like this.",
            "So now the whole brain is actually just a flat map and we can look at the whole brain in one go.",
            "OK so I can show you actually what data from an experiment looks like, so again this is the flat and brain of 1 subject and we're going to plot the data through time as the subject goes through an experiment.",
            "And so basically the data changes very slowly.",
            "It's actually not only is it one image for two seconds, but it's also imaging a slow process, which is a change in the oxygen level of the blood, and so you see that the input is very very diverse on so many dimensions, right?",
            "Well, Meanwhile brain activity is very slowly in different regions.",
            "And like different brain regions are responding to different parts of the activity and we actually want to understand how we actually want to kind of.",
            "Understand everything like try to model every single voxel in the brain and try to understand what's happening there OK?"
        ],
        [
            "Alright, so the problem with this with this, these experiments as you can see is there's no clear classes.",
            "I don't have two conditions and I cannot build a classifier, and it's not even interesting to use a classifying this problem because I actually want to build a predictor of brain activity.",
            "So I have this high complex input that's varying, like so many dimensions, and we're going to try to do is actually model what is going on.",
            "What kind of information is being processed and trying to see how the brain processes."
        ],
        [
            "OK so we built feature spaces so as I said I not only use movies, but sometimes also the subject is sitting in the scanner and listening to stories or reading stories, and these stories have acoustic properties.",
            "They have semantic properties so we can take.",
            "We can take the the phonetic features of those stories and we can establish some feature space.",
            "So here every row is a time point, so it's 2 seconds an experiment, and here it's supposed to have 39 dimensions corresponding to 39 different phonemes, and how many times they occur at each time point, so."
        ],
        [
            "This is a simple feature space.",
            "It's a phonetic feature space.",
            "Another feature space that we can build is word, two VEC or semantic feature space.",
            "In this case, we just use a word two VEC embeddings, but you can use really any model you want.",
            "So again for all the words that occur at every 2 seconds, we just average their work to VEC vectors, and now we have a semantic representation for that particular time point."
        ],
        [
            "For the movies we can do the same thing, so there are objects that are occurring.",
            "Those movies we can take them and we can find their word to vector representations and we can average them together and get a semantic vector for each time."
        ],
        [
            "OK, and also of course the movies have very rich visual properties, so the way we used them in the lab is to construct motion energy filters.",
            "So these are spatial spatial temporal Gabor filters.",
            "Now it's work OK so you can see how we can detect the motion in the image along main axis, different location, different directions and so this.",
            "Basically this is."
        ],
        [
            "A way of annotating the visual information at every given time point.",
            "And."
        ],
        [
            "Now we have different feature spaces.",
            "Annotating the content of the stimulus, and we also have activity from multiple subjects that are reading both listening to the stories and there also watching the movies.",
            "And each one of those subjects has 30 to 50,000 dimensions and about 300 three 1610 point per experiment.",
            "So I can also draw the subjects data in the same space.",
            "So every row now corresponds to one time points, and all of these voxels are just.",
            "Collapse into one one vector.",
            "Now there's a specific complexity here that because of the delay in the signal in the fMRI signal, so it's taken into account, but I'm not going to go into it right now."
        ],
        [
            "OK, so this is a small data scenario.",
            "And also the signal to noise ratio is not very good.",
            "So actually because we have all these diverse experiments and multiple subjects viewing the same thing, what we want to do is we want to generalize across these experiments across these different cognitive tasks and along across different feature spaces to build like 1 model."
        ],
        [
            "And what we want.",
            "So what typically what people do and imaging useful tasks, tasks to use our for example, to build the model that we can predict brain data from the features of the stimulus.",
            "So this will tell you that actually, you've understood how different.",
            "Brain regions work if you can actually predict them correctly.",
            "You also might want to just do a decoding task, so given a brain image you want to guess what the subject was seeing at that time, or read their brain.",
            "Also, different people's brains are shaped of somewhat differently.",
            "However, the organization is kind of similar, but it's similar in very nonlinear ways, and so a very big problem in imaging is how can you combine the data from these multiple subjects together.",
            "Another one is also that you might have data for all of the subjects, but for new experiments you only have for a few.",
            "So how can you transfer what you know from the subjects from other experiments into new experiment?",
            "So basically all these things are usually done separately by different models, and we want our model to actually do all of these things at once, and so this is a model that."
        ],
        [
            "Use I'm going to walk you through it one step at a time so we."
        ],
        [
            "Word with one cognitive event.",
            "So this is for example, the subject is sitting and he's listening to some words.",
            "These are the words that occur in those two seconds in that 2 second segments and at the same time.",
            "One image is recorded from one subject.",
            "And also we can represent the content of those words in a specific feature space.",
            "I'm just drawing one here, but all of them are used at the same time.",
            "Also, there's other other other subjects that are also looking at our listening to the sentence at the same time.",
            "And So what these are, these are multiple views of the same event, so the same event is the event is the perception of these words and we have different views from multiple places, multiple sources.",
            "And so the first level is every view gets its own encoder, but then we combined the bottleneck or the output of the encoders into one shared bottleneck.",
            "So this would be like latent cognitive space that's shared among all the all the modalities.",
            "And then we can reconcile reconstruct every single view from this shared bottleneck.",
            "OK."
        ],
        [
            "How can this help us do what we want to do?",
            "So once the model is trained, you can take this stimulus, any stimulus you can get the feature of the symbols and you can actually predict brain activity by at first."
        ],
        [
            "Estimating the shared space and then predicting the data for the individual subjects."
        ],
        [
            "You can also take the data from one subject, estimate this shared space an."
        ],
        [
            "Then predict the features of this demo.",
            "So do this decoding.",
            "You"
        ],
        [
            "Also take the activity for one subject and predict the activity for the other subjects."
        ],
        [
            "1."
        ],
        [
            "A tweak that we did to this model is actually this bottleneck layer.",
            "Sure, it's shared among all these views, but it's not really reasonable to think that word to back is going to contain all of the information that is present in the brain image, right?",
            "So what we did is we use the whole all of the bottleneck layer for all of the subject data, so it's."
        ],
        [
            "Actually has access to the output of the subject encoders and it."
        ],
        [
            "Also used as input to this subject decoders."
        ],
        [
            "Bye."
        ],
        [
            "Forward to back, we actually choose a subset of nodes."
        ],
        [
            "And these are the only nodes that have access to the output of the word to the encoder."
        ],
        [
            "Also, the only notes that I used for the word, two VEC decoder and we also do the same."
        ],
        [
            "I think for the different feature spaces that we have and we also I mean.",
            "Also it's unreasonable to assume that we actually found all of the relevant feature spaces, so there's some activity that will be shared among subjects that is not summarized by any other feature spaces.",
            "So we do leave some notes that are not attached to any of the feature spaces and the way we pick the number of nodes and how many notes assigned to each layer is by cross validation.",
            "And we do a grid search longer few parameters."
        ],
        [
            "OK so this is multimodal model representation learning we saw this many times in the school already of for like text to text or text to images.",
            "It's also this model is similar to the bimodal order split autoencoder.",
            "We also tried other things like GCC or deep correlated.",
            "Clearly this is autoencoders.",
            "So these are not so they're very good for arriving at a representation that is useful for classification, for example.",
            "But here what we really want to do is we want to do this reconstruction, so we really want this particular architecture that we have that estimates the same shared space and then reconstruct is optimized to reconstruct from that space all the fuse.",
            "Also, I actually saw this paper just today.",
            "This is a recent paper.",
            "Young is is on it and.",
            "Is is also using some more representation.",
            "Learning for four brain images, however it's a very different problem.",
            "This particular paper is.",
            "So different types of data from the same subject, like anatomical and structural data are used or are used together in a sequence to sequence model to predict behavioral parameters for that subject.",
            "Like cognitive cognitive skills.",
            "And here what we're really interested in in this model is actually to model the contents of brain activity, and so this is a very this is a separate task."
        ],
        [
            "OK, so here are the results.",
            "So I'm going to just a subset of the results, and I'm going to see what happens when we try to predict brain activity from the word two VEC features in the story, experiments and the way we measure our performance is by seeing how well we predict held out data.",
            "And the region we I'm plotting here on the flat map the correlation between held out data and predicted data, and so we can see there's a large part of the of the cortex that is predicted well by word to back."
        ],
        [
            "Interestingly, if we look at the phonetic data the phoneme data, there's a small portion of the.",
            "This is a temporal cortex in this particular portion that is well predicted.",
            "This actually the primary auditory system primary auditory cortex, which makes sense because these are phonetic features."
        ],
        [
            "When we go to the movie data and we use the motion energy features which are very which carry visual information, we can see that the visual or you can see my mouse.",
            "Sorry, the visual cortex which is in the middle of the middle.",
            "Over here.",
            "This region.",
            "Is is very well predicted by those features, which also makes sense."
        ],
        [
            "When we move to higher level features, so where to back features which are semantic and do not carry lower level information?",
            "You can see that this lower level visual cortex is no longer well predicted.",
            "Also."
        ],
        [
            "Predict one subject data from another subject data and you can see here.",
            "Actually now there's more regions of the brain that are well predicted and that makes sense because there is some information that was not.",
            "We were not able to predict by just using the features of the symbols because we don't have a complete feature set.",
            "Yes, those numbers.",
            "Yeah, this is a percentage of prediction, so this is fMRI data is pretty noisy, so these are kind of, so there's there's an inherent noise ceiling that you cannot go above it just because if you just repeat the experiment over and over.",
            "The exact same experiment, you will get very noisier.",
            "Yes, yes, this is the courage.",
            "OK, and so this is a result for predicting subjects, one from subject from subjects one and you can."
        ],
        [
            "See if it's what I saw it said yesterday.",
            "The fashions are very similar, however, their deformed in kind of a very nonlinear way.",
            "OK, so we were able to."
        ],
        [
            "To achieve this unified model to some extent, so we combined experiments, subject an feature spaces and we were able to run a lot of very, very standard neuroimaging tests that are used by a lot of neuroimaging research and actually our performance is very compatible to all of the, like the standard state of the art techniques and actually.",
            "These techniques are actually just using one of these problem at a time and optimizing that problem, but our method optimizes all these.",
            "All these tasks together.",
            "But also what it does is it learns this bottleneck layer right?",
            "So this is this embedded space for all of this data is shared shared information from all of the subjects and also from the information the stimulus features."
        ],
        [
            "And we tried to explore it as well.",
            "So if."
        ],
        [
            "If I just for example, just stress on the word two VEC nodes, so these represent kind of a latent representation of work packet, lower dimensional representation.",
            "But this is not just containing word two vec, how great varies or how to best?",
            "It reduces dimensionality.",
            "Actually also carries information about how to best predict brain data.",
            "So this is kind of work to back.",
            "That is more by brain data and for example we can turn on."
        ],
        [
            "One of the nodes and we can see like for example, this is a cluster, that is that is predicted or that is closest to the reconstruction.",
            "If we just use this node and we can even also see which brain regions are also connected to this node and via separate set of experiments we just like these words do tend to cluster in these regions and different subjects, or they do tend to activate these regions in difference in differences.",
            "OK."
        ],
        [
            "So this is just the first step of our approach.",
            "We so we were able to achieve all this without using any spatial information.",
            "We're just thinking of the brain as a vector, but you can see how special information might be very useful.",
            "So maybe we will implement like a convolution layer, but only at the like, maybe only at the first source first and second light levels of the encoders, because it's not reasonable to assume that.",
            "That so patterns and ephemera in an fMRI image are not really invariants in location, for example.",
            "So we do want them to stay localized, but maybe just move a little bit among subjects.",
            "We also this is a very slowly varying data and we did not use any temporal information, so we can add maybe some recurrence in our model.",
            "And also the way we.",
            "The way we allocated the.",
            "How many notes to give for each task was done by a grid search, but maybe we might think of a way to how to do that on line when when learning the network."
        ],
        [
            "OK, that was all, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, I'm a postdoc at UC Berkeley and this is joint work with other people in my lab and were Alex Fatima and Natalia and our advisor Jack Allen.",
                    "label": 0
                },
                {
                    "sent": "So I'm interested.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And how the brain represents information in general and what I use in my research is machine learning and noninvasive brain imaging.",
                    "label": 1
                },
                {
                    "sent": "So for example, fMRI, or also emoji and EG.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A typical fMRI experiment is just a subject line down the scanner and doing something, and the scanner generates a series of volumes, so every image takes about one to two seconds to acquire.",
                    "label": 1
                },
                {
                    "sent": "And this is a 3D image, and so every pixel here is a volume pixel, so we called it a voxel.",
                    "label": 0
                },
                {
                    "sent": "And basically that's one image after the other being generated.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So typically fMRI experiments are done in a kind of a simple fashion, so you isolate one thing that you want to study.",
                    "label": 0
                },
                {
                    "sent": "For example, what is the difference between abstract and concrete nouns?",
                    "label": 0
                },
                {
                    "sent": "And you find a set of concrete nouns and a set of abstract nouns, and you randomize them somehow, and then you do a sort of a 2 sample tests where you see how is the brain activity affected differently, where in the brain is there is a brain activity different along these two conditions, so.",
                    "label": 0
                },
                {
                    "sent": "In order to do that, you need to design a very good controlled stimulus that only varies along this one dimension, and that's like ends up having some problems because you might end up with artificial experiments that might not generate generalize very well to real life conditions.",
                    "label": 0
                },
                {
                    "sent": "Another problem is that if you want to study something like semantic representation like imagine you have to actually do this binary comparison for every semantic concept.",
                    "label": 0
                },
                {
                    "sent": "It will become like very expensive and impossible.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I'm interested in is to actually use naturalistic experiments, and by that I mean somebody just gets into, scan the scanner and does something that is similar to what they do in real life like Watch a movie or listen to a story or read a book.",
                    "label": 0
                },
                {
                    "sent": "Well before I talk more, I just want to show you an example of what the experiment looks like if you can just look at the screen.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is this is a reconstruction of the head of the subject from high dimensional scan, and this grid is actually those voxels.",
                    "label": 0
                },
                {
                    "sent": "So those voxels are kind of a little bit low dimension should not compare to the actual surface scans that we can get, and so there's a software that reconstructs.",
                    "label": 0
                },
                {
                    "sent": "So the software reconstructs the head of the subject, and we can also inflate the head of the subject through instruction, and then we can also just cut it and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Separated like this.",
                    "label": 0
                },
                {
                    "sent": "So now the whole brain is actually just a flat map and we can look at the whole brain in one go.",
                    "label": 0
                },
                {
                    "sent": "OK so I can show you actually what data from an experiment looks like, so again this is the flat and brain of 1 subject and we're going to plot the data through time as the subject goes through an experiment.",
                    "label": 0
                },
                {
                    "sent": "And so basically the data changes very slowly.",
                    "label": 0
                },
                {
                    "sent": "It's actually not only is it one image for two seconds, but it's also imaging a slow process, which is a change in the oxygen level of the blood, and so you see that the input is very very diverse on so many dimensions, right?",
                    "label": 0
                },
                {
                    "sent": "Well, Meanwhile brain activity is very slowly in different regions.",
                    "label": 0
                },
                {
                    "sent": "And like different brain regions are responding to different parts of the activity and we actually want to understand how we actually want to kind of.",
                    "label": 0
                },
                {
                    "sent": "Understand everything like try to model every single voxel in the brain and try to understand what's happening there OK?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so the problem with this with this, these experiments as you can see is there's no clear classes.",
                    "label": 1
                },
                {
                    "sent": "I don't have two conditions and I cannot build a classifier, and it's not even interesting to use a classifying this problem because I actually want to build a predictor of brain activity.",
                    "label": 0
                },
                {
                    "sent": "So I have this high complex input that's varying, like so many dimensions, and we're going to try to do is actually model what is going on.",
                    "label": 0
                },
                {
                    "sent": "What kind of information is being processed and trying to see how the brain processes.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so we built feature spaces so as I said I not only use movies, but sometimes also the subject is sitting in the scanner and listening to stories or reading stories, and these stories have acoustic properties.",
                    "label": 1
                },
                {
                    "sent": "They have semantic properties so we can take.",
                    "label": 0
                },
                {
                    "sent": "We can take the the phonetic features of those stories and we can establish some feature space.",
                    "label": 0
                },
                {
                    "sent": "So here every row is a time point, so it's 2 seconds an experiment, and here it's supposed to have 39 dimensions corresponding to 39 different phonemes, and how many times they occur at each time point, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a simple feature space.",
                    "label": 0
                },
                {
                    "sent": "It's a phonetic feature space.",
                    "label": 0
                },
                {
                    "sent": "Another feature space that we can build is word, two VEC or semantic feature space.",
                    "label": 0
                },
                {
                    "sent": "In this case, we just use a word two VEC embeddings, but you can use really any model you want.",
                    "label": 0
                },
                {
                    "sent": "So again for all the words that occur at every 2 seconds, we just average their work to VEC vectors, and now we have a semantic representation for that particular time point.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the movies we can do the same thing, so there are objects that are occurring.",
                    "label": 0
                },
                {
                    "sent": "Those movies we can take them and we can find their word to vector representations and we can average them together and get a semantic vector for each time.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and also of course the movies have very rich visual properties, so the way we used them in the lab is to construct motion energy filters.",
                    "label": 1
                },
                {
                    "sent": "So these are spatial spatial temporal Gabor filters.",
                    "label": 0
                },
                {
                    "sent": "Now it's work OK so you can see how we can detect the motion in the image along main axis, different location, different directions and so this.",
                    "label": 0
                },
                {
                    "sent": "Basically this is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A way of annotating the visual information at every given time point.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we have different feature spaces.",
                    "label": 1
                },
                {
                    "sent": "Annotating the content of the stimulus, and we also have activity from multiple subjects that are reading both listening to the stories and there also watching the movies.",
                    "label": 1
                },
                {
                    "sent": "And each one of those subjects has 30 to 50,000 dimensions and about 300 three 1610 point per experiment.",
                    "label": 0
                },
                {
                    "sent": "So I can also draw the subjects data in the same space.",
                    "label": 1
                },
                {
                    "sent": "So every row now corresponds to one time points, and all of these voxels are just.",
                    "label": 0
                },
                {
                    "sent": "Collapse into one one vector.",
                    "label": 0
                },
                {
                    "sent": "Now there's a specific complexity here that because of the delay in the signal in the fMRI signal, so it's taken into account, but I'm not going to go into it right now.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is a small data scenario.",
                    "label": 1
                },
                {
                    "sent": "And also the signal to noise ratio is not very good.",
                    "label": 0
                },
                {
                    "sent": "So actually because we have all these diverse experiments and multiple subjects viewing the same thing, what we want to do is we want to generalize across these experiments across these different cognitive tasks and along across different feature spaces to build like 1 model.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we want.",
                    "label": 0
                },
                {
                    "sent": "So what typically what people do and imaging useful tasks, tasks to use our for example, to build the model that we can predict brain data from the features of the stimulus.",
                    "label": 0
                },
                {
                    "sent": "So this will tell you that actually, you've understood how different.",
                    "label": 0
                },
                {
                    "sent": "Brain regions work if you can actually predict them correctly.",
                    "label": 0
                },
                {
                    "sent": "You also might want to just do a decoding task, so given a brain image you want to guess what the subject was seeing at that time, or read their brain.",
                    "label": 0
                },
                {
                    "sent": "Also, different people's brains are shaped of somewhat differently.",
                    "label": 0
                },
                {
                    "sent": "However, the organization is kind of similar, but it's similar in very nonlinear ways, and so a very big problem in imaging is how can you combine the data from these multiple subjects together.",
                    "label": 0
                },
                {
                    "sent": "Another one is also that you might have data for all of the subjects, but for new experiments you only have for a few.",
                    "label": 0
                },
                {
                    "sent": "So how can you transfer what you know from the subjects from other experiments into new experiment?",
                    "label": 0
                },
                {
                    "sent": "So basically all these things are usually done separately by different models, and we want our model to actually do all of these things at once, and so this is a model that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use I'm going to walk you through it one step at a time so we.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Word with one cognitive event.",
                    "label": 1
                },
                {
                    "sent": "So this is for example, the subject is sitting and he's listening to some words.",
                    "label": 0
                },
                {
                    "sent": "These are the words that occur in those two seconds in that 2 second segments and at the same time.",
                    "label": 0
                },
                {
                    "sent": "One image is recorded from one subject.",
                    "label": 0
                },
                {
                    "sent": "And also we can represent the content of those words in a specific feature space.",
                    "label": 0
                },
                {
                    "sent": "I'm just drawing one here, but all of them are used at the same time.",
                    "label": 0
                },
                {
                    "sent": "Also, there's other other other subjects that are also looking at our listening to the sentence at the same time.",
                    "label": 0
                },
                {
                    "sent": "And So what these are, these are multiple views of the same event, so the same event is the event is the perception of these words and we have different views from multiple places, multiple sources.",
                    "label": 1
                },
                {
                    "sent": "And so the first level is every view gets its own encoder, but then we combined the bottleneck or the output of the encoders into one shared bottleneck.",
                    "label": 0
                },
                {
                    "sent": "So this would be like latent cognitive space that's shared among all the all the modalities.",
                    "label": 0
                },
                {
                    "sent": "And then we can reconcile reconstruct every single view from this shared bottleneck.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How can this help us do what we want to do?",
                    "label": 0
                },
                {
                    "sent": "So once the model is trained, you can take this stimulus, any stimulus you can get the feature of the symbols and you can actually predict brain activity by at first.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimating the shared space and then predicting the data for the individual subjects.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also take the data from one subject, estimate this shared space an.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then predict the features of this demo.",
                    "label": 0
                },
                {
                    "sent": "So do this decoding.",
                    "label": 0
                },
                {
                    "sent": "You",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also take the activity for one subject and predict the activity for the other subjects.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A tweak that we did to this model is actually this bottleneck layer.",
                    "label": 1
                },
                {
                    "sent": "Sure, it's shared among all these views, but it's not really reasonable to think that word to back is going to contain all of the information that is present in the brain image, right?",
                    "label": 0
                },
                {
                    "sent": "So what we did is we use the whole all of the bottleneck layer for all of the subject data, so it's.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually has access to the output of the subject encoders and it.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also used as input to this subject decoders.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bye.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Forward to back, we actually choose a subset of nodes.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are the only nodes that have access to the output of the word to the encoder.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, the only notes that I used for the word, two VEC decoder and we also do the same.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think for the different feature spaces that we have and we also I mean.",
                    "label": 0
                },
                {
                    "sent": "Also it's unreasonable to assume that we actually found all of the relevant feature spaces, so there's some activity that will be shared among subjects that is not summarized by any other feature spaces.",
                    "label": 0
                },
                {
                    "sent": "So we do leave some notes that are not attached to any of the feature spaces and the way we pick the number of nodes and how many notes assigned to each layer is by cross validation.",
                    "label": 0
                },
                {
                    "sent": "And we do a grid search longer few parameters.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so this is multimodal model representation learning we saw this many times in the school already of for like text to text or text to images.",
                    "label": 0
                },
                {
                    "sent": "It's also this model is similar to the bimodal order split autoencoder.",
                    "label": 0
                },
                {
                    "sent": "We also tried other things like GCC or deep correlated.",
                    "label": 0
                },
                {
                    "sent": "Clearly this is autoencoders.",
                    "label": 0
                },
                {
                    "sent": "So these are not so they're very good for arriving at a representation that is useful for classification, for example.",
                    "label": 0
                },
                {
                    "sent": "But here what we really want to do is we want to do this reconstruction, so we really want this particular architecture that we have that estimates the same shared space and then reconstruct is optimized to reconstruct from that space all the fuse.",
                    "label": 0
                },
                {
                    "sent": "Also, I actually saw this paper just today.",
                    "label": 0
                },
                {
                    "sent": "This is a recent paper.",
                    "label": 0
                },
                {
                    "sent": "Young is is on it and.",
                    "label": 0
                },
                {
                    "sent": "Is is also using some more representation.",
                    "label": 0
                },
                {
                    "sent": "Learning for four brain images, however it's a very different problem.",
                    "label": 0
                },
                {
                    "sent": "This particular paper is.",
                    "label": 0
                },
                {
                    "sent": "So different types of data from the same subject, like anatomical and structural data are used or are used together in a sequence to sequence model to predict behavioral parameters for that subject.",
                    "label": 0
                },
                {
                    "sent": "Like cognitive cognitive skills.",
                    "label": 0
                },
                {
                    "sent": "And here what we're really interested in in this model is actually to model the contents of brain activity, and so this is a very this is a separate task.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here are the results.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to just a subset of the results, and I'm going to see what happens when we try to predict brain activity from the word two VEC features in the story, experiments and the way we measure our performance is by seeing how well we predict held out data.",
                    "label": 0
                },
                {
                    "sent": "And the region we I'm plotting here on the flat map the correlation between held out data and predicted data, and so we can see there's a large part of the of the cortex that is predicted well by word to back.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interestingly, if we look at the phonetic data the phoneme data, there's a small portion of the.",
                    "label": 0
                },
                {
                    "sent": "This is a temporal cortex in this particular portion that is well predicted.",
                    "label": 0
                },
                {
                    "sent": "This actually the primary auditory system primary auditory cortex, which makes sense because these are phonetic features.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we go to the movie data and we use the motion energy features which are very which carry visual information, we can see that the visual or you can see my mouse.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the visual cortex which is in the middle of the middle.",
                    "label": 0
                },
                {
                    "sent": "Over here.",
                    "label": 0
                },
                {
                    "sent": "This region.",
                    "label": 0
                },
                {
                    "sent": "Is is very well predicted by those features, which also makes sense.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we move to higher level features, so where to back features which are semantic and do not carry lower level information?",
                    "label": 0
                },
                {
                    "sent": "You can see that this lower level visual cortex is no longer well predicted.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Predict one subject data from another subject data and you can see here.",
                    "label": 0
                },
                {
                    "sent": "Actually now there's more regions of the brain that are well predicted and that makes sense because there is some information that was not.",
                    "label": 0
                },
                {
                    "sent": "We were not able to predict by just using the features of the symbols because we don't have a complete feature set.",
                    "label": 0
                },
                {
                    "sent": "Yes, those numbers.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a percentage of prediction, so this is fMRI data is pretty noisy, so these are kind of, so there's there's an inherent noise ceiling that you cannot go above it just because if you just repeat the experiment over and over.",
                    "label": 0
                },
                {
                    "sent": "The exact same experiment, you will get very noisier.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, this is the courage.",
                    "label": 0
                },
                {
                    "sent": "OK, and so this is a result for predicting subjects, one from subject from subjects one and you can.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See if it's what I saw it said yesterday.",
                    "label": 0
                },
                {
                    "sent": "The fashions are very similar, however, their deformed in kind of a very nonlinear way.",
                    "label": 0
                },
                {
                    "sent": "OK, so we were able to.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To achieve this unified model to some extent, so we combined experiments, subject an feature spaces and we were able to run a lot of very, very standard neuroimaging tests that are used by a lot of neuroimaging research and actually our performance is very compatible to all of the, like the standard state of the art techniques and actually.",
                    "label": 0
                },
                {
                    "sent": "These techniques are actually just using one of these problem at a time and optimizing that problem, but our method optimizes all these.",
                    "label": 0
                },
                {
                    "sent": "All these tasks together.",
                    "label": 0
                },
                {
                    "sent": "But also what it does is it learns this bottleneck layer right?",
                    "label": 0
                },
                {
                    "sent": "So this is this embedded space for all of this data is shared shared information from all of the subjects and also from the information the stimulus features.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we tried to explore it as well.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I just for example, just stress on the word two VEC nodes, so these represent kind of a latent representation of work packet, lower dimensional representation.",
                    "label": 0
                },
                {
                    "sent": "But this is not just containing word two vec, how great varies or how to best?",
                    "label": 0
                },
                {
                    "sent": "It reduces dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Actually also carries information about how to best predict brain data.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of work to back.",
                    "label": 0
                },
                {
                    "sent": "That is more by brain data and for example we can turn on.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the nodes and we can see like for example, this is a cluster, that is that is predicted or that is closest to the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "If we just use this node and we can even also see which brain regions are also connected to this node and via separate set of experiments we just like these words do tend to cluster in these regions and different subjects, or they do tend to activate these regions in difference in differences.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just the first step of our approach.",
                    "label": 0
                },
                {
                    "sent": "We so we were able to achieve all this without using any spatial information.",
                    "label": 0
                },
                {
                    "sent": "We're just thinking of the brain as a vector, but you can see how special information might be very useful.",
                    "label": 0
                },
                {
                    "sent": "So maybe we will implement like a convolution layer, but only at the like, maybe only at the first source first and second light levels of the encoders, because it's not reasonable to assume that.",
                    "label": 0
                },
                {
                    "sent": "That so patterns and ephemera in an fMRI image are not really invariants in location, for example.",
                    "label": 0
                },
                {
                    "sent": "So we do want them to stay localized, but maybe just move a little bit among subjects.",
                    "label": 0
                },
                {
                    "sent": "We also this is a very slowly varying data and we did not use any temporal information, so we can add maybe some recurrence in our model.",
                    "label": 0
                },
                {
                    "sent": "And also the way we.",
                    "label": 0
                },
                {
                    "sent": "The way we allocated the.",
                    "label": 0
                },
                {
                    "sent": "How many notes to give for each task was done by a grid search, but maybe we might think of a way to how to do that on line when when learning the network.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that was all, thank you.",
                    "label": 0
                }
            ]
        }
    }
}