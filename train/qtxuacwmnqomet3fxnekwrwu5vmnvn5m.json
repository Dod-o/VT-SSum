{
    "id": "qtxuacwmnqomet3fxnekwrwu5vmnvn5m",
    "title": "An Analysis of Linear Models, Linear Value-Function Approximation, and Feature Selection for Reinforcement Learning",
    "info": {
        "author": [
            "Ronald Parr, Department of Computer Science, Duke University"
        ],
        "published": "Aug. 12, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_parr_alm/",
    "segmentation": [
        [
            "OK, so this is joint work with my color coded coauthors, Lihong Li, Gavin Taylor, Christopher, Painter, Wakefield and."
        ],
        [
            "Michael wittmann.",
            "So here's a brief walk through our paper.",
            "The story starts off as it does for so many of us with a set of features and some samples, and we're going to do something slightly unorthodox with those features in samples, at least in this context, which is that we are going to use them to construct an approximate linear model and the way we do that is we're going to project the dynamics into the future space, so we're going to construct AK by K feature to feature transition matrix in AK by one approximate reward function.",
            "Then we're going to do is we're going to solve for this approximate model, and we're going to get the exact value function for this approximate model, and we can show this exact value function actually has to lie in the span of our basis.",
            "Now the interesting thing is that we've taken this one path starting with their samples to a linear combination of our features, which is our value function, and it turns out that if we take an alternate path, an renelle, STD, or linear TD on the same set of samples, we would have gotten exactly the same solution.",
            "So."
        ],
        [
            "That's the first.",
            "The first part of our paper, and then we're going to do is we're going to use the insight that we gain from this to derive a novel expression for the Bellman error in particular, we're going to express the Bellman error in terms of two other errors.",
            "One of them is what we call the reward error, which is the error in or the residual in the error in our approximate model.",
            "So the reward in our approximate model and the other is what we call the per feature error and.",
            "This is the residual in the model approximation for the approximate model.",
            "So the interesting thing about this representation is that it's going to let us talk about the Bellman error of the fixed point solution without actually computing the fixed point solution.",
            "So we can talk about how Delta R and Delta Phi influence the Bellman error without actually solving for W. So this gives us a number of insights into feature selection and how to how to choose features, how to talk about features again before we've actually even solve for the value function, which is always sort of a sore point for reinforcement learning, right?",
            "We're very often.",
            "It seems like we have to try something and see how it works before we really have any insight into what's going on.",
            "In particular, will have a number of theorems about about that follow from this representation of the Bellman error, and then we'll also introduce some experiments where we take a bunch of existing feature selection algorithms.",
            "So these different colored lines here, and we measure how they perform as we add features.",
            "Looking at both the Bellman error, the reward error and the feature error, and this gives us some insight into what's going on with these feet."
        ],
        [
            "Selection algorithms.",
            "OK, so here's the outline.",
            "Since it is the first reinforcement learning talk of the morning, I guess I will have to review the terminology.",
            "We'll just see."
        ],
        [
            "Do exactly what I showed you, so basic terminology we're talking about Markov reward process is so in the paper we talk a little bit about decisions, but mostly this is about evaluating a fixed policy.",
            "So we have states S1 through SNA real valued reward, a transition matrix PA discount less than one, and a value function which will represent with V. The value function of the true value function V star is the solution to a system."
        ],
        [
            "Linear equations.",
            "So for linear value function approximation, which is essentially the focus of this talk, we're going to be doing this and we have a very large state space and we're going to be picking some linearly independent set of features, fee one through K, and we're going to represent those with the feature matrix, capital fee, or every column of that matrix corresponds to a feature on the objective here is to find some set of weights W, or we can express an approximate value function V hat as a linear combination of the."
        ],
        [
            "Teachers so more terminology.",
            "We're going to talk about the bellman operator, and we're going to use capital T as shorthand for the bellman operator.",
            "And of course, this is used in the familiar value iteration algorithm and defines the fixed point.",
            "We'll also talk about the Bellman error.",
            "So the bellman error is the difference between the current value function and the result of applying the bellman operator.",
            "And the reason why we care about this is that the Bellman error can bound the actual error.",
            "That is the distance from our current value function."
        ],
        [
            "2V star.",
            "So.",
            "When I talk about the solution found by LST or linear TD, I'm talking about the linear fixed point solution.",
            "So let's just take a moment to introduce some technology, some terminology, and explain what I mean by that.",
            "So I'm going to use the operator Capital Pi sub fee to the operator that takes a value function and returns the set of weights that result when we project that value function into the span of feet.",
            "OK, so the interesting or important thing here just to keep track of is that the result of \u03c0 sub fee is actually a set of weights, not a value function.",
            "Now we define the linear fixed point as follows.",
            "If we start off with some value function in the span of fee, which will be some some W times fee, we apply the bellman operator.",
            "That's the red arrow.",
            "So we apply the Bellman operator which may pop us out of the span of fee.",
            "We then apply the projection operator to go back down into the span of thi.",
            "And we say that we're at the fixed point.",
            "If we have the same W at the end of this procedure as we had when we started an lots of familiar algorithms.",
            "For example, linear, TDL, STD will compute this linear fixed point as their solution."
        ],
        [
            "OK, so the first step is to show this equivalence between."
        ],
        [
            "Linear model in the linear fixed point.",
            "So to do that I have to define what I mean by the linear model.",
            "So again, we're going to pick some set of linearly independent features and the first step will be to construct an approximate reward function and the way we're going to do that is we're going to try to find some fee times our fee which to our fee is going to be the weights of our approximate reward function.",
            "So we want to find a fee times RFP, which is approximately equal to the true reward function, and we're going to do that by projecting the reward function into the span.",
            "Of our basis, so it's straightforward linear regression, nothing fancy there.",
            "The slightly more complicated thing is we want to construct an approximate model, so we want to approximate model to.",
            "Do we want our approximate model to take as input a set of features an give us the expected value of those features after one step.",
            "So how do we do that?",
            "Well, we take our transition matrix P * C R feature matrix and this becomes the target for regression problem.",
            "So we're going to take our.",
            "Production operator and multiply it by P times fee, and that's going to give us our approximate model which will refer to as P sub fee, and that's a K by K matrix which goes from few."
        ],
        [
            "There's 2 features.",
            "So now let's talk about the value function of the linear model.",
            "So what we can show is that for this type of a model, the value function is always going to be in the span of fee.",
            "So what that means is we can always express the value function as a linear combination of the columns of fee, and in fact, if the value function is bounded then we can solve for those weights directly.",
            "And the interesting thing to observe here is that this expression for the weights.",
            "For the approximate value function parallels the expression for V star, right?",
            "So were for the regular value function.",
            "We have an end by one vector.",
            "For V star we have a K by one vector for W, so the weights are playing the role of the value function.",
            "The approximate model the K by K approximate model plays the role of the true true transition model and the K by one approximate reward function plays the role of the true reward function.",
            "So everything sort of parallels what we do.",
            "If we had solved the problem exactly once."
        ],
        [
            "Establish this approximate model.",
            "So it's a theorem that for a set of features, the linear models exact value function and the linear fixed point solution are identical, and I'm not going to prove it in during the talk, but it's actually a fairly straightforward thing.",
            "Once you define the approximate model in the right way, it pretty much just falls out directly from the fixed point equation.",
            "And I'll mention that some preliminary observations along these lines were made by boy and in 99.",
            "In fact, you know, lots of people have looked at LST and said oh, that thing kind of looks like an approximate model.",
            "So what we show in the paper is exactly what approximate model?"
        ],
        [
            "It is.",
            "OK, so this is just a reminder of what I just showed you, so we take the data, construct a linear model, construct the exact solution to that model and we show this exactly the same thing you get as if you did LSD or linear TD with."
        ],
        [
            "Same set of features and samples.",
            "OK, let's talk about now.",
            "The bellman error and how it relates to."
        ],
        [
            "So the model error.",
            "So first have to define the model error so the model error is just the error of stills, two parts Delta R which is the error in the reward.",
            "So it's R minus the approximate R and Delta Phi which is the error in the transition model.",
            "So is the error in our ability to predict the next features.",
            "So it's our is the true expected next feature values minus the approximate next feature values that we get for our model.",
            "So this is going to be an end by K matrix.",
            "Because for every state we're going to have an error in each feature."
        ],
        [
            "OK so I have the following theorem in the paper which again breaks up the bellman error of the linear fixed point solution into these components.",
            "For one, is the reward error and the other is what we call the per feature error.",
            "So it's the error in our prediction of next feature values.",
            "And again why this is important is because it lets us talk about the error in the fixed point in terms of the error in the."
        ],
        [
            "In your model.",
            "OK, So what insights does this give us about feature selection?",
            "So there are a few interesting things that pop out of this."
        ],
        [
            "Yes.",
            "OK, so the first comment is that the features really need to model the reward.",
            "So if your features are not expressive enough to capture the reward function itself, then your residual Delta R can appear directly in your expression for the Bellman error.",
            "So in fact the reward itself is a very useful feature, right?",
            "'cause if you have the reward as a feature, you're going to zero out the Delta R component in."
        ],
        [
            "Bellman error.",
            "OK, the Delta Phi component, it turns out, is actually much more interesting, so let's ask ourselves, how could we achieve Delta Phi equals zero?",
            "Clearly that would be useful thing because it would knockout this component of the Bellman error.",
            "So the idea is that we would like the features in some sense to be able to predict themselves.",
            "So given a set of features, we'd like to be able to construct a linear predictor of the next feature values.",
            "So if we can achieve this, I'll say in the next slide how we can achieve it.",
            "But if we can achieve this a few interesting things happen.",
            "So one is at the bellman error.",
            "So if Delta Phi is 0, the bellman error depends entirely on Delta R. So in fact what this does is it reduces the feature selection problem into a regression problem on our itself.",
            "So if we pick a sort of if we have sort of a Dictionary of features that give us Delta Phi equals zero, then we just have a regression problem to solve.",
            "Other interesting thing is that if in fact we have Delta Phi equals zero, we can show that the linear fixed point solution takes on the following form is what we get if we use the exact transition model and the approximate."
        ],
        [
            "Reward.",
            "OK, so how do we get Delta Phi equals 0?",
            "Because that you may say oh, that's kind of interesting, but I don't really understand how we're ever going to be able to achieve that in the real world.",
            "So.",
            "It turns out that we can save some some sufficient conditions for having Delta Phi equal to 0, so one of course is if the rank of fee is the number of states, but that's sort of not that interesting, right?",
            "Because we don't really.",
            "We don't want to deal with things that large.",
            "That's why we're doing value function approximation in the 1st place.",
            "But another interesting condition we can specify is if Phi is composed of eigenvectors of the transition matrix now, not all of the eigenvectors, but in fact any subset of the eigenvectors of the transition matrix are sufficient.",
            "To give you Delta Phi equals zero.",
            "More generally.",
            "OK, we can say that we can achieve Delta Phi equals zero anytime.",
            "Our features span an invariant subspace of peak.",
            "OK, so this is a sort of a technical technical condition that lets us get Delta Phi equals 0.",
            "And there are lots of ways you can get invariant subspaces of P."
        ],
        [
            "OK, so this also gives us some insight into some of the incremental methods that people have proposed for adding features right so so far we've sort of talked about.",
            "You know, if you get a Delta or an adult, a fee that magically have these properties, what can you say about them?",
            "We can also say things about how to incrementally add features, and I'll mention three methods that people have proposed for incrementally building up a set of features.",
            "So one is one that we analyzed last year.",
            "We called it Bellman error basis functions, and this is.",
            "We did not actually invent.",
            "It was invented by other people, and the idea is that you want to take the bellman error of your current solution, add it as a feature.",
            "Now.",
            "Intuitively this is something you should do at least as well as value iteration.",
            "Another method which is inspired by the analysis we just introduced is you could add the model errors as features, so you could take your current residuals, add Delta R as a feature, add the columns of Delta Phi S features and you could take that as your next feature set.",
            "See how those do an repeat.",
            "Also another method that people have talked about recently is the Krylov basis, Merrick Patrick mentioned this recently in.",
            "Emphasizes relevance for MDP's and the idea here is that you're going to add P to the K * R for increasing powers of K as your basis functions.",
            "So in the paper we show the following theorem, which is that the Bellman error basis function approach the model error basis function approach and the Krylov basis approach are all equivalent if they are initialized with the same feature set.",
            "So if they're initialized with the empty set of features, they all give you exactly the same set of features.",
            "I want to give a special note of thanks to Merrick Patrick here, because Merrick actually pointed out to us the equivalence between the Krylov basis and Bellman airbases."
        ],
        [
            "Options.",
            "OK, this also gives us some insight into the method of proto value functions, so proto value functions are a technique that was recently introduced by Mohan Maggioni an the idea there is that they're going to start off with an adjacency graph, and they're going to take the eigenvectors of a modified adjacency graph, which will be the sum version of the graph Laplacian, and what we can do is we can interpret the adjacency graph as an approximate transition matrix and therefore we can interpret the proto value.",
            "Functions as approximating the eigenvectors of P. So, given the terminology we've introduced so far in the talk, we can say that Proto value functions can be viewed as a method of approximating a set of subspace invariant features.",
            "Now, this claim, this sort of this notion that proto value functions are approximately subspace invariant, is something that we can test empirically, and sometimes it's sort of true, and sometimes it."
        ],
        [
            "Not.",
            "OK, so finally I'll move on to."
        ],
        [
            "Experimental results so for experimental results we consider four algorithms.",
            "So the first algorithm is the proto value function algorithm, where these eigenvectors of the Laplacian are enumerated in order of smoothness, and what that means is in order of increasing eigenvalue, another method is one that we call PV FMP and the idea there is that we're going to use matching pursuits on the bellman error with a Dictionary of proto value functions.",
            "So we're going to take.",
            "All of the all the potential proto value functions, all the eigenvectors of the Laplacian.",
            "Throw them in a bag.",
            "An will pull them out in order of maximum dot product, with the Bellman residual.",
            "A third method we call, Igmp INP, is very similar to PDF MP, except we're going to use eigenvectors of the transition matrix as our Dictionary of features.",
            "Finally will consider it Bellman error basis functions or which we now can say are equivalent to model error basis functions on the Krylov basis.",
            "And what we're going to do is we're going to look at the three parts of this equation.",
            "We're going to look for each one of these as we add features, we're going to look at the Bellman error.",
            "The reward error in the future, and see how they change as we add features.",
            "Now we consider three different problems in the talk, I'm just going to focus on this 50 state chain problem, but on the on the poster in the paper there are two other problems that we can talk about."
        ],
        [
            "OK, so here's a 50 state chain which is probably familiar to some of you.",
            "It's just it's a chain of states, one connected to the other with rewards at these middle states.",
            "So the value function looks sort of like a suspension bridge.",
            "Again, what we have here is total Bellman error reward error, an feature error as we add basis functions in accordance with the different algorithms.",
            "So a few things to point out here.",
            "I'm going to start off with the Bellman error basis function method.",
            "That's the green line here.",
            "An we start off with the empty set of features.",
            "And what that means is that the first feature we add is the reward.",
            "So what that means is that the reward error Delta R is going to be 0 all the time for Bellman error basis functions.",
            "So what that means is that the feature error is going to be equal to the total Bellman error, so the green line here and the green line here are going to be identical, and we see that BBF performs quite well on this problem now.",
            "The other extreme I'll look at next is the igmp method.",
            "The Igmp method remember is using eigenvectors of the transition matrix as features.",
            "And this is the other extreme.",
            "OK, because we know that these are going to be informing invariant subspace of P. This is going to be the purple line and what we see here is the purple line is hugging the X axis for feature errors, so feature error is always zero for the igmp method and the reward error is going to be equal to the total Bellman error for the igmp method.",
            "And the interesting thing to see here is that even though that greatly simplifies the expression for the bellman error, it turns out that the reward cannot be easily expressed as a small number as a linear combination of a small number.",
            "Of eigenvectors of the transition matrix, so the actual performance with IMP here is not very good.",
            "Finally, we'll talk about the two methods related to Proto value functions, so I'll mention PDF MP first, 'cause it's sort of similar to igmp, so remember we had this conjecture that Proto value functions could be viewed as approximating the eigenvectors of the transition matrix, which means that the feature error should be low for proto value functions and we see it more or less is low, so that's that's blue line here and it stays relatively low, so we can view it as approximating the eigen.",
            "The subspace invariant features.",
            "But but so it gets some of the good properties of the igmp method, but it also doesn't perform all that well, because again, it's hard to express the reward as a linear combination of a small number of features, so the proto value function methods are here.",
            "This is part of value functions enumerated in order of increasing eigenvalue.",
            "That's the blue line, the red line is when we do matching pursuits with the proto value function dictionary we do a little bit better there, and that's because we're actually actively trying to reduce the error.",
            "And paying attention to the bellman error, in contrast to the blue line where we're just enumerating them in order of increasing eigenvalue and essentially ignoring how how they perform, just putting them in some predetermined."
        ],
        [
            "Order OK, so a few conclusions from the experiments.",
            "So the eigen P method will always have Delta Phi equals 0 because it's always going to form an invariant subspace of the transition matrix.",
            "We can view Proto value functions sometimes as approximating subspace invariant features and I'll mention just tangentially this is potentially useful because actually computing the eigenvectors of the transition matrix is an expensive thing.",
            "PDF MP's doing matching pursuits with proto value functions will dominate vanilla PDF 'cause it's paying attention to.",
            "The reward is actually adapting to the performance of the system.",
            "Bellman error basis functions will always have Delta R = 0 and in practice we see that they have a more steady and predictable reduction of development error than some of the other methods we've introduced.",
            "And one bottom line conclusion is don't ignore the reward when constructing."
        ],
        [
            "Features OK, so just the the talk in pictures again.",
            "There's the equivalence between the different different views.",
            "The model based on the direct value function based there is the expression for the bellman error in terms of the model error that gives us the insights into feature selection and then we have the experiments that show how different feature selection algorithms perform given these."
        ],
        [
            "Criteria.",
            "Thank you.",
            "Cross.",
            "How is there?",
            "Is there decision part much harder?",
            "It seems a lot harder than by little, yeah.",
            "So what about MVP's?",
            "So yeah, it is considerably.",
            "Pardon, no, I didn't pay for that question, so it is considerably harder.",
            "So so in the paper we talk about how to think about what LSP is doing in this context.",
            "And basically we can view LSP is constructing a new linear model at every iteration when the policy changes.",
            "Ann, it's so we can apply this analysis to sort of 1.",
            "One policy of LSP, but actually getting it to carry over traditional policies is difficult.",
            "Ron, thanks for the stock is very, very interesting and.",
            "As as often you and I are thinking alike and Alberta we're looking at very similar things with the linear models and.",
            "And then going inside the direction and and so that we have a UI paper like couple of days.",
            "But we're looking at.",
            "Control context and as well as the prediction and so I just want to mention that and.",
            "And then we go into things like fireplace sweeping and.",
            "But maybe this is some kind of question.",
            "The bellman error.",
            "Earlier maybe is maybe not the right thing to focus on.",
            "I mean you.",
            "You are.",
            "But please all my questions.",
            "Anyone hearing comments on which is when?",
            "I actually really like your comment on the.",
            "The the way in which these these means of extracting features are not very constructive.",
            "We sort of require us to know what Bellman areas.",
            "However the eigenvectors are, so it seems like that sort of.",
            "Is that the question?",
            "OK, so so.",
            "Actually yes, I tend to agree with that so.",
            "Of all the methods I talked about, I'd say the most in my opinion, the most practical is the Bellman error approach in our ice email paper last year.",
            "We talked about how to construct a. Lastly, before I said that I should mention again, we're not the first to do that either.",
            "So for example.",
            "The Keller Italia paper.",
            "Introduces in approximate Bellman error as a feature and in our ICL paper last year we talked about how to deal with an approximate Bellman error as a feature.",
            "So I think that's a much more reasonable thing to do because you can generate an approximate Bellman error just by looking at the TV signal 'cause the TV signal is going to be a sample of the approximate velmanette of sample of the Bellman error now.",
            "As far as the other methods, which for example would require you to get the eigenvectors of the transition matrix, I think that's an extremely impractical approach because if you have the transition matrix, you might as well just solve for the value function.",
            "Now.",
            "One thing that if I'll put my feet or head on for a moment, one thing that retard would point out is that computing the eigenvectors of the Laplacian in many ways could be easier than computing the eigenvectors of the transition matrix.",
            "OK, one because the last thing is going to be her mission, so it's going to be.",
            "Stable operation, the other is that the adjacency matrix in some sense is going to have.",
            "It's just binary, right?",
            "So it's going to have sort of lower sample complexity to actually construct that, then the transition matrix.",
            "But still I think it's an awful lot of work to do that, so I do have some concerns about that.",
            "We're running out of time, so let's end the speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is joint work with my color coded coauthors, Lihong Li, Gavin Taylor, Christopher, Painter, Wakefield and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Michael wittmann.",
                    "label": 0
                },
                {
                    "sent": "So here's a brief walk through our paper.",
                    "label": 1
                },
                {
                    "sent": "The story starts off as it does for so many of us with a set of features and some samples, and we're going to do something slightly unorthodox with those features in samples, at least in this context, which is that we are going to use them to construct an approximate linear model and the way we do that is we're going to project the dynamics into the future space, so we're going to construct AK by K feature to feature transition matrix in AK by one approximate reward function.",
                    "label": 1
                },
                {
                    "sent": "Then we're going to do is we're going to solve for this approximate model, and we're going to get the exact value function for this approximate model, and we can show this exact value function actually has to lie in the span of our basis.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting thing is that we've taken this one path starting with their samples to a linear combination of our features, which is our value function, and it turns out that if we take an alternate path, an renelle, STD, or linear TD on the same set of samples, we would have gotten exactly the same solution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's the first.",
                    "label": 0
                },
                {
                    "sent": "The first part of our paper, and then we're going to do is we're going to use the insight that we gain from this to derive a novel expression for the Bellman error in particular, we're going to express the Bellman error in terms of two other errors.",
                    "label": 1
                },
                {
                    "sent": "One of them is what we call the reward error, which is the error in or the residual in the error in our approximate model.",
                    "label": 0
                },
                {
                    "sent": "So the reward in our approximate model and the other is what we call the per feature error and.",
                    "label": 0
                },
                {
                    "sent": "This is the residual in the model approximation for the approximate model.",
                    "label": 0
                },
                {
                    "sent": "So the interesting thing about this representation is that it's going to let us talk about the Bellman error of the fixed point solution without actually computing the fixed point solution.",
                    "label": 1
                },
                {
                    "sent": "So we can talk about how Delta R and Delta Phi influence the Bellman error without actually solving for W. So this gives us a number of insights into feature selection and how to how to choose features, how to talk about features again before we've actually even solve for the value function, which is always sort of a sore point for reinforcement learning, right?",
                    "label": 0
                },
                {
                    "sent": "We're very often.",
                    "label": 0
                },
                {
                    "sent": "It seems like we have to try something and see how it works before we really have any insight into what's going on.",
                    "label": 0
                },
                {
                    "sent": "In particular, will have a number of theorems about about that follow from this representation of the Bellman error, and then we'll also introduce some experiments where we take a bunch of existing feature selection algorithms.",
                    "label": 1
                },
                {
                    "sent": "So these different colored lines here, and we measure how they perform as we add features.",
                    "label": 0
                },
                {
                    "sent": "Looking at both the Bellman error, the reward error and the feature error, and this gives us some insight into what's going on with these feet.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Selection algorithms.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the outline.",
                    "label": 0
                },
                {
                    "sent": "Since it is the first reinforcement learning talk of the morning, I guess I will have to review the terminology.",
                    "label": 0
                },
                {
                    "sent": "We'll just see.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do exactly what I showed you, so basic terminology we're talking about Markov reward process is so in the paper we talk a little bit about decisions, but mostly this is about evaluating a fixed policy.",
                    "label": 0
                },
                {
                    "sent": "So we have states S1 through SNA real valued reward, a transition matrix PA discount less than one, and a value function which will represent with V. The value function of the true value function V star is the solution to a system.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Linear equations.",
                    "label": 0
                },
                {
                    "sent": "So for linear value function approximation, which is essentially the focus of this talk, we're going to be doing this and we have a very large state space and we're going to be picking some linearly independent set of features, fee one through K, and we're going to represent those with the feature matrix, capital fee, or every column of that matrix corresponds to a feature on the objective here is to find some set of weights W, or we can express an approximate value function V hat as a linear combination of the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Teachers so more terminology.",
                    "label": 0
                },
                {
                    "sent": "We're going to talk about the bellman operator, and we're going to use capital T as shorthand for the bellman operator.",
                    "label": 0
                },
                {
                    "sent": "And of course, this is used in the familiar value iteration algorithm and defines the fixed point.",
                    "label": 1
                },
                {
                    "sent": "We'll also talk about the Bellman error.",
                    "label": 0
                },
                {
                    "sent": "So the bellman error is the difference between the current value function and the result of applying the bellman operator.",
                    "label": 1
                },
                {
                    "sent": "And the reason why we care about this is that the Bellman error can bound the actual error.",
                    "label": 0
                },
                {
                    "sent": "That is the distance from our current value function.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "2V star.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "When I talk about the solution found by LST or linear TD, I'm talking about the linear fixed point solution.",
                    "label": 1
                },
                {
                    "sent": "So let's just take a moment to introduce some technology, some terminology, and explain what I mean by that.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to use the operator Capital Pi sub fee to the operator that takes a value function and returns the set of weights that result when we project that value function into the span of feet.",
                    "label": 0
                },
                {
                    "sent": "OK, so the interesting or important thing here just to keep track of is that the result of \u03c0 sub fee is actually a set of weights, not a value function.",
                    "label": 1
                },
                {
                    "sent": "Now we define the linear fixed point as follows.",
                    "label": 0
                },
                {
                    "sent": "If we start off with some value function in the span of fee, which will be some some W times fee, we apply the bellman operator.",
                    "label": 0
                },
                {
                    "sent": "That's the red arrow.",
                    "label": 0
                },
                {
                    "sent": "So we apply the Bellman operator which may pop us out of the span of fee.",
                    "label": 0
                },
                {
                    "sent": "We then apply the projection operator to go back down into the span of thi.",
                    "label": 0
                },
                {
                    "sent": "And we say that we're at the fixed point.",
                    "label": 0
                },
                {
                    "sent": "If we have the same W at the end of this procedure as we had when we started an lots of familiar algorithms.",
                    "label": 0
                },
                {
                    "sent": "For example, linear, TDL, STD will compute this linear fixed point as their solution.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the first step is to show this equivalence between.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Linear model in the linear fixed point.",
                    "label": 1
                },
                {
                    "sent": "So to do that I have to define what I mean by the linear model.",
                    "label": 0
                },
                {
                    "sent": "So again, we're going to pick some set of linearly independent features and the first step will be to construct an approximate reward function and the way we're going to do that is we're going to try to find some fee times our fee which to our fee is going to be the weights of our approximate reward function.",
                    "label": 1
                },
                {
                    "sent": "So we want to find a fee times RFP, which is approximately equal to the true reward function, and we're going to do that by projecting the reward function into the span.",
                    "label": 0
                },
                {
                    "sent": "Of our basis, so it's straightforward linear regression, nothing fancy there.",
                    "label": 0
                },
                {
                    "sent": "The slightly more complicated thing is we want to construct an approximate model, so we want to approximate model to.",
                    "label": 0
                },
                {
                    "sent": "Do we want our approximate model to take as input a set of features an give us the expected value of those features after one step.",
                    "label": 0
                },
                {
                    "sent": "So how do we do that?",
                    "label": 0
                },
                {
                    "sent": "Well, we take our transition matrix P * C R feature matrix and this becomes the target for regression problem.",
                    "label": 0
                },
                {
                    "sent": "So we're going to take our.",
                    "label": 0
                },
                {
                    "sent": "Production operator and multiply it by P times fee, and that's going to give us our approximate model which will refer to as P sub fee, and that's a K by K matrix which goes from few.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's 2 features.",
                    "label": 0
                },
                {
                    "sent": "So now let's talk about the value function of the linear model.",
                    "label": 1
                },
                {
                    "sent": "So what we can show is that for this type of a model, the value function is always going to be in the span of fee.",
                    "label": 0
                },
                {
                    "sent": "So what that means is we can always express the value function as a linear combination of the columns of fee, and in fact, if the value function is bounded then we can solve for those weights directly.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing to observe here is that this expression for the weights.",
                    "label": 0
                },
                {
                    "sent": "For the approximate value function parallels the expression for V star, right?",
                    "label": 1
                },
                {
                    "sent": "So were for the regular value function.",
                    "label": 0
                },
                {
                    "sent": "We have an end by one vector.",
                    "label": 0
                },
                {
                    "sent": "For V star we have a K by one vector for W, so the weights are playing the role of the value function.",
                    "label": 0
                },
                {
                    "sent": "The approximate model the K by K approximate model plays the role of the true true transition model and the K by one approximate reward function plays the role of the true reward function.",
                    "label": 0
                },
                {
                    "sent": "So everything sort of parallels what we do.",
                    "label": 0
                },
                {
                    "sent": "If we had solved the problem exactly once.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Establish this approximate model.",
                    "label": 0
                },
                {
                    "sent": "So it's a theorem that for a set of features, the linear models exact value function and the linear fixed point solution are identical, and I'm not going to prove it in during the talk, but it's actually a fairly straightforward thing.",
                    "label": 1
                },
                {
                    "sent": "Once you define the approximate model in the right way, it pretty much just falls out directly from the fixed point equation.",
                    "label": 1
                },
                {
                    "sent": "And I'll mention that some preliminary observations along these lines were made by boy and in 99.",
                    "label": 0
                },
                {
                    "sent": "In fact, you know, lots of people have looked at LST and said oh, that thing kind of looks like an approximate model.",
                    "label": 0
                },
                {
                    "sent": "So what we show in the paper is exactly what approximate model?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just a reminder of what I just showed you, so we take the data, construct a linear model, construct the exact solution to that model and we show this exactly the same thing you get as if you did LSD or linear TD with.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same set of features and samples.",
                    "label": 0
                },
                {
                    "sent": "OK, let's talk about now.",
                    "label": 0
                },
                {
                    "sent": "The bellman error and how it relates to.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the model error.",
                    "label": 0
                },
                {
                    "sent": "So first have to define the model error so the model error is just the error of stills, two parts Delta R which is the error in the reward.",
                    "label": 0
                },
                {
                    "sent": "So it's R minus the approximate R and Delta Phi which is the error in the transition model.",
                    "label": 0
                },
                {
                    "sent": "So is the error in our ability to predict the next features.",
                    "label": 1
                },
                {
                    "sent": "So it's our is the true expected next feature values minus the approximate next feature values that we get for our model.",
                    "label": 1
                },
                {
                    "sent": "So this is going to be an end by K matrix.",
                    "label": 0
                },
                {
                    "sent": "Because for every state we're going to have an error in each feature.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I have the following theorem in the paper which again breaks up the bellman error of the linear fixed point solution into these components.",
                    "label": 1
                },
                {
                    "sent": "For one, is the reward error and the other is what we call the per feature error.",
                    "label": 0
                },
                {
                    "sent": "So it's the error in our prediction of next feature values.",
                    "label": 0
                },
                {
                    "sent": "And again why this is important is because it lets us talk about the error in the fixed point in terms of the error in the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In your model.",
                    "label": 0
                },
                {
                    "sent": "OK, So what insights does this give us about feature selection?",
                    "label": 1
                },
                {
                    "sent": "So there are a few interesting things that pop out of this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first comment is that the features really need to model the reward.",
                    "label": 0
                },
                {
                    "sent": "So if your features are not expressive enough to capture the reward function itself, then your residual Delta R can appear directly in your expression for the Bellman error.",
                    "label": 0
                },
                {
                    "sent": "So in fact the reward itself is a very useful feature, right?",
                    "label": 1
                },
                {
                    "sent": "'cause if you have the reward as a feature, you're going to zero out the Delta R component in.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bellman error.",
                    "label": 0
                },
                {
                    "sent": "OK, the Delta Phi component, it turns out, is actually much more interesting, so let's ask ourselves, how could we achieve Delta Phi equals zero?",
                    "label": 0
                },
                {
                    "sent": "Clearly that would be useful thing because it would knockout this component of the Bellman error.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we would like the features in some sense to be able to predict themselves.",
                    "label": 0
                },
                {
                    "sent": "So given a set of features, we'd like to be able to construct a linear predictor of the next feature values.",
                    "label": 0
                },
                {
                    "sent": "So if we can achieve this, I'll say in the next slide how we can achieve it.",
                    "label": 0
                },
                {
                    "sent": "But if we can achieve this a few interesting things happen.",
                    "label": 0
                },
                {
                    "sent": "So one is at the bellman error.",
                    "label": 0
                },
                {
                    "sent": "So if Delta Phi is 0, the bellman error depends entirely on Delta R. So in fact what this does is it reduces the feature selection problem into a regression problem on our itself.",
                    "label": 0
                },
                {
                    "sent": "So if we pick a sort of if we have sort of a Dictionary of features that give us Delta Phi equals zero, then we just have a regression problem to solve.",
                    "label": 0
                },
                {
                    "sent": "Other interesting thing is that if in fact we have Delta Phi equals zero, we can show that the linear fixed point solution takes on the following form is what we get if we use the exact transition model and the approximate.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reward.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we get Delta Phi equals 0?",
                    "label": 0
                },
                {
                    "sent": "Because that you may say oh, that's kind of interesting, but I don't really understand how we're ever going to be able to achieve that in the real world.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It turns out that we can save some some sufficient conditions for having Delta Phi equal to 0, so one of course is if the rank of fee is the number of states, but that's sort of not that interesting, right?",
                    "label": 0
                },
                {
                    "sent": "Because we don't really.",
                    "label": 0
                },
                {
                    "sent": "We don't want to deal with things that large.",
                    "label": 0
                },
                {
                    "sent": "That's why we're doing value function approximation in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "But another interesting condition we can specify is if Phi is composed of eigenvectors of the transition matrix now, not all of the eigenvectors, but in fact any subset of the eigenvectors of the transition matrix are sufficient.",
                    "label": 1
                },
                {
                    "sent": "To give you Delta Phi equals zero.",
                    "label": 0
                },
                {
                    "sent": "More generally.",
                    "label": 0
                },
                {
                    "sent": "OK, we can say that we can achieve Delta Phi equals zero anytime.",
                    "label": 0
                },
                {
                    "sent": "Our features span an invariant subspace of peak.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is a sort of a technical technical condition that lets us get Delta Phi equals 0.",
                    "label": 0
                },
                {
                    "sent": "And there are lots of ways you can get invariant subspaces of P.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this also gives us some insight into some of the incremental methods that people have proposed for adding features right so so far we've sort of talked about.",
                    "label": 0
                },
                {
                    "sent": "You know, if you get a Delta or an adult, a fee that magically have these properties, what can you say about them?",
                    "label": 0
                },
                {
                    "sent": "We can also say things about how to incrementally add features, and I'll mention three methods that people have proposed for incrementally building up a set of features.",
                    "label": 0
                },
                {
                    "sent": "So one is one that we analyzed last year.",
                    "label": 0
                },
                {
                    "sent": "We called it Bellman error basis functions, and this is.",
                    "label": 0
                },
                {
                    "sent": "We did not actually invent.",
                    "label": 0
                },
                {
                    "sent": "It was invented by other people, and the idea is that you want to take the bellman error of your current solution, add it as a feature.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Intuitively this is something you should do at least as well as value iteration.",
                    "label": 0
                },
                {
                    "sent": "Another method which is inspired by the analysis we just introduced is you could add the model errors as features, so you could take your current residuals, add Delta R as a feature, add the columns of Delta Phi S features and you could take that as your next feature set.",
                    "label": 0
                },
                {
                    "sent": "See how those do an repeat.",
                    "label": 0
                },
                {
                    "sent": "Also another method that people have talked about recently is the Krylov basis, Merrick Patrick mentioned this recently in.",
                    "label": 0
                },
                {
                    "sent": "Emphasizes relevance for MDP's and the idea here is that you're going to add P to the K * R for increasing powers of K as your basis functions.",
                    "label": 0
                },
                {
                    "sent": "So in the paper we show the following theorem, which is that the Bellman error basis function approach the model error basis function approach and the Krylov basis approach are all equivalent if they are initialized with the same feature set.",
                    "label": 1
                },
                {
                    "sent": "So if they're initialized with the empty set of features, they all give you exactly the same set of features.",
                    "label": 0
                },
                {
                    "sent": "I want to give a special note of thanks to Merrick Patrick here, because Merrick actually pointed out to us the equivalence between the Krylov basis and Bellman airbases.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Options.",
                    "label": 0
                },
                {
                    "sent": "OK, this also gives us some insight into the method of proto value functions, so proto value functions are a technique that was recently introduced by Mohan Maggioni an the idea there is that they're going to start off with an adjacency graph, and they're going to take the eigenvectors of a modified adjacency graph, which will be the sum version of the graph Laplacian, and what we can do is we can interpret the adjacency graph as an approximate transition matrix and therefore we can interpret the proto value.",
                    "label": 1
                },
                {
                    "sent": "Functions as approximating the eigenvectors of P. So, given the terminology we've introduced so far in the talk, we can say that Proto value functions can be viewed as a method of approximating a set of subspace invariant features.",
                    "label": 0
                },
                {
                    "sent": "Now, this claim, this sort of this notion that proto value functions are approximately subspace invariant, is something that we can test empirically, and sometimes it's sort of true, and sometimes it.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not.",
                    "label": 0
                },
                {
                    "sent": "OK, so finally I'll move on to.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experimental results so for experimental results we consider four algorithms.",
                    "label": 1
                },
                {
                    "sent": "So the first algorithm is the proto value function algorithm, where these eigenvectors of the Laplacian are enumerated in order of smoothness, and what that means is in order of increasing eigenvalue, another method is one that we call PV FMP and the idea there is that we're going to use matching pursuits on the bellman error with a Dictionary of proto value functions.",
                    "label": 1
                },
                {
                    "sent": "So we're going to take.",
                    "label": 0
                },
                {
                    "sent": "All of the all the potential proto value functions, all the eigenvectors of the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Throw them in a bag.",
                    "label": 0
                },
                {
                    "sent": "An will pull them out in order of maximum dot product, with the Bellman residual.",
                    "label": 1
                },
                {
                    "sent": "A third method we call, Igmp INP, is very similar to PDF MP, except we're going to use eigenvectors of the transition matrix as our Dictionary of features.",
                    "label": 0
                },
                {
                    "sent": "Finally will consider it Bellman error basis functions or which we now can say are equivalent to model error basis functions on the Krylov basis.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do is we're going to look at the three parts of this equation.",
                    "label": 0
                },
                {
                    "sent": "We're going to look for each one of these as we add features, we're going to look at the Bellman error.",
                    "label": 0
                },
                {
                    "sent": "The reward error in the future, and see how they change as we add features.",
                    "label": 0
                },
                {
                    "sent": "Now we consider three different problems in the talk, I'm just going to focus on this 50 state chain problem, but on the on the poster in the paper there are two other problems that we can talk about.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's a 50 state chain which is probably familiar to some of you.",
                    "label": 0
                },
                {
                    "sent": "It's just it's a chain of states, one connected to the other with rewards at these middle states.",
                    "label": 0
                },
                {
                    "sent": "So the value function looks sort of like a suspension bridge.",
                    "label": 0
                },
                {
                    "sent": "Again, what we have here is total Bellman error reward error, an feature error as we add basis functions in accordance with the different algorithms.",
                    "label": 1
                },
                {
                    "sent": "So a few things to point out here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start off with the Bellman error basis function method.",
                    "label": 0
                },
                {
                    "sent": "That's the green line here.",
                    "label": 0
                },
                {
                    "sent": "An we start off with the empty set of features.",
                    "label": 0
                },
                {
                    "sent": "And what that means is that the first feature we add is the reward.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that the reward error Delta R is going to be 0 all the time for Bellman error basis functions.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that the feature error is going to be equal to the total Bellman error, so the green line here and the green line here are going to be identical, and we see that BBF performs quite well on this problem now.",
                    "label": 0
                },
                {
                    "sent": "The other extreme I'll look at next is the igmp method.",
                    "label": 0
                },
                {
                    "sent": "The Igmp method remember is using eigenvectors of the transition matrix as features.",
                    "label": 0
                },
                {
                    "sent": "And this is the other extreme.",
                    "label": 0
                },
                {
                    "sent": "OK, because we know that these are going to be informing invariant subspace of P. This is going to be the purple line and what we see here is the purple line is hugging the X axis for feature errors, so feature error is always zero for the igmp method and the reward error is going to be equal to the total Bellman error for the igmp method.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing to see here is that even though that greatly simplifies the expression for the bellman error, it turns out that the reward cannot be easily expressed as a small number as a linear combination of a small number.",
                    "label": 0
                },
                {
                    "sent": "Of eigenvectors of the transition matrix, so the actual performance with IMP here is not very good.",
                    "label": 0
                },
                {
                    "sent": "Finally, we'll talk about the two methods related to Proto value functions, so I'll mention PDF MP first, 'cause it's sort of similar to igmp, so remember we had this conjecture that Proto value functions could be viewed as approximating the eigenvectors of the transition matrix, which means that the feature error should be low for proto value functions and we see it more or less is low, so that's that's blue line here and it stays relatively low, so we can view it as approximating the eigen.",
                    "label": 0
                },
                {
                    "sent": "The subspace invariant features.",
                    "label": 0
                },
                {
                    "sent": "But but so it gets some of the good properties of the igmp method, but it also doesn't perform all that well, because again, it's hard to express the reward as a linear combination of a small number of features, so the proto value function methods are here.",
                    "label": 0
                },
                {
                    "sent": "This is part of value functions enumerated in order of increasing eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "That's the blue line, the red line is when we do matching pursuits with the proto value function dictionary we do a little bit better there, and that's because we're actually actively trying to reduce the error.",
                    "label": 0
                },
                {
                    "sent": "And paying attention to the bellman error, in contrast to the blue line where we're just enumerating them in order of increasing eigenvalue and essentially ignoring how how they perform, just putting them in some predetermined.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Order OK, so a few conclusions from the experiments.",
                    "label": 1
                },
                {
                    "sent": "So the eigen P method will always have Delta Phi equals 0 because it's always going to form an invariant subspace of the transition matrix.",
                    "label": 1
                },
                {
                    "sent": "We can view Proto value functions sometimes as approximating subspace invariant features and I'll mention just tangentially this is potentially useful because actually computing the eigenvectors of the transition matrix is an expensive thing.",
                    "label": 0
                },
                {
                    "sent": "PDF MP's doing matching pursuits with proto value functions will dominate vanilla PDF 'cause it's paying attention to.",
                    "label": 1
                },
                {
                    "sent": "The reward is actually adapting to the performance of the system.",
                    "label": 0
                },
                {
                    "sent": "Bellman error basis functions will always have Delta R = 0 and in practice we see that they have a more steady and predictable reduction of development error than some of the other methods we've introduced.",
                    "label": 1
                },
                {
                    "sent": "And one bottom line conclusion is don't ignore the reward when constructing.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Features OK, so just the the talk in pictures again.",
                    "label": 0
                },
                {
                    "sent": "There's the equivalence between the different different views.",
                    "label": 0
                },
                {
                    "sent": "The model based on the direct value function based there is the expression for the bellman error in terms of the model error that gives us the insights into feature selection and then we have the experiments that show how different feature selection algorithms perform given these.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Criteria.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Cross.",
                    "label": 0
                },
                {
                    "sent": "How is there?",
                    "label": 0
                },
                {
                    "sent": "Is there decision part much harder?",
                    "label": 0
                },
                {
                    "sent": "It seems a lot harder than by little, yeah.",
                    "label": 0
                },
                {
                    "sent": "So what about MVP's?",
                    "label": 0
                },
                {
                    "sent": "So yeah, it is considerably.",
                    "label": 0
                },
                {
                    "sent": "Pardon, no, I didn't pay for that question, so it is considerably harder.",
                    "label": 0
                },
                {
                    "sent": "So so in the paper we talk about how to think about what LSP is doing in this context.",
                    "label": 0
                },
                {
                    "sent": "And basically we can view LSP is constructing a new linear model at every iteration when the policy changes.",
                    "label": 0
                },
                {
                    "sent": "Ann, it's so we can apply this analysis to sort of 1.",
                    "label": 0
                },
                {
                    "sent": "One policy of LSP, but actually getting it to carry over traditional policies is difficult.",
                    "label": 0
                },
                {
                    "sent": "Ron, thanks for the stock is very, very interesting and.",
                    "label": 0
                },
                {
                    "sent": "As as often you and I are thinking alike and Alberta we're looking at very similar things with the linear models and.",
                    "label": 0
                },
                {
                    "sent": "And then going inside the direction and and so that we have a UI paper like couple of days.",
                    "label": 0
                },
                {
                    "sent": "But we're looking at.",
                    "label": 0
                },
                {
                    "sent": "Control context and as well as the prediction and so I just want to mention that and.",
                    "label": 0
                },
                {
                    "sent": "And then we go into things like fireplace sweeping and.",
                    "label": 0
                },
                {
                    "sent": "But maybe this is some kind of question.",
                    "label": 0
                },
                {
                    "sent": "The bellman error.",
                    "label": 0
                },
                {
                    "sent": "Earlier maybe is maybe not the right thing to focus on.",
                    "label": 0
                },
                {
                    "sent": "I mean you.",
                    "label": 0
                },
                {
                    "sent": "You are.",
                    "label": 0
                },
                {
                    "sent": "But please all my questions.",
                    "label": 0
                },
                {
                    "sent": "Anyone hearing comments on which is when?",
                    "label": 0
                },
                {
                    "sent": "I actually really like your comment on the.",
                    "label": 0
                },
                {
                    "sent": "The the way in which these these means of extracting features are not very constructive.",
                    "label": 0
                },
                {
                    "sent": "We sort of require us to know what Bellman areas.",
                    "label": 0
                },
                {
                    "sent": "However the eigenvectors are, so it seems like that sort of.",
                    "label": 0
                },
                {
                    "sent": "Is that the question?",
                    "label": 0
                },
                {
                    "sent": "OK, so so.",
                    "label": 0
                },
                {
                    "sent": "Actually yes, I tend to agree with that so.",
                    "label": 0
                },
                {
                    "sent": "Of all the methods I talked about, I'd say the most in my opinion, the most practical is the Bellman error approach in our ice email paper last year.",
                    "label": 0
                },
                {
                    "sent": "We talked about how to construct a. Lastly, before I said that I should mention again, we're not the first to do that either.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "The Keller Italia paper.",
                    "label": 0
                },
                {
                    "sent": "Introduces in approximate Bellman error as a feature and in our ICL paper last year we talked about how to deal with an approximate Bellman error as a feature.",
                    "label": 0
                },
                {
                    "sent": "So I think that's a much more reasonable thing to do because you can generate an approximate Bellman error just by looking at the TV signal 'cause the TV signal is going to be a sample of the approximate velmanette of sample of the Bellman error now.",
                    "label": 0
                },
                {
                    "sent": "As far as the other methods, which for example would require you to get the eigenvectors of the transition matrix, I think that's an extremely impractical approach because if you have the transition matrix, you might as well just solve for the value function.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "One thing that if I'll put my feet or head on for a moment, one thing that retard would point out is that computing the eigenvectors of the Laplacian in many ways could be easier than computing the eigenvectors of the transition matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, one because the last thing is going to be her mission, so it's going to be.",
                    "label": 0
                },
                {
                    "sent": "Stable operation, the other is that the adjacency matrix in some sense is going to have.",
                    "label": 0
                },
                {
                    "sent": "It's just binary, right?",
                    "label": 0
                },
                {
                    "sent": "So it's going to have sort of lower sample complexity to actually construct that, then the transition matrix.",
                    "label": 0
                },
                {
                    "sent": "But still I think it's an awful lot of work to do that, so I do have some concerns about that.",
                    "label": 0
                },
                {
                    "sent": "We're running out of time, so let's end the speaker.",
                    "label": 0
                }
            ]
        }
    }
}