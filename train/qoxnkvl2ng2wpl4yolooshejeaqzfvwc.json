{
    "id": "qoxnkvl2ng2wpl4yolooshejeaqzfvwc",
    "title": "Nonparametric Bayesian Modelling",
    "info": {
        "author": [
            "Zoubin Ghahramani, Department of Engineering, University of Cambridge"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2012_ghahramani_nonparametric_bayesian/",
    "segmentation": [
        [
            "So I'm going to be giving a tutorial on nonparametric Bayesian modeling and to the people in the summer school.",
            "I apologize because obviously I will have a little bit of overlap with the material I covered in the summer school.",
            "There was quite a lot of material in nonparametric Bayesian modeling.",
            "They'll be about five or six slides of overlap.",
            "The rest of the material should be new.",
            "So my goal is not to bore all those people, but to give enough background for the rest of the people in the room to be able to follow what's going on.",
            "OK, so this tutorial is about nonparametric Bayesian modeling an I'm going to actually spend a little bit of time talking about each of these words and where it fits into the big picture."
        ],
        [
            "So first of all, the approach we're going to take is a probabilistic modeling approach in the sense that you know.",
            "A lot of machine learning is based around doing stuff with data.",
            "Right and one of the things you can do with data is try to build models of the data.",
            "And the framework we use for modeling is based on probability theory.",
            "So from a probability theory point of view, what's a model?",
            "Well, a model describes data that one could possibly observe from a system.",
            "And we're going to use the mathematics of probability theory to represent all the forms of uncertainty that we have about our different possible models.",
            "The parameters of our models, the noise processes in the data, etc.",
            "And the good news is that you can use that same mathematics of probability theory to then do inference about things in your model unknown quantities adapt your model, make predictions, learn from data, etc.",
            "So."
        ],
        [
            "This side should be familiar for the summer school.",
            "The good news from that probabilistic modeling framework is that there isn't really that much fundamental that you need to know to do modeling, so almost everything simply follows from these two rules of probability theory, the sum rule and the product rule.",
            "The sum rule says that the marginal probability of some variable X can be written as the sum over some other variable Y, and if that's continuous, this would turn into an integral.",
            "Of the joint probability of X&Y.",
            "The product rule says that the joint probability of X&Y can be factored into the probability of X times the probability of Y given X.",
            "Now if we take these two rules an we rearrange them, we get Bayes rule.",
            "Which this follows from these.",
            "And here I've written Bayes rule with different symbols from X&Y.",
            "I've written it with Theta corresponding to parameters of some Model D corresponding to some data that we might observe.",
            "Everything here is conditioned on some model assumptions.",
            "M or a model class M and I'll talk about learning that model class as well.",
            "And this rule that simply follows from the sum rule in the product rule.",
            "Has a very nice interpretation.",
            "It's the framework for learning, so learning in this framework just consists of starting out before observing the data.",
            "This learning is the process of change that happens from observing data.",
            "So before you observe the data you have some parameters you don't know what those parameters are.",
            "You represent your lack of knowledge about those parameters.",
            "Your uncertainty about those parameters as a probability distribution, sort of a soft way of specifying a range over those parameters.",
            "And then you observe the data and because your model."
        ],
        [
            "Of going back to hear your model was described within the language of probability theory 4."
        ],
        [
            "Or for any particular parameter value, you can compute the probability of the data that you actually observed under that parameter value.",
            "This is called the likelihood function.",
            "The sum rule in the product will tell you that you can multiply these two and renormalize it and that gives you this quantity here, which is the posterior distribution of your parameters data given the data and that process of learning is the transformation from the prior what you knew before the data to the posterior.",
            "What you know after the data.",
            "Clearly after the data you can still be uncertain about the parameters.",
            "Almost always you will be.",
            "And so the posterior represents that uncertainty in your parameters after the data.",
            "Of course, if you now observe more data, the posterior from the previous.",
            "Step becomes the prior for the new data that you've observed.",
            "OK, so.",
            "It's incredibly straightforward and very comfortingly, so we don't have to invoke any complicated ideas to do all of this.",
            "Now, how does this help us for things like prediction an model comparison?",
            "Well again, we have a rule book.",
            "It's a very short rule book, so we follow those rules if we want to make predictions about new data called the new data X.",
            "Given the data that we've observed so far, and our general model class that we've assumed.",
            "Then that prediction is an average by the sum rule.",
            "Of the predictions that you would get for any particular parameter value, Theta, which are expressed as a probability distribution over your new data point X.",
            "Weighted by that posterior that we computed in the previous equation here.",
            "And this is very intuitive and natural.",
            "It tells us that our prediction is obtained through an averaging process averaging over that thing that we didn't know the parameters.",
            "Now, if you want to do model comparison.",
            "If we have several models, MM prime and double prime, etc and we want to know given the data, which seems more reasonable, let's say then applying these rules at a higher level we get this expression here, which is the posterior probability of the model given the data expressed as this term here, which is called the marginal likelihood that was in the normalizer of this equation.",
            "Times the prior over the model classes.",
            "Divided by yet another higher level normalization constant.",
            "And now.",
            "That marginal likelihood this term in red is an integral of the likelihood with respect to the prior.",
            "So it says the probability of the data under a model is the average.",
            "Of the likelihood average with respect to the prior over the parameters.",
            "OK.",
            "So during the summer school I spent some time talking about properties of the marginal likelihood and Occam's razor and so on.",
            "I won't do that again, but I think that's all very important and exciting things to know about.",
            "Of course, this sort of model comparison frame."
        ],
        [
            "Work can be learned, can be applied in a wide, wide range of settings relevant to statistics and machine learning.",
            "So for example, we can use it to answer questions like how many clusters are there in the data, or if we're doing dimensionality reduction, what should be the intrinsic dimensionality of our dimensionality reduction method?",
            "If we're trying to predict some output, given many possible inputs, we can use that model comparison framework given by these equations to decide which input features are relevant for that output that we're trying to predict.",
            "Feature or variable selection, in other words.",
            "If we are fitting a dynamical system to some data, like some time series data.",
            "Then we can use this to infer the order of the dynamical system.",
            "If we're using hidden Markov models in some application like you know, here's a sequence of amino acids corresponding to bit of a protein.",
            "Or, you know, in language modeling or speech recognition we can use this framework for figuring out how many states we should have in our hidden Markov model, etc etc.",
            "You know, here's just a diagram representing a blind source separation independent components analysis.",
            "How many sources are there out there?",
            "An you know finally down at the bottom here.",
            "If you're trying to learn the structure of a graphical model, which edges should be there, which ones shouldn't be there?",
            "OK. Alright good."
        ],
        [
            "So now the.",
            "Story that I'm going to concentrate on is this Bayesian nonparametric framework for modeling.",
            "Now I've told you a little bit about what I mean by modeling.",
            "And I've told you about Bayesian sort of the basics of Bayesian inference on one slide, so.",
            "What about the nonparametrics aspect, and are in general you know why Bayesian nonparametrics this has ended up, you know, not you know.",
            "I didn't plan it this way, but it ended up that a lot of my research is on Bayesian Nonparametrics now, and anybody who does research you know you have to be able to ask them why are you doing that?",
            "And the answer that they should give you should be the main motivation for why they think that that's an interesting thing or an important thing to work on.",
            "So why Bayesian nonparametrics?"
        ],
        [
            "Well, I've already told you.",
            "Why the Bayesian framework is nice in some ways, it's incredibly simple.",
            "It's very straightforward.",
            "It's a.",
            "It's an application of the rules of probability theory, little over 100 years ago.",
            "The term Bayesian didn't even exist, it was just called probability or inverse probability.",
            "When you apply the rules in particular way.",
            "And so you know, there isn't a need to invent new formalisms.",
            "It's all about manipulation of uncertainty.",
            "In contrast, why nonparametrics the answer to that is the complexity.",
            "Of real world phenomena.",
            "So you want to use a simple framework so you can think simply about you know the procedure of learning and inference from data.",
            "But you don't want to assume that the world that you're trying to model the data that you're trying to model comes from a simple phenomenon, because that would be incorrect, right?",
            "So a lot of work that you would find in an elementary over a lot of the methods you'd find in an elementary.",
            "Statistics textbook are based around making things like linearity.",
            "Assumptions are very nice because we can analyze those very well with pencil and paper.",
            "But you know if you go around modeling a complicated.",
            "Dynamical system, assuming that it's linear, you might not be able to control it as we saw in the summer school with the inverted pendulum example, for example that you can't really solve with a linear system.",
            "So.",
            "I'm really going to focus on non parametric so that my models are rich enough to be able to capture the complexity of real world phenomena."
        ],
        [
            "So to understand nonparametrics it's useful to describe what parametric models are first.",
            "After a sip of coffee.",
            "So.",
            "A parametric model assumes that there's some finite set of parameters, and given those parameters, any predictions I might want to make about the model are independent about the observed data, so the parameters are summary of all the structure that we've learned about the data that's useful for making predictions.",
            "This is a conditional independence assumption in a sense.",
            "Parameters these parameters data capture everything there is to know about the data for future predictions.",
            "And I like to think of this as.",
            "All of learning can be thought of as.",
            "Information channel from past data to future predictions and in a parametric model all that information from past data goes through the parameters of the model.",
            "So if you have a simple model with a few parameters, you have a limited capacity channel taking you from past data to future predictions.",
            "Um?",
            "Non parametric models take that limited capacity channel and add many many lanes.",
            "In fact, usually infinitely many lanes to that Channel so that you actually can pass a growing amount of information from past data into future predictions.",
            "So just to be a little more precise about what I mean, nonparametric models assume that data distribution cannot be defined in terms of such a finite set of parameters, but often for nonparametric model to be useful or workable, it can be defined assuming some infinite dimensional parameter, Theta.",
            "And usually whenever we have an infinite dimensional parameter, we can think of these things as functions OK.",
            "So we'll talk about, you know unknown functions, Theta or unknown distributions, Theta, etc.",
            "Now when you have a nonparametric model, more information can flow from past data to future predictions.",
            "And this just makes those models more flexible.",
            "They have many more degrees of freedom.",
            "They can capture more subtle things in the data and in particular as you get more and more data, you will hopefully learn about more and more subtle aspects of your data.",
            "When it comes to making predictions.",
            "OK, any questions about that, yes.",
            "The dimensionality yes, sorry, I don't mean that it's a discrete parameter space.",
            "For example, a simple example is linear regression.",
            "Linear regression in 1D has two parameters.",
            "OK, of course those parameters are represented as real values, so this this argument about information is a little tricky.",
            "We have to be very careful about that, because obviously in one real number we could store an infinite number of bits.",
            "But nonetheless.",
            "A linear model can only capture a limited amount.",
            "Of complexity from the past.",
            "Data to make future predictions.",
            "Yes.",
            "And given that also holds for nonparametric models.",
            "Um?",
            "So.",
            "It's.",
            "It's true, but usually we don't.",
            "We there's no way of saying that we know all of Theta because they does infinite dimensional, so we can't.",
            "We can't represent that.",
            "We generally don't deal with nonparametric models by explicitly representing that infinite dimensional function is.",
            "That's just not doable on a computer, let's say.",
            "OK, so although the you're right that the conditional independence holds, it doesn't mean that you know we deal with nonparametric models by estimating that Theta somehow.",
            "Because we just can't do that.",
            "So instead what we tend to do with nonparametric models is we try to figure out some way of integrating out that infinite dimensional space.",
            "And then our predictions become dependent on the data set.",
            "So another way of distinguishing between parametric models and nonparametric models is that in nonparametric models.",
            "We tend to need to have store around either the statistics are a growing number of statistics of the data.",
            "As we get more and more data in order to make predictions.",
            "So these are more memory based and the amount of memory needed grows with the amount of data.",
            "OK. Any other questions?"
        ],
        [
            "So nonparametric models tend to be more flexible, so this picture here is meant to represent a mixture model.",
            "So if I assume that my data was generated by a mixture of Gaussians, then I say I'm going to fit up to five Gaussians then or up to here 6 Gaussians.",
            "Then no matter how much data I have, I only find structure equivalent to six Gaussians in the data, OK?",
            "Whereas the equivalent nonparametric model here, for example something called the Dirichlet process mixture, would be able to have more to be able to represent more an more clusters, the more data you observe.",
            "OK, similarly, if I'm trying to model some functional relationship between X&Y from some data, if I assume that I have a polynomial of some order M that I can only capture.",
            "Up to that polynomial of order M, Whereas the equivalent or an related nonparametric model, for example Gaussian process, would be able to capture much more structure in the XY relationship.",
            "Now, from a practical point of view, that flexibility should translate into better predictive performance.",
            "Of course, there's no guarantees about anything in the finite data case.",
            "You don't know it's going to happen, but you want to be able to use that flexibility to actually get better predictive performance.",
            "And there is a good evidence that that actually does happen in.",
            "Really, 6 scenarios like for example data compression or something like that, and finally one of the important things about the Bayesian framework is not only that we use these rules too.",
            "Think about our models, but also the idea that our models are meant to capture are from a subjective Bayesian point of view.",
            "Our models are meant to capture the best of in the best possible way.",
            "Our knowledge about the process that could have generated the data so.",
            "The use of nonparametric models seems almost necessary if we're going to be modeling complex phenomena, because if I, if I get, you know if I'm modeling some phenomenon like so take something simple.",
            "For example, people's height versus their age, right?",
            "Let's say I have some data people's height versus their age.",
            "Well, if I want to do a Bayesian analysis of that, assuming a linear function.",
            "I feel very uncomfortable because I know that that cannot possibly be the true relationship between height and age.",
            "OK, it's not the case that if we find 150 year old person then they're very, very likely to be very, very tall.",
            "But even if I go to a quadratic function, I don't really believe that the relationship is a quadratic.",
            "Or you know any finite order polynomial.",
            "Honestly, it just doesn't make sense.",
            "That's not how the real world works.",
            "So we need nonparametric models so that our models are realistic so that they could possibly capture with enough data what is actually happening in the real world.",
            "Now.",
            "From a practical point of view, I feel that.",
            "There have been a few revolutions in the way people do machine learning, and some of the great advances we've seen in machine learning have come from moving from parametric models and nonparametric models.",
            "So in a sense, almost all successful methods in machine learning are either.",
            "Essentially nonparametric or the footnote here, yet you probably can't see says or highly scalable.",
            "So if you're a company like Google, maybe you're not going to use a nonparametric model, but you want to use a model that you can apply on a billion web pages or something like that.",
            "So high scalability is obviously important, but in terms of predictive performance, the methods that seem to do very well.",
            "Kernel methods such as the SVM or Gaussian process.",
            "For example, are nonparametric an Moreover?",
            "You know, even the sort of formally parametric models these large networks or deep networks of different kinds have so many parameters that effectively their operating in an almost non parametric regime.",
            "They're very very flexible models.",
            "And even simple things that are very easy to apply.",
            "Something like K nearest neighbors is incredibly good in terms of predictive performance and a good thing always to compare two.",
            "And the reason is good as a predictive model is because it's a nonparametric model ascential.",
            "OK, this is kind of a hand WAVY reason to be nonparametric, but you know, really, the motivations are along these lines.",
            "OK, now."
        ],
        [
            "Now, there have been.",
            "There's been decades of work in Bayesian nonparametrics.",
            "Within statistics from around the 70s or so.",
            "And that that has grown a lot in recent years.",
            "And it's also it's grown not only in statistics, but it's grown a lot in recent years in machine learning.",
            "And if you're not familiar with this field.",
            "Then you pick up a typical paper that uses Bayesian nonparametrics.",
            "There is some complicated sounding process being mentioned, etc, and if you're new to this area it can feel very obscure like why are they using this thing and what do Chinese restaurant tables have to do with anything?",
            "And you know what the hell is going on so.",
            "The reason a lot of different stochastic processes are being invoked in this field of Bayesian nonparametrics, at least from the applied side.",
            "Of course, these are all very beautiful objects to study from a probability theory point of view, but from the applied side, the reason these things are being used is that they correspond to very nice and simple components that you can use in a variety of different models for different modeling goals.",
            "So for example, a lot of interesting problems correspond to assuming that there is some unknown function somewhere in your model OK, and so now if I have an unknown function and I don't want to use something simple and parametric like a polynomial, then I want to look for distributions on functions and a particularly nice family or distributions on functions is a Gaussian process.",
            "It's a nonparametric.",
            "Model for functions that can be plugged into a variety of modeling scenarios, so you'll see a lot a lot of papers on Gaussian processes.",
            "I'm sure there are a number of papers at AI stats on Gaussian processes as well.",
            "On the other hand, if you have an unknown distribution somewhere in your model, then there are various things that you could use the most classical one being a Dirichlet process, and again you'll see a lot of Dirichlet processes in papers within statistics and machine learning.",
            "In statistics, there actually processes tend to be used more when you have some unknown distribution.",
            "But often in machine learning, dearsley process and their associated combinatorial stochastic process called the Chinese restaurant process, are often used for clustering problems.",
            "And you know, I'll mention some of these things in a bit more detail, but I won't go actually into dishley processes very much, because that was covered a lot during the summer school already.",
            "So for those people, because of those people, I won't go into that that much.",
            "But for anybody who's interested in following this up, of course there are excellent video lectures on dearsley processes in great depth, including from the summer school, so lookout for those.",
            "Again, you know there are many different modeling goals one could have.",
            "You might want to do a hierarchical clustering and there are things like the Dursley diffusion tree or kingman's coalescent.",
            "Our model.",
            "Might need a sparse binary matrix in it somewhere, at least in a nonparametric context.",
            "You might want a sparse binary matrix with a growing number of columns, let's say.",
            "And I'll talk about the Indian buffet process that does that.",
            "Nonparametric Bayesian papers on survival analysis will tend to refer to things like the beta process etc etc.",
            "OK.",
            "So there is a zoo of different things out there, but there are good reasons why people use these different things.",
            "OK."
        ],
        [
            "The two most widely used stochastic processes in Bayesian nonparametrics.",
            "Are the gaussian.",
            "Process and Additionally process.",
            "So just in at a high level cartoon, let me describe what these things are.",
            "Gaussian process defines the distribution on functions.",
            "So here is a single sample drawn from a Gaussian process, shown as a little plot here.",
            "The way we would write that is that this.",
            "Function F is drawn from a Gaussian process.",
            "With mean function mu and covariance function C. So when we write this, we can think exactly analogously to writing something like vector X is drawn from a multivariate normal with mean mu and covariance matrix C. Except that instead of a vector, what you draw from a Gaussian process is a function which you could think of as an infinite dimensional vector.",
            "In fact, one that has potentially uncountably many dimensions.",
            "But the role of the mean and covariance functions is exactly the extension of the role of the mean vector and covariance matrix.",
            "The mean tells you on average what these functions will look like.",
            "And the covariance function tells you how much the function values will Co vary with each other, possibly as a function of the distance between.",
            "Say to be more precise, consider F evaluated X&F, evaluated at some other point X prime.",
            "Those two F values are generally going to be correlated and the covariance function is a function of X&X prime that tells you how much those two F values will covary.",
            "I'll say a little bit more about that.",
            "But let me move on to their sleep process.",
            "Completely analogously to this Adi Richley process defines a distribution on distributions, so G the object that we're drawing here is distribution I've just drawn.",
            "I've drawn a histogram of a kind of finitely binned histogram of a distribution distribution.",
            "Here is.",
            "Is simply a non negative function that integrates to one.",
            "We could think of it that way and the deer shape process has analogous to the Gaussian process parameter here G not, which is the base measure which controls the mean of G and a single scalar parameter Alpha called the concentration or scaling parameter which controls the variability around that mean.",
            "Of G. OK.",
            "So just like the Gaussian process is an infinite dimensional Gaussian, we can think of the Dirichlet process.",
            "As an infinite dimensional Dirichlet distribution.",
            "OK. No, I just wanted to mention a little bit about Gaussian process again because this was covered in great depth during the summer school.",
            "I won't go into this in any more detail other than to say one of the things I really like to do is to try to understand the relationships between different sorts of methods, and we can think of Gaussian processes as being very closely related to kernel methods that are so familiar within.",
            "Machine learning."
        ],
        [
            "So what do I mean by that?",
            "Well?",
            "Consider the following cube, which relates eight different classes of models to each other and the way I've drawn this cube is by starting from a really simple thing which is linear regression.",
            "So everybody here knows what linear regression is right now.",
            "Can do different operations to that model.",
            "Linear regression an from that we get different kinds of models coming out.",
            "For example, we can take a linear regression model and we can turn it into a classification model by taking the outputs of the linear regression and running them through a logistic function or a probate function.",
            "So for example, an if you follow these.",
            "These purple arrows you go from linear regression to the corresponding linear classification models such as logistic regression.",
            "This is an example of something that you would get here.",
            "You can take that linear regression model and you can kernel eyes it.",
            "To get a nonlinear model in other words, imagine doing linear regression, but first mapping your data into some very high or infinite dimensional feature space.",
            "That's what kernel regression corresponds to an the process of kernel Ising is following these orange arrows.",
            "You can take that linear regression and instead of fitting the parameters of that linear regression with least squares or maximum likelihood, or something like that, imagine Now doing Bayesian inference over those parameters, and this gives you Bayesian linear regression.",
            "OK, so these are three operations we can do to linear regression.",
            "But now we can apply multiple of these operations and.",
            "Get other models so a Gaussian process regression model can be thought of either as a kernelized Bayesian linear regression.",
            "Or it can be thought of as a Bayesian version of kernel regression.",
            "K equivalently.",
            "We could take kernel a, kernelized regression model and turn it into a kernel classification model such As for example, the support vector machine by following this arrow.",
            "We could have gotten that also by taking.",
            "A classification model such as logistic regression and kernel Ising that.",
            "And then if we apply all three of these operations in different orders, then we arrive here.",
            "Which is a classification version of Gaussian process regression and that can be thought of as a Bayesian version of kernel classification.",
            "Or in other words, you know.",
            "Take for example this.",
            "Or vector machine?",
            "If I look at the support vector machine and I say alright, how would I do that same sort of thing from a Bayesian framework?",
            "Then the natural answer is to do Gaussian process classification.",
            "Any questions about that?",
            "Nope.",
            "OK, in particular the point."
        ],
        [
            "I really want to make if you're not already familiar with it is that this covariance function here?",
            "Plays exactly the same rule as role as the Mercer kernel does in.",
            "Support vector machine.",
            "So any kernel that you could use in an SVM.",
            "By the fact that it's a valid kernel could also be used in a Gaussian process, and the interpretation of that is to think of that kernel representing.",
            "The way in which your function varies with space OK. Or distribution over your function."
        ],
        [
            "Alright, so let me move on."
        ],
        [
            "From there, because essentially I'm not going to spend anymore time talking about these very, very well studied models like the Gaussian process and the Dirichlet process.",
            "What I'm going to talk about for the rest of the tutorial is.",
            "How to take these Bayesian nonparametric ideas and apply them in other domains with other kinds of structured objects and data?",
            "In particular, I get to talk about.",
            "Doing time series modeling with hidden Markov models, I'm going to talk about models that involve sparse matrices and the uses of those.",
            "I'm also going to talk about covariances.",
            "I didn't mention that here, and finally I'm going to talk about modeling networks like biological or social networks using nonparametric ideas.",
            "So how am I doing?"
        ],
        [
            "OK, so let's let's talk about time series.",
            "I think we'll we'll take a break after an hour, so.",
            "I'll let you know when that is OK, so let's talk about time series.",
            "Now there's a huge amount of work on different kinds of time series models.",
            "But I'm going to focus on models that are based on the idea of a discrete state variable capturing the time series."
        ],
        [
            "These models are called hidden Markov models.",
            "So hidden Markov models have been used in vast number of different applications.",
            "Notably, it's speech recognition, bioinformatics about physics, text modeling, video, etc.",
            "But the basic structure of the hidden Markov model is fairly easy to understand and get intuitions about.",
            "What a hidden Markov model does is.",
            "It tries to model sequences of observations, so Y one through YT is a sequence of observations where the index is presenting time or location along some other kind of sequence.",
            "It could be a text sequence or biological sequence or something like that.",
            "It doesn't have to be time, but we're going to use the index T because usually we think about them as time.",
            "What a hidden Markov model.",
            "Says is we're going to model that sequence of observations Y by assuming that it was generated from a sequence of hidden states S1 through St.",
            "Furthermore, the hidden state sequence satisfies the Markov assumption, which means that that state variable.",
            "S of T. Captures everything about the past of the sequence that's needed for predicting the future of the sequence.",
            "So it's a summary of the past history of this time series, OK?",
            "So the Markov assumption represented as a directed graphical model looks like this.",
            "It's basically showing you that this variable here renders this part of the graph conditionally independent.",
            "That's called that the future of the graph conditionally independent from the past of the graph through these arrows.",
            "OK, now in.",
            "In a.",
            "Classical hidden Markov model.",
            "The basic assumption is that your states are discrete and that your state variable can take on say up to K values.",
            "So we'll do note that as integers one through K. By the Markov assumption, what that means is that the dynamics of the system is captured through some K by K transition matrix.",
            "OK.",
            "There are many ways of thinking about the hidden Markov model, but one particularly nice way of thinking about the hidden Markov model is that at every time slice, what's going on is that the data is being clustered.",
            "The variable S of T represents which of K possible clusters datapoint Y of T belongs in.",
            "But it's a time dependent clustering or mixture model in the sense that S of T depends the cluster at time T depends on the cluster of the previous time step.",
            "Now within the Bayesian nonparametrics literature.",
            "It's been widely known that if you want to take a mixture model with K mixer components or clustering model with K clusters, and you want to consider a nonparametric version of that where K goes to Infinity, the possible number of clusters goes to Infinity, then the tool that's most widely useful for that is the Dirichlet process mixture.",
            "So inspired by that knowledge of how to take one time slice model of this and make it nonparametric, we can now look at how to make the whole hidden Markov model nonparametric.",
            "So how do we remove this upper bound K on the number of states?",
            "How do we allow the hidden Markov model to have a countably infinite number of states?"
        ],
        [
            "So this is the basic idea behind the infinite hidden Markov model.",
            "It takes the previous model and it lets the number of hidden states go to Infinity.",
            "And there are various ways of understanding what the behavior of this model is.",
            "I'm not going to tell you the detailed derivation of this model, but you know there are a lot of papers that you can look at for that, but I do want to give you a bit of intuition for why we do this and how we do this.",
            "So let's first talk about why we do this and I'll make you focus on this plot here.",
            "With this plot here shows you is.",
            "Essentially, the entire text of Alice in Wonderland, the novel.",
            "OK, the way it shows you that text is the text goes from the beginning to the end of Alice in Wonderland there about, I guess.",
            "A little over 25,000 words.",
            "In Alice in Wonderland.",
            "And what what we've plotted here is every time a word occurs, we give it a new identity.",
            "So we the first word, we call it #1.",
            "If that word occurs again, we are visiting word one again.",
            "If we observe a new word, we give it a new index.",
            "So the envelope that you see here is the word and identity as a function of.",
            "The length of the text OK.",
            "So what's the point of showing this?",
            "Well, what's kind of interesting about this?",
            "To me at least, is that new words are appearing all the time in this text.",
            "OK, for the first time, they're appearing all the time, and if you wanted to have a model for.",
            "Words in this case, then you would really want to model that is allowed to invoke knew words as the text gets longer and longer.",
            "This is kind of part of the philosophy behind why we want a nonparametric model.",
            "Similarly, if you wanted to do something like analyze news stories or topics of tweets or anything like that from a growing corpus, you really want to be able to have a model which can invoke mus structure.",
            "As more and more data comes in.",
            "So sort of inspired by this growing envelope of real world data.",
            "What we want to have in our infinite hidden Markov model is the property that as if we sample sequence of hidden states from our infinite hidden Markov model, we want to get a behavior that looks something like this, which is the state that's visited on the Y axis as a function of time.",
            "And the property that this has is that.",
            "As you visit more and more as you go on for longer and longer, you will have visited more and more states, so this envelope grows as well.",
            "Now you don't want, this is another sample from particular parameter settings of an infinite hidden Markov model.",
            "This kind of gives you more left to right structure in HMM, but it's a little bit unsatisfying if your process has the property that every point in time it visits and you state OK.",
            "So the first point in time it goes to state one, then without loss of generality, the second state, we could call it State 2.",
            "State three, and so on.",
            "It doesn't give you very interesting dynamics if you never revisit past states, so we really want a growing number of states that we can visit overtime, but we also want the ability in our model to have.",
            "Structure within the state so that we can revisit old states so we have loops and states etc to get complicated dynamics.",
            "So.",
            "What I've shown here are just some samples from the prior over the state sequence of that infinite hidden Markov model.",
            "That is interesting because as a Bayesian, one of the things you do when you develop a model or what you should do when you develop a model is forget about the data you want to be able to sample from.",
            "The structure of that model from the prior and the predictions that it makes about the data without having observed any data.",
            "So you can understand the properties of your model before you expose it to the data, because maybe the properties are not what you want.",
            "Maybe you wanted this, but when you sample you get this.",
            "And then you have to think did I choose the right structure of the model, the right hyperparameters, etc.",
            "So this sort of idea was developed about 10 years ago, an.",
            "Back then it was rather impractical, but over the last 10 years alot has happened in particular.",
            "The model that we developed can be understood as.",
            "Generating the parameters of the transition matrix from a hierarchical Dirichlet process.",
            "So in the original hierarchical Dirichlet process, they showed that our bottle could be re derived as a from the HTTP framework.",
            "They somewhat unhelpfully, I think maybe.",
            "Change the name around a bit and called an HTP.",
            "Hmm, but it turns out that it gives you exactly the same distribution over state sequences, so it's basically the same model.",
            "But that gave a huge amount of insight into the infinite hmm and how it relates to Dirichlet processes, and also gave us a more efficient way of doing sampling from this model, but still not efficient to apply to very large datasets.",
            "So what's happened over the last few years is that a lot of people have been trying to develop more and more efficient methods for doing inference in these models, and even parallel and distributed implementations.",
            "So we can run these infinite HMMS on corpora with you know millions of data points.",
            "So that's exciting because although it took about 10 years, we're now at a stage where we can apply these things practically, and there's sort of stable code for doing that, etc.",
            "OK."
        ],
        [
            "Let me show you a nice application.",
            "I have an infinite hmm, or at least a couple of applications, so this is I will go actually into the application, but this is this is just an image of this sort of behavior that you would get out of applying an infinite hmm to some data.",
            "So in this particular case we have a time series of 1 dimensional observations.",
            "These are these dots and time is going along this axis and what the infinite hmm.",
            "Can do is.",
            "It infers the state that you're in at every point in time that's given by these colors.",
            "But it also tries to discover how many states you need to model this time series.",
            "So the fact that there are 1234 colors means that the infinite hmm here.",
            "Importantly, think about this, although it has an unbounded number of states, it can potentially use.",
            "It chose to use about four for this finite sequence.",
            "What that means in terms of prediction is if we go further into the future and we observe some very weird behavior.",
            "The information ma'am is now allowed to invoke and use state to account for that new behavior.",
            "This is somewhat different from what seems a very similar thing to do so often when I talk about basing on parametrics, I guess asked how different is it to have an infinite model like these as compared to doing inference about the number of states in a finite model.",
            "So we have a lot of good methods for figuring out how many states we should have in a finite hidden Markov model.",
            "So why don't I just figure out how many states I have in a finite hidden Markov model?",
            "Well in this case.",
            "From this finite amount of data.",
            "I would probably infer that there are 1234 states for finite hidden Markov model.",
            "But that's a bit unsatisfactory, I feel, because essentially I've put then my.",
            "In terms of predictions, I put my beliefs on this number 4, so if I observe future data and it doesn't fit in.",
            "Then I still have to account for that by.",
            "Saying it's one of the four states that I inferred from the data.",
            "So inferring the size of a finite model philosophically is very, very different than starting out with an infinite model.",
            "OK, let me just try to explain that again.",
            "If I'm trying to infer.",
            "A finite model.",
            "What I'm actually saying from a subjective Bayesian point of view, is that I believe this real world process came from a finite hidden Markov model.",
            "I just don't know what the order of that the number of states of that finite model is.",
            "Let me learn that from data that's very different from, say, I don't believe that this real world process came from a finite hidden Markov model, but I believe that if I had.",
            "Countably many countably infinitely many hidden states.",
            "I would be able to cap to capture this dynamics.",
            "Alright, so that's the infinite model infinite hmm applied to some 1 dimensional time series here.",
            "I don't know if you can see it, but these are frames from a video.",
            "Of somebody playing on the Nintendo Wii.",
            "OK, so they're doing different sorts of gestures on the Nintendo Wii.",
            "And what we used here was an extension of the infinite hidden Markov model.",
            "To try to segment this video into different gestures, for example batting, boxing, pitching and tennis.",
            "OK, so that's an application of an infinite hmm to video data, where again we don't know how many gestures there are.",
            "This isn't a classification problem, we're trying to infer these high level gestures from the data.",
            "Itself, now the modification of the infinite hidden Markov model that we use.",
            "Developed by Tom Stapleton, is this idea of having an infinite hmm where the hidden states are grouped together into perhaps what you could call behaviors so so one group of hidden states.",
            "This is more connected to each other and it's associated with one behavior.",
            "Another group is associated with another behavior, etc.",
            "So when you look at the transition matrix of that kind of infinite hidden Markov model, it has a block diagonal structure.",
            "OK, with sub connections of course between the different behaviors.",
            "So, or you could also think of this as a hidden Markov model with two levels of the hierarchy of hidden states.",
            "You have big states corresponding to large scale behaviors, and then within those you have sub states corresponding to different little actions within that large scale behavior.",
            "OK. Alright, I think maybe it's a good time for a 5 minute break.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to be giving a tutorial on nonparametric Bayesian modeling and to the people in the summer school.",
                    "label": 0
                },
                {
                    "sent": "I apologize because obviously I will have a little bit of overlap with the material I covered in the summer school.",
                    "label": 0
                },
                {
                    "sent": "There was quite a lot of material in nonparametric Bayesian modeling.",
                    "label": 1
                },
                {
                    "sent": "They'll be about five or six slides of overlap.",
                    "label": 0
                },
                {
                    "sent": "The rest of the material should be new.",
                    "label": 0
                },
                {
                    "sent": "So my goal is not to bore all those people, but to give enough background for the rest of the people in the room to be able to follow what's going on.",
                    "label": 0
                },
                {
                    "sent": "OK, so this tutorial is about nonparametric Bayesian modeling an I'm going to actually spend a little bit of time talking about each of these words and where it fits into the big picture.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, the approach we're going to take is a probabilistic modeling approach in the sense that you know.",
                    "label": 0
                },
                {
                    "sent": "A lot of machine learning is based around doing stuff with data.",
                    "label": 0
                },
                {
                    "sent": "Right and one of the things you can do with data is try to build models of the data.",
                    "label": 0
                },
                {
                    "sent": "And the framework we use for modeling is based on probability theory.",
                    "label": 0
                },
                {
                    "sent": "So from a probability theory point of view, what's a model?",
                    "label": 0
                },
                {
                    "sent": "Well, a model describes data that one could possibly observe from a system.",
                    "label": 1
                },
                {
                    "sent": "And we're going to use the mathematics of probability theory to represent all the forms of uncertainty that we have about our different possible models.",
                    "label": 0
                },
                {
                    "sent": "The parameters of our models, the noise processes in the data, etc.",
                    "label": 0
                },
                {
                    "sent": "And the good news is that you can use that same mathematics of probability theory to then do inference about things in your model unknown quantities adapt your model, make predictions, learn from data, etc.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This side should be familiar for the summer school.",
                    "label": 0
                },
                {
                    "sent": "The good news from that probabilistic modeling framework is that there isn't really that much fundamental that you need to know to do modeling, so almost everything simply follows from these two rules of probability theory, the sum rule and the product rule.",
                    "label": 0
                },
                {
                    "sent": "The sum rule says that the marginal probability of some variable X can be written as the sum over some other variable Y, and if that's continuous, this would turn into an integral.",
                    "label": 0
                },
                {
                    "sent": "Of the joint probability of X&Y.",
                    "label": 1
                },
                {
                    "sent": "The product rule says that the joint probability of X&Y can be factored into the probability of X times the probability of Y given X.",
                    "label": 0
                },
                {
                    "sent": "Now if we take these two rules an we rearrange them, we get Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "Which this follows from these.",
                    "label": 0
                },
                {
                    "sent": "And here I've written Bayes rule with different symbols from X&Y.",
                    "label": 0
                },
                {
                    "sent": "I've written it with Theta corresponding to parameters of some Model D corresponding to some data that we might observe.",
                    "label": 0
                },
                {
                    "sent": "Everything here is conditioned on some model assumptions.",
                    "label": 0
                },
                {
                    "sent": "M or a model class M and I'll talk about learning that model class as well.",
                    "label": 0
                },
                {
                    "sent": "And this rule that simply follows from the sum rule in the product rule.",
                    "label": 1
                },
                {
                    "sent": "Has a very nice interpretation.",
                    "label": 0
                },
                {
                    "sent": "It's the framework for learning, so learning in this framework just consists of starting out before observing the data.",
                    "label": 0
                },
                {
                    "sent": "This learning is the process of change that happens from observing data.",
                    "label": 0
                },
                {
                    "sent": "So before you observe the data you have some parameters you don't know what those parameters are.",
                    "label": 0
                },
                {
                    "sent": "You represent your lack of knowledge about those parameters.",
                    "label": 0
                },
                {
                    "sent": "Your uncertainty about those parameters as a probability distribution, sort of a soft way of specifying a range over those parameters.",
                    "label": 0
                },
                {
                    "sent": "And then you observe the data and because your model.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of going back to hear your model was described within the language of probability theory 4.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or for any particular parameter value, you can compute the probability of the data that you actually observed under that parameter value.",
                    "label": 0
                },
                {
                    "sent": "This is called the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "The sum rule in the product will tell you that you can multiply these two and renormalize it and that gives you this quantity here, which is the posterior distribution of your parameters data given the data and that process of learning is the transformation from the prior what you knew before the data to the posterior.",
                    "label": 0
                },
                {
                    "sent": "What you know after the data.",
                    "label": 0
                },
                {
                    "sent": "Clearly after the data you can still be uncertain about the parameters.",
                    "label": 0
                },
                {
                    "sent": "Almost always you will be.",
                    "label": 0
                },
                {
                    "sent": "And so the posterior represents that uncertainty in your parameters after the data.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you now observe more data, the posterior from the previous.",
                    "label": 0
                },
                {
                    "sent": "Step becomes the prior for the new data that you've observed.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "It's incredibly straightforward and very comfortingly, so we don't have to invoke any complicated ideas to do all of this.",
                    "label": 0
                },
                {
                    "sent": "Now, how does this help us for things like prediction an model comparison?",
                    "label": 0
                },
                {
                    "sent": "Well again, we have a rule book.",
                    "label": 0
                },
                {
                    "sent": "It's a very short rule book, so we follow those rules if we want to make predictions about new data called the new data X.",
                    "label": 0
                },
                {
                    "sent": "Given the data that we've observed so far, and our general model class that we've assumed.",
                    "label": 0
                },
                {
                    "sent": "Then that prediction is an average by the sum rule.",
                    "label": 1
                },
                {
                    "sent": "Of the predictions that you would get for any particular parameter value, Theta, which are expressed as a probability distribution over your new data point X.",
                    "label": 0
                },
                {
                    "sent": "Weighted by that posterior that we computed in the previous equation here.",
                    "label": 0
                },
                {
                    "sent": "And this is very intuitive and natural.",
                    "label": 0
                },
                {
                    "sent": "It tells us that our prediction is obtained through an averaging process averaging over that thing that we didn't know the parameters.",
                    "label": 0
                },
                {
                    "sent": "Now, if you want to do model comparison.",
                    "label": 1
                },
                {
                    "sent": "If we have several models, MM prime and double prime, etc and we want to know given the data, which seems more reasonable, let's say then applying these rules at a higher level we get this expression here, which is the posterior probability of the model given the data expressed as this term here, which is called the marginal likelihood that was in the normalizer of this equation.",
                    "label": 0
                },
                {
                    "sent": "Times the prior over the model classes.",
                    "label": 0
                },
                {
                    "sent": "Divided by yet another higher level normalization constant.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 1
                },
                {
                    "sent": "That marginal likelihood this term in red is an integral of the likelihood with respect to the prior.",
                    "label": 0
                },
                {
                    "sent": "So it says the probability of the data under a model is the average.",
                    "label": 0
                },
                {
                    "sent": "Of the likelihood average with respect to the prior over the parameters.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So during the summer school I spent some time talking about properties of the marginal likelihood and Occam's razor and so on.",
                    "label": 0
                },
                {
                    "sent": "I won't do that again, but I think that's all very important and exciting things to know about.",
                    "label": 0
                },
                {
                    "sent": "Of course, this sort of model comparison frame.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work can be learned, can be applied in a wide, wide range of settings relevant to statistics and machine learning.",
                    "label": 0
                },
                {
                    "sent": "So for example, we can use it to answer questions like how many clusters are there in the data, or if we're doing dimensionality reduction, what should be the intrinsic dimensionality of our dimensionality reduction method?",
                    "label": 1
                },
                {
                    "sent": "If we're trying to predict some output, given many possible inputs, we can use that model comparison framework given by these equations to decide which input features are relevant for that output that we're trying to predict.",
                    "label": 0
                },
                {
                    "sent": "Feature or variable selection, in other words.",
                    "label": 0
                },
                {
                    "sent": "If we are fitting a dynamical system to some data, like some time series data.",
                    "label": 1
                },
                {
                    "sent": "Then we can use this to infer the order of the dynamical system.",
                    "label": 0
                },
                {
                    "sent": "If we're using hidden Markov models in some application like you know, here's a sequence of amino acids corresponding to bit of a protein.",
                    "label": 0
                },
                {
                    "sent": "Or, you know, in language modeling or speech recognition we can use this framework for figuring out how many states we should have in our hidden Markov model, etc etc.",
                    "label": 1
                },
                {
                    "sent": "You know, here's just a diagram representing a blind source separation independent components analysis.",
                    "label": 0
                },
                {
                    "sent": "How many sources are there out there?",
                    "label": 0
                },
                {
                    "sent": "An you know finally down at the bottom here.",
                    "label": 1
                },
                {
                    "sent": "If you're trying to learn the structure of a graphical model, which edges should be there, which ones shouldn't be there?",
                    "label": 0
                },
                {
                    "sent": "OK. Alright good.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now the.",
                    "label": 0
                },
                {
                    "sent": "Story that I'm going to concentrate on is this Bayesian nonparametric framework for modeling.",
                    "label": 0
                },
                {
                    "sent": "Now I've told you a little bit about what I mean by modeling.",
                    "label": 0
                },
                {
                    "sent": "And I've told you about Bayesian sort of the basics of Bayesian inference on one slide, so.",
                    "label": 0
                },
                {
                    "sent": "What about the nonparametrics aspect, and are in general you know why Bayesian nonparametrics this has ended up, you know, not you know.",
                    "label": 0
                },
                {
                    "sent": "I didn't plan it this way, but it ended up that a lot of my research is on Bayesian Nonparametrics now, and anybody who does research you know you have to be able to ask them why are you doing that?",
                    "label": 0
                },
                {
                    "sent": "And the answer that they should give you should be the main motivation for why they think that that's an interesting thing or an important thing to work on.",
                    "label": 0
                },
                {
                    "sent": "So why Bayesian nonparametrics?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, I've already told you.",
                    "label": 0
                },
                {
                    "sent": "Why the Bayesian framework is nice in some ways, it's incredibly simple.",
                    "label": 0
                },
                {
                    "sent": "It's very straightforward.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's an application of the rules of probability theory, little over 100 years ago.",
                    "label": 0
                },
                {
                    "sent": "The term Bayesian didn't even exist, it was just called probability or inverse probability.",
                    "label": 0
                },
                {
                    "sent": "When you apply the rules in particular way.",
                    "label": 0
                },
                {
                    "sent": "And so you know, there isn't a need to invent new formalisms.",
                    "label": 0
                },
                {
                    "sent": "It's all about manipulation of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "In contrast, why nonparametrics the answer to that is the complexity.",
                    "label": 0
                },
                {
                    "sent": "Of real world phenomena.",
                    "label": 0
                },
                {
                    "sent": "So you want to use a simple framework so you can think simply about you know the procedure of learning and inference from data.",
                    "label": 0
                },
                {
                    "sent": "But you don't want to assume that the world that you're trying to model the data that you're trying to model comes from a simple phenomenon, because that would be incorrect, right?",
                    "label": 0
                },
                {
                    "sent": "So a lot of work that you would find in an elementary over a lot of the methods you'd find in an elementary.",
                    "label": 0
                },
                {
                    "sent": "Statistics textbook are based around making things like linearity.",
                    "label": 0
                },
                {
                    "sent": "Assumptions are very nice because we can analyze those very well with pencil and paper.",
                    "label": 0
                },
                {
                    "sent": "But you know if you go around modeling a complicated.",
                    "label": 0
                },
                {
                    "sent": "Dynamical system, assuming that it's linear, you might not be able to control it as we saw in the summer school with the inverted pendulum example, for example that you can't really solve with a linear system.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm really going to focus on non parametric so that my models are rich enough to be able to capture the complexity of real world phenomena.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to understand nonparametrics it's useful to describe what parametric models are first.",
                    "label": 0
                },
                {
                    "sent": "After a sip of coffee.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A parametric model assumes that there's some finite set of parameters, and given those parameters, any predictions I might want to make about the model are independent about the observed data, so the parameters are summary of all the structure that we've learned about the data that's useful for making predictions.",
                    "label": 1
                },
                {
                    "sent": "This is a conditional independence assumption in a sense.",
                    "label": 0
                },
                {
                    "sent": "Parameters these parameters data capture everything there is to know about the data for future predictions.",
                    "label": 1
                },
                {
                    "sent": "And I like to think of this as.",
                    "label": 0
                },
                {
                    "sent": "All of learning can be thought of as.",
                    "label": 0
                },
                {
                    "sent": "Information channel from past data to future predictions and in a parametric model all that information from past data goes through the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "So if you have a simple model with a few parameters, you have a limited capacity channel taking you from past data to future predictions.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Non parametric models take that limited capacity channel and add many many lanes.",
                    "label": 0
                },
                {
                    "sent": "In fact, usually infinitely many lanes to that Channel so that you actually can pass a growing amount of information from past data into future predictions.",
                    "label": 0
                },
                {
                    "sent": "So just to be a little more precise about what I mean, nonparametric models assume that data distribution cannot be defined in terms of such a finite set of parameters, but often for nonparametric model to be useful or workable, it can be defined assuming some infinite dimensional parameter, Theta.",
                    "label": 1
                },
                {
                    "sent": "And usually whenever we have an infinite dimensional parameter, we can think of these things as functions OK.",
                    "label": 0
                },
                {
                    "sent": "So we'll talk about, you know unknown functions, Theta or unknown distributions, Theta, etc.",
                    "label": 0
                },
                {
                    "sent": "Now when you have a nonparametric model, more information can flow from past data to future predictions.",
                    "label": 0
                },
                {
                    "sent": "And this just makes those models more flexible.",
                    "label": 0
                },
                {
                    "sent": "They have many more degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "They can capture more subtle things in the data and in particular as you get more and more data, you will hopefully learn about more and more subtle aspects of your data.",
                    "label": 0
                },
                {
                    "sent": "When it comes to making predictions.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions about that, yes.",
                    "label": 0
                },
                {
                    "sent": "The dimensionality yes, sorry, I don't mean that it's a discrete parameter space.",
                    "label": 0
                },
                {
                    "sent": "For example, a simple example is linear regression.",
                    "label": 0
                },
                {
                    "sent": "Linear regression in 1D has two parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, of course those parameters are represented as real values, so this this argument about information is a little tricky.",
                    "label": 0
                },
                {
                    "sent": "We have to be very careful about that, because obviously in one real number we could store an infinite number of bits.",
                    "label": 0
                },
                {
                    "sent": "But nonetheless.",
                    "label": 0
                },
                {
                    "sent": "A linear model can only capture a limited amount.",
                    "label": 0
                },
                {
                    "sent": "Of complexity from the past.",
                    "label": 0
                },
                {
                    "sent": "Data to make future predictions.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "And given that also holds for nonparametric models.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "It's true, but usually we don't.",
                    "label": 0
                },
                {
                    "sent": "We there's no way of saying that we know all of Theta because they does infinite dimensional, so we can't.",
                    "label": 0
                },
                {
                    "sent": "We can't represent that.",
                    "label": 0
                },
                {
                    "sent": "We generally don't deal with nonparametric models by explicitly representing that infinite dimensional function is.",
                    "label": 0
                },
                {
                    "sent": "That's just not doable on a computer, let's say.",
                    "label": 0
                },
                {
                    "sent": "OK, so although the you're right that the conditional independence holds, it doesn't mean that you know we deal with nonparametric models by estimating that Theta somehow.",
                    "label": 0
                },
                {
                    "sent": "Because we just can't do that.",
                    "label": 0
                },
                {
                    "sent": "So instead what we tend to do with nonparametric models is we try to figure out some way of integrating out that infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And then our predictions become dependent on the data set.",
                    "label": 0
                },
                {
                    "sent": "So another way of distinguishing between parametric models and nonparametric models is that in nonparametric models.",
                    "label": 1
                },
                {
                    "sent": "We tend to need to have store around either the statistics are a growing number of statistics of the data.",
                    "label": 0
                },
                {
                    "sent": "As we get more and more data in order to make predictions.",
                    "label": 0
                },
                {
                    "sent": "So these are more memory based and the amount of memory needed grows with the amount of data.",
                    "label": 0
                },
                {
                    "sent": "OK. Any other questions?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So nonparametric models tend to be more flexible, so this picture here is meant to represent a mixture model.",
                    "label": 0
                },
                {
                    "sent": "So if I assume that my data was generated by a mixture of Gaussians, then I say I'm going to fit up to five Gaussians then or up to here 6 Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Then no matter how much data I have, I only find structure equivalent to six Gaussians in the data, OK?",
                    "label": 0
                },
                {
                    "sent": "Whereas the equivalent nonparametric model here, for example something called the Dirichlet process mixture, would be able to have more to be able to represent more an more clusters, the more data you observe.",
                    "label": 0
                },
                {
                    "sent": "OK, similarly, if I'm trying to model some functional relationship between X&Y from some data, if I assume that I have a polynomial of some order M that I can only capture.",
                    "label": 0
                },
                {
                    "sent": "Up to that polynomial of order M, Whereas the equivalent or an related nonparametric model, for example Gaussian process, would be able to capture much more structure in the XY relationship.",
                    "label": 0
                },
                {
                    "sent": "Now, from a practical point of view, that flexibility should translate into better predictive performance.",
                    "label": 1
                },
                {
                    "sent": "Of course, there's no guarantees about anything in the finite data case.",
                    "label": 0
                },
                {
                    "sent": "You don't know it's going to happen, but you want to be able to use that flexibility to actually get better predictive performance.",
                    "label": 0
                },
                {
                    "sent": "And there is a good evidence that that actually does happen in.",
                    "label": 0
                },
                {
                    "sent": "Really, 6 scenarios like for example data compression or something like that, and finally one of the important things about the Bayesian framework is not only that we use these rules too.",
                    "label": 0
                },
                {
                    "sent": "Think about our models, but also the idea that our models are meant to capture are from a subjective Bayesian point of view.",
                    "label": 0
                },
                {
                    "sent": "Our models are meant to capture the best of in the best possible way.",
                    "label": 0
                },
                {
                    "sent": "Our knowledge about the process that could have generated the data so.",
                    "label": 0
                },
                {
                    "sent": "The use of nonparametric models seems almost necessary if we're going to be modeling complex phenomena, because if I, if I get, you know if I'm modeling some phenomenon like so take something simple.",
                    "label": 0
                },
                {
                    "sent": "For example, people's height versus their age, right?",
                    "label": 0
                },
                {
                    "sent": "Let's say I have some data people's height versus their age.",
                    "label": 0
                },
                {
                    "sent": "Well, if I want to do a Bayesian analysis of that, assuming a linear function.",
                    "label": 0
                },
                {
                    "sent": "I feel very uncomfortable because I know that that cannot possibly be the true relationship between height and age.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not the case that if we find 150 year old person then they're very, very likely to be very, very tall.",
                    "label": 0
                },
                {
                    "sent": "But even if I go to a quadratic function, I don't really believe that the relationship is a quadratic.",
                    "label": 0
                },
                {
                    "sent": "Or you know any finite order polynomial.",
                    "label": 0
                },
                {
                    "sent": "Honestly, it just doesn't make sense.",
                    "label": 0
                },
                {
                    "sent": "That's not how the real world works.",
                    "label": 0
                },
                {
                    "sent": "So we need nonparametric models so that our models are realistic so that they could possibly capture with enough data what is actually happening in the real world.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "From a practical point of view, I feel that.",
                    "label": 0
                },
                {
                    "sent": "There have been a few revolutions in the way people do machine learning, and some of the great advances we've seen in machine learning have come from moving from parametric models and nonparametric models.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, almost all successful methods in machine learning are either.",
                    "label": 1
                },
                {
                    "sent": "Essentially nonparametric or the footnote here, yet you probably can't see says or highly scalable.",
                    "label": 0
                },
                {
                    "sent": "So if you're a company like Google, maybe you're not going to use a nonparametric model, but you want to use a model that you can apply on a billion web pages or something like that.",
                    "label": 1
                },
                {
                    "sent": "So high scalability is obviously important, but in terms of predictive performance, the methods that seem to do very well.",
                    "label": 0
                },
                {
                    "sent": "Kernel methods such as the SVM or Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "For example, are nonparametric an Moreover?",
                    "label": 0
                },
                {
                    "sent": "You know, even the sort of formally parametric models these large networks or deep networks of different kinds have so many parameters that effectively their operating in an almost non parametric regime.",
                    "label": 0
                },
                {
                    "sent": "They're very very flexible models.",
                    "label": 0
                },
                {
                    "sent": "And even simple things that are very easy to apply.",
                    "label": 0
                },
                {
                    "sent": "Something like K nearest neighbors is incredibly good in terms of predictive performance and a good thing always to compare two.",
                    "label": 0
                },
                {
                    "sent": "And the reason is good as a predictive model is because it's a nonparametric model ascential.",
                    "label": 0
                },
                {
                    "sent": "OK, this is kind of a hand WAVY reason to be nonparametric, but you know, really, the motivations are along these lines.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, there have been.",
                    "label": 0
                },
                {
                    "sent": "There's been decades of work in Bayesian nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "Within statistics from around the 70s or so.",
                    "label": 0
                },
                {
                    "sent": "And that that has grown a lot in recent years.",
                    "label": 0
                },
                {
                    "sent": "And it's also it's grown not only in statistics, but it's grown a lot in recent years in machine learning.",
                    "label": 0
                },
                {
                    "sent": "And if you're not familiar with this field.",
                    "label": 0
                },
                {
                    "sent": "Then you pick up a typical paper that uses Bayesian nonparametrics.",
                    "label": 1
                },
                {
                    "sent": "There is some complicated sounding process being mentioned, etc, and if you're new to this area it can feel very obscure like why are they using this thing and what do Chinese restaurant tables have to do with anything?",
                    "label": 0
                },
                {
                    "sent": "And you know what the hell is going on so.",
                    "label": 0
                },
                {
                    "sent": "The reason a lot of different stochastic processes are being invoked in this field of Bayesian nonparametrics, at least from the applied side.",
                    "label": 0
                },
                {
                    "sent": "Of course, these are all very beautiful objects to study from a probability theory point of view, but from the applied side, the reason these things are being used is that they correspond to very nice and simple components that you can use in a variety of different models for different modeling goals.",
                    "label": 0
                },
                {
                    "sent": "So for example, a lot of interesting problems correspond to assuming that there is some unknown function somewhere in your model OK, and so now if I have an unknown function and I don't want to use something simple and parametric like a polynomial, then I want to look for distributions on functions and a particularly nice family or distributions on functions is a Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "It's a nonparametric.",
                    "label": 0
                },
                {
                    "sent": "Model for functions that can be plugged into a variety of modeling scenarios, so you'll see a lot a lot of papers on Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "I'm sure there are a number of papers at AI stats on Gaussian processes as well.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you have an unknown distribution somewhere in your model, then there are various things that you could use the most classical one being a Dirichlet process, and again you'll see a lot of Dirichlet processes in papers within statistics and machine learning.",
                    "label": 0
                },
                {
                    "sent": "In statistics, there actually processes tend to be used more when you have some unknown distribution.",
                    "label": 0
                },
                {
                    "sent": "But often in machine learning, dearsley process and their associated combinatorial stochastic process called the Chinese restaurant process, are often used for clustering problems.",
                    "label": 0
                },
                {
                    "sent": "And you know, I'll mention some of these things in a bit more detail, but I won't go actually into dishley processes very much, because that was covered a lot during the summer school already.",
                    "label": 0
                },
                {
                    "sent": "So for those people, because of those people, I won't go into that that much.",
                    "label": 0
                },
                {
                    "sent": "But for anybody who's interested in following this up, of course there are excellent video lectures on dearsley processes in great depth, including from the summer school, so lookout for those.",
                    "label": 0
                },
                {
                    "sent": "Again, you know there are many different modeling goals one could have.",
                    "label": 0
                },
                {
                    "sent": "You might want to do a hierarchical clustering and there are things like the Dursley diffusion tree or kingman's coalescent.",
                    "label": 0
                },
                {
                    "sent": "Our model.",
                    "label": 0
                },
                {
                    "sent": "Might need a sparse binary matrix in it somewhere, at least in a nonparametric context.",
                    "label": 0
                },
                {
                    "sent": "You might want a sparse binary matrix with a growing number of columns, let's say.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about the Indian buffet process that does that.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric Bayesian papers on survival analysis will tend to refer to things like the beta process etc etc.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So there is a zoo of different things out there, but there are good reasons why people use these different things.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The two most widely used stochastic processes in Bayesian nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "Are the gaussian.",
                    "label": 0
                },
                {
                    "sent": "Process and Additionally process.",
                    "label": 0
                },
                {
                    "sent": "So just in at a high level cartoon, let me describe what these things are.",
                    "label": 0
                },
                {
                    "sent": "Gaussian process defines the distribution on functions.",
                    "label": 1
                },
                {
                    "sent": "So here is a single sample drawn from a Gaussian process, shown as a little plot here.",
                    "label": 0
                },
                {
                    "sent": "The way we would write that is that this.",
                    "label": 0
                },
                {
                    "sent": "Function F is drawn from a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "With mean function mu and covariance function C. So when we write this, we can think exactly analogously to writing something like vector X is drawn from a multivariate normal with mean mu and covariance matrix C. Except that instead of a vector, what you draw from a Gaussian process is a function which you could think of as an infinite dimensional vector.",
                    "label": 1
                },
                {
                    "sent": "In fact, one that has potentially uncountably many dimensions.",
                    "label": 0
                },
                {
                    "sent": "But the role of the mean and covariance functions is exactly the extension of the role of the mean vector and covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "The mean tells you on average what these functions will look like.",
                    "label": 0
                },
                {
                    "sent": "And the covariance function tells you how much the function values will Co vary with each other, possibly as a function of the distance between.",
                    "label": 0
                },
                {
                    "sent": "Say to be more precise, consider F evaluated X&F, evaluated at some other point X prime.",
                    "label": 0
                },
                {
                    "sent": "Those two F values are generally going to be correlated and the covariance function is a function of X&X prime that tells you how much those two F values will covary.",
                    "label": 0
                },
                {
                    "sent": "I'll say a little bit more about that.",
                    "label": 1
                },
                {
                    "sent": "But let me move on to their sleep process.",
                    "label": 0
                },
                {
                    "sent": "Completely analogously to this Adi Richley process defines a distribution on distributions, so G the object that we're drawing here is distribution I've just drawn.",
                    "label": 0
                },
                {
                    "sent": "I've drawn a histogram of a kind of finitely binned histogram of a distribution distribution.",
                    "label": 0
                },
                {
                    "sent": "Here is.",
                    "label": 1
                },
                {
                    "sent": "Is simply a non negative function that integrates to one.",
                    "label": 1
                },
                {
                    "sent": "We could think of it that way and the deer shape process has analogous to the Gaussian process parameter here G not, which is the base measure which controls the mean of G and a single scalar parameter Alpha called the concentration or scaling parameter which controls the variability around that mean.",
                    "label": 0
                },
                {
                    "sent": "Of G. OK.",
                    "label": 0
                },
                {
                    "sent": "So just like the Gaussian process is an infinite dimensional Gaussian, we can think of the Dirichlet process.",
                    "label": 0
                },
                {
                    "sent": "As an infinite dimensional Dirichlet distribution.",
                    "label": 0
                },
                {
                    "sent": "OK. No, I just wanted to mention a little bit about Gaussian process again because this was covered in great depth during the summer school.",
                    "label": 0
                },
                {
                    "sent": "I won't go into this in any more detail other than to say one of the things I really like to do is to try to understand the relationships between different sorts of methods, and we can think of Gaussian processes as being very closely related to kernel methods that are so familiar within.",
                    "label": 0
                },
                {
                    "sent": "Machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "Consider the following cube, which relates eight different classes of models to each other and the way I've drawn this cube is by starting from a really simple thing which is linear regression.",
                    "label": 0
                },
                {
                    "sent": "So everybody here knows what linear regression is right now.",
                    "label": 0
                },
                {
                    "sent": "Can do different operations to that model.",
                    "label": 0
                },
                {
                    "sent": "Linear regression an from that we get different kinds of models coming out.",
                    "label": 1
                },
                {
                    "sent": "For example, we can take a linear regression model and we can turn it into a classification model by taking the outputs of the linear regression and running them through a logistic function or a probate function.",
                    "label": 0
                },
                {
                    "sent": "So for example, an if you follow these.",
                    "label": 0
                },
                {
                    "sent": "These purple arrows you go from linear regression to the corresponding linear classification models such as logistic regression.",
                    "label": 1
                },
                {
                    "sent": "This is an example of something that you would get here.",
                    "label": 0
                },
                {
                    "sent": "You can take that linear regression model and you can kernel eyes it.",
                    "label": 0
                },
                {
                    "sent": "To get a nonlinear model in other words, imagine doing linear regression, but first mapping your data into some very high or infinite dimensional feature space.",
                    "label": 0
                },
                {
                    "sent": "That's what kernel regression corresponds to an the process of kernel Ising is following these orange arrows.",
                    "label": 0
                },
                {
                    "sent": "You can take that linear regression and instead of fitting the parameters of that linear regression with least squares or maximum likelihood, or something like that, imagine Now doing Bayesian inference over those parameters, and this gives you Bayesian linear regression.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are three operations we can do to linear regression.",
                    "label": 0
                },
                {
                    "sent": "But now we can apply multiple of these operations and.",
                    "label": 0
                },
                {
                    "sent": "Get other models so a Gaussian process regression model can be thought of either as a kernelized Bayesian linear regression.",
                    "label": 0
                },
                {
                    "sent": "Or it can be thought of as a Bayesian version of kernel regression.",
                    "label": 0
                },
                {
                    "sent": "K equivalently.",
                    "label": 0
                },
                {
                    "sent": "We could take kernel a, kernelized regression model and turn it into a kernel classification model such As for example, the support vector machine by following this arrow.",
                    "label": 0
                },
                {
                    "sent": "We could have gotten that also by taking.",
                    "label": 0
                },
                {
                    "sent": "A classification model such as logistic regression and kernel Ising that.",
                    "label": 0
                },
                {
                    "sent": "And then if we apply all three of these operations in different orders, then we arrive here.",
                    "label": 0
                },
                {
                    "sent": "Which is a classification version of Gaussian process regression and that can be thought of as a Bayesian version of kernel classification.",
                    "label": 0
                },
                {
                    "sent": "Or in other words, you know.",
                    "label": 0
                },
                {
                    "sent": "Take for example this.",
                    "label": 0
                },
                {
                    "sent": "Or vector machine?",
                    "label": 0
                },
                {
                    "sent": "If I look at the support vector machine and I say alright, how would I do that same sort of thing from a Bayesian framework?",
                    "label": 0
                },
                {
                    "sent": "Then the natural answer is to do Gaussian process classification.",
                    "label": 0
                },
                {
                    "sent": "Any questions about that?",
                    "label": 0
                },
                {
                    "sent": "Nope.",
                    "label": 0
                },
                {
                    "sent": "OK, in particular the point.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I really want to make if you're not already familiar with it is that this covariance function here?",
                    "label": 0
                },
                {
                    "sent": "Plays exactly the same rule as role as the Mercer kernel does in.",
                    "label": 0
                },
                {
                    "sent": "Support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So any kernel that you could use in an SVM.",
                    "label": 0
                },
                {
                    "sent": "By the fact that it's a valid kernel could also be used in a Gaussian process, and the interpretation of that is to think of that kernel representing.",
                    "label": 0
                },
                {
                    "sent": "The way in which your function varies with space OK. Or distribution over your function.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so let me move on.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From there, because essentially I'm not going to spend anymore time talking about these very, very well studied models like the Gaussian process and the Dirichlet process.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to talk about for the rest of the tutorial is.",
                    "label": 0
                },
                {
                    "sent": "How to take these Bayesian nonparametric ideas and apply them in other domains with other kinds of structured objects and data?",
                    "label": 0
                },
                {
                    "sent": "In particular, I get to talk about.",
                    "label": 0
                },
                {
                    "sent": "Doing time series modeling with hidden Markov models, I'm going to talk about models that involve sparse matrices and the uses of those.",
                    "label": 1
                },
                {
                    "sent": "I'm also going to talk about covariances.",
                    "label": 0
                },
                {
                    "sent": "I didn't mention that here, and finally I'm going to talk about modeling networks like biological or social networks using nonparametric ideas.",
                    "label": 0
                },
                {
                    "sent": "So how am I doing?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's let's talk about time series.",
                    "label": 0
                },
                {
                    "sent": "I think we'll we'll take a break after an hour, so.",
                    "label": 0
                },
                {
                    "sent": "I'll let you know when that is OK, so let's talk about time series.",
                    "label": 0
                },
                {
                    "sent": "Now there's a huge amount of work on different kinds of time series models.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to focus on models that are based on the idea of a discrete state variable capturing the time series.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These models are called hidden Markov models.",
                    "label": 1
                },
                {
                    "sent": "So hidden Markov models have been used in vast number of different applications.",
                    "label": 0
                },
                {
                    "sent": "Notably, it's speech recognition, bioinformatics about physics, text modeling, video, etc.",
                    "label": 0
                },
                {
                    "sent": "But the basic structure of the hidden Markov model is fairly easy to understand and get intuitions about.",
                    "label": 0
                },
                {
                    "sent": "What a hidden Markov model does is.",
                    "label": 0
                },
                {
                    "sent": "It tries to model sequences of observations, so Y one through YT is a sequence of observations where the index is presenting time or location along some other kind of sequence.",
                    "label": 0
                },
                {
                    "sent": "It could be a text sequence or biological sequence or something like that.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have to be time, but we're going to use the index T because usually we think about them as time.",
                    "label": 0
                },
                {
                    "sent": "What a hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "Says is we're going to model that sequence of observations Y by assuming that it was generated from a sequence of hidden states S1 through St.",
                    "label": 1
                },
                {
                    "sent": "Furthermore, the hidden state sequence satisfies the Markov assumption, which means that that state variable.",
                    "label": 0
                },
                {
                    "sent": "S of T. Captures everything about the past of the sequence that's needed for predicting the future of the sequence.",
                    "label": 0
                },
                {
                    "sent": "So it's a summary of the past history of this time series, OK?",
                    "label": 0
                },
                {
                    "sent": "So the Markov assumption represented as a directed graphical model looks like this.",
                    "label": 0
                },
                {
                    "sent": "It's basically showing you that this variable here renders this part of the graph conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "That's called that the future of the graph conditionally independent from the past of the graph through these arrows.",
                    "label": 0
                },
                {
                    "sent": "OK, now in.",
                    "label": 0
                },
                {
                    "sent": "In a.",
                    "label": 0
                },
                {
                    "sent": "Classical hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "The basic assumption is that your states are discrete and that your state variable can take on say up to K values.",
                    "label": 0
                },
                {
                    "sent": "So we'll do note that as integers one through K. By the Markov assumption, what that means is that the dynamics of the system is captured through some K by K transition matrix.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "There are many ways of thinking about the hidden Markov model, but one particularly nice way of thinking about the hidden Markov model is that at every time slice, what's going on is that the data is being clustered.",
                    "label": 0
                },
                {
                    "sent": "The variable S of T represents which of K possible clusters datapoint Y of T belongs in.",
                    "label": 0
                },
                {
                    "sent": "But it's a time dependent clustering or mixture model in the sense that S of T depends the cluster at time T depends on the cluster of the previous time step.",
                    "label": 0
                },
                {
                    "sent": "Now within the Bayesian nonparametrics literature.",
                    "label": 0
                },
                {
                    "sent": "It's been widely known that if you want to take a mixture model with K mixer components or clustering model with K clusters, and you want to consider a nonparametric version of that where K goes to Infinity, the possible number of clusters goes to Infinity, then the tool that's most widely useful for that is the Dirichlet process mixture.",
                    "label": 0
                },
                {
                    "sent": "So inspired by that knowledge of how to take one time slice model of this and make it nonparametric, we can now look at how to make the whole hidden Markov model nonparametric.",
                    "label": 0
                },
                {
                    "sent": "So how do we remove this upper bound K on the number of states?",
                    "label": 0
                },
                {
                    "sent": "How do we allow the hidden Markov model to have a countably infinite number of states?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the basic idea behind the infinite hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "It takes the previous model and it lets the number of hidden states go to Infinity.",
                    "label": 1
                },
                {
                    "sent": "And there are various ways of understanding what the behavior of this model is.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to tell you the detailed derivation of this model, but you know there are a lot of papers that you can look at for that, but I do want to give you a bit of intuition for why we do this and how we do this.",
                    "label": 0
                },
                {
                    "sent": "So let's first talk about why we do this and I'll make you focus on this plot here.",
                    "label": 0
                },
                {
                    "sent": "With this plot here shows you is.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the entire text of Alice in Wonderland, the novel.",
                    "label": 0
                },
                {
                    "sent": "OK, the way it shows you that text is the text goes from the beginning to the end of Alice in Wonderland there about, I guess.",
                    "label": 0
                },
                {
                    "sent": "A little over 25,000 words.",
                    "label": 0
                },
                {
                    "sent": "In Alice in Wonderland.",
                    "label": 0
                },
                {
                    "sent": "And what what we've plotted here is every time a word occurs, we give it a new identity.",
                    "label": 0
                },
                {
                    "sent": "So we the first word, we call it #1.",
                    "label": 0
                },
                {
                    "sent": "If that word occurs again, we are visiting word one again.",
                    "label": 0
                },
                {
                    "sent": "If we observe a new word, we give it a new index.",
                    "label": 0
                },
                {
                    "sent": "So the envelope that you see here is the word and identity as a function of.",
                    "label": 0
                },
                {
                    "sent": "The length of the text OK.",
                    "label": 0
                },
                {
                    "sent": "So what's the point of showing this?",
                    "label": 0
                },
                {
                    "sent": "Well, what's kind of interesting about this?",
                    "label": 0
                },
                {
                    "sent": "To me at least, is that new words are appearing all the time in this text.",
                    "label": 0
                },
                {
                    "sent": "OK, for the first time, they're appearing all the time, and if you wanted to have a model for.",
                    "label": 0
                },
                {
                    "sent": "Words in this case, then you would really want to model that is allowed to invoke knew words as the text gets longer and longer.",
                    "label": 0
                },
                {
                    "sent": "This is kind of part of the philosophy behind why we want a nonparametric model.",
                    "label": 0
                },
                {
                    "sent": "Similarly, if you wanted to do something like analyze news stories or topics of tweets or anything like that from a growing corpus, you really want to be able to have a model which can invoke mus structure.",
                    "label": 0
                },
                {
                    "sent": "As more and more data comes in.",
                    "label": 0
                },
                {
                    "sent": "So sort of inspired by this growing envelope of real world data.",
                    "label": 0
                },
                {
                    "sent": "What we want to have in our infinite hidden Markov model is the property that as if we sample sequence of hidden states from our infinite hidden Markov model, we want to get a behavior that looks something like this, which is the state that's visited on the Y axis as a function of time.",
                    "label": 0
                },
                {
                    "sent": "And the property that this has is that.",
                    "label": 0
                },
                {
                    "sent": "As you visit more and more as you go on for longer and longer, you will have visited more and more states, so this envelope grows as well.",
                    "label": 0
                },
                {
                    "sent": "Now you don't want, this is another sample from particular parameter settings of an infinite hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "This kind of gives you more left to right structure in HMM, but it's a little bit unsatisfying if your process has the property that every point in time it visits and you state OK.",
                    "label": 0
                },
                {
                    "sent": "So the first point in time it goes to state one, then without loss of generality, the second state, we could call it State 2.",
                    "label": 0
                },
                {
                    "sent": "State three, and so on.",
                    "label": 0
                },
                {
                    "sent": "It doesn't give you very interesting dynamics if you never revisit past states, so we really want a growing number of states that we can visit overtime, but we also want the ability in our model to have.",
                    "label": 0
                },
                {
                    "sent": "Structure within the state so that we can revisit old states so we have loops and states etc to get complicated dynamics.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "What I've shown here are just some samples from the prior over the state sequence of that infinite hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "That is interesting because as a Bayesian, one of the things you do when you develop a model or what you should do when you develop a model is forget about the data you want to be able to sample from.",
                    "label": 0
                },
                {
                    "sent": "The structure of that model from the prior and the predictions that it makes about the data without having observed any data.",
                    "label": 0
                },
                {
                    "sent": "So you can understand the properties of your model before you expose it to the data, because maybe the properties are not what you want.",
                    "label": 0
                },
                {
                    "sent": "Maybe you wanted this, but when you sample you get this.",
                    "label": 0
                },
                {
                    "sent": "And then you have to think did I choose the right structure of the model, the right hyperparameters, etc.",
                    "label": 0
                },
                {
                    "sent": "So this sort of idea was developed about 10 years ago, an.",
                    "label": 0
                },
                {
                    "sent": "Back then it was rather impractical, but over the last 10 years alot has happened in particular.",
                    "label": 0
                },
                {
                    "sent": "The model that we developed can be understood as.",
                    "label": 1
                },
                {
                    "sent": "Generating the parameters of the transition matrix from a hierarchical Dirichlet process.",
                    "label": 0
                },
                {
                    "sent": "So in the original hierarchical Dirichlet process, they showed that our bottle could be re derived as a from the HTTP framework.",
                    "label": 0
                },
                {
                    "sent": "They somewhat unhelpfully, I think maybe.",
                    "label": 0
                },
                {
                    "sent": "Change the name around a bit and called an HTP.",
                    "label": 1
                },
                {
                    "sent": "Hmm, but it turns out that it gives you exactly the same distribution over state sequences, so it's basically the same model.",
                    "label": 0
                },
                {
                    "sent": "But that gave a huge amount of insight into the infinite hmm and how it relates to Dirichlet processes, and also gave us a more efficient way of doing sampling from this model, but still not efficient to apply to very large datasets.",
                    "label": 0
                },
                {
                    "sent": "So what's happened over the last few years is that a lot of people have been trying to develop more and more efficient methods for doing inference in these models, and even parallel and distributed implementations.",
                    "label": 0
                },
                {
                    "sent": "So we can run these infinite HMMS on corpora with you know millions of data points.",
                    "label": 0
                },
                {
                    "sent": "So that's exciting because although it took about 10 years, we're now at a stage where we can apply these things practically, and there's sort of stable code for doing that, etc.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me show you a nice application.",
                    "label": 0
                },
                {
                    "sent": "I have an infinite hmm, or at least a couple of applications, so this is I will go actually into the application, but this is this is just an image of this sort of behavior that you would get out of applying an infinite hmm to some data.",
                    "label": 0
                },
                {
                    "sent": "So in this particular case we have a time series of 1 dimensional observations.",
                    "label": 0
                },
                {
                    "sent": "These are these dots and time is going along this axis and what the infinite hmm.",
                    "label": 0
                },
                {
                    "sent": "Can do is.",
                    "label": 0
                },
                {
                    "sent": "It infers the state that you're in at every point in time that's given by these colors.",
                    "label": 0
                },
                {
                    "sent": "But it also tries to discover how many states you need to model this time series.",
                    "label": 0
                },
                {
                    "sent": "So the fact that there are 1234 colors means that the infinite hmm here.",
                    "label": 1
                },
                {
                    "sent": "Importantly, think about this, although it has an unbounded number of states, it can potentially use.",
                    "label": 0
                },
                {
                    "sent": "It chose to use about four for this finite sequence.",
                    "label": 0
                },
                {
                    "sent": "What that means in terms of prediction is if we go further into the future and we observe some very weird behavior.",
                    "label": 0
                },
                {
                    "sent": "The information ma'am is now allowed to invoke and use state to account for that new behavior.",
                    "label": 0
                },
                {
                    "sent": "This is somewhat different from what seems a very similar thing to do so often when I talk about basing on parametrics, I guess asked how different is it to have an infinite model like these as compared to doing inference about the number of states in a finite model.",
                    "label": 0
                },
                {
                    "sent": "So we have a lot of good methods for figuring out how many states we should have in a finite hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "So why don't I just figure out how many states I have in a finite hidden Markov model?",
                    "label": 0
                },
                {
                    "sent": "Well in this case.",
                    "label": 0
                },
                {
                    "sent": "From this finite amount of data.",
                    "label": 0
                },
                {
                    "sent": "I would probably infer that there are 1234 states for finite hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "But that's a bit unsatisfactory, I feel, because essentially I've put then my.",
                    "label": 0
                },
                {
                    "sent": "In terms of predictions, I put my beliefs on this number 4, so if I observe future data and it doesn't fit in.",
                    "label": 0
                },
                {
                    "sent": "Then I still have to account for that by.",
                    "label": 0
                },
                {
                    "sent": "Saying it's one of the four states that I inferred from the data.",
                    "label": 0
                },
                {
                    "sent": "So inferring the size of a finite model philosophically is very, very different than starting out with an infinite model.",
                    "label": 0
                },
                {
                    "sent": "OK, let me just try to explain that again.",
                    "label": 0
                },
                {
                    "sent": "If I'm trying to infer.",
                    "label": 0
                },
                {
                    "sent": "A finite model.",
                    "label": 0
                },
                {
                    "sent": "What I'm actually saying from a subjective Bayesian point of view, is that I believe this real world process came from a finite hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "I just don't know what the order of that the number of states of that finite model is.",
                    "label": 0
                },
                {
                    "sent": "Let me learn that from data that's very different from, say, I don't believe that this real world process came from a finite hidden Markov model, but I believe that if I had.",
                    "label": 0
                },
                {
                    "sent": "Countably many countably infinitely many hidden states.",
                    "label": 0
                },
                {
                    "sent": "I would be able to cap to capture this dynamics.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's the infinite model infinite hmm applied to some 1 dimensional time series here.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you can see it, but these are frames from a video.",
                    "label": 0
                },
                {
                    "sent": "Of somebody playing on the Nintendo Wii.",
                    "label": 0
                },
                {
                    "sent": "OK, so they're doing different sorts of gestures on the Nintendo Wii.",
                    "label": 0
                },
                {
                    "sent": "And what we used here was an extension of the infinite hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "To try to segment this video into different gestures, for example batting, boxing, pitching and tennis.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's an application of an infinite hmm to video data, where again we don't know how many gestures there are.",
                    "label": 0
                },
                {
                    "sent": "This isn't a classification problem, we're trying to infer these high level gestures from the data.",
                    "label": 0
                },
                {
                    "sent": "Itself, now the modification of the infinite hidden Markov model that we use.",
                    "label": 0
                },
                {
                    "sent": "Developed by Tom Stapleton, is this idea of having an infinite hmm where the hidden states are grouped together into perhaps what you could call behaviors so so one group of hidden states.",
                    "label": 0
                },
                {
                    "sent": "This is more connected to each other and it's associated with one behavior.",
                    "label": 0
                },
                {
                    "sent": "Another group is associated with another behavior, etc.",
                    "label": 0
                },
                {
                    "sent": "So when you look at the transition matrix of that kind of infinite hidden Markov model, it has a block diagonal structure.",
                    "label": 0
                },
                {
                    "sent": "OK, with sub connections of course between the different behaviors.",
                    "label": 0
                },
                {
                    "sent": "So, or you could also think of this as a hidden Markov model with two levels of the hierarchy of hidden states.",
                    "label": 0
                },
                {
                    "sent": "You have big states corresponding to large scale behaviors, and then within those you have sub states corresponding to different little actions within that large scale behavior.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, I think maybe it's a good time for a 5 minute break.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}