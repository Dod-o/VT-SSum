{
    "id": "uxl2pak36ndvwwsqwspunfht6f7lqmp4",
    "title": "Do We Need More Training Data or Better Models for Object Detection?",
    "info": {
        "author": [
            "Charless C. Fowlkes, University of California, Irvine"
        ],
        "published": "Oct. 9, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/bmvc2012_fowlkes_object_detection/",
    "segmentation": [
        [
            "Today I'd like to describe a series of experiments that Jean Gen, Carl, Dave and I explored, which was more or less motivated in trying to understand kind of what is the state of affairs with object detection?",
            "You where, where can we go from here?",
            "And so in object detection."
        ],
        [
            "Trying to recognize and localize multiple objects from different categories within an image.",
            "An here localization is represented by bounding boxes.",
            "We might like to be a little bit more detail than that and actually give precise segmentations of these objects, but in general this is a sort of interesting version of image understanding, and if we want to say OK, how good are we at doing this?",
            "A reasonable place to look at is the Pascal V."
        ],
        [
            "To see detection challenge which you heard about in the last talk.",
            "And so here what I've plotted is over the last few years you know how is the best performing algorithm doing at this challenge.",
            "And maybe the first thing you say is, well, I look at this and I'm seeing these average precision of .3, and that's not too good.",
            "But I'm an optimist and when I look at this plot, I see things going up right?",
            "We're getting better.",
            "Computer vision research is working and detection performance is improving.",
            "And I think you know this is a reasonable kind of benchmark for object detection.",
            "These are realistic.",
            "Cluttered scenes.",
            "Of course, someone who's maybe not as optimistic as me would look at this and say, well, you're plotting the wrong thing here.",
            "You're plotting performance by year and what you really should pot is performance as a function of the amount of training data that these algorithms."
        ],
        [
            "So as this benchmark is has developed, the average number of training examples per class is also going up, and you know if I look at this plot, maybe I say this is even even better correlated, right that every time I give your algorithm more training data, it does better.",
            "And so maybe computer vision researchers don't have much to do here.",
            "And this is all just sort of machine learning.",
            "Solving the problem for us, right?",
            "So this is an intriguing thing.",
            "People like to argue.",
            "Computer vision, machine learning and so on.",
            "But you can.",
            "We can we understand this is?",
            "Is it really the case?",
            "That sort of all we need to do now is just collect a bunch of training data and detection will be solved?",
            "Or you know what remains to be done.",
            "So I sort of can't speak in full generality about any detection algorithm, but I'm going to focus on this sort of scanning."
        ],
        [
            "Window based classification, which is a prevalent and successful way to tackle this problem where we have some positive and negative training examples.",
            "We train up a classifier, in this case say just a linear SVM which looks at Hog features and makes a decision whether this window contains an object or not.",
            "And so I say, well, I'm looking at a classifier.",
            "What fundamentally will limit the performance of this classifier in detecting objects?",
            "And one way to think about this is just say, well, you know, given some particular set of features to these hog feature vectors, there's some fundamental limits on my classification performance which."
        ],
        [
            "Late to that choice of feature space, right?",
            "So if I choose Hog features then there may be certain things I sort of put on these these hog goals which view the world in terms of hog features.",
            "What can I distinguish out there in the world so I can say, well, this is a bike, but maybe I can't tell you what particular brand of bike it is, and so this sort of overlap between class conditional densities in my feature space limits my ultimate classification performance, but somehow you gave me enough training data I should be able to build a classifier which at least achieves this sort of Bayes risk does as well as I can.",
            "And so I could."
        ],
        [
            "Then kind of an abstract picture, whereas I gave you more training data.",
            "Performance should improve for awhile, but it's going to saturate at something like the Bayes risk.",
            "Now, of course, that's a little bit optimistic, because we don't necessarily know that our classifier we're using can actually specify that Bayes optimal disk."
        ],
        [
            "And boundary right?",
            "So any model we choose may not be able to carve out exactly that precise boundary, but it has some bias, so perhaps we."
        ],
        [
            "We choose a linear SVM which.",
            "Doesn't quite achieve that.",
            "That Bayes optimal score, but does something slightly less, and part of what's going on here is our model has some kind of fixed complexity to it.",
            "If we think about what happens, you say OK, if I fix the amount of training data and kind of vary the complexity of my model, then we see another sort of idealized curve from our machine learning text."
        ],
        [
            "Note that says you know as I vary that model complexity, there's some sort of sweet spot at which I matched the amount of training data I have, and if."
        ],
        [
            "Model is sort of too simple.",
            "Then I under fit.",
            "If it's too complex and I overfit to training and my final Test performance is limited, so these are all things that were very comfortable with sort of pictures we have in our heads.",
            "The question is what happens when we actually go out and start looking at real detection datasets and so the first thing I'm going to do is sort of take you through a series of experiments which we should all know the answers to, right?",
            "So here's the first one."
        ],
        [
            "Let's let's think about a face detection task.",
            "We're going to train a single rigid hug template to detect faces using a linear SVM and the experiment I'm going to do is very the amount of positive training data, right?",
            "So I'll start off, say, with 20 training examples and I get some average precision .4 on my test set.",
            "And now I say OK, what's going to happen as I increase NUM?"
        ],
        [
            "Train examples is performance.",
            "Go up or go down.",
            "So who votes for up?",
            "Alright, and who votes for down?",
            "So I get two hands there.",
            "Someone's anticipated this.",
            "Maybe they've looked at the abstract because of course."
        ],
        [
            "That was what actually happened, right?",
            "I sent a student off to do this and they came back and said your predictions all wrong.",
            "What do you know?",
            "So what's going on here?",
            "Well, what's going on here is that I didn't change anything in this SVM training."
        ],
        [
            "Regime, including this regularization parameter C right, and if you think about this SVM objective, what happens is I increase the number of training examples that some in the right hand term there it gets larger and so the effective degree of regularization actually decreases.",
            "It ends up in degrees, is faster than the amount of training data grows and so you actually have to change C as you vary the number of positive training examples.",
            "And so if I actually do cross validation on C then you get sort of this nice blue curve which was the thing we were expecting.",
            "Right, so this is still machine learning 101 and."
        ],
        [
            "We say OK, remember when you go off and actually vary the amount of training data you need to go back and adjust that training parameter C and that's that's maybe something we've forgotten.",
            "So here what we're plotting is sort of the optimal setting of C for different amounts of positive training data, right?",
            "You see, there's a sweet spot in that sweet spot is changing as we change the amount of training data.",
            "So that's that simple.",
            "Someone maybe who thought about this, could have told us this ahead of time.",
            "So let's let's do something a little more interesting.",
            "So here's experiment number."
        ],
        [
            "I wanted to check faces, but these faces appear in different viewpoints right summer frontal, summer, sideways.",
            "My test sets going to consist of faces of all sorts of different viewpoints.",
            "Now I ask you, you have a choice between two training datasets, one which is only frontal faces and one which is all views of faces and so who would prefer data set A which includes all viewpoints.",
            "Alright, hands up.",
            "Alright, how about data set be?",
            "So a few people you open the machine learning textbook.",
            "It says your training distribution should match your test distribution.",
            "That's how we know we're going to learn the right thing, but it ends up that."
        ],
        [
            "That's sort of wrong in this case that a single template trained with only a few 100 frontal faces actually outperforms a template trained with you 800 images that include all these different views.",
            "Right, so the blue curve here is trained on all the pink curve is trained on only frontal views right?",
            "And so again, we've somehow run into this problem where more training data seems to be hurting us rather than helping us right?",
            "And in fact, having this weird biased kind of training set is doing well and it ends up that this is not just some kind of generalization problem."
        ],
        [
            "Both the training and the test performance.",
            "Right or worse, when we train on all viewpoints.",
            "So what's going on here?",
            "Well, if you think about the SVM right, it's very."
        ],
        [
            "Sensitive to outliers, right?",
            "So here's our sort of picture of what the SVM charges us when we make a mistake, we have some data point which is on the wrong side of the decision boundary.",
            "The hinge loss grows linearly in that direction.",
            "So if I have a few data points, these sort of side views of faces that are very hard to classify all distort my template in a crazy way to try to correct them.",
            "Because I'm paying a huge loss for them, right?",
            "And Interestingly, if you actually go and look at some of the systems people use, it have mixtures of templates.",
            "You'll find there's templates that are sort of allocated as junk templates to explain that outline data, and you can actually see this."
        ],
        [
            "So here is these two templates.",
            "One train with all views and one with frontal views.",
            "And you say look the frontal view template actually looks much nicer.",
            "It performs much nicer in practice.",
            "Alright, so we have to be sensitive to sort of what the SVM seizes outliers and not an A practical way to approach this is to sort of add mixture components to our models.",
            "Now we're going to think about growing the model complexity to handle these different."
        ],
        [
            "Two points rather than trying to make them all fit with one template, and so we'll have a collection of templates, and in this paper we describe a couple of different experiments, weed."
        ],
        [
            "Did in order to drive those clusters.",
            "So one way is to go sort of a fully supervised route.",
            "So I take some annotations.",
            "Maybe you actually tell me the viewpoints, say in this case of different faces, or I can try unsupervised clustering.",
            "So just doing K means on the hog feature vectors themselves, and one thing that we're doing here is we're sort of doing a hierarchical clustering in order to have a nice sampling of the data, so that when we compare, say two clusters to four clusters, those four clusters, the refinement of the two clusters.",
            "And this is to try to sort of decrease the variance when we compare performance across varying numbers of clusters.",
            "So the first thing I'd like to point out here is that it ends up having a little human supervision and driving these."
        ],
        [
            "Clusters is valuable, so having kind of nice clean clusters which avoid sort of lumping together outliers can give you increased performance.",
            "Here's an example for faces, and then another example for buses, which again we had sort of a human, give us some supervision in the clustering process, and we actually get a few percent benefit in having that sort of nice, clean, supervised clusters.",
            "Again, do the sensitivity of outliers, and so now once we have sorted out these details of regularization."
        ],
        [
            "Clustering, then we start to get these idealized pictures that we started out with right where as the amount of training data increases.",
            "Here I'm showing for mixture models with different numbers of mixtures.",
            "We see this general kind of flattening of performance as we increase the amount of training data as some coding at some level of performance.",
            "On the other hand, here's another way to slice it on the right where I say let's fix the amount of training data.",
            "Now vary the model complexity in terms of the number of mixtures, and I find that there's some sweet spot right where.",
            "So I have maybe 100 training examples.",
            "Per mixture component gives me sort of the best tradeoff between having model complexity and having enough training data to learn each model well.",
            "So we said, OK, this is working.",
            "What are we going to do now?",
            "We see this increased performance with increased amount of training data.",
            "Let's go off and see if we can act."
        ],
        [
            "Win the Pascal challenge by just taking this stupid strategy of adding more and more training data.",
            "Alright, So what do we do?",
            "We went off and collected basically 10 times as much training data as there was in the basic Pascal training data set and the way you do this is you sort of follow the same protocol that Pascal used to collect their data.",
            "Go to Flickr, collect images and then use some mechanical Turk workers to sort of filter through and give you bounding boxes.",
            "And so we've produced a data set.",
            "We only ended up doing it for 10 of the Pascal categories, but for those ten categories we have roughly.",
            "10 times as much training data and we were all very excited to go off and do this and so we."
        ],
        [
            "And we see well.",
            "So here again, I'm plotting this sort of performance versus number of training examples, and we've sort of done the cross validation to choose the regularization, the optimal number of mixer components for a given amount of training data.",
            "And again, we see this sort of behavior that we need roughly 100 positive training examples per mixture component.",
            "But the disappointing thing is that performance sort of saturates at about templates 10 templates for category, right?",
            "So if you're familiar.",
            "With Pascal you say, well, you know these numbers aren't a lot higher than anything that people submit to this competition.",
            "In fact, you know there was no need to go over here to this sort of 10 times as much data set, right?",
            "Things were saturating back here over at the sort of 2000 instead of 10,000.",
            "So all that data collection was in vain.",
            "This was somehow a dead end and you say, OK, well can we?",
            "Can we salvage this in some way?",
            "Can we at least say something?",
            "About this basic problem of detection, right so."
        ],
        [
            "So one way of looking at this, you say, well, you've grown the model complexity.",
            "You've grown the amount of training data you've hit, some saturation point, and so is it.",
            "The case that we're just sort of looking at this this fundamental limit of Bayes risk for the hog feature that we just can't see anymore detail within these images.",
            "So who thinks we've hit the Bayes risk?",
            "No one, so that's good.",
            "One reason you might know that is that if you're familiar with Pascal, you say, well, those numbers you just showed me aren't actually that good."
        ],
        [
            "And there's models out there which use the hog feature spaces input, but actually give much better performance.",
            "And so here is 1 which is quite well known.",
            "The deformable part model, right?",
            "So this represents the appearance is a collection of parts, each of which is sort of a discriminatively trained template that uses hog feature vectors as input, but allows those parts to wiggle around in the image right?",
            "And I'm going to describe an experiment in which we actually use a sort of fully supervised version of this, so we tell the model where the parts are in each training example.",
            "And so the point here is that this is using the same input feature space, but it has some sort of richer model under the hood to perform the classification, and as a result it can do much better.",
            "Right, so so if you think about this, this is a little bit mysterious because I described a model in which we added more and more rigid templates.",
            "And if you think about the deformable part model, one way to think about the deformable part model is just as a very very, very large collection of rigid temple."
        ],
        [
            "Right, so usually we think about a deformable part models where we sort of set it down on the image and wiggle the parts around to match things up.",
            "But instead you could think about every possible placement of parts as synthesizing a rigid template and all the deformable part model does is sort of score that exponentially large collection of rigid templates on an image and pick out the one which scores maximally.",
            "So it really is a sort of mixture model, except that it has this nice computational machinery of dynamic programming which lets you very rapidly.",
            "Index through this kind of exponentially large set of rigid templates and of course each template has its own bias which is set by how much you had to stretch the Springs to position it right.",
            "So from this point of view you say well, the deformable part model is also a collection of rigid templates.",
            "Why didn't you get there with this simple rigid?"
        ],
        [
            "Plates and so there's there's maybe two reasons this could be right, so one is that when you train the model, all those rigid templates aren't independent of each other, right?",
            "The parts themselves have the same appearance, they just get moved around a different location.",
            "So you can think of some how the parameters of those part appearances are tide.",
            "And of course the other thing is that the model can extrapolate to things you haven't seen before, right?",
            "So it's not enough to have kind of a template for every exemplar that you've observed in training.",
            "But now you're going to extrapolate to new.",
            "Conditions and so.",
            "The final experiment I want to describe is something which is trying to tease apart these two different contributions which we call a rigid mixture of parts.",
            "So we're going to learn part appearance temp."
        ],
        [
            "It's from supervised annotations.",
            "We're going to have one mixture component for sort of each spatial configuration seen in training, but we aren't going to do this extrapolation, right?",
            "So we won't consider new placements of the parts that we haven't seen in training, so this again is just a large mixture of appearances.",
            "It has the sharing of parameters, but no extrapolation, and so now we ask the question, which of these two elements contributes to."
        ],
        [
            "Norman, so here is again this sort of plot on faces of the number of training examples at the bottom.",
            "The blue curve is a single rigid template.",
            "The green curve is our sort of best mixture of templates where we add mixtures.",
            "Red on top is the supervised different part model and the cyan curve is this intermediate right in which we share parameters.",
            "We share part appearances, but we don't extrapolate the new configurations and so we see that roughly half and half right.",
            "Both of these things contribute to the performance of the deferrable part model in.",
            "Tackling new images and so the final thing, I just want to point out is that you know this.",
            "This idea of sharing parameters across part."
        ],
        [
            "Places are really powerful thing, so here is sort of an example of this face detection model being run on face detection in real world images and the red curve here is it's deformable part models, essentially trained with something like 100 training examples, but where you have detailed annotations and compared to something like Google picasaorface.com which we know had probably millions of training examples, this sort of very simple.",
            "In terms of training data set complexity, but intelligently thought out sharing of parameters as well.",
            "So just to."
        ],
        [
            "Include you say, well, more training data helps, but only if you're careful so you can't be naive about this.",
            "You've got to do regularization in the right way.",
            "You need sort of clean training data to protect the SVM from outliers and having the proper correspondence is actually key, and we sort of summarize and say somehow the biggest gains we see are not from increasing the amount of training data, but for more intelligent design of the model, that is having a richer representation.",
            "So again, our models in the end compile down to just linear classifiers run on hog feature space.",
            "But the parameters are shared in an intelligent way and so."
        ],
        [
            "That I'll thank you and my coauthors."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I'd like to describe a series of experiments that Jean Gen, Carl, Dave and I explored, which was more or less motivated in trying to understand kind of what is the state of affairs with object detection?",
                    "label": 0
                },
                {
                    "sent": "You where, where can we go from here?",
                    "label": 0
                },
                {
                    "sent": "And so in object detection.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trying to recognize and localize multiple objects from different categories within an image.",
                    "label": 1
                },
                {
                    "sent": "An here localization is represented by bounding boxes.",
                    "label": 0
                },
                {
                    "sent": "We might like to be a little bit more detail than that and actually give precise segmentations of these objects, but in general this is a sort of interesting version of image understanding, and if we want to say OK, how good are we at doing this?",
                    "label": 0
                },
                {
                    "sent": "A reasonable place to look at is the Pascal V.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To see detection challenge which you heard about in the last talk.",
                    "label": 0
                },
                {
                    "sent": "And so here what I've plotted is over the last few years you know how is the best performing algorithm doing at this challenge.",
                    "label": 0
                },
                {
                    "sent": "And maybe the first thing you say is, well, I look at this and I'm seeing these average precision of .3, and that's not too good.",
                    "label": 0
                },
                {
                    "sent": "But I'm an optimist and when I look at this plot, I see things going up right?",
                    "label": 0
                },
                {
                    "sent": "We're getting better.",
                    "label": 0
                },
                {
                    "sent": "Computer vision research is working and detection performance is improving.",
                    "label": 0
                },
                {
                    "sent": "And I think you know this is a reasonable kind of benchmark for object detection.",
                    "label": 0
                },
                {
                    "sent": "These are realistic.",
                    "label": 0
                },
                {
                    "sent": "Cluttered scenes.",
                    "label": 0
                },
                {
                    "sent": "Of course, someone who's maybe not as optimistic as me would look at this and say, well, you're plotting the wrong thing here.",
                    "label": 0
                },
                {
                    "sent": "You're plotting performance by year and what you really should pot is performance as a function of the amount of training data that these algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as this benchmark is has developed, the average number of training examples per class is also going up, and you know if I look at this plot, maybe I say this is even even better correlated, right that every time I give your algorithm more training data, it does better.",
                    "label": 1
                },
                {
                    "sent": "And so maybe computer vision researchers don't have much to do here.",
                    "label": 0
                },
                {
                    "sent": "And this is all just sort of machine learning.",
                    "label": 0
                },
                {
                    "sent": "Solving the problem for us, right?",
                    "label": 0
                },
                {
                    "sent": "So this is an intriguing thing.",
                    "label": 0
                },
                {
                    "sent": "People like to argue.",
                    "label": 0
                },
                {
                    "sent": "Computer vision, machine learning and so on.",
                    "label": 0
                },
                {
                    "sent": "But you can.",
                    "label": 0
                },
                {
                    "sent": "We can we understand this is?",
                    "label": 0
                },
                {
                    "sent": "Is it really the case?",
                    "label": 1
                },
                {
                    "sent": "That sort of all we need to do now is just collect a bunch of training data and detection will be solved?",
                    "label": 0
                },
                {
                    "sent": "Or you know what remains to be done.",
                    "label": 0
                },
                {
                    "sent": "So I sort of can't speak in full generality about any detection algorithm, but I'm going to focus on this sort of scanning.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Window based classification, which is a prevalent and successful way to tackle this problem where we have some positive and negative training examples.",
                    "label": 0
                },
                {
                    "sent": "We train up a classifier, in this case say just a linear SVM which looks at Hog features and makes a decision whether this window contains an object or not.",
                    "label": 0
                },
                {
                    "sent": "And so I say, well, I'm looking at a classifier.",
                    "label": 0
                },
                {
                    "sent": "What fundamentally will limit the performance of this classifier in detecting objects?",
                    "label": 0
                },
                {
                    "sent": "And one way to think about this is just say, well, you know, given some particular set of features to these hog feature vectors, there's some fundamental limits on my classification performance which.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Late to that choice of feature space, right?",
                    "label": 1
                },
                {
                    "sent": "So if I choose Hog features then there may be certain things I sort of put on these these hog goals which view the world in terms of hog features.",
                    "label": 0
                },
                {
                    "sent": "What can I distinguish out there in the world so I can say, well, this is a bike, but maybe I can't tell you what particular brand of bike it is, and so this sort of overlap between class conditional densities in my feature space limits my ultimate classification performance, but somehow you gave me enough training data I should be able to build a classifier which at least achieves this sort of Bayes risk does as well as I can.",
                    "label": 0
                },
                {
                    "sent": "And so I could.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then kind of an abstract picture, whereas I gave you more training data.",
                    "label": 0
                },
                {
                    "sent": "Performance should improve for awhile, but it's going to saturate at something like the Bayes risk.",
                    "label": 1
                },
                {
                    "sent": "Now, of course, that's a little bit optimistic, because we don't necessarily know that our classifier we're using can actually specify that Bayes optimal disk.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And boundary right?",
                    "label": 0
                },
                {
                    "sent": "So any model we choose may not be able to carve out exactly that precise boundary, but it has some bias, so perhaps we.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We choose a linear SVM which.",
                    "label": 0
                },
                {
                    "sent": "Doesn't quite achieve that.",
                    "label": 0
                },
                {
                    "sent": "That Bayes optimal score, but does something slightly less, and part of what's going on here is our model has some kind of fixed complexity to it.",
                    "label": 0
                },
                {
                    "sent": "If we think about what happens, you say OK, if I fix the amount of training data and kind of vary the complexity of my model, then we see another sort of idealized curve from our machine learning text.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Note that says you know as I vary that model complexity, there's some sort of sweet spot at which I matched the amount of training data I have, and if.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model is sort of too simple.",
                    "label": 0
                },
                {
                    "sent": "Then I under fit.",
                    "label": 0
                },
                {
                    "sent": "If it's too complex and I overfit to training and my final Test performance is limited, so these are all things that were very comfortable with sort of pictures we have in our heads.",
                    "label": 0
                },
                {
                    "sent": "The question is what happens when we actually go out and start looking at real detection datasets and so the first thing I'm going to do is sort of take you through a series of experiments which we should all know the answers to, right?",
                    "label": 0
                },
                {
                    "sent": "So here's the first one.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's let's think about a face detection task.",
                    "label": 0
                },
                {
                    "sent": "We're going to train a single rigid hug template to detect faces using a linear SVM and the experiment I'm going to do is very the amount of positive training data, right?",
                    "label": 1
                },
                {
                    "sent": "So I'll start off, say, with 20 training examples and I get some average precision .4 on my test set.",
                    "label": 0
                },
                {
                    "sent": "And now I say OK, what's going to happen as I increase NUM?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Train examples is performance.",
                    "label": 0
                },
                {
                    "sent": "Go up or go down.",
                    "label": 0
                },
                {
                    "sent": "So who votes for up?",
                    "label": 0
                },
                {
                    "sent": "Alright, and who votes for down?",
                    "label": 0
                },
                {
                    "sent": "So I get two hands there.",
                    "label": 0
                },
                {
                    "sent": "Someone's anticipated this.",
                    "label": 0
                },
                {
                    "sent": "Maybe they've looked at the abstract because of course.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That was what actually happened, right?",
                    "label": 0
                },
                {
                    "sent": "I sent a student off to do this and they came back and said your predictions all wrong.",
                    "label": 0
                },
                {
                    "sent": "What do you know?",
                    "label": 0
                },
                {
                    "sent": "So what's going on here?",
                    "label": 0
                },
                {
                    "sent": "Well, what's going on here is that I didn't change anything in this SVM training.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regime, including this regularization parameter C right, and if you think about this SVM objective, what happens is I increase the number of training examples that some in the right hand term there it gets larger and so the effective degree of regularization actually decreases.",
                    "label": 0
                },
                {
                    "sent": "It ends up in degrees, is faster than the amount of training data grows and so you actually have to change C as you vary the number of positive training examples.",
                    "label": 0
                },
                {
                    "sent": "And so if I actually do cross validation on C then you get sort of this nice blue curve which was the thing we were expecting.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is still machine learning 101 and.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We say OK, remember when you go off and actually vary the amount of training data you need to go back and adjust that training parameter C and that's that's maybe something we've forgotten.",
                    "label": 1
                },
                {
                    "sent": "So here what we're plotting is sort of the optimal setting of C for different amounts of positive training data, right?",
                    "label": 0
                },
                {
                    "sent": "You see, there's a sweet spot in that sweet spot is changing as we change the amount of training data.",
                    "label": 0
                },
                {
                    "sent": "So that's that simple.",
                    "label": 0
                },
                {
                    "sent": "Someone maybe who thought about this, could have told us this ahead of time.",
                    "label": 0
                },
                {
                    "sent": "So let's let's do something a little more interesting.",
                    "label": 0
                },
                {
                    "sent": "So here's experiment number.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I wanted to check faces, but these faces appear in different viewpoints right summer frontal, summer, sideways.",
                    "label": 0
                },
                {
                    "sent": "My test sets going to consist of faces of all sorts of different viewpoints.",
                    "label": 1
                },
                {
                    "sent": "Now I ask you, you have a choice between two training datasets, one which is only frontal faces and one which is all views of faces and so who would prefer data set A which includes all viewpoints.",
                    "label": 1
                },
                {
                    "sent": "Alright, hands up.",
                    "label": 0
                },
                {
                    "sent": "Alright, how about data set be?",
                    "label": 0
                },
                {
                    "sent": "So a few people you open the machine learning textbook.",
                    "label": 0
                },
                {
                    "sent": "It says your training distribution should match your test distribution.",
                    "label": 0
                },
                {
                    "sent": "That's how we know we're going to learn the right thing, but it ends up that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's sort of wrong in this case that a single template trained with only a few 100 frontal faces actually outperforms a template trained with you 800 images that include all these different views.",
                    "label": 1
                },
                {
                    "sent": "Right, so the blue curve here is trained on all the pink curve is trained on only frontal views right?",
                    "label": 0
                },
                {
                    "sent": "And so again, we've somehow run into this problem where more training data seems to be hurting us rather than helping us right?",
                    "label": 0
                },
                {
                    "sent": "And in fact, having this weird biased kind of training set is doing well and it ends up that this is not just some kind of generalization problem.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Both the training and the test performance.",
                    "label": 1
                },
                {
                    "sent": "Right or worse, when we train on all viewpoints.",
                    "label": 0
                },
                {
                    "sent": "So what's going on here?",
                    "label": 0
                },
                {
                    "sent": "Well, if you think about the SVM right, it's very.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sensitive to outliers, right?",
                    "label": 0
                },
                {
                    "sent": "So here's our sort of picture of what the SVM charges us when we make a mistake, we have some data point which is on the wrong side of the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "The hinge loss grows linearly in that direction.",
                    "label": 0
                },
                {
                    "sent": "So if I have a few data points, these sort of side views of faces that are very hard to classify all distort my template in a crazy way to try to correct them.",
                    "label": 0
                },
                {
                    "sent": "Because I'm paying a huge loss for them, right?",
                    "label": 0
                },
                {
                    "sent": "And Interestingly, if you actually go and look at some of the systems people use, it have mixtures of templates.",
                    "label": 0
                },
                {
                    "sent": "You'll find there's templates that are sort of allocated as junk templates to explain that outline data, and you can actually see this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is these two templates.",
                    "label": 0
                },
                {
                    "sent": "One train with all views and one with frontal views.",
                    "label": 1
                },
                {
                    "sent": "And you say look the frontal view template actually looks much nicer.",
                    "label": 0
                },
                {
                    "sent": "It performs much nicer in practice.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we have to be sensitive to sort of what the SVM seizes outliers and not an A practical way to approach this is to sort of add mixture components to our models.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to think about growing the model complexity to handle these different.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two points rather than trying to make them all fit with one template, and so we'll have a collection of templates, and in this paper we describe a couple of different experiments, weed.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did in order to drive those clusters.",
                    "label": 0
                },
                {
                    "sent": "So one way is to go sort of a fully supervised route.",
                    "label": 0
                },
                {
                    "sent": "So I take some annotations.",
                    "label": 0
                },
                {
                    "sent": "Maybe you actually tell me the viewpoints, say in this case of different faces, or I can try unsupervised clustering.",
                    "label": 0
                },
                {
                    "sent": "So just doing K means on the hog feature vectors themselves, and one thing that we're doing here is we're sort of doing a hierarchical clustering in order to have a nice sampling of the data, so that when we compare, say two clusters to four clusters, those four clusters, the refinement of the two clusters.",
                    "label": 0
                },
                {
                    "sent": "And this is to try to sort of decrease the variance when we compare performance across varying numbers of clusters.",
                    "label": 0
                },
                {
                    "sent": "So the first thing I'd like to point out here is that it ends up having a little human supervision and driving these.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clusters is valuable, so having kind of nice clean clusters which avoid sort of lumping together outliers can give you increased performance.",
                    "label": 0
                },
                {
                    "sent": "Here's an example for faces, and then another example for buses, which again we had sort of a human, give us some supervision in the clustering process, and we actually get a few percent benefit in having that sort of nice, clean, supervised clusters.",
                    "label": 0
                },
                {
                    "sent": "Again, do the sensitivity of outliers, and so now once we have sorted out these details of regularization.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Clustering, then we start to get these idealized pictures that we started out with right where as the amount of training data increases.",
                    "label": 0
                },
                {
                    "sent": "Here I'm showing for mixture models with different numbers of mixtures.",
                    "label": 0
                },
                {
                    "sent": "We see this general kind of flattening of performance as we increase the amount of training data as some coding at some level of performance.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, here's another way to slice it on the right where I say let's fix the amount of training data.",
                    "label": 0
                },
                {
                    "sent": "Now vary the model complexity in terms of the number of mixtures, and I find that there's some sweet spot right where.",
                    "label": 1
                },
                {
                    "sent": "So I have maybe 100 training examples.",
                    "label": 0
                },
                {
                    "sent": "Per mixture component gives me sort of the best tradeoff between having model complexity and having enough training data to learn each model well.",
                    "label": 0
                },
                {
                    "sent": "So we said, OK, this is working.",
                    "label": 0
                },
                {
                    "sent": "What are we going to do now?",
                    "label": 1
                },
                {
                    "sent": "We see this increased performance with increased amount of training data.",
                    "label": 0
                },
                {
                    "sent": "Let's go off and see if we can act.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Win the Pascal challenge by just taking this stupid strategy of adding more and more training data.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what do we do?",
                    "label": 0
                },
                {
                    "sent": "We went off and collected basically 10 times as much training data as there was in the basic Pascal training data set and the way you do this is you sort of follow the same protocol that Pascal used to collect their data.",
                    "label": 0
                },
                {
                    "sent": "Go to Flickr, collect images and then use some mechanical Turk workers to sort of filter through and give you bounding boxes.",
                    "label": 1
                },
                {
                    "sent": "And so we've produced a data set.",
                    "label": 0
                },
                {
                    "sent": "We only ended up doing it for 10 of the Pascal categories, but for those ten categories we have roughly.",
                    "label": 0
                },
                {
                    "sent": "10 times as much training data and we were all very excited to go off and do this and so we.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we see well.",
                    "label": 0
                },
                {
                    "sent": "So here again, I'm plotting this sort of performance versus number of training examples, and we've sort of done the cross validation to choose the regularization, the optimal number of mixer components for a given amount of training data.",
                    "label": 1
                },
                {
                    "sent": "And again, we see this sort of behavior that we need roughly 100 positive training examples per mixture component.",
                    "label": 1
                },
                {
                    "sent": "But the disappointing thing is that performance sort of saturates at about templates 10 templates for category, right?",
                    "label": 0
                },
                {
                    "sent": "So if you're familiar.",
                    "label": 0
                },
                {
                    "sent": "With Pascal you say, well, you know these numbers aren't a lot higher than anything that people submit to this competition.",
                    "label": 0
                },
                {
                    "sent": "In fact, you know there was no need to go over here to this sort of 10 times as much data set, right?",
                    "label": 0
                },
                {
                    "sent": "Things were saturating back here over at the sort of 2000 instead of 10,000.",
                    "label": 0
                },
                {
                    "sent": "So all that data collection was in vain.",
                    "label": 0
                },
                {
                    "sent": "This was somehow a dead end and you say, OK, well can we?",
                    "label": 0
                },
                {
                    "sent": "Can we salvage this in some way?",
                    "label": 0
                },
                {
                    "sent": "Can we at least say something?",
                    "label": 0
                },
                {
                    "sent": "About this basic problem of detection, right so.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one way of looking at this, you say, well, you've grown the model complexity.",
                    "label": 0
                },
                {
                    "sent": "You've grown the amount of training data you've hit, some saturation point, and so is it.",
                    "label": 0
                },
                {
                    "sent": "The case that we're just sort of looking at this this fundamental limit of Bayes risk for the hog feature that we just can't see anymore detail within these images.",
                    "label": 1
                },
                {
                    "sent": "So who thinks we've hit the Bayes risk?",
                    "label": 0
                },
                {
                    "sent": "No one, so that's good.",
                    "label": 0
                },
                {
                    "sent": "One reason you might know that is that if you're familiar with Pascal, you say, well, those numbers you just showed me aren't actually that good.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's models out there which use the hog feature spaces input, but actually give much better performance.",
                    "label": 0
                },
                {
                    "sent": "And so here is 1 which is quite well known.",
                    "label": 0
                },
                {
                    "sent": "The deformable part model, right?",
                    "label": 1
                },
                {
                    "sent": "So this represents the appearance is a collection of parts, each of which is sort of a discriminatively trained template that uses hog feature vectors as input, but allows those parts to wiggle around in the image right?",
                    "label": 0
                },
                {
                    "sent": "And I'm going to describe an experiment in which we actually use a sort of fully supervised version of this, so we tell the model where the parts are in each training example.",
                    "label": 0
                },
                {
                    "sent": "And so the point here is that this is using the same input feature space, but it has some sort of richer model under the hood to perform the classification, and as a result it can do much better.",
                    "label": 1
                },
                {
                    "sent": "Right, so so if you think about this, this is a little bit mysterious because I described a model in which we added more and more rigid templates.",
                    "label": 0
                },
                {
                    "sent": "And if you think about the deformable part model, one way to think about the deformable part model is just as a very very, very large collection of rigid temple.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so usually we think about a deformable part models where we sort of set it down on the image and wiggle the parts around to match things up.",
                    "label": 0
                },
                {
                    "sent": "But instead you could think about every possible placement of parts as synthesizing a rigid template and all the deformable part model does is sort of score that exponentially large collection of rigid templates on an image and pick out the one which scores maximally.",
                    "label": 1
                },
                {
                    "sent": "So it really is a sort of mixture model, except that it has this nice computational machinery of dynamic programming which lets you very rapidly.",
                    "label": 0
                },
                {
                    "sent": "Index through this kind of exponentially large set of rigid templates and of course each template has its own bias which is set by how much you had to stretch the Springs to position it right.",
                    "label": 0
                },
                {
                    "sent": "So from this point of view you say well, the deformable part model is also a collection of rigid templates.",
                    "label": 0
                },
                {
                    "sent": "Why didn't you get there with this simple rigid?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Plates and so there's there's maybe two reasons this could be right, so one is that when you train the model, all those rigid templates aren't independent of each other, right?",
                    "label": 0
                },
                {
                    "sent": "The parts themselves have the same appearance, they just get moved around a different location.",
                    "label": 0
                },
                {
                    "sent": "So you can think of some how the parameters of those part appearances are tide.",
                    "label": 0
                },
                {
                    "sent": "And of course the other thing is that the model can extrapolate to things you haven't seen before, right?",
                    "label": 1
                },
                {
                    "sent": "So it's not enough to have kind of a template for every exemplar that you've observed in training.",
                    "label": 0
                },
                {
                    "sent": "But now you're going to extrapolate to new.",
                    "label": 0
                },
                {
                    "sent": "Conditions and so.",
                    "label": 0
                },
                {
                    "sent": "The final experiment I want to describe is something which is trying to tease apart these two different contributions which we call a rigid mixture of parts.",
                    "label": 1
                },
                {
                    "sent": "So we're going to learn part appearance temp.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's from supervised annotations.",
                    "label": 0
                },
                {
                    "sent": "We're going to have one mixture component for sort of each spatial configuration seen in training, but we aren't going to do this extrapolation, right?",
                    "label": 1
                },
                {
                    "sent": "So we won't consider new placements of the parts that we haven't seen in training, so this again is just a large mixture of appearances.",
                    "label": 1
                },
                {
                    "sent": "It has the sharing of parameters, but no extrapolation, and so now we ask the question, which of these two elements contributes to.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Norman, so here is again this sort of plot on faces of the number of training examples at the bottom.",
                    "label": 0
                },
                {
                    "sent": "The blue curve is a single rigid template.",
                    "label": 0
                },
                {
                    "sent": "The green curve is our sort of best mixture of templates where we add mixtures.",
                    "label": 0
                },
                {
                    "sent": "Red on top is the supervised different part model and the cyan curve is this intermediate right in which we share parameters.",
                    "label": 0
                },
                {
                    "sent": "We share part appearances, but we don't extrapolate the new configurations and so we see that roughly half and half right.",
                    "label": 0
                },
                {
                    "sent": "Both of these things contribute to the performance of the deferrable part model in.",
                    "label": 1
                },
                {
                    "sent": "Tackling new images and so the final thing, I just want to point out is that you know this.",
                    "label": 0
                },
                {
                    "sent": "This idea of sharing parameters across part.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Places are really powerful thing, so here is sort of an example of this face detection model being run on face detection in real world images and the red curve here is it's deformable part models, essentially trained with something like 100 training examples, but where you have detailed annotations and compared to something like Google picasaorface.com which we know had probably millions of training examples, this sort of very simple.",
                    "label": 0
                },
                {
                    "sent": "In terms of training data set complexity, but intelligently thought out sharing of parameters as well.",
                    "label": 0
                },
                {
                    "sent": "So just to.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Include you say, well, more training data helps, but only if you're careful so you can't be naive about this.",
                    "label": 1
                },
                {
                    "sent": "You've got to do regularization in the right way.",
                    "label": 1
                },
                {
                    "sent": "You need sort of clean training data to protect the SVM from outliers and having the proper correspondence is actually key, and we sort of summarize and say somehow the biggest gains we see are not from increasing the amount of training data, but for more intelligent design of the model, that is having a richer representation.",
                    "label": 0
                },
                {
                    "sent": "So again, our models in the end compile down to just linear classifiers run on hog feature space.",
                    "label": 0
                },
                {
                    "sent": "But the parameters are shared in an intelligent way and so.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That I'll thank you and my coauthors.",
                    "label": 0
                }
            ]
        }
    }
}