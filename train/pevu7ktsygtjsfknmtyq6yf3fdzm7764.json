{
    "id": "pevu7ktsygtjsfknmtyq6yf3fdzm7764",
    "title": "Spacetime Forests with Complementary Features for Dynamic Scene Recognition",
    "info": {
        "author": [
            "Christoph Feichtenhofer, Graz University of Technology"
        ],
        "published": "April 3, 2014",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bmvc2013_feichtenhofer_scene_recognition/",
    "segmentation": [
        [
            "So this is joint work with axle pins from Tia grads and Richard Wiles from York University, Toronto.",
            "So first let."
        ],
        [
            "We start with defining the task of dynamic scene classification.",
            "The goal of dynamic scene classification is to assign a semantic label to each input sequence, for example, of each sequence arrival sequence, forest fire sequence.",
            "Or a traffic sequence.",
            "As you can see here."
        ],
        [
            "So there are several challenges in dynamic scene recognition.",
            "First of all, there are the typical challenges of image classification tasks.",
            "For example, you have scenes captured from different viewpoints.",
            "You have different illumination characteristics.",
            "You have different scales of the object due to camera.",
            "Assume you have different appearances of the objects and you have different backgrounds and background clutter.",
            "Furthermore, there are challenge."
        ],
        [
            "Search challenges which are small changes within the between the classes.",
            "For example here you can see three different classes of dynamic scenes on the left side you can see River sequences in the middle you can see waterfalls on the right side you can see a fountain class and all these scenes look pretty similar in appearance but different in dynamic.",
            "Dynamics.",
            "Mother chat."
        ],
        [
            "Launch an image classification is that there are large variations within the classes, so on this slide you can see six different sequences showing up landslides since they all look very different and it's therefore how to learn a model which represents this class well."
        ],
        [
            "Further challenges in dynamic scene classification are that we have to deal with camera movement in some sequences.",
            "As you can see in this, our launch sequences on the top here.",
            "And we have also to deal with scene cuts in some sequences.",
            "As you can see down here in this volcanic eruption sequence or in the forest fire sequence on the right.",
            "So."
        ],
        [
            "Recently there have been two notable approaches for dynamic scene recognition.",
            "First of all is the work of Shroff and colleagues at CPR 2010 where they have proposed a combination of just features and chaotic invariants to make assumptions about the regularity or the busyness of the motion in the scenes.",
            "Another notable approaches, the one of their parties and colleagues center that CPR.",
            "Last year they presented the spacetime orientation features computed over the whole image sequence to recognize dynamic scenes.",
            "And one thing all these approaches have in common is that they compute one single feature vector for the whole input sequence.",
            "Therefore they have to see the whole input sequence to recognize it."
        ],
        [
            "We tackle this problem by proposing temporal slicing our temporal slicing.",
            "Basically, parcels the input sequence into several discrete temporal slices of fixed duration, and therefore we are able to incrementally classify the input sequence online, and also this temporal slicing helps us to untangle camera motion from the scene dynamics, because we think that camera motion occurs at the coarse temporal scale.",
            "For example, if you have long pants or long tsums.",
            "And the scene dynamics happen with the finer temporal scale.",
            "Therefore, this temporal slicing helps us to untangle the camera motion from scene dynamics."
        ],
        [
            "So after parceling the input sequence into this temporal slices, we compute a complementary descriptor by applying spatial filters to this temporal slices.",
            "And aggregating them in local regions to build histograms of oriented filter measurements.",
            "And we also apply spatial temporal filters through our temporal slices.",
            "To generate temporal features.",
            "This ends up with spatially and temporally oriented features.",
            "We also add color features to our representation and all these features are used in a random forest classifier.",
            "To predict the class label online in an incremental manner."
        ],
        [
            "So now let me go into detail in our complimentary spacetime descriptor.",
            "First of all, we extract spatial information while applying gossan third derivative filters to the image.",
            "Four at each location for several orientations and several spatial scales.",
            "And we aggregate the squared measurements, which is the energy filter and she basically over local regions.",
            "And then we sum them up in these regions and build histograms for specific filter orientations and specific scales."
        ],
        [
            "Our temporal descriptor is generated by applying spacetime filters, which are Gaussian for derivative filters in the spatial temporal domain to the image sequence.",
            "We again squared responses and aggregate them over each for each spacetime point, we use several free dimensional filter orientations and several scales.",
            "Now this filter measurements are a function of the local appearance and of the temporal dynamics, and we would like to get features independent of the spatial appearance.",
            "Therefore, we think of the motion in the frequency domain where the motion occurs through the plane as a plane through the origin, and if we sum across all filtering tations consistent with this single frequency domain plane, we can get spatial appearance independent features.",
            "So we basically some overall filter measurements consistent with a single frequency domain plane for a Gaussian and derivative filter we need to sum across M + 1 motion direction.",
            "Consistent energy samples that generate the temporal energy across a single frequency domain plane.",
            "This has been previously used for dynamic texture classification, this feature representation."
        ],
        [
            "And we again build histograms of temporal oriented energies by steering this frequency domain plane.",
            "For several directions, for example, motions across right work directions, upward directions, and so on."
        ],
        [
            "So all these features are a function of local spatial or temporal orientation and also of the contrast.",
            "And now to remove the contrast dependency.",
            "The responses from the previous step are divided by the sum of all responses at each point.",
            "For all orientations, and we also add a small bias epsilon to indicate regions, but overall orientation structure is very low for regions with overall very low energy which are homogeneous region and these homogeneous regions are represented by an additional feature vector feature channel.",
            "We're just called epsilon hat here.",
            "Note that for regions where the overall energy is very very low, the summation in the second equation here will tend to zero, and therefore this epsilon hat will tend to one indicate homogeneous regions in the image."
        ],
        [
            "So now on this slide and illustration for our spatial descriptor is given for one temporal slice of an avalanche sequence.",
            "We apply several filtering tations for rotations you can see here and compute the energies in this slice.",
            "And we also compute this number normalized epsilon to indicate lack of orientation structure.",
            "We compute our measurements for several different scales.",
            "Each scale is varied by one octave in the image."
        ],
        [
            "On the next slide, you can see the temporal descriptor for a waterfall sequence, one single slice of a waterfall sequence.",
            "We compute motion across various direction by steering this frequency domain plane.",
            "First of all we use static static channel which indicates zero velocity.",
            "Basically we use also rightward motion upward motion, leftward motion downward, horizontal flicker, and vertical flicker channels.",
            "And Additionally, this epsilon hat channel to indicate homogeneous region of spatial temporal structure.",
            "As you can see on this slide in regions of the waterfall, the downward channel has very strong energy and in static regions to static channel has strong energy just obviously."
        ],
        [
            "So after having established this descriptor of spatial and temporal orientation, we also add color information to our complimentary descriptor.",
            "Color information is represented by a free bin histogram of the LV color space.",
            "And after using all these measurements, we aggregate them in space.",
            "Spatial temporal regions formed by a spatial pyramid.",
            "For each slice of the input sequence."
        ],
        [
            "So we basically pull our energy measurements to a single feature vector, and this feature vector.",
            "Stand used in classification within a random forest classifier."
        ],
        [
            "We use the classic random forest introduced by Leo Breiman in 2001.",
            "This random forest consists of two random sources.",
            "First of all, it's subsamples the training data."
        ],
        [
            "To allow high generalizability and low overfitting.",
            "And it all."
        ],
        [
            "Also consists of a random split selection at each split node.",
            "We aim to maximize the information gain at each split node, so this is a classic random forest and now our."
        ],
        [
            "FaceTime Random Forest differs from the classic random forests in the way that we restrict the node optimization process, which maximizes the information gain to a single feature type only.",
            "This is because we think that some classes may be better distinguished by a single feature channel.",
            "So for example, a forest fire sequence may be better represented by the color information, not the other hand traffic sequence.",
            "Maybe better distinguished by its motion information.",
            "So therefore we trained specific trees for each of our complementary feature channels."
        ],
        [
            "And then in the classification step.",
            "We send our spatial features through the spatial trees and our temporal features through the temporal trees and also our color features for the color trees.",
            "Then we accumulate all the tree votes and generate a prediction predict class prediction for each temporal slice.",
            "Therefore, we are able to predict the input sequence at all times and not just after having seen the whole sequence.",
            "And after accumulating all the predictions for all temporal slices, we can predict the whole input sequence."
        ],
        [
            "Now let me move to onto the results.",
            "We have evaluated our approach on two different datasets.",
            "First of all, the Maryland in the wild data set.",
            "It consists of 130 video clips showing different classes for different scene classes, and it's notable that this data set consists of unconstrained camera movement in some scenes, as you have seen.",
            "For the avalanche clips.",
            "So some of the avalanche clips have very strong camera motion.",
            "For example in others as well."
        ],
        [
            "So on this slide you can see the previous results on this data set for each class individually and on the last row the overall results.",
            "The average results recognition rates for several feature descriptors and classifiers, and the previous state of the art here is given by the combination of just features and chaotic invariants of Shroff and colleagues.",
            "Now after we use our new complementary descriptor combined with just a random forest classifier and using one single temporal slice, here we can already achieve classification rate of 57% here.",
            "And if we use all temporal information, we achieve a little bit better performance.",
            "But if we use our space time random forests with specific feature channels trained in specific trees, we can achieve further performance gain of 7% here.",
            "But just using one single temporal slice, and if we use all temporal information, we gain 68% of recognition rate on this challenging data set.",
            "And in this case it's specially notable that we achieve new state of the art performance, but just using one single temporal slice.",
            "So we basically just have to look at 16 frames, one slices, 16 frames in our computations here of each clip and already achieved a new state of the art performance."
        ],
        [
            "We Additionally evaluated our approach on the newer wire pandemic scene data set.",
            "This is a larger data set consisting of 30 videos in each Category, 14 categories and it is captured by a static camera."
        ],
        [
            "The previous state of the art results on this data set are given by the work of their punishment colleagues in 2010.",
            "It is notable that they use very similar feature representation than we do, and if we use our complementary descriptor and just use one temporal slice, we already gained the same performance as they do.",
            "By using all temporal information.",
            "Using more temporal information only increases the performance slightly, and using the space time random forest further increases performance to ending up at 68% for role performance.",
            "And again, it is very notable here that we achieve new state of the art performance but just lose looking at one temporal slice of the input sequence."
        ],
        [
            "So in this light we illustrate the complementarity of our descriptor again.",
            "So this is achieved, you can see the performance plotted here for specific featured channels only, so we trained random forest, but just using spatial descriptor, temporal descriptors or color informations.",
            "And as you can see, for example if you want to classify traffic scenes, it is very important to use spatial or temporal information and the color becomes very low important here.",
            "On the other hand, if you want to classify forest, fire sequence, color information becomes very important and achieves already 70% of recognition rate on this Maryland data set, whereas temporal information spatial information is not that important for this classes.",
            "K."
        ],
        [
            "So in summary, we have presented temporal slicing which allows to disentangle camera motion from the scene dynamics, and it also allows an incremental processing of the input sequence.",
            "And an online prediction.",
            "We've also introduced the complimentary spacetime orientation descriptor, which is used in combination with our spacetime random forests to online classified as input sequences overtime, and we have achieved a new state of the art recognition rates both with and without camera motion, but just using a single temporal slice for classification.",
            "Thank you very much for your attention.",
            "Thank you very much for your presentation.",
            "I should have three questions.",
            "The first one is how many frames in one slice.",
            "Now because you use the temporal stasis right?",
            "And the second question is, how can you measure it is robust to the motion of the camera and the third question is I want to know the depth of the tree because you use the.",
            "Can we?",
            "Can we just start with the 1st first question?",
            "So the first question is how many frames is 1 slice right?",
            "One slice is set to 16 frames in our evaluations.",
            "And we also tried large slice with alterations, but setting it to 16 frames is achieved quite the same performance as setting a larger slice with on the stabilized data set and for scenes with camera motion.",
            "The smaller the slices are, the better the performance gets.",
            "Basically, and we choose 16 frames slice with because this is limited by our free dimensional filters which have to be at least 16 frames long in the temporal domain.",
            "To get the first derivative of Gaussian filter.",
            "OK yeah, the second question is, is your method robust to the motion of camera?",
            "Motion of camera.",
            "Is it robust against the camera motion?",
            "Yeah, that's that's why we use this temporal slicing and the small scale filtering in the space time filtering.",
            "So that's why when they see the way is the depths of the toy.",
            "Sorry, thanks the devs.",
            "The death of Troy.",
            "The death of the tree, yes.",
            "The death of the tree.",
            "OK, it's defined by the we train the tree.",
            "We do not any.",
            "We do not prune the tree or anything like that.",
            "We train the tree until there is just one single class label at the leaf node.",
            "So we train the tree until its full extent.",
            "OK, the question is becauses you used three and 331 is used for the special feature.",
            "The second is user temporal and the third one is used for the color red.",
            "Does the three choice has the same depth?",
            "We just experimented with free channels because we have our descriptive design for free.",
            "Complimentary color channels, free complimentary channels.",
            "Do you mean if we could use more channels?",
            "What is the?",
            "I don't understand the question correctly.",
            "Past the three trees had us has the same depth.",
            "They have same day yeah yeah oh the death is defined by the training process so it is random basically."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is joint work with axle pins from Tia grads and Richard Wiles from York University, Toronto.",
                    "label": 0
                },
                {
                    "sent": "So first let.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We start with defining the task of dynamic scene classification.",
                    "label": 0
                },
                {
                    "sent": "The goal of dynamic scene classification is to assign a semantic label to each input sequence, for example, of each sequence arrival sequence, forest fire sequence.",
                    "label": 1
                },
                {
                    "sent": "Or a traffic sequence.",
                    "label": 0
                },
                {
                    "sent": "As you can see here.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are several challenges in dynamic scene recognition.",
                    "label": 0
                },
                {
                    "sent": "First of all, there are the typical challenges of image classification tasks.",
                    "label": 1
                },
                {
                    "sent": "For example, you have scenes captured from different viewpoints.",
                    "label": 0
                },
                {
                    "sent": "You have different illumination characteristics.",
                    "label": 0
                },
                {
                    "sent": "You have different scales of the object due to camera.",
                    "label": 0
                },
                {
                    "sent": "Assume you have different appearances of the objects and you have different backgrounds and background clutter.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, there are challenge.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Search challenges which are small changes within the between the classes.",
                    "label": 0
                },
                {
                    "sent": "For example here you can see three different classes of dynamic scenes on the left side you can see River sequences in the middle you can see waterfalls on the right side you can see a fountain class and all these scenes look pretty similar in appearance but different in dynamic.",
                    "label": 0
                },
                {
                    "sent": "Dynamics.",
                    "label": 0
                },
                {
                    "sent": "Mother chat.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Launch an image classification is that there are large variations within the classes, so on this slide you can see six different sequences showing up landslides since they all look very different and it's therefore how to learn a model which represents this class well.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Further challenges in dynamic scene classification are that we have to deal with camera movement in some sequences.",
                    "label": 0
                },
                {
                    "sent": "As you can see in this, our launch sequences on the top here.",
                    "label": 0
                },
                {
                    "sent": "And we have also to deal with scene cuts in some sequences.",
                    "label": 1
                },
                {
                    "sent": "As you can see down here in this volcanic eruption sequence or in the forest fire sequence on the right.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recently there have been two notable approaches for dynamic scene recognition.",
                    "label": 0
                },
                {
                    "sent": "First of all is the work of Shroff and colleagues at CPR 2010 where they have proposed a combination of just features and chaotic invariants to make assumptions about the regularity or the busyness of the motion in the scenes.",
                    "label": 0
                },
                {
                    "sent": "Another notable approaches, the one of their parties and colleagues center that CPR.",
                    "label": 0
                },
                {
                    "sent": "Last year they presented the spacetime orientation features computed over the whole image sequence to recognize dynamic scenes.",
                    "label": 0
                },
                {
                    "sent": "And one thing all these approaches have in common is that they compute one single feature vector for the whole input sequence.",
                    "label": 1
                },
                {
                    "sent": "Therefore they have to see the whole input sequence to recognize it.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We tackle this problem by proposing temporal slicing our temporal slicing.",
                    "label": 0
                },
                {
                    "sent": "Basically, parcels the input sequence into several discrete temporal slices of fixed duration, and therefore we are able to incrementally classify the input sequence online, and also this temporal slicing helps us to untangle camera motion from the scene dynamics, because we think that camera motion occurs at the coarse temporal scale.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have long pants or long tsums.",
                    "label": 0
                },
                {
                    "sent": "And the scene dynamics happen with the finer temporal scale.",
                    "label": 0
                },
                {
                    "sent": "Therefore, this temporal slicing helps us to untangle the camera motion from scene dynamics.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So after parceling the input sequence into this temporal slices, we compute a complementary descriptor by applying spatial filters to this temporal slices.",
                    "label": 0
                },
                {
                    "sent": "And aggregating them in local regions to build histograms of oriented filter measurements.",
                    "label": 0
                },
                {
                    "sent": "And we also apply spatial temporal filters through our temporal slices.",
                    "label": 0
                },
                {
                    "sent": "To generate temporal features.",
                    "label": 0
                },
                {
                    "sent": "This ends up with spatially and temporally oriented features.",
                    "label": 1
                },
                {
                    "sent": "We also add color features to our representation and all these features are used in a random forest classifier.",
                    "label": 0
                },
                {
                    "sent": "To predict the class label online in an incremental manner.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let me go into detail in our complimentary spacetime descriptor.",
                    "label": 0
                },
                {
                    "sent": "First of all, we extract spatial information while applying gossan third derivative filters to the image.",
                    "label": 1
                },
                {
                    "sent": "Four at each location for several orientations and several spatial scales.",
                    "label": 0
                },
                {
                    "sent": "And we aggregate the squared measurements, which is the energy filter and she basically over local regions.",
                    "label": 0
                },
                {
                    "sent": "And then we sum them up in these regions and build histograms for specific filter orientations and specific scales.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our temporal descriptor is generated by applying spacetime filters, which are Gaussian for derivative filters in the spatial temporal domain to the image sequence.",
                    "label": 0
                },
                {
                    "sent": "We again squared responses and aggregate them over each for each spacetime point, we use several free dimensional filter orientations and several scales.",
                    "label": 0
                },
                {
                    "sent": "Now this filter measurements are a function of the local appearance and of the temporal dynamics, and we would like to get features independent of the spatial appearance.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we think of the motion in the frequency domain where the motion occurs through the plane as a plane through the origin, and if we sum across all filtering tations consistent with this single frequency domain plane, we can get spatial appearance independent features.",
                    "label": 0
                },
                {
                    "sent": "So we basically some overall filter measurements consistent with a single frequency domain plane for a Gaussian and derivative filter we need to sum across M + 1 motion direction.",
                    "label": 0
                },
                {
                    "sent": "Consistent energy samples that generate the temporal energy across a single frequency domain plane.",
                    "label": 1
                },
                {
                    "sent": "This has been previously used for dynamic texture classification, this feature representation.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we again build histograms of temporal oriented energies by steering this frequency domain plane.",
                    "label": 0
                },
                {
                    "sent": "For several directions, for example, motions across right work directions, upward directions, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So all these features are a function of local spatial or temporal orientation and also of the contrast.",
                    "label": 1
                },
                {
                    "sent": "And now to remove the contrast dependency.",
                    "label": 0
                },
                {
                    "sent": "The responses from the previous step are divided by the sum of all responses at each point.",
                    "label": 0
                },
                {
                    "sent": "For all orientations, and we also add a small bias epsilon to indicate regions, but overall orientation structure is very low for regions with overall very low energy which are homogeneous region and these homogeneous regions are represented by an additional feature vector feature channel.",
                    "label": 0
                },
                {
                    "sent": "We're just called epsilon hat here.",
                    "label": 0
                },
                {
                    "sent": "Note that for regions where the overall energy is very very low, the summation in the second equation here will tend to zero, and therefore this epsilon hat will tend to one indicate homogeneous regions in the image.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now on this slide and illustration for our spatial descriptor is given for one temporal slice of an avalanche sequence.",
                    "label": 0
                },
                {
                    "sent": "We apply several filtering tations for rotations you can see here and compute the energies in this slice.",
                    "label": 0
                },
                {
                    "sent": "And we also compute this number normalized epsilon to indicate lack of orientation structure.",
                    "label": 0
                },
                {
                    "sent": "We compute our measurements for several different scales.",
                    "label": 0
                },
                {
                    "sent": "Each scale is varied by one octave in the image.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the next slide, you can see the temporal descriptor for a waterfall sequence, one single slice of a waterfall sequence.",
                    "label": 0
                },
                {
                    "sent": "We compute motion across various direction by steering this frequency domain plane.",
                    "label": 0
                },
                {
                    "sent": "First of all we use static static channel which indicates zero velocity.",
                    "label": 0
                },
                {
                    "sent": "Basically we use also rightward motion upward motion, leftward motion downward, horizontal flicker, and vertical flicker channels.",
                    "label": 0
                },
                {
                    "sent": "And Additionally, this epsilon hat channel to indicate homogeneous region of spatial temporal structure.",
                    "label": 0
                },
                {
                    "sent": "As you can see on this slide in regions of the waterfall, the downward channel has very strong energy and in static regions to static channel has strong energy just obviously.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So after having established this descriptor of spatial and temporal orientation, we also add color information to our complimentary descriptor.",
                    "label": 0
                },
                {
                    "sent": "Color information is represented by a free bin histogram of the LV color space.",
                    "label": 1
                },
                {
                    "sent": "And after using all these measurements, we aggregate them in space.",
                    "label": 1
                },
                {
                    "sent": "Spatial temporal regions formed by a spatial pyramid.",
                    "label": 0
                },
                {
                    "sent": "For each slice of the input sequence.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we basically pull our energy measurements to a single feature vector, and this feature vector.",
                    "label": 0
                },
                {
                    "sent": "Stand used in classification within a random forest classifier.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use the classic random forest introduced by Leo Breiman in 2001.",
                    "label": 0
                },
                {
                    "sent": "This random forest consists of two random sources.",
                    "label": 1
                },
                {
                    "sent": "First of all, it's subsamples the training data.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To allow high generalizability and low overfitting.",
                    "label": 0
                },
                {
                    "sent": "And it all.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also consists of a random split selection at each split node.",
                    "label": 0
                },
                {
                    "sent": "We aim to maximize the information gain at each split node, so this is a classic random forest and now our.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "FaceTime Random Forest differs from the classic random forests in the way that we restrict the node optimization process, which maximizes the information gain to a single feature type only.",
                    "label": 1
                },
                {
                    "sent": "This is because we think that some classes may be better distinguished by a single feature channel.",
                    "label": 0
                },
                {
                    "sent": "So for example, a forest fire sequence may be better represented by the color information, not the other hand traffic sequence.",
                    "label": 0
                },
                {
                    "sent": "Maybe better distinguished by its motion information.",
                    "label": 0
                },
                {
                    "sent": "So therefore we trained specific trees for each of our complementary feature channels.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then in the classification step.",
                    "label": 0
                },
                {
                    "sent": "We send our spatial features through the spatial trees and our temporal features through the temporal trees and also our color features for the color trees.",
                    "label": 0
                },
                {
                    "sent": "Then we accumulate all the tree votes and generate a prediction predict class prediction for each temporal slice.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we are able to predict the input sequence at all times and not just after having seen the whole sequence.",
                    "label": 0
                },
                {
                    "sent": "And after accumulating all the predictions for all temporal slices, we can predict the whole input sequence.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let me move to onto the results.",
                    "label": 0
                },
                {
                    "sent": "We have evaluated our approach on two different datasets.",
                    "label": 0
                },
                {
                    "sent": "First of all, the Maryland in the wild data set.",
                    "label": 1
                },
                {
                    "sent": "It consists of 130 video clips showing different classes for different scene classes, and it's notable that this data set consists of unconstrained camera movement in some scenes, as you have seen.",
                    "label": 0
                },
                {
                    "sent": "For the avalanche clips.",
                    "label": 0
                },
                {
                    "sent": "So some of the avalanche clips have very strong camera motion.",
                    "label": 0
                },
                {
                    "sent": "For example in others as well.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So on this slide you can see the previous results on this data set for each class individually and on the last row the overall results.",
                    "label": 0
                },
                {
                    "sent": "The average results recognition rates for several feature descriptors and classifiers, and the previous state of the art here is given by the combination of just features and chaotic invariants of Shroff and colleagues.",
                    "label": 1
                },
                {
                    "sent": "Now after we use our new complementary descriptor combined with just a random forest classifier and using one single temporal slice, here we can already achieve classification rate of 57% here.",
                    "label": 0
                },
                {
                    "sent": "And if we use all temporal information, we achieve a little bit better performance.",
                    "label": 0
                },
                {
                    "sent": "But if we use our space time random forests with specific feature channels trained in specific trees, we can achieve further performance gain of 7% here.",
                    "label": 0
                },
                {
                    "sent": "But just using one single temporal slice, and if we use all temporal information, we gain 68% of recognition rate on this challenging data set.",
                    "label": 0
                },
                {
                    "sent": "And in this case it's specially notable that we achieve new state of the art performance, but just using one single temporal slice.",
                    "label": 0
                },
                {
                    "sent": "So we basically just have to look at 16 frames, one slices, 16 frames in our computations here of each clip and already achieved a new state of the art performance.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We Additionally evaluated our approach on the newer wire pandemic scene data set.",
                    "label": 0
                },
                {
                    "sent": "This is a larger data set consisting of 30 videos in each Category, 14 categories and it is captured by a static camera.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The previous state of the art results on this data set are given by the work of their punishment colleagues in 2010.",
                    "label": 1
                },
                {
                    "sent": "It is notable that they use very similar feature representation than we do, and if we use our complementary descriptor and just use one temporal slice, we already gained the same performance as they do.",
                    "label": 0
                },
                {
                    "sent": "By using all temporal information.",
                    "label": 0
                },
                {
                    "sent": "Using more temporal information only increases the performance slightly, and using the space time random forest further increases performance to ending up at 68% for role performance.",
                    "label": 0
                },
                {
                    "sent": "And again, it is very notable here that we achieve new state of the art performance but just lose looking at one temporal slice of the input sequence.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this light we illustrate the complementarity of our descriptor again.",
                    "label": 1
                },
                {
                    "sent": "So this is achieved, you can see the performance plotted here for specific featured channels only, so we trained random forest, but just using spatial descriptor, temporal descriptors or color informations.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, for example if you want to classify traffic scenes, it is very important to use spatial or temporal information and the color becomes very low important here.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you want to classify forest, fire sequence, color information becomes very important and achieves already 70% of recognition rate on this Maryland data set, whereas temporal information spatial information is not that important for this classes.",
                    "label": 0
                },
                {
                    "sent": "K.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary, we have presented temporal slicing which allows to disentangle camera motion from the scene dynamics, and it also allows an incremental processing of the input sequence.",
                    "label": 0
                },
                {
                    "sent": "And an online prediction.",
                    "label": 0
                },
                {
                    "sent": "We've also introduced the complimentary spacetime orientation descriptor, which is used in combination with our spacetime random forests to online classified as input sequences overtime, and we have achieved a new state of the art recognition rates both with and without camera motion, but just using a single temporal slice for classification.",
                    "label": 1
                },
                {
                    "sent": "Thank you very much for your attention.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for your presentation.",
                    "label": 0
                },
                {
                    "sent": "I should have three questions.",
                    "label": 0
                },
                {
                    "sent": "The first one is how many frames in one slice.",
                    "label": 0
                },
                {
                    "sent": "Now because you use the temporal stasis right?",
                    "label": 0
                },
                {
                    "sent": "And the second question is, how can you measure it is robust to the motion of the camera and the third question is I want to know the depth of the tree because you use the.",
                    "label": 0
                },
                {
                    "sent": "Can we?",
                    "label": 0
                },
                {
                    "sent": "Can we just start with the 1st first question?",
                    "label": 0
                },
                {
                    "sent": "So the first question is how many frames is 1 slice right?",
                    "label": 0
                },
                {
                    "sent": "One slice is set to 16 frames in our evaluations.",
                    "label": 0
                },
                {
                    "sent": "And we also tried large slice with alterations, but setting it to 16 frames is achieved quite the same performance as setting a larger slice with on the stabilized data set and for scenes with camera motion.",
                    "label": 0
                },
                {
                    "sent": "The smaller the slices are, the better the performance gets.",
                    "label": 0
                },
                {
                    "sent": "Basically, and we choose 16 frames slice with because this is limited by our free dimensional filters which have to be at least 16 frames long in the temporal domain.",
                    "label": 0
                },
                {
                    "sent": "To get the first derivative of Gaussian filter.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, the second question is, is your method robust to the motion of camera?",
                    "label": 0
                },
                {
                    "sent": "Motion of camera.",
                    "label": 0
                },
                {
                    "sent": "Is it robust against the camera motion?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's why we use this temporal slicing and the small scale filtering in the space time filtering.",
                    "label": 0
                },
                {
                    "sent": "So that's why when they see the way is the depths of the toy.",
                    "label": 0
                },
                {
                    "sent": "Sorry, thanks the devs.",
                    "label": 0
                },
                {
                    "sent": "The death of Troy.",
                    "label": 0
                },
                {
                    "sent": "The death of the tree, yes.",
                    "label": 0
                },
                {
                    "sent": "The death of the tree.",
                    "label": 0
                },
                {
                    "sent": "OK, it's defined by the we train the tree.",
                    "label": 0
                },
                {
                    "sent": "We do not any.",
                    "label": 0
                },
                {
                    "sent": "We do not prune the tree or anything like that.",
                    "label": 0
                },
                {
                    "sent": "We train the tree until there is just one single class label at the leaf node.",
                    "label": 0
                },
                {
                    "sent": "So we train the tree until its full extent.",
                    "label": 0
                },
                {
                    "sent": "OK, the question is becauses you used three and 331 is used for the special feature.",
                    "label": 0
                },
                {
                    "sent": "The second is user temporal and the third one is used for the color red.",
                    "label": 0
                },
                {
                    "sent": "Does the three choice has the same depth?",
                    "label": 0
                },
                {
                    "sent": "We just experimented with free channels because we have our descriptive design for free.",
                    "label": 0
                },
                {
                    "sent": "Complimentary color channels, free complimentary channels.",
                    "label": 0
                },
                {
                    "sent": "Do you mean if we could use more channels?",
                    "label": 0
                },
                {
                    "sent": "What is the?",
                    "label": 0
                },
                {
                    "sent": "I don't understand the question correctly.",
                    "label": 0
                },
                {
                    "sent": "Past the three trees had us has the same depth.",
                    "label": 0
                },
                {
                    "sent": "They have same day yeah yeah oh the death is defined by the training process so it is random basically.",
                    "label": 0
                }
            ]
        }
    }
}