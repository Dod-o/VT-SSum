{
    "id": "ot3tkr7dl33z4ryo5rcq5r23xek4mqje",
    "title": "Deep NLP Applications and Dynamic Memory Networks",
    "info": {
        "author": [
            "Richard Socher, Computer Science Department, Stanford University"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_socher_nlp_applications/",
    "segmentation": [
        [
            "Glad you're here.",
            "'cause today we can actually use all the stuff we learned yesterday about recurrent neural networks and also some of the days before and recursive neural networks and put it all together for a bunch."
        ],
        [
            "Fun applications before we do that and focus on natural language processing.",
            "I actually want to give you at least one reason why.",
            "Maybe on your research side, you might want to focus if you're already hooked on deep learning on natural language processing instead of on computer vision.",
            "I think in the end computer vision will actually be very much commoditized in the next couple of years, or in some ways it already is.",
            "So let me show you an example of what I mean by this.",
            "Let's say you wanted to train your own new image classifier.",
            "And you don't yet know you know how well it would work to distinguish two different classes.",
            "So let's say for instance, you have some chocolate chip cookies.",
            "And you want to train a classifier that distinguishes chocolate chip cookies from oatmeal Raisin cookies.",
            "Nowadays all you need to do is say alright here I have some chocolate chips, cookies, chocolate chip cookies.",
            "Then here I have five oatmeal Raisin cookies.",
            "And you drag and drop those into your browser.",
            "You say this reason.",
            "And then you say, alright, that's my cookie classifier.",
            "An you say uploading train and then basically this gets sent to a back end, trains an algorithm before I could finish the sentence.",
            "You now have production level classifier with three lines of Python code.",
            "And you can test it too by dragging dropping.",
            "And basically.",
            "You know, upload any image and will tell you what kind of cookie it is.",
            "So if it's that easy to do something already then it's probably not that greater.",
            "Volunteer to just start doing research in that field right now.",
            "Now of course computer vision is much more than just image classification, but it is a pretty central piece of that field and this is anybody can do that.",
            "You don't even need a password, you just go to metamind IO and created classifier and.",
            "There you have it, so you can do that for a lot of different things and a lot of people have used it for a lot of different things.",
            "You can use it for not safe for classifier.",
            "I won't show you those images and you know anything from malaria, blood samples to different skirts, to different plankton samples, dresses, company logos and so on.",
            "It's it's pretty commoditized, yeah?",
            "It is a little bit more than that, so if you have enough images, it actually trains a much larger network.",
            "Also, if you have very few images, it does a lot of transfer learning from a pre trained large neural net and it only trains few of the higher layers depending on how many images you have.",
            "So of course, again, vision is much more than just image classification, but this is basically how far deep learning has already gone, and this is one of the shameless plug demos from my company Metamind."
        ],
        [
            "So today I want to focus on some fun applications of deep natural language processing.",
            "We'll start with some character recurrent neural networks, which are a lot of fun, but not always.",
            "They don't always get state of the art performance on some hard benchmark datasets, and then we'll move on to some multimodal applications where we use the ideas that we had on the image side and actually.",
            "Improve or combine that basically with sentences.",
            "We basically you're going to map sentences and images into the same vector space so that we can find one from the other and then have another multimodal kind of application where we can predict engagement on Twitter before even sending out a tweet just from the images and the text, which is an interesting application and that humans aren't very good at it and machines can actually still do, you know, get much beyond.",
            "Random and actually can get pretty high accuracy for this kind of task.",
            "Will do will look at a pretty hard task question answering and one old model that we've used to solve that before.",
            "We then go onto a new model that we just introduced two months ago and never mind that I think will combine a lot of the power of all these different models that we've introduced before and it will make strong use of recurrent neural networks."
        ],
        [
            "Alright, so character recurrent neural Nets basically a very simple, similar and simple we have.",
            "Here again these exact hidden to hidden representations and internally here we could actually train also a more complex LTM or Gru model.",
            "Here we just visualized very simple recurrent neural net.",
            "And all we do is basically try to output the next character and this is a very cool demo from an slides from Andrej Karpathy, who's also at Stanford."
        ],
        [
            "And just to give you an idea of how they work basic."
        ],
        [
            "We use these LCM so instead of having a."
        ],
        [
            "Note here that is just a simple recurrent neural network node.",
            "This is actually more complex."
        ],
        [
            "And then the most interesting is also like I said here are sort of fun samples.",
            "How much can the model actually learned?",
            "Is it able to learn, for instance, that when you (that later on, you might want to close that parenthesis and things like that?",
            "That will tell us something about how much it can store in its own memory.",
            "So let's look at some results here."
        ],
        [
            "You're dead, Andre."
        ],
        [
            "Has put up.",
            "Here's a character or an end that is trained on Shakespeare and what you'll see is that essentially from afar it will look somewhat like Shakespeare, but as you look closer it doesn't make much semantic sense, so there's it's not trying to generate and try to create a coherent story or anything, but what is kind of amazing already is that it actually has been able to memorize words just from a very complex character sequence, so at least it has understood.",
            "A lot of and sort of stored in its memory and the LCM waits a lot of different words of the English language."
        ],
        [
            "Now on Wikipedia, when you train it, it looks more like a complex Wikipedia entry.",
            "It even added some links.",
            "And again, it doesn't make much sense semantically."
        ],
        [
            "This one is interesting as latech, but if you've ever played with Latex, you know that even for humans it's hard to make it compile sometimes.",
            "So here he actually had to manually fix some of the syntax errors to get that, but in the end it still looked largely like this, so either only minor fixes and from afar it looks like a reasonable math paper almost.",
            "And then here."
        ],
        [
            "This is the.",
            "This is a very interesting example in that it actually learned to do indentation correctly.",
            "It learn to define functions and it actually learned to properly parenthesize various sub expressions for code.",
            "So there we actually see that LCMS on the character level have the potential to learn different kinds of stacks.",
            "In some ways you put certain parenthesis on a stack, and then you want to be able to.",
            "Eventually pop it again and close it.",
            "So pretty interesting sort of observation that comes out of visualizing it this way.",
            "Are there any questions about recurrent neural Nets and on characters?",
            "Yeah.",
            "I don't think this code will run.",
            "It looks like I said semantically it doesn't make sense.",
            "It just looks similar to what it's trained on.",
            "I would think so.",
            "I don't think there was any comparison in his work or or like exactly like an XY axis.",
            "Where is the size in here 'cause it's kind of hard to measure like what is reasonable?",
            "I mean, semantically, none of them are that reasonable.",
            "But syntactically from afar it looks looks good.",
            "It is, it is so."
        ],
        [
            "Ray is actually very good at publishing all the code for his things and people have now trained it on a variety of different other corpora.",
            "Alright."
        ],
        [
            "So the next, the next task is in many ways a lot harder, and essentially the first well, we start that now and then we'll come back to question, answering again, this is a paper by more deran lot of coauthors, neural networks for factoid question, answering over paragraph.",
            "So we're going to try to give you a paragraph is a little bit like the Jeopardy Watson system, except that it was just a couple of grad students and professors and not a whole large team.",
            "Of hundreds of people for multiple years, but basically you're given a question where you have a bunch of facts in the beginning.",
            "They're very obscure, and then they get more and more obvious as you read on, and then in the end you want to basically give an answer in the form of the best entity that describes this paragraph.",
            "Does anybody have an idea what the answer here would be very good?"
        ],
        [
            "Well, read audience.",
            "So this is Thomas Mann here."
        ],
        [
            "Famous German author.",
            "The way this model trains."
        ],
        [
            "The sentence representations here is actually."
        ],
        [
            "Recursive neural network similar to the ones that Chris had introduced.",
            "So here we have word vectors and then we have a so-called dependency tree that basically takes in the word vectors, Maps them to hidden States and basically repeats that application until you have a root vector at the very top and then you can take that route vector and either average over at average it over with others in a paragraph, or if it's just a single entity, or.",
            "Sentence you take it as is."
        ],
        [
            "So what's interesting here is after we train, we can essentially Yep.",
            "Oh yes, the good."
        ],
        [
            "And so does it make use of the whole paragraph, so.",
            "And they actually ran a competition in the end against real people.",
            "The sort of national champions of Quizbowl, which is the sort of a high school and college student trivia competition.",
            "And there basically the model can click the buzzer as soon as it thinks it knows what's going on, and so can human.",
            "So as the human competitors read it or get it read, they can basically buzzin as soon as they know, and so.",
            "You're basically.",
            "Do indeed try to solve it before you read the whole thing.",
            "You don't just use the last sentence, but yeah, it's true.",
            "If it was just given as is, then the last sentence would often be the most helpful.",
            "Discretion."
        ],
        [
            "So now after we trained model to basically try to predict all these potentially different answers here, what we can observe is that with an ice T Sini, plot and projection of the vectors of the different answers.",
            "That again, we observe how the model actually stored different facts inside this continuous space.",
            "Again, we won't be able to pinpoint and say this exact dimension captures that their presidents or anything like that.",
            "But as we projected down to two dimensions from around 100."
        ],
        [
            "Then we still see various interesting patterns.",
            "So for instance, early United States Presidents cluster up here and this overall this cluster here are essentially US presidents and their other clusters of just explorers and emperors, or policy's or wars rebellions and battles.",
            "And things like that.",
            "So they basically each cluster around and model tries to store as many facts as it can from these paragraphs inside the entity."
        ],
        [
            "Presentations."
        ],
        [
            "Now this was in the beginning when they published the first version of this paper, where the model for some of the sub competitions such as history in many cases already outperformed.",
            "People.",
            "And then."
        ],
        [
            "Other competitions though literature, especially it wasn't able to outperform humans very often and the main problem here is actually training data.",
            "So you might think, oh, you know this will stay like that forever, but I think part of the problem was that if you actually want to win these kinds."
        ],
        [
            "Literature questions, then you actually will have to start reading some of the novel.",
            "So if you here want to be able to answer this question just based on the 1st sentence, you need to know title characters that forges father signatures to get out of school invoice draft by feigning desire to join.",
            "So basically that you will never get from either the training corpus by itself or in which they eventually also incorporated Wikipedia articles about the main author.",
            "You would either get it from going inside Wikipedia articles, trying to go through all this up Lings.",
            "For instance, authors, or you'd have to actually go and download the whole books and then try to train on that, and then incorporate that information so without."
        ],
        [
            "That it will be very hard to win these kinds of questions.",
            "Alright, are there any questions about?",
            "Question answering in this sort of factoid way in many ways."
        ],
        [
            "In the end, we basically wanted to store as many facts as possible in this fuzzy vector space, and we won't necessarily be able with this kind of model to reason logically.",
            "For instance over combination effects.",
            "Yeah, so in many ways right?",
            "This was a handful of academics working on this project and creating a reasonably good system that in the end on sort of a National College level, was able to perform on par with the national champions.",
            "So in that sense, it's it's quite impressive.",
            "On the other hand, IBM IBM system is in some ways more impressive in that it has.",
            "Larger variety of questions that it can answer.",
            "Things about numbers and locations of cities and very specific things.",
            "And there are a lot more cultural references that it can figure out.",
            "However, it seemed like a lot of the IBM system.",
            "Required a lot of human engineering and feature engineering for specific types of questions there.",
            "After teams that worked on specific types of jeopardy questions, so it was very hard to take that system afterwards and just apply it to a different domain such as the medical domain or something like that, and then hope that it would still work.",
            "'cause a lot of the there's a lot of feature engineering going on for specific types of jeopardy.",
            "Questions.",
            "Where is he?"
        ],
        [
            "Here we really just train recursive neural networks on arbitrary sentences and then try to predict the answers.",
            "Yeah.",
            "Sure, so."
        ],
        [
            "Basically, here you can use a Max margin loss where you try to predict that this vector multiplied by the."
        ],
        [
            "Entity representation here, for instance of."
        ],
        [
            "Most modern or various presidents.",
            "As you multiply these two, you want the inner product of those to be larger than if you were to multiply the inner product of one of these entity vectors, where the sentence that it doesn't describe it.",
            "All that and you want to basically maximize that difference up to margin.",
            "So unfortunately I don't have a white board here, but it's basically the Max over 0, 1 minus the inner product between the correct one.",
            "Just inner product of an incorrect pair.",
            "I actually described that 'cause we use that same objective function for images and text mapping and you'll see it there in more detail.",
            "How important is it to use what formulism?",
            "Oh, the dependency grammar so."
        ],
        [
            "It actually does work a lot better than the constituency trees in this case, and Chris is also found with Kaisheng.",
            "That dependency trees for sentiment analysis, for instance, can also work better.",
            "But I think actually."
        ],
        [
            "As as the text get larger and larger and eventually, especially for a lot of different factoid questions, it becomes more important to just kind of find specific phrases.",
            "You can probably go and use less and less of a tree structure too.",
            "Right, so that's why I just mentioned as you get longer and longer, you don't need as much structure and you need to for some of the simpler factoid questions.",
            "You just need to find specific phrases and then bag of words can actually also do very well.",
            "As you try to actually understand how a specific actor acts upon a specific object and things like that, then you really want the sentence structures."
        ],
        [
            "Alright, so now in the next fun application is on visual grounding.",
            "So here the main idea is, instead of mapping sentences to facts and entities were trying to map sentences to images and we do that by again having here a deep natural language processing model.",
            "In our case there will be a dependency tree, but you could use a lot of other models.",
            "And we have a convolutional neural network on this side will go into some more details in a little bit and we basically tried to map them into the same space."
        ],
        [
            "Now we will again use dependency trees which capture more of the semantic structure, and we that's essentially the same exact type of model that we used for question answering.",
            "And then on the image side, of course we will use a very deep convolutional neural network and will actually use some pre training on both sides of the model.",
            "So on the language side we will have word vectors that are pre trained and we can essentially use to initialize and have the model understand that.",
            "Cars and vehicles and things like that are very much similar and that will allow us to transfer knowledge alot better.",
            "And you see, an end will actually be pre trained on Imagenet.",
            "And our main objective function is the same kind of Max margin loss over pairs that are correct.",
            "So just to give you an idea here I'll show you the training data set.",
            "So the training data here looks like this, where essentially you have an image and then for each image you have five sentences describing that image, and now for each of these five pairs we want them to basically have large inner product.",
            "If they are indeed part of the same.",
            "The same triplet.",
            "And we'll show you live demo after we go through."
        ],
        [
            "The objective function.",
            "Basically here we will look over each of these pairs.",
            "So for each image I we have 5 sentences J and this.",
            "This is the set of all sentences that doesn't include the five sentences of the image.",
            "And then we will basically here have a margin, and we're going to try to increase this inner product up to this margin and decrease the inner product between the correct image and an incorrect sentence.",
            "So basically try to push incorrect sentence descriptions.",
            "Far away.",
            "Yeah.",
            "You basically can either sum over all the other sentences of all the other images, or you just subsample this some 'cause there are a lot of other sentences.",
            "So you just randomly sample an incorrect.",
            "If in most most likely if you take a sentence describing another image, it will not be a good sense description for that specific other image.",
            "In some ways, yes.",
            "In other ways, there might still be somewhat similar images, right?",
            "You might stop sample here.",
            "Dog images that or images that also include dogs but not in the same constellation and so on.",
            "But it's still in the end.",
            "It's still a heart hard task to optimize, but you in some ways hopefully now can see how to take derivatives of this, which is fairly straightforward.",
            "And then you can basically take derivatives of all the parameters that are inside.",
            "The computation for this, and it's all just back propagation, but it's still non trivial to train the model.",
            "In fact, because the images, image sentence datasets are usually not large enough, you can't really train to whole CNN and back rub through all of it.",
            "Usually just train the top layer that Maps the last features into the Joint multimodal space and on the enemy side you can actually train the whole model 'cause it has many fewer parameters.",
            "It's a great question essay.",
            "Once the model is well trained, you will often observe that this is much larger than this, and then this will, let's say B1.",
            "This will be 5.",
            "This will be .5 or something and then this whole thing is smaller than zero and then you'll get to 0 and you can actually just ignore that example.",
            "So the beauty of the Max margin loss function is that overtime it will only focus on the things that it's still not good at and I'll just ignore the things that can already.",
            "Very accurately distinguished.",
            "Alright."
        ],
        [
            "So here are some examples and then we'll go to a live demo.",
            "So here we do One Direction where given an image we want to find.",
            "This right sentences describing that image and again in the final Test set there 5 correct sentences that should be describing that are correctly describing that image and hundreds or thousands of other sentences that don't describe that image very accurately.",
            "And here we essentially in green Show if that sentence was indeed officially the right sentence, and in red if it wasn't.",
            "And you can see here some some good examples and this one here.",
            "You might think you know this is good the first one, but ended missed the last the next four of the write sentences and actually described different ones or use different ones here that are the nearest neighbors.",
            "Now in terms of inner products in this space.",
            "And if you know a little bit about vision, you can kind of see some of the things that it incorrectly picked up.",
            "So the propeller plane here, kind of that.",
            "We'll the white jet landing gear.",
            "This might be the reason why it confused it for white Jetton.",
            "And in this one looks a little ridiculous because it's an elderly woman, but in the end it's actually not that bad, because other than the elderly, it is a person that catches a ride on the back of something that looks like a bicycle, right?",
            "So you might want to actually better way to compare this here is by engram overlap and in Bleu scores as if you were to do machine translation.",
            "Yeah, of course.",
            "So those are all from the test set.",
            "There aren't images or sentences from the training set.",
            "OK, so so.",
            "This picture.",
            "So."
        ],
        [
            "Good question.",
            "So you basically take an image and then you compute the inner product of that image vector with all the sentence vectors from the test set and you pick the nearest neighbors.",
            "Based on that inner product.",
            "Yeah.",
            "That's right, yeah, you subsample you subsample.",
            "Some of these here to speed it up.",
            "At the end, you actually observe that a lot in Max margin objective functions where you could spend a lot more time on each example and try to make sure that those five sentences have a better in our product and all the other ones.",
            "Or you can just more often go through more examples, and it's often more advantageous to go through more examples and train each just a little bit.",
            "Yeah.",
            "It's not what.",
            "Oh yeah, it's not.",
            "But that's that's OK.",
            "It turns out to be just fine with just standard SGD.",
            "We used to use even LB FGS for these kinds of functions, and that work too, but in the end, just mini batch SGD works just fine.",
            "Yeah.",
            "Yep.",
            "The Lambda here basically just gives you a margin, and it's another hyperparameter you can set.",
            "I often just set it to one.",
            "That's fine, but sometimes it makes sense depending on the rest of the model.",
            "To make it a little smaller, or if you want to push the margin to be even larger and try to have it be even more robust, you can tweak it, but usually it's somewhere between .1 and two.",
            "It's not not a large range, it's basically just a margin between after which if it can accurately.",
            "Put the correct sentences so far apart from all the incorrect pairs.",
            "Then you're done with that combination and makes it more robust at Test time.",
            "It's a great question.",
            "We haven't done it in this work, but I've tried it in previous work before where I tried to be very clever about my subsampling, where I say, let's say I have a unit sort of ball around a current current sentence vector image vector, and then I tried to not sample the ones that are very close, but then sample some that are little closer and not.",
            "Really far away.",
            "Lots of things like that didn't matter at all in the end, just random subsamples were the way to go for two previous projects before this one.",
            "So in this one we just took it as is.",
            "But yeah, intuitively, I felt like this should help.",
            "Didn't in practice."
        ],
        [
            "Alright, so."
        ],
        [
            "So here are some numbers.",
            "All these numbers have now actually been improved there.",
            "Now, larger datasets that have even more training and testing images, and they're even better models out there too.",
            "At the time in 2013, this was the state of the art results on this data set the Flickr One Catus data set from UIUC.",
            "And what's fun is that it actually in in many cases does does quite surprising and cool things, so I can type in bird, for instance.",
            "And now what we do here is whenever I type in language in this box, that basically Maps that language into a vector space and then it has a precomputed set of.",
            "All the test images from the data set that essentially.",
            "Try it, you know it has a vector for and then computes the inner product between the vector of the text in the box and then depending on how close they are basically mapped mapped, how close they are to the word.",
            "So basically it only looks at the pixels right?",
            "There's no as in standard sort of web search where you actually have text around the image is this is just based on the pixels you map it through this year end to CNN to the same vector space and then you find inner products.",
            "Yep.",
            "So the only meaning is the distance between the center of the box to the center of the image where where it is around that unit, like around the circle doesn't matter.",
            "OK, so if I type in bird we basically get similar into standard image classification pictures of a single bird.",
            "But what's cool is I can type in birds and now it actually tries.",
            "It has learned some sense of plurality and is trying to show me pictures only of multiple birds if they aren't datasets, I would actually love to make this public, But the problem is you really need to look at what images there were and what what the training data look like.",
            "So if you look here an you basically.",
            "It will only be able to do things with these kinds of these kinds of objects, so if you try to type in zebra or something, it will just show you something random 'cause it's never seen.",
            "As he seen a zebra.",
            "So that's why this demo isn't public.",
            "So now we have single words.",
            "That's pretty standard, but now the cool part is we can also go and type in multiple words.",
            "And that's a little slow.",
            "And now it tries to actually combine the two and has learned some sense of compositionality how these words map to, ideally, something visual that makes sense when you put them together in that order, so birds and water's trying to find you know birds and water or birds and trees, tries to find.",
            "The few images of birds in trees.",
            "You can also have different kinds of adjectives, so if you just type in yellow for instance, then it just shows you a bunch of random yellow objects.",
            "If you type in yellow.",
            "I know boss or something, then tries to closest to matches or actual yellow buses and then the next closest matches are just the other buses in the data set.",
            "You can type in horse an it will show you.",
            "Pictures of single horse and then horse with bald man for instance.",
            "That actually is able to pull out.",
            "We get in a second.",
            "There the one picture of horse with a bald man and in the interesting errors it makes are also ascentia Lee people with helmets who from afar into model that."
        ],
        [
            "Doesn't know much about the war."
        ],
        [
            "Look somewhat like people who are involved, so this is basically one fun life demo of this multimodal mapping.",
            "Now another fun task that is actually much harder for humans.",
            "'cause it.",
            "It shows like a baby just doesn't make any sense.",
            "Yeah.",
            "That's right.",
            "Understand exactly what's going on.",
            "That's right, I mean, if it doesn't have something in the test set of images that can retrieve, then it just retrieves the next couple of images.",
            "There will be.",
            "There will be further away, but it will just retrieve that an in general, especially this model.",
            "But even the models that came afterwards, you're right in many cases they don't really understand the images, right?",
            "There's still a lot of work that can be done there.",
            "Sometimes you type in Black Horse, but then all the background is black and.",
            "It's actually, you know, a Brown horse or something like that, so there's still a lot of issues and we'll get to that in a second after after this demo.",
            "But eventually you can try to generate sentences too, and there you see even more issues where it really.",
            "In some ways it captures core current statistics of certain patterns in images for certain words in the language.",
            "So many of the words like width and next tool and colors weren't an image net and they still worked as long as you had training examples of the multimodal data set.",
            "It's kind of hard to put a number on that, so we already know, for instance, all the adjectives, all the prepositions, and all these things aren't in there, but you need those words to combine real sentences.",
            "I don't know if there are any.",
            "In fact, I think the authors of the Flickr data set from you, you see actually looked at and focused on words that are more likely in image net already in the 1st place.",
            "So just might not be the right data set to run that comparison.",
            "My hunch is though, that the image net kinds of vectors that you get from CNN's are quite general.",
            "So what I showed you.",
            "In here, for instance, so image net does not have any blood samples, but somewhat surprisingly, it still helps to have image net initialized vectors on blood samples, so they're really quite a good general representation of images.",
            "Read one more question, I think.",
            "Alright, so here is another task that also requires understanding both images and sentences.",
            "But instead of trying to map 1 to the other, if there are similar, it essentially tries to predict engagement.",
            "So what do we mean by engagement and social media?",
            "It could be either you click on a link or you favorite it something, or you retreat it something and.",
            "Here is an example of a photo that is engaging and of text that I'll show you in a second that is not engaging, and overall this is not very engaging.",
            "At roughly .5 overall and when we look at that tweet, we observe that that is indeed true.",
            "So in this this tweet comes actually from several months after that training data set had been collected, so the training data set here.",
            "There's no magic, but basically tells you for this tweet.",
            "And given this specific vertical like movies or cars or.",
            "Entertainment and things like that, and this was above average in terms of the engagement for that particular user who had these kinds of numbers of followers and things like that.",
            "So you had to normalize quite a bit and properly understand.",
            "How to actually collect your labels.",
            "But once you have done that, it's quite amazing and quite accurate.",
            "So here we have the tweet text.",
            "Take a closer look at the world of Tomorrowland and somewhat interesting Lee.",
            "Here you have take a closer look and it's actually as a phrase, not very engaging.",
            "But on.",
            "The image here.",
            "So basically people don't like to be told what to do.",
            "This is 1 takeaway message here for this overall tweet.",
            "Now on the image side though, this is pretty engaging and we can also query the mall and ask it why it thinks it's engaging and basically F here.",
            "Hence the long hair and the cool looking buildings in the background and so we can essentially query the scene and also about how much each part mattered and now to contrast this, this is a.",
            "Very engaging tweet.",
            "This says Matt Max gathers up all that we seem to crave from our movies and Yanks it to the limit.",
            "Great sentence, very engaging.",
            "And now the picture is also really engaging.",
            "Just cool background the car the I lot of.",
            "Lot of good things and if we look at that.",
            "Tweet indeed.",
            "It had gotten, you know, a lot of retreats in favorites for a movie, so this is just, you know, these are just example."
        ],
        [
            "Loads of multimodal applications where it makes sense to map images and sentences into same vector space and try to predict either one from the other or try to predict an overall label that requires understanding of both.",
            "Alright, now this was.",
            "Gary's but we didn't do it.",
            "There are lots of other things you could try to incorporate on top of that time series and so on, but we didn't work.",
            "So now I mentioned that instead of just finding the right sentence, there's been follow-up work by a bunch of people earlier.",
            "Earlier this year, yeah, right, so they all put put it on archive late last year, but then all the papers came out this year basically trying to not just find the right sentence but actually generate that sentence.",
            "And so the main idea was you again, take a CNN similar to what we described before, but instead of having a recursive neural network that just Maps existing sentences, you have a recurrent one that tries to sample now conditioned on the image tries to sample.",
            "The description, so it's very similar to the machine translation work that I described yesterday, where we have that same kind of LCM that basically has one hot output vectors, but instead it actually.",
            "Just finds descriptions and so you might think, wow, this is cool is so much more epic than just finding the sentences.",
            "But one thing after some careful analysis and follow up work folks at Microsoft."
        ],
        [
            "Actually found is that a lot of the times they."
        ],
        [
            "Sentences that it generated were actually exactly verbatim test or training data sentences.",
            "So in the end the LCMS were just very, very good at memorizing dose training sentences and then generating them, but really kind of just from memory, and it's."
        ],
        [
            "Especially, you can observe that when it adds a bunch of facts to us to an image that had absolutely nothing to do with the image, but we're sort of like that.",
            "And in training data, nevertheless, there's you know this is a much more interesting and next step than just finding them.",
            "There's still like 20 to 40% of the sentences are not exactly like that in the training data, and here are a couple of examples where it works.",
            "Very well, so this one is a nice sort of standard sentence, something that you might observe a lot in the training data to then as it gets sort of to weirder and weirder combinations, it will often fall back on things that are more similar to what it seems, so there not that many ferrets in the training data.",
            "So I will say it's a cat.",
            "Not many pictures of little babies holding.",
            "Toothbrushes so it will be a baseball bat.",
            "Also teddy bear, but it's it's in many ways.",
            "There's again no magic, right?",
            "We learn core current statistics of specific types of patterns, but deep learning is extremely good at learning those kinds of statistics and in a very good way to generalize is much better than.",
            "I think any other machine learning MoD."
        ],
        [
            "Right now all right?",
            "So in the remaining time I want to talk about dynamic dynamic memory networks.",
            "I'm very excited about this work because I feel like it has some potential to generalize very well to a lot of different."
        ],
        [
            "Task so the first observation that we've made is that in some ways all NLP tasks can be reduced to question answering, and this is an insight that I've had over some drinks, but I think actually some students from Russia's group like four or five years ago at NIPS are just sort of having 70 good time.",
            "And I'm like, you know guys, everything is a QA problem.",
            "They're like OK, Richard, but So what?"
        ],
        [
            "I'd like first there some more sceptical and you can come up with good examples of what it is, but once they're like OK, that's true, but is it very useful?",
            "Maybe not.",
            "So here are some examples you have sort of the standard question answering type problems where you have some semantic typical question, answers things you might want to search online.",
            "You know where was Obama's wife born?",
            "You might want to then reasonover different facts and a knowledge base and things like that.",
            "That is the standard QA task.",
            "That's right.",
            "Everything is fresh and dancing even yeah.",
            "So then you can might ask machine translation, right?",
            "What is the translation of that sentence into French sequence modeling tasks like named entity recognition and say what are named entities in that sentence?",
            "Classification problems very simple.",
            "What's the X sentiment, engagement, and so on?",
            "And even multi sentence joint classification problems like coreference resolution.",
            "You could ask like who does their refer to in the last sentence where it's their car was stolen and before that it was mentioned who's car it is.",
            "So really pretty much every task in NLP and I encourage you to try to think of examples.",
            "I couldn't get out of where that wouldn't work.",
            "There you go.",
            "There you go.",
            "That's exactly the."
        ],
        [
            "It's interesting.",
            "But maybe useless until."
        ],
        [
            "We have a model that actually makes it useful, because what it does is that it always only trains on some input and a question answer pair over that input, and that's exactly the dynamic memory network and part of the reason I'm excited about it.",
            "So it's essentially a neural network based model.",
            "It will be end to end trainable, in which any QA task can be trained using just these triplets of input questions and answers.",
            "And it has several relation rationes to previous work that also use the idea of memory.",
            "Basically being able to retrieve or pay attention to specific facts from the past.",
            "Fortunately, Phil introduced those to us yesterday.",
            "They have memory networks, neural Turing machines and other recent work from I think now DeepMind Google Deep Mind Teaching machines to read and comprehend.",
            "So there's some related work to this, but I think the dynamic memory networks are the most general in that they only they don't.",
            "Only tackle specific types of algorithmic tasks like neural Turing machines or only the traditional type."
        ],
        [
            "Types of question answering, but really a whole host of other kinds of tasks.",
            "So this led us to this work, which is already an archive.",
            "Two it's called ask me anything dynamic memory networks for natural language."
        ],
        [
            "Assessing and this was joint work with a large team of interns were very lucky this summer at Megamind and had a whole host of Super smart interns.",
            "Some of them actually owes on for almost 8 months."
        ],
        [
            "And here's here's the first sort of output input output pairs that actually came from this model.",
            "And then we'll go and dig deeper into how this model actually works.",
            "So here is a first example.",
            "This actually comes from Facebook data Set, which is synthetic and has had some issues, but it kind of is an interesting sort of necessary but not sufficient condition for you being able to reason logically over over some facts, so here.",
            "We have sentence Mary walk to the bathroom center into the garden and so on.",
            "And you ask, where's the milk?",
            "And basically to answer this question, the model has to 1st understand that you know something to do with Sarah, Cassandra or Sandra took the milk there, but now you still don't know from that input where the milk actually is.",
            "Now needs to find out where Sander was and she went to the garden.",
            "So the answer is garden.",
            "So that is the more traditional kind of question answering task.",
            "Now the next input could be everybody's happy and you ask them all what's the sentiment and gives you positive you can ask for the named entities.",
            "Part of speech tags or, you know, in French, that translation into French and these are all coming from them."
        ],
        [
            "So here's the overview, and we'll go over each of these modules in detail in the next couple of slides, but on a very high level.",
            "We tried to organize some of the existing work that people have worked on already, such as word vectors, and try to see them as almost software engineering modules.",
            "And whenever we have arrows here, this will be a fairly.",
            "Straight forward forward propagation in a neural network and at training time those will all go backwards and the whole thing can be back propagated from the final answer.",
            "So we will have some input text sequence or input module.",
            "That I will have also next to it.",
            "A question usually associated with some kind of specific input.",
            "You can also say general input and you kind of search it based on the question, but for now we'll assume there's a specific input that is a text sequence.",
            "We have a question now that question will trigger.",
            "Specific ways would pay attention to parts of that input.",
            "Those will be regurgitated or reasoned over in the episodic memory and from the episodic memory, and the question will then try to generate an answer.",
            "And in some sense, here semantic memory is just a rewording of word vectors.",
            "But you can also add other kinds of facts in their knowledge bases and back propagate facts into into the semantic memory."
        ],
        [
            "Alright, so let's look over the modules one at a time.",
            "So the input module is essentially responsible for computing the representations of textual inputs.",
            "But as you're sure I mentioned, this could actually be any other kind of input.",
            "Any kind of input you can run a sequence model over could be used and then try to be retrieved later on.",
            "You only assumption that we make here is that there's a temporal sequence that can be indexed by a certain time stamp.",
            "Then for written language, essentially these will just be our word vectors and we can train the word vectors and this general module, both unsupervised and supervised.",
            "So the input module can train word vectors just like any other normal like worked avec org.",
            "Love kind of way to train current statistics and propagate those into the words.",
            "But you can also eventually from the task themselves propagate facts into the word vectors and into this module.",
            "You'll eventually get context independent, just word vectors and context dependent hidden states that really take into consideration all the knowledge from before and in some cases you can also have a BI directional.",
            "Sequence model that takes into consideration facts from before and after.",
            "In our work we just use the cloud vectors from Jeffrey Pennington from last year to initialize the semantic memory module, which is basically given as input to these.",
            "Input modules.",
            "And then to compute the context states we will use a recurrent neural network and now we don't need to go through this in too much detail."
        ],
        [
            "'cause we already went over that yesterday, but this will essentially be a large gated recurrent unit.",
            "You can also use analostan if you want there.",
            "It's pretty pretty independent of that decision.",
            "We often compared both.",
            "In many cases the Gru works slightly better for machine translation systems work even better than than Gru's, so this is standard.",
            "Yeah, you and we assume that this HT is essentially the output at every time step for the input module.",
            "Any questions about the input module?"
        ],
        [
            "It's a good point.",
            "You don't necessarily have to do it.",
            "You can actually go always through the input sequence.",
            "Very good observation.",
            "Um?",
            "So the question is actually very similar.",
            "It's the same Gru.",
            "Actually it just takes in also word vectors for the question an computes a final question vector Q.",
            "We often will drop the sub index T. It's just the last time step of the question sequence.",
            "So just words cheer you final output."
        ],
        [
            "So we'll go into that in a lot more detail right here and then in the summary again, to really understand the details.",
            "So basically the goal, the high level goal of the episodic memory is to combine the previous three modules, the semantic input and question modules outputs in order to reason over them, and then give the resulting knowledge and form of again a vector to the answer module.",
            "And essentially the goal will be that based on the question, we can dynamically retrieve the necessary information over our inputs.",
            "And.",
            "If that is not enough to just iterate over all the inputs once, because maybe you get new information and you need to now go back and look for other information.",
            "It has the ability to iterate over the input and that is very crucial for exactly these kinds of examples here where."
        ],
        [
            "Might ask where is the milk?",
            "And now based on that question to model might go over all the text and try to find any fact for milk.",
            "Any facts about milk and then realize OK Sandra took the milk there now only now after it went through all the facts does it know that Sandra is actually important for answering this question.",
            "Now if it only had that ability to go over the input sequence once, it wouldn't be able to actually tell you much 'cause it hasn't paid attention to Sandra in the first iteration.",
            "So now it needs to use those facts and iterate over the text again and try to find facts about Sandra, so that can then answer the question."
        ],
        [
            "Alright now yeah, like this is exactly what I just mentioned.",
            "We really for transitive inference we need that ability to reason over and retrieve and pay attention to specific time steps.",
            "And this was kind of the idea.",
            "Being able to think of specific time type specific points in time over your sort of previous experience is the reason we call this the episodic memory an there's some interesting neuroscience behind episodic memories too.",
            "In humans, so the seed of the episodic memory in humans, actually the hippocampus.",
            "And if you disrupt the hippocampus that actually impairs your ability to reason transitively over logical facts too.",
            "And."
        ],
        [
            "So now let's look over the details of this model.",
            "This one is sort of least beautiful equation of the paper, but it's a fairly simple way to get there.",
            "Basically, this is a way to compute gates.",
            "So let's say we have here a specific sentence.",
            "We have a specific memory we'll get to that in a bit.",
            "Basically, the memory in the very beginning will just be the question itself, but then we'll update the memory based on.",
            "Iterating over the inputs and now we'll have a large concatenation here of elements that basically try to correlate a specific sentence in the input with the question, the sentence with the memory that we currently have in the first iteration.",
            "Again, it's just a question, but then overtime as we accumulate more and more facts and the memory, this will change and I'll tell you how in a second and then basically just a bunch of different correlation and sort of distance metrics between.",
            "All these three vectors.",
            "And then we just pipe it through a deep network of very simple two layer neural network to get a gate.",
            "Now this is based."
        ],
        [
            "A way that the question can turn on specific gates over the input sentence is."
        ],
        [
            "And then."
        ],
        [
            "Then we can finally summarize the important facts in an episode vector, where we take the softmax over all these gates and sum them up over the sentence vectors at the different time steps.",
            "So."
        ],
        [
            "So this is exactly here an example."
        ],
        [
            "Of what happened if we only passed over data once and that we couldn't actually?"
        ],
        [
            "Sir, I already went over that.",
            "Now the ability here of the model to be able to iterate over the inputs multiple times is very important.",
            "So the first time the iterates over them, the memory state is just a question, but the second time we will actually update the memory state based on separate Gru.",
            "So now we have an input Gru.",
            "We have a first Gru over the inputs that is triggered from the question and then we have.",
            "A third year you that tries to accumulate the facts, so this one here again."
        ],
        [
            "Was a combination of all the facts that mattered based on a weighted sum of those sentences.",
            "So here the facts are in the sentences.",
            "The gates tell us how important is that sentence.",
            "To answer the question right now, and you basically have a sum over."
        ],
        [
            "Those.",
            "And those are the final inputs to 1/3 Gru in that sequence.",
            "So it's basically this is what I meant with like having the Lego pieces and keep putting them together.",
            "It's a pretty complex way of putting them together, but it's also fairly clean, right?",
            "You just have a large set of."
        ],
        [
            "Practical Gru's and then the answer module is in many cases many ways very simple.",
            "Again, you just have again sequence model Gru that basically takes us input.",
            "The question at every time step and the previous previous output just again just to generate if model just like we had in machine translation where you output award at each time step during the decoding and the previous time step of that answer module the hidden state.",
            "Of the Gru."
        ],
        [
            "So the YT minus one is the output of the softmax that you input to the next step.",
            "Oh well, right?",
            "So you actually do similar to what people do in empty where you actually take the single one to one hot vector of that output.",
            "So this is putting it all together in one large graph.",
            "So let's walk through it again.",
            "So here we have some input story.",
            "We compute our input module.",
            "Independent of what the question is, we could ask multiple different questions on that same input.",
            "Then we have a question here where is the football now?",
            "We have a vector, a question vector from the question module Q and now that question vector triggers an attention over these different facts.",
            "So you might say where's the football?",
            "The first fact that mattered this is these are the equations that or the arrows that come into here are basically John put down the football.",
            "That is the only one that has a very large weight because there isn't.",
            "There aren't any other facts about the football.",
            "In this input sequence, so John put down a football gets a weight of one for this Gru.",
            "And you know, we kind of.",
            "There are some other ones that might have tiny bits of weights, but we just assume or we just don't show the arrows here, but in general there they all have the option to go in here.",
            "It's just that if the wait is close to zero and this is an actual example from the test set, if the weight is close to zero, we don't draw the arrows.",
            "OK, so now we have here jump down football is the most important one now that gets fed into this Gru that has its input.",
            "That you know.",
            "Inputs that are weighted by the attention mechanism coming from the question module and all these interactions and the gates.",
            "Now that gets fed into the third year you which goes over the memory.",
            "And now that memory Gru is now the input together with the question over another iteration over the input module and there is in fact a classifier that selects whether it has enough information to answer the current question or not.",
            "So this is dynamic.",
            "How many steps how many iterations you do over your input?",
            "Some question, some question, answer, input, triplets require you.",
            "3 require three iterations over the input.",
            "Some are very simple.",
            "You know John put read the book what the John read the book and so then you only need one iteration over the input and this is dynamically determined by the model.",
            "And then once you basically iterate over this reasonably large wait to John, move to the bedroom 'cause it thinks you know.",
            "Maybe that's where the football is, 'cause we know John mattered, but then later on realizes and this is.",
            "One beautiful difference here.",
            "Between the dynamic memory network and the original memory networks from Jason West and they had to have because they had a bag of sentences representation, they had to add an extra feature saying this sentence happened before that other sentence in some ways and manually defined feature.",
            "Whereas here because it's all the sequence models, actually learns what things happen before or after.",
            "And then we have the final output here M and that's what's given as input to the answer module.",
            "Now you might ask, how do we do the sequence generation?",
            "And we actually enable the model to also have outputs.",
            "So the answer module is actually a little more complex than in this CAS, the depending on the question, the answer module can actually be triggered at every state of this input as well, so not just at the end, but also at everyone.",
            "And so that's very important.",
            "And much easier to train and test if you have sequence models, but also questions like give me a list of all English Kings or something like that where you don't want to have to ask the model to 1st memorize all of them in one vector and then try to output them all.",
            "But it can basically output them right away.",
            "Yeah.",
            "Where is the football?",
            "Oh, I think because of the data set and will actually go in a live demo and we'll see how it's very specific in some ways.",
            "And this is a problem with the synthetic data set.",
            "Where is the football usually to ask?",
            "Where is the football right now?",
            "Or at the very end?",
            "And the model here already figured out that as you go over this, this was the last time and so it just puts a large weight on that more often and then sometimes in the data set.",
            "John moved to the bedroom is actually like that was the last time John was mentioned, so it doesn't know for sure.",
            "But in the training data sees so often.",
            "Multiple facts about it that it knows that for these kinds of questions, the lost state matters much more so than others.",
            "Alright, so now."
        ],
        [
            "The tasks that this model gets above state of the art performance and then close to it, plus minus usually like two 2% accuracy or trueblue points are these three here.",
            "So we have question answering that sort of requires some logical reasoning and we'll go over some more examples.",
            "In a second, the types of questions that are in this data set.",
            "Outperforms Facebook only by like .3%, but it also doesn't require sort of the previous sentence feature or engram or character ngram or word ngram kinds of features.",
            "It's basically all enter, end, trainable sequence models.",
            "Then it actually outperforms almost two decades of NLP research on part of speech tagging.",
            "This is a pretty old data set the Penn treebank and then on sentiment analysis it also outperforms a lot of other.",
            "Papers that some of which are my own where we use tree structures, but it essentially just goes over these inputs doesn't require that restructures to very accurately classify the sentiment.",
            "And then on any art, we're still 2% or one point 7% or so below state of the art.",
            "The state of the art actually uses also deep learning at this point and.",
            "Actually trains word vectors together with the.",
            "With the named entity tags and a very large gazetteer features so lists of cities and peoples names and so on.",
            "It's a it's a tough task and on machine translation we only looked at English and French were 2% true blue points below the Google like LST Emtala stem results that were described yesterday and then coreference resolution.",
            "We actually now have the state of the art on this data set from this year.",
            "Go ahead.",
            "I'll some folks at Maryland.",
            "And then we're just a little bit below on a Connell data set for coreference."
        ],
        [
            "So here's some more details before we go into live demo.",
            "So these are basically different subtasks that the Facebook data set includes.",
            "So there are single supporting facts that are very simple.",
            "You just find one sentence and you're done.",
            "We have two supporting facts, three supporting facts.",
            "And you have yes, no questions counting lists, so this is this is kind of interesting, so here for all these kinds of questions you really only have one output vector in the end.",
            "But it's still pretty good at counting and lists, and much better than than the original memory networks.",
            "Simple negation is fairly straightforward, indefinite knowledge, so answers that include may because it doesn't give you exact facts.",
            "We have some simple coreference resolution.",
            "Time reasoning and then the model the tasks were actually all the models are still fairly bad at is our positional reasoning and pathfinding and we tried to understand why we still really haven't really understood why we're not that good at those.",
            "In theory they should work too, but in practice they don't.",
            "So this is the Bobby data set.",
            "These are the results on.",
            "Part of speech tagging, part of speech tagging, is a pretty fought over task for over two decades, so you're in sort of just the last couple of percentage points out of it, and on sentiment analysis here.",
            "We have the state of the art on the binary task, which we're now at 88, and our best recursive neural tensor networks were at 85, so you're really getting more and more of the really hard cases right, but in many cases even or sentiment is actually an interesting task in that you can very quickly get to something like 70% with a very simple bag of words, baseline right, 70%, not hard.",
            "You just need to find all the senses that mention awesome, wonderful.",
            "Amazing and then there's only so much you can do to the word amazing and wonderful to make it negative, right?",
            "So you get somewhere pretty quickly, but then as you get, try to inch out the last couple of percentage is at that point you either need to have more and more world knowledge so you have the larger and larger data set that understands that if you talk about I don't know Matthew Mcconaughy, it's probably a good movie and I don't want to mention any other actors names.",
            "But you know, there's some actors that are associated with pretty bad movies and you just kind of have to know that.",
            "And or you really have to capture complex syntactic structures, like expert wises, contrastive conjunctions, where you might say in the beginning.",
            "I like the movie, but very quickly it petered out and became pretty boring and you now need to understand that this, you know, expert, why the Why matters more, so it actually becomes a more interesting task as you get higher and higher performance.",
            "Alright now let's look at some examples.",
            "Of this, so here we have.",
            "A story yesterday, Bill Journey to the office, blah blah, and now we ask where was married before the bedroom.",
            "And now you can think about how you would solve this, but essentially here we can visualize the episodes and basically realized OK. First it asks very puts very much emphasis on where Mary was when Mary was in the bedroom, and so was this afternoon and then basically realized.",
            "OK, that's not enough to answer this question.",
            "It has the second episode goes over it again, and then says Mary Journey to the kitchen this morning.",
            "And so the answer is kitchen.",
            "So this is just one example, here's another one.",
            "So let's see, maybe we'll try to do it together and see if we are very good at it.",
            "So what color is Greg?",
            "Now how would you solve that problem?",
            "Craig's right now.",
            "Alright, very fast, that's right.",
            "So what's interesting here is that it actually requires multiple steps of reasoning, right?",
            "So first you say, alright, Greg, what's important about Greg?",
            "He's a rhino now.",
            "It doesn't tell you Greg is white, so it's clearly more than one episode that you need.",
            "But now we look at rhinos.",
            "And there's also no sentence that just says rhinos are of this color.",
            "But instead it actually says, alright.",
            "So we look for Rhino.",
            "Bernard is a rhino.",
            "OK, now we need to look for Bernard and then it tells us Bernard is white and let's hope the model got it rained.",
            "Yes.",
            "I mean, that's actually an interesting that is actually interesting.",
            "Point here is that in many cases, so this one here is kind of simple.",
            "But if we just change no burden artist some other color, that's exactly right.",
            "It can actually now do things based on that story, and in some sense that is a very different regime to what we usually do have just capturing a lot of current statistics right?",
            "Because it put the attention on that sentence, it can output with a much higher probability that word.",
            "Now if you said like.",
            "Some crazy color that wasn't ever in the data set.",
            "It still won't do it.",
            "In fact, it's kind of easy to add.",
            "You could just edit word vector and then at Test time train you or vectors to then be and add them to the softmax and things like that, but it still to me a much more interesting regime 'cause it's different to just re collect a lot of core current statistics and then capture capture those.",
            "It really needs to do something for that specific story.",
            "That is right.",
            "That's right.",
            "And the only reason it gets his right is 'cause it's seen.",
            "Other kinds of examples of this nature.",
            "They weren't exactly that in most cases, though, that data set has a little bit of overlap for some of the types of questions, but in general, if it's never seen, that kind of input question answer triplet, that would have given it this type of reasoning, it wouldn't get it.",
            "But if it's actually sees that you know rhinos can be of different colors and things like that, then some answers might be.",
            "Maybe then, it would also eventually say maybe you know if you ask, like is the color red or something like that.",
            "Yeah, so there's basically again only if you have.",
            "Only if you have this in the training data, but there are indefinite knowledge kinds of questions now.",
            "One interesting problem that.",
            "Right?",
            "It's not going to answer maybe and the problem here is that, and this is exactly the problem with these synthetic datasets, or we just may be the size and complexity of this particular synthetic data set, but the indefinite knowledge kinds of questions.",
            "Won't generalize to other kinds of indefinite knowledge questions, so I forgot what exactly indefinite knowledge questions asked, but there you might ask, maybe, but if you now take one of these, yes, no questions or like 3 supporting fact question and you add somewhere in between that line of reasoning, some other fact, it won't say.",
            "Maybe it basically doesn't combine as much as we would want all these different types of reasoning steps.",
            "It really learn specific patterns.",
            "From each of the sub datasets.",
            "Sorry we have one Darren.",
            "I'm walking North and then I turn right?",
            "I mean that that seems like something that is reasonably simple if it's seen if it snows about North East, and West that it should be able to do those kinds of things.",
            "Yeah, 'cause it has the IT has the theater.",
            "It has a sequence of steps that you've seen before so it knows in that context.",
            "In the input module you know what happened before.",
            "Performance degrade.",
            "So three supporting facts.",
            "I mean, that is basically the reason.",
            "So I think the main main problem there is that we don't have very variety of different features and it can retrieve in sort of a bag of features, model the different aspects.",
            "Perfectly we have to we go over the sequence and as the sequence gets longer and longer and the three supporting fact questions sometimes you can have 40 or so inputs, the sequence degrades an I think they're probably smart ways of increasing the LCMS, and things like that for that particular task, but then you may be more likely to overfit on other tasks.",
            "Representation for memory.",
            "It works.",
            "So I mean, it's a little more complex to just describe the whole model in a sentence without many visuals, but you have essentially bag of words and bag of ngram representations for the different sentences, and you can retrieve them after you retrieve the first one.",
            "You have perfect access to all the other ones.",
            "Again, you don't try to store them all in one sequence.",
            "I think that's probably the main reason why they don't degrade.",
            "Yeah."
        ],
        [
            "So this model won't do that well, but the sentiment model that was trained with the same.",
            "It's the same kind of DMN will actually get that, so maybe that's a good segue to say like come up with some hard examples.",
            "So despite the wonderful reviews.",
            "This movie wasn't a particular particularly positive experience, and now it's that ask what's the sentiment?",
            "And now it will tell me it's negative.",
            "So this is a somewhat harder example, so you have wonderful reviews and positive experience and have two interesting kinds of negation.",
            "Despite this, not sort of very obvious negation.",
            "And at least it wasn't.",
            "Here is further away from the positive experience that it's trying to negate, and it still gets this.",
            "Very correct, and here's a little bit of an attempt at visualizing what's going on.",
            "So here we basically have the hidden states of the input for each of the words, so despite it starts in a specific way and then you have here the wasn't.",
            "For instance, once you have the negation, some some of the hidden units basically turn on.",
            "Or become much stronger than they were before, and this is an attempt to sort of say, alright.",
            "If we had a classifier at every time step.",
            "This is just for visualization.",
            "It's not actually necessary for the model.",
            "What is going on?",
            "So it says here, despite the wonderful reviews.",
            "So it's very mildly positive, but not very much so.",
            "And then once you say it wasn't, it basically is very hard to recover from that.",
            "And even if you say positive experience.",
            "Basically, knows once there was a nod here that these things will be negated even if they are positive.",
            "That is an excellent idea and exactly what you would want to do and what we may or may not be doing right now.",
            "Yeah.",
            "So the I'm not sure the."
        ],
        [
            "1010 memory I don't think it gets higher than the 93.6 on the Bobby data."
        ],
        [
            "That, and I don't think any of the other memory networks actually show results on as well, like the question answering traditional question answering as well as sentiment classification, which is much different and as well as.",
            "And maybe this is.",
            "Right, well and it's.",
            "It's all sequence models that also allow you to output outputs at every element of the sequence or at the end.",
            "So here for instance, we ask what's the part of speech and basically have here an answer of the part of speech sequence for that sentence.",
            "So they're basically three completely different kinds of tasks, and I don't think any of the other memory networks.",
            "That kind of line of work has been shown to get state of the art results on three pretty competitive datasets.",
            "So in the interest of time will have only three more minutes.",
            "Maybe we'll get to your question at the end, but."
        ],
        [
            "So with that I want to conclude basically there's a potential statement here that all but if you can think of some examples that aren't, let me know.",
            "I think all NLP tasks can be reduced to a question answering kind of problem and in the beginning it was kind of a trivial but useless insight until you have a model that tries to actually use that and train on that kind of input question answer.",
            "Mail type triplets.",
            "And so I think one of the next interesting steps will actually be to train one very very large multi task.",
            "Yeah man, right now you can essentially trigger different demands to pipe depending on the question, but if you actually and we train on these three task we can actually train a multitask model too.",
            "But then you won't get to you won't inch out the last couple of percentage is because the different tasks will interfere little bit and reduced accuracy by like 1 or 2%.",
            "So that is kind of an interesting next question.",
            "How can you compartmentalize some of the knowledge and still have these state of the art model on that task, but also actually have all of them the sharing the weights in some way so compartmentalized but also share the weights in one gigantic DMN and I'm pretty excited about, you.",
            "Know if we can get there for NLP and deep learning.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Glad you're here.",
                    "label": 0
                },
                {
                    "sent": "'cause today we can actually use all the stuff we learned yesterday about recurrent neural networks and also some of the days before and recursive neural networks and put it all together for a bunch.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fun applications before we do that and focus on natural language processing.",
                    "label": 0
                },
                {
                    "sent": "I actually want to give you at least one reason why.",
                    "label": 0
                },
                {
                    "sent": "Maybe on your research side, you might want to focus if you're already hooked on deep learning on natural language processing instead of on computer vision.",
                    "label": 0
                },
                {
                    "sent": "I think in the end computer vision will actually be very much commoditized in the next couple of years, or in some ways it already is.",
                    "label": 0
                },
                {
                    "sent": "So let me show you an example of what I mean by this.",
                    "label": 0
                },
                {
                    "sent": "Let's say you wanted to train your own new image classifier.",
                    "label": 0
                },
                {
                    "sent": "And you don't yet know you know how well it would work to distinguish two different classes.",
                    "label": 0
                },
                {
                    "sent": "So let's say for instance, you have some chocolate chip cookies.",
                    "label": 0
                },
                {
                    "sent": "And you want to train a classifier that distinguishes chocolate chip cookies from oatmeal Raisin cookies.",
                    "label": 0
                },
                {
                    "sent": "Nowadays all you need to do is say alright here I have some chocolate chips, cookies, chocolate chip cookies.",
                    "label": 0
                },
                {
                    "sent": "Then here I have five oatmeal Raisin cookies.",
                    "label": 0
                },
                {
                    "sent": "And you drag and drop those into your browser.",
                    "label": 0
                },
                {
                    "sent": "You say this reason.",
                    "label": 0
                },
                {
                    "sent": "And then you say, alright, that's my cookie classifier.",
                    "label": 0
                },
                {
                    "sent": "An you say uploading train and then basically this gets sent to a back end, trains an algorithm before I could finish the sentence.",
                    "label": 0
                },
                {
                    "sent": "You now have production level classifier with three lines of Python code.",
                    "label": 0
                },
                {
                    "sent": "And you can test it too by dragging dropping.",
                    "label": 0
                },
                {
                    "sent": "And basically.",
                    "label": 0
                },
                {
                    "sent": "You know, upload any image and will tell you what kind of cookie it is.",
                    "label": 0
                },
                {
                    "sent": "So if it's that easy to do something already then it's probably not that greater.",
                    "label": 0
                },
                {
                    "sent": "Volunteer to just start doing research in that field right now.",
                    "label": 0
                },
                {
                    "sent": "Now of course computer vision is much more than just image classification, but it is a pretty central piece of that field and this is anybody can do that.",
                    "label": 1
                },
                {
                    "sent": "You don't even need a password, you just go to metamind IO and created classifier and.",
                    "label": 0
                },
                {
                    "sent": "There you have it, so you can do that for a lot of different things and a lot of people have used it for a lot of different things.",
                    "label": 0
                },
                {
                    "sent": "You can use it for not safe for classifier.",
                    "label": 0
                },
                {
                    "sent": "I won't show you those images and you know anything from malaria, blood samples to different skirts, to different plankton samples, dresses, company logos and so on.",
                    "label": 0
                },
                {
                    "sent": "It's it's pretty commoditized, yeah?",
                    "label": 0
                },
                {
                    "sent": "It is a little bit more than that, so if you have enough images, it actually trains a much larger network.",
                    "label": 0
                },
                {
                    "sent": "Also, if you have very few images, it does a lot of transfer learning from a pre trained large neural net and it only trains few of the higher layers depending on how many images you have.",
                    "label": 0
                },
                {
                    "sent": "So of course, again, vision is much more than just image classification, but this is basically how far deep learning has already gone, and this is one of the shameless plug demos from my company Metamind.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today I want to focus on some fun applications of deep natural language processing.",
                    "label": 0
                },
                {
                    "sent": "We'll start with some character recurrent neural networks, which are a lot of fun, but not always.",
                    "label": 0
                },
                {
                    "sent": "They don't always get state of the art performance on some hard benchmark datasets, and then we'll move on to some multimodal applications where we use the ideas that we had on the image side and actually.",
                    "label": 0
                },
                {
                    "sent": "Improve or combine that basically with sentences.",
                    "label": 0
                },
                {
                    "sent": "We basically you're going to map sentences and images into the same vector space so that we can find one from the other and then have another multimodal kind of application where we can predict engagement on Twitter before even sending out a tweet just from the images and the text, which is an interesting application and that humans aren't very good at it and machines can actually still do, you know, get much beyond.",
                    "label": 0
                },
                {
                    "sent": "Random and actually can get pretty high accuracy for this kind of task.",
                    "label": 0
                },
                {
                    "sent": "Will do will look at a pretty hard task question answering and one old model that we've used to solve that before.",
                    "label": 0
                },
                {
                    "sent": "We then go onto a new model that we just introduced two months ago and never mind that I think will combine a lot of the power of all these different models that we've introduced before and it will make strong use of recurrent neural networks.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so character recurrent neural Nets basically a very simple, similar and simple we have.",
                    "label": 0
                },
                {
                    "sent": "Here again these exact hidden to hidden representations and internally here we could actually train also a more complex LTM or Gru model.",
                    "label": 0
                },
                {
                    "sent": "Here we just visualized very simple recurrent neural net.",
                    "label": 0
                },
                {
                    "sent": "And all we do is basically try to output the next character and this is a very cool demo from an slides from Andrej Karpathy, who's also at Stanford.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just to give you an idea of how they work basic.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use these LCM so instead of having a.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Note here that is just a simple recurrent neural network node.",
                    "label": 0
                },
                {
                    "sent": "This is actually more complex.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the most interesting is also like I said here are sort of fun samples.",
                    "label": 0
                },
                {
                    "sent": "How much can the model actually learned?",
                    "label": 0
                },
                {
                    "sent": "Is it able to learn, for instance, that when you (that later on, you might want to close that parenthesis and things like that?",
                    "label": 0
                },
                {
                    "sent": "That will tell us something about how much it can store in its own memory.",
                    "label": 0
                },
                {
                    "sent": "So let's look at some results here.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're dead, Andre.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Has put up.",
                    "label": 0
                },
                {
                    "sent": "Here's a character or an end that is trained on Shakespeare and what you'll see is that essentially from afar it will look somewhat like Shakespeare, but as you look closer it doesn't make much semantic sense, so there's it's not trying to generate and try to create a coherent story or anything, but what is kind of amazing already is that it actually has been able to memorize words just from a very complex character sequence, so at least it has understood.",
                    "label": 0
                },
                {
                    "sent": "A lot of and sort of stored in its memory and the LCM waits a lot of different words of the English language.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now on Wikipedia, when you train it, it looks more like a complex Wikipedia entry.",
                    "label": 0
                },
                {
                    "sent": "It even added some links.",
                    "label": 0
                },
                {
                    "sent": "And again, it doesn't make much sense semantically.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one is interesting as latech, but if you've ever played with Latex, you know that even for humans it's hard to make it compile sometimes.",
                    "label": 0
                },
                {
                    "sent": "So here he actually had to manually fix some of the syntax errors to get that, but in the end it still looked largely like this, so either only minor fixes and from afar it looks like a reasonable math paper almost.",
                    "label": 0
                },
                {
                    "sent": "And then here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is a very interesting example in that it actually learned to do indentation correctly.",
                    "label": 0
                },
                {
                    "sent": "It learn to define functions and it actually learned to properly parenthesize various sub expressions for code.",
                    "label": 0
                },
                {
                    "sent": "So there we actually see that LCMS on the character level have the potential to learn different kinds of stacks.",
                    "label": 0
                },
                {
                    "sent": "In some ways you put certain parenthesis on a stack, and then you want to be able to.",
                    "label": 0
                },
                {
                    "sent": "Eventually pop it again and close it.",
                    "label": 0
                },
                {
                    "sent": "So pretty interesting sort of observation that comes out of visualizing it this way.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions about recurrent neural Nets and on characters?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I don't think this code will run.",
                    "label": 0
                },
                {
                    "sent": "It looks like I said semantically it doesn't make sense.",
                    "label": 0
                },
                {
                    "sent": "It just looks similar to what it's trained on.",
                    "label": 0
                },
                {
                    "sent": "I would think so.",
                    "label": 0
                },
                {
                    "sent": "I don't think there was any comparison in his work or or like exactly like an XY axis.",
                    "label": 0
                },
                {
                    "sent": "Where is the size in here 'cause it's kind of hard to measure like what is reasonable?",
                    "label": 0
                },
                {
                    "sent": "I mean, semantically, none of them are that reasonable.",
                    "label": 0
                },
                {
                    "sent": "But syntactically from afar it looks looks good.",
                    "label": 0
                },
                {
                    "sent": "It is, it is so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ray is actually very good at publishing all the code for his things and people have now trained it on a variety of different other corpora.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the next, the next task is in many ways a lot harder, and essentially the first well, we start that now and then we'll come back to question, answering again, this is a paper by more deran lot of coauthors, neural networks for factoid question, answering over paragraph.",
                    "label": 0
                },
                {
                    "sent": "So we're going to try to give you a paragraph is a little bit like the Jeopardy Watson system, except that it was just a couple of grad students and professors and not a whole large team.",
                    "label": 0
                },
                {
                    "sent": "Of hundreds of people for multiple years, but basically you're given a question where you have a bunch of facts in the beginning.",
                    "label": 0
                },
                {
                    "sent": "They're very obscure, and then they get more and more obvious as you read on, and then in the end you want to basically give an answer in the form of the best entity that describes this paragraph.",
                    "label": 0
                },
                {
                    "sent": "Does anybody have an idea what the answer here would be very good?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, read audience.",
                    "label": 0
                },
                {
                    "sent": "So this is Thomas Mann here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Famous German author.",
                    "label": 0
                },
                {
                    "sent": "The way this model trains.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The sentence representations here is actually.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recursive neural network similar to the ones that Chris had introduced.",
                    "label": 1
                },
                {
                    "sent": "So here we have word vectors and then we have a so-called dependency tree that basically takes in the word vectors, Maps them to hidden States and basically repeats that application until you have a root vector at the very top and then you can take that route vector and either average over at average it over with others in a paragraph, or if it's just a single entity, or.",
                    "label": 0
                },
                {
                    "sent": "Sentence you take it as is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's interesting here is after we train, we can essentially Yep.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, the good.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so does it make use of the whole paragraph, so.",
                    "label": 0
                },
                {
                    "sent": "And they actually ran a competition in the end against real people.",
                    "label": 0
                },
                {
                    "sent": "The sort of national champions of Quizbowl, which is the sort of a high school and college student trivia competition.",
                    "label": 0
                },
                {
                    "sent": "And there basically the model can click the buzzer as soon as it thinks it knows what's going on, and so can human.",
                    "label": 0
                },
                {
                    "sent": "So as the human competitors read it or get it read, they can basically buzzin as soon as they know, and so.",
                    "label": 0
                },
                {
                    "sent": "You're basically.",
                    "label": 0
                },
                {
                    "sent": "Do indeed try to solve it before you read the whole thing.",
                    "label": 0
                },
                {
                    "sent": "You don't just use the last sentence, but yeah, it's true.",
                    "label": 0
                },
                {
                    "sent": "If it was just given as is, then the last sentence would often be the most helpful.",
                    "label": 0
                },
                {
                    "sent": "Discretion.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now after we trained model to basically try to predict all these potentially different answers here, what we can observe is that with an ice T Sini, plot and projection of the vectors of the different answers.",
                    "label": 0
                },
                {
                    "sent": "That again, we observe how the model actually stored different facts inside this continuous space.",
                    "label": 0
                },
                {
                    "sent": "Again, we won't be able to pinpoint and say this exact dimension captures that their presidents or anything like that.",
                    "label": 0
                },
                {
                    "sent": "But as we projected down to two dimensions from around 100.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we still see various interesting patterns.",
                    "label": 0
                },
                {
                    "sent": "So for instance, early United States Presidents cluster up here and this overall this cluster here are essentially US presidents and their other clusters of just explorers and emperors, or policy's or wars rebellions and battles.",
                    "label": 0
                },
                {
                    "sent": "And things like that.",
                    "label": 0
                },
                {
                    "sent": "So they basically each cluster around and model tries to store as many facts as it can from these paragraphs inside the entity.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Presentations.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this was in the beginning when they published the first version of this paper, where the model for some of the sub competitions such as history in many cases already outperformed.",
                    "label": 0
                },
                {
                    "sent": "People.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other competitions though literature, especially it wasn't able to outperform humans very often and the main problem here is actually training data.",
                    "label": 0
                },
                {
                    "sent": "So you might think, oh, you know this will stay like that forever, but I think part of the problem was that if you actually want to win these kinds.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Literature questions, then you actually will have to start reading some of the novel.",
                    "label": 0
                },
                {
                    "sent": "So if you here want to be able to answer this question just based on the 1st sentence, you need to know title characters that forges father signatures to get out of school invoice draft by feigning desire to join.",
                    "label": 0
                },
                {
                    "sent": "So basically that you will never get from either the training corpus by itself or in which they eventually also incorporated Wikipedia articles about the main author.",
                    "label": 0
                },
                {
                    "sent": "You would either get it from going inside Wikipedia articles, trying to go through all this up Lings.",
                    "label": 0
                },
                {
                    "sent": "For instance, authors, or you'd have to actually go and download the whole books and then try to train on that, and then incorporate that information so without.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That it will be very hard to win these kinds of questions.",
                    "label": 0
                },
                {
                    "sent": "Alright, are there any questions about?",
                    "label": 0
                },
                {
                    "sent": "Question answering in this sort of factoid way in many ways.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the end, we basically wanted to store as many facts as possible in this fuzzy vector space, and we won't necessarily be able with this kind of model to reason logically.",
                    "label": 0
                },
                {
                    "sent": "For instance over combination effects.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in many ways right?",
                    "label": 0
                },
                {
                    "sent": "This was a handful of academics working on this project and creating a reasonably good system that in the end on sort of a National College level, was able to perform on par with the national champions.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, it's it's quite impressive.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, IBM IBM system is in some ways more impressive in that it has.",
                    "label": 0
                },
                {
                    "sent": "Larger variety of questions that it can answer.",
                    "label": 0
                },
                {
                    "sent": "Things about numbers and locations of cities and very specific things.",
                    "label": 0
                },
                {
                    "sent": "And there are a lot more cultural references that it can figure out.",
                    "label": 0
                },
                {
                    "sent": "However, it seemed like a lot of the IBM system.",
                    "label": 0
                },
                {
                    "sent": "Required a lot of human engineering and feature engineering for specific types of questions there.",
                    "label": 0
                },
                {
                    "sent": "After teams that worked on specific types of jeopardy questions, so it was very hard to take that system afterwards and just apply it to a different domain such as the medical domain or something like that, and then hope that it would still work.",
                    "label": 0
                },
                {
                    "sent": "'cause a lot of the there's a lot of feature engineering going on for specific types of jeopardy.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Where is he?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we really just train recursive neural networks on arbitrary sentences and then try to predict the answers.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Sure, so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, here you can use a Max margin loss where you try to predict that this vector multiplied by the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Entity representation here, for instance of.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most modern or various presidents.",
                    "label": 0
                },
                {
                    "sent": "As you multiply these two, you want the inner product of those to be larger than if you were to multiply the inner product of one of these entity vectors, where the sentence that it doesn't describe it.",
                    "label": 0
                },
                {
                    "sent": "All that and you want to basically maximize that difference up to margin.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately I don't have a white board here, but it's basically the Max over 0, 1 minus the inner product between the correct one.",
                    "label": 0
                },
                {
                    "sent": "Just inner product of an incorrect pair.",
                    "label": 0
                },
                {
                    "sent": "I actually described that 'cause we use that same objective function for images and text mapping and you'll see it there in more detail.",
                    "label": 0
                },
                {
                    "sent": "How important is it to use what formulism?",
                    "label": 0
                },
                {
                    "sent": "Oh, the dependency grammar so.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It actually does work a lot better than the constituency trees in this case, and Chris is also found with Kaisheng.",
                    "label": 0
                },
                {
                    "sent": "That dependency trees for sentiment analysis, for instance, can also work better.",
                    "label": 0
                },
                {
                    "sent": "But I think actually.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As as the text get larger and larger and eventually, especially for a lot of different factoid questions, it becomes more important to just kind of find specific phrases.",
                    "label": 0
                },
                {
                    "sent": "You can probably go and use less and less of a tree structure too.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's why I just mentioned as you get longer and longer, you don't need as much structure and you need to for some of the simpler factoid questions.",
                    "label": 0
                },
                {
                    "sent": "You just need to find specific phrases and then bag of words can actually also do very well.",
                    "label": 0
                },
                {
                    "sent": "As you try to actually understand how a specific actor acts upon a specific object and things like that, then you really want the sentence structures.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now in the next fun application is on visual grounding.",
                    "label": 0
                },
                {
                    "sent": "So here the main idea is, instead of mapping sentences to facts and entities were trying to map sentences to images and we do that by again having here a deep natural language processing model.",
                    "label": 0
                },
                {
                    "sent": "In our case there will be a dependency tree, but you could use a lot of other models.",
                    "label": 0
                },
                {
                    "sent": "And we have a convolutional neural network on this side will go into some more details in a little bit and we basically tried to map them into the same space.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we will again use dependency trees which capture more of the semantic structure, and we that's essentially the same exact type of model that we used for question answering.",
                    "label": 0
                },
                {
                    "sent": "And then on the image side, of course we will use a very deep convolutional neural network and will actually use some pre training on both sides of the model.",
                    "label": 0
                },
                {
                    "sent": "So on the language side we will have word vectors that are pre trained and we can essentially use to initialize and have the model understand that.",
                    "label": 0
                },
                {
                    "sent": "Cars and vehicles and things like that are very much similar and that will allow us to transfer knowledge alot better.",
                    "label": 0
                },
                {
                    "sent": "And you see, an end will actually be pre trained on Imagenet.",
                    "label": 0
                },
                {
                    "sent": "And our main objective function is the same kind of Max margin loss over pairs that are correct.",
                    "label": 0
                },
                {
                    "sent": "So just to give you an idea here I'll show you the training data set.",
                    "label": 0
                },
                {
                    "sent": "So the training data here looks like this, where essentially you have an image and then for each image you have five sentences describing that image, and now for each of these five pairs we want them to basically have large inner product.",
                    "label": 0
                },
                {
                    "sent": "If they are indeed part of the same.",
                    "label": 0
                },
                {
                    "sent": "The same triplet.",
                    "label": 0
                },
                {
                    "sent": "And we'll show you live demo after we go through.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The objective function.",
                    "label": 0
                },
                {
                    "sent": "Basically here we will look over each of these pairs.",
                    "label": 0
                },
                {
                    "sent": "So for each image I we have 5 sentences J and this.",
                    "label": 0
                },
                {
                    "sent": "This is the set of all sentences that doesn't include the five sentences of the image.",
                    "label": 0
                },
                {
                    "sent": "And then we will basically here have a margin, and we're going to try to increase this inner product up to this margin and decrease the inner product between the correct image and an incorrect sentence.",
                    "label": 0
                },
                {
                    "sent": "So basically try to push incorrect sentence descriptions.",
                    "label": 0
                },
                {
                    "sent": "Far away.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You basically can either sum over all the other sentences of all the other images, or you just subsample this some 'cause there are a lot of other sentences.",
                    "label": 0
                },
                {
                    "sent": "So you just randomly sample an incorrect.",
                    "label": 0
                },
                {
                    "sent": "If in most most likely if you take a sentence describing another image, it will not be a good sense description for that specific other image.",
                    "label": 0
                },
                {
                    "sent": "In some ways, yes.",
                    "label": 0
                },
                {
                    "sent": "In other ways, there might still be somewhat similar images, right?",
                    "label": 0
                },
                {
                    "sent": "You might stop sample here.",
                    "label": 0
                },
                {
                    "sent": "Dog images that or images that also include dogs but not in the same constellation and so on.",
                    "label": 0
                },
                {
                    "sent": "But it's still in the end.",
                    "label": 0
                },
                {
                    "sent": "It's still a heart hard task to optimize, but you in some ways hopefully now can see how to take derivatives of this, which is fairly straightforward.",
                    "label": 0
                },
                {
                    "sent": "And then you can basically take derivatives of all the parameters that are inside.",
                    "label": 0
                },
                {
                    "sent": "The computation for this, and it's all just back propagation, but it's still non trivial to train the model.",
                    "label": 0
                },
                {
                    "sent": "In fact, because the images, image sentence datasets are usually not large enough, you can't really train to whole CNN and back rub through all of it.",
                    "label": 0
                },
                {
                    "sent": "Usually just train the top layer that Maps the last features into the Joint multimodal space and on the enemy side you can actually train the whole model 'cause it has many fewer parameters.",
                    "label": 0
                },
                {
                    "sent": "It's a great question essay.",
                    "label": 0
                },
                {
                    "sent": "Once the model is well trained, you will often observe that this is much larger than this, and then this will, let's say B1.",
                    "label": 0
                },
                {
                    "sent": "This will be 5.",
                    "label": 0
                },
                {
                    "sent": "This will be .5 or something and then this whole thing is smaller than zero and then you'll get to 0 and you can actually just ignore that example.",
                    "label": 0
                },
                {
                    "sent": "So the beauty of the Max margin loss function is that overtime it will only focus on the things that it's still not good at and I'll just ignore the things that can already.",
                    "label": 0
                },
                {
                    "sent": "Very accurately distinguished.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some examples and then we'll go to a live demo.",
                    "label": 0
                },
                {
                    "sent": "So here we do One Direction where given an image we want to find.",
                    "label": 0
                },
                {
                    "sent": "This right sentences describing that image and again in the final Test set there 5 correct sentences that should be describing that are correctly describing that image and hundreds or thousands of other sentences that don't describe that image very accurately.",
                    "label": 0
                },
                {
                    "sent": "And here we essentially in green Show if that sentence was indeed officially the right sentence, and in red if it wasn't.",
                    "label": 0
                },
                {
                    "sent": "And you can see here some some good examples and this one here.",
                    "label": 0
                },
                {
                    "sent": "You might think you know this is good the first one, but ended missed the last the next four of the write sentences and actually described different ones or use different ones here that are the nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "Now in terms of inner products in this space.",
                    "label": 0
                },
                {
                    "sent": "And if you know a little bit about vision, you can kind of see some of the things that it incorrectly picked up.",
                    "label": 0
                },
                {
                    "sent": "So the propeller plane here, kind of that.",
                    "label": 0
                },
                {
                    "sent": "We'll the white jet landing gear.",
                    "label": 0
                },
                {
                    "sent": "This might be the reason why it confused it for white Jetton.",
                    "label": 0
                },
                {
                    "sent": "And in this one looks a little ridiculous because it's an elderly woman, but in the end it's actually not that bad, because other than the elderly, it is a person that catches a ride on the back of something that looks like a bicycle, right?",
                    "label": 0
                },
                {
                    "sent": "So you might want to actually better way to compare this here is by engram overlap and in Bleu scores as if you were to do machine translation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course.",
                    "label": 0
                },
                {
                    "sent": "So those are all from the test set.",
                    "label": 0
                },
                {
                    "sent": "There aren't images or sentences from the training set.",
                    "label": 0
                },
                {
                    "sent": "OK, so so.",
                    "label": 0
                },
                {
                    "sent": "This picture.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good question.",
                    "label": 0
                },
                {
                    "sent": "So you basically take an image and then you compute the inner product of that image vector with all the sentence vectors from the test set and you pick the nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "Based on that inner product.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's right, yeah, you subsample you subsample.",
                    "label": 0
                },
                {
                    "sent": "Some of these here to speed it up.",
                    "label": 0
                },
                {
                    "sent": "At the end, you actually observe that a lot in Max margin objective functions where you could spend a lot more time on each example and try to make sure that those five sentences have a better in our product and all the other ones.",
                    "label": 0
                },
                {
                    "sent": "Or you can just more often go through more examples, and it's often more advantageous to go through more examples and train each just a little bit.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It's not what.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, it's not.",
                    "label": 0
                },
                {
                    "sent": "But that's that's OK.",
                    "label": 0
                },
                {
                    "sent": "It turns out to be just fine with just standard SGD.",
                    "label": 0
                },
                {
                    "sent": "We used to use even LB FGS for these kinds of functions, and that work too, but in the end, just mini batch SGD works just fine.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "The Lambda here basically just gives you a margin, and it's another hyperparameter you can set.",
                    "label": 0
                },
                {
                    "sent": "I often just set it to one.",
                    "label": 0
                },
                {
                    "sent": "That's fine, but sometimes it makes sense depending on the rest of the model.",
                    "label": 0
                },
                {
                    "sent": "To make it a little smaller, or if you want to push the margin to be even larger and try to have it be even more robust, you can tweak it, but usually it's somewhere between .1 and two.",
                    "label": 0
                },
                {
                    "sent": "It's not not a large range, it's basically just a margin between after which if it can accurately.",
                    "label": 0
                },
                {
                    "sent": "Put the correct sentences so far apart from all the incorrect pairs.",
                    "label": 1
                },
                {
                    "sent": "Then you're done with that combination and makes it more robust at Test time.",
                    "label": 0
                },
                {
                    "sent": "It's a great question.",
                    "label": 0
                },
                {
                    "sent": "We haven't done it in this work, but I've tried it in previous work before where I tried to be very clever about my subsampling, where I say, let's say I have a unit sort of ball around a current current sentence vector image vector, and then I tried to not sample the ones that are very close, but then sample some that are little closer and not.",
                    "label": 0
                },
                {
                    "sent": "Really far away.",
                    "label": 0
                },
                {
                    "sent": "Lots of things like that didn't matter at all in the end, just random subsamples were the way to go for two previous projects before this one.",
                    "label": 0
                },
                {
                    "sent": "So in this one we just took it as is.",
                    "label": 0
                },
                {
                    "sent": "But yeah, intuitively, I felt like this should help.",
                    "label": 0
                },
                {
                    "sent": "Didn't in practice.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some numbers.",
                    "label": 0
                },
                {
                    "sent": "All these numbers have now actually been improved there.",
                    "label": 0
                },
                {
                    "sent": "Now, larger datasets that have even more training and testing images, and they're even better models out there too.",
                    "label": 0
                },
                {
                    "sent": "At the time in 2013, this was the state of the art results on this data set the Flickr One Catus data set from UIUC.",
                    "label": 0
                },
                {
                    "sent": "And what's fun is that it actually in in many cases does does quite surprising and cool things, so I can type in bird, for instance.",
                    "label": 0
                },
                {
                    "sent": "And now what we do here is whenever I type in language in this box, that basically Maps that language into a vector space and then it has a precomputed set of.",
                    "label": 0
                },
                {
                    "sent": "All the test images from the data set that essentially.",
                    "label": 0
                },
                {
                    "sent": "Try it, you know it has a vector for and then computes the inner product between the vector of the text in the box and then depending on how close they are basically mapped mapped, how close they are to the word.",
                    "label": 0
                },
                {
                    "sent": "So basically it only looks at the pixels right?",
                    "label": 0
                },
                {
                    "sent": "There's no as in standard sort of web search where you actually have text around the image is this is just based on the pixels you map it through this year end to CNN to the same vector space and then you find inner products.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So the only meaning is the distance between the center of the box to the center of the image where where it is around that unit, like around the circle doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I type in bird we basically get similar into standard image classification pictures of a single bird.",
                    "label": 0
                },
                {
                    "sent": "But what's cool is I can type in birds and now it actually tries.",
                    "label": 0
                },
                {
                    "sent": "It has learned some sense of plurality and is trying to show me pictures only of multiple birds if they aren't datasets, I would actually love to make this public, But the problem is you really need to look at what images there were and what what the training data look like.",
                    "label": 0
                },
                {
                    "sent": "So if you look here an you basically.",
                    "label": 0
                },
                {
                    "sent": "It will only be able to do things with these kinds of these kinds of objects, so if you try to type in zebra or something, it will just show you something random 'cause it's never seen.",
                    "label": 0
                },
                {
                    "sent": "As he seen a zebra.",
                    "label": 0
                },
                {
                    "sent": "So that's why this demo isn't public.",
                    "label": 0
                },
                {
                    "sent": "So now we have single words.",
                    "label": 0
                },
                {
                    "sent": "That's pretty standard, but now the cool part is we can also go and type in multiple words.",
                    "label": 0
                },
                {
                    "sent": "And that's a little slow.",
                    "label": 0
                },
                {
                    "sent": "And now it tries to actually combine the two and has learned some sense of compositionality how these words map to, ideally, something visual that makes sense when you put them together in that order, so birds and water's trying to find you know birds and water or birds and trees, tries to find.",
                    "label": 0
                },
                {
                    "sent": "The few images of birds in trees.",
                    "label": 0
                },
                {
                    "sent": "You can also have different kinds of adjectives, so if you just type in yellow for instance, then it just shows you a bunch of random yellow objects.",
                    "label": 0
                },
                {
                    "sent": "If you type in yellow.",
                    "label": 0
                },
                {
                    "sent": "I know boss or something, then tries to closest to matches or actual yellow buses and then the next closest matches are just the other buses in the data set.",
                    "label": 0
                },
                {
                    "sent": "You can type in horse an it will show you.",
                    "label": 0
                },
                {
                    "sent": "Pictures of single horse and then horse with bald man for instance.",
                    "label": 0
                },
                {
                    "sent": "That actually is able to pull out.",
                    "label": 0
                },
                {
                    "sent": "We get in a second.",
                    "label": 0
                },
                {
                    "sent": "There the one picture of horse with a bald man and in the interesting errors it makes are also ascentia Lee people with helmets who from afar into model that.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doesn't know much about the war.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look somewhat like people who are involved, so this is basically one fun life demo of this multimodal mapping.",
                    "label": 0
                },
                {
                    "sent": "Now another fun task that is actually much harder for humans.",
                    "label": 0
                },
                {
                    "sent": "'cause it.",
                    "label": 0
                },
                {
                    "sent": "It shows like a baby just doesn't make any sense.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "Understand exactly what's going on.",
                    "label": 0
                },
                {
                    "sent": "That's right, I mean, if it doesn't have something in the test set of images that can retrieve, then it just retrieves the next couple of images.",
                    "label": 0
                },
                {
                    "sent": "There will be.",
                    "label": 0
                },
                {
                    "sent": "There will be further away, but it will just retrieve that an in general, especially this model.",
                    "label": 0
                },
                {
                    "sent": "But even the models that came afterwards, you're right in many cases they don't really understand the images, right?",
                    "label": 0
                },
                {
                    "sent": "There's still a lot of work that can be done there.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you type in Black Horse, but then all the background is black and.",
                    "label": 0
                },
                {
                    "sent": "It's actually, you know, a Brown horse or something like that, so there's still a lot of issues and we'll get to that in a second after after this demo.",
                    "label": 0
                },
                {
                    "sent": "But eventually you can try to generate sentences too, and there you see even more issues where it really.",
                    "label": 0
                },
                {
                    "sent": "In some ways it captures core current statistics of certain patterns in images for certain words in the language.",
                    "label": 0
                },
                {
                    "sent": "So many of the words like width and next tool and colors weren't an image net and they still worked as long as you had training examples of the multimodal data set.",
                    "label": 0
                },
                {
                    "sent": "It's kind of hard to put a number on that, so we already know, for instance, all the adjectives, all the prepositions, and all these things aren't in there, but you need those words to combine real sentences.",
                    "label": 0
                },
                {
                    "sent": "I don't know if there are any.",
                    "label": 0
                },
                {
                    "sent": "In fact, I think the authors of the Flickr data set from you, you see actually looked at and focused on words that are more likely in image net already in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "So just might not be the right data set to run that comparison.",
                    "label": 0
                },
                {
                    "sent": "My hunch is though, that the image net kinds of vectors that you get from CNN's are quite general.",
                    "label": 0
                },
                {
                    "sent": "So what I showed you.",
                    "label": 0
                },
                {
                    "sent": "In here, for instance, so image net does not have any blood samples, but somewhat surprisingly, it still helps to have image net initialized vectors on blood samples, so they're really quite a good general representation of images.",
                    "label": 0
                },
                {
                    "sent": "Read one more question, I think.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here is another task that also requires understanding both images and sentences.",
                    "label": 0
                },
                {
                    "sent": "But instead of trying to map 1 to the other, if there are similar, it essentially tries to predict engagement.",
                    "label": 0
                },
                {
                    "sent": "So what do we mean by engagement and social media?",
                    "label": 0
                },
                {
                    "sent": "It could be either you click on a link or you favorite it something, or you retreat it something and.",
                    "label": 0
                },
                {
                    "sent": "Here is an example of a photo that is engaging and of text that I'll show you in a second that is not engaging, and overall this is not very engaging.",
                    "label": 0
                },
                {
                    "sent": "At roughly .5 overall and when we look at that tweet, we observe that that is indeed true.",
                    "label": 0
                },
                {
                    "sent": "So in this this tweet comes actually from several months after that training data set had been collected, so the training data set here.",
                    "label": 0
                },
                {
                    "sent": "There's no magic, but basically tells you for this tweet.",
                    "label": 0
                },
                {
                    "sent": "And given this specific vertical like movies or cars or.",
                    "label": 0
                },
                {
                    "sent": "Entertainment and things like that, and this was above average in terms of the engagement for that particular user who had these kinds of numbers of followers and things like that.",
                    "label": 0
                },
                {
                    "sent": "So you had to normalize quite a bit and properly understand.",
                    "label": 0
                },
                {
                    "sent": "How to actually collect your labels.",
                    "label": 0
                },
                {
                    "sent": "But once you have done that, it's quite amazing and quite accurate.",
                    "label": 0
                },
                {
                    "sent": "So here we have the tweet text.",
                    "label": 0
                },
                {
                    "sent": "Take a closer look at the world of Tomorrowland and somewhat interesting Lee.",
                    "label": 0
                },
                {
                    "sent": "Here you have take a closer look and it's actually as a phrase, not very engaging.",
                    "label": 0
                },
                {
                    "sent": "But on.",
                    "label": 0
                },
                {
                    "sent": "The image here.",
                    "label": 0
                },
                {
                    "sent": "So basically people don't like to be told what to do.",
                    "label": 0
                },
                {
                    "sent": "This is 1 takeaway message here for this overall tweet.",
                    "label": 0
                },
                {
                    "sent": "Now on the image side though, this is pretty engaging and we can also query the mall and ask it why it thinks it's engaging and basically F here.",
                    "label": 0
                },
                {
                    "sent": "Hence the long hair and the cool looking buildings in the background and so we can essentially query the scene and also about how much each part mattered and now to contrast this, this is a.",
                    "label": 0
                },
                {
                    "sent": "Very engaging tweet.",
                    "label": 0
                },
                {
                    "sent": "This says Matt Max gathers up all that we seem to crave from our movies and Yanks it to the limit.",
                    "label": 0
                },
                {
                    "sent": "Great sentence, very engaging.",
                    "label": 0
                },
                {
                    "sent": "And now the picture is also really engaging.",
                    "label": 0
                },
                {
                    "sent": "Just cool background the car the I lot of.",
                    "label": 0
                },
                {
                    "sent": "Lot of good things and if we look at that.",
                    "label": 0
                },
                {
                    "sent": "Tweet indeed.",
                    "label": 0
                },
                {
                    "sent": "It had gotten, you know, a lot of retreats in favorites for a movie, so this is just, you know, these are just example.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Loads of multimodal applications where it makes sense to map images and sentences into same vector space and try to predict either one from the other or try to predict an overall label that requires understanding of both.",
                    "label": 0
                },
                {
                    "sent": "Alright, now this was.",
                    "label": 0
                },
                {
                    "sent": "Gary's but we didn't do it.",
                    "label": 0
                },
                {
                    "sent": "There are lots of other things you could try to incorporate on top of that time series and so on, but we didn't work.",
                    "label": 0
                },
                {
                    "sent": "So now I mentioned that instead of just finding the right sentence, there's been follow-up work by a bunch of people earlier.",
                    "label": 0
                },
                {
                    "sent": "Earlier this year, yeah, right, so they all put put it on archive late last year, but then all the papers came out this year basically trying to not just find the right sentence but actually generate that sentence.",
                    "label": 0
                },
                {
                    "sent": "And so the main idea was you again, take a CNN similar to what we described before, but instead of having a recursive neural network that just Maps existing sentences, you have a recurrent one that tries to sample now conditioned on the image tries to sample.",
                    "label": 0
                },
                {
                    "sent": "The description, so it's very similar to the machine translation work that I described yesterday, where we have that same kind of LCM that basically has one hot output vectors, but instead it actually.",
                    "label": 0
                },
                {
                    "sent": "Just finds descriptions and so you might think, wow, this is cool is so much more epic than just finding the sentences.",
                    "label": 0
                },
                {
                    "sent": "But one thing after some careful analysis and follow up work folks at Microsoft.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually found is that a lot of the times they.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sentences that it generated were actually exactly verbatim test or training data sentences.",
                    "label": 0
                },
                {
                    "sent": "So in the end the LCMS were just very, very good at memorizing dose training sentences and then generating them, but really kind of just from memory, and it's.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Especially, you can observe that when it adds a bunch of facts to us to an image that had absolutely nothing to do with the image, but we're sort of like that.",
                    "label": 0
                },
                {
                    "sent": "And in training data, nevertheless, there's you know this is a much more interesting and next step than just finding them.",
                    "label": 0
                },
                {
                    "sent": "There's still like 20 to 40% of the sentences are not exactly like that in the training data, and here are a couple of examples where it works.",
                    "label": 0
                },
                {
                    "sent": "Very well, so this one is a nice sort of standard sentence, something that you might observe a lot in the training data to then as it gets sort of to weirder and weirder combinations, it will often fall back on things that are more similar to what it seems, so there not that many ferrets in the training data.",
                    "label": 0
                },
                {
                    "sent": "So I will say it's a cat.",
                    "label": 0
                },
                {
                    "sent": "Not many pictures of little babies holding.",
                    "label": 0
                },
                {
                    "sent": "Toothbrushes so it will be a baseball bat.",
                    "label": 0
                },
                {
                    "sent": "Also teddy bear, but it's it's in many ways.",
                    "label": 0
                },
                {
                    "sent": "There's again no magic, right?",
                    "label": 0
                },
                {
                    "sent": "We learn core current statistics of specific types of patterns, but deep learning is extremely good at learning those kinds of statistics and in a very good way to generalize is much better than.",
                    "label": 0
                },
                {
                    "sent": "I think any other machine learning MoD.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right now all right?",
                    "label": 0
                },
                {
                    "sent": "So in the remaining time I want to talk about dynamic dynamic memory networks.",
                    "label": 0
                },
                {
                    "sent": "I'm very excited about this work because I feel like it has some potential to generalize very well to a lot of different.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Task so the first observation that we've made is that in some ways all NLP tasks can be reduced to question answering, and this is an insight that I've had over some drinks, but I think actually some students from Russia's group like four or five years ago at NIPS are just sort of having 70 good time.",
                    "label": 0
                },
                {
                    "sent": "And I'm like, you know guys, everything is a QA problem.",
                    "label": 0
                },
                {
                    "sent": "They're like OK, Richard, but So what?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd like first there some more sceptical and you can come up with good examples of what it is, but once they're like OK, that's true, but is it very useful?",
                    "label": 0
                },
                {
                    "sent": "Maybe not.",
                    "label": 0
                },
                {
                    "sent": "So here are some examples you have sort of the standard question answering type problems where you have some semantic typical question, answers things you might want to search online.",
                    "label": 0
                },
                {
                    "sent": "You know where was Obama's wife born?",
                    "label": 0
                },
                {
                    "sent": "You might want to then reasonover different facts and a knowledge base and things like that.",
                    "label": 0
                },
                {
                    "sent": "That is the standard QA task.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "Everything is fresh and dancing even yeah.",
                    "label": 0
                },
                {
                    "sent": "So then you can might ask machine translation, right?",
                    "label": 0
                },
                {
                    "sent": "What is the translation of that sentence into French sequence modeling tasks like named entity recognition and say what are named entities in that sentence?",
                    "label": 0
                },
                {
                    "sent": "Classification problems very simple.",
                    "label": 0
                },
                {
                    "sent": "What's the X sentiment, engagement, and so on?",
                    "label": 0
                },
                {
                    "sent": "And even multi sentence joint classification problems like coreference resolution.",
                    "label": 0
                },
                {
                    "sent": "You could ask like who does their refer to in the last sentence where it's their car was stolen and before that it was mentioned who's car it is.",
                    "label": 0
                },
                {
                    "sent": "So really pretty much every task in NLP and I encourage you to try to think of examples.",
                    "label": 0
                },
                {
                    "sent": "I couldn't get out of where that wouldn't work.",
                    "label": 0
                },
                {
                    "sent": "There you go.",
                    "label": 0
                },
                {
                    "sent": "There you go.",
                    "label": 0
                },
                {
                    "sent": "That's exactly the.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's interesting.",
                    "label": 0
                },
                {
                    "sent": "But maybe useless until.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a model that actually makes it useful, because what it does is that it always only trains on some input and a question answer pair over that input, and that's exactly the dynamic memory network and part of the reason I'm excited about it.",
                    "label": 0
                },
                {
                    "sent": "So it's essentially a neural network based model.",
                    "label": 0
                },
                {
                    "sent": "It will be end to end trainable, in which any QA task can be trained using just these triplets of input questions and answers.",
                    "label": 0
                },
                {
                    "sent": "And it has several relation rationes to previous work that also use the idea of memory.",
                    "label": 0
                },
                {
                    "sent": "Basically being able to retrieve or pay attention to specific facts from the past.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, Phil introduced those to us yesterday.",
                    "label": 0
                },
                {
                    "sent": "They have memory networks, neural Turing machines and other recent work from I think now DeepMind Google Deep Mind Teaching machines to read and comprehend.",
                    "label": 0
                },
                {
                    "sent": "So there's some related work to this, but I think the dynamic memory networks are the most general in that they only they don't.",
                    "label": 0
                },
                {
                    "sent": "Only tackle specific types of algorithmic tasks like neural Turing machines or only the traditional type.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Types of question answering, but really a whole host of other kinds of tasks.",
                    "label": 0
                },
                {
                    "sent": "So this led us to this work, which is already an archive.",
                    "label": 0
                },
                {
                    "sent": "Two it's called ask me anything dynamic memory networks for natural language.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assessing and this was joint work with a large team of interns were very lucky this summer at Megamind and had a whole host of Super smart interns.",
                    "label": 0
                },
                {
                    "sent": "Some of them actually owes on for almost 8 months.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's here's the first sort of output input output pairs that actually came from this model.",
                    "label": 0
                },
                {
                    "sent": "And then we'll go and dig deeper into how this model actually works.",
                    "label": 0
                },
                {
                    "sent": "So here is a first example.",
                    "label": 0
                },
                {
                    "sent": "This actually comes from Facebook data Set, which is synthetic and has had some issues, but it kind of is an interesting sort of necessary but not sufficient condition for you being able to reason logically over over some facts, so here.",
                    "label": 0
                },
                {
                    "sent": "We have sentence Mary walk to the bathroom center into the garden and so on.",
                    "label": 0
                },
                {
                    "sent": "And you ask, where's the milk?",
                    "label": 0
                },
                {
                    "sent": "And basically to answer this question, the model has to 1st understand that you know something to do with Sarah, Cassandra or Sandra took the milk there, but now you still don't know from that input where the milk actually is.",
                    "label": 0
                },
                {
                    "sent": "Now needs to find out where Sander was and she went to the garden.",
                    "label": 0
                },
                {
                    "sent": "So the answer is garden.",
                    "label": 0
                },
                {
                    "sent": "So that is the more traditional kind of question answering task.",
                    "label": 0
                },
                {
                    "sent": "Now the next input could be everybody's happy and you ask them all what's the sentiment and gives you positive you can ask for the named entities.",
                    "label": 0
                },
                {
                    "sent": "Part of speech tags or, you know, in French, that translation into French and these are all coming from them.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the overview, and we'll go over each of these modules in detail in the next couple of slides, but on a very high level.",
                    "label": 0
                },
                {
                    "sent": "We tried to organize some of the existing work that people have worked on already, such as word vectors, and try to see them as almost software engineering modules.",
                    "label": 0
                },
                {
                    "sent": "And whenever we have arrows here, this will be a fairly.",
                    "label": 0
                },
                {
                    "sent": "Straight forward forward propagation in a neural network and at training time those will all go backwards and the whole thing can be back propagated from the final answer.",
                    "label": 0
                },
                {
                    "sent": "So we will have some input text sequence or input module.",
                    "label": 0
                },
                {
                    "sent": "That I will have also next to it.",
                    "label": 0
                },
                {
                    "sent": "A question usually associated with some kind of specific input.",
                    "label": 0
                },
                {
                    "sent": "You can also say general input and you kind of search it based on the question, but for now we'll assume there's a specific input that is a text sequence.",
                    "label": 0
                },
                {
                    "sent": "We have a question now that question will trigger.",
                    "label": 0
                },
                {
                    "sent": "Specific ways would pay attention to parts of that input.",
                    "label": 0
                },
                {
                    "sent": "Those will be regurgitated or reasoned over in the episodic memory and from the episodic memory, and the question will then try to generate an answer.",
                    "label": 0
                },
                {
                    "sent": "And in some sense, here semantic memory is just a rewording of word vectors.",
                    "label": 0
                },
                {
                    "sent": "But you can also add other kinds of facts in their knowledge bases and back propagate facts into into the semantic memory.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let's look over the modules one at a time.",
                    "label": 0
                },
                {
                    "sent": "So the input module is essentially responsible for computing the representations of textual inputs.",
                    "label": 0
                },
                {
                    "sent": "But as you're sure I mentioned, this could actually be any other kind of input.",
                    "label": 0
                },
                {
                    "sent": "Any kind of input you can run a sequence model over could be used and then try to be retrieved later on.",
                    "label": 0
                },
                {
                    "sent": "You only assumption that we make here is that there's a temporal sequence that can be indexed by a certain time stamp.",
                    "label": 1
                },
                {
                    "sent": "Then for written language, essentially these will just be our word vectors and we can train the word vectors and this general module, both unsupervised and supervised.",
                    "label": 0
                },
                {
                    "sent": "So the input module can train word vectors just like any other normal like worked avec org.",
                    "label": 0
                },
                {
                    "sent": "Love kind of way to train current statistics and propagate those into the words.",
                    "label": 0
                },
                {
                    "sent": "But you can also eventually from the task themselves propagate facts into the word vectors and into this module.",
                    "label": 0
                },
                {
                    "sent": "You'll eventually get context independent, just word vectors and context dependent hidden states that really take into consideration all the knowledge from before and in some cases you can also have a BI directional.",
                    "label": 0
                },
                {
                    "sent": "Sequence model that takes into consideration facts from before and after.",
                    "label": 0
                },
                {
                    "sent": "In our work we just use the cloud vectors from Jeffrey Pennington from last year to initialize the semantic memory module, which is basically given as input to these.",
                    "label": 0
                },
                {
                    "sent": "Input modules.",
                    "label": 0
                },
                {
                    "sent": "And then to compute the context states we will use a recurrent neural network and now we don't need to go through this in too much detail.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause we already went over that yesterday, but this will essentially be a large gated recurrent unit.",
                    "label": 0
                },
                {
                    "sent": "You can also use analostan if you want there.",
                    "label": 0
                },
                {
                    "sent": "It's pretty pretty independent of that decision.",
                    "label": 0
                },
                {
                    "sent": "We often compared both.",
                    "label": 0
                },
                {
                    "sent": "In many cases the Gru works slightly better for machine translation systems work even better than than Gru's, so this is standard.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you and we assume that this HT is essentially the output at every time step for the input module.",
                    "label": 0
                },
                {
                    "sent": "Any questions about the input module?",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a good point.",
                    "label": 0
                },
                {
                    "sent": "You don't necessarily have to do it.",
                    "label": 0
                },
                {
                    "sent": "You can actually go always through the input sequence.",
                    "label": 0
                },
                {
                    "sent": "Very good observation.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the question is actually very similar.",
                    "label": 0
                },
                {
                    "sent": "It's the same Gru.",
                    "label": 0
                },
                {
                    "sent": "Actually it just takes in also word vectors for the question an computes a final question vector Q.",
                    "label": 0
                },
                {
                    "sent": "We often will drop the sub index T. It's just the last time step of the question sequence.",
                    "label": 0
                },
                {
                    "sent": "So just words cheer you final output.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we'll go into that in a lot more detail right here and then in the summary again, to really understand the details.",
                    "label": 0
                },
                {
                    "sent": "So basically the goal, the high level goal of the episodic memory is to combine the previous three modules, the semantic input and question modules outputs in order to reason over them, and then give the resulting knowledge and form of again a vector to the answer module.",
                    "label": 0
                },
                {
                    "sent": "And essentially the goal will be that based on the question, we can dynamically retrieve the necessary information over our inputs.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If that is not enough to just iterate over all the inputs once, because maybe you get new information and you need to now go back and look for other information.",
                    "label": 0
                },
                {
                    "sent": "It has the ability to iterate over the input and that is very crucial for exactly these kinds of examples here where.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Might ask where is the milk?",
                    "label": 0
                },
                {
                    "sent": "And now based on that question to model might go over all the text and try to find any fact for milk.",
                    "label": 0
                },
                {
                    "sent": "Any facts about milk and then realize OK Sandra took the milk there now only now after it went through all the facts does it know that Sandra is actually important for answering this question.",
                    "label": 0
                },
                {
                    "sent": "Now if it only had that ability to go over the input sequence once, it wouldn't be able to actually tell you much 'cause it hasn't paid attention to Sandra in the first iteration.",
                    "label": 0
                },
                {
                    "sent": "So now it needs to use those facts and iterate over the text again and try to find facts about Sandra, so that can then answer the question.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright now yeah, like this is exactly what I just mentioned.",
                    "label": 0
                },
                {
                    "sent": "We really for transitive inference we need that ability to reason over and retrieve and pay attention to specific time steps.",
                    "label": 0
                },
                {
                    "sent": "And this was kind of the idea.",
                    "label": 0
                },
                {
                    "sent": "Being able to think of specific time type specific points in time over your sort of previous experience is the reason we call this the episodic memory an there's some interesting neuroscience behind episodic memories too.",
                    "label": 0
                },
                {
                    "sent": "In humans, so the seed of the episodic memory in humans, actually the hippocampus.",
                    "label": 0
                },
                {
                    "sent": "And if you disrupt the hippocampus that actually impairs your ability to reason transitively over logical facts too.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's look over the details of this model.",
                    "label": 0
                },
                {
                    "sent": "This one is sort of least beautiful equation of the paper, but it's a fairly simple way to get there.",
                    "label": 0
                },
                {
                    "sent": "Basically, this is a way to compute gates.",
                    "label": 0
                },
                {
                    "sent": "So let's say we have here a specific sentence.",
                    "label": 0
                },
                {
                    "sent": "We have a specific memory we'll get to that in a bit.",
                    "label": 0
                },
                {
                    "sent": "Basically, the memory in the very beginning will just be the question itself, but then we'll update the memory based on.",
                    "label": 0
                },
                {
                    "sent": "Iterating over the inputs and now we'll have a large concatenation here of elements that basically try to correlate a specific sentence in the input with the question, the sentence with the memory that we currently have in the first iteration.",
                    "label": 0
                },
                {
                    "sent": "Again, it's just a question, but then overtime as we accumulate more and more facts and the memory, this will change and I'll tell you how in a second and then basically just a bunch of different correlation and sort of distance metrics between.",
                    "label": 0
                },
                {
                    "sent": "All these three vectors.",
                    "label": 0
                },
                {
                    "sent": "And then we just pipe it through a deep network of very simple two layer neural network to get a gate.",
                    "label": 0
                },
                {
                    "sent": "Now this is based.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A way that the question can turn on specific gates over the input sentence is.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we can finally summarize the important facts in an episode vector, where we take the softmax over all these gates and sum them up over the sentence vectors at the different time steps.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is exactly here an example.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of what happened if we only passed over data once and that we couldn't actually?",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sir, I already went over that.",
                    "label": 0
                },
                {
                    "sent": "Now the ability here of the model to be able to iterate over the inputs multiple times is very important.",
                    "label": 1
                },
                {
                    "sent": "So the first time the iterates over them, the memory state is just a question, but the second time we will actually update the memory state based on separate Gru.",
                    "label": 0
                },
                {
                    "sent": "So now we have an input Gru.",
                    "label": 0
                },
                {
                    "sent": "We have a first Gru over the inputs that is triggered from the question and then we have.",
                    "label": 0
                },
                {
                    "sent": "A third year you that tries to accumulate the facts, so this one here again.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was a combination of all the facts that mattered based on a weighted sum of those sentences.",
                    "label": 0
                },
                {
                    "sent": "So here the facts are in the sentences.",
                    "label": 0
                },
                {
                    "sent": "The gates tell us how important is that sentence.",
                    "label": 0
                },
                {
                    "sent": "To answer the question right now, and you basically have a sum over.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those.",
                    "label": 0
                },
                {
                    "sent": "And those are the final inputs to 1/3 Gru in that sequence.",
                    "label": 0
                },
                {
                    "sent": "So it's basically this is what I meant with like having the Lego pieces and keep putting them together.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty complex way of putting them together, but it's also fairly clean, right?",
                    "label": 0
                },
                {
                    "sent": "You just have a large set of.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Practical Gru's and then the answer module is in many cases many ways very simple.",
                    "label": 0
                },
                {
                    "sent": "Again, you just have again sequence model Gru that basically takes us input.",
                    "label": 0
                },
                {
                    "sent": "The question at every time step and the previous previous output just again just to generate if model just like we had in machine translation where you output award at each time step during the decoding and the previous time step of that answer module the hidden state.",
                    "label": 0
                },
                {
                    "sent": "Of the Gru.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the YT minus one is the output of the softmax that you input to the next step.",
                    "label": 0
                },
                {
                    "sent": "Oh well, right?",
                    "label": 0
                },
                {
                    "sent": "So you actually do similar to what people do in empty where you actually take the single one to one hot vector of that output.",
                    "label": 0
                },
                {
                    "sent": "So this is putting it all together in one large graph.",
                    "label": 0
                },
                {
                    "sent": "So let's walk through it again.",
                    "label": 0
                },
                {
                    "sent": "So here we have some input story.",
                    "label": 0
                },
                {
                    "sent": "We compute our input module.",
                    "label": 0
                },
                {
                    "sent": "Independent of what the question is, we could ask multiple different questions on that same input.",
                    "label": 0
                },
                {
                    "sent": "Then we have a question here where is the football now?",
                    "label": 0
                },
                {
                    "sent": "We have a vector, a question vector from the question module Q and now that question vector triggers an attention over these different facts.",
                    "label": 0
                },
                {
                    "sent": "So you might say where's the football?",
                    "label": 0
                },
                {
                    "sent": "The first fact that mattered this is these are the equations that or the arrows that come into here are basically John put down the football.",
                    "label": 0
                },
                {
                    "sent": "That is the only one that has a very large weight because there isn't.",
                    "label": 0
                },
                {
                    "sent": "There aren't any other facts about the football.",
                    "label": 0
                },
                {
                    "sent": "In this input sequence, so John put down a football gets a weight of one for this Gru.",
                    "label": 1
                },
                {
                    "sent": "And you know, we kind of.",
                    "label": 1
                },
                {
                    "sent": "There are some other ones that might have tiny bits of weights, but we just assume or we just don't show the arrows here, but in general there they all have the option to go in here.",
                    "label": 0
                },
                {
                    "sent": "It's just that if the wait is close to zero and this is an actual example from the test set, if the weight is close to zero, we don't draw the arrows.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have here jump down football is the most important one now that gets fed into this Gru that has its input.",
                    "label": 0
                },
                {
                    "sent": "That you know.",
                    "label": 0
                },
                {
                    "sent": "Inputs that are weighted by the attention mechanism coming from the question module and all these interactions and the gates.",
                    "label": 1
                },
                {
                    "sent": "Now that gets fed into the third year you which goes over the memory.",
                    "label": 0
                },
                {
                    "sent": "And now that memory Gru is now the input together with the question over another iteration over the input module and there is in fact a classifier that selects whether it has enough information to answer the current question or not.",
                    "label": 0
                },
                {
                    "sent": "So this is dynamic.",
                    "label": 0
                },
                {
                    "sent": "How many steps how many iterations you do over your input?",
                    "label": 0
                },
                {
                    "sent": "Some question, some question, answer, input, triplets require you.",
                    "label": 1
                },
                {
                    "sent": "3 require three iterations over the input.",
                    "label": 0
                },
                {
                    "sent": "Some are very simple.",
                    "label": 0
                },
                {
                    "sent": "You know John put read the book what the John read the book and so then you only need one iteration over the input and this is dynamically determined by the model.",
                    "label": 0
                },
                {
                    "sent": "And then once you basically iterate over this reasonably large wait to John, move to the bedroom 'cause it thinks you know.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's where the football is, 'cause we know John mattered, but then later on realizes and this is.",
                    "label": 0
                },
                {
                    "sent": "One beautiful difference here.",
                    "label": 0
                },
                {
                    "sent": "Between the dynamic memory network and the original memory networks from Jason West and they had to have because they had a bag of sentences representation, they had to add an extra feature saying this sentence happened before that other sentence in some ways and manually defined feature.",
                    "label": 1
                },
                {
                    "sent": "Whereas here because it's all the sequence models, actually learns what things happen before or after.",
                    "label": 0
                },
                {
                    "sent": "And then we have the final output here M and that's what's given as input to the answer module.",
                    "label": 1
                },
                {
                    "sent": "Now you might ask, how do we do the sequence generation?",
                    "label": 1
                },
                {
                    "sent": "And we actually enable the model to also have outputs.",
                    "label": 0
                },
                {
                    "sent": "So the answer module is actually a little more complex than in this CAS, the depending on the question, the answer module can actually be triggered at every state of this input as well, so not just at the end, but also at everyone.",
                    "label": 0
                },
                {
                    "sent": "And so that's very important.",
                    "label": 0
                },
                {
                    "sent": "And much easier to train and test if you have sequence models, but also questions like give me a list of all English Kings or something like that where you don't want to have to ask the model to 1st memorize all of them in one vector and then try to output them all.",
                    "label": 0
                },
                {
                    "sent": "But it can basically output them right away.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Where is the football?",
                    "label": 0
                },
                {
                    "sent": "Oh, I think because of the data set and will actually go in a live demo and we'll see how it's very specific in some ways.",
                    "label": 0
                },
                {
                    "sent": "And this is a problem with the synthetic data set.",
                    "label": 0
                },
                {
                    "sent": "Where is the football usually to ask?",
                    "label": 0
                },
                {
                    "sent": "Where is the football right now?",
                    "label": 0
                },
                {
                    "sent": "Or at the very end?",
                    "label": 0
                },
                {
                    "sent": "And the model here already figured out that as you go over this, this was the last time and so it just puts a large weight on that more often and then sometimes in the data set.",
                    "label": 0
                },
                {
                    "sent": "John moved to the bedroom is actually like that was the last time John was mentioned, so it doesn't know for sure.",
                    "label": 0
                },
                {
                    "sent": "But in the training data sees so often.",
                    "label": 0
                },
                {
                    "sent": "Multiple facts about it that it knows that for these kinds of questions, the lost state matters much more so than others.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The tasks that this model gets above state of the art performance and then close to it, plus minus usually like two 2% accuracy or trueblue points are these three here.",
                    "label": 0
                },
                {
                    "sent": "So we have question answering that sort of requires some logical reasoning and we'll go over some more examples.",
                    "label": 0
                },
                {
                    "sent": "In a second, the types of questions that are in this data set.",
                    "label": 0
                },
                {
                    "sent": "Outperforms Facebook only by like .3%, but it also doesn't require sort of the previous sentence feature or engram or character ngram or word ngram kinds of features.",
                    "label": 0
                },
                {
                    "sent": "It's basically all enter, end, trainable sequence models.",
                    "label": 0
                },
                {
                    "sent": "Then it actually outperforms almost two decades of NLP research on part of speech tagging.",
                    "label": 0
                },
                {
                    "sent": "This is a pretty old data set the Penn treebank and then on sentiment analysis it also outperforms a lot of other.",
                    "label": 0
                },
                {
                    "sent": "Papers that some of which are my own where we use tree structures, but it essentially just goes over these inputs doesn't require that restructures to very accurately classify the sentiment.",
                    "label": 0
                },
                {
                    "sent": "And then on any art, we're still 2% or one point 7% or so below state of the art.",
                    "label": 0
                },
                {
                    "sent": "The state of the art actually uses also deep learning at this point and.",
                    "label": 0
                },
                {
                    "sent": "Actually trains word vectors together with the.",
                    "label": 0
                },
                {
                    "sent": "With the named entity tags and a very large gazetteer features so lists of cities and peoples names and so on.",
                    "label": 0
                },
                {
                    "sent": "It's a it's a tough task and on machine translation we only looked at English and French were 2% true blue points below the Google like LST Emtala stem results that were described yesterday and then coreference resolution.",
                    "label": 0
                },
                {
                    "sent": "We actually now have the state of the art on this data set from this year.",
                    "label": 0
                },
                {
                    "sent": "Go ahead.",
                    "label": 0
                },
                {
                    "sent": "I'll some folks at Maryland.",
                    "label": 0
                },
                {
                    "sent": "And then we're just a little bit below on a Connell data set for coreference.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's some more details before we go into live demo.",
                    "label": 0
                },
                {
                    "sent": "So these are basically different subtasks that the Facebook data set includes.",
                    "label": 0
                },
                {
                    "sent": "So there are single supporting facts that are very simple.",
                    "label": 0
                },
                {
                    "sent": "You just find one sentence and you're done.",
                    "label": 0
                },
                {
                    "sent": "We have two supporting facts, three supporting facts.",
                    "label": 0
                },
                {
                    "sent": "And you have yes, no questions counting lists, so this is this is kind of interesting, so here for all these kinds of questions you really only have one output vector in the end.",
                    "label": 0
                },
                {
                    "sent": "But it's still pretty good at counting and lists, and much better than than the original memory networks.",
                    "label": 0
                },
                {
                    "sent": "Simple negation is fairly straightforward, indefinite knowledge, so answers that include may because it doesn't give you exact facts.",
                    "label": 0
                },
                {
                    "sent": "We have some simple coreference resolution.",
                    "label": 0
                },
                {
                    "sent": "Time reasoning and then the model the tasks were actually all the models are still fairly bad at is our positional reasoning and pathfinding and we tried to understand why we still really haven't really understood why we're not that good at those.",
                    "label": 1
                },
                {
                    "sent": "In theory they should work too, but in practice they don't.",
                    "label": 1
                },
                {
                    "sent": "So this is the Bobby data set.",
                    "label": 0
                },
                {
                    "sent": "These are the results on.",
                    "label": 0
                },
                {
                    "sent": "Part of speech tagging, part of speech tagging, is a pretty fought over task for over two decades, so you're in sort of just the last couple of percentage points out of it, and on sentiment analysis here.",
                    "label": 0
                },
                {
                    "sent": "We have the state of the art on the binary task, which we're now at 88, and our best recursive neural tensor networks were at 85, so you're really getting more and more of the really hard cases right, but in many cases even or sentiment is actually an interesting task in that you can very quickly get to something like 70% with a very simple bag of words, baseline right, 70%, not hard.",
                    "label": 0
                },
                {
                    "sent": "You just need to find all the senses that mention awesome, wonderful.",
                    "label": 0
                },
                {
                    "sent": "Amazing and then there's only so much you can do to the word amazing and wonderful to make it negative, right?",
                    "label": 0
                },
                {
                    "sent": "So you get somewhere pretty quickly, but then as you get, try to inch out the last couple of percentage is at that point you either need to have more and more world knowledge so you have the larger and larger data set that understands that if you talk about I don't know Matthew Mcconaughy, it's probably a good movie and I don't want to mention any other actors names.",
                    "label": 0
                },
                {
                    "sent": "But you know, there's some actors that are associated with pretty bad movies and you just kind of have to know that.",
                    "label": 0
                },
                {
                    "sent": "And or you really have to capture complex syntactic structures, like expert wises, contrastive conjunctions, where you might say in the beginning.",
                    "label": 0
                },
                {
                    "sent": "I like the movie, but very quickly it petered out and became pretty boring and you now need to understand that this, you know, expert, why the Why matters more, so it actually becomes a more interesting task as you get higher and higher performance.",
                    "label": 0
                },
                {
                    "sent": "Alright now let's look at some examples.",
                    "label": 0
                },
                {
                    "sent": "Of this, so here we have.",
                    "label": 0
                },
                {
                    "sent": "A story yesterday, Bill Journey to the office, blah blah, and now we ask where was married before the bedroom.",
                    "label": 1
                },
                {
                    "sent": "And now you can think about how you would solve this, but essentially here we can visualize the episodes and basically realized OK. First it asks very puts very much emphasis on where Mary was when Mary was in the bedroom, and so was this afternoon and then basically realized.",
                    "label": 0
                },
                {
                    "sent": "OK, that's not enough to answer this question.",
                    "label": 0
                },
                {
                    "sent": "It has the second episode goes over it again, and then says Mary Journey to the kitchen this morning.",
                    "label": 0
                },
                {
                    "sent": "And so the answer is kitchen.",
                    "label": 0
                },
                {
                    "sent": "So this is just one example, here's another one.",
                    "label": 0
                },
                {
                    "sent": "So let's see, maybe we'll try to do it together and see if we are very good at it.",
                    "label": 0
                },
                {
                    "sent": "So what color is Greg?",
                    "label": 0
                },
                {
                    "sent": "Now how would you solve that problem?",
                    "label": 0
                },
                {
                    "sent": "Craig's right now.",
                    "label": 0
                },
                {
                    "sent": "Alright, very fast, that's right.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting here is that it actually requires multiple steps of reasoning, right?",
                    "label": 0
                },
                {
                    "sent": "So first you say, alright, Greg, what's important about Greg?",
                    "label": 0
                },
                {
                    "sent": "He's a rhino now.",
                    "label": 0
                },
                {
                    "sent": "It doesn't tell you Greg is white, so it's clearly more than one episode that you need.",
                    "label": 0
                },
                {
                    "sent": "But now we look at rhinos.",
                    "label": 0
                },
                {
                    "sent": "And there's also no sentence that just says rhinos are of this color.",
                    "label": 0
                },
                {
                    "sent": "But instead it actually says, alright.",
                    "label": 0
                },
                {
                    "sent": "So we look for Rhino.",
                    "label": 0
                },
                {
                    "sent": "Bernard is a rhino.",
                    "label": 0
                },
                {
                    "sent": "OK, now we need to look for Bernard and then it tells us Bernard is white and let's hope the model got it rained.",
                    "label": 1
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's actually an interesting that is actually interesting.",
                    "label": 0
                },
                {
                    "sent": "Point here is that in many cases, so this one here is kind of simple.",
                    "label": 0
                },
                {
                    "sent": "But if we just change no burden artist some other color, that's exactly right.",
                    "label": 0
                },
                {
                    "sent": "It can actually now do things based on that story, and in some sense that is a very different regime to what we usually do have just capturing a lot of current statistics right?",
                    "label": 0
                },
                {
                    "sent": "Because it put the attention on that sentence, it can output with a much higher probability that word.",
                    "label": 0
                },
                {
                    "sent": "Now if you said like.",
                    "label": 0
                },
                {
                    "sent": "Some crazy color that wasn't ever in the data set.",
                    "label": 0
                },
                {
                    "sent": "It still won't do it.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's kind of easy to add.",
                    "label": 0
                },
                {
                    "sent": "You could just edit word vector and then at Test time train you or vectors to then be and add them to the softmax and things like that, but it still to me a much more interesting regime 'cause it's different to just re collect a lot of core current statistics and then capture capture those.",
                    "label": 0
                },
                {
                    "sent": "It really needs to do something for that specific story.",
                    "label": 0
                },
                {
                    "sent": "That is right.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "And the only reason it gets his right is 'cause it's seen.",
                    "label": 0
                },
                {
                    "sent": "Other kinds of examples of this nature.",
                    "label": 0
                },
                {
                    "sent": "They weren't exactly that in most cases, though, that data set has a little bit of overlap for some of the types of questions, but in general, if it's never seen, that kind of input question answer triplet, that would have given it this type of reasoning, it wouldn't get it.",
                    "label": 0
                },
                {
                    "sent": "But if it's actually sees that you know rhinos can be of different colors and things like that, then some answers might be.",
                    "label": 0
                },
                {
                    "sent": "Maybe then, it would also eventually say maybe you know if you ask, like is the color red or something like that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there's basically again only if you have.",
                    "label": 0
                },
                {
                    "sent": "Only if you have this in the training data, but there are indefinite knowledge kinds of questions now.",
                    "label": 0
                },
                {
                    "sent": "One interesting problem that.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "It's not going to answer maybe and the problem here is that, and this is exactly the problem with these synthetic datasets, or we just may be the size and complexity of this particular synthetic data set, but the indefinite knowledge kinds of questions.",
                    "label": 0
                },
                {
                    "sent": "Won't generalize to other kinds of indefinite knowledge questions, so I forgot what exactly indefinite knowledge questions asked, but there you might ask, maybe, but if you now take one of these, yes, no questions or like 3 supporting fact question and you add somewhere in between that line of reasoning, some other fact, it won't say.",
                    "label": 0
                },
                {
                    "sent": "Maybe it basically doesn't combine as much as we would want all these different types of reasoning steps.",
                    "label": 0
                },
                {
                    "sent": "It really learn specific patterns.",
                    "label": 0
                },
                {
                    "sent": "From each of the sub datasets.",
                    "label": 0
                },
                {
                    "sent": "Sorry we have one Darren.",
                    "label": 0
                },
                {
                    "sent": "I'm walking North and then I turn right?",
                    "label": 1
                },
                {
                    "sent": "I mean that that seems like something that is reasonably simple if it's seen if it snows about North East, and West that it should be able to do those kinds of things.",
                    "label": 0
                },
                {
                    "sent": "Yeah, 'cause it has the IT has the theater.",
                    "label": 0
                },
                {
                    "sent": "It has a sequence of steps that you've seen before so it knows in that context.",
                    "label": 0
                },
                {
                    "sent": "In the input module you know what happened before.",
                    "label": 0
                },
                {
                    "sent": "Performance degrade.",
                    "label": 0
                },
                {
                    "sent": "So three supporting facts.",
                    "label": 0
                },
                {
                    "sent": "I mean, that is basically the reason.",
                    "label": 0
                },
                {
                    "sent": "So I think the main main problem there is that we don't have very variety of different features and it can retrieve in sort of a bag of features, model the different aspects.",
                    "label": 0
                },
                {
                    "sent": "Perfectly we have to we go over the sequence and as the sequence gets longer and longer and the three supporting fact questions sometimes you can have 40 or so inputs, the sequence degrades an I think they're probably smart ways of increasing the LCMS, and things like that for that particular task, but then you may be more likely to overfit on other tasks.",
                    "label": 0
                },
                {
                    "sent": "Representation for memory.",
                    "label": 0
                },
                {
                    "sent": "It works.",
                    "label": 1
                },
                {
                    "sent": "So I mean, it's a little more complex to just describe the whole model in a sentence without many visuals, but you have essentially bag of words and bag of ngram representations for the different sentences, and you can retrieve them after you retrieve the first one.",
                    "label": 0
                },
                {
                    "sent": "You have perfect access to all the other ones.",
                    "label": 0
                },
                {
                    "sent": "Again, you don't try to store them all in one sequence.",
                    "label": 0
                },
                {
                    "sent": "I think that's probably the main reason why they don't degrade.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this model won't do that well, but the sentiment model that was trained with the same.",
                    "label": 0
                },
                {
                    "sent": "It's the same kind of DMN will actually get that, so maybe that's a good segue to say like come up with some hard examples.",
                    "label": 0
                },
                {
                    "sent": "So despite the wonderful reviews.",
                    "label": 0
                },
                {
                    "sent": "This movie wasn't a particular particularly positive experience, and now it's that ask what's the sentiment?",
                    "label": 0
                },
                {
                    "sent": "And now it will tell me it's negative.",
                    "label": 0
                },
                {
                    "sent": "So this is a somewhat harder example, so you have wonderful reviews and positive experience and have two interesting kinds of negation.",
                    "label": 0
                },
                {
                    "sent": "Despite this, not sort of very obvious negation.",
                    "label": 0
                },
                {
                    "sent": "And at least it wasn't.",
                    "label": 0
                },
                {
                    "sent": "Here is further away from the positive experience that it's trying to negate, and it still gets this.",
                    "label": 0
                },
                {
                    "sent": "Very correct, and here's a little bit of an attempt at visualizing what's going on.",
                    "label": 0
                },
                {
                    "sent": "So here we basically have the hidden states of the input for each of the words, so despite it starts in a specific way and then you have here the wasn't.",
                    "label": 0
                },
                {
                    "sent": "For instance, once you have the negation, some some of the hidden units basically turn on.",
                    "label": 0
                },
                {
                    "sent": "Or become much stronger than they were before, and this is an attempt to sort of say, alright.",
                    "label": 0
                },
                {
                    "sent": "If we had a classifier at every time step.",
                    "label": 0
                },
                {
                    "sent": "This is just for visualization.",
                    "label": 0
                },
                {
                    "sent": "It's not actually necessary for the model.",
                    "label": 0
                },
                {
                    "sent": "What is going on?",
                    "label": 0
                },
                {
                    "sent": "So it says here, despite the wonderful reviews.",
                    "label": 0
                },
                {
                    "sent": "So it's very mildly positive, but not very much so.",
                    "label": 0
                },
                {
                    "sent": "And then once you say it wasn't, it basically is very hard to recover from that.",
                    "label": 0
                },
                {
                    "sent": "And even if you say positive experience.",
                    "label": 0
                },
                {
                    "sent": "Basically, knows once there was a nod here that these things will be negated even if they are positive.",
                    "label": 0
                },
                {
                    "sent": "That is an excellent idea and exactly what you would want to do and what we may or may not be doing right now.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So the I'm not sure the.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1010 memory I don't think it gets higher than the 93.6 on the Bobby data.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That, and I don't think any of the other memory networks actually show results on as well, like the question answering traditional question answering as well as sentiment classification, which is much different and as well as.",
                    "label": 0
                },
                {
                    "sent": "And maybe this is.",
                    "label": 0
                },
                {
                    "sent": "Right, well and it's.",
                    "label": 0
                },
                {
                    "sent": "It's all sequence models that also allow you to output outputs at every element of the sequence or at the end.",
                    "label": 0
                },
                {
                    "sent": "So here for instance, we ask what's the part of speech and basically have here an answer of the part of speech sequence for that sentence.",
                    "label": 0
                },
                {
                    "sent": "So they're basically three completely different kinds of tasks, and I don't think any of the other memory networks.",
                    "label": 0
                },
                {
                    "sent": "That kind of line of work has been shown to get state of the art results on three pretty competitive datasets.",
                    "label": 0
                },
                {
                    "sent": "So in the interest of time will have only three more minutes.",
                    "label": 0
                },
                {
                    "sent": "Maybe we'll get to your question at the end, but.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with that I want to conclude basically there's a potential statement here that all but if you can think of some examples that aren't, let me know.",
                    "label": 0
                },
                {
                    "sent": "I think all NLP tasks can be reduced to a question answering kind of problem and in the beginning it was kind of a trivial but useless insight until you have a model that tries to actually use that and train on that kind of input question answer.",
                    "label": 0
                },
                {
                    "sent": "Mail type triplets.",
                    "label": 0
                },
                {
                    "sent": "And so I think one of the next interesting steps will actually be to train one very very large multi task.",
                    "label": 0
                },
                {
                    "sent": "Yeah man, right now you can essentially trigger different demands to pipe depending on the question, but if you actually and we train on these three task we can actually train a multitask model too.",
                    "label": 0
                },
                {
                    "sent": "But then you won't get to you won't inch out the last couple of percentage is because the different tasks will interfere little bit and reduced accuracy by like 1 or 2%.",
                    "label": 0
                },
                {
                    "sent": "So that is kind of an interesting next question.",
                    "label": 0
                },
                {
                    "sent": "How can you compartmentalize some of the knowledge and still have these state of the art model on that task, but also actually have all of them the sharing the weights in some way so compartmentalized but also share the weights in one gigantic DMN and I'm pretty excited about, you.",
                    "label": 0
                },
                {
                    "sent": "Know if we can get there for NLP and deep learning.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}