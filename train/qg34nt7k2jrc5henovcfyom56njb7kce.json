{
    "id": "qg34nt7k2jrc5henovcfyom56njb7kce",
    "title": "Graphical Models",
    "info": {
        "author": [
            "Cedric Archambeau, University College London"
        ],
        "published": "Aug. 5, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/bootcamp2010_archambeau_gm/",
    "segmentation": [
        [
            "So I was supposed to give this lecture with my colleague.",
            "But I had to, you couldn't make it anywhere.",
            "So in the end I had to do everything so.",
            "So I will be talking about a graphical most probably graphical mounts.",
            "And so we will have the sessions the whole lectures this morning."
        ],
        [
            "Into two different.",
            "So I'll just start with a couple of reference material if you're you want to read more about it.",
            "There are a couple of you might already seen a couple of references in the previous, at least in Enza introduction.",
            "But there is a very recent book from definite Color and Friedman from from Stanford, which is a very good book on this topic.",
            "Very good, but the machine learning in general lots of very complete.",
            "Very very thick book.",
            "There is a book of Christopher Bishop from Microsoft Research which is and I'm going to follow mostly this chapter.",
            "In this presentation you can download the chapter from this web page so.",
            "When we put the slides online, you can just, you can just click on the link or you can just go and look for the title of the book and you will find the web page with all the chapter information about the datasets.",
            "And also all the figures that I will be using in the in this slide deck.",
            "So it is also.",
            "Uh, and.",
            "More bit more technical introduction from Kevin Murphy, a bit older on graphical models if you're interested or you want to see a bit more about the details and some some earlier work on that.",
            "And then there is some.",
            "You can see that there are some.",
            "Much of the material, and it's like from the there's a book like in 1988 by my Pearl, so it's not like a new field.",
            "It's been studied for quite a long time in statistics and then more recently people started looking at it in machine learning.",
            "So one also inspiration for this talk will be the integral of the BearCat, an ACL 2009.",
            "So thank you to Barry for.",
            "For the good advice, there are also lots of talks on video lectures like this one will be recorded so you can have a look at the data afterwards as well."
        ],
        [
            "So probably this is my my take on machine learning and more like interested in the statistical probabilistic side of machine learning and I view machine learning as a real.",
            "Yeah, it's at the intersection between statistics and computer science.",
            "So you have like a lot of examples about that in the first talk.",
            "The thing is that nowadays you have like data which is everywhere you have, like web images, sound sensors, networks and so you can gather information from all over the place.",
            "Now the problem is that first this data is noisy.",
            "You have like you don't have like perfect positions because of the sensors.",
            "That type of sensors because of outliers are typical observations.",
            "People that just like on the web images things that are wrongly labeled.",
            "For example, when you look at images.",
            "And the second problem is that you have a lot of data, so the data is increasing exponentially and it's becoming more and more difficult to take all that data into account.",
            "So you might imagine the more data you use, the better, But the problem with that is that there are computational issues.",
            "And so you have to do approximations.",
            "You have to select datasets.",
            "You have to do.",
            "Whatever you can imagine to extract as much information as you can, but while remaining tractable.",
            "So in any case, the in order to deal with noise.",
            "Graphical models are that's interesting.",
            "Point to look at probabilistic models and graphical models are one of these techniques and also graphical models have some interesting properties about tractability.",
            "In some specific cases, in any case.",
            "So in fact, the questions that we.",
            "We would like to answer the ideas that we're going to.",
            "We have data that we're going to model.",
            "For example, in a probabilistic way, so we have, we assumed it was some generation process of the data, which is some simplified version of reality, and so these models have parameters, so you want to learn those parameters.",
            "You have to find some way to do that given the data field served.",
            "You want also to compute some probabilities about specific outcomes.",
            "What's the probability of something happening?",
            "And then you want to make predictions.",
            "So try to generalize like I think it's already introduced before, so we want to generalize on new data points when you don't know the actual value.",
            "And of course, the central question is if we can do that efficiently.",
            "So there will be a part of this talk will be about that."
        ],
        [
            "So now about graphical models.",
            "This is again like the intersection of of two fields, which is probability theory and graph theory.",
            "And as I said, they're designed really to deal with in certainty in a principled way, and to have some some decomposition using some structure of the graphical model to reduce the complexity.",
            "So they are very important in recent years in machine learning and computational statistics, and the ideas that you have.",
            "Like since you have a lot of data, a lot of dimensions and parameters and and features you might have, you will have like probabilistic model, that I multivariant very big ones typically, and so you will find you are interested in some ways of structuring those probably density functions and decomposing then decomposing them according to some.",
            "But we call conditional independence assumptions, which we discuss in more detail afterwards.",
            "So the graph theoretic aspect is is very appealing because it's it's quite an interesting way of analyzing those distributions without reading the actual computations.",
            "So based on some properties of the graph, you can decompose those.",
            "You can have some some insights on how the densities is decomposing, so it does give you some really.",
            "It gives you really some idea of how this distribution is decomposed without having to assume a particular specific parametric form of distribution.",
            "So it's like general in dependencies conditional in dependencies.",
            "So that's very convenient if you want to.",
            "If you have like a lot of information about different subproblems, you can formalize them according to some some.",
            "Sub graph I would say.",
            "Some specific structure and then once you want to build like bigger.",
            "Bigger systems you have, like probability theory to combine all those parts, and that's roughly the probability is like the glue for the whole systems.",
            "So this system and probability does you also how to incorporate data when you observe data and how you learn to parameters."
        ],
        [
            "So please don't don't hesitate to ask questions if you have any.",
            "So graphical models are applied in many in many fields, ranging from by informatics.",
            "I will have an example just afterwards, but it's equally apply to natural language processing document processing, so we're also having an example of that when you have time series, for example speech processing or image processing, computer vision, they're very important where because Markov random fields, for example, are some.",
            "Some type of graphical models.",
            "They're very popular in other types of convexity, social Sciences, and so on, so it's something which is widely spread and has lots of applications."
        ],
        [
            "So it's only a couple of.",
            "A couple of examples, just to give you an idea of the applications.",
            "I guess you will be very interested in the application themselves.",
            "I will spend a pretty not too much time on applications apart from being in the beginning and then we'll go more into the the technical details.",
            "So you might have like the opposite, but.",
            "I think it's more useful to do it this way.",
            "You can always read about application afterwards, so roughly.",
            "So one way of, for example, organizing large sets of documents, large collection of documents is to use topic models.",
            "So topic models are roughly are.",
            "Clustering techniques, so unsupervised learning techniques to identify different themes or topics.",
            "What you call topics in large sets of large corpora.",
            "So this is done.",
            "Roughly.",
            "This is kind of quite interesting because it's capturing some semantics about sometimes hitting the the other sets of the set of document, which for us is very easy to discover, but which is very difficult from computer to discover.",
            "So here you have.",
            "One of the original papers on on topic models like some some history about it before, but this is like the more recent article based model for topic models, which is called the Little Jewish allocation.",
            "And was proposed in 2003.",
            "And roughly here you see what the model learns, so you had like a typically.",
            "Topics are described by a set of vocabulary.",
            "I mean, in fact.",
            "In fact, books are correspond to a discrete distribution over vocabulary words.",
            "So in fact you have like a weight associated to each word in the vocabulary, and then you can re rank them by the probability of this word describing this topic.",
            "So this is the four list that you see.",
            "It's like a re rank ordering of those those words according to different types of that were discovered.",
            "So in general you don't have read the label of the topic, just F. You know that's a big 1, two and three.",
            "So once you see the list of words, you can say OK.",
            "This looks like something which has some semantic meaning, and so people kind of labeled it like this artist is.",
            "This is topic children, education and so on.",
            "So you see there is some.",
            "So meaning when you go through the list of the words.",
            "So if this this this type of model is is based on uh, it's a generative model for text, but it makes very crude assumptions about that text, so it's based on the bag of words assumption, which means that in fact you assume every data point every word is generated independently in the document, so you ignore completely the sequence of words in the document, but to capture the semantics is quite a good model.",
            "This is this reasonable results.",
            "So.",
            "So that's roughly you have like a piece of text.",
            "And you see the coloring corresponds to the coloring of the different.",
            "Topics I'm sorry it's supposed to be in color.",
            "I noticed I lost the color yesterday evening so.",
            "So now we said we don't, yeah.",
            "It's a grayscale.",
            "We should be able to.",
            "At least two to find the budget one is quite easy to identify in the text.",
            "So the roughly the idea when I say it's a bag of course assumption that means that when you have this text, you assume that you are going to the gentleman.",
            "In fact, you're going to select every time you generate will award.",
            "You kind of select one topic, and then you draw a word from that from the distribution to generate a word."
        ],
        [
            "So another type of application for graphical models are models and bench formatics.",
            "So there's like plenty.",
            "Plenty of working in this field.",
            "Somatic systems biology.",
            "So this is 1 simple example where you have sequences.",
            "DNA sequences and you have like you said, you kind of compare different sequences with one another.",
            "So here on the left you have like two types of raw sequences and the question is.",
            "Usually in some cases you have like kind of gaps into sequences or so you don't, so they're not like perfectly corresponding to each other, and so you want to align them and you want to find the correspondence between the sequence and being able to see which are the very similar sequences.",
            "So on the right you have kind of.",
            "If I state machine reflects on the different states you have, like if this is letter matches it and do you have for the next steps are going to insert a new letter.",
            "Are you going to delete the letter so that allows you to model sequence?",
            "And you have some probability of transition between those different States and so typically you want to compare sequences for four in the same family or to understand better evolution in some way.",
            "Or evolution of different species identify which are the species that are similar."
        ],
        [
            "So third application is image denoising.",
            "Anne.",
            "And those typically those are Markov random fields that are used for that, and the idea is that you want to use some.",
            "Those are graphical models that are defined over neighborhood of, for example pixels.",
            "Which are the nodes of the graph?",
            "And so the idea here is you have in the top the top 4 pictures you.",
            "I mean the left one is the original one and then you have like the second is a is like the noisy picture that you observe that you get.",
            "So we've never observed the left left side one and so you want to reconstruct that one based on some assumptions which I said it's like neighborhood information.",
            "So if you use one type of graphical model then you will end up with the third.",
            "The third picture, again in color.",
            "It seems less good than in green, white and black and white, but the you can you see that it's a bit more hazy compared to the computer, the right one, so the right one is actually more so high order Markov random fields and more complicated model.",
            "And it does give better results.",
            "A closer match to the original picture on the left.",
            "This is a.",
            "This is the 2nd row is a similar type of.",
            "It's another example I see that here you don't see it very well, but I mean it's the same ordering the left image is their original 1, the one you observed is you observe is the second one, and then you reconstruct the original one in the 2nd and 3rd and the 4th column."
        ],
        [
            "And then since I'm from Xerox, I have to talk about printers a little bit, so the result you can even use a graphical models in when you try to manage printer reference infrastructures for example.",
            "So typical the setting here.",
            "Is that you were kind of managing like a fleet of printers, which means, like for so, that's for example, a big company, so that might fit.",
            "That means like up to $100,000.",
            "Maybe a bit a lot, but the account or or a couple of 100 printers.",
            "And so you're managing them from from a distance, so you need to know when to match that efficiently.",
            "You have need to know where the printers located, whether the characters characteristics of every printer.",
            "You also need to know whether the people using which are the people who are the people using those printers.",
            "And so you have some some user profile, but then you want to detect automatically if a printer is moved, for example, or if there is that kind of a failure, because softfail detection.",
            "So in this case for example, you're going to try to identify without going and see what's happening with the printer.",
            "If that printer is in fact working or not properly, and you can do that based on the usage of those printers by which, which users?",
            "And again, you can formalize that sort of a time series with some sort of like some users that are interesting, couple of States and devices in some states, or any of the states of the of the state.",
            "If it's of the printer.",
            "If it's active or failure, failing, or and so on."
        ],
        [
            "So that was just to give you a little.",
            "Flavor of a couple of object applications of graphical models.",
            "I think you had lots of examples also on Monday.",
            "And so this is a bit like other types of ideas for applying our graphical models.",
            "So I will start.",
            "This is the overview of.",
            "Of today's lecture.",
            "I will.",
            "I will first start with some basics about graphical models in the introduction.",
            "And in fact, in the first part that we will, we will probably stop after Markov networks.",
            "So we will describe these two types of graphic graphical models which are based on networks and Markov random fields.",
            "We might go a little bit into the exact inference, but then we will in the second part.",
            "The idea is to talk about the technical leader.",
            "How do you influence in those graphic amounts or how do you learn probabilities about some hidden nodes?",
            "And how do you do the learning?",
            "So how you learn the parameters in graphic amounts?",
            "And in fact you will have the so the practical sessions will be.",
            "There will be many.",
            "One will be an inference and one will be an unlearned in some specific cases."
        ],
        [
            "Right?",
            "So some so those are two examples of graphical models left on the right.",
            "Just to make sure we we talked about the same thing.",
            "We have nodes that are around that represents from variables when they're empty and like like mu on the left.",
            "Those are observed variables when they're shaded or the blue.",
            "Here there will be, we assume to be there soon to be observed.",
            "And then those nodes are connected in some way.",
            "So on the left you have directed arrows which denote the conditional conditional probabilities.",
            "And on the right you have, so this is called a base network on the left and on the right.",
            "This is a undirected graph, which is a Markov network.",
            "And again, those links corresponds to conditioning dependencies.",
            "This could correspond to dependencies between the nodes, and in fact it's a bit more space fact when you missing edges that you have conditional independency so.",
            "The link actually indicates like dependencies, so there's another little technical notation I won't use it a lot, but you will encounter it much more often when you if you continue reading about that this topic, so you have like also plates on the left which did not like repetitions.",
            "So in fact this is a.",
            "This is nothing else then.",
            "The shorthand notation for.",
            "Short attention for for this.",
            "But the short end representation rather.",
            "So there are also.",
            "We will not talk about combination of graphs, but people have looked at that as well.",
            "And then those are called chain graphs.",
            "Just two.",
            "Put on the so I will talk about an observed latent and hidden variables exchangeably so it's the same thing.",
            "I'm just using different names for the same type of object."
        ],
        [
            "So really, central central concept in.",
            "In probability graphical models is conditional conditional independence.",
            "And this I will just start with a brief description.",
            "I think you might have touched this bit yesterday.",
            "But in any case, statistically independent.",
            "So he said two variable X&Y are independent if the joint probability factorizes.",
            "So it's just a product of so the joint probability of X&Y is the product of the of X * P of Y.",
            "And then we say that X is conditionally independent of Y given Z.",
            "If you have this specific type of factorization, the first line.",
            "Do you have a like a sticker or something like that?",
            "No.",
            "OK, no thanks, will use it.",
            "So typically here.",
            "When you see the little hand.",
            "So if you have the joint probability of X&Y given Z, you can decompose it using the product rule of probabilities.",
            "Which is this?",
            "This is one way of decomposing it, and so we said if X&Y are condition dependent, then P of X given Y&Z is equal to 2P of X given Z, so it's conditioned benefit.",
            "Why?",
            "Of course, this could decompose this probability.",
            "I mean, this is one of the composite probability you can decompose in the other way around and then you obtain like kind of the same behavior for PFY given XZ is equal to.",
            "Given that.",
            "Right?",
            "So this is really quite important.",
            "One reason why this is important is that once you you know.",
            "That some variables are condition dependent.",
            "Then you can rewrite joint distribution in a more compact way.",
            "An intuitive way of seeing that is if looking at this equation.",
            "Well here this isn't.",
            "This is a problem in this probability at least, probably on the left is a is a probability of three variables, which is then decomposed in the product of two probabilities of two variables and that is one thing that we would like to exploit in the rest of the.",
            "So I will need a condition dependence by CI, so condition dependencies impose constraint.",
            "On the model.",
            "So the the kind of in use the structure.",
            "And.",
            "And the reason why this is the case that some random variables cannot take arbitrary values when you condition other variables, so you don't have like the full space of possibilities, you have only a reduced one.",
            "I mean, it's quite easy to see that in, for example, in this in the in.",
            "I mean, if you look at this, this typical example Y&Z determined some values for X, so you have like a much more if why are discrete variables that discrete values if they have the same the same size, the combination of the two is like something.",
            "Take care values.",
            "This is going to be squared possibilities for obtaining a value of X, whereas in this case value of Exxon depend on K values.",
            "That's where you gain.",
            "Yeah, represent."
        ],
        [
            "And so condition dependencies are quite.",
            "Quite intuitive concept is quite easy to understand if you have some examples, so probably nothing like this.",
            "My genome is independent of my grandpa of my grandparents unify conditioned on my mother's Geo.",
            "For my grandmother, my mother's.",
            "In the images you could.",
            "You could assume this is maybe less obvious, but you could assume that.",
            "Color of pixel is independent from colors far away in the image compared to the Direct Line neighbors of that pixels.",
            "The color of their neighbors of that pixel.",
            "And you can imagine how many other examples of the similar kind."
        ],
        [
            "So the central.",
            "Topic for today is the is probably graphical models.",
            "And so.",
            "We will have a set of random variables there by XN.",
            "And the values taken by those rabbits are, and so the random variables are denoted by the capital.",
            "Extend the valves taken bad news from the barrels are denoted by the small X accents.",
            "I will use small pee whether their discrete or continuous distributions or densities, because usually everything that that will be described we hold in both cases for continuous and discrete distributions.",
            "But the idea is that a public company is a graphical model that describes a family of joint distributions.",
            "And the joint distribution has some structure according to the conditional dependencies or condition.",
            "Temple independence assumptions, because usually you have like those are assumptions modeling assumptions.",
            "Which are denoted as as I explained before.",
            "So as I said, the the the advantage of graphical models that you can read off those.",
            "At least some of those, most of the condition.",
            "No.",
            "Sorry, all condition dependence properties from the graph by manipulations of the graph.",
            "And this simplified structure makes computation tractable and also storage more efficient.",
            "So we're addressing one of those key problems I mentioned at the beginning.",
            "So one way one people, and where people like to view that and that will become more clear afterwards is that you have like a joint distribution of over all the variables you have, like some graphical directed graphical model here, which I have some that has some missing links and this can be viewed as a sort of filtering of the distribution into like a distribution with some structure, some factorized.",
            "That satisfies some sex factorized conditional independency.",
            "And that's the mist, the weather.",
            "A large part of the this stock will be about discovering this condition in dependencies.",
            "And the reasoning on the graph."
        ],
        [
            "So the 1st.",
            "Typographic amounts are going to.",
            "To discuss our Bayesian networks.",
            "So businesses are directed graphs.",
            "So the errors denote."
        ],
        [
            "Dependencies.",
            "So call it also directed graphical models interchangeably.",
            "You have people also call them based Nets.",
            "I mean are different names for the same the same thing here you have like 3 nodes with some conditional dependencies so.",
            "In this case, so so thereby signatures are described by directed acyclic graphs.",
            "So that's that means that there are no cycles.",
            "I mean, there are no directed cycles in the graph.",
            "Anne.",
            "And we say that, for example, node A is the parent of naughty.",
            "And Conversely, note C is a child of, not a let's just like some terminology, and in fact what the arrow denotes kind of indicates is that.",
            "No, sorry and the one property in Bayesian networks is that the ancestors are so we have a chain chain of of node.",
            "The ancestor independent of.",
            "Sorry, the nodes independent ancestors given its parent, which is quite natural concept.",
            "So if we want to write the joint probability of a B&C.",
            "Just to choose some ordering.",
            "Of note and if you choose some ordering and you can decompose so the meaning of the graph is that C is is is conditioned on A&BI mean this gives you a way of decomposing the joint so the two errors coming from NB means that you can say this depends on NBP.",
            "Depends bid dependent B sorry, dependence A and then you have the marginal over.",
            "So if you choose if you change the order.",
            "Of the nodes, then you will obtain like you potentially will attain a different.",
            "A different composition.",
            "But not in this case.",
            "Is that is that is that clear cut?",
            "This is really important how you decompose, how you go from the graph to the decomposition of the joint.",
            "So this distribution as I said can be discrete, or they can be continuous.",
            "I was mainly focused on discrete distribution in this talk, but they can be continuous.",
            "Typically the only thing you have to do is that you have discrete variables.",
            "When you have you marginalized, you have to make some to compute sums.",
            "When you have continuous, you have to compute integrals."
        ],
        [
            "Did you, did you see that yes, you did talk about that yesterday or discuss that yesterday, marginalization?",
            "So organization or so?",
            "So there's some.",
            "There's some rule of probabilities.",
            "And that's roughly tells you that if you have.",
            "If you have a joint distribution of X&Y, the marginal.",
            "Here in this case, if it is the margin, otherwise just if it's a joint anyway, you sum over the X and in the case of continuous variables you just replace this by an integral."
        ],
        [
            "So.",
            "So what we have seen here is the."
        ],
        [
            "Small example where because of this.",
            "This dependencies in the graph, the hours in the graph you can you have like you can decompose the joint."
        ],
        [
            "Ticularly so if you choose a fixed ordering of the nodes.",
            "Then you have in certain factory color factorization of the joint distribution.",
            "And so we typically once you have the graph general graph, you can directly write this factorization down.",
            "We will denote PF and the parents of X of N. So for a very general joint distribution you can write it as a product of our excavator parts yet.",
            "So.",
            "What does she want?",
            "That's kind of the summer of probability, so if you so when you say if you typically.",
            "So if you look at the.",
            "So you have, like, X&Y?",
            "You have a discrete, so they have discrete values, so you have like.",
            "So depending on the combination of YX you will have a different hate here if you want to know what is the probability of of why independent of X.",
            "This corresponds to summing over all the peaks correspond to every value of X. I don't know if that was I didn't kind of so you have no.",
            "If that was the right way of saying we have like kind of a peak every.",
            "So this is just like the sum over X.",
            "If you want to have a total peak over X / Y.",
            "It's not a very formal way of explaining things, but hope this makes it clear.",
            "Yes, so the decomposition depending on during the decomposition is not unique.",
            "But so you have a product of factors and the distribution is correctly normalized.",
            "So once you've chosen.",
            "Like the condition, the condition dependencies you have chosen, like the parametric form of those distributions, those condition distributions, then the product will be always properly normalized for.",
            "Because of for the probabilities.",
            "But interesting business you have like local conditional distribution.",
            "So you're looking at local parts of the graph.",
            "So the question is whether this factorization is answering the questions.",
            "The question we were asking in the beginning at least is that has that any use."
        ],
        [
            "In terms of representation and so on.",
            "And so for this week, and we can look at a simple, simple graphical model, which is a chain.",
            "And we can also infer the 1st, so you can first look at this directed graphical model.",
            "This tag you can decompose the joint of exercise 2X and this will decompose in the following way.",
            "So you have the marginal over X because X does not depend on any other variable.",
            "Thank you, that's not like the Steelers were like visible vet.",
            "So the Joint Committee composed so you have X or 4X1 does not depend on any variables you have, like the marginal X one, time times the condition of X2 given X1, which is the 2nd.",
            "Due to the second error, and then if you have only three nodes and you have like this further decomposition in this way.",
            "So we assume that all the those variables are discrete and can take K values K different values.",
            "They all take the same, choosing the same set of values.",
            "To make it simple.",
            "And so, in fact, the factorization allows us to exploit some interesting properties that if you do not do the factorization, if you look at the marginal of X2.",
            "Then you have to sum over all the value of X1 the next week of this joint.",
            "So in fact here you have like a big if you wanted.",
            "If there's a discrete values you have, like a big tensor like a 3 dimensional matrix.",
            "If you want so you have like this is order of of Kate cubed.",
            "Different.",
            "Possible values, whereas if you.",
            "If you look at.",
            "If you so, if you look at if you decompose it according to this this factorization, then you can isolate the different terms.",
            "So use distributive loads of product and sum.",
            "So you can rearrange the sums because there's only depends on those two variables.",
            "Yeah.",
            "Yeah, there's a an error here, should be.",
            "This would be the probability of X1 anyway.",
            "And we can look at the prefix on here and CX-2 anyway, so this is this is should be the marginal of X1 in this case.",
            "So if we ordered determine those two cents.",
            "And in this case you only have this is of the order of complexity K squared, because in fact you have like every term has like a matrix of times K * K. So so you have like much less options there and then, since you have like 2 two terms, this is like 2 * K ^2.",
            "So this is not only interesting when you want to do computations, but this is also interesting in terms of parametrization.",
            "So roughly, this is exactly the same argument.",
            "So if you have this number now I'm looking at at.",
            "A sequence of random chain of of length M. So in fact you have like for every node you if you can take values you have like K -- 1.",
            "Parameters, because because the last one should.",
            "This should normalize to one with our discrete distribution.",
            "So the sum should be one.",
            "So you have like 1 constraint.",
            "That's why you have this minus one.",
            "So if you want to represent the joint then you have like K to the bar M -- 1 parameters, whereas if you look at the decomposition, you have K -- 1 parameters for the X of 1 and you have K * K -- 1 for those conditionals and minus one times because it's change of variables.",
            "So you have more efficient representation."
        ],
        [
            "So now the question is.",
            "We have some.",
            "Ideas about how to model the data?",
            "So we impose that by some local conditional dependencies, so you have like local conditional probabilities.",
            "Now, if you and those as I said, those are typically used by human experts, so they know some some like it like for example in doctors they know that this disease will not depend on this type of judgment or some other type of.",
            "So symptom so you have.",
            "Like some, you can have some conditional dependencies assumptions so.",
            "So this condition dependencies are usually by the expert they incorporated to the mall to formalize, like for example this specific model.",
            "But now the question is.",
            "So what we do, we remove links to impose us in dependencies.",
            "Now the question is, is removing those links, imposing those conditional independence is equivalent to factorization exactly equivalent, and in another to make it more precise, do we induce additional condition dependencies by imposing some?",
            "Do we induce others?",
            "He's a good thing or not.",
            "In fact there is 1, so we would not.",
            "We do not talk about D separation in so D for directed graph separation, so it's hard to see how to the set of rules that allows you to read off conditional dependencies in index.",
            "I'm.",
            "And the interesting bit about those those disapparation rules is that you.",
            "You do not require too.",
            "You don't have to work with the joint distribution directly, they probably directly directly work with the graph and then you transpose that to the joint distribution."
        ],
        [
            "So we have to introduce a couple of names for the different nodes in the directed graph, so therefore you have like head to tail nodes, tail to tail nodes and head to head nodes.",
            "So let's start with head to tail nodes and check the independence.",
            "Condition properties, so here we have again.",
            "We have a chain of three nodes and the question is so they had to turn out in this graph is grassy, so had today is because there is an arrow coming in and there is a tale of the other error coming out.",
            "So let's hit tail and so the question is whether A&B are independent in this type of graph.",
            "And so we're going to use so you have like the product rule of probabilities.",
            "You also have the same rule, so I mean I already used it, but I I can.",
            "I can remind me the sum of probabilities, so this is.",
            "I think this was clear.",
            "Hopefully this is clear because otherwise.",
            "You will have it.",
            "You had some troubles understanding the previous part, so that's the product rule of probabilities.",
            "And then you also have.",
            "If you decompose the product rule of probabilities in the other way.",
            "Yep.",
            "So if you decompose it the other way, you can you have like those.",
            "Those two terms that can rearrange the terms of the different factors and so you can compute.",
            "Before I give an X.",
            "Is equal to P of X given YP of Y. Divide by P of X.",
            "And I think we've discussed this yesterday.",
            "That's base rule, so this gives you the posterior probability of Y given X.",
            "Just give you a way of computing this probability of given X by knowing the prior probability that the likelihood of X given Y and the prior over of Y divided by the marginal effects.",
            "So those three rules are really key rules.",
            "They used all the time.",
            "Is that effectively everything's following for just from those three routes?",
            "In probability theory, so we can talk quite a lot.",
            "About just three words.",
            "So right so?",
            "So we want to check if the joint fact arises in this way.",
            "So this is using some.",
            "There's some rule of probabilities.",
            "This is equal to this this quantity, so it's the marginal.",
            "We look again at the joint.",
            "And then we use the graph to decompose this joint distribution so margin of a * C given eight times the probability of B given C. You can isolate B of A and so you obtain near this.",
            "In fact, in this joint PBC, given A and this, so where you sum over C, so this is going to give you the probability of B given A and.",
            "So we see that they are not independent.",
            "I mean, in general they're not independent.",
            "There could be some some specific cases, but in general.",
            "When you have like head to tail nodes and supersedes NB are not independent."
        ],
        [
            "Now let's check for conditional independency.",
            "So it's against a head to tail example.",
            "But now we assume that sees observed.",
            "So we conditioned on C, and so we're going to check again.",
            "The condition dependency this time, so does this joint probability of AB given C. Decomposing of the property given C times already given C. And it's here that we will be using Bayes rule so typically so we have this joint probability.",
            "This can be written in this form.",
            "So that's just applying the rule of the product rule.",
            "Right?",
            "There's just the product rule.",
            "And divided by the marginal of C. Then again, we're going to decompose the joint as before.",
            "And since we condition on so C is observed, we can apply here.",
            "Here we can use this to compute.",
            "Sorry, we can use this those three terms and apply base rule.",
            "So we obtained be of a given see.",
            "So we have this product of those two conditional dependencies and so we do obtain the result that is independent of the given see.",
            "So this is quite a slight interesting.",
            "I will come back to that later on.",
            "But if you pay some attention to this, the transformation from behavior to hear this is roughly giving you the probably of a given see.",
            "So what you've done is inverted the link in the directed graph."
        ],
        [
            "So this is this is the again the product of probabilities, so that's.",
            "So.",
            "Sorry this is the product of probabilities, but that exploits conditional independency between.",
            "So.",
            "So here BFC.",
            "B is independent of of a given its parent.",
            "Right which you see.",
            "OK, that's that's the first.",
            "Question I should have, so that's the first one.",
            "That's how we construct.",
            "Those basic networks."
        ],
        [
            "So this is a.",
            "This is this property that is imported from the beginning."
        ],
        [
            "OK, so now the next next.",
            "The next type of.",
            "Of note, is there are detailed detailed.",
            "So we consider different.",
            "So in this case it's again, so see that is a tail to tail node and so we want to check independence and conditional dependency.",
            "So independency again we kind of look at is P of does the product party of NB Factorizes in this way?",
            "And we kind of use the same strategy.",
            "We start by expanding writing the joint of the tree variables.",
            "But the marginalized for amortized over C. Then look at the graph and decompose.",
            "Decompose the graph according to the conditional dependencies.",
            "So that's directly right off the graph and then.",
            "I will check and correct that later on.",
            "I think I've copied like the wrong, modified the wrong.",
            "Yeah, it's just a bit.",
            "It's like a yeah, there's a bit of issues like I've missed 111 step which is you recombine, so you use the Bayes rule, but you have to.",
            "Prada who let's the factor PFA go back to the other side.",
            "So yes, it's like a rewrite, sorry it's like rewriting so you kind of have P of a given C times Pfc, so it's a joint and then you re decompose it in the other way, writing P of a times Pfc evening.",
            "That's right, so you can isolate a.",
            "And then you have the you have this.",
            "This marginally of the joint of B&C, given a which you obtain this type of form, which means that they are not independent."
        ],
        [
            "We have a similar result as in the head to tail notes, so so we had no independence, but we had condition.",
            "We have condition independency.",
            "So we assume now that she is observed and we do the same type of operation.",
            "So I want to check if this conditional here is going to factorize.",
            "In this way.",
            "You write again the joint and divide by the factor T. You decompose the graph as in the previous slide.",
            "You apply Bayes rule as in the previous slide.",
            "And you obtain this conditional at this.",
            "This product of conditionals.",
            "I'm not going to spend too much time in all the different settings, you can just recheck that by yourself.",
            "I think it's quite straightforward to do."
        ],
        [
            "Although I did have a problem in the previous slide, but that's because I'm tired.",
            "So now the third type of node is the head to tail node.",
            "And here so so here.",
            "The head to 10 or this note C so you have like 2 incoming errors in.",
            "See.",
            "And so again, let's check for the independence.",
            "So again, the same operation.",
            "You write a joint, but the using the.",
            "I'm sorry you can marginalise it if you want.",
            "You decompose according to the graph, so it's P of the marginal of paid at times the marginal of B times the condition of C given A&B.",
            "You can let the sun go into the.",
            "You can, we can ask.",
            "You can remove in the property of improved from the same.",
            "And so this sums to one.",
            "So you obtain the product of NB.",
            "So you have like independence.",
            "So this is the opposite.",
            "Behaviors in the other cases.",
            "So here we obtain independence.",
            "But we will."
        ],
        [
            "Do not obtain conditional independence.",
            "Where in the previous types of note we didn't have like independence, but we had conditional independence."
        ],
        [
            "Right?",
            "So once we.",
            "So we have like those rules we have.",
            "We have those condition dependence properties depending on the type of node.",
            "So then we also introduce thing we called blocked path.",
            "And we say that the path is blocked.",
            "If there are only observed head to tail or tail to tail nodes means that there is conditional independency.",
            "That means it's blocked or there is like an observed head to head to head node which has none of it descendants that are observed.",
            "So.",
            "Those correspond to the dependencies that were satisfying the trip, so.",
            "Previous previous slide.",
            "Tell me that when you have a head to tail or tail to tail, you have condition dependency independency there observed so the path is blocked so the different parts of the graph are independent.",
            "So for example in this case you have here.",
            "Here you have like a tail to tail node, which means that the path from A to B is blocked and so a.",
            "This is a condition independent of B given C and you also have here, here and observed.",
            "Head to tail node.",
            "We know with no observed sentence.",
            "So in fact is condition dependent of be given E as well.",
            "On the unconscious here you have.",
            "This is a head to head node which is which is an observed, but it has its end and so in fact in this is this graph is not conditioned condition independent of B given C or is not condition dependent.",
            "F given given C. Is just applying.",
            "So this is so here you have.",
            "This is just applying the previous rule.",
            "So if you condition an E, although it's an observed head to head node, so we should have independence of A&B.",
            "Because there is, C is observed, A&B are not independent.",
            "This is just examples of blocked path."
        ],
        [
            "Alright, so to summarize.",
            "Anne.",
            "So this separation.",
            "So we have like we said that if you have three subsets.",
            "No, no intersecting subsets of graph ABC.",
            "So subset of nodes in the graph we see that subset is conditionally independent of B given subset C. If it's possible.",
            "If all paths sorry from A to B conditioned on CR blocked.",
            "And hopefully that means there isolated from each other and so to just to check if every path is blocked, you have to look at all the nodes and check for head to tail, tail to tail and head to head nodes and if you encounter head to tail or tail to tail nodes then those should be observed that there should be in a subset C. If you encounter had two head nodes, then there should be.",
            "Not observed nor its descendants, so there should not be in C. So that's the that's how you check for D separation.",
            "Now when you look at so far I've only had like a latent hidden notes and an observed notes.",
            "If you had like parameters in the model, you can also include them there also and observe if you wanted that could be viewed as unavailable.",
            "Sorry, but they kind of their observed, so they kind of always at the top of the graphical model, so they always they always tail to tail.",
            "And so that they don't do not play any role in this operation.",
            "So you can ignore them.",
            "Because the deterministic values.",
            "And so we have a theorem that I will not.",
            "I will not prove, but roughly what it says is that factorization and conditional independence properties are equivalent.",
            "And so first factorisation, Blacks condition independency.",
            "So if you please like a product distribution that Factorizes according to the graph, so you have like this based on network.",
            "If a B&C are subsets are disjoint subsets, so that means that they have known noting in common.",
            "If you can check for this separation then then the joint can be decomposed as a according to this conditional conditional independency.",
            "So is independently given C. So that means that if you have.",
            "So if you have like you have imposed any type of.",
            "Condition dependence properties within a graphical model and then you check for the separation.",
            "Between different other parts of the model you can rewrite the joint in this different way as well.",
            "Now if you have condition dependency, that also implies factorization.",
            "And that's kind of.",
            "Quite natural, and so will be used so far.",
            "So that means that if pieces by sum sum continue pendency properties.",
            "Implied by this oppression.",
            "Then in fact, factorise according to this deck.",
            "So it can be described by this directed cyclic graph at the joint."
        ],
        [
            "You can decompose it.",
            "You have this factorized form of the joint.",
            "By the condition of extend given the parents.",
            "And so we can.",
            "We would like to know what are the variables.",
            "If you have like you look at some variable XI, what are all the other variables in the?",
            "Which other variables of the graph does excite depend on?",
            "So we do have two.",
            "So what is the minimal set of variables unit you have to take into account?",
            "And so the way to check that is you have the joint neutral update this conditional.",
            "It's against the.",
            "The joint divided by.",
            "Divided by this normalizing constant so you have to sum over XI because your condition and all the other variables.",
            "You see that here.",
            "All the.",
            "All distributions were.",
            "So exactly neither.",
            "Appear.",
            "At one time it appeared it appears inside.",
            "Here it's like the the variable you have like 1 X I-1 distribution where XIXX is equal to XI and then exact can also appear in one of the parents of.",
            "Sets so.",
            "When excited, not happy either here or there.",
            "Then you can actually you can.",
            "Not this distribution does not depend on XI, so you can just ignore those terms, the simplify.",
            "You can put them out of the sun.",
            "And so the remaining factors here on top are.",
            "Probably have exact given its parents.",
            "And the probability of all the other nodes where XI appears appears in the.",
            "In the set of parents of XNI.",
            "And so typically what does that mean?",
            "That means that the remaining variables are are those ones.",
            "So you have the parents of XI.",
            "So excited when the perfect size and you have like the children and the parents of the children are the remaining ones.",
            "So that's what is called the Markov blanket of a basic network.",
            "So this is a minimal set of variables to isolate except from the rest of the graph.",
            "I mean another way of viewing this.",
            "Is that?",
            "So.",
            "Here, the coloring does not mean that.",
            "I mean, the fact is that if you assume you ignore for the moment the coloring.",
            "The reason why, for example, I mean it's quite natural that excite depend the parents of XI are you need to know the perfect condition to escalate this photograph.",
            "It's quite natural that you have to consider the.",
            "The children now the reason why you have also depends in the parents of the children is because of.",
            "If you assume that everything is an observed.",
            "Then"
        ],
        [
            "You have the head to head head, head to head notes and you see that A&B.",
            "I mean, if sorry if she's observed then India not independent, so that's why you have you have this."
        ],
        [
            "Is dependency on the parents of the children?",
            "OK."
        ],
        [
            "So we do it like in one run until 10 or we have like a Brexiteer.",
            "OK.",
            "Right, so that's worse for the explanation based networks, not let's move on to Markov networks.",
            "Which are also common in Markov random fields, factory use Markov random fields more often that then Markov networks."
        ],
        [
            "So I'm not good friends are.",
            "Undirected graphical models.",
            "There is.",
            "And the reason to look at those other type of graphical models is that you might have some conditional independency assumptions that you cannot satisfy in the Bayesian network.",
            "And in fact vice versa so.",
            "You cannot represent any factorization.",
            "That means that you cannot prove any factorization of a joint via Business Network.",
            "There's some that cannot be represented by, so that's why people start looking at other types of graphical unadaptive graphical models which aren't directed.",
            "Also, the idea is that we want something which is easier to check then the separation.",
            "So you want to check from coded conditioning dependencies without having to check all those D separation rules, but just looking at the different hard the graph looks like and you want just which part is independent from the other part of the graph.",
            "By directly working on simple graph semantics.",
            "Supergrafx separation.",
            "Anne.",
            "So again, microphone field is set of probability distribution distributions that is associated to a certain and directed graph.",
            "So again you can have for the same graph you can have different types of distributions that are associated with this graph.",
            "At this factorized forms, an here simply here.",
            "Before you had like, we're looking at directed edges.",
            "Here just the absence, so you have the condition dependency when you have, like a directed edge dependency.",
            "Sorry, condition independence.",
            "If your absence of directed Edge here, you just have a condition dependency.",
            "When you have an absence of an edge and again the variables can be discrete or random.",
            "They can be discrete or continuous."
        ],
        [
            "So condition dependency here is checked by simple graph separation, which is much easier to check then then.",
            "Then this separation.",
            "So we just look at.",
            "So if you have.",
            "A subset of nodes.",
            "So we have a.",
            "The subset aids would be a subset C, so we see that we have graph separation between A&B.",
            "That means that all path going from A to B are going through C and so this just implies that a condition dependent of B given C. So it's very easy to check.",
            "Don't.",
            "And so we say that in that case.",
            "The path is blocked from A to B.",
            "Also, the the Markov blanket is very easy to do to identify.",
            "This is just like the old nodes that are surrounding a specific node.",
            "So typically yeah.",
            "I mean this is a.",
            "White people usually use as.",
            "So this is like a certain directed graph.",
            "If you want to check all the variables that are.",
            "How to isolate this note from rest of the graph?",
            "You just need to condition.",
            "Unders on on those notes.",
            "All the organelles connecting.",
            "Connecting this note.",
            "Integration points.",
            "Quantitative variable.",
            "The first point there.",
            "Yeah, connecting losing it to be passed.",
            "Tracy.",
            "In this graph you can get to be without going crazy.",
            "Doesn't change the definition.",
            "I get to keep my going.",
            "Through the other, she did.",
            "She did good so she can see that you can get to see from E. Yeah, so you can get to be from.",
            "Either way please.",
            "No.",
            "No, that is not allowed.",
            "Yeah, but sorry.",
            "See, it's like those sees those is this set of notes right?",
            "OK, so yeah see is not under this like a set of notes, so it's like we're talking about sets of the same.",
            "Thing is for.",
            "In the case of, in the case of directed graph, so we're looking at.",
            "In the end, we're not looking at nodes, but at.",
            "Sets of nodes.",
            "So we were checking, but the separation between sets of nodes and he would just looking at separation between nodes, so graph separation between us."
        ],
        [
            "So an important another important concept in directed graph is clear clicks.",
            "And so a click is a is a complete sub graph.",
            "That means that every pair.",
            "Every pair of note in this subset of nodes, every pair is connected by an edge.",
            "So we have of course like X.",
            "One X2 is like one click, you just have one link you cannot.",
            "And another another click.",
            "Allergic leak is extreme X4X2.",
            "So we also say that click is maximal if it's not included in other another click.",
            "Anne.",
            "Another way of viewing that is that you cannot add an edge to a click without.",
            "Destroy the click property.",
            "So typically here.",
            "If you add to this is a maximal clique.",
            "If you add X one, then you're missing 1 edge between X1 and X4.",
            "So it is not a click anymore."
        ],
        [
            "No, of course.",
            "The interesting bit about.",
            "Undirected graphical models is that you can check for separation very easily.",
            "You just look at graph separation.",
            "Now the problem is that.",
            "You get it.",
            "You're paying something for that, is that.",
            "You have you.",
            "You can decompose a graph as before the product of functions.",
            "Except that here you need to introduce a normalizing constants instead.",
            "To do for the distribution to be properly normalized.",
            "So in fact, when you look at factorization graphical models, you look at all the maximal cliques.",
            "So we do know the maximum feedback CK.",
            "And so you have a product of functions PSI, which depends on the notes in CK.",
            "So that's what keeps side CKX sub C case.",
            "All the nodes of the graph belonging to CK.",
            "So I will use from no one and I don't think I've used it before, but.",
            "I will use bold X to concatenate a set of access when it's appropriate.",
            "And just for notation.",
            "So we don't have like local conditional distributions, but here we have like we call that local potential functions.",
            "So that they only depend on on local set of nodes.",
            "The only constraint is as is, that's ICK.",
            "Need to be a positive, not negative.",
            "In fact, yeah, maybe not negative.",
            "And this is simply because you want this to be a probability density function, so it has to be positive.",
            "So as I said in this case, those potential functions not as in the case of Bayes networks, that here they do not need to be conditionals.",
            "Or or marginals that can be any type of functions as long as they are positive.",
            "But the price you have to pay that in order to be a proper density, you need to introduce a normalizing constant and normalizing constant is kind of this product sum over all possible values of X of the set of nodes.",
            "You have written down.",
            "Write down the like the joint probability and speculation of the clicks, but like what's the underlying principle that tells you that you can do it like this?",
            "I can show it, why why?",
            "This follows directly from the separation the graph separation property.",
            "So that means that the fact that you're you can factorize it means that this that some parts of the graph do not depend on the rest, and they only depend on the local subset of nodes and.",
            "So probability of Wakeley given all the other things, and then you install these factors or.",
            "Yeah, that's roughly.",
            "Yeah, that's roughly what's happening here.",
            "You can, there will be a slide where you can do the mapping between the two, but in fact this is more general because it is not the sites do not need to be probability densities that can be.",
            "Any type of function as a positive.",
            "Just so you have more flexibility in that respect, but the price you have to pay is that you have to compute this quantity and the problem with this quantity is that this is.",
            "This is in general very difficult to compute.",
            "And so there are a lot of cases where it's a, it's an issue.",
            "So it's this quantity is sometimes called for partition function.",
            "That's an indication from physics."
        ],
        [
            "So again, it's like describing the relations between separation, conditional dependency and factorization.",
            "And we have the same.",
            "The same result as in the case of Bayesian networks, meaning that the condition depends property leads to some factorization.",
            "So if you have a factorization that implies that you have some condition dependency property that is satisfied, and so it is sufficient that A&B.",
            "So if you like again 3 subsets of nodes, non intersecting subsets of nodes of the graph, then we say that A&B.",
            "NBR separated by.",
            "See if if you can check that a separate in separated by.",
            "In, be a separate necessary.",
            "Then you can decompose the joint over those subset account.",
            "This condition dependency criterium.",
            "And now the opposite is also true, except that you have to impose that.",
            "That the probability of X is positive for all values back, so you don't have like 0 values of X, which is some restriction.",
            "This other reasons.",
            "So the fact that condition dependency implies factorization is known as the Hammersley Clifford theorem, which is a quite important theorem in graph theory.",
            "I will not prove the theorem, but.",
            "So you've heard the term.",
            "So.",
            "Another point is that since.",
            "The size just to be positive.",
            "We can also view them at so rewrite them in another form.",
            "We can say OK, this quantity is equal.",
            "2 is equal.",
            "This is what I mean.",
            "This is proportional to this afternoon was in constant is proportional to the exponential of some function E which if you interested in physics this looks accurate energy function and then those potentials potentials looks like look like Bozeman distribution.",
            "This is just like a way of.",
            "People that start to look at graphical models were also interested in physics problems."
        ],
        [
            "Right?",
            "So about the differences in the.",
            "The similarities between Markov random fields and.",
            "And Bayesian networks.",
            "So again, just to summarize, some of the things that we've seen so far is that conditional independence properties.",
            "Can be included in the in the representation of the graph.",
            "In the structure of the graph and the defined family of joint probability distribution that are structured in some particular way that factorizes in particular way.",
            "Condition depends.",
            "It can be checked by a separation in undirected graph or D separations, and those are check between groups of variables, not between variables independently, but just groups of variables.",
            "So the fact that we have like those local independent structure that means that we can, we can apply characterization of the joint which will lead to some simplification in terms of.",
            "Farmer translation and from in terms of computations.",
            "And an important point is that in all those cases you do not need to specify.",
            "You need to specify the.",
            "So you do not need to specify the actual form of the function, so all we've done so far is that we kind of kind of reasons on the density to join density decompose density without choosing for specific form of this joint distribution or the factors of this distribution.",
            "Just we can just conclude if some condition pens properties or factorizations depending on which which approach we take.",
            "Now the main difference is that the set of densities that are represented by director to undirected Tamils is different.",
            "I mean, it's not selects you can map some of them.",
            "So you can map based networks to Markov random fields and vice versa.",
            "But there are some distributions that cannot be captured by based networks and there are some distribution that cannot be captured by microphone field.",
            "So we need both type of tools.",
            "To deal with all types of.",
            "Of distributions, and that's why people also looked at combination of both, because that kind of widened the type of.",
            "Not with that you can consider.",
            "The main, so the interesting bit about microphone feeds that you do not need to.",
            "You can just look at potential which are arbitrary positive functions.",
            "But what you pay is that you have to compute a normalizing constant.",
            "Obtain the proper probability density would probably distribution, and that this this constant is coupling all the factors in the graph so that creates dependencies and that makes it more difficult.",
            "For example, in general in learning."
        ],
        [
            "So that's to come back to your to your question here.",
            "So just to illustrate that in some cases you can.",
            "You can map 1 to the other.",
            "If you look again at this.",
            "This chain of random variables.",
            "So we had this decomposition in terms of the base network, so we have like a marginal of X.",
            "One times the conditionals of XN plus one given XN.",
            "Product overall and so you can directly write that down into the form of a Markov random field, where in fact you.",
            "Every year every function PSI corresponds to the conditional.",
            "Here the only difference is that this function here you have absorbed X1 into you take this one is equal to the joint X one X2, so you have like someone has observed the conditional plus times the times the marginal.",
            "Now in this case, when you do this type of mapping then.",
            "Of course, this is equal to 1.",
            "That's not the case in.",
            "That's not always the case.",
            "In fact.",
            "It sounds it's the main complications when you want to map a Bayesian network into a Markov random field is when you have had two head nodes.",
            "So for example, this is like 1.",
            "One example where so here X Forza head to head node.",
            "And again, if you look at the condition dependency property, you see that in fact, when you want to, so typically what people do is they if you want to change a big network tomorrow from field, you just replace the edges by by undirected edges that show you the mapping.",
            "Then you write the joint, but you have to be careful when you have had two head nodes, because here just replacing those edges by ours is going to be not equivalent.",
            "In fact, you kind of you have dependencies between X1X2.",
            "An an X1 and X3 so you have to add edges here between those variables as well and this is called moralization, so that's how you do the mapping between one and two.",
            "So this is this is also this.",
            "This follows directly from the head to head property, saying that if you have a head to head.",
            "Note here, then X one is not independent X2, right?",
            "That's why we have to at this age."
        ],
        [
            "Right?",
            "So good, we have some time to start with the so I wanted to start a little bit about exact inference.",
            "Check.",
            "Get to where I want.",
            "Right, so now we move to the.",
            "The more interesting bit of the presentation, so before it was more like a descriptive of the different types of graphical models, what they represent.",
            "Quickly condition dependencies.",
            "How was the correspondence between the different type of networks are differences and now we're interested in?",
            "In how to do inference so we will be looking at exact inference, not approximate inference.",
            "That means that we will compute exact probability distributions over some subsets of nodes.",
            "So we're interested in computing marginals or computing."
        ],
        [
            "Pursue this air condition distributions over some subset.",
            "So we can one one of the the interest what we're interested in is, like you have observations.",
            "You have, like some data our observed.",
            "That means that I have some data.",
            "Sorry.",
            "So I mean, there's some nodes are are clamped to some fixed value.",
            "So that's quite easy to incorporate into the into the graphical model.",
            "It seems like in the past how we do that, just like fix the condition on those values.",
            "And so the goal is to compute the pressure distribution over one or more subsets of those those nodes given the observed ones.",
            "So that's what we're really interested in.",
            "So we have to compute marginals.",
            "So for example, we have X and given all the observed rivals.",
            "The type of inference algorithm will be looking at our belief propagation, which is a special case of the sum product algorithm.",
            "So we're looking at.",
            "Belief propagation, the chain and then move to a more general algorithm to sum product algorithm.",
            "That said, that's for computing marginal distributions then.",
            "We will discuss the Max product algorithm.",
            "And the Maxim algorithm, which is a variant of the same product.",
            "So instead of marginals you will look at the most likely configuration in your graphic amounts of the most like the value, the configuration, the value of the state that is the most probable.",
            "I will then briefly, but not too long, so I would just like to have like 1 summary slide on the junk junction tree algorithm, which is a more complicated algorithm that can work on more general class of graphs, but I will not have the time to go into the details of that part.",
            "Now the real the key.",
            "The key rule that we're going to apply is a distributive law, which is which.",
            "You all know which is which is based on this decomposition of a sum of products into a product of.",
            "Times 8 times.",
            "So roughly here you have like 3 operation in here we have like 2 operations and so we're going to explore that to make the algorithm more efficient.",
            "I already illustrated like some concepts about that in the beginning when I was looking at a chain.",
            "Directed chain so.",
            "The way those inference algorithms are viewed in general is propagating messages around the graph.",
            "So we say that nodes are propagating messages between each other, which is roughly information.",
            "About about the dependencies between the variables.",
            "And so we will talk about message.",
            "We are stuck with message passing algorithms.",
            "So there is another family.",
            "So of course exact inference is what we would be.",
            "So we wanted to exact computations.",
            "That's where we interested in the 1st place.",
            "Now when you have like very large large graphs that even if you have like an efficient representation exploiting distributive law.",
            "Not able to.",
            "Is still too demanding so you have to start looking at approximate inference techniques will not have time to look at that, but one example is variational message message crashing.",
            "Passing.",
            "Sorry and there are many other approaches such as and.",
            "You could also use people sampling or marketing Monte Carlo, but I will not have time to discuss those, so this is."
        ],
        [
            "Just to formalize what I comment I did earlier on that.",
            "What happens when you apply Bayes rule in a graphical model so?",
            "These are we stating more clearly what I've written on the blackboard, so roughly based on this is a specific form.",
            "So typically you have XX as an observation and Y as a latent variable parameter, for example.",
            "So this is called.",
            "This quantity is called the likelihood.",
            "So the likelihood of observing this observation given some parameter Y and so.",
            "You might have some additional information about the parameter.",
            "You might know that the property is only needs is for example strictly positive, so you might have some constraints on those parameters and you can encode data.",
            "One way of encoding those constraints is to consider prior distribution over those parameters.",
            "And so basically just tells us how to update a prior.",
            "Believe you have over some value on some variable Y in light of the data.",
            "Concluded likelihood and you update this prior believe into a posterior belief of given the observation X.",
            "And there is 1 central.",
            "There's one important hidden normalizing constant.",
            "Here is the marginal over of the data, which is also called versus.",
            "It can be called the partition function, which is also called the evidence in other settings.",
            "And this is really a key quantity in basin.",
            "Statistics and basic machine learning.",
            "So now if we look at this.",
            "This directed graphical model.",
            "Simple graphical model.",
            "So if you want to match this equation to this perfect model of triplets X by Y&X.",
            "Sorry for that.",
            "So.",
            "So here you have like given this graphical model here on the on the left you have, you can write a joint can decompose, I mean in any way you can decompose it given this dependency can decompose it in this way.",
            "And now if you apply if you apply Bayes rule.",
            "You see that you have to obtain.",
            "That the factor going to the other side, you can rewrite it in this way.",
            "Meaning that you effect effectively you've reverted inverted your of your.",
            "So effectively what you're doing is what we're interested in.",
            "This, like reverting the arrows in index.",
            "At least the ones from the observations to the."
        ],
        [
            "Viable so the 1st.",
            "Exact inference algorithm.",
            "Look at belief propagation, so belief propagation is really you have mean.",
            "The word says what we we have like.",
            "Some messages going from nodes exchange between different nodes and so in this case this is called a Markov chain.",
            "I will go a bit, I will discuss a bit more what the market party later on.",
            "So just think about it as this.",
            "Chain of very discrete variables.",
            "Which is represented here in terms of undirected graphical model.",
            "And the question is if we can compute the marginal probability of XN efficiently.",
            "So there is a couple of times I said this, and if computation would be of the order.",
            "If those discrete variables can take key values of your key to the power N, and being the number of nodes in the chain.",
            "And that can be seen by the fact that you want to if you want to do this marginalization then you isolate the normalizing constant.",
            "I need you to do the sum over all variables in the chain except X of N times the product of all those factors which depend on which every time depends on on two variables depending on so.",
            "I didn't quite explain it very well before, but so in fact this factor here depends on expanding so because there is a link between X1X2, so that's how you write this.",
            "This factors, which is related to your question, I think.",
            "So this is just like the joint made explicit.",
            "And you have to sum over all variables except extent.",
            "In fact, the idea when at least in the changes, is very easy.",
            "Is that you can.",
            "Since you do not send over extend, you can you can kind of isolate.",
            "You can regroup all the factors into two groups, so you can put all the sums for values extend smaller than an on the left and you can also re group all the factors and the sums of very larger than N on the right and so we can rewrite this product here as the product of 2 messages, M, Alpha and beta.",
            "Which are function of XN?",
            "Anyway to fix it?",
            "Because you don't come over over extensor extent appears in one of the factors and so by rearranging it like in according to the distributive law, you really have this product of you do the same, then you product transfer that successively from one to the other.",
            "So what is interesting in this representation is that in fact, if you look at every node you have again, you only have to look at.",
            "Pairs of nodes and so this is completely K squared and then you have since you have entered the complexities like oh and key squared here.",
            "So this one little additional detail is that so we called forward and backward messages.",
            "So if you look at extent, you have the message going from left to right, up to extend and the one from right to left.",
            "That's pretty straightforward."
        ],
        [
            "So if we want to compute P of XN, we just need all the incoming messages in XN.",
            "So in this new kind of change we just need 2 messages.",
            "And to compute the message on the left, you need all the previous ones in the chain and the computer.",
            "Once on the right you need all the subsequent 1 interchange.",
            "So if you're here, you pay attention to the."
        ],
        [
            "To the form of this.",
            "Of the expression of an Alpha male beta, the left and right messages."
        ],
        [
            "You can recognize that if you."
        ],
        [
            "Look at the successive ones.",
            "You will have just like 1 some.",
            "So if you look at X. X N -- 1 You will have all those oldest messages on the right, which means that if."
        ],
        [
            "You can write.",
            "The message you can ride the messages in Alpha and beta in the recursive way, so roughly it's like the sum.",
            "In this case you have like at.",
            "If you look interested in X and then you have like the factor XN including X 10 -- 1, XN times the message arriving at X and minus one.",
            "And of course, use two to initialize the recursion.",
            "You need to have some initial conditions and what people do.",
            "What you can typically do is that you assume the the message at the very end of the chain is equal to 1, and then so we can start the recursion.",
            "So if you're interested in one specific value, start at the end of the end at the root and the leaf in that both sides of the chain and just you propagate towards this.",
            "This is the point of interest.",
            "So you can easily include observed values.",
            "That means only that the values are fixed to specific.",
            "This is the variable is fixed with specific value.",
            "And you what you need only if you have like a chain of nodes, you only need N -- 1 messages to be computer times two, so directions and at every node minus one.",
            "Because you have this constraint on the the terminal ones.",
            "What is interesting is that.",
            "Once you have the merge."
        ],
        [
            "So if you look at.",
            "If you look at if you have like a computed P of X and given in terms of those messages, you end up with the product of those two messages that XN normalized by some constant.",
            "So you can compute.",
            "In fact, this normalizing constant anywhere in the chain by just summing over the values over extend.",
            "So if you're in."
        ],
        [
            "And extend, and that's a good way.",
            "A good location to compute the normalizing constant.",
            "You can just compute it this way, which is very cheap to compute.",
            "Much cheaper in any case, then compute looking at the total joint.",
            "And then finally.",
            "What is another interesting property is that if you're interested in.",
            "If the joint of the sort of marginal of two variables to extend of X N -- 1.",
            "It's quite."
        ],
        [
            "Easy to see that you just look at the messages you just.",
            "You will have like 2 of variables missing in two variables missing in the same.",
            "So you will have some overextending extra minus one so we can just propagate to those two variables the messages.",
            "So just like 1 missing if you want message from X -- 1, two X and so you just have the potential of this.",
            "Those two variables and the message arriving at those."
        ],
        [
            "Those two nodes and so the.",
            "The marginal of extent extra minus one is just the product of the left message, the right message and the potential linking those two variables."
        ],
        [
            "2 numbers in concern so I will.",
            "I will finish after this slide.",
            "So we can take a.",
            "Some air breather with so.",
            "In fact, we have illustrated belief propagation in a in a chain.",
            "It was quite quite natural.",
            "I mean quite easy to to formulate in terms of messages, in fact that.",
            "Propagation is applicable in any type of of tree.",
            "And would be exactly very similar.",
            "In fact, the idea would be that you will message will be arriving.",
            "You will need all the message arriving at a certain node and those messages will capture the information from all the all the variables in the rest of the graph.",
            "So examples of trees are, so here are three examples of trees.",
            "That means there's only one path between every every node.",
            "And so this is like an undirected example.",
            "Here you have like out of this traditional, I mean more intuitive way of a tree like in terms of a directed graph.",
            "You also have, like other types of graphs, which are going to Poly trees in directed graphical models.",
            "Or in this case you have you have multiple sources.",
            "Sorry, I wasn't specific enough, so in this case we call the route the top node and those are the Leafs.",
            "In this type of of tree, there's still one path from one or the other, but there are multiple routes, so that's why we call them Poly trees.",
            "So in the case of an undirected graphical models, you don't just have one path between the different nodes.",
            "And that's a tree.",
            "So it isn't so so like in the in the chain.",
            "If you have a tree, then you directly know where to start the algorithm because you have the leaves and and the roots.",
            "Are you?",
            "The idea is that you're going to start at the leaves, propagate all the messages up to the root, and then from the route back to the leaves.",
            "In a very similar way as in the case of a chain.",
            "And so there will be.",
            "That will be the.",
            "The topic of the the next next part where we'll be looking at how to do belief propagation, or more specifically the sum product algorithm for inference entries.",
            "Dingus"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I was supposed to give this lecture with my colleague.",
                    "label": 0
                },
                {
                    "sent": "But I had to, you couldn't make it anywhere.",
                    "label": 0
                },
                {
                    "sent": "So in the end I had to do everything so.",
                    "label": 0
                },
                {
                    "sent": "So I will be talking about a graphical most probably graphical mounts.",
                    "label": 0
                },
                {
                    "sent": "And so we will have the sessions the whole lectures this morning.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Into two different.",
                    "label": 0
                },
                {
                    "sent": "So I'll just start with a couple of reference material if you're you want to read more about it.",
                    "label": 1
                },
                {
                    "sent": "There are a couple of you might already seen a couple of references in the previous, at least in Enza introduction.",
                    "label": 0
                },
                {
                    "sent": "But there is a very recent book from definite Color and Friedman from from Stanford, which is a very good book on this topic.",
                    "label": 0
                },
                {
                    "sent": "Very good, but the machine learning in general lots of very complete.",
                    "label": 1
                },
                {
                    "sent": "Very very thick book.",
                    "label": 0
                },
                {
                    "sent": "There is a book of Christopher Bishop from Microsoft Research which is and I'm going to follow mostly this chapter.",
                    "label": 0
                },
                {
                    "sent": "In this presentation you can download the chapter from this web page so.",
                    "label": 0
                },
                {
                    "sent": "When we put the slides online, you can just, you can just click on the link or you can just go and look for the title of the book and you will find the web page with all the chapter information about the datasets.",
                    "label": 1
                },
                {
                    "sent": "And also all the figures that I will be using in the in this slide deck.",
                    "label": 0
                },
                {
                    "sent": "So it is also.",
                    "label": 0
                },
                {
                    "sent": "Uh, and.",
                    "label": 0
                },
                {
                    "sent": "More bit more technical introduction from Kevin Murphy, a bit older on graphical models if you're interested or you want to see a bit more about the details and some some earlier work on that.",
                    "label": 0
                },
                {
                    "sent": "And then there is some.",
                    "label": 0
                },
                {
                    "sent": "You can see that there are some.",
                    "label": 0
                },
                {
                    "sent": "Much of the material, and it's like from the there's a book like in 1988 by my Pearl, so it's not like a new field.",
                    "label": 0
                },
                {
                    "sent": "It's been studied for quite a long time in statistics and then more recently people started looking at it in machine learning.",
                    "label": 0
                },
                {
                    "sent": "So one also inspiration for this talk will be the integral of the BearCat, an ACL 2009.",
                    "label": 0
                },
                {
                    "sent": "So thank you to Barry for.",
                    "label": 1
                },
                {
                    "sent": "For the good advice, there are also lots of talks on video lectures like this one will be recorded so you can have a look at the data afterwards as well.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So probably this is my my take on machine learning and more like interested in the statistical probabilistic side of machine learning and I view machine learning as a real.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's at the intersection between statistics and computer science.",
                    "label": 1
                },
                {
                    "sent": "So you have like a lot of examples about that in the first talk.",
                    "label": 0
                },
                {
                    "sent": "The thing is that nowadays you have like data which is everywhere you have, like web images, sound sensors, networks and so you can gather information from all over the place.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is that first this data is noisy.",
                    "label": 0
                },
                {
                    "sent": "You have like you don't have like perfect positions because of the sensors.",
                    "label": 0
                },
                {
                    "sent": "That type of sensors because of outliers are typical observations.",
                    "label": 0
                },
                {
                    "sent": "People that just like on the web images things that are wrongly labeled.",
                    "label": 0
                },
                {
                    "sent": "For example, when you look at images.",
                    "label": 0
                },
                {
                    "sent": "And the second problem is that you have a lot of data, so the data is increasing exponentially and it's becoming more and more difficult to take all that data into account.",
                    "label": 0
                },
                {
                    "sent": "So you might imagine the more data you use, the better, But the problem with that is that there are computational issues.",
                    "label": 0
                },
                {
                    "sent": "And so you have to do approximations.",
                    "label": 0
                },
                {
                    "sent": "You have to select datasets.",
                    "label": 0
                },
                {
                    "sent": "You have to do.",
                    "label": 0
                },
                {
                    "sent": "Whatever you can imagine to extract as much information as you can, but while remaining tractable.",
                    "label": 0
                },
                {
                    "sent": "So in any case, the in order to deal with noise.",
                    "label": 0
                },
                {
                    "sent": "Graphical models are that's interesting.",
                    "label": 0
                },
                {
                    "sent": "Point to look at probabilistic models and graphical models are one of these techniques and also graphical models have some interesting properties about tractability.",
                    "label": 0
                },
                {
                    "sent": "In some specific cases, in any case.",
                    "label": 0
                },
                {
                    "sent": "So in fact, the questions that we.",
                    "label": 1
                },
                {
                    "sent": "We would like to answer the ideas that we're going to.",
                    "label": 0
                },
                {
                    "sent": "We have data that we're going to model.",
                    "label": 0
                },
                {
                    "sent": "For example, in a probabilistic way, so we have, we assumed it was some generation process of the data, which is some simplified version of reality, and so these models have parameters, so you want to learn those parameters.",
                    "label": 0
                },
                {
                    "sent": "You have to find some way to do that given the data field served.",
                    "label": 0
                },
                {
                    "sent": "You want also to compute some probabilities about specific outcomes.",
                    "label": 0
                },
                {
                    "sent": "What's the probability of something happening?",
                    "label": 0
                },
                {
                    "sent": "And then you want to make predictions.",
                    "label": 0
                },
                {
                    "sent": "So try to generalize like I think it's already introduced before, so we want to generalize on new data points when you don't know the actual value.",
                    "label": 0
                },
                {
                    "sent": "And of course, the central question is if we can do that efficiently.",
                    "label": 0
                },
                {
                    "sent": "So there will be a part of this talk will be about that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now about graphical models.",
                    "label": 1
                },
                {
                    "sent": "This is again like the intersection of of two fields, which is probability theory and graph theory.",
                    "label": 1
                },
                {
                    "sent": "And as I said, they're designed really to deal with in certainty in a principled way, and to have some some decomposition using some structure of the graphical model to reduce the complexity.",
                    "label": 0
                },
                {
                    "sent": "So they are very important in recent years in machine learning and computational statistics, and the ideas that you have.",
                    "label": 1
                },
                {
                    "sent": "Like since you have a lot of data, a lot of dimensions and parameters and and features you might have, you will have like probabilistic model, that I multivariant very big ones typically, and so you will find you are interested in some ways of structuring those probably density functions and decomposing then decomposing them according to some.",
                    "label": 0
                },
                {
                    "sent": "But we call conditional independence assumptions, which we discuss in more detail afterwards.",
                    "label": 0
                },
                {
                    "sent": "So the graph theoretic aspect is is very appealing because it's it's quite an interesting way of analyzing those distributions without reading the actual computations.",
                    "label": 0
                },
                {
                    "sent": "So based on some properties of the graph, you can decompose those.",
                    "label": 0
                },
                {
                    "sent": "You can have some some insights on how the densities is decomposing, so it does give you some really.",
                    "label": 0
                },
                {
                    "sent": "It gives you really some idea of how this distribution is decomposed without having to assume a particular specific parametric form of distribution.",
                    "label": 0
                },
                {
                    "sent": "So it's like general in dependencies conditional in dependencies.",
                    "label": 0
                },
                {
                    "sent": "So that's very convenient if you want to.",
                    "label": 1
                },
                {
                    "sent": "If you have like a lot of information about different subproblems, you can formalize them according to some some.",
                    "label": 0
                },
                {
                    "sent": "Sub graph I would say.",
                    "label": 0
                },
                {
                    "sent": "Some specific structure and then once you want to build like bigger.",
                    "label": 0
                },
                {
                    "sent": "Bigger systems you have, like probability theory to combine all those parts, and that's roughly the probability is like the glue for the whole systems.",
                    "label": 0
                },
                {
                    "sent": "So this system and probability does you also how to incorporate data when you observe data and how you learn to parameters.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So please don't don't hesitate to ask questions if you have any.",
                    "label": 0
                },
                {
                    "sent": "So graphical models are applied in many in many fields, ranging from by informatics.",
                    "label": 1
                },
                {
                    "sent": "I will have an example just afterwards, but it's equally apply to natural language processing document processing, so we're also having an example of that when you have time series, for example speech processing or image processing, computer vision, they're very important where because Markov random fields, for example, are some.",
                    "label": 1
                },
                {
                    "sent": "Some type of graphical models.",
                    "label": 0
                },
                {
                    "sent": "They're very popular in other types of convexity, social Sciences, and so on, so it's something which is widely spread and has lots of applications.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's only a couple of.",
                    "label": 0
                },
                {
                    "sent": "A couple of examples, just to give you an idea of the applications.",
                    "label": 0
                },
                {
                    "sent": "I guess you will be very interested in the application themselves.",
                    "label": 0
                },
                {
                    "sent": "I will spend a pretty not too much time on applications apart from being in the beginning and then we'll go more into the the technical details.",
                    "label": 0
                },
                {
                    "sent": "So you might have like the opposite, but.",
                    "label": 0
                },
                {
                    "sent": "I think it's more useful to do it this way.",
                    "label": 0
                },
                {
                    "sent": "You can always read about application afterwards, so roughly.",
                    "label": 0
                },
                {
                    "sent": "So one way of, for example, organizing large sets of documents, large collection of documents is to use topic models.",
                    "label": 0
                },
                {
                    "sent": "So topic models are roughly are.",
                    "label": 0
                },
                {
                    "sent": "Clustering techniques, so unsupervised learning techniques to identify different themes or topics.",
                    "label": 0
                },
                {
                    "sent": "What you call topics in large sets of large corpora.",
                    "label": 1
                },
                {
                    "sent": "So this is done.",
                    "label": 0
                },
                {
                    "sent": "Roughly.",
                    "label": 0
                },
                {
                    "sent": "This is kind of quite interesting because it's capturing some semantics about sometimes hitting the the other sets of the set of document, which for us is very easy to discover, but which is very difficult from computer to discover.",
                    "label": 0
                },
                {
                    "sent": "So here you have.",
                    "label": 0
                },
                {
                    "sent": "One of the original papers on on topic models like some some history about it before, but this is like the more recent article based model for topic models, which is called the Little Jewish allocation.",
                    "label": 0
                },
                {
                    "sent": "And was proposed in 2003.",
                    "label": 0
                },
                {
                    "sent": "And roughly here you see what the model learns, so you had like a typically.",
                    "label": 0
                },
                {
                    "sent": "Topics are described by a set of vocabulary.",
                    "label": 0
                },
                {
                    "sent": "I mean, in fact.",
                    "label": 0
                },
                {
                    "sent": "In fact, books are correspond to a discrete distribution over vocabulary words.",
                    "label": 0
                },
                {
                    "sent": "So in fact you have like a weight associated to each word in the vocabulary, and then you can re rank them by the probability of this word describing this topic.",
                    "label": 0
                },
                {
                    "sent": "So this is the four list that you see.",
                    "label": 0
                },
                {
                    "sent": "It's like a re rank ordering of those those words according to different types of that were discovered.",
                    "label": 0
                },
                {
                    "sent": "So in general you don't have read the label of the topic, just F. You know that's a big 1, two and three.",
                    "label": 0
                },
                {
                    "sent": "So once you see the list of words, you can say OK.",
                    "label": 0
                },
                {
                    "sent": "This looks like something which has some semantic meaning, and so people kind of labeled it like this artist is.",
                    "label": 0
                },
                {
                    "sent": "This is topic children, education and so on.",
                    "label": 0
                },
                {
                    "sent": "So you see there is some.",
                    "label": 0
                },
                {
                    "sent": "So meaning when you go through the list of the words.",
                    "label": 0
                },
                {
                    "sent": "So if this this this type of model is is based on uh, it's a generative model for text, but it makes very crude assumptions about that text, so it's based on the bag of words assumption, which means that in fact you assume every data point every word is generated independently in the document, so you ignore completely the sequence of words in the document, but to capture the semantics is quite a good model.",
                    "label": 1
                },
                {
                    "sent": "This is this reasonable results.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So that's roughly you have like a piece of text.",
                    "label": 0
                },
                {
                    "sent": "And you see the coloring corresponds to the coloring of the different.",
                    "label": 0
                },
                {
                    "sent": "Topics I'm sorry it's supposed to be in color.",
                    "label": 0
                },
                {
                    "sent": "I noticed I lost the color yesterday evening so.",
                    "label": 0
                },
                {
                    "sent": "So now we said we don't, yeah.",
                    "label": 0
                },
                {
                    "sent": "It's a grayscale.",
                    "label": 0
                },
                {
                    "sent": "We should be able to.",
                    "label": 0
                },
                {
                    "sent": "At least two to find the budget one is quite easy to identify in the text.",
                    "label": 0
                },
                {
                    "sent": "So the roughly the idea when I say it's a bag of course assumption that means that when you have this text, you assume that you are going to the gentleman.",
                    "label": 0
                },
                {
                    "sent": "In fact, you're going to select every time you generate will award.",
                    "label": 0
                },
                {
                    "sent": "You kind of select one topic, and then you draw a word from that from the distribution to generate a word.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another type of application for graphical models are models and bench formatics.",
                    "label": 0
                },
                {
                    "sent": "So there's like plenty.",
                    "label": 0
                },
                {
                    "sent": "Plenty of working in this field.",
                    "label": 0
                },
                {
                    "sent": "Somatic systems biology.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 simple example where you have sequences.",
                    "label": 0
                },
                {
                    "sent": "DNA sequences and you have like you said, you kind of compare different sequences with one another.",
                    "label": 0
                },
                {
                    "sent": "So here on the left you have like two types of raw sequences and the question is.",
                    "label": 0
                },
                {
                    "sent": "Usually in some cases you have like kind of gaps into sequences or so you don't, so they're not like perfectly corresponding to each other, and so you want to align them and you want to find the correspondence between the sequence and being able to see which are the very similar sequences.",
                    "label": 0
                },
                {
                    "sent": "So on the right you have kind of.",
                    "label": 0
                },
                {
                    "sent": "If I state machine reflects on the different states you have, like if this is letter matches it and do you have for the next steps are going to insert a new letter.",
                    "label": 0
                },
                {
                    "sent": "Are you going to delete the letter so that allows you to model sequence?",
                    "label": 0
                },
                {
                    "sent": "And you have some probability of transition between those different States and so typically you want to compare sequences for four in the same family or to understand better evolution in some way.",
                    "label": 0
                },
                {
                    "sent": "Or evolution of different species identify which are the species that are similar.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So third application is image denoising.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "And those typically those are Markov random fields that are used for that, and the idea is that you want to use some.",
                    "label": 0
                },
                {
                    "sent": "Those are graphical models that are defined over neighborhood of, for example pixels.",
                    "label": 0
                },
                {
                    "sent": "Which are the nodes of the graph?",
                    "label": 0
                },
                {
                    "sent": "And so the idea here is you have in the top the top 4 pictures you.",
                    "label": 0
                },
                {
                    "sent": "I mean the left one is the original one and then you have like the second is a is like the noisy picture that you observe that you get.",
                    "label": 0
                },
                {
                    "sent": "So we've never observed the left left side one and so you want to reconstruct that one based on some assumptions which I said it's like neighborhood information.",
                    "label": 0
                },
                {
                    "sent": "So if you use one type of graphical model then you will end up with the third.",
                    "label": 0
                },
                {
                    "sent": "The third picture, again in color.",
                    "label": 0
                },
                {
                    "sent": "It seems less good than in green, white and black and white, but the you can you see that it's a bit more hazy compared to the computer, the right one, so the right one is actually more so high order Markov random fields and more complicated model.",
                    "label": 0
                },
                {
                    "sent": "And it does give better results.",
                    "label": 0
                },
                {
                    "sent": "A closer match to the original picture on the left.",
                    "label": 0
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "This is the 2nd row is a similar type of.",
                    "label": 0
                },
                {
                    "sent": "It's another example I see that here you don't see it very well, but I mean it's the same ordering the left image is their original 1, the one you observed is you observe is the second one, and then you reconstruct the original one in the 2nd and 3rd and the 4th column.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then since I'm from Xerox, I have to talk about printers a little bit, so the result you can even use a graphical models in when you try to manage printer reference infrastructures for example.",
                    "label": 0
                },
                {
                    "sent": "So typical the setting here.",
                    "label": 0
                },
                {
                    "sent": "Is that you were kind of managing like a fleet of printers, which means, like for so, that's for example, a big company, so that might fit.",
                    "label": 0
                },
                {
                    "sent": "That means like up to $100,000.",
                    "label": 0
                },
                {
                    "sent": "Maybe a bit a lot, but the account or or a couple of 100 printers.",
                    "label": 0
                },
                {
                    "sent": "And so you're managing them from from a distance, so you need to know when to match that efficiently.",
                    "label": 0
                },
                {
                    "sent": "You have need to know where the printers located, whether the characters characteristics of every printer.",
                    "label": 0
                },
                {
                    "sent": "You also need to know whether the people using which are the people who are the people using those printers.",
                    "label": 0
                },
                {
                    "sent": "And so you have some some user profile, but then you want to detect automatically if a printer is moved, for example, or if there is that kind of a failure, because softfail detection.",
                    "label": 0
                },
                {
                    "sent": "So in this case for example, you're going to try to identify without going and see what's happening with the printer.",
                    "label": 0
                },
                {
                    "sent": "If that printer is in fact working or not properly, and you can do that based on the usage of those printers by which, which users?",
                    "label": 0
                },
                {
                    "sent": "And again, you can formalize that sort of a time series with some sort of like some users that are interesting, couple of States and devices in some states, or any of the states of the of the state.",
                    "label": 0
                },
                {
                    "sent": "If it's of the printer.",
                    "label": 0
                },
                {
                    "sent": "If it's active or failure, failing, or and so on.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that was just to give you a little.",
                    "label": 0
                },
                {
                    "sent": "Flavor of a couple of object applications of graphical models.",
                    "label": 0
                },
                {
                    "sent": "I think you had lots of examples also on Monday.",
                    "label": 0
                },
                {
                    "sent": "And so this is a bit like other types of ideas for applying our graphical models.",
                    "label": 0
                },
                {
                    "sent": "So I will start.",
                    "label": 0
                },
                {
                    "sent": "This is the overview of.",
                    "label": 0
                },
                {
                    "sent": "Of today's lecture.",
                    "label": 0
                },
                {
                    "sent": "I will.",
                    "label": 0
                },
                {
                    "sent": "I will first start with some basics about graphical models in the introduction.",
                    "label": 0
                },
                {
                    "sent": "And in fact, in the first part that we will, we will probably stop after Markov networks.",
                    "label": 0
                },
                {
                    "sent": "So we will describe these two types of graphic graphical models which are based on networks and Markov random fields.",
                    "label": 1
                },
                {
                    "sent": "We might go a little bit into the exact inference, but then we will in the second part.",
                    "label": 0
                },
                {
                    "sent": "The idea is to talk about the technical leader.",
                    "label": 0
                },
                {
                    "sent": "How do you influence in those graphic amounts or how do you learn probabilities about some hidden nodes?",
                    "label": 0
                },
                {
                    "sent": "And how do you do the learning?",
                    "label": 0
                },
                {
                    "sent": "So how you learn the parameters in graphic amounts?",
                    "label": 0
                },
                {
                    "sent": "And in fact you will have the so the practical sessions will be.",
                    "label": 0
                },
                {
                    "sent": "There will be many.",
                    "label": 0
                },
                {
                    "sent": "One will be an inference and one will be an unlearned in some specific cases.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So some so those are two examples of graphical models left on the right.",
                    "label": 0
                },
                {
                    "sent": "Just to make sure we we talked about the same thing.",
                    "label": 0
                },
                {
                    "sent": "We have nodes that are around that represents from variables when they're empty and like like mu on the left.",
                    "label": 0
                },
                {
                    "sent": "Those are observed variables when they're shaded or the blue.",
                    "label": 1
                },
                {
                    "sent": "Here there will be, we assume to be there soon to be observed.",
                    "label": 0
                },
                {
                    "sent": "And then those nodes are connected in some way.",
                    "label": 1
                },
                {
                    "sent": "So on the left you have directed arrows which denote the conditional conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "And on the right you have, so this is called a base network on the left and on the right.",
                    "label": 0
                },
                {
                    "sent": "This is a undirected graph, which is a Markov network.",
                    "label": 0
                },
                {
                    "sent": "And again, those links corresponds to conditioning dependencies.",
                    "label": 0
                },
                {
                    "sent": "This could correspond to dependencies between the nodes, and in fact it's a bit more space fact when you missing edges that you have conditional independency so.",
                    "label": 0
                },
                {
                    "sent": "The link actually indicates like dependencies, so there's another little technical notation I won't use it a lot, but you will encounter it much more often when you if you continue reading about that this topic, so you have like also plates on the left which did not like repetitions.",
                    "label": 0
                },
                {
                    "sent": "So in fact this is a.",
                    "label": 0
                },
                {
                    "sent": "This is nothing else then.",
                    "label": 0
                },
                {
                    "sent": "The shorthand notation for.",
                    "label": 0
                },
                {
                    "sent": "Short attention for for this.",
                    "label": 0
                },
                {
                    "sent": "But the short end representation rather.",
                    "label": 0
                },
                {
                    "sent": "So there are also.",
                    "label": 0
                },
                {
                    "sent": "We will not talk about combination of graphs, but people have looked at that as well.",
                    "label": 0
                },
                {
                    "sent": "And then those are called chain graphs.",
                    "label": 1
                },
                {
                    "sent": "Just two.",
                    "label": 0
                },
                {
                    "sent": "Put on the so I will talk about an observed latent and hidden variables exchangeably so it's the same thing.",
                    "label": 0
                },
                {
                    "sent": "I'm just using different names for the same type of object.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So really, central central concept in.",
                    "label": 0
                },
                {
                    "sent": "In probability graphical models is conditional conditional independence.",
                    "label": 1
                },
                {
                    "sent": "And this I will just start with a brief description.",
                    "label": 0
                },
                {
                    "sent": "I think you might have touched this bit yesterday.",
                    "label": 0
                },
                {
                    "sent": "But in any case, statistically independent.",
                    "label": 0
                },
                {
                    "sent": "So he said two variable X&Y are independent if the joint probability factorizes.",
                    "label": 0
                },
                {
                    "sent": "So it's just a product of so the joint probability of X&Y is the product of the of X * P of Y.",
                    "label": 0
                },
                {
                    "sent": "And then we say that X is conditionally independent of Y given Z.",
                    "label": 0
                },
                {
                    "sent": "If you have this specific type of factorization, the first line.",
                    "label": 0
                },
                {
                    "sent": "Do you have a like a sticker or something like that?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "OK, no thanks, will use it.",
                    "label": 0
                },
                {
                    "sent": "So typically here.",
                    "label": 0
                },
                {
                    "sent": "When you see the little hand.",
                    "label": 0
                },
                {
                    "sent": "So if you have the joint probability of X&Y given Z, you can decompose it using the product rule of probabilities.",
                    "label": 0
                },
                {
                    "sent": "Which is this?",
                    "label": 0
                },
                {
                    "sent": "This is one way of decomposing it, and so we said if X&Y are condition dependent, then P of X given Y&Z is equal to 2P of X given Z, so it's conditioned benefit.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Of course, this could decompose this probability.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is one of the composite probability you can decompose in the other way around and then you obtain like kind of the same behavior for PFY given XZ is equal to.",
                    "label": 0
                },
                {
                    "sent": "Given that.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So this is really quite important.",
                    "label": 0
                },
                {
                    "sent": "One reason why this is important is that once you you know.",
                    "label": 0
                },
                {
                    "sent": "That some variables are condition dependent.",
                    "label": 0
                },
                {
                    "sent": "Then you can rewrite joint distribution in a more compact way.",
                    "label": 0
                },
                {
                    "sent": "An intuitive way of seeing that is if looking at this equation.",
                    "label": 0
                },
                {
                    "sent": "Well here this isn't.",
                    "label": 0
                },
                {
                    "sent": "This is a problem in this probability at least, probably on the left is a is a probability of three variables, which is then decomposed in the product of two probabilities of two variables and that is one thing that we would like to exploit in the rest of the.",
                    "label": 0
                },
                {
                    "sent": "So I will need a condition dependence by CI, so condition dependencies impose constraint.",
                    "label": 0
                },
                {
                    "sent": "On the model.",
                    "label": 0
                },
                {
                    "sent": "So the the kind of in use the structure.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And the reason why this is the case that some random variables cannot take arbitrary values when you condition other variables, so you don't have like the full space of possibilities, you have only a reduced one.",
                    "label": 1
                },
                {
                    "sent": "I mean, it's quite easy to see that in, for example, in this in the in.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you look at this, this typical example Y&Z determined some values for X, so you have like a much more if why are discrete variables that discrete values if they have the same the same size, the combination of the two is like something.",
                    "label": 0
                },
                {
                    "sent": "Take care values.",
                    "label": 0
                },
                {
                    "sent": "This is going to be squared possibilities for obtaining a value of X, whereas in this case value of Exxon depend on K values.",
                    "label": 0
                },
                {
                    "sent": "That's where you gain.",
                    "label": 0
                },
                {
                    "sent": "Yeah, represent.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so condition dependencies are quite.",
                    "label": 0
                },
                {
                    "sent": "Quite intuitive concept is quite easy to understand if you have some examples, so probably nothing like this.",
                    "label": 0
                },
                {
                    "sent": "My genome is independent of my grandpa of my grandparents unify conditioned on my mother's Geo.",
                    "label": 1
                },
                {
                    "sent": "For my grandmother, my mother's.",
                    "label": 0
                },
                {
                    "sent": "In the images you could.",
                    "label": 0
                },
                {
                    "sent": "You could assume this is maybe less obvious, but you could assume that.",
                    "label": 0
                },
                {
                    "sent": "Color of pixel is independent from colors far away in the image compared to the Direct Line neighbors of that pixels.",
                    "label": 0
                },
                {
                    "sent": "The color of their neighbors of that pixel.",
                    "label": 1
                },
                {
                    "sent": "And you can imagine how many other examples of the similar kind.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the central.",
                    "label": 0
                },
                {
                    "sent": "Topic for today is the is probably graphical models.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "We will have a set of random variables there by XN.",
                    "label": 1
                },
                {
                    "sent": "And the values taken by those rabbits are, and so the random variables are denoted by the capital.",
                    "label": 0
                },
                {
                    "sent": "Extend the valves taken bad news from the barrels are denoted by the small X accents.",
                    "label": 0
                },
                {
                    "sent": "I will use small pee whether their discrete or continuous distributions or densities, because usually everything that that will be described we hold in both cases for continuous and discrete distributions.",
                    "label": 0
                },
                {
                    "sent": "But the idea is that a public company is a graphical model that describes a family of joint distributions.",
                    "label": 1
                },
                {
                    "sent": "And the joint distribution has some structure according to the conditional dependencies or condition.",
                    "label": 0
                },
                {
                    "sent": "Temple independence assumptions, because usually you have like those are assumptions modeling assumptions.",
                    "label": 0
                },
                {
                    "sent": "Which are denoted as as I explained before.",
                    "label": 0
                },
                {
                    "sent": "So as I said, the the the advantage of graphical models that you can read off those.",
                    "label": 0
                },
                {
                    "sent": "At least some of those, most of the condition.",
                    "label": 1
                },
                {
                    "sent": "No.",
                    "label": 1
                },
                {
                    "sent": "Sorry, all condition dependence properties from the graph by manipulations of the graph.",
                    "label": 0
                },
                {
                    "sent": "And this simplified structure makes computation tractable and also storage more efficient.",
                    "label": 0
                },
                {
                    "sent": "So we're addressing one of those key problems I mentioned at the beginning.",
                    "label": 0
                },
                {
                    "sent": "So one way one people, and where people like to view that and that will become more clear afterwards is that you have like a joint distribution of over all the variables you have, like some graphical directed graphical model here, which I have some that has some missing links and this can be viewed as a sort of filtering of the distribution into like a distribution with some structure, some factorized.",
                    "label": 0
                },
                {
                    "sent": "That satisfies some sex factorized conditional independency.",
                    "label": 0
                },
                {
                    "sent": "And that's the mist, the weather.",
                    "label": 0
                },
                {
                    "sent": "A large part of the this stock will be about discovering this condition in dependencies.",
                    "label": 0
                },
                {
                    "sent": "And the reasoning on the graph.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the 1st.",
                    "label": 0
                },
                {
                    "sent": "Typographic amounts are going to.",
                    "label": 0
                },
                {
                    "sent": "To discuss our Bayesian networks.",
                    "label": 1
                },
                {
                    "sent": "So businesses are directed graphs.",
                    "label": 0
                },
                {
                    "sent": "So the errors denote.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dependencies.",
                    "label": 0
                },
                {
                    "sent": "So call it also directed graphical models interchangeably.",
                    "label": 1
                },
                {
                    "sent": "You have people also call them based Nets.",
                    "label": 0
                },
                {
                    "sent": "I mean are different names for the same the same thing here you have like 3 nodes with some conditional dependencies so.",
                    "label": 0
                },
                {
                    "sent": "In this case, so so thereby signatures are described by directed acyclic graphs.",
                    "label": 0
                },
                {
                    "sent": "So that's that means that there are no cycles.",
                    "label": 0
                },
                {
                    "sent": "I mean, there are no directed cycles in the graph.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "And we say that, for example, node A is the parent of naughty.",
                    "label": 1
                },
                {
                    "sent": "And Conversely, note C is a child of, not a let's just like some terminology, and in fact what the arrow denotes kind of indicates is that.",
                    "label": 0
                },
                {
                    "sent": "No, sorry and the one property in Bayesian networks is that the ancestors are so we have a chain chain of of node.",
                    "label": 1
                },
                {
                    "sent": "The ancestor independent of.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the nodes independent ancestors given its parent, which is quite natural concept.",
                    "label": 0
                },
                {
                    "sent": "So if we want to write the joint probability of a B&C.",
                    "label": 0
                },
                {
                    "sent": "Just to choose some ordering.",
                    "label": 0
                },
                {
                    "sent": "Of note and if you choose some ordering and you can decompose so the meaning of the graph is that C is is is conditioned on A&BI mean this gives you a way of decomposing the joint so the two errors coming from NB means that you can say this depends on NBP.",
                    "label": 0
                },
                {
                    "sent": "Depends bid dependent B sorry, dependence A and then you have the marginal over.",
                    "label": 0
                },
                {
                    "sent": "So if you choose if you change the order.",
                    "label": 0
                },
                {
                    "sent": "Of the nodes, then you will obtain like you potentially will attain a different.",
                    "label": 0
                },
                {
                    "sent": "A different composition.",
                    "label": 0
                },
                {
                    "sent": "But not in this case.",
                    "label": 0
                },
                {
                    "sent": "Is that is that is that clear cut?",
                    "label": 1
                },
                {
                    "sent": "This is really important how you decompose, how you go from the graph to the decomposition of the joint.",
                    "label": 1
                },
                {
                    "sent": "So this distribution as I said can be discrete, or they can be continuous.",
                    "label": 0
                },
                {
                    "sent": "I was mainly focused on discrete distribution in this talk, but they can be continuous.",
                    "label": 0
                },
                {
                    "sent": "Typically the only thing you have to do is that you have discrete variables.",
                    "label": 0
                },
                {
                    "sent": "When you have you marginalized, you have to make some to compute sums.",
                    "label": 0
                },
                {
                    "sent": "When you have continuous, you have to compute integrals.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did you, did you see that yes, you did talk about that yesterday or discuss that yesterday, marginalization?",
                    "label": 0
                },
                {
                    "sent": "So organization or so?",
                    "label": 0
                },
                {
                    "sent": "So there's some.",
                    "label": 0
                },
                {
                    "sent": "There's some rule of probabilities.",
                    "label": 0
                },
                {
                    "sent": "And that's roughly tells you that if you have.",
                    "label": 0
                },
                {
                    "sent": "If you have a joint distribution of X&Y, the marginal.",
                    "label": 0
                },
                {
                    "sent": "Here in this case, if it is the margin, otherwise just if it's a joint anyway, you sum over the X and in the case of continuous variables you just replace this by an integral.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So what we have seen here is the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Small example where because of this.",
                    "label": 0
                },
                {
                    "sent": "This dependencies in the graph, the hours in the graph you can you have like you can decompose the joint.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ticularly so if you choose a fixed ordering of the nodes.",
                    "label": 1
                },
                {
                    "sent": "Then you have in certain factory color factorization of the joint distribution.",
                    "label": 0
                },
                {
                    "sent": "And so we typically once you have the graph general graph, you can directly write this factorization down.",
                    "label": 0
                },
                {
                    "sent": "We will denote PF and the parents of X of N. So for a very general joint distribution you can write it as a product of our excavator parts yet.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What does she want?",
                    "label": 0
                },
                {
                    "sent": "That's kind of the summer of probability, so if you so when you say if you typically.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the.",
                    "label": 0
                },
                {
                    "sent": "So you have, like, X&Y?",
                    "label": 0
                },
                {
                    "sent": "You have a discrete, so they have discrete values, so you have like.",
                    "label": 0
                },
                {
                    "sent": "So depending on the combination of YX you will have a different hate here if you want to know what is the probability of of why independent of X.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to summing over all the peaks correspond to every value of X. I don't know if that was I didn't kind of so you have no.",
                    "label": 0
                },
                {
                    "sent": "If that was the right way of saying we have like kind of a peak every.",
                    "label": 0
                },
                {
                    "sent": "So this is just like the sum over X.",
                    "label": 0
                },
                {
                    "sent": "If you want to have a total peak over X / Y.",
                    "label": 0
                },
                {
                    "sent": "It's not a very formal way of explaining things, but hope this makes it clear.",
                    "label": 1
                },
                {
                    "sent": "Yes, so the decomposition depending on during the decomposition is not unique.",
                    "label": 0
                },
                {
                    "sent": "But so you have a product of factors and the distribution is correctly normalized.",
                    "label": 0
                },
                {
                    "sent": "So once you've chosen.",
                    "label": 0
                },
                {
                    "sent": "Like the condition, the condition dependencies you have chosen, like the parametric form of those distributions, those condition distributions, then the product will be always properly normalized for.",
                    "label": 1
                },
                {
                    "sent": "Because of for the probabilities.",
                    "label": 0
                },
                {
                    "sent": "But interesting business you have like local conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "So you're looking at local parts of the graph.",
                    "label": 0
                },
                {
                    "sent": "So the question is whether this factorization is answering the questions.",
                    "label": 0
                },
                {
                    "sent": "The question we were asking in the beginning at least is that has that any use.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of representation and so on.",
                    "label": 0
                },
                {
                    "sent": "And so for this week, and we can look at a simple, simple graphical model, which is a chain.",
                    "label": 0
                },
                {
                    "sent": "And we can also infer the 1st, so you can first look at this directed graphical model.",
                    "label": 1
                },
                {
                    "sent": "This tag you can decompose the joint of exercise 2X and this will decompose in the following way.",
                    "label": 0
                },
                {
                    "sent": "So you have the marginal over X because X does not depend on any other variable.",
                    "label": 0
                },
                {
                    "sent": "Thank you, that's not like the Steelers were like visible vet.",
                    "label": 0
                },
                {
                    "sent": "So the Joint Committee composed so you have X or 4X1 does not depend on any variables you have, like the marginal X one, time times the condition of X2 given X1, which is the 2nd.",
                    "label": 0
                },
                {
                    "sent": "Due to the second error, and then if you have only three nodes and you have like this further decomposition in this way.",
                    "label": 0
                },
                {
                    "sent": "So we assume that all the those variables are discrete and can take K values K different values.",
                    "label": 0
                },
                {
                    "sent": "They all take the same, choosing the same set of values.",
                    "label": 0
                },
                {
                    "sent": "To make it simple.",
                    "label": 0
                },
                {
                    "sent": "And so, in fact, the factorization allows us to exploit some interesting properties that if you do not do the factorization, if you look at the marginal of X2.",
                    "label": 1
                },
                {
                    "sent": "Then you have to sum over all the value of X1 the next week of this joint.",
                    "label": 0
                },
                {
                    "sent": "So in fact here you have like a big if you wanted.",
                    "label": 0
                },
                {
                    "sent": "If there's a discrete values you have, like a big tensor like a 3 dimensional matrix.",
                    "label": 0
                },
                {
                    "sent": "If you want so you have like this is order of of Kate cubed.",
                    "label": 0
                },
                {
                    "sent": "Different.",
                    "label": 0
                },
                {
                    "sent": "Possible values, whereas if you.",
                    "label": 0
                },
                {
                    "sent": "If you look at.",
                    "label": 0
                },
                {
                    "sent": "If you so, if you look at if you decompose it according to this this factorization, then you can isolate the different terms.",
                    "label": 0
                },
                {
                    "sent": "So use distributive loads of product and sum.",
                    "label": 0
                },
                {
                    "sent": "So you can rearrange the sums because there's only depends on those two variables.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's a an error here, should be.",
                    "label": 0
                },
                {
                    "sent": "This would be the probability of X1 anyway.",
                    "label": 0
                },
                {
                    "sent": "And we can look at the prefix on here and CX-2 anyway, so this is this is should be the marginal of X1 in this case.",
                    "label": 0
                },
                {
                    "sent": "So if we ordered determine those two cents.",
                    "label": 0
                },
                {
                    "sent": "And in this case you only have this is of the order of complexity K squared, because in fact you have like every term has like a matrix of times K * K. So so you have like much less options there and then, since you have like 2 two terms, this is like 2 * K ^2.",
                    "label": 0
                },
                {
                    "sent": "So this is not only interesting when you want to do computations, but this is also interesting in terms of parametrization.",
                    "label": 0
                },
                {
                    "sent": "So roughly, this is exactly the same argument.",
                    "label": 0
                },
                {
                    "sent": "So if you have this number now I'm looking at at.",
                    "label": 0
                },
                {
                    "sent": "A sequence of random chain of of length M. So in fact you have like for every node you if you can take values you have like K -- 1.",
                    "label": 0
                },
                {
                    "sent": "Parameters, because because the last one should.",
                    "label": 0
                },
                {
                    "sent": "This should normalize to one with our discrete distribution.",
                    "label": 0
                },
                {
                    "sent": "So the sum should be one.",
                    "label": 0
                },
                {
                    "sent": "So you have like 1 constraint.",
                    "label": 0
                },
                {
                    "sent": "That's why you have this minus one.",
                    "label": 1
                },
                {
                    "sent": "So if you want to represent the joint then you have like K to the bar M -- 1 parameters, whereas if you look at the decomposition, you have K -- 1 parameters for the X of 1 and you have K * K -- 1 for those conditionals and minus one times because it's change of variables.",
                    "label": 1
                },
                {
                    "sent": "So you have more efficient representation.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the question is.",
                    "label": 0
                },
                {
                    "sent": "We have some.",
                    "label": 0
                },
                {
                    "sent": "Ideas about how to model the data?",
                    "label": 0
                },
                {
                    "sent": "So we impose that by some local conditional dependencies, so you have like local conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "Now, if you and those as I said, those are typically used by human experts, so they know some some like it like for example in doctors they know that this disease will not depend on this type of judgment or some other type of.",
                    "label": 0
                },
                {
                    "sent": "So symptom so you have.",
                    "label": 0
                },
                {
                    "sent": "Like some, you can have some conditional dependencies assumptions so.",
                    "label": 0
                },
                {
                    "sent": "So this condition dependencies are usually by the expert they incorporated to the mall to formalize, like for example this specific model.",
                    "label": 1
                },
                {
                    "sent": "But now the question is.",
                    "label": 0
                },
                {
                    "sent": "So what we do, we remove links to impose us in dependencies.",
                    "label": 0
                },
                {
                    "sent": "Now the question is, is removing those links, imposing those conditional independence is equivalent to factorization exactly equivalent, and in another to make it more precise, do we induce additional condition dependencies by imposing some?",
                    "label": 1
                },
                {
                    "sent": "Do we induce others?",
                    "label": 0
                },
                {
                    "sent": "He's a good thing or not.",
                    "label": 0
                },
                {
                    "sent": "In fact there is 1, so we would not.",
                    "label": 0
                },
                {
                    "sent": "We do not talk about D separation in so D for directed graph separation, so it's hard to see how to the set of rules that allows you to read off conditional dependencies in index.",
                    "label": 1
                },
                {
                    "sent": "I'm.",
                    "label": 1
                },
                {
                    "sent": "And the interesting bit about those those disapparation rules is that you.",
                    "label": 0
                },
                {
                    "sent": "You do not require too.",
                    "label": 0
                },
                {
                    "sent": "You don't have to work with the joint distribution directly, they probably directly directly work with the graph and then you transpose that to the joint distribution.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have to introduce a couple of names for the different nodes in the directed graph, so therefore you have like head to tail nodes, tail to tail nodes and head to head nodes.",
                    "label": 0
                },
                {
                    "sent": "So let's start with head to tail nodes and check the independence.",
                    "label": 0
                },
                {
                    "sent": "Condition properties, so here we have again.",
                    "label": 0
                },
                {
                    "sent": "We have a chain of three nodes and the question is so they had to turn out in this graph is grassy, so had today is because there is an arrow coming in and there is a tale of the other error coming out.",
                    "label": 0
                },
                {
                    "sent": "So let's hit tail and so the question is whether A&B are independent in this type of graph.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to use so you have like the product rule of probabilities.",
                    "label": 0
                },
                {
                    "sent": "You also have the same rule, so I mean I already used it, but I I can.",
                    "label": 0
                },
                {
                    "sent": "I can remind me the sum of probabilities, so this is.",
                    "label": 0
                },
                {
                    "sent": "I think this was clear.",
                    "label": 0
                },
                {
                    "sent": "Hopefully this is clear because otherwise.",
                    "label": 0
                },
                {
                    "sent": "You will have it.",
                    "label": 0
                },
                {
                    "sent": "You had some troubles understanding the previous part, so that's the product rule of probabilities.",
                    "label": 0
                },
                {
                    "sent": "And then you also have.",
                    "label": 0
                },
                {
                    "sent": "If you decompose the product rule of probabilities in the other way.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So if you decompose it the other way, you can you have like those.",
                    "label": 0
                },
                {
                    "sent": "Those two terms that can rearrange the terms of the different factors and so you can compute.",
                    "label": 0
                },
                {
                    "sent": "Before I give an X.",
                    "label": 0
                },
                {
                    "sent": "Is equal to P of X given YP of Y. Divide by P of X.",
                    "label": 0
                },
                {
                    "sent": "And I think we've discussed this yesterday.",
                    "label": 0
                },
                {
                    "sent": "That's base rule, so this gives you the posterior probability of Y given X.",
                    "label": 0
                },
                {
                    "sent": "Just give you a way of computing this probability of given X by knowing the prior probability that the likelihood of X given Y and the prior over of Y divided by the marginal effects.",
                    "label": 0
                },
                {
                    "sent": "So those three rules are really key rules.",
                    "label": 0
                },
                {
                    "sent": "They used all the time.",
                    "label": 0
                },
                {
                    "sent": "Is that effectively everything's following for just from those three routes?",
                    "label": 0
                },
                {
                    "sent": "In probability theory, so we can talk quite a lot.",
                    "label": 0
                },
                {
                    "sent": "About just three words.",
                    "label": 0
                },
                {
                    "sent": "So right so?",
                    "label": 0
                },
                {
                    "sent": "So we want to check if the joint fact arises in this way.",
                    "label": 0
                },
                {
                    "sent": "So this is using some.",
                    "label": 0
                },
                {
                    "sent": "There's some rule of probabilities.",
                    "label": 0
                },
                {
                    "sent": "This is equal to this this quantity, so it's the marginal.",
                    "label": 0
                },
                {
                    "sent": "We look again at the joint.",
                    "label": 0
                },
                {
                    "sent": "And then we use the graph to decompose this joint distribution so margin of a * C given eight times the probability of B given C. You can isolate B of A and so you obtain near this.",
                    "label": 0
                },
                {
                    "sent": "In fact, in this joint PBC, given A and this, so where you sum over C, so this is going to give you the probability of B given A and.",
                    "label": 0
                },
                {
                    "sent": "So we see that they are not independent.",
                    "label": 0
                },
                {
                    "sent": "I mean, in general they're not independent.",
                    "label": 0
                },
                {
                    "sent": "There could be some some specific cases, but in general.",
                    "label": 0
                },
                {
                    "sent": "When you have like head to tail nodes and supersedes NB are not independent.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's check for conditional independency.",
                    "label": 0
                },
                {
                    "sent": "So it's against a head to tail example.",
                    "label": 0
                },
                {
                    "sent": "But now we assume that sees observed.",
                    "label": 0
                },
                {
                    "sent": "So we conditioned on C, and so we're going to check again.",
                    "label": 0
                },
                {
                    "sent": "The condition dependency this time, so does this joint probability of AB given C. Decomposing of the property given C times already given C. And it's here that we will be using Bayes rule so typically so we have this joint probability.",
                    "label": 0
                },
                {
                    "sent": "This can be written in this form.",
                    "label": 0
                },
                {
                    "sent": "So that's just applying the rule of the product rule.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "There's just the product rule.",
                    "label": 0
                },
                {
                    "sent": "And divided by the marginal of C. Then again, we're going to decompose the joint as before.",
                    "label": 0
                },
                {
                    "sent": "And since we condition on so C is observed, we can apply here.",
                    "label": 1
                },
                {
                    "sent": "Here we can use this to compute.",
                    "label": 0
                },
                {
                    "sent": "Sorry, we can use this those three terms and apply base rule.",
                    "label": 0
                },
                {
                    "sent": "So we obtained be of a given see.",
                    "label": 0
                },
                {
                    "sent": "So we have this product of those two conditional dependencies and so we do obtain the result that is independent of the given see.",
                    "label": 0
                },
                {
                    "sent": "So this is quite a slight interesting.",
                    "label": 0
                },
                {
                    "sent": "I will come back to that later on.",
                    "label": 0
                },
                {
                    "sent": "But if you pay some attention to this, the transformation from behavior to hear this is roughly giving you the probably of a given see.",
                    "label": 0
                },
                {
                    "sent": "So what you've done is inverted the link in the directed graph.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is this is the again the product of probabilities, so that's.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Sorry this is the product of probabilities, but that exploits conditional independency between.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So here BFC.",
                    "label": 0
                },
                {
                    "sent": "B is independent of of a given its parent.",
                    "label": 0
                },
                {
                    "sent": "Right which you see.",
                    "label": 0
                },
                {
                    "sent": "OK, that's that's the first.",
                    "label": 0
                },
                {
                    "sent": "Question I should have, so that's the first one.",
                    "label": 0
                },
                {
                    "sent": "That's how we construct.",
                    "label": 0
                },
                {
                    "sent": "Those basic networks.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "This is this property that is imported from the beginning.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now the next next.",
                    "label": 0
                },
                {
                    "sent": "The next type of.",
                    "label": 0
                },
                {
                    "sent": "Of note, is there are detailed detailed.",
                    "label": 0
                },
                {
                    "sent": "So we consider different.",
                    "label": 0
                },
                {
                    "sent": "So in this case it's again, so see that is a tail to tail node and so we want to check independence and conditional dependency.",
                    "label": 0
                },
                {
                    "sent": "So independency again we kind of look at is P of does the product party of NB Factorizes in this way?",
                    "label": 0
                },
                {
                    "sent": "And we kind of use the same strategy.",
                    "label": 0
                },
                {
                    "sent": "We start by expanding writing the joint of the tree variables.",
                    "label": 0
                },
                {
                    "sent": "But the marginalized for amortized over C. Then look at the graph and decompose.",
                    "label": 0
                },
                {
                    "sent": "Decompose the graph according to the conditional dependencies.",
                    "label": 0
                },
                {
                    "sent": "So that's directly right off the graph and then.",
                    "label": 0
                },
                {
                    "sent": "I will check and correct that later on.",
                    "label": 0
                },
                {
                    "sent": "I think I've copied like the wrong, modified the wrong.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's just a bit.",
                    "label": 0
                },
                {
                    "sent": "It's like a yeah, there's a bit of issues like I've missed 111 step which is you recombine, so you use the Bayes rule, but you have to.",
                    "label": 0
                },
                {
                    "sent": "Prada who let's the factor PFA go back to the other side.",
                    "label": 0
                },
                {
                    "sent": "So yes, it's like a rewrite, sorry it's like rewriting so you kind of have P of a given C times Pfc, so it's a joint and then you re decompose it in the other way, writing P of a times Pfc evening.",
                    "label": 0
                },
                {
                    "sent": "That's right, so you can isolate a.",
                    "label": 0
                },
                {
                    "sent": "And then you have the you have this.",
                    "label": 0
                },
                {
                    "sent": "This marginally of the joint of B&C, given a which you obtain this type of form, which means that they are not independent.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a similar result as in the head to tail notes, so so we had no independence, but we had condition.",
                    "label": 0
                },
                {
                    "sent": "We have condition independency.",
                    "label": 0
                },
                {
                    "sent": "So we assume now that she is observed and we do the same type of operation.",
                    "label": 0
                },
                {
                    "sent": "So I want to check if this conditional here is going to factorize.",
                    "label": 0
                },
                {
                    "sent": "In this way.",
                    "label": 0
                },
                {
                    "sent": "You write again the joint and divide by the factor T. You decompose the graph as in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "You apply Bayes rule as in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "And you obtain this conditional at this.",
                    "label": 0
                },
                {
                    "sent": "This product of conditionals.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to spend too much time in all the different settings, you can just recheck that by yourself.",
                    "label": 0
                },
                {
                    "sent": "I think it's quite straightforward to do.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Although I did have a problem in the previous slide, but that's because I'm tired.",
                    "label": 0
                },
                {
                    "sent": "So now the third type of node is the head to tail node.",
                    "label": 0
                },
                {
                    "sent": "And here so so here.",
                    "label": 0
                },
                {
                    "sent": "The head to 10 or this note C so you have like 2 incoming errors in.",
                    "label": 0
                },
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "And so again, let's check for the independence.",
                    "label": 1
                },
                {
                    "sent": "So again, the same operation.",
                    "label": 0
                },
                {
                    "sent": "You write a joint, but the using the.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry you can marginalise it if you want.",
                    "label": 0
                },
                {
                    "sent": "You decompose according to the graph, so it's P of the marginal of paid at times the marginal of B times the condition of C given A&B.",
                    "label": 0
                },
                {
                    "sent": "You can let the sun go into the.",
                    "label": 0
                },
                {
                    "sent": "You can, we can ask.",
                    "label": 0
                },
                {
                    "sent": "You can remove in the property of improved from the same.",
                    "label": 0
                },
                {
                    "sent": "And so this sums to one.",
                    "label": 0
                },
                {
                    "sent": "So you obtain the product of NB.",
                    "label": 1
                },
                {
                    "sent": "So you have like independence.",
                    "label": 0
                },
                {
                    "sent": "So this is the opposite.",
                    "label": 0
                },
                {
                    "sent": "Behaviors in the other cases.",
                    "label": 0
                },
                {
                    "sent": "So here we obtain independence.",
                    "label": 1
                },
                {
                    "sent": "But we will.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do not obtain conditional independence.",
                    "label": 0
                },
                {
                    "sent": "Where in the previous types of note we didn't have like independence, but we had conditional independence.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So once we.",
                    "label": 0
                },
                {
                    "sent": "So we have like those rules we have.",
                    "label": 0
                },
                {
                    "sent": "We have those condition dependence properties depending on the type of node.",
                    "label": 0
                },
                {
                    "sent": "So then we also introduce thing we called blocked path.",
                    "label": 0
                },
                {
                    "sent": "And we say that the path is blocked.",
                    "label": 0
                },
                {
                    "sent": "If there are only observed head to tail or tail to tail nodes means that there is conditional independency.",
                    "label": 0
                },
                {
                    "sent": "That means it's blocked or there is like an observed head to head to head node which has none of it descendants that are observed.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Those correspond to the dependencies that were satisfying the trip, so.",
                    "label": 0
                },
                {
                    "sent": "Previous previous slide.",
                    "label": 0
                },
                {
                    "sent": "Tell me that when you have a head to tail or tail to tail, you have condition dependency independency there observed so the path is blocked so the different parts of the graph are independent.",
                    "label": 0
                },
                {
                    "sent": "So for example in this case you have here.",
                    "label": 0
                },
                {
                    "sent": "Here you have like a tail to tail node, which means that the path from A to B is blocked and so a.",
                    "label": 0
                },
                {
                    "sent": "This is a condition independent of B given C and you also have here, here and observed.",
                    "label": 0
                },
                {
                    "sent": "Head to tail node.",
                    "label": 0
                },
                {
                    "sent": "We know with no observed sentence.",
                    "label": 0
                },
                {
                    "sent": "So in fact is condition dependent of be given E as well.",
                    "label": 0
                },
                {
                    "sent": "On the unconscious here you have.",
                    "label": 0
                },
                {
                    "sent": "This is a head to head node which is which is an observed, but it has its end and so in fact in this is this graph is not conditioned condition independent of B given C or is not condition dependent.",
                    "label": 1
                },
                {
                    "sent": "F given given C. Is just applying.",
                    "label": 0
                },
                {
                    "sent": "So this is so here you have.",
                    "label": 0
                },
                {
                    "sent": "This is just applying the previous rule.",
                    "label": 0
                },
                {
                    "sent": "So if you condition an E, although it's an observed head to head node, so we should have independence of A&B.",
                    "label": 0
                },
                {
                    "sent": "Because there is, C is observed, A&B are not independent.",
                    "label": 0
                },
                {
                    "sent": "This is just examples of blocked path.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so to summarize.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So this separation.",
                    "label": 0
                },
                {
                    "sent": "So we have like we said that if you have three subsets.",
                    "label": 0
                },
                {
                    "sent": "No, no intersecting subsets of graph ABC.",
                    "label": 0
                },
                {
                    "sent": "So subset of nodes in the graph we see that subset is conditionally independent of B given subset C. If it's possible.",
                    "label": 0
                },
                {
                    "sent": "If all paths sorry from A to B conditioned on CR blocked.",
                    "label": 1
                },
                {
                    "sent": "And hopefully that means there isolated from each other and so to just to check if every path is blocked, you have to look at all the nodes and check for head to tail, tail to tail and head to head nodes and if you encounter head to tail or tail to tail nodes then those should be observed that there should be in a subset C. If you encounter had two head nodes, then there should be.",
                    "label": 0
                },
                {
                    "sent": "Not observed nor its descendants, so there should not be in C. So that's the that's how you check for D separation.",
                    "label": 0
                },
                {
                    "sent": "Now when you look at so far I've only had like a latent hidden notes and an observed notes.",
                    "label": 0
                },
                {
                    "sent": "If you had like parameters in the model, you can also include them there also and observe if you wanted that could be viewed as unavailable.",
                    "label": 0
                },
                {
                    "sent": "Sorry, but they kind of their observed, so they kind of always at the top of the graphical model, so they always they always tail to tail.",
                    "label": 0
                },
                {
                    "sent": "And so that they don't do not play any role in this operation.",
                    "label": 1
                },
                {
                    "sent": "So you can ignore them.",
                    "label": 0
                },
                {
                    "sent": "Because the deterministic values.",
                    "label": 0
                },
                {
                    "sent": "And so we have a theorem that I will not.",
                    "label": 0
                },
                {
                    "sent": "I will not prove, but roughly what it says is that factorization and conditional independence properties are equivalent.",
                    "label": 0
                },
                {
                    "sent": "And so first factorisation, Blacks condition independency.",
                    "label": 0
                },
                {
                    "sent": "So if you please like a product distribution that Factorizes according to the graph, so you have like this based on network.",
                    "label": 0
                },
                {
                    "sent": "If a B&C are subsets are disjoint subsets, so that means that they have known noting in common.",
                    "label": 1
                },
                {
                    "sent": "If you can check for this separation then then the joint can be decomposed as a according to this conditional conditional independency.",
                    "label": 0
                },
                {
                    "sent": "So is independently given C. So that means that if you have.",
                    "label": 0
                },
                {
                    "sent": "So if you have like you have imposed any type of.",
                    "label": 0
                },
                {
                    "sent": "Condition dependence properties within a graphical model and then you check for the separation.",
                    "label": 0
                },
                {
                    "sent": "Between different other parts of the model you can rewrite the joint in this different way as well.",
                    "label": 0
                },
                {
                    "sent": "Now if you have condition dependency, that also implies factorization.",
                    "label": 0
                },
                {
                    "sent": "And that's kind of.",
                    "label": 0
                },
                {
                    "sent": "Quite natural, and so will be used so far.",
                    "label": 0
                },
                {
                    "sent": "So that means that if pieces by sum sum continue pendency properties.",
                    "label": 1
                },
                {
                    "sent": "Implied by this oppression.",
                    "label": 0
                },
                {
                    "sent": "Then in fact, factorise according to this deck.",
                    "label": 0
                },
                {
                    "sent": "So it can be described by this directed cyclic graph at the joint.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can decompose it.",
                    "label": 0
                },
                {
                    "sent": "You have this factorized form of the joint.",
                    "label": 0
                },
                {
                    "sent": "By the condition of extend given the parents.",
                    "label": 0
                },
                {
                    "sent": "And so we can.",
                    "label": 0
                },
                {
                    "sent": "We would like to know what are the variables.",
                    "label": 0
                },
                {
                    "sent": "If you have like you look at some variable XI, what are all the other variables in the?",
                    "label": 0
                },
                {
                    "sent": "Which other variables of the graph does excite depend on?",
                    "label": 0
                },
                {
                    "sent": "So we do have two.",
                    "label": 0
                },
                {
                    "sent": "So what is the minimal set of variables unit you have to take into account?",
                    "label": 0
                },
                {
                    "sent": "And so the way to check that is you have the joint neutral update this conditional.",
                    "label": 0
                },
                {
                    "sent": "It's against the.",
                    "label": 0
                },
                {
                    "sent": "The joint divided by.",
                    "label": 0
                },
                {
                    "sent": "Divided by this normalizing constant so you have to sum over XI because your condition and all the other variables.",
                    "label": 0
                },
                {
                    "sent": "You see that here.",
                    "label": 0
                },
                {
                    "sent": "All the.",
                    "label": 0
                },
                {
                    "sent": "All distributions were.",
                    "label": 0
                },
                {
                    "sent": "So exactly neither.",
                    "label": 0
                },
                {
                    "sent": "Appear.",
                    "label": 0
                },
                {
                    "sent": "At one time it appeared it appears inside.",
                    "label": 0
                },
                {
                    "sent": "Here it's like the the variable you have like 1 X I-1 distribution where XIXX is equal to XI and then exact can also appear in one of the parents of.",
                    "label": 0
                },
                {
                    "sent": "Sets so.",
                    "label": 0
                },
                {
                    "sent": "When excited, not happy either here or there.",
                    "label": 0
                },
                {
                    "sent": "Then you can actually you can.",
                    "label": 0
                },
                {
                    "sent": "Not this distribution does not depend on XI, so you can just ignore those terms, the simplify.",
                    "label": 0
                },
                {
                    "sent": "You can put them out of the sun.",
                    "label": 0
                },
                {
                    "sent": "And so the remaining factors here on top are.",
                    "label": 0
                },
                {
                    "sent": "Probably have exact given its parents.",
                    "label": 0
                },
                {
                    "sent": "And the probability of all the other nodes where XI appears appears in the.",
                    "label": 0
                },
                {
                    "sent": "In the set of parents of XNI.",
                    "label": 0
                },
                {
                    "sent": "And so typically what does that mean?",
                    "label": 0
                },
                {
                    "sent": "That means that the remaining variables are are those ones.",
                    "label": 0
                },
                {
                    "sent": "So you have the parents of XI.",
                    "label": 0
                },
                {
                    "sent": "So excited when the perfect size and you have like the children and the parents of the children are the remaining ones.",
                    "label": 0
                },
                {
                    "sent": "So that's what is called the Markov blanket of a basic network.",
                    "label": 1
                },
                {
                    "sent": "So this is a minimal set of variables to isolate except from the rest of the graph.",
                    "label": 1
                },
                {
                    "sent": "I mean another way of viewing this.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here, the coloring does not mean that.",
                    "label": 0
                },
                {
                    "sent": "I mean, the fact is that if you assume you ignore for the moment the coloring.",
                    "label": 0
                },
                {
                    "sent": "The reason why, for example, I mean it's quite natural that excite depend the parents of XI are you need to know the perfect condition to escalate this photograph.",
                    "label": 0
                },
                {
                    "sent": "It's quite natural that you have to consider the.",
                    "label": 0
                },
                {
                    "sent": "The children now the reason why you have also depends in the parents of the children is because of.",
                    "label": 0
                },
                {
                    "sent": "If you assume that everything is an observed.",
                    "label": 0
                },
                {
                    "sent": "Then",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have the head to head head, head to head notes and you see that A&B.",
                    "label": 0
                },
                {
                    "sent": "I mean, if sorry if she's observed then India not independent, so that's why you have you have this.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is dependency on the parents of the children?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we do it like in one run until 10 or we have like a Brexiteer.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's worse for the explanation based networks, not let's move on to Markov networks.",
                    "label": 0
                },
                {
                    "sent": "Which are also common in Markov random fields, factory use Markov random fields more often that then Markov networks.",
                    "label": 1
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm not good friends are.",
                    "label": 0
                },
                {
                    "sent": "Undirected graphical models.",
                    "label": 0
                },
                {
                    "sent": "There is.",
                    "label": 0
                },
                {
                    "sent": "And the reason to look at those other type of graphical models is that you might have some conditional independency assumptions that you cannot satisfy in the Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "And in fact vice versa so.",
                    "label": 0
                },
                {
                    "sent": "You cannot represent any factorization.",
                    "label": 0
                },
                {
                    "sent": "That means that you cannot prove any factorization of a joint via Business Network.",
                    "label": 0
                },
                {
                    "sent": "There's some that cannot be represented by, so that's why people start looking at other types of graphical unadaptive graphical models which aren't directed.",
                    "label": 1
                },
                {
                    "sent": "Also, the idea is that we want something which is easier to check then the separation.",
                    "label": 0
                },
                {
                    "sent": "So you want to check from coded conditioning dependencies without having to check all those D separation rules, but just looking at the different hard the graph looks like and you want just which part is independent from the other part of the graph.",
                    "label": 0
                },
                {
                    "sent": "By directly working on simple graph semantics.",
                    "label": 1
                },
                {
                    "sent": "Supergrafx separation.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So again, microphone field is set of probability distribution distributions that is associated to a certain and directed graph.",
                    "label": 1
                },
                {
                    "sent": "So again you can have for the same graph you can have different types of distributions that are associated with this graph.",
                    "label": 0
                },
                {
                    "sent": "At this factorized forms, an here simply here.",
                    "label": 0
                },
                {
                    "sent": "Before you had like, we're looking at directed edges.",
                    "label": 0
                },
                {
                    "sent": "Here just the absence, so you have the condition dependency when you have, like a directed edge dependency.",
                    "label": 0
                },
                {
                    "sent": "Sorry, condition independence.",
                    "label": 0
                },
                {
                    "sent": "If your absence of directed Edge here, you just have a condition dependency.",
                    "label": 1
                },
                {
                    "sent": "When you have an absence of an edge and again the variables can be discrete or random.",
                    "label": 1
                },
                {
                    "sent": "They can be discrete or continuous.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So condition dependency here is checked by simple graph separation, which is much easier to check then then.",
                    "label": 0
                },
                {
                    "sent": "Then this separation.",
                    "label": 0
                },
                {
                    "sent": "So we just look at.",
                    "label": 0
                },
                {
                    "sent": "So if you have.",
                    "label": 0
                },
                {
                    "sent": "A subset of nodes.",
                    "label": 0
                },
                {
                    "sent": "So we have a.",
                    "label": 0
                },
                {
                    "sent": "The subset aids would be a subset C, so we see that we have graph separation between A&B.",
                    "label": 0
                },
                {
                    "sent": "That means that all path going from A to B are going through C and so this just implies that a condition dependent of B given C. So it's very easy to check.",
                    "label": 0
                },
                {
                    "sent": "Don't.",
                    "label": 0
                },
                {
                    "sent": "And so we say that in that case.",
                    "label": 0
                },
                {
                    "sent": "The path is blocked from A to B.",
                    "label": 1
                },
                {
                    "sent": "Also, the the Markov blanket is very easy to do to identify.",
                    "label": 0
                },
                {
                    "sent": "This is just like the old nodes that are surrounding a specific node.",
                    "label": 0
                },
                {
                    "sent": "So typically yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean this is a.",
                    "label": 0
                },
                {
                    "sent": "White people usually use as.",
                    "label": 0
                },
                {
                    "sent": "So this is like a certain directed graph.",
                    "label": 0
                },
                {
                    "sent": "If you want to check all the variables that are.",
                    "label": 0
                },
                {
                    "sent": "How to isolate this note from rest of the graph?",
                    "label": 0
                },
                {
                    "sent": "You just need to condition.",
                    "label": 0
                },
                {
                    "sent": "Unders on on those notes.",
                    "label": 0
                },
                {
                    "sent": "All the organelles connecting.",
                    "label": 0
                },
                {
                    "sent": "Connecting this note.",
                    "label": 0
                },
                {
                    "sent": "Integration points.",
                    "label": 0
                },
                {
                    "sent": "Quantitative variable.",
                    "label": 0
                },
                {
                    "sent": "The first point there.",
                    "label": 0
                },
                {
                    "sent": "Yeah, connecting losing it to be passed.",
                    "label": 0
                },
                {
                    "sent": "Tracy.",
                    "label": 0
                },
                {
                    "sent": "In this graph you can get to be without going crazy.",
                    "label": 0
                },
                {
                    "sent": "Doesn't change the definition.",
                    "label": 0
                },
                {
                    "sent": "I get to keep my going.",
                    "label": 0
                },
                {
                    "sent": "Through the other, she did.",
                    "label": 0
                },
                {
                    "sent": "She did good so she can see that you can get to see from E. Yeah, so you can get to be from.",
                    "label": 0
                },
                {
                    "sent": "Either way please.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "No, that is not allowed.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but sorry.",
                    "label": 0
                },
                {
                    "sent": "See, it's like those sees those is this set of notes right?",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah see is not under this like a set of notes, so it's like we're talking about sets of the same.",
                    "label": 0
                },
                {
                    "sent": "Thing is for.",
                    "label": 0
                },
                {
                    "sent": "In the case of, in the case of directed graph, so we're looking at.",
                    "label": 0
                },
                {
                    "sent": "In the end, we're not looking at nodes, but at.",
                    "label": 0
                },
                {
                    "sent": "Sets of nodes.",
                    "label": 0
                },
                {
                    "sent": "So we were checking, but the separation between sets of nodes and he would just looking at separation between nodes, so graph separation between us.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So an important another important concept in directed graph is clear clicks.",
                    "label": 0
                },
                {
                    "sent": "And so a click is a is a complete sub graph.",
                    "label": 0
                },
                {
                    "sent": "That means that every pair.",
                    "label": 0
                },
                {
                    "sent": "Every pair of note in this subset of nodes, every pair is connected by an edge.",
                    "label": 1
                },
                {
                    "sent": "So we have of course like X.",
                    "label": 0
                },
                {
                    "sent": "One X2 is like one click, you just have one link you cannot.",
                    "label": 0
                },
                {
                    "sent": "And another another click.",
                    "label": 0
                },
                {
                    "sent": "Allergic leak is extreme X4X2.",
                    "label": 0
                },
                {
                    "sent": "So we also say that click is maximal if it's not included in other another click.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Another way of viewing that is that you cannot add an edge to a click without.",
                    "label": 0
                },
                {
                    "sent": "Destroy the click property.",
                    "label": 0
                },
                {
                    "sent": "So typically here.",
                    "label": 1
                },
                {
                    "sent": "If you add to this is a maximal clique.",
                    "label": 1
                },
                {
                    "sent": "If you add X one, then you're missing 1 edge between X1 and X4.",
                    "label": 0
                },
                {
                    "sent": "So it is not a click anymore.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, of course.",
                    "label": 0
                },
                {
                    "sent": "The interesting bit about.",
                    "label": 0
                },
                {
                    "sent": "Undirected graphical models is that you can check for separation very easily.",
                    "label": 1
                },
                {
                    "sent": "You just look at graph separation.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is that.",
                    "label": 0
                },
                {
                    "sent": "You get it.",
                    "label": 0
                },
                {
                    "sent": "You're paying something for that, is that.",
                    "label": 0
                },
                {
                    "sent": "You have you.",
                    "label": 0
                },
                {
                    "sent": "You can decompose a graph as before the product of functions.",
                    "label": 1
                },
                {
                    "sent": "Except that here you need to introduce a normalizing constants instead.",
                    "label": 0
                },
                {
                    "sent": "To do for the distribution to be properly normalized.",
                    "label": 0
                },
                {
                    "sent": "So in fact, when you look at factorization graphical models, you look at all the maximal cliques.",
                    "label": 1
                },
                {
                    "sent": "So we do know the maximum feedback CK.",
                    "label": 0
                },
                {
                    "sent": "And so you have a product of functions PSI, which depends on the notes in CK.",
                    "label": 0
                },
                {
                    "sent": "So that's what keeps side CKX sub C case.",
                    "label": 1
                },
                {
                    "sent": "All the nodes of the graph belonging to CK.",
                    "label": 0
                },
                {
                    "sent": "So I will use from no one and I don't think I've used it before, but.",
                    "label": 1
                },
                {
                    "sent": "I will use bold X to concatenate a set of access when it's appropriate.",
                    "label": 0
                },
                {
                    "sent": "And just for notation.",
                    "label": 0
                },
                {
                    "sent": "So we don't have like local conditional distributions, but here we have like we call that local potential functions.",
                    "label": 0
                },
                {
                    "sent": "So that they only depend on on local set of nodes.",
                    "label": 0
                },
                {
                    "sent": "The only constraint is as is, that's ICK.",
                    "label": 0
                },
                {
                    "sent": "Need to be a positive, not negative.",
                    "label": 0
                },
                {
                    "sent": "In fact, yeah, maybe not negative.",
                    "label": 0
                },
                {
                    "sent": "And this is simply because you want this to be a probability density function, so it has to be positive.",
                    "label": 0
                },
                {
                    "sent": "So as I said in this case, those potential functions not as in the case of Bayes networks, that here they do not need to be conditionals.",
                    "label": 1
                },
                {
                    "sent": "Or or marginals that can be any type of functions as long as they are positive.",
                    "label": 0
                },
                {
                    "sent": "But the price you have to pay that in order to be a proper density, you need to introduce a normalizing constant and normalizing constant is kind of this product sum over all possible values of X of the set of nodes.",
                    "label": 0
                },
                {
                    "sent": "You have written down.",
                    "label": 0
                },
                {
                    "sent": "Write down the like the joint probability and speculation of the clicks, but like what's the underlying principle that tells you that you can do it like this?",
                    "label": 0
                },
                {
                    "sent": "I can show it, why why?",
                    "label": 0
                },
                {
                    "sent": "This follows directly from the separation the graph separation property.",
                    "label": 0
                },
                {
                    "sent": "So that means that the fact that you're you can factorize it means that this that some parts of the graph do not depend on the rest, and they only depend on the local subset of nodes and.",
                    "label": 0
                },
                {
                    "sent": "So probability of Wakeley given all the other things, and then you install these factors or.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's roughly.",
                    "label": 1
                },
                {
                    "sent": "Yeah, that's roughly what's happening here.",
                    "label": 1
                },
                {
                    "sent": "You can, there will be a slide where you can do the mapping between the two, but in fact this is more general because it is not the sites do not need to be probability densities that can be.",
                    "label": 0
                },
                {
                    "sent": "Any type of function as a positive.",
                    "label": 0
                },
                {
                    "sent": "Just so you have more flexibility in that respect, but the price you have to pay is that you have to compute this quantity and the problem with this quantity is that this is.",
                    "label": 0
                },
                {
                    "sent": "This is in general very difficult to compute.",
                    "label": 0
                },
                {
                    "sent": "And so there are a lot of cases where it's a, it's an issue.",
                    "label": 0
                },
                {
                    "sent": "So it's this quantity is sometimes called for partition function.",
                    "label": 0
                },
                {
                    "sent": "That's an indication from physics.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, it's like describing the relations between separation, conditional dependency and factorization.",
                    "label": 0
                },
                {
                    "sent": "And we have the same.",
                    "label": 0
                },
                {
                    "sent": "The same result as in the case of Bayesian networks, meaning that the condition depends property leads to some factorization.",
                    "label": 0
                },
                {
                    "sent": "So if you have a factorization that implies that you have some condition dependency property that is satisfied, and so it is sufficient that A&B.",
                    "label": 0
                },
                {
                    "sent": "So if you like again 3 subsets of nodes, non intersecting subsets of nodes of the graph, then we say that A&B.",
                    "label": 1
                },
                {
                    "sent": "NBR separated by.",
                    "label": 0
                },
                {
                    "sent": "See if if you can check that a separate in separated by.",
                    "label": 0
                },
                {
                    "sent": "In, be a separate necessary.",
                    "label": 0
                },
                {
                    "sent": "Then you can decompose the joint over those subset account.",
                    "label": 0
                },
                {
                    "sent": "This condition dependency criterium.",
                    "label": 0
                },
                {
                    "sent": "And now the opposite is also true, except that you have to impose that.",
                    "label": 0
                },
                {
                    "sent": "That the probability of X is positive for all values back, so you don't have like 0 values of X, which is some restriction.",
                    "label": 0
                },
                {
                    "sent": "This other reasons.",
                    "label": 1
                },
                {
                    "sent": "So the fact that condition dependency implies factorization is known as the Hammersley Clifford theorem, which is a quite important theorem in graph theory.",
                    "label": 0
                },
                {
                    "sent": "I will not prove the theorem, but.",
                    "label": 0
                },
                {
                    "sent": "So you've heard the term.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "Another point is that since.",
                    "label": 1
                },
                {
                    "sent": "The size just to be positive.",
                    "label": 0
                },
                {
                    "sent": "We can also view them at so rewrite them in another form.",
                    "label": 0
                },
                {
                    "sent": "We can say OK, this quantity is equal.",
                    "label": 0
                },
                {
                    "sent": "2 is equal.",
                    "label": 0
                },
                {
                    "sent": "This is what I mean.",
                    "label": 0
                },
                {
                    "sent": "This is proportional to this afternoon was in constant is proportional to the exponential of some function E which if you interested in physics this looks accurate energy function and then those potentials potentials looks like look like Bozeman distribution.",
                    "label": 0
                },
                {
                    "sent": "This is just like a way of.",
                    "label": 0
                },
                {
                    "sent": "People that start to look at graphical models were also interested in physics problems.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So about the differences in the.",
                    "label": 0
                },
                {
                    "sent": "The similarities between Markov random fields and.",
                    "label": 0
                },
                {
                    "sent": "And Bayesian networks.",
                    "label": 0
                },
                {
                    "sent": "So again, just to summarize, some of the things that we've seen so far is that conditional independence properties.",
                    "label": 0
                },
                {
                    "sent": "Can be included in the in the representation of the graph.",
                    "label": 1
                },
                {
                    "sent": "In the structure of the graph and the defined family of joint probability distribution that are structured in some particular way that factorizes in particular way.",
                    "label": 0
                },
                {
                    "sent": "Condition depends.",
                    "label": 0
                },
                {
                    "sent": "It can be checked by a separation in undirected graph or D separations, and those are check between groups of variables, not between variables independently, but just groups of variables.",
                    "label": 0
                },
                {
                    "sent": "So the fact that we have like those local independent structure that means that we can, we can apply characterization of the joint which will lead to some simplification in terms of.",
                    "label": 0
                },
                {
                    "sent": "Farmer translation and from in terms of computations.",
                    "label": 0
                },
                {
                    "sent": "And an important point is that in all those cases you do not need to specify.",
                    "label": 0
                },
                {
                    "sent": "You need to specify the.",
                    "label": 0
                },
                {
                    "sent": "So you do not need to specify the actual form of the function, so all we've done so far is that we kind of kind of reasons on the density to join density decompose density without choosing for specific form of this joint distribution or the factors of this distribution.",
                    "label": 0
                },
                {
                    "sent": "Just we can just conclude if some condition pens properties or factorizations depending on which which approach we take.",
                    "label": 1
                },
                {
                    "sent": "Now the main difference is that the set of densities that are represented by director to undirected Tamils is different.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not selects you can map some of them.",
                    "label": 0
                },
                {
                    "sent": "So you can map based networks to Markov random fields and vice versa.",
                    "label": 0
                },
                {
                    "sent": "But there are some distributions that cannot be captured by based networks and there are some distribution that cannot be captured by microphone field.",
                    "label": 0
                },
                {
                    "sent": "So we need both type of tools.",
                    "label": 0
                },
                {
                    "sent": "To deal with all types of.",
                    "label": 0
                },
                {
                    "sent": "Of distributions, and that's why people also looked at combination of both, because that kind of widened the type of.",
                    "label": 0
                },
                {
                    "sent": "Not with that you can consider.",
                    "label": 0
                },
                {
                    "sent": "The main, so the interesting bit about microphone feeds that you do not need to.",
                    "label": 1
                },
                {
                    "sent": "You can just look at potential which are arbitrary positive functions.",
                    "label": 0
                },
                {
                    "sent": "But what you pay is that you have to compute a normalizing constant.",
                    "label": 0
                },
                {
                    "sent": "Obtain the proper probability density would probably distribution, and that this this constant is coupling all the factors in the graph so that creates dependencies and that makes it more difficult.",
                    "label": 0
                },
                {
                    "sent": "For example, in general in learning.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's to come back to your to your question here.",
                    "label": 0
                },
                {
                    "sent": "So just to illustrate that in some cases you can.",
                    "label": 0
                },
                {
                    "sent": "You can map 1 to the other.",
                    "label": 0
                },
                {
                    "sent": "If you look again at this.",
                    "label": 0
                },
                {
                    "sent": "This chain of random variables.",
                    "label": 0
                },
                {
                    "sent": "So we had this decomposition in terms of the base network, so we have like a marginal of X.",
                    "label": 0
                },
                {
                    "sent": "One times the conditionals of XN plus one given XN.",
                    "label": 0
                },
                {
                    "sent": "Product overall and so you can directly write that down into the form of a Markov random field, where in fact you.",
                    "label": 0
                },
                {
                    "sent": "Every year every function PSI corresponds to the conditional.",
                    "label": 0
                },
                {
                    "sent": "Here the only difference is that this function here you have absorbed X1 into you take this one is equal to the joint X one X2, so you have like someone has observed the conditional plus times the times the marginal.",
                    "label": 0
                },
                {
                    "sent": "Now in this case, when you do this type of mapping then.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "That's not the case in.",
                    "label": 0
                },
                {
                    "sent": "That's not always the case.",
                    "label": 0
                },
                {
                    "sent": "In fact.",
                    "label": 0
                },
                {
                    "sent": "It sounds it's the main complications when you want to map a Bayesian network into a Markov random field is when you have had two head nodes.",
                    "label": 1
                },
                {
                    "sent": "So for example, this is like 1.",
                    "label": 0
                },
                {
                    "sent": "One example where so here X Forza head to head node.",
                    "label": 0
                },
                {
                    "sent": "And again, if you look at the condition dependency property, you see that in fact, when you want to, so typically what people do is they if you want to change a big network tomorrow from field, you just replace the edges by by undirected edges that show you the mapping.",
                    "label": 0
                },
                {
                    "sent": "Then you write the joint, but you have to be careful when you have had two head nodes, because here just replacing those edges by ours is going to be not equivalent.",
                    "label": 0
                },
                {
                    "sent": "In fact, you kind of you have dependencies between X1X2.",
                    "label": 0
                },
                {
                    "sent": "An an X1 and X3 so you have to add edges here between those variables as well and this is called moralization, so that's how you do the mapping between one and two.",
                    "label": 0
                },
                {
                    "sent": "So this is this is also this.",
                    "label": 0
                },
                {
                    "sent": "This follows directly from the head to head property, saying that if you have a head to head.",
                    "label": 0
                },
                {
                    "sent": "Note here, then X one is not independent X2, right?",
                    "label": 0
                },
                {
                    "sent": "That's why we have to at this age.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So good, we have some time to start with the so I wanted to start a little bit about exact inference.",
                    "label": 0
                },
                {
                    "sent": "Check.",
                    "label": 0
                },
                {
                    "sent": "Get to where I want.",
                    "label": 0
                },
                {
                    "sent": "Right, so now we move to the.",
                    "label": 0
                },
                {
                    "sent": "The more interesting bit of the presentation, so before it was more like a descriptive of the different types of graphical models, what they represent.",
                    "label": 0
                },
                {
                    "sent": "Quickly condition dependencies.",
                    "label": 0
                },
                {
                    "sent": "How was the correspondence between the different type of networks are differences and now we're interested in?",
                    "label": 0
                },
                {
                    "sent": "In how to do inference so we will be looking at exact inference, not approximate inference.",
                    "label": 1
                },
                {
                    "sent": "That means that we will compute exact probability distributions over some subsets of nodes.",
                    "label": 0
                },
                {
                    "sent": "So we're interested in computing marginals or computing.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pursue this air condition distributions over some subset.",
                    "label": 0
                },
                {
                    "sent": "So we can one one of the the interest what we're interested in is, like you have observations.",
                    "label": 0
                },
                {
                    "sent": "You have, like some data our observed.",
                    "label": 0
                },
                {
                    "sent": "That means that I have some data.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So I mean, there's some nodes are are clamped to some fixed value.",
                    "label": 1
                },
                {
                    "sent": "So that's quite easy to incorporate into the into the graphical model.",
                    "label": 0
                },
                {
                    "sent": "It seems like in the past how we do that, just like fix the condition on those values.",
                    "label": 0
                },
                {
                    "sent": "And so the goal is to compute the pressure distribution over one or more subsets of those those nodes given the observed ones.",
                    "label": 1
                },
                {
                    "sent": "So that's what we're really interested in.",
                    "label": 0
                },
                {
                    "sent": "So we have to compute marginals.",
                    "label": 0
                },
                {
                    "sent": "So for example, we have X and given all the observed rivals.",
                    "label": 0
                },
                {
                    "sent": "The type of inference algorithm will be looking at our belief propagation, which is a special case of the sum product algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we're looking at.",
                    "label": 0
                },
                {
                    "sent": "Belief propagation, the chain and then move to a more general algorithm to sum product algorithm.",
                    "label": 0
                },
                {
                    "sent": "That said, that's for computing marginal distributions then.",
                    "label": 0
                },
                {
                    "sent": "We will discuss the Max product algorithm.",
                    "label": 0
                },
                {
                    "sent": "And the Maxim algorithm, which is a variant of the same product.",
                    "label": 0
                },
                {
                    "sent": "So instead of marginals you will look at the most likely configuration in your graphic amounts of the most like the value, the configuration, the value of the state that is the most probable.",
                    "label": 1
                },
                {
                    "sent": "I will then briefly, but not too long, so I would just like to have like 1 summary slide on the junk junction tree algorithm, which is a more complicated algorithm that can work on more general class of graphs, but I will not have the time to go into the details of that part.",
                    "label": 0
                },
                {
                    "sent": "Now the real the key.",
                    "label": 0
                },
                {
                    "sent": "The key rule that we're going to apply is a distributive law, which is which.",
                    "label": 0
                },
                {
                    "sent": "You all know which is which is based on this decomposition of a sum of products into a product of.",
                    "label": 0
                },
                {
                    "sent": "Times 8 times.",
                    "label": 0
                },
                {
                    "sent": "So roughly here you have like 3 operation in here we have like 2 operations and so we're going to explore that to make the algorithm more efficient.",
                    "label": 0
                },
                {
                    "sent": "I already illustrated like some concepts about that in the beginning when I was looking at a chain.",
                    "label": 0
                },
                {
                    "sent": "Directed chain so.",
                    "label": 1
                },
                {
                    "sent": "The way those inference algorithms are viewed in general is propagating messages around the graph.",
                    "label": 0
                },
                {
                    "sent": "So we say that nodes are propagating messages between each other, which is roughly information.",
                    "label": 0
                },
                {
                    "sent": "About about the dependencies between the variables.",
                    "label": 0
                },
                {
                    "sent": "And so we will talk about message.",
                    "label": 0
                },
                {
                    "sent": "We are stuck with message passing algorithms.",
                    "label": 0
                },
                {
                    "sent": "So there is another family.",
                    "label": 0
                },
                {
                    "sent": "So of course exact inference is what we would be.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to exact computations.",
                    "label": 0
                },
                {
                    "sent": "That's where we interested in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "Now when you have like very large large graphs that even if you have like an efficient representation exploiting distributive law.",
                    "label": 0
                },
                {
                    "sent": "Not able to.",
                    "label": 0
                },
                {
                    "sent": "Is still too demanding so you have to start looking at approximate inference techniques will not have time to look at that, but one example is variational message message crashing.",
                    "label": 0
                },
                {
                    "sent": "Passing.",
                    "label": 0
                },
                {
                    "sent": "Sorry and there are many other approaches such as and.",
                    "label": 0
                },
                {
                    "sent": "You could also use people sampling or marketing Monte Carlo, but I will not have time to discuss those, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to formalize what I comment I did earlier on that.",
                    "label": 0
                },
                {
                    "sent": "What happens when you apply Bayes rule in a graphical model so?",
                    "label": 0
                },
                {
                    "sent": "These are we stating more clearly what I've written on the blackboard, so roughly based on this is a specific form.",
                    "label": 0
                },
                {
                    "sent": "So typically you have XX as an observation and Y as a latent variable parameter, for example.",
                    "label": 0
                },
                {
                    "sent": "So this is called.",
                    "label": 0
                },
                {
                    "sent": "This quantity is called the likelihood.",
                    "label": 0
                },
                {
                    "sent": "So the likelihood of observing this observation given some parameter Y and so.",
                    "label": 0
                },
                {
                    "sent": "You might have some additional information about the parameter.",
                    "label": 0
                },
                {
                    "sent": "You might know that the property is only needs is for example strictly positive, so you might have some constraints on those parameters and you can encode data.",
                    "label": 0
                },
                {
                    "sent": "One way of encoding those constraints is to consider prior distribution over those parameters.",
                    "label": 0
                },
                {
                    "sent": "And so basically just tells us how to update a prior.",
                    "label": 1
                },
                {
                    "sent": "Believe you have over some value on some variable Y in light of the data.",
                    "label": 0
                },
                {
                    "sent": "Concluded likelihood and you update this prior believe into a posterior belief of given the observation X.",
                    "label": 1
                },
                {
                    "sent": "And there is 1 central.",
                    "label": 0
                },
                {
                    "sent": "There's one important hidden normalizing constant.",
                    "label": 0
                },
                {
                    "sent": "Here is the marginal over of the data, which is also called versus.",
                    "label": 0
                },
                {
                    "sent": "It can be called the partition function, which is also called the evidence in other settings.",
                    "label": 0
                },
                {
                    "sent": "And this is really a key quantity in basin.",
                    "label": 0
                },
                {
                    "sent": "Statistics and basic machine learning.",
                    "label": 0
                },
                {
                    "sent": "So now if we look at this.",
                    "label": 0
                },
                {
                    "sent": "This directed graphical model.",
                    "label": 0
                },
                {
                    "sent": "Simple graphical model.",
                    "label": 0
                },
                {
                    "sent": "So if you want to match this equation to this perfect model of triplets X by Y&X.",
                    "label": 0
                },
                {
                    "sent": "Sorry for that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So here you have like given this graphical model here on the on the left you have, you can write a joint can decompose, I mean in any way you can decompose it given this dependency can decompose it in this way.",
                    "label": 0
                },
                {
                    "sent": "And now if you apply if you apply Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "You see that you have to obtain.",
                    "label": 0
                },
                {
                    "sent": "That the factor going to the other side, you can rewrite it in this way.",
                    "label": 0
                },
                {
                    "sent": "Meaning that you effect effectively you've reverted inverted your of your.",
                    "label": 0
                },
                {
                    "sent": "So effectively what you're doing is what we're interested in.",
                    "label": 0
                },
                {
                    "sent": "This, like reverting the arrows in index.",
                    "label": 0
                },
                {
                    "sent": "At least the ones from the observations to the.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Viable so the 1st.",
                    "label": 0
                },
                {
                    "sent": "Exact inference algorithm.",
                    "label": 0
                },
                {
                    "sent": "Look at belief propagation, so belief propagation is really you have mean.",
                    "label": 0
                },
                {
                    "sent": "The word says what we we have like.",
                    "label": 0
                },
                {
                    "sent": "Some messages going from nodes exchange between different nodes and so in this case this is called a Markov chain.",
                    "label": 1
                },
                {
                    "sent": "I will go a bit, I will discuss a bit more what the market party later on.",
                    "label": 0
                },
                {
                    "sent": "So just think about it as this.",
                    "label": 0
                },
                {
                    "sent": "Chain of very discrete variables.",
                    "label": 0
                },
                {
                    "sent": "Which is represented here in terms of undirected graphical model.",
                    "label": 0
                },
                {
                    "sent": "And the question is if we can compute the marginal probability of XN efficiently.",
                    "label": 1
                },
                {
                    "sent": "So there is a couple of times I said this, and if computation would be of the order.",
                    "label": 0
                },
                {
                    "sent": "If those discrete variables can take key values of your key to the power N, and being the number of nodes in the chain.",
                    "label": 0
                },
                {
                    "sent": "And that can be seen by the fact that you want to if you want to do this marginalization then you isolate the normalizing constant.",
                    "label": 0
                },
                {
                    "sent": "I need you to do the sum over all variables in the chain except X of N times the product of all those factors which depend on which every time depends on on two variables depending on so.",
                    "label": 0
                },
                {
                    "sent": "I didn't quite explain it very well before, but so in fact this factor here depends on expanding so because there is a link between X1X2, so that's how you write this.",
                    "label": 0
                },
                {
                    "sent": "This factors, which is related to your question, I think.",
                    "label": 0
                },
                {
                    "sent": "So this is just like the joint made explicit.",
                    "label": 0
                },
                {
                    "sent": "And you have to sum over all variables except extent.",
                    "label": 0
                },
                {
                    "sent": "In fact, the idea when at least in the changes, is very easy.",
                    "label": 0
                },
                {
                    "sent": "Is that you can.",
                    "label": 0
                },
                {
                    "sent": "Since you do not send over extend, you can you can kind of isolate.",
                    "label": 0
                },
                {
                    "sent": "You can regroup all the factors into two groups, so you can put all the sums for values extend smaller than an on the left and you can also re group all the factors and the sums of very larger than N on the right and so we can rewrite this product here as the product of 2 messages, M, Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "Which are function of XN?",
                    "label": 0
                },
                {
                    "sent": "Anyway to fix it?",
                    "label": 0
                },
                {
                    "sent": "Because you don't come over over extensor extent appears in one of the factors and so by rearranging it like in according to the distributive law, you really have this product of you do the same, then you product transfer that successively from one to the other.",
                    "label": 0
                },
                {
                    "sent": "So what is interesting in this representation is that in fact, if you look at every node you have again, you only have to look at.",
                    "label": 0
                },
                {
                    "sent": "Pairs of nodes and so this is completely K squared and then you have since you have entered the complexities like oh and key squared here.",
                    "label": 1
                },
                {
                    "sent": "So this one little additional detail is that so we called forward and backward messages.",
                    "label": 0
                },
                {
                    "sent": "So if you look at extent, you have the message going from left to right, up to extend and the one from right to left.",
                    "label": 0
                },
                {
                    "sent": "That's pretty straightforward.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we want to compute P of XN, we just need all the incoming messages in XN.",
                    "label": 1
                },
                {
                    "sent": "So in this new kind of change we just need 2 messages.",
                    "label": 1
                },
                {
                    "sent": "And to compute the message on the left, you need all the previous ones in the chain and the computer.",
                    "label": 1
                },
                {
                    "sent": "Once on the right you need all the subsequent 1 interchange.",
                    "label": 0
                },
                {
                    "sent": "So if you're here, you pay attention to the.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the form of this.",
                    "label": 0
                },
                {
                    "sent": "Of the expression of an Alpha male beta, the left and right messages.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can recognize that if you.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at the successive ones.",
                    "label": 0
                },
                {
                    "sent": "You will have just like 1 some.",
                    "label": 0
                },
                {
                    "sent": "So if you look at X. X N -- 1 You will have all those oldest messages on the right, which means that if.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can write.",
                    "label": 0
                },
                {
                    "sent": "The message you can ride the messages in Alpha and beta in the recursive way, so roughly it's like the sum.",
                    "label": 1
                },
                {
                    "sent": "In this case you have like at.",
                    "label": 0
                },
                {
                    "sent": "If you look interested in X and then you have like the factor XN including X 10 -- 1, XN times the message arriving at X and minus one.",
                    "label": 0
                },
                {
                    "sent": "And of course, use two to initialize the recursion.",
                    "label": 0
                },
                {
                    "sent": "You need to have some initial conditions and what people do.",
                    "label": 0
                },
                {
                    "sent": "What you can typically do is that you assume the the message at the very end of the chain is equal to 1, and then so we can start the recursion.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in one specific value, start at the end of the end at the root and the leaf in that both sides of the chain and just you propagate towards this.",
                    "label": 0
                },
                {
                    "sent": "This is the point of interest.",
                    "label": 0
                },
                {
                    "sent": "So you can easily include observed values.",
                    "label": 1
                },
                {
                    "sent": "That means only that the values are fixed to specific.",
                    "label": 0
                },
                {
                    "sent": "This is the variable is fixed with specific value.",
                    "label": 0
                },
                {
                    "sent": "And you what you need only if you have like a chain of nodes, you only need N -- 1 messages to be computer times two, so directions and at every node minus one.",
                    "label": 1
                },
                {
                    "sent": "Because you have this constraint on the the terminal ones.",
                    "label": 0
                },
                {
                    "sent": "What is interesting is that.",
                    "label": 0
                },
                {
                    "sent": "Once you have the merge.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you look at.",
                    "label": 0
                },
                {
                    "sent": "If you look at if you have like a computed P of X and given in terms of those messages, you end up with the product of those two messages that XN normalized by some constant.",
                    "label": 0
                },
                {
                    "sent": "So you can compute.",
                    "label": 0
                },
                {
                    "sent": "In fact, this normalizing constant anywhere in the chain by just summing over the values over extend.",
                    "label": 0
                },
                {
                    "sent": "So if you're in.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And extend, and that's a good way.",
                    "label": 0
                },
                {
                    "sent": "A good location to compute the normalizing constant.",
                    "label": 0
                },
                {
                    "sent": "You can just compute it this way, which is very cheap to compute.",
                    "label": 0
                },
                {
                    "sent": "Much cheaper in any case, then compute looking at the total joint.",
                    "label": 0
                },
                {
                    "sent": "And then finally.",
                    "label": 0
                },
                {
                    "sent": "What is another interesting property is that if you're interested in.",
                    "label": 0
                },
                {
                    "sent": "If the joint of the sort of marginal of two variables to extend of X N -- 1.",
                    "label": 0
                },
                {
                    "sent": "It's quite.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Easy to see that you just look at the messages you just.",
                    "label": 0
                },
                {
                    "sent": "You will have like 2 of variables missing in two variables missing in the same.",
                    "label": 0
                },
                {
                    "sent": "So you will have some overextending extra minus one so we can just propagate to those two variables the messages.",
                    "label": 0
                },
                {
                    "sent": "So just like 1 missing if you want message from X -- 1, two X and so you just have the potential of this.",
                    "label": 0
                },
                {
                    "sent": "Those two variables and the message arriving at those.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those two nodes and so the.",
                    "label": 0
                },
                {
                    "sent": "The marginal of extent extra minus one is just the product of the left message, the right message and the potential linking those two variables.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "2 numbers in concern so I will.",
                    "label": 0
                },
                {
                    "sent": "I will finish after this slide.",
                    "label": 0
                },
                {
                    "sent": "So we can take a.",
                    "label": 0
                },
                {
                    "sent": "Some air breather with so.",
                    "label": 0
                },
                {
                    "sent": "In fact, we have illustrated belief propagation in a in a chain.",
                    "label": 1
                },
                {
                    "sent": "It was quite quite natural.",
                    "label": 0
                },
                {
                    "sent": "I mean quite easy to to formulate in terms of messages, in fact that.",
                    "label": 0
                },
                {
                    "sent": "Propagation is applicable in any type of of tree.",
                    "label": 0
                },
                {
                    "sent": "And would be exactly very similar.",
                    "label": 0
                },
                {
                    "sent": "In fact, the idea would be that you will message will be arriving.",
                    "label": 0
                },
                {
                    "sent": "You will need all the message arriving at a certain node and those messages will capture the information from all the all the variables in the rest of the graph.",
                    "label": 1
                },
                {
                    "sent": "So examples of trees are, so here are three examples of trees.",
                    "label": 0
                },
                {
                    "sent": "That means there's only one path between every every node.",
                    "label": 0
                },
                {
                    "sent": "And so this is like an undirected example.",
                    "label": 0
                },
                {
                    "sent": "Here you have like out of this traditional, I mean more intuitive way of a tree like in terms of a directed graph.",
                    "label": 0
                },
                {
                    "sent": "You also have, like other types of graphs, which are going to Poly trees in directed graphical models.",
                    "label": 0
                },
                {
                    "sent": "Or in this case you have you have multiple sources.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I wasn't specific enough, so in this case we call the route the top node and those are the Leafs.",
                    "label": 0
                },
                {
                    "sent": "In this type of of tree, there's still one path from one or the other, but there are multiple routes, so that's why we call them Poly trees.",
                    "label": 1
                },
                {
                    "sent": "So in the case of an undirected graphical models, you don't just have one path between the different nodes.",
                    "label": 0
                },
                {
                    "sent": "And that's a tree.",
                    "label": 1
                },
                {
                    "sent": "So it isn't so so like in the in the chain.",
                    "label": 0
                },
                {
                    "sent": "If you have a tree, then you directly know where to start the algorithm because you have the leaves and and the roots.",
                    "label": 0
                },
                {
                    "sent": "Are you?",
                    "label": 0
                },
                {
                    "sent": "The idea is that you're going to start at the leaves, propagate all the messages up to the root, and then from the route back to the leaves.",
                    "label": 0
                },
                {
                    "sent": "In a very similar way as in the case of a chain.",
                    "label": 0
                },
                {
                    "sent": "And so there will be.",
                    "label": 0
                },
                {
                    "sent": "That will be the.",
                    "label": 0
                },
                {
                    "sent": "The topic of the the next next part where we'll be looking at how to do belief propagation, or more specifically the sum product algorithm for inference entries.",
                    "label": 0
                },
                {
                    "sent": "Dingus",
                    "label": 0
                }
            ]
        }
    }
}