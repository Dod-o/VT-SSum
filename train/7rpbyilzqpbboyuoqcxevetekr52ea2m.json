{
    "id": "7rpbyilzqpbboyuoqcxevetekr52ea2m",
    "title": "Confidence-Weighted Linear Classification",
    "info": {
        "author": [
            "Fernando C. N. Pereira, Google, Inc."
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_pereira_cwl/",
    "segmentation": [
        [
            "Alright, I'm going talked about some work that was done with Mark Brett sat at coffee Creamer at numerous Pennsylvania, and we've continued when I was ten when we started it and I'm now at Google.",
            "So."
        ],
        [
            "Here's I stuck with some motivation from natural language processing an.",
            "We have typically not interesting problems in Ashland process, have very large datasets and with large number of features.",
            "An many of those features are only weakly correlated with the target label.",
            "And we have also heavy tails.",
            "PETA feature distributions typically host, so this plot shows feature ranks against the account for each feature on the sentiment classification data set, which I'll talk about a bit more later.",
            "So as you can see the plot you have, you know very large number of features were relatively small counts, and yet those features may be and are informative.",
            "Now typically when you train your classifiers for text classification problems, I'll just here I'll only focus on text classification.",
            "These features.",
            "These features are associated with certain words.",
            "So for instance, there might be worth counts and one question arises, which is, should you treat the features that occur rarely.",
            "The words that occur rarely the same way as you teach towards that occur."
        ],
        [
            "Frequently.",
            "So we're going to talk about a particular sentiment classification.",
            "Trying to decide looking at the review like that where the weather.",
            "The reviewer like what they reviewed or not.",
            "You know we can see here that probably they liked it.",
            "They have words like extraordinary.",
            "Best recommended.",
            "Although then there's a word ridiculously that may be a sort of a negative negative indicator, although of course it's ridiculous entertaining, which is sort of one of those kind of contract seemingly contradictory things for a linear classifier.",
            "Way, we're trying to decide whether this is a positive or negative review.",
            "So when we draining linear classifier."
        ],
        [
            "There's four problem like this.",
            "For instance, if I see many positive reviews with word best, I'm going to tend to make the wait for best in my classifier to go up.",
            "Now later on I see a negative review.",
            "It says boring book fast.",
            "If you want to sleep, sleep in seconds.",
            "So wait a minute.",
            "So maybe best is not as indicative as that, so I'm going to tend to reduce both.",
            "Best see some see that review.",
            "It's negative so I'm good.",
            "Use both the weight of best in the weight of boring.",
            "But in fact, best is a very common feature and boring.",
            "The less common feature.",
            "So maybe what I really should do is to reduce the weight of boring last in weighted vest, so I should ideally I might want to have a variable learning rate depending on the frequency with which I've seen each of the features or."
        ],
        [
            "His words in this particular case.",
            "So just I'm going to fix some notation here and which I'll use throughout the talk.",
            "We talking about linear classifiers learned online, so we're going to have the notation.",
            "The relevant notation is we have a weight vector.",
            "We have an instance representative as a vector, which in this case will be typically binary feature vectors.",
            "Occurrence or nonoccurrence of particular words.",
            "For example, and we doing classify by choose by doing the sign of this dot product of this weight vector and this feature vector."
        ],
        [
            "So, so we're going to use online learning.",
            "We get, although some more recent work, which I'll not talk about.",
            "We we have a batch version of the work I'm presenting, we're going to because typically we find that for many application natural language processing, it's online learning is very memory efficient, very simple to implement, an often competitive with batch methods.",
            "And so to review what we do, how such mental operates.",
            "We get an input instance.",
            "We output some prediction and receive some feedback on that, computes some loss, and then use that to update to some prediction rule that we computing.",
            "So we're going around in rounds, getting new instances and updating the prediction rule.",
            "And."
        ],
        [
            "Going to focus on sort of spambayes additive update methods where each which feature feature.",
            "So which feature weight is updated by multiplying some.",
            "So they take the the value of the feature on particular instance X.",
            "We take the target label Y and some learning rate, and then update the weight of that feature and all of the work of the talk is going to be on what.",
            "How do we come up with the learning rate?",
            "So two well known rate schedule are the perception rate schedule, where 80 is always one.",
            "Or it could be some other constants which sometimes people do and the other one which is inspired by this work at the pace of the passive aggressive schemes where the you have a variable learning rate that is just the amount is just enough to correctly classify the current inst.",
            "And that's"
        ],
        [
            "To inspire what follows.",
            "So what we're going to think about is we want to have an idea of confidence in the weights that we are learning.",
            "Weights which we are very confident of.",
            "We want to learn more slowly because we already have a strong confidence in them weights that we are not confident of at all.",
            "We are willing to learn much more rapidly or that much more rapidly.",
            "And to model this, we're going to think in terms of a Gaussian distribution, whether the mean weight vector mu and a covariance Sigma an think into it in sort of conceptually, think of that Gaussian distribution being a distribution over the weight vectors.",
            "So inversion space you can think of these mean weight vector mu.",
            "An example which is separates the weight vectors that correctly classify that that example from the weight vectors that incorrectly classified that example.",
            "And what we're going to try to do is to move to keep this Gaussian distribution.",
            "Such that the the successive examples are correctly classified with high probability with."
        ],
        [
            "After this distribution.",
            "So another way to think about this is think about the market so given.",
            "The margin M which is Y times the dot product of WX.",
            "Think of it as a Gaussian distributed random variable based on that distribution.",
            "And.",
            "So the.",
            "If you look at that Gaussian distribution, then the probability of correct classification is given by what's on the right here, which is 5 is the is a community distribution of the.",
            "The normal distribution and this is this is the margin design margin, and this is the sort of the norm.",
            "The sort of Mahalanobis norm of the example X under discovered this covariance matrix Sigma.",
            "So to kind of visualize."
        ],
        [
            "What's going on here?",
            "Again, in terms of version space, you can imagine the hypothesis a current hypothesis being a Gaussian distribution over weights.",
            "We so we want to place most of the probability on the region that correctly classifieds the example."
        ],
        [
            "Now, if we are already there, we do nothing.",
            "We stay.",
            "It's a passive step because the alternative Gaussian distribution has most of its mass on the correct side of that example.",
            "So."
        ],
        [
            "On the other hand, if our distribution is on the wrong side.",
            "We have to move it to the right side so move the mean.",
            "Anne Anne's at the same time we have to.",
            "Move the variance to be adapted to that new example.",
            "And now I will get no more become more."
        ],
        [
            "Only the minute.",
            "Basically, if you think about the passive aggressive update which finds the new weight vector, W is the minimum of all the closest to the existing weight vector that correctly classifieds the current instance with margin of 1.",
            "Then the new update is going to be.",
            "The minimum is the GNU Gaussian distribution that is closest in KL to.",
            "Subject to the constraint that the probability of correct classification under that distribution is greater than a certain amount, which is essentially confidence parameter.",
            "So you can think of the ETA as being a sort of probabilistic version of a margin.",
            "So we're going to work through this this model and with some simplifications and then examine some."
        ],
        [
            "Experimental results.",
            "So.",
            "The optimization problem that arises from the from that update gives we have to minimize.",
            "That KL divergent switch is expanded out here.",
            "Subject, which is, that's a nice convex optimization problem subject to some constraints, and here complication arises.",
            "So that constraint that the probability of correct classification is at least eight to the confidence parameter can be riveted into this following form.",
            "Which is a.",
            "It's a nice form so it's in terms of this.",
            "Essentially in the standard deviation of the for that particular.",
            "Example, but.",
            "We're 55 is the inverse.",
            "Use the confidence parameter, but there's a problem.",
            "This is not convex.",
            "Um?"
        ],
        [
            "So what we do is in this particular paper there's another way of solving this problem, by the way, which which we developed since then.",
            "But for this current presentation, what we've done is removed.",
            "Basically get linearize a problem.",
            "Getting rid of this square root.",
            "So that's now convex.",
            "So we can solve the problem directly, in fact.",
            "So this is solving this directly.",
            "We call the exact variance method.",
            "Now in many cases for natural language we actually want to restrict do this restricted to diagonal covariance matrices.",
            "Now you can do that directly.",
            "You can optimize this directly, constraining Sigma to be diagonal, but there is a sort of approximate solution which is has a close form arrangement which I'll describe."
        ],
        [
            "Now this seems quite complicated, but I will so but.",
            "The main points here are I take.",
            "So we're going to do is solve the original problem, not with the diagonal covariance, and then project onto diagonal.",
            "Now the algebra is pretty is a bit complicated.",
            "It's in the paper, but basically so we start with initial.",
            "Mean vector, which is zero and an initial covariance, which is the identity scale by some factor A.",
            "And the fact is actually not very important than that can indicate to be set in the held out data.",
            "And then we compute the margin, the sign margin from the example on this and the current instance, then sort of scaled by the current covariance and then compute the quantity which is, which essentially corresponds to a learning rate.",
            "And notice that this is a Max between the quantity and zero and that means that basically in some cases if this Alpha I is 0 the update, the update is 0, meaning that you stay the same, the same hypothesis.",
            "That's the passive case.",
            "If Alpha I is not zero, then you what's going to happen is we're going to update mu and this is with variable learning rate.",
            "Where Alpha eyes, part of the variable learning rate and the other part is given by Sigma.",
            "I so now if Sigma is diagonal basically what's going to happen is that the component of Sigma corresponding to a particular feature in the diagonal of Sigma is going to be.",
            "Added to muai by amounted scaled by that value of Sigma.",
            "So now if you look at the update for the inverse covariance here an remember.",
            "Now we thinking about the diagonal inverse covariance.",
            "So this is basically these are the inverses of the elements in the diagonal.",
            "What you see is that the update to death inverse covariance is basically a scaled count of the features.",
            "So if the excise binary so diaga XI for the particular feature is the weather feature is on or not so multiplied by Alpha I and FI you basically get is the inverse of your covariance is going to grow.",
            "Why the observation of that feature?",
            "So the features that I observe a lot.",
            "That's the inverse Sigma I4 diagonal element for that feature is going to be large, meaning that the corresponding variance element is going to be very small, corresponding its learning rate is going to be very small.",
            "For features I see is rarely the opposite occurs.",
            "So the, uh, the diagonal element in the sort of the variance element is going to be very hot, so that you have a much higher learning rate."
        ],
        [
            "To visualize this.",
            "Give you so, so this is the synthetic data example where we have 20 features of which only two are informative.",
            "The other ones are noise.",
            "And these are after 30 rounds through 30 instances and on the left is the full covariance case, which actually will not discuss in all actually experiments.",
            "So what you see here is the blue circles represent the... are.",
            "Several pairs of noise features.",
            "These are the sort of the their covariance.",
            "Actually, I'm scale by 20, so that's 20 times.",
            "Covariance... and you see that these are all kind of around 0, but with a lot of noise.",
            "The Green Point is our target hypothesis, mean covariance and the block... which is very elongated there already is.",
            "The which corresponds to the covariance times scale by 20 of the two informative features.",
            "So you see that bills are getting very close to the intended separator.",
            "Now on the diagonal model, and this is a different scale on the axis.",
            "Unfortunately what you see is that the... are still much larger and they have to be axis aligned because it's the diagonal case that's our target and there's a little black... that you can't quite see there.",
            "Unfortunately, because it's behind that green circle, but that corresponds to the... that shows you the learning rate for the two important.",
            "The two significant features so you see that for the noise features I just I am willing to change their weight very rapidly because I don't know anything about them for the non noise features I'm not."
        ],
        [
            "So we have here at the same after 90 rounds and again this scaling issues.",
            "This is just this is in the diagonal case.",
            "Again I'm very close to what to our target here.",
            "I'm also very close.",
            "I cannot.",
            "You cannot see the.",
            "You can barely see the black... there in the middle and these still their covariances.",
            "I'm it's smaller, but they're still around all around zero as you expect.",
            "So these pictures show that the fact that we want is operating, so the learning rates are much larger for the features that Porch will have less information than the features that we have a lot more information, and this is not just the number of accounts, but also whether the feature that corrected are classifying correctly or not.",
            "So featured offers a lot but is not properly does not give much information about the label.",
            "Also is going to have a much."
        ],
        [
            "Faster learning rate.",
            "So now we're going to go over some experiments.",
            "And so these are online to batch experiments, so we doing multiple passes or over the training data and compute error accuracy.",
            "There's also more information about community.",
            "Sort of the.",
            "Online learning rates in the paper and these are binary problems from newsgroups Reuters.",
            "The sentiment classification.",
            "So we have some sentiment data that's described in the paper.",
            "Some three divisions.",
            "Binary divisions of."
        ],
        [
            "Write Reuters with bag of words representation.",
            "Binary features an three divisions of news."
        ],
        [
            "Newsgroups.",
            "So here's the sort of the snippet.",
            "Typical behavior of this method.",
            "We have number of iterations passes over the data.",
            "This accuracy in the Y axis and they have three methods here.",
            "One is passive aggressive method.",
            "The green and blue are variance exactly is the method that I didn't describe in detail.",
            "We are optimally maintain a diagonal covariance, but we optimize that directly and the blue one is the variance approximate method I describe in more detail where we project onto the diagonal matrices.",
            "For reasons that we don't fully understand, these approximate method performs consistently better.",
            "The the other thing to note, which is important for both methods is that after one iteration, one pass through the data they already doing competitively with passive aggressive after five iterations.",
            "So basically they converge in one path to the date.",
            "There is not much changed as the data continue.",
            "This is 1 just one snapshot.",
            "There are many other snapshots we have in the paper and many others that we have done that we have not didn't have room to put there.",
            "Select me move onto.",
            "Understand."
        ],
        [
            "Why is this?",
            "Stop."
        ],
        [
            "Sorry."
        ],
        [
            "Some some there's a lot of statistics in the paper.",
            "I'll just highlight the following, so the variance method that we choose, the one that I described in most detail on 20 newsgroups, Reuters and sentiment, and it almost always performs better than a variety of batch methods on the same data and all the hyper parameters have been set here by using held out data support vector machines with the same features, maximum entropy and stochastic gradient descent on the.",
            "Huber loss and you see that consistently this method performs better and we believe the reason for that, although we don't have full analysis of that yet, is that we taking into account the.",
            "Frequencies of different word features.",
            "And most of these results are significant to some level."
        ],
        [
            "Now this sort of just where there's one other thing we try to do explore this method is the idea.",
            "If you have a very large training set.",
            "If you and you split it over many training runs over different machines, can we use this idea is to combine those runs.",
            "So one thing we could do is to.",
            "Do a sort of uniform combination of weight vectors of different classifiers.",
            "The other thing we can do is to try to find if we use this math learning method is to try to find the closest entailed by the closest hypothesis entail divergance to the one that we do, all the ones we trained on the individual classifiers.",
            "And if you do that, there is actually you can compute means and the inverse covariance is very directly."
        ],
        [
            "And just to give you a quick.",
            "View of that we see that the average of each of the classifiers.",
            "If you use the classifiers trained independently, is a red bar.",
            "The uniform mixing of the classifiers is the blue bar in the black bar is the one done using this method, and the black is the variance method.",
            "Training on the entire data and dashes.",
            "Passive aggressive training on the entire data.",
            "So we see that we can do better than passive aggressive even though we did trade in parallel.",
            "And in that is consistent across 4 if you if you only if each of the parallel classifiers is enough data.",
            "Now if they have seen much less data then of course they will perform."
        ],
        [
            "As well.",
            "Out just to finalize an interest of time.",
            "So LP has very skewed feature distributions and we tried here to represent explicitly how much do we know about each feature.",
            "An homage to trust each feature weight.",
            "This achieves fairly high accuracy, better than even some previous batch methods, very fast convergence, and ineffective way of doing model combination.",
            "The current work we have a direct inverse version of the original optimization, which is work since we submitted this paper.",
            "Also a batch algorithm that we are investigating and we're doing some explorations in using full covariance or at least larger covariance that then just diagonal to cap capture correlations between features.",
            "That's all, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, I'm going talked about some work that was done with Mark Brett sat at coffee Creamer at numerous Pennsylvania, and we've continued when I was ten when we started it and I'm now at Google.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's I stuck with some motivation from natural language processing an.",
                    "label": 1
                },
                {
                    "sent": "We have typically not interesting problems in Ashland process, have very large datasets and with large number of features.",
                    "label": 0
                },
                {
                    "sent": "An many of those features are only weakly correlated with the target label.",
                    "label": 1
                },
                {
                    "sent": "And we have also heavy tails.",
                    "label": 0
                },
                {
                    "sent": "PETA feature distributions typically host, so this plot shows feature ranks against the account for each feature on the sentiment classification data set, which I'll talk about a bit more later.",
                    "label": 0
                },
                {
                    "sent": "So as you can see the plot you have, you know very large number of features were relatively small counts, and yet those features may be and are informative.",
                    "label": 0
                },
                {
                    "sent": "Now typically when you train your classifiers for text classification problems, I'll just here I'll only focus on text classification.",
                    "label": 0
                },
                {
                    "sent": "These features.",
                    "label": 1
                },
                {
                    "sent": "These features are associated with certain words.",
                    "label": 0
                },
                {
                    "sent": "So for instance, there might be worth counts and one question arises, which is, should you treat the features that occur rarely.",
                    "label": 0
                },
                {
                    "sent": "The words that occur rarely the same way as you teach towards that occur.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Frequently.",
                    "label": 0
                },
                {
                    "sent": "So we're going to talk about a particular sentiment classification.",
                    "label": 0
                },
                {
                    "sent": "Trying to decide looking at the review like that where the weather.",
                    "label": 0
                },
                {
                    "sent": "The reviewer like what they reviewed or not.",
                    "label": 0
                },
                {
                    "sent": "You know we can see here that probably they liked it.",
                    "label": 0
                },
                {
                    "sent": "They have words like extraordinary.",
                    "label": 0
                },
                {
                    "sent": "Best recommended.",
                    "label": 0
                },
                {
                    "sent": "Although then there's a word ridiculously that may be a sort of a negative negative indicator, although of course it's ridiculous entertaining, which is sort of one of those kind of contract seemingly contradictory things for a linear classifier.",
                    "label": 0
                },
                {
                    "sent": "Way, we're trying to decide whether this is a positive or negative review.",
                    "label": 0
                },
                {
                    "sent": "So when we draining linear classifier.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's four problem like this.",
                    "label": 0
                },
                {
                    "sent": "For instance, if I see many positive reviews with word best, I'm going to tend to make the wait for best in my classifier to go up.",
                    "label": 0
                },
                {
                    "sent": "Now later on I see a negative review.",
                    "label": 0
                },
                {
                    "sent": "It says boring book fast.",
                    "label": 0
                },
                {
                    "sent": "If you want to sleep, sleep in seconds.",
                    "label": 1
                },
                {
                    "sent": "So wait a minute.",
                    "label": 0
                },
                {
                    "sent": "So maybe best is not as indicative as that, so I'm going to tend to reduce both.",
                    "label": 0
                },
                {
                    "sent": "Best see some see that review.",
                    "label": 0
                },
                {
                    "sent": "It's negative so I'm good.",
                    "label": 0
                },
                {
                    "sent": "Use both the weight of best in the weight of boring.",
                    "label": 0
                },
                {
                    "sent": "But in fact, best is a very common feature and boring.",
                    "label": 0
                },
                {
                    "sent": "The less common feature.",
                    "label": 0
                },
                {
                    "sent": "So maybe what I really should do is to reduce the weight of boring last in weighted vest, so I should ideally I might want to have a variable learning rate depending on the frequency with which I've seen each of the features or.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "His words in this particular case.",
                    "label": 0
                },
                {
                    "sent": "So just I'm going to fix some notation here and which I'll use throughout the talk.",
                    "label": 0
                },
                {
                    "sent": "We talking about linear classifiers learned online, so we're going to have the notation.",
                    "label": 1
                },
                {
                    "sent": "The relevant notation is we have a weight vector.",
                    "label": 1
                },
                {
                    "sent": "We have an instance representative as a vector, which in this case will be typically binary feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Occurrence or nonoccurrence of particular words.",
                    "label": 0
                },
                {
                    "sent": "For example, and we doing classify by choose by doing the sign of this dot product of this weight vector and this feature vector.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so we're going to use online learning.",
                    "label": 1
                },
                {
                    "sent": "We get, although some more recent work, which I'll not talk about.",
                    "label": 0
                },
                {
                    "sent": "We we have a batch version of the work I'm presenting, we're going to because typically we find that for many application natural language processing, it's online learning is very memory efficient, very simple to implement, an often competitive with batch methods.",
                    "label": 0
                },
                {
                    "sent": "And so to review what we do, how such mental operates.",
                    "label": 0
                },
                {
                    "sent": "We get an input instance.",
                    "label": 1
                },
                {
                    "sent": "We output some prediction and receive some feedback on that, computes some loss, and then use that to update to some prediction rule that we computing.",
                    "label": 1
                },
                {
                    "sent": "So we're going around in rounds, getting new instances and updating the prediction rule.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to focus on sort of spambayes additive update methods where each which feature feature.",
                    "label": 0
                },
                {
                    "sent": "So which feature weight is updated by multiplying some.",
                    "label": 0
                },
                {
                    "sent": "So they take the the value of the feature on particular instance X.",
                    "label": 1
                },
                {
                    "sent": "We take the target label Y and some learning rate, and then update the weight of that feature and all of the work of the talk is going to be on what.",
                    "label": 1
                },
                {
                    "sent": "How do we come up with the learning rate?",
                    "label": 0
                },
                {
                    "sent": "So two well known rate schedule are the perception rate schedule, where 80 is always one.",
                    "label": 0
                },
                {
                    "sent": "Or it could be some other constants which sometimes people do and the other one which is inspired by this work at the pace of the passive aggressive schemes where the you have a variable learning rate that is just the amount is just enough to correctly classify the current inst.",
                    "label": 0
                },
                {
                    "sent": "And that's",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To inspire what follows.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to think about is we want to have an idea of confidence in the weights that we are learning.",
                    "label": 0
                },
                {
                    "sent": "Weights which we are very confident of.",
                    "label": 0
                },
                {
                    "sent": "We want to learn more slowly because we already have a strong confidence in them weights that we are not confident of at all.",
                    "label": 0
                },
                {
                    "sent": "We are willing to learn much more rapidly or that much more rapidly.",
                    "label": 0
                },
                {
                    "sent": "And to model this, we're going to think in terms of a Gaussian distribution, whether the mean weight vector mu and a covariance Sigma an think into it in sort of conceptually, think of that Gaussian distribution being a distribution over the weight vectors.",
                    "label": 0
                },
                {
                    "sent": "So inversion space you can think of these mean weight vector mu.",
                    "label": 0
                },
                {
                    "sent": "An example which is separates the weight vectors that correctly classify that that example from the weight vectors that incorrectly classified that example.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to try to do is to move to keep this Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Such that the the successive examples are correctly classified with high probability with.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After this distribution.",
                    "label": 0
                },
                {
                    "sent": "So another way to think about this is think about the market so given.",
                    "label": 0
                },
                {
                    "sent": "The margin M which is Y times the dot product of WX.",
                    "label": 0
                },
                {
                    "sent": "Think of it as a Gaussian distributed random variable based on that distribution.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "If you look at that Gaussian distribution, then the probability of correct classification is given by what's on the right here, which is 5 is the is a community distribution of the.",
                    "label": 0
                },
                {
                    "sent": "The normal distribution and this is this is the margin design margin, and this is the sort of the norm.",
                    "label": 0
                },
                {
                    "sent": "The sort of Mahalanobis norm of the example X under discovered this covariance matrix Sigma.",
                    "label": 0
                },
                {
                    "sent": "So to kind of visualize.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's going on here?",
                    "label": 0
                },
                {
                    "sent": "Again, in terms of version space, you can imagine the hypothesis a current hypothesis being a Gaussian distribution over weights.",
                    "label": 0
                },
                {
                    "sent": "We so we want to place most of the probability on the region that correctly classifieds the example.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, if we are already there, we do nothing.",
                    "label": 0
                },
                {
                    "sent": "We stay.",
                    "label": 0
                },
                {
                    "sent": "It's a passive step because the alternative Gaussian distribution has most of its mass on the correct side of that example.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the other hand, if our distribution is on the wrong side.",
                    "label": 0
                },
                {
                    "sent": "We have to move it to the right side so move the mean.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne's at the same time we have to.",
                    "label": 0
                },
                {
                    "sent": "Move the variance to be adapted to that new example.",
                    "label": 1
                },
                {
                    "sent": "And now I will get no more become more.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Only the minute.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you think about the passive aggressive update which finds the new weight vector, W is the minimum of all the closest to the existing weight vector that correctly classifieds the current instance with margin of 1.",
                    "label": 0
                },
                {
                    "sent": "Then the new update is going to be.",
                    "label": 1
                },
                {
                    "sent": "The minimum is the GNU Gaussian distribution that is closest in KL to.",
                    "label": 0
                },
                {
                    "sent": "Subject to the constraint that the probability of correct classification under that distribution is greater than a certain amount, which is essentially confidence parameter.",
                    "label": 0
                },
                {
                    "sent": "So you can think of the ETA as being a sort of probabilistic version of a margin.",
                    "label": 0
                },
                {
                    "sent": "So we're going to work through this this model and with some simplifications and then examine some.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experimental results.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The optimization problem that arises from the from that update gives we have to minimize.",
                    "label": 1
                },
                {
                    "sent": "That KL divergent switch is expanded out here.",
                    "label": 0
                },
                {
                    "sent": "Subject, which is, that's a nice convex optimization problem subject to some constraints, and here complication arises.",
                    "label": 0
                },
                {
                    "sent": "So that constraint that the probability of correct classification is at least eight to the confidence parameter can be riveted into this following form.",
                    "label": 0
                },
                {
                    "sent": "Which is a.",
                    "label": 0
                },
                {
                    "sent": "It's a nice form so it's in terms of this.",
                    "label": 0
                },
                {
                    "sent": "Essentially in the standard deviation of the for that particular.",
                    "label": 0
                },
                {
                    "sent": "Example, but.",
                    "label": 0
                },
                {
                    "sent": "We're 55 is the inverse.",
                    "label": 0
                },
                {
                    "sent": "Use the confidence parameter, but there's a problem.",
                    "label": 0
                },
                {
                    "sent": "This is not convex.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we do is in this particular paper there's another way of solving this problem, by the way, which which we developed since then.",
                    "label": 0
                },
                {
                    "sent": "But for this current presentation, what we've done is removed.",
                    "label": 0
                },
                {
                    "sent": "Basically get linearize a problem.",
                    "label": 0
                },
                {
                    "sent": "Getting rid of this square root.",
                    "label": 0
                },
                {
                    "sent": "So that's now convex.",
                    "label": 0
                },
                {
                    "sent": "So we can solve the problem directly, in fact.",
                    "label": 0
                },
                {
                    "sent": "So this is solving this directly.",
                    "label": 0
                },
                {
                    "sent": "We call the exact variance method.",
                    "label": 1
                },
                {
                    "sent": "Now in many cases for natural language we actually want to restrict do this restricted to diagonal covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "Now you can do that directly.",
                    "label": 0
                },
                {
                    "sent": "You can optimize this directly, constraining Sigma to be diagonal, but there is a sort of approximate solution which is has a close form arrangement which I'll describe.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this seems quite complicated, but I will so but.",
                    "label": 0
                },
                {
                    "sent": "The main points here are I take.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do is solve the original problem, not with the diagonal covariance, and then project onto diagonal.",
                    "label": 0
                },
                {
                    "sent": "Now the algebra is pretty is a bit complicated.",
                    "label": 0
                },
                {
                    "sent": "It's in the paper, but basically so we start with initial.",
                    "label": 0
                },
                {
                    "sent": "Mean vector, which is zero and an initial covariance, which is the identity scale by some factor A.",
                    "label": 0
                },
                {
                    "sent": "And the fact is actually not very important than that can indicate to be set in the held out data.",
                    "label": 0
                },
                {
                    "sent": "And then we compute the margin, the sign margin from the example on this and the current instance, then sort of scaled by the current covariance and then compute the quantity which is, which essentially corresponds to a learning rate.",
                    "label": 0
                },
                {
                    "sent": "And notice that this is a Max between the quantity and zero and that means that basically in some cases if this Alpha I is 0 the update, the update is 0, meaning that you stay the same, the same hypothesis.",
                    "label": 0
                },
                {
                    "sent": "That's the passive case.",
                    "label": 0
                },
                {
                    "sent": "If Alpha I is not zero, then you what's going to happen is we're going to update mu and this is with variable learning rate.",
                    "label": 0
                },
                {
                    "sent": "Where Alpha eyes, part of the variable learning rate and the other part is given by Sigma.",
                    "label": 1
                },
                {
                    "sent": "I so now if Sigma is diagonal basically what's going to happen is that the component of Sigma corresponding to a particular feature in the diagonal of Sigma is going to be.",
                    "label": 0
                },
                {
                    "sent": "Added to muai by amounted scaled by that value of Sigma.",
                    "label": 0
                },
                {
                    "sent": "So now if you look at the update for the inverse covariance here an remember.",
                    "label": 1
                },
                {
                    "sent": "Now we thinking about the diagonal inverse covariance.",
                    "label": 0
                },
                {
                    "sent": "So this is basically these are the inverses of the elements in the diagonal.",
                    "label": 0
                },
                {
                    "sent": "What you see is that the update to death inverse covariance is basically a scaled count of the features.",
                    "label": 0
                },
                {
                    "sent": "So if the excise binary so diaga XI for the particular feature is the weather feature is on or not so multiplied by Alpha I and FI you basically get is the inverse of your covariance is going to grow.",
                    "label": 0
                },
                {
                    "sent": "Why the observation of that feature?",
                    "label": 0
                },
                {
                    "sent": "So the features that I observe a lot.",
                    "label": 0
                },
                {
                    "sent": "That's the inverse Sigma I4 diagonal element for that feature is going to be large, meaning that the corresponding variance element is going to be very small, corresponding its learning rate is going to be very small.",
                    "label": 0
                },
                {
                    "sent": "For features I see is rarely the opposite occurs.",
                    "label": 0
                },
                {
                    "sent": "So the, uh, the diagonal element in the sort of the variance element is going to be very hot, so that you have a much higher learning rate.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To visualize this.",
                    "label": 0
                },
                {
                    "sent": "Give you so, so this is the synthetic data example where we have 20 features of which only two are informative.",
                    "label": 0
                },
                {
                    "sent": "The other ones are noise.",
                    "label": 0
                },
                {
                    "sent": "And these are after 30 rounds through 30 instances and on the left is the full covariance case, which actually will not discuss in all actually experiments.",
                    "label": 0
                },
                {
                    "sent": "So what you see here is the blue circles represent the... are.",
                    "label": 0
                },
                {
                    "sent": "Several pairs of noise features.",
                    "label": 1
                },
                {
                    "sent": "These are the sort of the their covariance.",
                    "label": 0
                },
                {
                    "sent": "Actually, I'm scale by 20, so that's 20 times.",
                    "label": 0
                },
                {
                    "sent": "Covariance... and you see that these are all kind of around 0, but with a lot of noise.",
                    "label": 0
                },
                {
                    "sent": "The Green Point is our target hypothesis, mean covariance and the block... which is very elongated there already is.",
                    "label": 1
                },
                {
                    "sent": "The which corresponds to the covariance times scale by 20 of the two informative features.",
                    "label": 0
                },
                {
                    "sent": "So you see that bills are getting very close to the intended separator.",
                    "label": 0
                },
                {
                    "sent": "Now on the diagonal model, and this is a different scale on the axis.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately what you see is that the... are still much larger and they have to be axis aligned because it's the diagonal case that's our target and there's a little black... that you can't quite see there.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, because it's behind that green circle, but that corresponds to the... that shows you the learning rate for the two important.",
                    "label": 0
                },
                {
                    "sent": "The two significant features so you see that for the noise features I just I am willing to change their weight very rapidly because I don't know anything about them for the non noise features I'm not.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have here at the same after 90 rounds and again this scaling issues.",
                    "label": 1
                },
                {
                    "sent": "This is just this is in the diagonal case.",
                    "label": 0
                },
                {
                    "sent": "Again I'm very close to what to our target here.",
                    "label": 0
                },
                {
                    "sent": "I'm also very close.",
                    "label": 0
                },
                {
                    "sent": "I cannot.",
                    "label": 0
                },
                {
                    "sent": "You cannot see the.",
                    "label": 0
                },
                {
                    "sent": "You can barely see the black... there in the middle and these still their covariances.",
                    "label": 0
                },
                {
                    "sent": "I'm it's smaller, but they're still around all around zero as you expect.",
                    "label": 0
                },
                {
                    "sent": "So these pictures show that the fact that we want is operating, so the learning rates are much larger for the features that Porch will have less information than the features that we have a lot more information, and this is not just the number of accounts, but also whether the feature that corrected are classifying correctly or not.",
                    "label": 0
                },
                {
                    "sent": "So featured offers a lot but is not properly does not give much information about the label.",
                    "label": 0
                },
                {
                    "sent": "Also is going to have a much.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Faster learning rate.",
                    "label": 0
                },
                {
                    "sent": "So now we're going to go over some experiments.",
                    "label": 0
                },
                {
                    "sent": "And so these are online to batch experiments, so we doing multiple passes or over the training data and compute error accuracy.",
                    "label": 1
                },
                {
                    "sent": "There's also more information about community.",
                    "label": 0
                },
                {
                    "sent": "Sort of the.",
                    "label": 1
                },
                {
                    "sent": "Online learning rates in the paper and these are binary problems from newsgroups Reuters.",
                    "label": 0
                },
                {
                    "sent": "The sentiment classification.",
                    "label": 0
                },
                {
                    "sent": "So we have some sentiment data that's described in the paper.",
                    "label": 0
                },
                {
                    "sent": "Some three divisions.",
                    "label": 0
                },
                {
                    "sent": "Binary divisions of.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Write Reuters with bag of words representation.",
                    "label": 0
                },
                {
                    "sent": "Binary features an three divisions of news.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Newsgroups.",
                    "label": 0
                },
                {
                    "sent": "So here's the sort of the snippet.",
                    "label": 0
                },
                {
                    "sent": "Typical behavior of this method.",
                    "label": 0
                },
                {
                    "sent": "We have number of iterations passes over the data.",
                    "label": 0
                },
                {
                    "sent": "This accuracy in the Y axis and they have three methods here.",
                    "label": 0
                },
                {
                    "sent": "One is passive aggressive method.",
                    "label": 0
                },
                {
                    "sent": "The green and blue are variance exactly is the method that I didn't describe in detail.",
                    "label": 0
                },
                {
                    "sent": "We are optimally maintain a diagonal covariance, but we optimize that directly and the blue one is the variance approximate method I describe in more detail where we project onto the diagonal matrices.",
                    "label": 0
                },
                {
                    "sent": "For reasons that we don't fully understand, these approximate method performs consistently better.",
                    "label": 0
                },
                {
                    "sent": "The the other thing to note, which is important for both methods is that after one iteration, one pass through the data they already doing competitively with passive aggressive after five iterations.",
                    "label": 0
                },
                {
                    "sent": "So basically they converge in one path to the date.",
                    "label": 0
                },
                {
                    "sent": "There is not much changed as the data continue.",
                    "label": 0
                },
                {
                    "sent": "This is 1 just one snapshot.",
                    "label": 0
                },
                {
                    "sent": "There are many other snapshots we have in the paper and many others that we have done that we have not didn't have room to put there.",
                    "label": 0
                },
                {
                    "sent": "Select me move onto.",
                    "label": 0
                },
                {
                    "sent": "Understand.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why is this?",
                    "label": 0
                },
                {
                    "sent": "Stop.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some some there's a lot of statistics in the paper.",
                    "label": 0
                },
                {
                    "sent": "I'll just highlight the following, so the variance method that we choose, the one that I described in most detail on 20 newsgroups, Reuters and sentiment, and it almost always performs better than a variety of batch methods on the same data and all the hyper parameters have been set here by using held out data support vector machines with the same features, maximum entropy and stochastic gradient descent on the.",
                    "label": 0
                },
                {
                    "sent": "Huber loss and you see that consistently this method performs better and we believe the reason for that, although we don't have full analysis of that yet, is that we taking into account the.",
                    "label": 0
                },
                {
                    "sent": "Frequencies of different word features.",
                    "label": 0
                },
                {
                    "sent": "And most of these results are significant to some level.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this sort of just where there's one other thing we try to do explore this method is the idea.",
                    "label": 0
                },
                {
                    "sent": "If you have a very large training set.",
                    "label": 0
                },
                {
                    "sent": "If you and you split it over many training runs over different machines, can we use this idea is to combine those runs.",
                    "label": 0
                },
                {
                    "sent": "So one thing we could do is to.",
                    "label": 0
                },
                {
                    "sent": "Do a sort of uniform combination of weight vectors of different classifiers.",
                    "label": 1
                },
                {
                    "sent": "The other thing we can do is to try to find if we use this math learning method is to try to find the closest entailed by the closest hypothesis entail divergance to the one that we do, all the ones we trained on the individual classifiers.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, there is actually you can compute means and the inverse covariance is very directly.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just to give you a quick.",
                    "label": 0
                },
                {
                    "sent": "View of that we see that the average of each of the classifiers.",
                    "label": 0
                },
                {
                    "sent": "If you use the classifiers trained independently, is a red bar.",
                    "label": 0
                },
                {
                    "sent": "The uniform mixing of the classifiers is the blue bar in the black bar is the one done using this method, and the black is the variance method.",
                    "label": 0
                },
                {
                    "sent": "Training on the entire data and dashes.",
                    "label": 0
                },
                {
                    "sent": "Passive aggressive training on the entire data.",
                    "label": 0
                },
                {
                    "sent": "So we see that we can do better than passive aggressive even though we did trade in parallel.",
                    "label": 0
                },
                {
                    "sent": "And in that is consistent across 4 if you if you only if each of the parallel classifiers is enough data.",
                    "label": 0
                },
                {
                    "sent": "Now if they have seen much less data then of course they will perform.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "Out just to finalize an interest of time.",
                    "label": 0
                },
                {
                    "sent": "So LP has very skewed feature distributions and we tried here to represent explicitly how much do we know about each feature.",
                    "label": 1
                },
                {
                    "sent": "An homage to trust each feature weight.",
                    "label": 0
                },
                {
                    "sent": "This achieves fairly high accuracy, better than even some previous batch methods, very fast convergence, and ineffective way of doing model combination.",
                    "label": 1
                },
                {
                    "sent": "The current work we have a direct inverse version of the original optimization, which is work since we submitted this paper.",
                    "label": 0
                },
                {
                    "sent": "Also a batch algorithm that we are investigating and we're doing some explorations in using full covariance or at least larger covariance that then just diagonal to cap capture correlations between features.",
                    "label": 0
                },
                {
                    "sent": "That's all, thank you.",
                    "label": 0
                }
            ]
        }
    }
}