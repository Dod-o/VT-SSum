{
    "id": "wvjvi4dmnjm3jqczt6bdyqyn6zsm66ok",
    "title": "An Alternating Direction Method for Dual MAP LP Relaxation",
    "info": {
        "author": [
            "Amir Globerson, School of Computer Science and Engineering, The Hebrew University of Jerusalem"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_globerson_alternating/",
    "segmentation": [
        [
            "So when the problem we're looking at is the math problem in graphical models and essentially in many machine learning tasks at what you want to do is find the maximum assignment in a probabilistic graphical model and that is had many, many applications over the years in language processing, computer vision, biology and many others."
        ],
        [
            "Anne.",
            "OK, so some examples are part of speech tagging, so you have a sentence in natural language and you want to assign parts of speech to every word.",
            "Or in vision you have two images and you want to find the stereo depth within the image."
        ],
        [
            "A computational biology.",
            "You have a protein you want from its structure to induce the sequence of."
        ],
        [
            "Amino acids.",
            "Or the other way around, you have a sequence and you want to induce the structure.",
            "So all these things can be cast as a problem of finding a map assignment in a graphical model."
        ],
        [
            "OK, so unfortunately the math problem is for even relatively basic graphs is NP hard.",
            "But so how is this useful in practice?",
            "So it turns out that linear programming relaxations are very effective in practice and often can solve problems that are are supposedly computationally hard.",
            "But it turns out that these relaxations solve them exactly.",
            "But there is an algorithmic problem which is to solve these relaxations in practice and do it in a fast way that will allow a real time applications.",
            "So if you just use an off the shelf solvers for solving these LP's that turns out to be a pretty slow.",
            "So over the last few years, there's been a lot of effort to come up with the effective scalable algorithms for solving such LP's so summer, so the current status is some work pretty well, but there are still relatively slow, and the faster algorithms actually turned out to be not globally convergent.",
            "For reasons I'll talk about later, so it's still a open research problem to come up with effective algorithms for.",
            "What?"
        ],
        [
            "Problem.",
            "OK, So what we do in this work is presented a novel algorithm for solving map LP relaxations.",
            "It's guaranteed to converge to the global optimum of the LP.",
            "It's scalable and only involves close former updates so they aren't parameters to tune.",
            "App.",
            "And it's based on the dual LP relaxation, which I'll describe later an it uses.",
            "A type of algorithm called alternating gap direction method of multipliers which has a long history, but it's recently been re popularized by a survey by Stephen Boyd and colleagues.",
            "So it's a cool method, and if you take nothing from this talk, at least you will know about it."
        ],
        [
            "OK, so more formally, what's the map problem?",
            "You build the distribution over these variables, so the probability of an assignment to X1 to XN is proportional to the exponent over some of individual terms.",
            "The data X is a function over each variable X one, and then you have a higher order factors data.",
            "CXC where C is a subset of 1 two North and these induce higher order interactions between the X is right.",
            "So probably the most popular case of these models is the pairwise Markov random field, where you have the higher order factors are just pairwise factors.",
            "So you have a factor for each over a set of pairs of inj."
        ],
        [
            "So for example, in the state of the problem you will have available for each pixel in the image indicating the depth of the image, and you will have an interaction between.",
            "Neighboring pixels saying that they should have similar values so that you'll have a continuous area map.",
            "OK, So what you often want to do when you have these models is to find the assignment that has the maximum probability because that often gives you the solution to the application that you want.",
            "OK.",
            "So this is what we called the map."
        ],
        [
            "Rob.",
            "So as I said, generally this problem is a discrete optimization problem is NP hard.",
            "But LP relaxations work pretty well, so I'll describe what those are.",
            "So here's the problem.",
            "The discrete optimization problem that we want to solve.",
            "So finding an X that maximizes this term.",
            "And here's how the LP relaxation goes.",
            "So first we construct will not we this this these relaxations have a long history and Wainwright and Jordan gave a nicer review those.",
            "So here's an exact equivalence.",
            "So the map problem is equivalent to the following LP.",
            "You variables are distributions over individual variables and the click the see the XY variables so you have a variables LP variables for those which you should understand this distributions over other, either XC or XC.",
            "OK, so that looks like an LP, so what's the problem?",
            "App.",
            "Well, the problem is that these distributions need to belong to something called the marginal polytope that says that they need to come from some global distribution P, and it turns out that this polytope is hard to describe in the sense that requires an exponential number of inequality.",
            "So you can't really solve this problem, although it's equivalent, you can't really solve it because it's it's an intractable."
        ],
        [
            "So what people have done is to replace this constraint with a much simpler constraint called the local polytope.",
            "We just says that your distributions over clusters C and distribution over individual variables have to agree with each other, so the distributions over XC need to agree with the distributions over I OK, they need to marginalized too mu IXI.",
            "OK, so this is a tractable linear program.",
            "It only involves a polynomial number of constraints.",
            "And as I said, you could solve it with off the shelf solvers, but that would typically be too slow."
        ],
        [
            "OK, and it gives you an upper bound on the true map value, and as I said often it actually finds the correct solution and you also have optimality certificates so."
        ],
        [
            "You solved it."
        ],
        [
            "OK. And now it turns out that algorithmically it's worthwhile looking at the dual of this LP, because things become nicer and decompose in the night away.",
            "OK, So what is the dual of this LP relaxation?",
            "So the dual variables you can think of his messages will call them Delta J, so I'm focusing on the pairwise case now.",
            "So we'll have two available Delta I JXJ, which you should think of as a message from node I to know Jay about the value XJ.",
            "OK, so say you have a graph like this.",
            "Then you have a variable Delta IJXJ.",
            "OK, so these are the dual variables.",
            "So what's the objective?",
            "So we find a new set of parameters.",
            "So originally we had the pairwise parameters Theta, J and single parameters data.",
            "I now we define Theta Bar XI, XJ and Theta Bar XI in the following way using the Deltas.",
            "So they're just a linear function of the deltas.",
            "And your dual objective is this simple thing.",
            "It says you minimize over Delta the sum over I where you maximize over each Singleton term individually and over each pairwise term individually OK. And so originally you had a Max over X.",
            "The sum of the Singleton individuals, and now you have the Max inside, so it's much simpler.",
            "OK, so this is the dual object.",
            "And the maximum as I said, is over."
        ],
        [
            "The smaller OK.",
            "Right, so let's turn to the more general case.",
            "That was for pairwise case.",
            "What I talked about before and in the general case you have dual variables that are.",
            "You should think of his messages from the set C to the variable.",
            "I OK, so Delta CI is a message from the cluster C to the variable I XI, and this is the general form of the Lagrangian dual.",
            "OK, and now what we want to focus on is algorithms for solving this.",
            "Optimization problem."
        ],
        [
            "So two common approaches are to do coordinate dissent on the deltas, and we've developed a few algorithms and to do that.",
            "And it turns out that because this function is not strictly convex, these types of algorithms, while being pretty effective in practice, might get stuck in corners.",
            "OK, Anna, different approaches to use subgradient or variants of subgradient and that turns out to be.",
            "Although it has nicer theoretical properties, it turns out to be slower in practice."
        ],
        [
            "OK, So what we're looking for is globally optimal methods that don't get stuck.",
            "But work well in practice.",
            "Right, so I mentioned using subgradient.",
            "An alternative is to take this function that I said was not strictly convex and to smooth it by adding entropy terms and that's been done by a few people.",
            "And there's also a variant of that for a accelerated gradient methods by a user at all, and it turns out that these smooth methods, because they have a temperature parameter.",
            "There are numerical issues that come up, and it turns out that you either get a solution that's relatively inaccurate or that it converges more slowly, but it's."
        ],
        [
            "Work will impact.",
            "OK, so we propose a method as I said, based on the idea of augmented Lagrangian.",
            "And it's a.",
            "It's a nice general approach to optimization, so I want to spend a few slides.",
            "Talking about that.",
            "OK, so what's the idea?",
            "So say you have an optimization problem and minimizing a function F of X subject to linear constraints.",
            "So let's look at an equivalent problem where you add a term AX minus B squared, but you keep the original constraint, so you've done nothing right, because if the constraint is satisfied, then this term, this quadratic term is just zero.",
            "OK, so there's an equivalence here.",
            "OK, and now write the Lagrangian for this problem, so it's the usual Lagrangian plus this quadratic term, which is part of the objective.",
            "OK."
        ],
        [
            "So that was the Lagrangian, and now the idea is to solve the dual problem, so to maximize over the dual variable NI the minimum over the Lagrangian.",
            "OK, so this is just solving the dual essentially.",
            "So the simplest approach to solving this is just using subgradient descent on the function of knee.",
            "OK, so what's the subgradient with respect to need?",
            "At.",
            "It involves maximizing the function at the given value of any binding.",
            "The maximizing X, and then there's a gradient is just a XT plus one, which is this maximizer minus B. OK, so it's that follows.",
            "From a calculating the subgradient.",
            "OK, so here's an algorithm that would work.",
            "You do subgradient descent on any."
        ],
        [
            "But the caveat here, so I guess that."
        ],
        [
            "Caveat one is you don't want to say set this step size epsilon.",
            "That's always a pain.",
            "So it turns out."
        ],
        [
            "That instead of the step size epsilon you can use that row which you used in smoothing and that would actually converge and you don't need to do adaptive step size or anything, so that's the first day magic that happens here, so that's very nice.",
            "But the other."
        ],
        [
            "Problem is that at every step you need to solve this maximization problem over X, which can be quite costly.",
            "I mean, you usually need some iterative approach to solving that and you don't want to do that.",
            "OK, so that's why the variant of augmented Lagrangian is often not easy."
        ],
        [
            "Apply.",
            "So now comes the second idea, which is this alternating direction method of multipliers.",
            "As I said, it has a long history from Gabbi and Mercier in the 70s, and to avoid a recent review by boy.",
            "It says the following so often you have your function.",
            "Your objective can be decomposed into new functions where each individual function is easy to handle.",
            "OK, so you can partition your variables into two sets where it's easy to handle each set separately.",
            "OK, so that often happen so.",
            "So your function your objective is F of X * G of Z and your constraints are equal, say.",
            "OK, so now as before we add this quadratic smoothing term.",
            "Which does nothing at this point, and we write the augmented delagrange and so if we were to follow what we did before doing.",
            "Dual ascent on me would be the following, so need T + 1 would be in ET plus the subgradient subgradient calculating subgradient would involve maximizing over X&Z right because your variables are X and then OK, so that doesn't look any better than what we had before because we still have to maximize over all the variables of your original primal problem.",
            "So the key nice trick about the alternating direction method is that it tells you you don't have to do this.",
            "You can do something similar."
        ],
        [
            "So instead of optimizing over X&Z jointly.",
            "Which you don't want."
        ],
        [
            "Do you fix said you take that from iteration T?",
            "Minimizing and you minimize over X.",
            "Only X.",
            "Given that, fix the T at the next step, you take the TX T + 1 that you obtained, and you minimize over said OK, and now you obtain X, T + 1 and Z, T + 1, and those are the ones that you use in the subgradient OK.",
            "So it's not clear immediately why this would work, and actually if you add more variables that small changes that you can make to this and it won't work.",
            "So there's something particular about this setup of the decomposition which makes this method work.",
            "And actually if you do this, whatever value of row you use, this will converge globally, so it's a it's a very effective method and can be applied to a wide range of problems.",
            "OK, so now the only problem in when you come to use this for a particular problem is to find the decomposition of X&Z in F&G.",
            "That will let you do these two minimization problems easily, and that's essentially what."
        ],
        [
            "We did here, so there has been recently another application of this idea to solving the map LP, but they actually preceded via the primal, which gave him a globally convergent algorithm as well.",
            "But it could only be applied to binary potentials, and as we show empirically, it seems that it's better to use to optimize via dual because it's faster."
        ],
        [
            "Right, so let's let me tell you very briefly how we do this decomposition and apply the the alternating direction method.",
            "So recall that this is the tool that we want to solve.",
            "OK, so it's not clear it doesn't decompose in any nice way.",
            "It doesn't seem too, so here's so we do a couple of tricks to make it decompose.",
            "So the first thing we do is we take this sum over Delta all the way."
        ],
        [
            "On the right, and we call it Lambda CX."
        ],
        [
            "OK, and we add a constraint saying that Lambda CXY is equal to the sum over Delta.",
            "OK, so it still doesn't decompose like we wanted to.",
            "So what we do?",
            "Is called this DD bar."
        ],
        [
            "And then ask the Delta Equal Delta bar so they are essentially the same OK and now it decomposes like we want to.",
            "I mean it took a many iterations to get this to decompose in a way where we could get a nice algorithm.",
            "So there are many other things you could try that.",
            "Problem in some will not work.",
            "OK.",
            "So what's the decomposition?",
            "So RZ variable is now Delta and Lambda.",
            "OK, so Delta and Lambda.",
            "You should think of is a single variable and this function here is G of Delta in Lambda and.",
            "XX is not part of this and RX variable is Delta Bar.",
            "OK, so that's about as X and Delta and Lambda RZ.",
            "OK, and here are our constraint is a Delta bar equals Delta Lambda, so that's that's the linear constraint.",
            "So it all falls into this framework that we had before OK.",
            "So again Z is Delta and Lambda and X is Delta bar.",
            "OK, so now it looks like what we had before and now the question is whether we can do these updates.",
            "These Adm updates.",
            "Really inefficiently in the answer we can.",
            "OK.",
            "Right, so it's.",
            "And I didn't say about the worst case performance of this is of one over epsilon squared iterations to get epsilon accuracy, but in practice it's."
        ],
        [
            "Faster."
        ],
        [
            "OK, so actually deriving the updates is a bit technical so I won't go too much into that just to show you what's involved.",
            "So to get the Lambda update you need to minimize over the Lambda.",
            "This augmented Lagrangian, as I said before and that's why."
        ],
        [
            "It looks like, but if you simplify it boils down to solving AQP of this form, so you have a quadratic term.",
            "That's a linear term plus a term that's a maximum over the elements of X.",
            "The quadratic it's a QP plus some maxterm and it."
        ],
        [
            "Turns out that you can solve this in closed form.",
            "These types of problems can be solved in closed forms and people have worked on related problems by just sorting.",
            "OK, so you just sort you take this vector.",
            "VI twinkle, bitten sort.",
            "Then you get a closed form solution and the nice thing is that we get closed form updates for all the steps within the."
        ],
        [
            "OK.",
            "Sorry.",
            "Right?"
        ],
        [
            "OK, so this is the algorithm.",
            "You got it right.",
            "But"
        ],
        [
            "All the steps are easy to apply and the most complex thing is sorting.",
            "The vectors, but that's pretty fast.",
            "OK, so just to show you about how this works, we compared it to.",
            "Accordingly decent algorithm that we developed a few years ago, and as I said, works well in practice, but has this can sometimes get stuck in corners or local corner.",
            "We also compared it to.",
            "An algorithm by using Seattle that's accelerated the composition.",
            "Insane application of Nesterov's fast.",
            "In Tiger Rhythm and our method."
        ],
        [
            "Right, so where we looked at protein design problems.",
            "We have a protein backbone and you want to find the sequence of amino acids that where this, say 3D configuration is stable.",
            "OK, so a DLP is our algorithm.",
            "Our new algorithm an and you can see that MP that's decoding descent algorithm.",
            "Actually it makes much more progress initially, but it gets stuck.",
            "It's suboptimal at the end you see the blue line, the dark blue line is above the other two lines, so it's suboptimal.",
            "It converges to a sub optimal point.",
            "And the the augmented method makes the convergence faster than the accelerated Nesterov method."
        ],
        [
            "OK, so here's another rant.",
            "An instance where are method that converges faster than the accelerated dual decomposition.",
            "We don't."
        ],
        [
            "OK. Anne.",
            "OK, we also compared it to.",
            "A primal based method of Martins at all, and it turns out that the dual as I said that it seems like it's worthwhile to approach this problem by."
        ],
        [
            "2.",
            "OK, so just to conclude representing novel algorithm for map LP relaxation based on a DMM, the updates are closed form an improve.",
            "It converges to there.",
            "The solution of the LP approximation is empirically effective in practice."
        ],
        [
            "So thanks.",
            "Time is over, but we think I take one or two small short questions.",
            "Questions, so you solve the difficult problem for lots and lots of small tricks and you still have some.",
            "Problem left or do you think you have some other problem?",
            "Well, first of all, it's in an approximation, so we haven't solved the math problem.",
            "We've solved or came up with a faster algorithm for solving LP's, but the real problem is finding solutions for the map problem in their cases where this relaxation fails but also algorithmically.",
            "I'm sure you can work on faster algorithm."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when the problem we're looking at is the math problem in graphical models and essentially in many machine learning tasks at what you want to do is find the maximum assignment in a probabilistic graphical model and that is had many, many applications over the years in language processing, computer vision, biology and many others.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "OK, so some examples are part of speech tagging, so you have a sentence in natural language and you want to assign parts of speech to every word.",
                    "label": 0
                },
                {
                    "sent": "Or in vision you have two images and you want to find the stereo depth within the image.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A computational biology.",
                    "label": 0
                },
                {
                    "sent": "You have a protein you want from its structure to induce the sequence of.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amino acids.",
                    "label": 0
                },
                {
                    "sent": "Or the other way around, you have a sequence and you want to induce the structure.",
                    "label": 0
                },
                {
                    "sent": "So all these things can be cast as a problem of finding a map assignment in a graphical model.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so unfortunately the math problem is for even relatively basic graphs is NP hard.",
                    "label": 0
                },
                {
                    "sent": "But so how is this useful in practice?",
                    "label": 0
                },
                {
                    "sent": "So it turns out that linear programming relaxations are very effective in practice and often can solve problems that are are supposedly computationally hard.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that these relaxations solve them exactly.",
                    "label": 0
                },
                {
                    "sent": "But there is an algorithmic problem which is to solve these relaxations in practice and do it in a fast way that will allow a real time applications.",
                    "label": 0
                },
                {
                    "sent": "So if you just use an off the shelf solvers for solving these LP's that turns out to be a pretty slow.",
                    "label": 0
                },
                {
                    "sent": "So over the last few years, there's been a lot of effort to come up with the effective scalable algorithms for solving such LP's so summer, so the current status is some work pretty well, but there are still relatively slow, and the faster algorithms actually turned out to be not globally convergent.",
                    "label": 0
                },
                {
                    "sent": "For reasons I'll talk about later, so it's still a open research problem to come up with effective algorithms for.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we do in this work is presented a novel algorithm for solving map LP relaxations.",
                    "label": 0
                },
                {
                    "sent": "It's guaranteed to converge to the global optimum of the LP.",
                    "label": 0
                },
                {
                    "sent": "It's scalable and only involves close former updates so they aren't parameters to tune.",
                    "label": 0
                },
                {
                    "sent": "App.",
                    "label": 0
                },
                {
                    "sent": "And it's based on the dual LP relaxation, which I'll describe later an it uses.",
                    "label": 0
                },
                {
                    "sent": "A type of algorithm called alternating gap direction method of multipliers which has a long history, but it's recently been re popularized by a survey by Stephen Boyd and colleagues.",
                    "label": 0
                },
                {
                    "sent": "So it's a cool method, and if you take nothing from this talk, at least you will know about it.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so more formally, what's the map problem?",
                    "label": 0
                },
                {
                    "sent": "You build the distribution over these variables, so the probability of an assignment to X1 to XN is proportional to the exponent over some of individual terms.",
                    "label": 0
                },
                {
                    "sent": "The data X is a function over each variable X one, and then you have a higher order factors data.",
                    "label": 0
                },
                {
                    "sent": "CXC where C is a subset of 1 two North and these induce higher order interactions between the X is right.",
                    "label": 0
                },
                {
                    "sent": "So probably the most popular case of these models is the pairwise Markov random field, where you have the higher order factors are just pairwise factors.",
                    "label": 0
                },
                {
                    "sent": "So you have a factor for each over a set of pairs of inj.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, in the state of the problem you will have available for each pixel in the image indicating the depth of the image, and you will have an interaction between.",
                    "label": 0
                },
                {
                    "sent": "Neighboring pixels saying that they should have similar values so that you'll have a continuous area map.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you often want to do when you have these models is to find the assignment that has the maximum probability because that often gives you the solution to the application that you want.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is what we called the map.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rob.",
                    "label": 0
                },
                {
                    "sent": "So as I said, generally this problem is a discrete optimization problem is NP hard.",
                    "label": 0
                },
                {
                    "sent": "But LP relaxations work pretty well, so I'll describe what those are.",
                    "label": 0
                },
                {
                    "sent": "So here's the problem.",
                    "label": 0
                },
                {
                    "sent": "The discrete optimization problem that we want to solve.",
                    "label": 0
                },
                {
                    "sent": "So finding an X that maximizes this term.",
                    "label": 0
                },
                {
                    "sent": "And here's how the LP relaxation goes.",
                    "label": 0
                },
                {
                    "sent": "So first we construct will not we this this these relaxations have a long history and Wainwright and Jordan gave a nicer review those.",
                    "label": 0
                },
                {
                    "sent": "So here's an exact equivalence.",
                    "label": 0
                },
                {
                    "sent": "So the map problem is equivalent to the following LP.",
                    "label": 1
                },
                {
                    "sent": "You variables are distributions over individual variables and the click the see the XY variables so you have a variables LP variables for those which you should understand this distributions over other, either XC or XC.",
                    "label": 0
                },
                {
                    "sent": "OK, so that looks like an LP, so what's the problem?",
                    "label": 0
                },
                {
                    "sent": "App.",
                    "label": 0
                },
                {
                    "sent": "Well, the problem is that these distributions need to belong to something called the marginal polytope that says that they need to come from some global distribution P, and it turns out that this polytope is hard to describe in the sense that requires an exponential number of inequality.",
                    "label": 0
                },
                {
                    "sent": "So you can't really solve this problem, although it's equivalent, you can't really solve it because it's it's an intractable.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what people have done is to replace this constraint with a much simpler constraint called the local polytope.",
                    "label": 0
                },
                {
                    "sent": "We just says that your distributions over clusters C and distribution over individual variables have to agree with each other, so the distributions over XC need to agree with the distributions over I OK, they need to marginalized too mu IXI.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a tractable linear program.",
                    "label": 0
                },
                {
                    "sent": "It only involves a polynomial number of constraints.",
                    "label": 0
                },
                {
                    "sent": "And as I said, you could solve it with off the shelf solvers, but that would typically be too slow.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and it gives you an upper bound on the true map value, and as I said often it actually finds the correct solution and you also have optimality certificates so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You solved it.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. And now it turns out that algorithmically it's worthwhile looking at the dual of this LP, because things become nicer and decompose in the night away.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the dual of this LP relaxation?",
                    "label": 0
                },
                {
                    "sent": "So the dual variables you can think of his messages will call them Delta J, so I'm focusing on the pairwise case now.",
                    "label": 0
                },
                {
                    "sent": "So we'll have two available Delta I JXJ, which you should think of as a message from node I to know Jay about the value XJ.",
                    "label": 0
                },
                {
                    "sent": "OK, so say you have a graph like this.",
                    "label": 0
                },
                {
                    "sent": "Then you have a variable Delta IJXJ.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the dual variables.",
                    "label": 0
                },
                {
                    "sent": "So what's the objective?",
                    "label": 0
                },
                {
                    "sent": "So we find a new set of parameters.",
                    "label": 0
                },
                {
                    "sent": "So originally we had the pairwise parameters Theta, J and single parameters data.",
                    "label": 0
                },
                {
                    "sent": "I now we define Theta Bar XI, XJ and Theta Bar XI in the following way using the Deltas.",
                    "label": 0
                },
                {
                    "sent": "So they're just a linear function of the deltas.",
                    "label": 0
                },
                {
                    "sent": "And your dual objective is this simple thing.",
                    "label": 0
                },
                {
                    "sent": "It says you minimize over Delta the sum over I where you maximize over each Singleton term individually and over each pairwise term individually OK. And so originally you had a Max over X.",
                    "label": 0
                },
                {
                    "sent": "The sum of the Singleton individuals, and now you have the Max inside, so it's much simpler.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the dual object.",
                    "label": 0
                },
                {
                    "sent": "And the maximum as I said, is over.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The smaller OK.",
                    "label": 0
                },
                {
                    "sent": "Right, so let's turn to the more general case.",
                    "label": 0
                },
                {
                    "sent": "That was for pairwise case.",
                    "label": 1
                },
                {
                    "sent": "What I talked about before and in the general case you have dual variables that are.",
                    "label": 0
                },
                {
                    "sent": "You should think of his messages from the set C to the variable.",
                    "label": 0
                },
                {
                    "sent": "I OK, so Delta CI is a message from the cluster C to the variable I XI, and this is the general form of the Lagrangian dual.",
                    "label": 1
                },
                {
                    "sent": "OK, and now what we want to focus on is algorithms for solving this.",
                    "label": 0
                },
                {
                    "sent": "Optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So two common approaches are to do coordinate dissent on the deltas, and we've developed a few algorithms and to do that.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that because this function is not strictly convex, these types of algorithms, while being pretty effective in practice, might get stuck in corners.",
                    "label": 0
                },
                {
                    "sent": "OK, Anna, different approaches to use subgradient or variants of subgradient and that turns out to be.",
                    "label": 0
                },
                {
                    "sent": "Although it has nicer theoretical properties, it turns out to be slower in practice.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what we're looking for is globally optimal methods that don't get stuck.",
                    "label": 0
                },
                {
                    "sent": "But work well in practice.",
                    "label": 0
                },
                {
                    "sent": "Right, so I mentioned using subgradient.",
                    "label": 0
                },
                {
                    "sent": "An alternative is to take this function that I said was not strictly convex and to smooth it by adding entropy terms and that's been done by a few people.",
                    "label": 0
                },
                {
                    "sent": "And there's also a variant of that for a accelerated gradient methods by a user at all, and it turns out that these smooth methods, because they have a temperature parameter.",
                    "label": 0
                },
                {
                    "sent": "There are numerical issues that come up, and it turns out that you either get a solution that's relatively inaccurate or that it converges more slowly, but it's.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work will impact.",
                    "label": 0
                },
                {
                    "sent": "OK, so we propose a method as I said, based on the idea of augmented Lagrangian.",
                    "label": 0
                },
                {
                    "sent": "And it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a nice general approach to optimization, so I want to spend a few slides.",
                    "label": 0
                },
                {
                    "sent": "Talking about that.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the idea?",
                    "label": 0
                },
                {
                    "sent": "So say you have an optimization problem and minimizing a function F of X subject to linear constraints.",
                    "label": 0
                },
                {
                    "sent": "So let's look at an equivalent problem where you add a term AX minus B squared, but you keep the original constraint, so you've done nothing right, because if the constraint is satisfied, then this term, this quadratic term is just zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's an equivalence here.",
                    "label": 0
                },
                {
                    "sent": "OK, and now write the Lagrangian for this problem, so it's the usual Lagrangian plus this quadratic term, which is part of the objective.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that was the Lagrangian, and now the idea is to solve the dual problem, so to maximize over the dual variable NI the minimum over the Lagrangian.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just solving the dual essentially.",
                    "label": 0
                },
                {
                    "sent": "So the simplest approach to solving this is just using subgradient descent on the function of knee.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the subgradient with respect to need?",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "It involves maximizing the function at the given value of any binding.",
                    "label": 0
                },
                {
                    "sent": "The maximizing X, and then there's a gradient is just a XT plus one, which is this maximizer minus B. OK, so it's that follows.",
                    "label": 0
                },
                {
                    "sent": "From a calculating the subgradient.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's an algorithm that would work.",
                    "label": 0
                },
                {
                    "sent": "You do subgradient descent on any.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the caveat here, so I guess that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Caveat one is you don't want to say set this step size epsilon.",
                    "label": 0
                },
                {
                    "sent": "That's always a pain.",
                    "label": 0
                },
                {
                    "sent": "So it turns out.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That instead of the step size epsilon you can use that row which you used in smoothing and that would actually converge and you don't need to do adaptive step size or anything, so that's the first day magic that happens here, so that's very nice.",
                    "label": 0
                },
                {
                    "sent": "But the other.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem is that at every step you need to solve this maximization problem over X, which can be quite costly.",
                    "label": 0
                },
                {
                    "sent": "I mean, you usually need some iterative approach to solving that and you don't want to do that.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why the variant of augmented Lagrangian is often not easy.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apply.",
                    "label": 0
                },
                {
                    "sent": "So now comes the second idea, which is this alternating direction method of multipliers.",
                    "label": 0
                },
                {
                    "sent": "As I said, it has a long history from Gabbi and Mercier in the 70s, and to avoid a recent review by boy.",
                    "label": 0
                },
                {
                    "sent": "It says the following so often you have your function.",
                    "label": 0
                },
                {
                    "sent": "Your objective can be decomposed into new functions where each individual function is easy to handle.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can partition your variables into two sets where it's easy to handle each set separately.",
                    "label": 0
                },
                {
                    "sent": "OK, so that often happen so.",
                    "label": 0
                },
                {
                    "sent": "So your function your objective is F of X * G of Z and your constraints are equal, say.",
                    "label": 0
                },
                {
                    "sent": "OK, so now as before we add this quadratic smoothing term.",
                    "label": 0
                },
                {
                    "sent": "Which does nothing at this point, and we write the augmented delagrange and so if we were to follow what we did before doing.",
                    "label": 0
                },
                {
                    "sent": "Dual ascent on me would be the following, so need T + 1 would be in ET plus the subgradient subgradient calculating subgradient would involve maximizing over X&Z right because your variables are X and then OK, so that doesn't look any better than what we had before because we still have to maximize over all the variables of your original primal problem.",
                    "label": 0
                },
                {
                    "sent": "So the key nice trick about the alternating direction method is that it tells you you don't have to do this.",
                    "label": 0
                },
                {
                    "sent": "You can do something similar.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So instead of optimizing over X&Z jointly.",
                    "label": 0
                },
                {
                    "sent": "Which you don't want.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do you fix said you take that from iteration T?",
                    "label": 0
                },
                {
                    "sent": "Minimizing and you minimize over X.",
                    "label": 0
                },
                {
                    "sent": "Only X.",
                    "label": 0
                },
                {
                    "sent": "Given that, fix the T at the next step, you take the TX T + 1 that you obtained, and you minimize over said OK, and now you obtain X, T + 1 and Z, T + 1, and those are the ones that you use in the subgradient OK.",
                    "label": 0
                },
                {
                    "sent": "So it's not clear immediately why this would work, and actually if you add more variables that small changes that you can make to this and it won't work.",
                    "label": 0
                },
                {
                    "sent": "So there's something particular about this setup of the decomposition which makes this method work.",
                    "label": 0
                },
                {
                    "sent": "And actually if you do this, whatever value of row you use, this will converge globally, so it's a it's a very effective method and can be applied to a wide range of problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so now the only problem in when you come to use this for a particular problem is to find the decomposition of X&Z in F&G.",
                    "label": 0
                },
                {
                    "sent": "That will let you do these two minimization problems easily, and that's essentially what.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did here, so there has been recently another application of this idea to solving the map LP, but they actually preceded via the primal, which gave him a globally convergent algorithm as well.",
                    "label": 0
                },
                {
                    "sent": "But it could only be applied to binary potentials, and as we show empirically, it seems that it's better to use to optimize via dual because it's faster.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so let's let me tell you very briefly how we do this decomposition and apply the the alternating direction method.",
                    "label": 0
                },
                {
                    "sent": "So recall that this is the tool that we want to solve.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not clear it doesn't decompose in any nice way.",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem too, so here's so we do a couple of tricks to make it decompose.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we do is we take this sum over Delta all the way.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the right, and we call it Lambda CX.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and we add a constraint saying that Lambda CXY is equal to the sum over Delta.",
                    "label": 0
                },
                {
                    "sent": "OK, so it still doesn't decompose like we wanted to.",
                    "label": 0
                },
                {
                    "sent": "So what we do?",
                    "label": 0
                },
                {
                    "sent": "Is called this DD bar.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then ask the Delta Equal Delta bar so they are essentially the same OK and now it decomposes like we want to.",
                    "label": 0
                },
                {
                    "sent": "I mean it took a many iterations to get this to decompose in a way where we could get a nice algorithm.",
                    "label": 0
                },
                {
                    "sent": "So there are many other things you could try that.",
                    "label": 0
                },
                {
                    "sent": "Problem in some will not work.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what's the decomposition?",
                    "label": 0
                },
                {
                    "sent": "So RZ variable is now Delta and Lambda.",
                    "label": 0
                },
                {
                    "sent": "OK, so Delta and Lambda.",
                    "label": 0
                },
                {
                    "sent": "You should think of is a single variable and this function here is G of Delta in Lambda and.",
                    "label": 0
                },
                {
                    "sent": "XX is not part of this and RX variable is Delta Bar.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's about as X and Delta and Lambda RZ.",
                    "label": 0
                },
                {
                    "sent": "OK, and here are our constraint is a Delta bar equals Delta Lambda, so that's that's the linear constraint.",
                    "label": 0
                },
                {
                    "sent": "So it all falls into this framework that we had before OK.",
                    "label": 0
                },
                {
                    "sent": "So again Z is Delta and Lambda and X is Delta bar.",
                    "label": 0
                },
                {
                    "sent": "OK, so now it looks like what we had before and now the question is whether we can do these updates.",
                    "label": 0
                },
                {
                    "sent": "These Adm updates.",
                    "label": 0
                },
                {
                    "sent": "Really inefficiently in the answer we can.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's.",
                    "label": 0
                },
                {
                    "sent": "And I didn't say about the worst case performance of this is of one over epsilon squared iterations to get epsilon accuracy, but in practice it's.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Faster.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so actually deriving the updates is a bit technical so I won't go too much into that just to show you what's involved.",
                    "label": 0
                },
                {
                    "sent": "So to get the Lambda update you need to minimize over the Lambda.",
                    "label": 0
                },
                {
                    "sent": "This augmented Lagrangian, as I said before and that's why.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It looks like, but if you simplify it boils down to solving AQP of this form, so you have a quadratic term.",
                    "label": 0
                },
                {
                    "sent": "That's a linear term plus a term that's a maximum over the elements of X.",
                    "label": 0
                },
                {
                    "sent": "The quadratic it's a QP plus some maxterm and it.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turns out that you can solve this in closed form.",
                    "label": 0
                },
                {
                    "sent": "These types of problems can be solved in closed forms and people have worked on related problems by just sorting.",
                    "label": 0
                },
                {
                    "sent": "OK, so you just sort you take this vector.",
                    "label": 0
                },
                {
                    "sent": "VI twinkle, bitten sort.",
                    "label": 0
                },
                {
                    "sent": "Then you get a closed form solution and the nice thing is that we get closed form updates for all the steps within the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "You got it right.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the steps are easy to apply and the most complex thing is sorting.",
                    "label": 0
                },
                {
                    "sent": "The vectors, but that's pretty fast.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to show you about how this works, we compared it to.",
                    "label": 0
                },
                {
                    "sent": "Accordingly decent algorithm that we developed a few years ago, and as I said, works well in practice, but has this can sometimes get stuck in corners or local corner.",
                    "label": 0
                },
                {
                    "sent": "We also compared it to.",
                    "label": 0
                },
                {
                    "sent": "An algorithm by using Seattle that's accelerated the composition.",
                    "label": 0
                },
                {
                    "sent": "Insane application of Nesterov's fast.",
                    "label": 0
                },
                {
                    "sent": "In Tiger Rhythm and our method.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so where we looked at protein design problems.",
                    "label": 0
                },
                {
                    "sent": "We have a protein backbone and you want to find the sequence of amino acids that where this, say 3D configuration is stable.",
                    "label": 0
                },
                {
                    "sent": "OK, so a DLP is our algorithm.",
                    "label": 0
                },
                {
                    "sent": "Our new algorithm an and you can see that MP that's decoding descent algorithm.",
                    "label": 0
                },
                {
                    "sent": "Actually it makes much more progress initially, but it gets stuck.",
                    "label": 0
                },
                {
                    "sent": "It's suboptimal at the end you see the blue line, the dark blue line is above the other two lines, so it's suboptimal.",
                    "label": 0
                },
                {
                    "sent": "It converges to a sub optimal point.",
                    "label": 0
                },
                {
                    "sent": "And the the augmented method makes the convergence faster than the accelerated Nesterov method.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's another rant.",
                    "label": 0
                },
                {
                    "sent": "An instance where are method that converges faster than the accelerated dual decomposition.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Anne.",
                    "label": 0
                },
                {
                    "sent": "OK, we also compared it to.",
                    "label": 0
                },
                {
                    "sent": "A primal based method of Martins at all, and it turns out that the dual as I said that it seems like it's worthwhile to approach this problem by.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to conclude representing novel algorithm for map LP relaxation based on a DMM, the updates are closed form an improve.",
                    "label": 0
                },
                {
                    "sent": "It converges to there.",
                    "label": 0
                },
                {
                    "sent": "The solution of the LP approximation is empirically effective in practice.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thanks.",
                    "label": 0
                },
                {
                    "sent": "Time is over, but we think I take one or two small short questions.",
                    "label": 0
                },
                {
                    "sent": "Questions, so you solve the difficult problem for lots and lots of small tricks and you still have some.",
                    "label": 0
                },
                {
                    "sent": "Problem left or do you think you have some other problem?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, it's in an approximation, so we haven't solved the math problem.",
                    "label": 0
                },
                {
                    "sent": "We've solved or came up with a faster algorithm for solving LP's, but the real problem is finding solutions for the map problem in their cases where this relaxation fails but also algorithmically.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you can work on faster algorithm.",
                    "label": 0
                }
            ]
        }
    }
}