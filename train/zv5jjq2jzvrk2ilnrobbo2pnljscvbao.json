{
    "id": "zv5jjq2jzvrk2ilnrobbo2pnljscvbao",
    "title": "HDT-MR: A Scalable Solution for RDF Compression with HDT and MapReduce",
    "info": {
        "author": [
            "Jos\u00e9 M. Gim\u00e9nez-Garc\u00eda, Computer Science Department, University of Valladolid"
        ],
        "published": "July 15, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2015_gimenez_garcia_scalable_solution/",
    "segmentation": [
        [
            "Hello everybody, I'm a Masters degree students or for the University or Valladolid, Spain.",
            "I'm going to talk about HTML scalable solution for RDF compression with Acti MapReduce."
        ],
        [
            "So I'll start with an introduction into it.",
            "I talk about the motivation and a little bit about MapReduce anixety.",
            "Then I talk about our solution AC TM R&D processes it involves.",
            "Hey next, I described experimental evaluation with it and finally some conclusions on possible future work."
        ],
        [
            "So I seen everyone here can agree that the standard text formats for every RDF are pretty barrels and it leads not only to space overheads, but also consumption overheads.",
            "We have seen some dogs in previous days in which people needs to deal with RDF in memory and access data in an efficient way for example.",
            "So as it is.",
            "Possible solution for that kind of issues.",
            "It's a binary serialization based on compact data extractors that provides not only compression but also efficient random access to triples, but it can be said that with that solution, ATT moves the scalability issues regarding volume from the consumer to the publisher because the publisher has to generate the.",
            "City and it can be no trivial task because it involves heavy memory consumption.",
            "So we have devised essay TMR which uses a map reduce to deal with get generation of aset from huge volumes of RDF."
        ],
        [
            "So a little bit about not reduce, in case anyone of you doesn't know how it works.",
            "It's a framework programming model originally developed by Google and adopted by Apache and Apache Hadoop, which is a most common implementation, and in fact the one we used here is designed to process a large amounts of data in a distributed way.",
            "It has a master slave architecture where the master manages all the process and maintains all the bookkeeping information and their slaves store the data in a distributed way and perform the actual work over the data.",
            "It operates until the on the paradigm of moving the algorithms to the data instead of moving the data to the machines, it has to be processed.",
            "In order to avoid unnecessary network traffic, nonetheless, it relies a lot on exhaustive IO operations, an intensive bandwidth usage.",
            "It has a two faces.",
            "The mad face in with the notes.",
            "Read the data if possible in the same machine they are.",
            "They're running an process it, and I need a page key value, then a framework groups the those space by key.",
            "So reducers receive all the values associated with this key and then the reduce phase performs final operation and limits the.",
            "Final output."
        ],
        [
            "So about AZT itself?",
            "It has a three part he there dictionary and triples.",
            "The header contains metadata about the data set and the dictionary entry points are the most important ones.",
            "The dictionary has three different sections, one for the terms who are at the same time, subjects and objects of different ripples in the in the datasets.",
            "Two other ones in which we store the the terms which are only subjects or only object and the last one in which we store the predicates.",
            "This is on it that way first lady cause by dividing by by role.",
            "We can have a little more little ideas so we can save space and at the same time in many datasets many terms act at the same time.",
            "Subjects an object, so having a section for that kind of terms, let us avoid application.",
            "If not, we would have to have the intersection in the subject section in the object.",
            "And then this dictionary is in practice encoded using differential compression to take advancing to take advantage of the redundance that at the half.",
            "Then using those IDs and the triples are encoded.",
            "Hey concept Aleasa forest of trees where the subjects are the roots.",
            "The middle layer are the predicate associated with those subjects and their lives are the objects related with the base subject predicate.",
            "And in practice, this concept to authorities is stored in bitmap triples where the subjects are implicit by their order and the predicates and the objects are stored in a sequence of IDs with sequence align.",
            "With this sequence of ideas that relates its ID with a with a note in the upper layer.",
            "So we have here an example which represent two professors working for our Department and University.",
            "And we see, for example, the the Department is the one who at at the same time as subject and object.",
            "So within the first section, the professors are only subject in the graph, so they are in the second section.",
            "The literals are only objects, so they are intersection and in the fourth section are the predicates."
        ],
        [
            "So how is AZT a serialized in the classical model solution while it involves three steps in the first day we classify the RDF terms.",
            "We do so by parsing data triples one by one, creating 3 hash tables, one for its role and the at the same time.",
            "The tables are encoded using the temporary ID provided by.",
            "Those has tables, after all the all the troubles.",
            "Our past the the S or the subject object come on common terms is built with that we can proceed to build a dictionary.",
            "And we see this section is sorted lexicographically and our relation between the Ivy generated in the previews in the previous step under any one right from the server is a store.",
            "Then we make the differential compression to create the dictionary.",
            "After that we will do the triples at ripples in the first step where had the the terms.",
            "Place by temporary ID, but you use the relation created in the previous step to give them the definitive ideas.",
            "Then those triples are sorted by subject predicate, an object, and with that we can just traverse them sequentially and create the the sequence IDs and under its sequences we need to create the triples so we can see we need 4 has tables, an original structure to relate the temporary ID's with affinity ones.",
            "So the memory consumption is high.",
            "So as the size of the data set.",
            "We try to to stay alive, get bigger.",
            "The generation of ATP gets harder and harder."
        ],
        [
            "So how is it your mother tries to tackle this issue and generate HTTP?",
            "What is comprised of to to face is the first one is for dictionary encoding and the second one for Trip Listen calling."
        ],
        [
            "So we have the triplets distributed among the then.",
            "Also they might reduce cluster and the first job reach them and identify the roles of each one."
        ],
        [
            "Of its time, sorry.",
            "So we have here, for example, the triples of the graph we saw earlier distributed amount to notes here.",
            "The map phase reads the triples one by one for each triple emits three outputs in which its output is pair key value where the key is the term itself and the value is the draw it it has in the triple.",
            "Then the framework groups the whole space by by the term and sent all the times where they all the pace with the same term to the same producer.",
            "The reducer dense makes a composition of the roles that it receives.",
            "An generates the final output.",
            "For example, if you remember the apartment that was at the same time, subject and object we see in the in the first node, for example, the last triple has the Department acting as an object, so the map phase emits that.",
            "That person is an object, and for example in the second node, the last triple, the Department is a subject, so they might face limits at pace again that the Department subject.",
            "So all the papers related with.",
            "Elements are sent to their Reducer 2 here, which generates an output setting updates at the same time, subject and object."
        ],
        [
            "So we have the information we need to generate a city stood in a dictionary.",
            "Sorry, but we need that information and globally sorted, but that is not a trivial task in MapReduce becausw for create a global sort.",
            "We need to tell its reducer which pairs it has to receive in order to have a global or file or we cannot do it arbitrarily, becauses we know.",
            "RDF is very is cute, so we could end up sending most of the triples to only one reducer and losing most of the benefits of using distributed computing.",
            "So before the second job we use Hadoop tool to sample the data and generate what is called a partition files, which is used by the second job to know how to distribute evenly among the producers the data generator.",
            "Global order"
        ],
        [
            "So you've seen that file?",
            "The second job reads the output of the first job.",
            "The map are what are known as identity mappers, which do nothing with the data just rate it's it's pair, animate it directly, then the framework.",
            "Sort this data an groups in a way that it produces a sorted output.",
            "I need say reducer depending on the on the terms on the rolls its term has right.",
            "It's on a different output.",
            "So at the end we have four different output 148 dictionary section.",
            "And he globally sorted."
        ],
        [
            "Then we perform a final operation in the master, which just traverse that information sequentially.",
            "Uh, did generates the differential encoding needed for the photo dictionary."
        ],
        [
            "So for the triplets, the first thing we do is this dictionary we have created with this compressed and we can access eights in formation randomly in an efficient way.",
            "We distribute them among the notes."
        ],
        [
            "So the next job can read the original triples, look up for their ID in the dictionary, an replace each term with its ID.",
            "So then after that it admits it to the producers who just write them on the final output.",
            "So."
        ],
        [
            "Again, we have all the information we need without the triples with their ideas, but again we need it sorted globally, so we create another partition file, sampling the data passed that file to the second job."
        ],
        [
            "We will use this with entity.",
            "My person reduces who have no internal logic and the important work is made by the framework who groups an answer.",
            "They those triples in order to have a final output sorting."
        ],
        [
            "So at the end we have those strippers with their ideas sorted by subject, predicate, and object.",
            "We can proceed to create the sequence of ideas and David sequence necessary for the triples, and we did.",
            "We have a city."
        ],
        [
            "So we have we have evaluated and how we work.",
            "We have compared the generation of ATP in the classical 10 solution against the CCT MI.",
            "So for the for the solution, we have used a powerful machine with 128 gigabytes of RAM for the cluster 11 machines, one for the master wait, 48 gigabytes of RAM, and then machine of commodity hardware with eight gigabytes of RAM.",
            "Trying to make a fair comparison, if you Add all the gigabytes or the cluster you see that is the same as the memory that the single node has.",
            "A form of reduce execution we have.",
            "We use a Hadoop version one to one."
        ],
        [
            "We have the state it wait three real world data set linked your data DVD and Ike and also a mashup with all of them without sub to more than one billion triples.",
            "We also have generated synthetic data.",
            "We tell you VM from 1000 universities for up to 40,000 universities, which is more than 5 billion triples for the first data set.",
            "Data sets.",
            "The steps have been by 1000 University until 8000.",
            "Universities.",
            "From that point the datasets have been generated by steps of 4000 universities.",
            "In the paper we also have the compression rates we have it, which are similar to the to the previews literature on entity.",
            "But as those data sets have never been compressed until now, we think those data is interesting."
        ],
        [
            "Sold a valuation itself.",
            "We see that for the real world datasets on DVD and Link geodata, the mono solution wins against HTML.",
            "And in the case of Ike and or solution is slightly better but also their cells are almost compatible.",
            "In the case on the synthetic datasets the monologue solution starts winning but the data set data size increase.",
            "DMR starts being competitive up until a point or 5000 universities which is around 670 millions triples in weights as HTM R wins.",
            "This is be cause they model.",
            "Solution starts paginating informationen into the hard drive and so we come in slower up to the point that 7000 universities is the last day to set them on and off.",
            "Solution can manage and after that it just crashes if it tries to generate ICT for larger that data set."
        ],
        [
            "On the other hand, ATM R is able to to minus the generation of entity in an almost linear way with UVM datasets, and it's also able to generate ATP for the massage with them that the model solution is not able to do."
        ],
        [
            "So as a final conclusion.",
            "We have taken all the scalability issues arising.",
            "The density generation for huge datasets we have done it so by moving from the modern old paradigm for to the distributed paradyne using MapReduce and we have evaluated in real world datasets an synthetic data set up to more than 5 billion triples.",
            "As our future work, we will try to exploit those achievements and try to foster development of a new application that need to fast access to huge datasets and we also have some future ideas about using a city on map reduce to deal with issues relating with reasoning or specular query.",
            "Unwilling, the gold is in our web page at the ft.org and also paper and all the future information will be there too."
        ],
        [
            "So that's everything.",
            "If you have any questions, you are welcome to ask.",
            "In the second job where so the first job builds the dictionary and then in the second job you you look up at travel and for each term you need to query the dictionary for that term, right?",
            "I was wondering if the dictionary is distributed, how do you like?",
            "Do you assume that the dictionary fits in each of the machines?",
            "Yes, as we have the dictionary already compressed in a city, it allows us to for the commodity hardware machines to load it and be able to.",
            "To look up for the data.",
            "There was a work of Daniel Abadi couple of years ago doing more or less similar stuff, but would RDF 3X as a background.",
            "Can you comment what the main difference be besides the fact that you are using HTT and they're using RDF 3X?",
            "I don't really know, sorry.",
            "Thank you for the interesting talk.",
            "I'm interested in where do you see to use cases where you want to apply this kind of approach?",
            "Why are we seeing?",
            "Currently they they are their application that need to access to our DFA in memory or that needs to access to huge datasets.",
            "For example, as far as I know.",
            "Link data fragments.",
            "Use a CD S in that one of its underlying technologies, an if they need to create active of grid datasets, they could find the usual.",
            "So like the difference in the results between the synthetic data and real real datasets, and it seems that like the map reduce solution outperform for really big data and you said this is probably because the in memory implementation has to beige to the hard disk.",
            "But it also could be because they like real world data, will have more collect bias in the distribution, so some notes will be very popular, some some want which usually has big effect on the map reduce implemented like.",
            "Performance, so do you think that's a factor?",
            "Like LBM data?",
            "The distribution of the like the node distribution as the degrees of the nodes will be very similar.",
            "You won't have a hugely popular notes and some notes that hardly length while in real datasets.",
            "That would be the case.",
            "So I'm wondering if if you looked whether this this is the reason or it's just the size of the data.",
            "Well, we think it's a mostly the size of the data we cause.",
            "We make a sample in order to try to distribute evenly the data among the notes.",
            "Maybe they.",
            "The different distributions among the different kind of terms taxes is also possible influence, but within the main causes there is this like.",
            "I have a question which is related to the partitioning that you do.",
            "You said that you use some sampling techniques.",
            "Can you elaborate a bit more on what kind of sampling you use?",
            "Well, we use a tool that has all provides.",
            "It just reads the data at random random lines and generates the as much partitions.",
            "So producers we have in the cluster.",
            "OK, so this tool basically allows you to use only one sampling technique.",
            "Well, it's possible to use different sampling techniques, but we used an easy one.",
            "Yeah, OK, so that also means you did not like compare what the effect of using different sampling techniques would be now, not really OK. Other questions.",
            "I want more related to.",
            "I mean, do you?",
            "Do you have plans of extending HTTP?",
            "An towards.",
            "Features that allow you to, well, update the data in HD.",
            "Question and currently updating a city is a talent cause you see.",
            "In the description, it needs our global order on all the data, so inserting data in the in the Middle East is very hard.",
            "I know there are some words that try to.",
            "To make some other ones at the end and after some time create another one from there from scratch.",
            "But there is no solution right now.",
            "OK. Any further questions?",
            "Not out service.",
            "Thank to speak again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everybody, I'm a Masters degree students or for the University or Valladolid, Spain.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about HTML scalable solution for RDF compression with Acti MapReduce.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll start with an introduction into it.",
                    "label": 0
                },
                {
                    "sent": "I talk about the motivation and a little bit about MapReduce anixety.",
                    "label": 0
                },
                {
                    "sent": "Then I talk about our solution AC TM R&D processes it involves.",
                    "label": 0
                },
                {
                    "sent": "Hey next, I described experimental evaluation with it and finally some conclusions on possible future work.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I seen everyone here can agree that the standard text formats for every RDF are pretty barrels and it leads not only to space overheads, but also consumption overheads.",
                    "label": 0
                },
                {
                    "sent": "We have seen some dogs in previous days in which people needs to deal with RDF in memory and access data in an efficient way for example.",
                    "label": 0
                },
                {
                    "sent": "So as it is.",
                    "label": 0
                },
                {
                    "sent": "Possible solution for that kind of issues.",
                    "label": 0
                },
                {
                    "sent": "It's a binary serialization based on compact data extractors that provides not only compression but also efficient random access to triples, but it can be said that with that solution, ATT moves the scalability issues regarding volume from the consumer to the publisher because the publisher has to generate the.",
                    "label": 1
                },
                {
                    "sent": "City and it can be no trivial task because it involves heavy memory consumption.",
                    "label": 0
                },
                {
                    "sent": "So we have devised essay TMR which uses a map reduce to deal with get generation of aset from huge volumes of RDF.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a little bit about not reduce, in case anyone of you doesn't know how it works.",
                    "label": 0
                },
                {
                    "sent": "It's a framework programming model originally developed by Google and adopted by Apache and Apache Hadoop, which is a most common implementation, and in fact the one we used here is designed to process a large amounts of data in a distributed way.",
                    "label": 1
                },
                {
                    "sent": "It has a master slave architecture where the master manages all the process and maintains all the bookkeeping information and their slaves store the data in a distributed way and perform the actual work over the data.",
                    "label": 0
                },
                {
                    "sent": "It operates until the on the paradigm of moving the algorithms to the data instead of moving the data to the machines, it has to be processed.",
                    "label": 1
                },
                {
                    "sent": "In order to avoid unnecessary network traffic, nonetheless, it relies a lot on exhaustive IO operations, an intensive bandwidth usage.",
                    "label": 0
                },
                {
                    "sent": "It has a two faces.",
                    "label": 0
                },
                {
                    "sent": "The mad face in with the notes.",
                    "label": 0
                },
                {
                    "sent": "Read the data if possible in the same machine they are.",
                    "label": 0
                },
                {
                    "sent": "They're running an process it, and I need a page key value, then a framework groups the those space by key.",
                    "label": 0
                },
                {
                    "sent": "So reducers receive all the values associated with this key and then the reduce phase performs final operation and limits the.",
                    "label": 0
                },
                {
                    "sent": "Final output.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So about AZT itself?",
                    "label": 0
                },
                {
                    "sent": "It has a three part he there dictionary and triples.",
                    "label": 0
                },
                {
                    "sent": "The header contains metadata about the data set and the dictionary entry points are the most important ones.",
                    "label": 0
                },
                {
                    "sent": "The dictionary has three different sections, one for the terms who are at the same time, subjects and objects of different ripples in the in the datasets.",
                    "label": 0
                },
                {
                    "sent": "Two other ones in which we store the the terms which are only subjects or only object and the last one in which we store the predicates.",
                    "label": 0
                },
                {
                    "sent": "This is on it that way first lady cause by dividing by by role.",
                    "label": 0
                },
                {
                    "sent": "We can have a little more little ideas so we can save space and at the same time in many datasets many terms act at the same time.",
                    "label": 0
                },
                {
                    "sent": "Subjects an object, so having a section for that kind of terms, let us avoid application.",
                    "label": 0
                },
                {
                    "sent": "If not, we would have to have the intersection in the subject section in the object.",
                    "label": 0
                },
                {
                    "sent": "And then this dictionary is in practice encoded using differential compression to take advancing to take advantage of the redundance that at the half.",
                    "label": 0
                },
                {
                    "sent": "Then using those IDs and the triples are encoded.",
                    "label": 0
                },
                {
                    "sent": "Hey concept Aleasa forest of trees where the subjects are the roots.",
                    "label": 0
                },
                {
                    "sent": "The middle layer are the predicate associated with those subjects and their lives are the objects related with the base subject predicate.",
                    "label": 0
                },
                {
                    "sent": "And in practice, this concept to authorities is stored in bitmap triples where the subjects are implicit by their order and the predicates and the objects are stored in a sequence of IDs with sequence align.",
                    "label": 0
                },
                {
                    "sent": "With this sequence of ideas that relates its ID with a with a note in the upper layer.",
                    "label": 0
                },
                {
                    "sent": "So we have here an example which represent two professors working for our Department and University.",
                    "label": 0
                },
                {
                    "sent": "And we see, for example, the the Department is the one who at at the same time as subject and object.",
                    "label": 0
                },
                {
                    "sent": "So within the first section, the professors are only subject in the graph, so they are in the second section.",
                    "label": 0
                },
                {
                    "sent": "The literals are only objects, so they are intersection and in the fourth section are the predicates.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how is AZT a serialized in the classical model solution while it involves three steps in the first day we classify the RDF terms.",
                    "label": 0
                },
                {
                    "sent": "We do so by parsing data triples one by one, creating 3 hash tables, one for its role and the at the same time.",
                    "label": 0
                },
                {
                    "sent": "The tables are encoded using the temporary ID provided by.",
                    "label": 1
                },
                {
                    "sent": "Those has tables, after all the all the troubles.",
                    "label": 0
                },
                {
                    "sent": "Our past the the S or the subject object come on common terms is built with that we can proceed to build a dictionary.",
                    "label": 0
                },
                {
                    "sent": "And we see this section is sorted lexicographically and our relation between the Ivy generated in the previews in the previous step under any one right from the server is a store.",
                    "label": 1
                },
                {
                    "sent": "Then we make the differential compression to create the dictionary.",
                    "label": 0
                },
                {
                    "sent": "After that we will do the triples at ripples in the first step where had the the terms.",
                    "label": 0
                },
                {
                    "sent": "Place by temporary ID, but you use the relation created in the previous step to give them the definitive ideas.",
                    "label": 0
                },
                {
                    "sent": "Then those triples are sorted by subject predicate, an object, and with that we can just traverse them sequentially and create the the sequence IDs and under its sequences we need to create the triples so we can see we need 4 has tables, an original structure to relate the temporary ID's with affinity ones.",
                    "label": 1
                },
                {
                    "sent": "So the memory consumption is high.",
                    "label": 0
                },
                {
                    "sent": "So as the size of the data set.",
                    "label": 0
                },
                {
                    "sent": "We try to to stay alive, get bigger.",
                    "label": 0
                },
                {
                    "sent": "The generation of ATP gets harder and harder.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how is it your mother tries to tackle this issue and generate HTTP?",
                    "label": 0
                },
                {
                    "sent": "What is comprised of to to face is the first one is for dictionary encoding and the second one for Trip Listen calling.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have the triplets distributed among the then.",
                    "label": 0
                },
                {
                    "sent": "Also they might reduce cluster and the first job reach them and identify the roles of each one.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of its time, sorry.",
                    "label": 0
                },
                {
                    "sent": "So we have here, for example, the triples of the graph we saw earlier distributed amount to notes here.",
                    "label": 0
                },
                {
                    "sent": "The map phase reads the triples one by one for each triple emits three outputs in which its output is pair key value where the key is the term itself and the value is the draw it it has in the triple.",
                    "label": 0
                },
                {
                    "sent": "Then the framework groups the whole space by by the term and sent all the times where they all the pace with the same term to the same producer.",
                    "label": 0
                },
                {
                    "sent": "The reducer dense makes a composition of the roles that it receives.",
                    "label": 0
                },
                {
                    "sent": "An generates the final output.",
                    "label": 0
                },
                {
                    "sent": "For example, if you remember the apartment that was at the same time, subject and object we see in the in the first node, for example, the last triple has the Department acting as an object, so the map phase emits that.",
                    "label": 0
                },
                {
                    "sent": "That person is an object, and for example in the second node, the last triple, the Department is a subject, so they might face limits at pace again that the Department subject.",
                    "label": 0
                },
                {
                    "sent": "So all the papers related with.",
                    "label": 0
                },
                {
                    "sent": "Elements are sent to their Reducer 2 here, which generates an output setting updates at the same time, subject and object.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have the information we need to generate a city stood in a dictionary.",
                    "label": 0
                },
                {
                    "sent": "Sorry, but we need that information and globally sorted, but that is not a trivial task in MapReduce becausw for create a global sort.",
                    "label": 0
                },
                {
                    "sent": "We need to tell its reducer which pairs it has to receive in order to have a global or file or we cannot do it arbitrarily, becauses we know.",
                    "label": 0
                },
                {
                    "sent": "RDF is very is cute, so we could end up sending most of the triples to only one reducer and losing most of the benefits of using distributed computing.",
                    "label": 0
                },
                {
                    "sent": "So before the second job we use Hadoop tool to sample the data and generate what is called a partition files, which is used by the second job to know how to distribute evenly among the producers the data generator.",
                    "label": 0
                },
                {
                    "sent": "Global order",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you've seen that file?",
                    "label": 0
                },
                {
                    "sent": "The second job reads the output of the first job.",
                    "label": 0
                },
                {
                    "sent": "The map are what are known as identity mappers, which do nothing with the data just rate it's it's pair, animate it directly, then the framework.",
                    "label": 0
                },
                {
                    "sent": "Sort this data an groups in a way that it produces a sorted output.",
                    "label": 0
                },
                {
                    "sent": "I need say reducer depending on the on the terms on the rolls its term has right.",
                    "label": 0
                },
                {
                    "sent": "It's on a different output.",
                    "label": 0
                },
                {
                    "sent": "So at the end we have four different output 148 dictionary section.",
                    "label": 0
                },
                {
                    "sent": "And he globally sorted.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we perform a final operation in the master, which just traverse that information sequentially.",
                    "label": 0
                },
                {
                    "sent": "Uh, did generates the differential encoding needed for the photo dictionary.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the triplets, the first thing we do is this dictionary we have created with this compressed and we can access eights in formation randomly in an efficient way.",
                    "label": 0
                },
                {
                    "sent": "We distribute them among the notes.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the next job can read the original triples, look up for their ID in the dictionary, an replace each term with its ID.",
                    "label": 0
                },
                {
                    "sent": "So then after that it admits it to the producers who just write them on the final output.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, we have all the information we need without the triples with their ideas, but again we need it sorted globally, so we create another partition file, sampling the data passed that file to the second job.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will use this with entity.",
                    "label": 0
                },
                {
                    "sent": "My person reduces who have no internal logic and the important work is made by the framework who groups an answer.",
                    "label": 0
                },
                {
                    "sent": "They those triples in order to have a final output sorting.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at the end we have those strippers with their ideas sorted by subject, predicate, and object.",
                    "label": 0
                },
                {
                    "sent": "We can proceed to create the sequence of ideas and David sequence necessary for the triples, and we did.",
                    "label": 0
                },
                {
                    "sent": "We have a city.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have we have evaluated and how we work.",
                    "label": 0
                },
                {
                    "sent": "We have compared the generation of ATP in the classical 10 solution against the CCT MI.",
                    "label": 0
                },
                {
                    "sent": "So for the for the solution, we have used a powerful machine with 128 gigabytes of RAM for the cluster 11 machines, one for the master wait, 48 gigabytes of RAM, and then machine of commodity hardware with eight gigabytes of RAM.",
                    "label": 0
                },
                {
                    "sent": "Trying to make a fair comparison, if you Add all the gigabytes or the cluster you see that is the same as the memory that the single node has.",
                    "label": 0
                },
                {
                    "sent": "A form of reduce execution we have.",
                    "label": 0
                },
                {
                    "sent": "We use a Hadoop version one to one.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have the state it wait three real world data set linked your data DVD and Ike and also a mashup with all of them without sub to more than one billion triples.",
                    "label": 0
                },
                {
                    "sent": "We also have generated synthetic data.",
                    "label": 0
                },
                {
                    "sent": "We tell you VM from 1000 universities for up to 40,000 universities, which is more than 5 billion triples for the first data set.",
                    "label": 0
                },
                {
                    "sent": "Data sets.",
                    "label": 0
                },
                {
                    "sent": "The steps have been by 1000 University until 8000.",
                    "label": 0
                },
                {
                    "sent": "Universities.",
                    "label": 0
                },
                {
                    "sent": "From that point the datasets have been generated by steps of 4000 universities.",
                    "label": 0
                },
                {
                    "sent": "In the paper we also have the compression rates we have it, which are similar to the to the previews literature on entity.",
                    "label": 0
                },
                {
                    "sent": "But as those data sets have never been compressed until now, we think those data is interesting.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sold a valuation itself.",
                    "label": 0
                },
                {
                    "sent": "We see that for the real world datasets on DVD and Link geodata, the mono solution wins against HTML.",
                    "label": 0
                },
                {
                    "sent": "And in the case of Ike and or solution is slightly better but also their cells are almost compatible.",
                    "label": 0
                },
                {
                    "sent": "In the case on the synthetic datasets the monologue solution starts winning but the data set data size increase.",
                    "label": 0
                },
                {
                    "sent": "DMR starts being competitive up until a point or 5000 universities which is around 670 millions triples in weights as HTM R wins.",
                    "label": 0
                },
                {
                    "sent": "This is be cause they model.",
                    "label": 0
                },
                {
                    "sent": "Solution starts paginating informationen into the hard drive and so we come in slower up to the point that 7000 universities is the last day to set them on and off.",
                    "label": 0
                },
                {
                    "sent": "Solution can manage and after that it just crashes if it tries to generate ICT for larger that data set.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, ATM R is able to to minus the generation of entity in an almost linear way with UVM datasets, and it's also able to generate ATP for the massage with them that the model solution is not able to do.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as a final conclusion.",
                    "label": 0
                },
                {
                    "sent": "We have taken all the scalability issues arising.",
                    "label": 1
                },
                {
                    "sent": "The density generation for huge datasets we have done it so by moving from the modern old paradigm for to the distributed paradyne using MapReduce and we have evaluated in real world datasets an synthetic data set up to more than 5 billion triples.",
                    "label": 1
                },
                {
                    "sent": "As our future work, we will try to exploit those achievements and try to foster development of a new application that need to fast access to huge datasets and we also have some future ideas about using a city on map reduce to deal with issues relating with reasoning or specular query.",
                    "label": 0
                },
                {
                    "sent": "Unwilling, the gold is in our web page at the ft.org and also paper and all the future information will be there too.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's everything.",
                    "label": 0
                },
                {
                    "sent": "If you have any questions, you are welcome to ask.",
                    "label": 0
                },
                {
                    "sent": "In the second job where so the first job builds the dictionary and then in the second job you you look up at travel and for each term you need to query the dictionary for that term, right?",
                    "label": 0
                },
                {
                    "sent": "I was wondering if the dictionary is distributed, how do you like?",
                    "label": 0
                },
                {
                    "sent": "Do you assume that the dictionary fits in each of the machines?",
                    "label": 0
                },
                {
                    "sent": "Yes, as we have the dictionary already compressed in a city, it allows us to for the commodity hardware machines to load it and be able to.",
                    "label": 0
                },
                {
                    "sent": "To look up for the data.",
                    "label": 0
                },
                {
                    "sent": "There was a work of Daniel Abadi couple of years ago doing more or less similar stuff, but would RDF 3X as a background.",
                    "label": 0
                },
                {
                    "sent": "Can you comment what the main difference be besides the fact that you are using HTT and they're using RDF 3X?",
                    "label": 0
                },
                {
                    "sent": "I don't really know, sorry.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the interesting talk.",
                    "label": 0
                },
                {
                    "sent": "I'm interested in where do you see to use cases where you want to apply this kind of approach?",
                    "label": 0
                },
                {
                    "sent": "Why are we seeing?",
                    "label": 0
                },
                {
                    "sent": "Currently they they are their application that need to access to our DFA in memory or that needs to access to huge datasets.",
                    "label": 0
                },
                {
                    "sent": "For example, as far as I know.",
                    "label": 0
                },
                {
                    "sent": "Link data fragments.",
                    "label": 0
                },
                {
                    "sent": "Use a CD S in that one of its underlying technologies, an if they need to create active of grid datasets, they could find the usual.",
                    "label": 0
                },
                {
                    "sent": "So like the difference in the results between the synthetic data and real real datasets, and it seems that like the map reduce solution outperform for really big data and you said this is probably because the in memory implementation has to beige to the hard disk.",
                    "label": 0
                },
                {
                    "sent": "But it also could be because they like real world data, will have more collect bias in the distribution, so some notes will be very popular, some some want which usually has big effect on the map reduce implemented like.",
                    "label": 0
                },
                {
                    "sent": "Performance, so do you think that's a factor?",
                    "label": 0
                },
                {
                    "sent": "Like LBM data?",
                    "label": 0
                },
                {
                    "sent": "The distribution of the like the node distribution as the degrees of the nodes will be very similar.",
                    "label": 0
                },
                {
                    "sent": "You won't have a hugely popular notes and some notes that hardly length while in real datasets.",
                    "label": 0
                },
                {
                    "sent": "That would be the case.",
                    "label": 0
                },
                {
                    "sent": "So I'm wondering if if you looked whether this this is the reason or it's just the size of the data.",
                    "label": 0
                },
                {
                    "sent": "Well, we think it's a mostly the size of the data we cause.",
                    "label": 0
                },
                {
                    "sent": "We make a sample in order to try to distribute evenly the data among the notes.",
                    "label": 0
                },
                {
                    "sent": "Maybe they.",
                    "label": 0
                },
                {
                    "sent": "The different distributions among the different kind of terms taxes is also possible influence, but within the main causes there is this like.",
                    "label": 0
                },
                {
                    "sent": "I have a question which is related to the partitioning that you do.",
                    "label": 0
                },
                {
                    "sent": "You said that you use some sampling techniques.",
                    "label": 0
                },
                {
                    "sent": "Can you elaborate a bit more on what kind of sampling you use?",
                    "label": 0
                },
                {
                    "sent": "Well, we use a tool that has all provides.",
                    "label": 0
                },
                {
                    "sent": "It just reads the data at random random lines and generates the as much partitions.",
                    "label": 0
                },
                {
                    "sent": "So producers we have in the cluster.",
                    "label": 0
                },
                {
                    "sent": "OK, so this tool basically allows you to use only one sampling technique.",
                    "label": 0
                },
                {
                    "sent": "Well, it's possible to use different sampling techniques, but we used an easy one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so that also means you did not like compare what the effect of using different sampling techniques would be now, not really OK. Other questions.",
                    "label": 0
                },
                {
                    "sent": "I want more related to.",
                    "label": 0
                },
                {
                    "sent": "I mean, do you?",
                    "label": 0
                },
                {
                    "sent": "Do you have plans of extending HTTP?",
                    "label": 0
                },
                {
                    "sent": "An towards.",
                    "label": 0
                },
                {
                    "sent": "Features that allow you to, well, update the data in HD.",
                    "label": 0
                },
                {
                    "sent": "Question and currently updating a city is a talent cause you see.",
                    "label": 0
                },
                {
                    "sent": "In the description, it needs our global order on all the data, so inserting data in the in the Middle East is very hard.",
                    "label": 0
                },
                {
                    "sent": "I know there are some words that try to.",
                    "label": 0
                },
                {
                    "sent": "To make some other ones at the end and after some time create another one from there from scratch.",
                    "label": 0
                },
                {
                    "sent": "But there is no solution right now.",
                    "label": 0
                },
                {
                    "sent": "OK. Any further questions?",
                    "label": 0
                },
                {
                    "sent": "Not out service.",
                    "label": 0
                },
                {
                    "sent": "Thank to speak again.",
                    "label": 0
                }
            ]
        }
    }
}