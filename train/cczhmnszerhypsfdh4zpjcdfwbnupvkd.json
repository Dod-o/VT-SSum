{
    "id": "cczhmnszerhypsfdh4zpjcdfwbnupvkd",
    "title": "Introduction to feature selection",
    "info": {
        "author": [
            "Isabelle Guyon, Clopinet"
        ],
        "published": "July 4, 2007",
        "recorded": "July 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Preprocessing"
        ]
    },
    "url": "http://videolectures.net/bootcamp07_guyon_ifs/",
    "segmentation": [
        [
            "The sum of you.",
            "So let me just summarize without the essential things to remember and all the details you don't need to remember.",
            "Eventually it's good to have the keywords and you're here so that you know you've heard of them.",
            "You know this is related to the problem of overfitting.",
            "But the essential thing to remember is that we want to have models that are not too complex, but we don't want to perfectly explain the training data.",
            "And we all we care about is making predictions on future data.",
            "In the model selection problem.",
            "If we have many models, we can arrange them in order of complexity and we can try.",
            "Try various complexity models and find the model.",
            "The model complexity which is best suited to the problem using cross validation.",
            "Or we can use ensemble methods and trying many models and average their decisions and this will allow us to reduce the problem of variance.",
            "Now in this new lecture here, we're not diving into the problem for which you know I was invited to lecture here, which is the problem of feature selection.",
            "So everything up tonight it's kind of an introduction to this problem of feature selection."
        ],
        [
            "So first some notation examples.",
            "So in many problems that have been arising recently, there are thousands to millions of low level features.",
            "And this arises, for example in the spectrometry.",
            "Or in biology they have been your DNA microarray data and which you have.",
            "10s of thousands of low level measurements, each measurement corresponding to the abundance of.",
            "M RNA or protein inserra.",
            "And we might want to be able to diagnose disease from all these tiny measurements that were making each each of which by itself is not very predictive.",
            "But if you consider them together then they help you diagnose disease.",
            "You also have high resolution images or video and that case you have, you know huge quantities of data that come originally with very low level features and you might want to do first or some kind of feature extraction.",
            "You still sometimes left with many many features.",
            "So we want to decide which of these features are going to be best for making predictions.",
            "So we have originally this big matrix of features and an example, and we want to reduce it to a much smaller matrix with only N prime features."
        ],
        [
            "The first example I'm thinking here is an example of leukemia diagnosis that appeared in 1999 in science and the authors were classifying.",
            "The abundance of the expression of some genes with some macro data.",
            "And this representation here is really a representation of the data matrix in terms of a heat map, and we'll see a lot of those.",
            "Each coefficient of the matrix is represented by a little box an if the coefficient is positive, the boxes in the reddish color, if it's negative, it's in the blueish color, so the more red, the more positive and more blue, the more negative.",
            "And the authors here have selected a number of features that are most correlated to a target value.",
            "So they have two types of leukemia, the AML type and the LL type corresponding to two classes.",
            "The plus one class and the minus one class.",
            "So this here represents the target vector Y.",
            "And.",
            "This here represent columns of the data matrix.",
            "And they have selected those columns of the data matrix which correlate most with the target vector or anti correlate mode.",
            "So this is the negative target vector.",
            "So you also have some.",
            "Collins of the data matrix that correlate well with the negative target vector.",
            "And those are the features that they selected as being most predictive of whether you have an LL type or an AML type of leukemia.",
            "This message of future selections based on correlation with the target are called univariate because each variable, so each column of the matrix here represents one variable.",
            "The variable in that case is the abundance of a certain M RNA.",
            "So the activity of a certain gene in the blood.",
            "And so each feature which is each activity of Gina.",
            "In the in the selected features here are correlated with the target, so individually you can use.",
            "You know each of these genes to predict disease.",
            "But you see that there is some noise.",
            "It's not.",
            "It's not very perfectly right here and not very perfectly blue here.",
            "So you might be better off voting over a number of genes, and this is what the authors of that paper did.",
            "They voted over now."
        ],
        [
            "Jeans.",
            "Now you don't need to limit yourself to only two classes.",
            "In this example of prostate cancer diagnosis.",
            "Also from DNA microarray data, we looked at gene expression in order to predict whether you had one of several types of prostate disease.",
            "So there was great three prostate cancer grade 4 prostate cancer and benign prostate hyperplasia.",
            "And using these three genes, one could make a separation.",
            "Pretty nice separation between three clusters of types of prostate tissues.",
            "And other authors have also using microarray data showed that you can do effective gene selection.",
            "So genes in that case."
        ],
        [
            "Features.",
            "And here we represent accuracy as a function of the number of genes and and some people doing the characters.",
            "I've asked me.",
            "Do you see an optimum when you you vary the number of genes?",
            "Well, here the authors.",
            "I've tried a number of strategies on how to build a multi class classifier and so some of them so this the one at the top is a support vector machine using ovaa which is 1 versus the other multiclass classification.",
            "And you have various other possibilities and as you can see on this graph, when you vary the number of genes, sometimes you go through an optimum and sometimes you actually level off or you keep increasing the accuracy as you increase the number of features.",
            "Or jeans.",
            "So we will have to find a way of determining what is the optimum number of genes or features.",
            "For some practical purpose, sometimes even though using all the features gives you the best performance, you might want to use fewer features because only at the expense of, say, a 10% degradation of performance, you can have harder or 1000 times less features, and this may mean you know more efficacy or easier explanation of date."
        ],
        [
            "On this other example, we're looking now the problem of drug discovery.",
            "In this case, people have been screening compounds for their ability to bind to a target site, which is a trombone.",
            "It's a key receptor in blood clotting.",
            "And the training set consisted of a number of compounds that were active in a number of inactive compounds and we had to predict for the test set which compounds were going to be binding.",
            "The features used were some very low Level 3 dimensional properties of the molecule.",
            "So our presence you know of a Groupement which is oriented in a certain way, etc.",
            "And in that case, again, we can see as we vary the number of features, then the success rate of prediction goes through an optimum.",
            "And in fact, you know of these hundreds of thousands of features here.",
            "The Axis you know is cut at 100, but it goes up to 100,000.",
            "You have if you use only 20 or so, you have the optimum."
        ],
        [
            "Here's another example of feature selection in a completely different domain.",
            "It's in text filtering.",
            "The authors using some non corpora, the Reuters Newswire, the 20 newsgroup and the data set of web pages and a very simple representation of data which is called the bag of word.",
            "Perform these experiments.",
            "So what bag of word means is that you're representing a document by features that are frequencies of words in that document.",
            "So here you have about, you know 100,000 features, so so 100,000 words were retained, and of course it's a very sparse representation, because many of the words never occur in some documents.",
            "By performing feature selection there obtains some of the interesting results.",
            "Here for these are for the newsgroups.",
            "So in the newsgroup out, that isn't the most representative features.",
            "Artesa, Mattison, mortal morality without the three first ones, you know for com dot graphics.",
            "And you can see that for space and it also makes sense you have space.",
            "NASA in orbit for religion, God, Church and sin and politics of the Middle East.",
            "You have Israel, Armenian and Turkish.",
            "And and for you know this this other newsgroup which is slightly different from this one has a other keywords, but this allows you to characterize groups of text in terms of the most prominent keyword."
        ],
        [
            "Here is another application of face recognition.",
            "The authors of that paper tried to apply two different feature selection algorithms to determine in the image what are the parts that are most useful to recognize faces, and here it was a discrimination simply between male and female.",
            "With the relief algorithm, if they used 100 features, they found this part of the neck most you know important.",
            "With 500 features, they started having some part of the eye and with thousand features you see that there is, you know, more.",
            "There are more details that are being picked up by the algorithm.",
            "Releases an algorithm that does not filter redundancy in the feature set.",
            "So as you can notice that features are selected on both sides of the face, even though they might be redundant because the face is symmetric, so they propose another algorithm called Simba, which eliminates redundancy and so that algorithm picks up features only on the half of the face and does not repeat them on the other half."
        ],
        [
            "So let's now dive into, you know, after we reviewed some of the examples of feature selection, let's dive into the algorithm themselves.",
            "And the first I'll introduce some nomenclature.",
            "So we first so that there is a.",
            "The simplest methods are so-called univariate methods and they consider one variable at a time and see whether that variable is predictive of the target.",
            "On the contrary, multivariate methods consider subsets of variable that together predict well the target.",
            "People in this field also make a distinction between filter methods and wrapper methods.",
            "Filter methods, rank features or subsets of features independently of the predictor of the classifier.",
            "They have criteria that work on their own.",
            "Whereas wrapper methods use the classifier or their learning machine to assess the features or the feature subsets."
        ],
        [
            "So first we're going to review some very simple unique."
        ],
        [
            "Various filter methods.",
            "So what is individual feature relevance?",
            "So again, you know in the in an example of disease diagnosis you may have features characterizing a patient, his weight, his age, number of kids, medical history, and some measurements made in blood.",
            "And let's say that we have here 1 feature that is the age.",
            "And we want to discriminate between patients who are at risk of heart disease or patients who are healthy.",
            "Well, if it turns out that we have the distribution of the two patients which are completely overlaid.",
            "Then we're going to say that that feature is useless.",
            "We can't make a separation between the two populations of interest based on that feature, so there is all reason to discard it.",
            "Formally, if the joint probability of the variable of interest and the target variable.",
            "Is equal to the probability of the individual variables.",
            "Then there is independence between the variable in the target.",
            "Or equivalently, if we use Bayes rule, we can also say that the probability of the variable given the target is equal to the probability of the variable.",
            "So essentially ignoring the label.",
            "Gives you the same distribution.",
            "Here you have, you know in red you have the conditional density for y = 1.",
            "And the conditional density for Wycombe minus one.",
            "And they are almost identical.",
            "So if you remove the label and use the conditional density and use the density not condition, you get the same thing as the conditional density.",
            "So the 3rd way of expressing that is also that the P of XI given like 01 is also equal to PLX.",
            "I give a 1 equal equal minus one.",
            "So how do we express that in terms of?"
        ],
        [
            "Ineffective algorithm.",
            "People have been defining many criteria of relevance.",
            "In order to determine which variables one should retain.",
            "One simple criterion of relevance is just to use the displacement between the two distributions.",
            "So imagine that instead of what we saw in the previous slide, we now see that we have the distribution of.",
            "You know the two populations they're not overlapping anymore.",
            "We can measure the difference between the means.",
            "And we need to normalize them by the standard deviation so that we get a criterion which is independent on the scale of the feature.",
            "Other criteria include the area under the Roc curve for single features.",
            "So people familiar with classification problems know that we were using this our C criterion.",
            "These RC curve to measure classification accuracy.",
            "If you're not familiar with that, it doesn't matter, but in essence, what's happening is that we're taking individual features as classifiers.",
            "You can take an individual feature and set a threshold.",
            "On the value that this feature is taking, if it's a continuous feature, for example, if it's age, you can vary a threshold on the age and use the age to make your prediction.",
            "If you do that, you can.",
            "You're monitoring sensitivity and specificity by varying these threshold, the sensitivity is the error rate on the positive class and the specificity the error rate on the negative class.",
            "So assume that you're trying to predict heart disease as a using age as a predictor.",
            "If you put a threshold which is very very high, and let's say at 90 year old and saying that everybody.",
            "90 you're going to predict is at risk of heart disease, and everybody under 90 is not at risk of heart disease.",
            "Well, you're making your going probably to make very few errors on the negative class, so you don't make very few going to very few decisions in which you're going to say that a person is not at risk of heart disease actually is, but on the other hand, if you vary the threshold and put it too low, you would make more errors on the positive class.",
            "So if you vary that threshold.",
            "You get the tradeoff between sensitivity and specificity, and this is what that curve is and the area under the Roc curve.",
            "Is a measurement that people use in the classification field to determine how good your classifier is and it can also be used to determine how good your individual feature is as a predictor.",
            "Because an individual feature is like a tiny classifier.",
            "If you very."
        ],
        [
            "This threshold.",
            "Alright, so now let's go back to our example of leukemia that we had.",
            "One of the first slides.",
            "This is exactly what the authors did.",
            "The final criterion that they called the signal to noise ratio criterion.",
            "And that is the difference between the means of the positive and the negative class, rescaled by the average standard deviation.",
            "As it turns out, this criterion is very similar to just making the DOT product between X&Y.",
            "Once you have already standardized the data.",
            "So if instead of computing this criterion you first standardize the data so you first divide all the features by the standard deviation.",
            "Then you have already that normalization performed.",
            "And because we include Y as plus or minus one, if you have the same number of examples in the positive class and in the negative class, then performing the DOT product between X&Y.",
            "Is equivalent to making the subtraction between these.",
            "Some of the examples of the positive class and the sum of examples of the negative class.",
            "So it's similar to what you have, you know, divided by the total number of examples.",
            "It's similar to subtracting the mean of one class by the mean of the other class.",
            "Hence you know that there is high similarity between this criterion and the person correlation coefficient.",
            "We will see you in the practical class that there are people have been defining many criteria that seemingly are different, but in reality compute quantities that are very similar.",
            "And when you look at the ranking that the features, the ranking of the features that are obtained with these different criteria, there are virtually."
        ],
        [
            "Identical.",
            "Now let's go back to this notion of independence.",
            "First I told you you know about independence, and now I'm talking to you about.",
            "You know this criteria that probably have nothing to do with this independence.",
            "How do we measure dependence?",
            "One way of measuring dependencies by mutual information.",
            "If independence is defined by the equality of P of X&Y, with P of XPN or Y.",
            "We need to measure.",
            "We need to find a way of measuring the distance between this quantity and this quantity.",
            "To determine dependence.",
            "And this is what is done with mutual information.",
            "It's also the callback library divergent between the joint density and the product of the individual densities.",
            "And only this is the log of the ratio of these two quantities.",
            "And the expected value is taken over the distribution of P of X&Y.",
            "So while not using, you know mutual information as a criterion for ranking features.",
            "Well, actually, why not?",
            "You know you can use it and people have done that and have been very successful with that.",
            "The only difficulty is that it's hard to estimate densities or probability distributions, so if you have.",
            "Binary variables then it's usually feasible and gives good results.",
            "If you have continuous variables, you first probably have to discretize or you have to go through some estimation of density which."
        ],
        [
            "Is non trivial.",
            "Now there is a correspondence between richer information and correlation, so remember this very simple signal to noise ratio criterion that was used in the lab paper.",
            "Is similar to the person correlation coefficient.",
            "As it turns out, correlation is a measurement of linear dependency, whereas mutual information is a measurement of dependency that does not preclude of the type of dependency that you have, whether it's linear or not.",
            "Of course you have a price to pay for that in terms of complexity of estimation.",
            "So in this like here I'm showing you know density for X, for density for Y.",
            "And these are, you know, the marginal densities and on these boxes I'm showing two different joint densities.",
            "So in this joint density here, there is really a structure.",
            "And this joint density here is, you know, pretty much up hazard.",
            "From the point of view of the correlation coefficient.",
            "There is not much difference.",
            "There is very little correlation between X&Y on this plot here and very little correlation between X&Y on this slide here, right, because correlation measures linear dependency, you cannot make your line.",
            "That explain explains why.",
            "Well, you know why from X, either here nor here, but with respect to mutual information is much more mutual information in this graph then in this graph, because you can find a good explanation of why given X.",
            "In this graph, whereas here you basically cannot."
        ],
        [
            "And in fact, for the Goshen distribution, it can be shown that there is a formal relationship between mutual information and correlation given by this formula.",
            "But for other cases, the."
        ],
        [
            "Relationship is not that trivial.",
            "So many, many criteria of univariate feature selection and in the chapter three of the book on feature extraction that I mentioned in the introduction.",
            "You find tons of those, and here's the table.",
            "So you can refer to that if you want."
        ],
        [
            "More details but now.",
            "I would like to mention one additional subtlety of these univariate criteria.",
            "People in the literature have been using them a lot because they are related to a methods that have been used a lot in statistics to determine relevance.",
            "Relevance of variables.",
            "In statistic, people use statistical tests.",
            "To determine relevance.",
            "An for example, the T test.",
            "That will be measuring the whether there is a significant difference between two distributions that are assumed to be normal.",
            "The null hypothesis is that there is equality between the means.",
            "And the T statistic is such that if the null hypothesis is true, then.",
            "This distribution, that is the difference between the means rescaled by a standard deviation, be scaled by the standard error.",
            "And this this term here, you know, is the number of the positive examples and the number of the negative examples.",
            "This follows the student distribution with N + -- -- 2 degrees of freedom.",
            "So you can carry out that test and then if you want a certain risk value.",
            "That this particular feature is a really relevant feature.",
            "Then you can use that test to make that decision.",
            "Or you can use the P value of the test.",
            "As a ranking criterion.",
            "So the advantage of using the P value rather than just statistic like that is that the P value is in a way normalized and you'll be able to compare P values across different statistical tests.",
            "So as you can notice here, this T test statistic is very similar to the signal to noise ratio statistic that was introduced at over.",
            "You know, uh-huh Ristic motivation by the authors of the Colored Paper.",
            "In the practical class, will go over.",
            "You know some ways of computing.",
            "P values even if you don't know the distribution.",
            "Because for most cases, and we're not most cases that a wide variety of cases people have come up with heuristic ways of ranking features that work very well in practice.",
            "But I'm not populated, don't correspond to any known.",
            "Statistical distribution, yet it's often useful to compute P values in order to know what is the false positive rate.",
            "So the rate of features that are falsely thought of being relevant, so we'll go over that in the practical class and see how we can, you know, calculate false positive rates even if we don't have a tabulated."
        ],
        [
            "Distribution.",
            "So in chapter two of the book, you have many different statistical tests and the in the more general case here is this.",
            "How this works.",
            "You define a null hypothesis which basically tells you that X&Y are independent.",
            "And then you define a relevance index.",
            "And that's equivalent to your test statistic.",
            "Then you you derive from running, you know the test the P value.",
            "And this P value is equivalent to the false positive rate, so the fraction.",
            "Features that are thought of being relevant.",
            "In the the North distribution.",
            "So what happens is that in the framework of statistical tests.",
            "You take this.",
            "No, I prothesis that corresponds to the feature is irrelevant, and that corresponds to what people call the null distribution.",
            "So distribution of irrelevant features.",
            "And what you're trying to figure out is for a given value of your relevance criterion.",
            "What is the fraction of the irrelevant features that are selected?",
            "So here is you know the the reasoning behind that.",
            "Let's take some garbage feature and let's try to apply or relevance criterion to that.",
            "Those garbage features.",
            "Are we going to select some features out of this garbage features by applying this relevance criterion?",
            "If yes, then that means that we're going to have some false positive in our real problem, right?",
            "You know real problem?",
            "We don't want to select garbage features.",
            "We want to separate the good ones from the bad ones.",
            "So in order to test how good.",
            "Or method is we're going to purposely put some garbage pictures into our methods and see how many of the garbage is selecting and this is what this is doing.",
            "So now this is going to be maybe a little bit into some technical details, but there's a problem with this methodology.",
            "The problem is that if you apply multiple times, the statistical tests, then you're running into a problem called multiple testing, and the reason is very similar to what we have.",
            "You know in the problem of overfitting or the problem of model selection.",
            "If you have many, many models right to choose from, there is a very high chance that you're going to pick one that's going to fit your data perfectly.",
            "Like we said before.",
            "Well, for feature selection we have the same thing, right?",
            "If you have many many features and for each feature you're repeating this test, is this a good feature?",
            "Is this that feature well?",
            "In the end, just out of sheer luck is very probable.",
            "You're going to find out in the end of feature that's going to pass your test, but this is just by chance.",
            "So if you repeat test multiple times, your P value actually is wrong and it should be corrected.",
            "This is what the Bonferroni correction means.",
            "Bonferroni Correction is a first order correction that just replaces the P value by end times the P value, and being the number of times to perform the test.",
            "So people have been also introducing another notion called the false discovery rate, and that alleviates this problem of multiple testing.",
            "Instead of calculating the false positive rate, which is the fraction of features that you select when you inject garbage features.",
            "The false positive the forces cover rate is the fraction of bad features that you have among your selected features.",
            "So say you are taking a big bag of features you have good and bad ones in them.",
            "And you select a certain chunk of them.",
            "The full discovery rate is the fraction of bad guys you have in this chunk that you've selected.",
            "And it can be shown to be bounded by the false positive rate times the number of features over the number of selected features.",
            "So don't you need to go into details about that?",
            "It's very simple to show you can do that by yourself.",
            "Um?",
            "The reason why this is interesting is that this false positive rate is provided by the P values of the test.",
            "So you can compute the false discover rate simply from the P value.",
            "And the probe method that will go over, you know, in the in the class.",
            "In the practical class.",
            "Consists in the approximating the false positive rate.",
            "By the fraction of.",
            "Of guys that you select when you just take complete garbage features.",
            "So the problem method consists in taking your original data, scrambling it completely.",
            "You take your original matrix, you scramble it and then now you simulate you know garbage features out of this scrambled matrix and you apply your feature selection criterion.",
            "And you look at what is the fraction of.",
            "Other features that you have selected and that match the criterion that you've decided so, for example, exceeds a given threshold of your selection criterion.",
            "So once you've done that, you can you know.",
            "Plugging this into this formula and obtain the false discovery rate so will go through these exercises.",
            "The goal of you know this whole thing again is to determine what is the fraction of false positive that you have in the selected features.",
            "This is an important you know.",
            "For example in the medical domain.",
            "If you.",
            "If you're using feature selection for drug discovery or for finding some drug targets.",
            "If each feature represents a drug target or or candidate drug.",
            "Then you selecting features, I'll have hundreds or thousands of candidates and you retain 100 that are good candidates.",
            "It's interesting to know among these hundred good candidates.",
            "How you know what is an estimate of the fraction of those guys which are actually bad guys, which are actually completely meaningless and that were selected accidentally?",
            "Just because you know you have some limitation of your accurate statistical accuracy of your predictive method."
        ],
        [
            "Um, in the next section.",
            "We're going to go towards more complicated feature selection methods that you consider several variables at a time, so up to now, we went through the simple exercise of taking each individual variable and seeing how productive they are of the target.",
            "Now, how?"
        ],
        [
            "You know, considering the subsets of variables.",
            "First, why do we care?",
            "Why should we bother considering multiple variables at a time?",
            "It's more complicated.",
            "It's more computationally expensive, and it's also more statistically difficult to do.",
            "So the reason the justification it lies in, you know these two figures that are scatter plots.",
            "And you have two.",
            "Distributions of patients.",
            "So patients with one type of disease.",
            "Separation between these two populations.",
            "And we can also look at what happens in projection on the axis.",
            "So considering one variable at a time, it's it's a 2 dimensional problem.",
            "But we can project it in one dimension.",
            "And if we do so, if we project everything onto the age variable.",
            "We see that age reasonably well separates the two populations.",
            "If we project here on say, wait, I mean I'm making some arbitrary example, then we see that there is overlap between the two distribution and so this variable here alone by itself is not a good choice to separate the two populations where this variable here is a good choice.",
            "This is what would be the result of a univariate feature selection.",
            "We would decide pick this variable, throw away this one.",
            "But now if you look in two dimensions, you can see that there is a better separation of the two populations if you add this variable.",
            "This variable, which alone is irrelevant when taken with another one can help you perform a better separation.",
            "Now consider the second example.",
            "Here it's even more dramatic.",
            "Because now you have two variables that alone.",
            "Do not separate the data.",
            "But when you consider them together, they give you a very good separation of the data, even though it's not a linear separation.",
            "In this case, it's a nonlinear separation because you have each class is composed of two clusters and they are arranged in this very particular way which is known you know, and as to the chess board problem.",
            "It's one of you know, the next year machine learning example that's always quoted, because when you look in projection you can't separate.",
            "The examples.",
            "And so multivariate methods are going to allow us to identify features that together are predictive, but individually or not."
        ],
        [
            "So.",
            "We're now opposing filters versus wrappers.",
            "The main goal is going still to be ranking subsets of useful features.",
            "The filter approach consistently considering all the features, putting them into a box called filter out, putting some subset of good features, and then using them for making prediction.",
            "Whereas the wrapper method consider the features generates some subsets of candidate features.",
            "Passes them to the predictor.",
            "The predictor you know is trained, and we compute the prediction power of the feature subset and then eventually we guide the search for a new feature subset and we tried many times until we find an optimum feature subset.",
            "Now.",
            "What connects this problem with the problem of the previous lecture is that there is a danger of overfitting.",
            "If you're considering many subsets of features.",
            "It is like we had in the previous lecture when we considered many learning machines or we considered highly complex learning machines.",
            "The complexity of our learning problem in that case is going to be related to the number of feature subsets that we are considering in our search.",
            "So we run at risk of overfit."
        ],
        [
            "Thing.",
            "In the chapter three of the book, there are many research strategies that are considered, and I'm mentioning here a few just to give you, you know, a broad overview.",
            "There are so called forward and backward methods, so forward selection methods consistent starting with an empty set of feature and progressively adding features backward elimination, you start with the full set, then you progressively eliminate.",
            "Then there is deep search where you consider a path and keeping all the time K candidate features.",
            "There's the generalized sequential forward selection.",
            "When N minus query and minus K features are.",
            "I left sorry there is a.",
            "There's a you know and a problem in the typesetting here.",
            "So anyways, it requires.",
            "And minus K to the G chosen minus key GI.",
            "Think here completely Seawell trainings so more trainings are done each step but there are fewer steps than in then in the simple sequential forward selection.",
            "Then there is the PTA method.",
            "So it's where you put L and take away R. So you go forward a certain number of steps L and then you go backwards certain number of steps.",
            "And then finally there is the floating search, which I think is the most popular search methods right now for wrapper.",
            "And what you do is that you either start going forward or start going backward in your search and you go forward until you're not improving anymore and then you start going backward until you're not improving anymore, and so on and so forth, and you oscillate between going forward and backward.",
            "In your search.",
            "And of course, you know I didn't mention it, but you could also do an exhaustive search so you could generate all possible subsets of variables and investigate them all, and you can do genetic algorithms.",
            "You can do any type of search you want."
        ],
        [
            "The problem being that if you search harder.",
            "You're going to incur some complexity penalty.",
            "In this graph, I'm representing the case in which you have only four features.",
            "So these are represented by these four numbers here.",
            "And this is the state space of all possible feature subsets.",
            "So 1 means the feature is present and zero the feature is absent.",
            "So we have also here all the possible cases of feature present feature absent for features.",
            "And you go from one note to the next by just removing or adding one feature.",
            "So this is the elementary step that you performing in searching in this space.",
            "And there are two to the end possible feature subsets for N features."
        ],
        [
            "Now this is this highly complex and this can be replaced by."
        ],
        [
            "Yeah, simpler methods called embedded methods or nested subset methods, so I'm illustrating this with a simple example.",
            "Where you input all the features you train a support vector machine, which is a special case of classifier.",
            "Eliminate useless features.",
            "The ones that don't are not needed to make predictions.",
            "Examining whether the performances are degraded or not, an.",
            "If yes, you continue eliminating.",
            "If no, you stop and to distract you a little bit from the boring lecture and making little animation that's showing you how this works is just for marketing people to amuse them.",
            "OK."
        ],
        [
            "Those.",
            "We're going to more details about, you know embedded methods in the next lecture.",
            "How do you are you going to assess the validity of the subsets that are features that we're going to obtain?",
            "Well, it's very similar to what we did in the previous lecture.",
            "We also going to split the data into training, validation, and test set.",
            "We can also do cross validation, but here to simplify the argument, we're going to assume like we assumed here that we have a lot of data and we don't need to do cross validation.",
            "We can just split into training validation.",
            "And they said one split.",
            "And so in red is the training in blue validation an in green the test?",
            "So for each feature subset, we're going to train a predictor on the training data.",
            "And then we're going to select the feature subset which performs best on the validation data.",
            "And we can, as I said, you know, average over many splits if we need to reduce variance by performing cross validation.",
            "No.",
            "When we finish that, we can take the result of feature selection."
        ],
        [
            "And test on new test data.",
            "This scheme allows us to estimate the complexity of the feature selection process as we had before you remember that we had, you know, in the guaranteed risk framework we had the guaranteed risk, which is a allows us to bound the expected risk the real race came.",
            "In the dragon of Learning machine, people often called the true risk.",
            "The generalization error.",
            "So the error that you're going to make if you have infinite amount of data.",
            "And in the previous lecture we bounded the true risk of the generalization error by the training error plus some complexity term.",
            "Now we're looking at the second level of inference, so feature selection.",
            "Is very similar to hyperparameter selection, so you remember in the previous lecture actually one of the hyperparameters I had told you about with was the Sigma.",
            "Was in fact the number of features, so feature selection is in a way hyperparameter selection, so everything takes place here.",
            "The training set for feature selection is the validation set.",
            "Right, and the training.",
            "The two training part here is to train the parameters of the models.",
            "And so we can again at this new level of inference, because that's the second level of inference, because it's using validation data.",
            "We can similarly apply generalization bounds.",
            "And now what was sticking the place in the bound of the training error is the validation error, and we have an extra complexity term.",
            "Right?",
            "So.",
            "We're going to look at what is this complexity term here, and this M_2.",
            "Here is the number of is the dimension of the validation set.",
            "In this table I'm showing you how many, how many subsets of features we are considering.",
            "If we do exhaustive search wrapper.",
            "So if we consider all possible subsets of features, then we considering 2 to the power and feature subsets.",
            "If we do, if we're doing other methods like forward selection, rapper Type, I haven't proved that to you, but will do that probably as an exercise in the exercise class, or I'll explain it to you in the next lecture.",
            "I think in the next lecture I'll show you that.",
            "So we incur these number of feature subsets and if we do simply feature ranking.",
            "So for example if we do like you need very univariate methods and we rank the features according to a certain criterion then we considering only an.",
            "Feature subsets.",
            "So here's the feature ranking thing.",
            "You know.",
            "The first feature is the one which is the most productive.",
            "The second feature, the second best, etc, etc.",
            "You can use feature ranking for determining feature subsets because you can take the first subset is this one, the second subset is this one etc.",
            "You increase subsets.",
            "Using the feature rank and in that case you considering N. If you have a total of N features.",
            "You're considering an feature subsets.",
            "So this is one you know, one way of doing.",
            "Selections of feature subsets just varying the number of features in this way.",
            "In that case, what complexity do you incur?",
            "Well, in the statistical learning theory, if you are trying to pick.",
            "Learning machines among a finite number of learning machines.",
            "It is easy to compute this complexity term here.",
            "The complexity is always proportional to either the log of the number of machines that you tried or to the square root of the log, depending on you know whether or not you have zero here in this first part.",
            "But roughly speaking, if you just consider the log in, then the complexity you incur for an exhaustive search wrapper is N. Whereas here for these other methods it's only log of N. So you see, that's pretty dramatic, right?",
            "All the methods that are very intensive in search.",
            "Message that try all possible subsets or methods that try a lot of subsets, like genetic algorithms or simulated annealing that do a very exhaustive search.",
            "For example, try just you know a fraction of the total number of subsets, but it still scales.",
            "You know with with still scales exponentially with the number of features.",
            "All of those are pretty complex and will have you know a big epsilon term here.",
            "Where all the methods that are like forward selection methods or or feature ranking methods.",
            "They only incur log event in complexity.",
            "So they are going to overfit much less, and this is the message here.",
            "The message here is that if you have a large number of features shy away from trying all possible subsets.",
            "Of course you probably won't do that anyways because it will be computationally too expensive.",
            "But aside from being too computationally expensive, it's also very dangerous statistically.",
            "That is dangerous in terms that you will be likely overfitting.",
            "Likely you will be selecting the feature subset that explains well your training data, but it's going to perform poorly on future data.",
            "OK, so this just shows.",
            "Two cases like before, you know you had this tradeoff between the first term here is the validation error, and here is the complexity of feature selection and you have an optimum in between very similar to what we had.",
            "You know, in the first lecture.",
            "And if you have less complex methods.",
            "You may you may do worse in terms of cross validation error.",
            "But nevertheless."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The sum of you.",
                    "label": 0
                },
                {
                    "sent": "So let me just summarize without the essential things to remember and all the details you don't need to remember.",
                    "label": 0
                },
                {
                    "sent": "Eventually it's good to have the keywords and you're here so that you know you've heard of them.",
                    "label": 0
                },
                {
                    "sent": "You know this is related to the problem of overfitting.",
                    "label": 0
                },
                {
                    "sent": "But the essential thing to remember is that we want to have models that are not too complex, but we don't want to perfectly explain the training data.",
                    "label": 0
                },
                {
                    "sent": "And we all we care about is making predictions on future data.",
                    "label": 0
                },
                {
                    "sent": "In the model selection problem.",
                    "label": 0
                },
                {
                    "sent": "If we have many models, we can arrange them in order of complexity and we can try.",
                    "label": 0
                },
                {
                    "sent": "Try various complexity models and find the model.",
                    "label": 0
                },
                {
                    "sent": "The model complexity which is best suited to the problem using cross validation.",
                    "label": 0
                },
                {
                    "sent": "Or we can use ensemble methods and trying many models and average their decisions and this will allow us to reduce the problem of variance.",
                    "label": 0
                },
                {
                    "sent": "Now in this new lecture here, we're not diving into the problem for which you know I was invited to lecture here, which is the problem of feature selection.",
                    "label": 0
                },
                {
                    "sent": "So everything up tonight it's kind of an introduction to this problem of feature selection.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first some notation examples.",
                    "label": 0
                },
                {
                    "sent": "So in many problems that have been arising recently, there are thousands to millions of low level features.",
                    "label": 1
                },
                {
                    "sent": "And this arises, for example in the spectrometry.",
                    "label": 0
                },
                {
                    "sent": "Or in biology they have been your DNA microarray data and which you have.",
                    "label": 0
                },
                {
                    "sent": "10s of thousands of low level measurements, each measurement corresponding to the abundance of.",
                    "label": 0
                },
                {
                    "sent": "M RNA or protein inserra.",
                    "label": 0
                },
                {
                    "sent": "And we might want to be able to diagnose disease from all these tiny measurements that were making each each of which by itself is not very predictive.",
                    "label": 0
                },
                {
                    "sent": "But if you consider them together then they help you diagnose disease.",
                    "label": 0
                },
                {
                    "sent": "You also have high resolution images or video and that case you have, you know huge quantities of data that come originally with very low level features and you might want to do first or some kind of feature extraction.",
                    "label": 0
                },
                {
                    "sent": "You still sometimes left with many many features.",
                    "label": 0
                },
                {
                    "sent": "So we want to decide which of these features are going to be best for making predictions.",
                    "label": 0
                },
                {
                    "sent": "So we have originally this big matrix of features and an example, and we want to reduce it to a much smaller matrix with only N prime features.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first example I'm thinking here is an example of leukemia diagnosis that appeared in 1999 in science and the authors were classifying.",
                    "label": 0
                },
                {
                    "sent": "The abundance of the expression of some genes with some macro data.",
                    "label": 0
                },
                {
                    "sent": "And this representation here is really a representation of the data matrix in terms of a heat map, and we'll see a lot of those.",
                    "label": 0
                },
                {
                    "sent": "Each coefficient of the matrix is represented by a little box an if the coefficient is positive, the boxes in the reddish color, if it's negative, it's in the blueish color, so the more red, the more positive and more blue, the more negative.",
                    "label": 0
                },
                {
                    "sent": "And the authors here have selected a number of features that are most correlated to a target value.",
                    "label": 0
                },
                {
                    "sent": "So they have two types of leukemia, the AML type and the LL type corresponding to two classes.",
                    "label": 0
                },
                {
                    "sent": "The plus one class and the minus one class.",
                    "label": 0
                },
                {
                    "sent": "So this here represents the target vector Y.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This here represent columns of the data matrix.",
                    "label": 0
                },
                {
                    "sent": "And they have selected those columns of the data matrix which correlate most with the target vector or anti correlate mode.",
                    "label": 0
                },
                {
                    "sent": "So this is the negative target vector.",
                    "label": 0
                },
                {
                    "sent": "So you also have some.",
                    "label": 0
                },
                {
                    "sent": "Collins of the data matrix that correlate well with the negative target vector.",
                    "label": 0
                },
                {
                    "sent": "And those are the features that they selected as being most predictive of whether you have an LL type or an AML type of leukemia.",
                    "label": 0
                },
                {
                    "sent": "This message of future selections based on correlation with the target are called univariate because each variable, so each column of the matrix here represents one variable.",
                    "label": 0
                },
                {
                    "sent": "The variable in that case is the abundance of a certain M RNA.",
                    "label": 0
                },
                {
                    "sent": "So the activity of a certain gene in the blood.",
                    "label": 0
                },
                {
                    "sent": "And so each feature which is each activity of Gina.",
                    "label": 0
                },
                {
                    "sent": "In the in the selected features here are correlated with the target, so individually you can use.",
                    "label": 0
                },
                {
                    "sent": "You know each of these genes to predict disease.",
                    "label": 0
                },
                {
                    "sent": "But you see that there is some noise.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not very perfectly right here and not very perfectly blue here.",
                    "label": 0
                },
                {
                    "sent": "So you might be better off voting over a number of genes, and this is what the authors of that paper did.",
                    "label": 0
                },
                {
                    "sent": "They voted over now.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Jeans.",
                    "label": 0
                },
                {
                    "sent": "Now you don't need to limit yourself to only two classes.",
                    "label": 0
                },
                {
                    "sent": "In this example of prostate cancer diagnosis.",
                    "label": 0
                },
                {
                    "sent": "Also from DNA microarray data, we looked at gene expression in order to predict whether you had one of several types of prostate disease.",
                    "label": 0
                },
                {
                    "sent": "So there was great three prostate cancer grade 4 prostate cancer and benign prostate hyperplasia.",
                    "label": 1
                },
                {
                    "sent": "And using these three genes, one could make a separation.",
                    "label": 0
                },
                {
                    "sent": "Pretty nice separation between three clusters of types of prostate tissues.",
                    "label": 0
                },
                {
                    "sent": "And other authors have also using microarray data showed that you can do effective gene selection.",
                    "label": 0
                },
                {
                    "sent": "So genes in that case.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Features.",
                    "label": 0
                },
                {
                    "sent": "And here we represent accuracy as a function of the number of genes and and some people doing the characters.",
                    "label": 0
                },
                {
                    "sent": "I've asked me.",
                    "label": 0
                },
                {
                    "sent": "Do you see an optimum when you you vary the number of genes?",
                    "label": 0
                },
                {
                    "sent": "Well, here the authors.",
                    "label": 0
                },
                {
                    "sent": "I've tried a number of strategies on how to build a multi class classifier and so some of them so this the one at the top is a support vector machine using ovaa which is 1 versus the other multiclass classification.",
                    "label": 0
                },
                {
                    "sent": "And you have various other possibilities and as you can see on this graph, when you vary the number of genes, sometimes you go through an optimum and sometimes you actually level off or you keep increasing the accuracy as you increase the number of features.",
                    "label": 0
                },
                {
                    "sent": "Or jeans.",
                    "label": 0
                },
                {
                    "sent": "So we will have to find a way of determining what is the optimum number of genes or features.",
                    "label": 0
                },
                {
                    "sent": "For some practical purpose, sometimes even though using all the features gives you the best performance, you might want to use fewer features because only at the expense of, say, a 10% degradation of performance, you can have harder or 1000 times less features, and this may mean you know more efficacy or easier explanation of date.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On this other example, we're looking now the problem of drug discovery.",
                    "label": 0
                },
                {
                    "sent": "In this case, people have been screening compounds for their ability to bind to a target site, which is a trombone.",
                    "label": 1
                },
                {
                    "sent": "It's a key receptor in blood clotting.",
                    "label": 1
                },
                {
                    "sent": "And the training set consisted of a number of compounds that were active in a number of inactive compounds and we had to predict for the test set which compounds were going to be binding.",
                    "label": 0
                },
                {
                    "sent": "The features used were some very low Level 3 dimensional properties of the molecule.",
                    "label": 0
                },
                {
                    "sent": "So our presence you know of a Groupement which is oriented in a certain way, etc.",
                    "label": 0
                },
                {
                    "sent": "And in that case, again, we can see as we vary the number of features, then the success rate of prediction goes through an optimum.",
                    "label": 0
                },
                {
                    "sent": "And in fact, you know of these hundreds of thousands of features here.",
                    "label": 0
                },
                {
                    "sent": "The Axis you know is cut at 100, but it goes up to 100,000.",
                    "label": 0
                },
                {
                    "sent": "You have if you use only 20 or so, you have the optimum.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another example of feature selection in a completely different domain.",
                    "label": 0
                },
                {
                    "sent": "It's in text filtering.",
                    "label": 0
                },
                {
                    "sent": "The authors using some non corpora, the Reuters Newswire, the 20 newsgroup and the data set of web pages and a very simple representation of data which is called the bag of word.",
                    "label": 0
                },
                {
                    "sent": "Perform these experiments.",
                    "label": 0
                },
                {
                    "sent": "So what bag of word means is that you're representing a document by features that are frequencies of words in that document.",
                    "label": 0
                },
                {
                    "sent": "So here you have about, you know 100,000 features, so so 100,000 words were retained, and of course it's a very sparse representation, because many of the words never occur in some documents.",
                    "label": 0
                },
                {
                    "sent": "By performing feature selection there obtains some of the interesting results.",
                    "label": 0
                },
                {
                    "sent": "Here for these are for the newsgroups.",
                    "label": 0
                },
                {
                    "sent": "So in the newsgroup out, that isn't the most representative features.",
                    "label": 0
                },
                {
                    "sent": "Artesa, Mattison, mortal morality without the three first ones, you know for com dot graphics.",
                    "label": 0
                },
                {
                    "sent": "And you can see that for space and it also makes sense you have space.",
                    "label": 0
                },
                {
                    "sent": "NASA in orbit for religion, God, Church and sin and politics of the Middle East.",
                    "label": 0
                },
                {
                    "sent": "You have Israel, Armenian and Turkish.",
                    "label": 0
                },
                {
                    "sent": "And and for you know this this other newsgroup which is slightly different from this one has a other keywords, but this allows you to characterize groups of text in terms of the most prominent keyword.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is another application of face recognition.",
                    "label": 1
                },
                {
                    "sent": "The authors of that paper tried to apply two different feature selection algorithms to determine in the image what are the parts that are most useful to recognize faces, and here it was a discrimination simply between male and female.",
                    "label": 0
                },
                {
                    "sent": "With the relief algorithm, if they used 100 features, they found this part of the neck most you know important.",
                    "label": 0
                },
                {
                    "sent": "With 500 features, they started having some part of the eye and with thousand features you see that there is, you know, more.",
                    "label": 0
                },
                {
                    "sent": "There are more details that are being picked up by the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Releases an algorithm that does not filter redundancy in the feature set.",
                    "label": 0
                },
                {
                    "sent": "So as you can notice that features are selected on both sides of the face, even though they might be redundant because the face is symmetric, so they propose another algorithm called Simba, which eliminates redundancy and so that algorithm picks up features only on the half of the face and does not repeat them on the other half.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's now dive into, you know, after we reviewed some of the examples of feature selection, let's dive into the algorithm themselves.",
                    "label": 0
                },
                {
                    "sent": "And the first I'll introduce some nomenclature.",
                    "label": 0
                },
                {
                    "sent": "So we first so that there is a.",
                    "label": 0
                },
                {
                    "sent": "The simplest methods are so-called univariate methods and they consider one variable at a time and see whether that variable is predictive of the target.",
                    "label": 1
                },
                {
                    "sent": "On the contrary, multivariate methods consider subsets of variable that together predict well the target.",
                    "label": 0
                },
                {
                    "sent": "People in this field also make a distinction between filter methods and wrapper methods.",
                    "label": 0
                },
                {
                    "sent": "Filter methods, rank features or subsets of features independently of the predictor of the classifier.",
                    "label": 1
                },
                {
                    "sent": "They have criteria that work on their own.",
                    "label": 1
                },
                {
                    "sent": "Whereas wrapper methods use the classifier or their learning machine to assess the features or the feature subsets.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first we're going to review some very simple unique.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Various filter methods.",
                    "label": 0
                },
                {
                    "sent": "So what is individual feature relevance?",
                    "label": 1
                },
                {
                    "sent": "So again, you know in the in an example of disease diagnosis you may have features characterizing a patient, his weight, his age, number of kids, medical history, and some measurements made in blood.",
                    "label": 0
                },
                {
                    "sent": "And let's say that we have here 1 feature that is the age.",
                    "label": 0
                },
                {
                    "sent": "And we want to discriminate between patients who are at risk of heart disease or patients who are healthy.",
                    "label": 0
                },
                {
                    "sent": "Well, if it turns out that we have the distribution of the two patients which are completely overlaid.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to say that that feature is useless.",
                    "label": 0
                },
                {
                    "sent": "We can't make a separation between the two populations of interest based on that feature, so there is all reason to discard it.",
                    "label": 0
                },
                {
                    "sent": "Formally, if the joint probability of the variable of interest and the target variable.",
                    "label": 0
                },
                {
                    "sent": "Is equal to the probability of the individual variables.",
                    "label": 0
                },
                {
                    "sent": "Then there is independence between the variable in the target.",
                    "label": 0
                },
                {
                    "sent": "Or equivalently, if we use Bayes rule, we can also say that the probability of the variable given the target is equal to the probability of the variable.",
                    "label": 0
                },
                {
                    "sent": "So essentially ignoring the label.",
                    "label": 0
                },
                {
                    "sent": "Gives you the same distribution.",
                    "label": 0
                },
                {
                    "sent": "Here you have, you know in red you have the conditional density for y = 1.",
                    "label": 0
                },
                {
                    "sent": "And the conditional density for Wycombe minus one.",
                    "label": 0
                },
                {
                    "sent": "And they are almost identical.",
                    "label": 0
                },
                {
                    "sent": "So if you remove the label and use the conditional density and use the density not condition, you get the same thing as the conditional density.",
                    "label": 0
                },
                {
                    "sent": "So the 3rd way of expressing that is also that the P of XI given like 01 is also equal to PLX.",
                    "label": 0
                },
                {
                    "sent": "I give a 1 equal equal minus one.",
                    "label": 0
                },
                {
                    "sent": "So how do we express that in terms of?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ineffective algorithm.",
                    "label": 0
                },
                {
                    "sent": "People have been defining many criteria of relevance.",
                    "label": 0
                },
                {
                    "sent": "In order to determine which variables one should retain.",
                    "label": 0
                },
                {
                    "sent": "One simple criterion of relevance is just to use the displacement between the two distributions.",
                    "label": 0
                },
                {
                    "sent": "So imagine that instead of what we saw in the previous slide, we now see that we have the distribution of.",
                    "label": 0
                },
                {
                    "sent": "You know the two populations they're not overlapping anymore.",
                    "label": 0
                },
                {
                    "sent": "We can measure the difference between the means.",
                    "label": 0
                },
                {
                    "sent": "And we need to normalize them by the standard deviation so that we get a criterion which is independent on the scale of the feature.",
                    "label": 0
                },
                {
                    "sent": "Other criteria include the area under the Roc curve for single features.",
                    "label": 1
                },
                {
                    "sent": "So people familiar with classification problems know that we were using this our C criterion.",
                    "label": 0
                },
                {
                    "sent": "These RC curve to measure classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "If you're not familiar with that, it doesn't matter, but in essence, what's happening is that we're taking individual features as classifiers.",
                    "label": 0
                },
                {
                    "sent": "You can take an individual feature and set a threshold.",
                    "label": 0
                },
                {
                    "sent": "On the value that this feature is taking, if it's a continuous feature, for example, if it's age, you can vary a threshold on the age and use the age to make your prediction.",
                    "label": 0
                },
                {
                    "sent": "If you do that, you can.",
                    "label": 0
                },
                {
                    "sent": "You're monitoring sensitivity and specificity by varying these threshold, the sensitivity is the error rate on the positive class and the specificity the error rate on the negative class.",
                    "label": 0
                },
                {
                    "sent": "So assume that you're trying to predict heart disease as a using age as a predictor.",
                    "label": 0
                },
                {
                    "sent": "If you put a threshold which is very very high, and let's say at 90 year old and saying that everybody.",
                    "label": 0
                },
                {
                    "sent": "90 you're going to predict is at risk of heart disease, and everybody under 90 is not at risk of heart disease.",
                    "label": 0
                },
                {
                    "sent": "Well, you're making your going probably to make very few errors on the negative class, so you don't make very few going to very few decisions in which you're going to say that a person is not at risk of heart disease actually is, but on the other hand, if you vary the threshold and put it too low, you would make more errors on the positive class.",
                    "label": 0
                },
                {
                    "sent": "So if you vary that threshold.",
                    "label": 0
                },
                {
                    "sent": "You get the tradeoff between sensitivity and specificity, and this is what that curve is and the area under the Roc curve.",
                    "label": 0
                },
                {
                    "sent": "Is a measurement that people use in the classification field to determine how good your classifier is and it can also be used to determine how good your individual feature is as a predictor.",
                    "label": 0
                },
                {
                    "sent": "Because an individual feature is like a tiny classifier.",
                    "label": 1
                },
                {
                    "sent": "If you very.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This threshold.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now let's go back to our example of leukemia that we had.",
                    "label": 0
                },
                {
                    "sent": "One of the first slides.",
                    "label": 0
                },
                {
                    "sent": "This is exactly what the authors did.",
                    "label": 0
                },
                {
                    "sent": "The final criterion that they called the signal to noise ratio criterion.",
                    "label": 0
                },
                {
                    "sent": "And that is the difference between the means of the positive and the negative class, rescaled by the average standard deviation.",
                    "label": 0
                },
                {
                    "sent": "As it turns out, this criterion is very similar to just making the DOT product between X&Y.",
                    "label": 0
                },
                {
                    "sent": "Once you have already standardized the data.",
                    "label": 0
                },
                {
                    "sent": "So if instead of computing this criterion you first standardize the data so you first divide all the features by the standard deviation.",
                    "label": 0
                },
                {
                    "sent": "Then you have already that normalization performed.",
                    "label": 0
                },
                {
                    "sent": "And because we include Y as plus or minus one, if you have the same number of examples in the positive class and in the negative class, then performing the DOT product between X&Y.",
                    "label": 0
                },
                {
                    "sent": "Is equivalent to making the subtraction between these.",
                    "label": 0
                },
                {
                    "sent": "Some of the examples of the positive class and the sum of examples of the negative class.",
                    "label": 0
                },
                {
                    "sent": "So it's similar to what you have, you know, divided by the total number of examples.",
                    "label": 0
                },
                {
                    "sent": "It's similar to subtracting the mean of one class by the mean of the other class.",
                    "label": 0
                },
                {
                    "sent": "Hence you know that there is high similarity between this criterion and the person correlation coefficient.",
                    "label": 0
                },
                {
                    "sent": "We will see you in the practical class that there are people have been defining many criteria that seemingly are different, but in reality compute quantities that are very similar.",
                    "label": 0
                },
                {
                    "sent": "And when you look at the ranking that the features, the ranking of the features that are obtained with these different criteria, there are virtually.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Identical.",
                    "label": 0
                },
                {
                    "sent": "Now let's go back to this notion of independence.",
                    "label": 0
                },
                {
                    "sent": "First I told you you know about independence, and now I'm talking to you about.",
                    "label": 0
                },
                {
                    "sent": "You know this criteria that probably have nothing to do with this independence.",
                    "label": 0
                },
                {
                    "sent": "How do we measure dependence?",
                    "label": 0
                },
                {
                    "sent": "One way of measuring dependencies by mutual information.",
                    "label": 0
                },
                {
                    "sent": "If independence is defined by the equality of P of X&Y, with P of XPN or Y.",
                    "label": 0
                },
                {
                    "sent": "We need to measure.",
                    "label": 0
                },
                {
                    "sent": "We need to find a way of measuring the distance between this quantity and this quantity.",
                    "label": 0
                },
                {
                    "sent": "To determine dependence.",
                    "label": 0
                },
                {
                    "sent": "And this is what is done with mutual information.",
                    "label": 0
                },
                {
                    "sent": "It's also the callback library divergent between the joint density and the product of the individual densities.",
                    "label": 0
                },
                {
                    "sent": "And only this is the log of the ratio of these two quantities.",
                    "label": 0
                },
                {
                    "sent": "And the expected value is taken over the distribution of P of X&Y.",
                    "label": 0
                },
                {
                    "sent": "So while not using, you know mutual information as a criterion for ranking features.",
                    "label": 0
                },
                {
                    "sent": "Well, actually, why not?",
                    "label": 0
                },
                {
                    "sent": "You know you can use it and people have done that and have been very successful with that.",
                    "label": 0
                },
                {
                    "sent": "The only difficulty is that it's hard to estimate densities or probability distributions, so if you have.",
                    "label": 0
                },
                {
                    "sent": "Binary variables then it's usually feasible and gives good results.",
                    "label": 0
                },
                {
                    "sent": "If you have continuous variables, you first probably have to discretize or you have to go through some estimation of density which.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is non trivial.",
                    "label": 0
                },
                {
                    "sent": "Now there is a correspondence between richer information and correlation, so remember this very simple signal to noise ratio criterion that was used in the lab paper.",
                    "label": 0
                },
                {
                    "sent": "Is similar to the person correlation coefficient.",
                    "label": 0
                },
                {
                    "sent": "As it turns out, correlation is a measurement of linear dependency, whereas mutual information is a measurement of dependency that does not preclude of the type of dependency that you have, whether it's linear or not.",
                    "label": 0
                },
                {
                    "sent": "Of course you have a price to pay for that in terms of complexity of estimation.",
                    "label": 0
                },
                {
                    "sent": "So in this like here I'm showing you know density for X, for density for Y.",
                    "label": 0
                },
                {
                    "sent": "And these are, you know, the marginal densities and on these boxes I'm showing two different joint densities.",
                    "label": 0
                },
                {
                    "sent": "So in this joint density here, there is really a structure.",
                    "label": 0
                },
                {
                    "sent": "And this joint density here is, you know, pretty much up hazard.",
                    "label": 0
                },
                {
                    "sent": "From the point of view of the correlation coefficient.",
                    "label": 0
                },
                {
                    "sent": "There is not much difference.",
                    "label": 0
                },
                {
                    "sent": "There is very little correlation between X&Y on this plot here and very little correlation between X&Y on this slide here, right, because correlation measures linear dependency, you cannot make your line.",
                    "label": 0
                },
                {
                    "sent": "That explain explains why.",
                    "label": 0
                },
                {
                    "sent": "Well, you know why from X, either here nor here, but with respect to mutual information is much more mutual information in this graph then in this graph, because you can find a good explanation of why given X.",
                    "label": 0
                },
                {
                    "sent": "In this graph, whereas here you basically cannot.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in fact, for the Goshen distribution, it can be shown that there is a formal relationship between mutual information and correlation given by this formula.",
                    "label": 0
                },
                {
                    "sent": "But for other cases, the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relationship is not that trivial.",
                    "label": 0
                },
                {
                    "sent": "So many, many criteria of univariate feature selection and in the chapter three of the book on feature extraction that I mentioned in the introduction.",
                    "label": 0
                },
                {
                    "sent": "You find tons of those, and here's the table.",
                    "label": 0
                },
                {
                    "sent": "So you can refer to that if you want.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More details but now.",
                    "label": 0
                },
                {
                    "sent": "I would like to mention one additional subtlety of these univariate criteria.",
                    "label": 0
                },
                {
                    "sent": "People in the literature have been using them a lot because they are related to a methods that have been used a lot in statistics to determine relevance.",
                    "label": 0
                },
                {
                    "sent": "Relevance of variables.",
                    "label": 0
                },
                {
                    "sent": "In statistic, people use statistical tests.",
                    "label": 0
                },
                {
                    "sent": "To determine relevance.",
                    "label": 0
                },
                {
                    "sent": "An for example, the T test.",
                    "label": 0
                },
                {
                    "sent": "That will be measuring the whether there is a significant difference between two distributions that are assumed to be normal.",
                    "label": 0
                },
                {
                    "sent": "The null hypothesis is that there is equality between the means.",
                    "label": 0
                },
                {
                    "sent": "And the T statistic is such that if the null hypothesis is true, then.",
                    "label": 1
                },
                {
                    "sent": "This distribution, that is the difference between the means rescaled by a standard deviation, be scaled by the standard error.",
                    "label": 0
                },
                {
                    "sent": "And this this term here, you know, is the number of the positive examples and the number of the negative examples.",
                    "label": 0
                },
                {
                    "sent": "This follows the student distribution with N + -- -- 2 degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "So you can carry out that test and then if you want a certain risk value.",
                    "label": 0
                },
                {
                    "sent": "That this particular feature is a really relevant feature.",
                    "label": 0
                },
                {
                    "sent": "Then you can use that test to make that decision.",
                    "label": 0
                },
                {
                    "sent": "Or you can use the P value of the test.",
                    "label": 0
                },
                {
                    "sent": "As a ranking criterion.",
                    "label": 0
                },
                {
                    "sent": "So the advantage of using the P value rather than just statistic like that is that the P value is in a way normalized and you'll be able to compare P values across different statistical tests.",
                    "label": 0
                },
                {
                    "sent": "So as you can notice here, this T test statistic is very similar to the signal to noise ratio statistic that was introduced at over.",
                    "label": 0
                },
                {
                    "sent": "You know, uh-huh Ristic motivation by the authors of the Colored Paper.",
                    "label": 0
                },
                {
                    "sent": "In the practical class, will go over.",
                    "label": 0
                },
                {
                    "sent": "You know some ways of computing.",
                    "label": 0
                },
                {
                    "sent": "P values even if you don't know the distribution.",
                    "label": 0
                },
                {
                    "sent": "Because for most cases, and we're not most cases that a wide variety of cases people have come up with heuristic ways of ranking features that work very well in practice.",
                    "label": 0
                },
                {
                    "sent": "But I'm not populated, don't correspond to any known.",
                    "label": 0
                },
                {
                    "sent": "Statistical distribution, yet it's often useful to compute P values in order to know what is the false positive rate.",
                    "label": 0
                },
                {
                    "sent": "So the rate of features that are falsely thought of being relevant, so we'll go over that in the practical class and see how we can, you know, calculate false positive rates even if we don't have a tabulated.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distribution.",
                    "label": 0
                },
                {
                    "sent": "So in chapter two of the book, you have many different statistical tests and the in the more general case here is this.",
                    "label": 0
                },
                {
                    "sent": "How this works.",
                    "label": 0
                },
                {
                    "sent": "You define a null hypothesis which basically tells you that X&Y are independent.",
                    "label": 0
                },
                {
                    "sent": "And then you define a relevance index.",
                    "label": 1
                },
                {
                    "sent": "And that's equivalent to your test statistic.",
                    "label": 0
                },
                {
                    "sent": "Then you you derive from running, you know the test the P value.",
                    "label": 0
                },
                {
                    "sent": "And this P value is equivalent to the false positive rate, so the fraction.",
                    "label": 0
                },
                {
                    "sent": "Features that are thought of being relevant.",
                    "label": 0
                },
                {
                    "sent": "In the the North distribution.",
                    "label": 0
                },
                {
                    "sent": "So what happens is that in the framework of statistical tests.",
                    "label": 0
                },
                {
                    "sent": "You take this.",
                    "label": 0
                },
                {
                    "sent": "No, I prothesis that corresponds to the feature is irrelevant, and that corresponds to what people call the null distribution.",
                    "label": 0
                },
                {
                    "sent": "So distribution of irrelevant features.",
                    "label": 0
                },
                {
                    "sent": "And what you're trying to figure out is for a given value of your relevance criterion.",
                    "label": 0
                },
                {
                    "sent": "What is the fraction of the irrelevant features that are selected?",
                    "label": 0
                },
                {
                    "sent": "So here is you know the the reasoning behind that.",
                    "label": 0
                },
                {
                    "sent": "Let's take some garbage feature and let's try to apply or relevance criterion to that.",
                    "label": 0
                },
                {
                    "sent": "Those garbage features.",
                    "label": 0
                },
                {
                    "sent": "Are we going to select some features out of this garbage features by applying this relevance criterion?",
                    "label": 0
                },
                {
                    "sent": "If yes, then that means that we're going to have some false positive in our real problem, right?",
                    "label": 0
                },
                {
                    "sent": "You know real problem?",
                    "label": 0
                },
                {
                    "sent": "We don't want to select garbage features.",
                    "label": 0
                },
                {
                    "sent": "We want to separate the good ones from the bad ones.",
                    "label": 0
                },
                {
                    "sent": "So in order to test how good.",
                    "label": 0
                },
                {
                    "sent": "Or method is we're going to purposely put some garbage pictures into our methods and see how many of the garbage is selecting and this is what this is doing.",
                    "label": 0
                },
                {
                    "sent": "So now this is going to be maybe a little bit into some technical details, but there's a problem with this methodology.",
                    "label": 0
                },
                {
                    "sent": "The problem is that if you apply multiple times, the statistical tests, then you're running into a problem called multiple testing, and the reason is very similar to what we have.",
                    "label": 0
                },
                {
                    "sent": "You know in the problem of overfitting or the problem of model selection.",
                    "label": 0
                },
                {
                    "sent": "If you have many, many models right to choose from, there is a very high chance that you're going to pick one that's going to fit your data perfectly.",
                    "label": 0
                },
                {
                    "sent": "Like we said before.",
                    "label": 0
                },
                {
                    "sent": "Well, for feature selection we have the same thing, right?",
                    "label": 0
                },
                {
                    "sent": "If you have many many features and for each feature you're repeating this test, is this a good feature?",
                    "label": 0
                },
                {
                    "sent": "Is this that feature well?",
                    "label": 0
                },
                {
                    "sent": "In the end, just out of sheer luck is very probable.",
                    "label": 0
                },
                {
                    "sent": "You're going to find out in the end of feature that's going to pass your test, but this is just by chance.",
                    "label": 1
                },
                {
                    "sent": "So if you repeat test multiple times, your P value actually is wrong and it should be corrected.",
                    "label": 0
                },
                {
                    "sent": "This is what the Bonferroni correction means.",
                    "label": 0
                },
                {
                    "sent": "Bonferroni Correction is a first order correction that just replaces the P value by end times the P value, and being the number of times to perform the test.",
                    "label": 0
                },
                {
                    "sent": "So people have been also introducing another notion called the false discovery rate, and that alleviates this problem of multiple testing.",
                    "label": 1
                },
                {
                    "sent": "Instead of calculating the false positive rate, which is the fraction of features that you select when you inject garbage features.",
                    "label": 0
                },
                {
                    "sent": "The false positive the forces cover rate is the fraction of bad features that you have among your selected features.",
                    "label": 0
                },
                {
                    "sent": "So say you are taking a big bag of features you have good and bad ones in them.",
                    "label": 0
                },
                {
                    "sent": "And you select a certain chunk of them.",
                    "label": 0
                },
                {
                    "sent": "The full discovery rate is the fraction of bad guys you have in this chunk that you've selected.",
                    "label": 0
                },
                {
                    "sent": "And it can be shown to be bounded by the false positive rate times the number of features over the number of selected features.",
                    "label": 0
                },
                {
                    "sent": "So don't you need to go into details about that?",
                    "label": 0
                },
                {
                    "sent": "It's very simple to show you can do that by yourself.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The reason why this is interesting is that this false positive rate is provided by the P values of the test.",
                    "label": 0
                },
                {
                    "sent": "So you can compute the false discover rate simply from the P value.",
                    "label": 0
                },
                {
                    "sent": "And the probe method that will go over, you know, in the in the class.",
                    "label": 0
                },
                {
                    "sent": "In the practical class.",
                    "label": 1
                },
                {
                    "sent": "Consists in the approximating the false positive rate.",
                    "label": 0
                },
                {
                    "sent": "By the fraction of.",
                    "label": 0
                },
                {
                    "sent": "Of guys that you select when you just take complete garbage features.",
                    "label": 0
                },
                {
                    "sent": "So the problem method consists in taking your original data, scrambling it completely.",
                    "label": 0
                },
                {
                    "sent": "You take your original matrix, you scramble it and then now you simulate you know garbage features out of this scrambled matrix and you apply your feature selection criterion.",
                    "label": 0
                },
                {
                    "sent": "And you look at what is the fraction of.",
                    "label": 0
                },
                {
                    "sent": "Other features that you have selected and that match the criterion that you've decided so, for example, exceeds a given threshold of your selection criterion.",
                    "label": 0
                },
                {
                    "sent": "So once you've done that, you can you know.",
                    "label": 0
                },
                {
                    "sent": "Plugging this into this formula and obtain the false discovery rate so will go through these exercises.",
                    "label": 0
                },
                {
                    "sent": "The goal of you know this whole thing again is to determine what is the fraction of false positive that you have in the selected features.",
                    "label": 0
                },
                {
                    "sent": "This is an important you know.",
                    "label": 0
                },
                {
                    "sent": "For example in the medical domain.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                },
                {
                    "sent": "If you're using feature selection for drug discovery or for finding some drug targets.",
                    "label": 0
                },
                {
                    "sent": "If each feature represents a drug target or or candidate drug.",
                    "label": 0
                },
                {
                    "sent": "Then you selecting features, I'll have hundreds or thousands of candidates and you retain 100 that are good candidates.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to know among these hundred good candidates.",
                    "label": 0
                },
                {
                    "sent": "How you know what is an estimate of the fraction of those guys which are actually bad guys, which are actually completely meaningless and that were selected accidentally?",
                    "label": 0
                },
                {
                    "sent": "Just because you know you have some limitation of your accurate statistical accuracy of your predictive method.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, in the next section.",
                    "label": 0
                },
                {
                    "sent": "We're going to go towards more complicated feature selection methods that you consider several variables at a time, so up to now, we went through the simple exercise of taking each individual variable and seeing how productive they are of the target.",
                    "label": 0
                },
                {
                    "sent": "Now, how?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, considering the subsets of variables.",
                    "label": 0
                },
                {
                    "sent": "First, why do we care?",
                    "label": 0
                },
                {
                    "sent": "Why should we bother considering multiple variables at a time?",
                    "label": 0
                },
                {
                    "sent": "It's more complicated.",
                    "label": 0
                },
                {
                    "sent": "It's more computationally expensive, and it's also more statistically difficult to do.",
                    "label": 0
                },
                {
                    "sent": "So the reason the justification it lies in, you know these two figures that are scatter plots.",
                    "label": 0
                },
                {
                    "sent": "And you have two.",
                    "label": 0
                },
                {
                    "sent": "Distributions of patients.",
                    "label": 0
                },
                {
                    "sent": "So patients with one type of disease.",
                    "label": 0
                },
                {
                    "sent": "Separation between these two populations.",
                    "label": 0
                },
                {
                    "sent": "And we can also look at what happens in projection on the axis.",
                    "label": 0
                },
                {
                    "sent": "So considering one variable at a time, it's it's a 2 dimensional problem.",
                    "label": 0
                },
                {
                    "sent": "But we can project it in one dimension.",
                    "label": 0
                },
                {
                    "sent": "And if we do so, if we project everything onto the age variable.",
                    "label": 0
                },
                {
                    "sent": "We see that age reasonably well separates the two populations.",
                    "label": 0
                },
                {
                    "sent": "If we project here on say, wait, I mean I'm making some arbitrary example, then we see that there is overlap between the two distribution and so this variable here alone by itself is not a good choice to separate the two populations where this variable here is a good choice.",
                    "label": 0
                },
                {
                    "sent": "This is what would be the result of a univariate feature selection.",
                    "label": 0
                },
                {
                    "sent": "We would decide pick this variable, throw away this one.",
                    "label": 0
                },
                {
                    "sent": "But now if you look in two dimensions, you can see that there is a better separation of the two populations if you add this variable.",
                    "label": 0
                },
                {
                    "sent": "This variable, which alone is irrelevant when taken with another one can help you perform a better separation.",
                    "label": 0
                },
                {
                    "sent": "Now consider the second example.",
                    "label": 0
                },
                {
                    "sent": "Here it's even more dramatic.",
                    "label": 0
                },
                {
                    "sent": "Because now you have two variables that alone.",
                    "label": 0
                },
                {
                    "sent": "Do not separate the data.",
                    "label": 0
                },
                {
                    "sent": "But when you consider them together, they give you a very good separation of the data, even though it's not a linear separation.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's a nonlinear separation because you have each class is composed of two clusters and they are arranged in this very particular way which is known you know, and as to the chess board problem.",
                    "label": 0
                },
                {
                    "sent": "It's one of you know, the next year machine learning example that's always quoted, because when you look in projection you can't separate.",
                    "label": 0
                },
                {
                    "sent": "The examples.",
                    "label": 0
                },
                {
                    "sent": "And so multivariate methods are going to allow us to identify features that together are predictive, but individually or not.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We're now opposing filters versus wrappers.",
                    "label": 0
                },
                {
                    "sent": "The main goal is going still to be ranking subsets of useful features.",
                    "label": 1
                },
                {
                    "sent": "The filter approach consistently considering all the features, putting them into a box called filter out, putting some subset of good features, and then using them for making prediction.",
                    "label": 0
                },
                {
                    "sent": "Whereas the wrapper method consider the features generates some subsets of candidate features.",
                    "label": 0
                },
                {
                    "sent": "Passes them to the predictor.",
                    "label": 0
                },
                {
                    "sent": "The predictor you know is trained, and we compute the prediction power of the feature subset and then eventually we guide the search for a new feature subset and we tried many times until we find an optimum feature subset.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 1
                },
                {
                    "sent": "What connects this problem with the problem of the previous lecture is that there is a danger of overfitting.",
                    "label": 0
                },
                {
                    "sent": "If you're considering many subsets of features.",
                    "label": 0
                },
                {
                    "sent": "It is like we had in the previous lecture when we considered many learning machines or we considered highly complex learning machines.",
                    "label": 0
                },
                {
                    "sent": "The complexity of our learning problem in that case is going to be related to the number of feature subsets that we are considering in our search.",
                    "label": 0
                },
                {
                    "sent": "So we run at risk of overfit.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing.",
                    "label": 0
                },
                {
                    "sent": "In the chapter three of the book, there are many research strategies that are considered, and I'm mentioning here a few just to give you, you know, a broad overview.",
                    "label": 0
                },
                {
                    "sent": "There are so called forward and backward methods, so forward selection methods consistent starting with an empty set of feature and progressively adding features backward elimination, you start with the full set, then you progressively eliminate.",
                    "label": 0
                },
                {
                    "sent": "Then there is deep search where you consider a path and keeping all the time K candidate features.",
                    "label": 0
                },
                {
                    "sent": "There's the generalized sequential forward selection.",
                    "label": 1
                },
                {
                    "sent": "When N minus query and minus K features are.",
                    "label": 0
                },
                {
                    "sent": "I left sorry there is a.",
                    "label": 0
                },
                {
                    "sent": "There's a you know and a problem in the typesetting here.",
                    "label": 0
                },
                {
                    "sent": "So anyways, it requires.",
                    "label": 0
                },
                {
                    "sent": "And minus K to the G chosen minus key GI.",
                    "label": 0
                },
                {
                    "sent": "Think here completely Seawell trainings so more trainings are done each step but there are fewer steps than in then in the simple sequential forward selection.",
                    "label": 1
                },
                {
                    "sent": "Then there is the PTA method.",
                    "label": 0
                },
                {
                    "sent": "So it's where you put L and take away R. So you go forward a certain number of steps L and then you go backwards certain number of steps.",
                    "label": 0
                },
                {
                    "sent": "And then finally there is the floating search, which I think is the most popular search methods right now for wrapper.",
                    "label": 0
                },
                {
                    "sent": "And what you do is that you either start going forward or start going backward in your search and you go forward until you're not improving anymore and then you start going backward until you're not improving anymore, and so on and so forth, and you oscillate between going forward and backward.",
                    "label": 0
                },
                {
                    "sent": "In your search.",
                    "label": 0
                },
                {
                    "sent": "And of course, you know I didn't mention it, but you could also do an exhaustive search so you could generate all possible subsets of variables and investigate them all, and you can do genetic algorithms.",
                    "label": 0
                },
                {
                    "sent": "You can do any type of search you want.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem being that if you search harder.",
                    "label": 0
                },
                {
                    "sent": "You're going to incur some complexity penalty.",
                    "label": 0
                },
                {
                    "sent": "In this graph, I'm representing the case in which you have only four features.",
                    "label": 0
                },
                {
                    "sent": "So these are represented by these four numbers here.",
                    "label": 0
                },
                {
                    "sent": "And this is the state space of all possible feature subsets.",
                    "label": 0
                },
                {
                    "sent": "So 1 means the feature is present and zero the feature is absent.",
                    "label": 0
                },
                {
                    "sent": "So we have also here all the possible cases of feature present feature absent for features.",
                    "label": 0
                },
                {
                    "sent": "And you go from one note to the next by just removing or adding one feature.",
                    "label": 0
                },
                {
                    "sent": "So this is the elementary step that you performing in searching in this space.",
                    "label": 0
                },
                {
                    "sent": "And there are two to the end possible feature subsets for N features.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this is this highly complex and this can be replaced by.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, simpler methods called embedded methods or nested subset methods, so I'm illustrating this with a simple example.",
                    "label": 0
                },
                {
                    "sent": "Where you input all the features you train a support vector machine, which is a special case of classifier.",
                    "label": 0
                },
                {
                    "sent": "Eliminate useless features.",
                    "label": 0
                },
                {
                    "sent": "The ones that don't are not needed to make predictions.",
                    "label": 0
                },
                {
                    "sent": "Examining whether the performances are degraded or not, an.",
                    "label": 0
                },
                {
                    "sent": "If yes, you continue eliminating.",
                    "label": 0
                },
                {
                    "sent": "If no, you stop and to distract you a little bit from the boring lecture and making little animation that's showing you how this works is just for marketing people to amuse them.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those.",
                    "label": 0
                },
                {
                    "sent": "We're going to more details about, you know embedded methods in the next lecture.",
                    "label": 0
                },
                {
                    "sent": "How do you are you going to assess the validity of the subsets that are features that we're going to obtain?",
                    "label": 0
                },
                {
                    "sent": "Well, it's very similar to what we did in the previous lecture.",
                    "label": 0
                },
                {
                    "sent": "We also going to split the data into training, validation, and test set.",
                    "label": 1
                },
                {
                    "sent": "We can also do cross validation, but here to simplify the argument, we're going to assume like we assumed here that we have a lot of data and we don't need to do cross validation.",
                    "label": 0
                },
                {
                    "sent": "We can just split into training validation.",
                    "label": 0
                },
                {
                    "sent": "And they said one split.",
                    "label": 0
                },
                {
                    "sent": "And so in red is the training in blue validation an in green the test?",
                    "label": 1
                },
                {
                    "sent": "So for each feature subset, we're going to train a predictor on the training data.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to select the feature subset which performs best on the validation data.",
                    "label": 1
                },
                {
                    "sent": "And we can, as I said, you know, average over many splits if we need to reduce variance by performing cross validation.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "When we finish that, we can take the result of feature selection.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And test on new test data.",
                    "label": 0
                },
                {
                    "sent": "This scheme allows us to estimate the complexity of the feature selection process as we had before you remember that we had, you know, in the guaranteed risk framework we had the guaranteed risk, which is a allows us to bound the expected risk the real race came.",
                    "label": 0
                },
                {
                    "sent": "In the dragon of Learning machine, people often called the true risk.",
                    "label": 0
                },
                {
                    "sent": "The generalization error.",
                    "label": 0
                },
                {
                    "sent": "So the error that you're going to make if you have infinite amount of data.",
                    "label": 0
                },
                {
                    "sent": "And in the previous lecture we bounded the true risk of the generalization error by the training error plus some complexity term.",
                    "label": 0
                },
                {
                    "sent": "Now we're looking at the second level of inference, so feature selection.",
                    "label": 0
                },
                {
                    "sent": "Is very similar to hyperparameter selection, so you remember in the previous lecture actually one of the hyperparameters I had told you about with was the Sigma.",
                    "label": 0
                },
                {
                    "sent": "Was in fact the number of features, so feature selection is in a way hyperparameter selection, so everything takes place here.",
                    "label": 0
                },
                {
                    "sent": "The training set for feature selection is the validation set.",
                    "label": 0
                },
                {
                    "sent": "Right, and the training.",
                    "label": 0
                },
                {
                    "sent": "The two training part here is to train the parameters of the models.",
                    "label": 0
                },
                {
                    "sent": "And so we can again at this new level of inference, because that's the second level of inference, because it's using validation data.",
                    "label": 0
                },
                {
                    "sent": "We can similarly apply generalization bounds.",
                    "label": 0
                },
                {
                    "sent": "And now what was sticking the place in the bound of the training error is the validation error, and we have an extra complexity term.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at what is this complexity term here, and this M_2.",
                    "label": 0
                },
                {
                    "sent": "Here is the number of is the dimension of the validation set.",
                    "label": 0
                },
                {
                    "sent": "In this table I'm showing you how many, how many subsets of features we are considering.",
                    "label": 0
                },
                {
                    "sent": "If we do exhaustive search wrapper.",
                    "label": 1
                },
                {
                    "sent": "So if we consider all possible subsets of features, then we considering 2 to the power and feature subsets.",
                    "label": 0
                },
                {
                    "sent": "If we do, if we're doing other methods like forward selection, rapper Type, I haven't proved that to you, but will do that probably as an exercise in the exercise class, or I'll explain it to you in the next lecture.",
                    "label": 0
                },
                {
                    "sent": "I think in the next lecture I'll show you that.",
                    "label": 1
                },
                {
                    "sent": "So we incur these number of feature subsets and if we do simply feature ranking.",
                    "label": 0
                },
                {
                    "sent": "So for example if we do like you need very univariate methods and we rank the features according to a certain criterion then we considering only an.",
                    "label": 0
                },
                {
                    "sent": "Feature subsets.",
                    "label": 0
                },
                {
                    "sent": "So here's the feature ranking thing.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "The first feature is the one which is the most productive.",
                    "label": 0
                },
                {
                    "sent": "The second feature, the second best, etc, etc.",
                    "label": 0
                },
                {
                    "sent": "You can use feature ranking for determining feature subsets because you can take the first subset is this one, the second subset is this one etc.",
                    "label": 0
                },
                {
                    "sent": "You increase subsets.",
                    "label": 0
                },
                {
                    "sent": "Using the feature rank and in that case you considering N. If you have a total of N features.",
                    "label": 0
                },
                {
                    "sent": "You're considering an feature subsets.",
                    "label": 0
                },
                {
                    "sent": "So this is one you know, one way of doing.",
                    "label": 0
                },
                {
                    "sent": "Selections of feature subsets just varying the number of features in this way.",
                    "label": 1
                },
                {
                    "sent": "In that case, what complexity do you incur?",
                    "label": 0
                },
                {
                    "sent": "Well, in the statistical learning theory, if you are trying to pick.",
                    "label": 0
                },
                {
                    "sent": "Learning machines among a finite number of learning machines.",
                    "label": 0
                },
                {
                    "sent": "It is easy to compute this complexity term here.",
                    "label": 0
                },
                {
                    "sent": "The complexity is always proportional to either the log of the number of machines that you tried or to the square root of the log, depending on you know whether or not you have zero here in this first part.",
                    "label": 0
                },
                {
                    "sent": "But roughly speaking, if you just consider the log in, then the complexity you incur for an exhaustive search wrapper is N. Whereas here for these other methods it's only log of N. So you see, that's pretty dramatic, right?",
                    "label": 0
                },
                {
                    "sent": "All the methods that are very intensive in search.",
                    "label": 0
                },
                {
                    "sent": "Message that try all possible subsets or methods that try a lot of subsets, like genetic algorithms or simulated annealing that do a very exhaustive search.",
                    "label": 0
                },
                {
                    "sent": "For example, try just you know a fraction of the total number of subsets, but it still scales.",
                    "label": 1
                },
                {
                    "sent": "You know with with still scales exponentially with the number of features.",
                    "label": 0
                },
                {
                    "sent": "All of those are pretty complex and will have you know a big epsilon term here.",
                    "label": 0
                },
                {
                    "sent": "Where all the methods that are like forward selection methods or or feature ranking methods.",
                    "label": 0
                },
                {
                    "sent": "They only incur log event in complexity.",
                    "label": 0
                },
                {
                    "sent": "So they are going to overfit much less, and this is the message here.",
                    "label": 0
                },
                {
                    "sent": "The message here is that if you have a large number of features shy away from trying all possible subsets.",
                    "label": 0
                },
                {
                    "sent": "Of course you probably won't do that anyways because it will be computationally too expensive.",
                    "label": 0
                },
                {
                    "sent": "But aside from being too computationally expensive, it's also very dangerous statistically.",
                    "label": 0
                },
                {
                    "sent": "That is dangerous in terms that you will be likely overfitting.",
                    "label": 0
                },
                {
                    "sent": "Likely you will be selecting the feature subset that explains well your training data, but it's going to perform poorly on future data.",
                    "label": 0
                },
                {
                    "sent": "OK, so this just shows.",
                    "label": 0
                },
                {
                    "sent": "Two cases like before, you know you had this tradeoff between the first term here is the validation error, and here is the complexity of feature selection and you have an optimum in between very similar to what we had.",
                    "label": 0
                },
                {
                    "sent": "You know, in the first lecture.",
                    "label": 0
                },
                {
                    "sent": "And if you have less complex methods.",
                    "label": 0
                },
                {
                    "sent": "You may you may do worse in terms of cross validation error.",
                    "label": 0
                },
                {
                    "sent": "But nevertheless.",
                    "label": 0
                }
            ]
        }
    }
}