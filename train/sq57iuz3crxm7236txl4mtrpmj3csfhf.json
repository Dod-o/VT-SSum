{
    "id": "sq57iuz3crxm7236txl4mtrpmj3csfhf",
    "title": "Knowledge Graph Consolidation by Unifying Synonymous Relationships",
    "info": {
        "author": [
            "Jan-Christoph Kalo, TU Braunschweig"
        ],
        "published": "Jan. 27, 2020",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_kalo_knowledge_graph/",
    "segmentation": [
        [
            "Exactly thank you John Luca and thanks everyone for being here and listening to my talk.",
            "So I will tell you something about finding synonymous relationships in knowledge, graphs and 1st of."
        ],
        [
            "All I will introduce to you ontology alignment, but I guess most of you know, But ontology alignment is and might have even visited the workshop on Saturday.",
            "At least I see a couple of phases here that I saw also there and then I will tell you what the differences of my approach to knowledge ontology alignment is.",
            "So into ontology alignment.",
            "You usually have two or more ontologies or knowledge graphs.",
            "And you want to find corresponding concepts in both of them.",
            "So that means that you want to identify, for example, instances in both knowledge graphs that have the same view world.",
            "Meaning that might in this example, for example be Albert Einstein in the green Knowledge graph to Einstein, Albert in the red Knowledge Graph or for example the relationship one into birthplace.",
            "And there's several methods that can solve that problem, so there's string based methods that look at the labels, their structural methods, and some machine learning techniques that can help there.",
            "We"
        ],
        [
            "Look at the little bit different problem problem.",
            "We look at single knowledge graphs because we notice that there's a lot of heterogeneous data also in single knowledge graphs, so duplicate information applicant concepts that also can be joined and should be joined together.",
            "In this case we only look at relationships, but all of our methods can also be transferred to instances and classes.",
            "So what we want to find us when we're looking at the screen on that graph.",
            "Here we want to find out that born in is synonym a synonym of birthplace.",
            "So it's quite.",
            "Easy task, but it's actually a big problem in today's or in several of today's knowledge graphs.",
            "And this problem comes from either bad data integration.",
            "So if you take several knowledge graphs or databases, integrate them into one large knowledge graph, you might miss some duplicates and you end up with problems and with duplicate information in there or when you have open collaborative knowledge graphs like the PDF for example, where several people can contribute.",
            "There you might end up with more information about the same relationship for DB Pedia.",
            "For example, I found that there are at least 27 different birthdays relationships in there with typos in it with in different languages, and nobody really sticks to the same relationship.",
            "And that is a really big problem when you want to query the data of course, or it comes from open information extraction.",
            "So when you take some natural language texts you want to extract triples from there, put that into some RDF store.",
            "And do not stick to a fixed vocabulary.",
            "You might end up with some synonyms and.",
            "We want to solve their problems somehow.",
            "But"
        ],
        [
            "It's actually not that easy.",
            "You cannot just take ontology alignment tools as they are there becausw.",
            "You have some.",
            "Well, you have different data actually.",
            "In our first example on my first slide, you saw that there was the birthplace relationship for Max Planck and the Born in relationship for Albert Einstein know both file.",
            "But Einstein and you could just use the instance data so much.",
            "Albert Einstein from one database to Einstein, Albert to the other one and then use this overlap in the instance data to match relationships.",
            "And that is actually something that many, many ontology alignment tools are doing today to match classes and to match relationships.",
            "But that does not work in our case becausw.",
            "Many of these synonyms do not have an overlap in their extension, so they're not sharing subjects.",
            "They're not sharing objects, or only few of them.",
            "Another problem that we saw is that we did not want to use string metrics because we saw that.",
            "They just restrict the whole problem.",
            "The solution too much because we saw that we have a lot of relationships that don't have labels or have some cryptic your eyes as identifiers and their string metrics do not really work.",
            "So we wanted to have something that is purely data driven and does not make any assumptions on the data and so we also try to include that into our problem and.",
            "We had another problem that is that we don't have training data because it is currently quite popular to just use some deep learning approach or future engineers.",
            "Something use some classifier and then solve some problem.",
            "In that case we thought that well there is a data cleaning problem where we usually don't have much training data or no trainer training data at all.",
            "So using a machine, pure machine learning approach does not simply work in that case.",
            "And that's why we decided for looking."
        ],
        [
            "At Knowledge Graph embeddings, I hope, or I guess most of you have heard of, knowledge graph embeddings.",
            "There has been just a session and before lunch about knowledge graph embedding's and they're quite popular right now 'cause they're used for predicting new knowledge.",
            "The basic idea is you take triples and you transfer them into some vector space, meaning that you use the entities that you have that you make a vector out of them or matrix out of them.",
            "Use the relationships with the same.",
            "And the classes, for example two and then you can use some vector operations and use that to predict new knowledge.",
            "Recently there has also been some work on where this vector representations has been used to do entity alignments.",
            "So I saw a poster and I think there's also a paper on that on this conference where people used entity alignments to do to measure the similarity of entities and use that.",
            "To do entity alignments.",
            "And that is an idea that we transferred.",
            "And two relationships because they are, in my opinion there is no work that analyzes the suitability of relationship alignment relationship vectors to measure semantic similarity, and we try to evaluate that for synonym detection and that's what we did in this work."
        ],
        [
            "So the basic idea is we take a knowledge graph.",
            "We train the sum knowledge graph, embedding arbitrary knowledge graph embedding on it.",
            "You take the relationship representation, so that might either be a vector matrix or multiple matrices, and they use some similarity matrix on it.",
            "So in our case we use that one metric and cosine similarity.",
            "And we.",
            "Then came compute similarity histograms for every single relationship where we measure the similarity from one relationship to all the other relationships in the Knowledge graph, and then we can come up with such a histogram where we have the similarity on the X axis.",
            "And the number of relationships with this similarity on the Y axis.",
            "And when we look here at the first diagram on the left hand side, we see that most relationships have.",
            "That's an average similarity of around 7:00 to our relationship that we're currently really looking at.",
            "But there's one single relationship that is an outlier on the left hand side, and these are usually the relationships that are synonym.",
            "Am I yeah, mostly asking them and then we can use a simple outlier detection in this histogram.",
            "Figure out that there's one relationship which is extremely similar, or.",
            "Quite identical and then detect this one as a synonym.",
            "Then we have cases that are a little bit more complex, like the one here in the middle where we have again the mass.",
            "Of relationships having similarity of around 7:00 or 8, but we have a couple of relational relationships that have.",
            "I'm smaller relationship similarity values, so having a similarity of two and three is quite frequent here and that.",
            "Well, in that case it could either be that we have multiple synonyms so that we just have around 4 synonyms.",
            "So everything maybe around with their similarity value over 5, or we only have one or none of them.",
            "So that is quite difficult case and I will come to that later and then we have the easy cases on the right hand side so we have no outlier here, at least not on the left hand side.",
            "So we have no similar relationship at all.",
            "And this means we never.",
            "We usually don't have a synonym here.",
            "Yeah, that's what we usually moreles did we did an outlier detection.",
            "On this histograms, and then evaluated that on several.",
            "Am datasets."
        ],
        [
            "So we actually used eight state of the art embeddings, so from the basic trends E model that you might now to newer models and we evaluated it against the baseline that was published at SWC in 2013 that was built on Association rule mining.",
            "And we evaluate it on large scale real world knowledge graphs.",
            "So the problem here is that we don't have a gold standard for that and that it is.",
            "It would usually mean that you all have to evaluate it on your own.",
            "You have need a lot of manual work to go through the data.",
            "Also the results and identify synonyms on yourself, but that would be too much work.",
            "Becausw Freebase for example has more than 10,000 and relationships, DP, Pedia.",
            "Two wiki data has a couple of thousands 2.",
            "So we came up with a different idea so we said OK, we only make a manual evaluation for DB pedia.",
            "But for the other ones we make synthetics introduce syntactic synonyms into the data.",
            "I know that this is a little bit problematic, but at least we can analyze.",
            "The performance at a little bit with that idea.",
            "What we did is.",
            "Anne.",
            "If we have an existing synonym.",
            "Birthplace for example.",
            "We take all the triples with birthplace in it and just rename some of them into birthplace Prime, and then let our approach we identify that birthplace is synonym to birthplace Prime.",
            "Sounds quite easy, but it is actually quite good idea to evaluate the different approaches and we did that for.",
            "FP15K that is a popular data set that is useful.",
            "Knowledge, embeddings and evaluating knowledge embeddings.",
            "Then we did it on a sample of wiki data.",
            "I'm at that covers all relationships in Big Data, But the problem is that it is performance wise it's not possible to train knowledge embeddings on complete data because it's just too large.",
            "And we did it on the PDF.",
            "The October version 2016.",
            "Here we didn't manual evaluation for precision at K, so we took the top 500 results for all of our approaches and evaluated them manually with two persons and.",
            "Came up with a list of around 3000 synonyms.",
            "Yeah, and.",
            "Wait?",
            "OK."
        ],
        [
            "That's our first graph, so I'm only showing two of them, because otherwise it would be a little bit too much.",
            "So these are is a president recall diagram.",
            "I hope all of you know how to read it and it's for weekly data.",
            "And let's first focus on the baseline results.",
            "So that is the black dotted line down here.",
            "What you can see here is that the Association rule based baseline is having very low precision, so it is not really going above 20%.",
            "And another interesting point is maybe that you that it does not exceed a record of .3.",
            "That is because it is based on some minimum support value and it if you set that lower you get higher recall values, but for much much lower precision values.",
            "So we optimized for F1 measure here and that is actually the best we could get out of it, so it's not getting any better there.",
            "Well, on the other hand side we have the eight embedding models here in various colors.",
            "Maybe we just focus on the blue line because I think that's more or less the best one measure wise.",
            "That is also not great, so we have maybe a precision of around 7060% until you get to have a recall of .4, then it slowly decreases and ends down here at recall at .9, meaning that we actually never find.",
            "We never fight.",
            "We cannot find all synonyms here because we're not going to recall it 100%.",
            "We having a very very low precision here, which means that there seems to be some problems with these embedding models, and they're not really capturing the synonym semantics in Ricky data, even though they're only synthetic synonyms and not real synonyms that are in the data.",
            "And."
        ],
        [
            "Now results for our manual evaluation, so here we evaluated precision at K, so we put out a ranked list of classified results for all our methods, and again we can start with the baseline.",
            "So it's down here again and we again see Kate has lost precision, that's why.",
            "It's the baseline, but the main problem here is you see that it's increasing with increasing K, which is usually not the case.",
            "It should be the other way around.",
            "That is because the baseline is making some assumptions on the data that are used for the ranking of their results, which are actually not true for data set.",
            "So that's why it looks like that and you now might think that you can draw this line further out of the diagram, but that's actually not happen because of this minimum support value again.",
            "So.",
            "This is the best that we could get out of it and we see again.",
            "Adding approaches here having quite good results, but also.",
            "Decreasing at.",
            "Well, if we had top 500, we only have a precision of 70% left OK."
        ],
        [
            "So what did not work?",
            "That's quite interesting thing here.",
            "So first of all, embedding models are extremely bad for working with rare relationships.",
            "So if we only have 10 or 20 triples prior relationship there, they represent the semantics very badly.",
            "So there they don't work.",
            "Then we have relationships that have very high overlap in the extension, meaning they're always appear for same subject and object triples.",
            "We for example in freebase funds relationships.",
            "For the currency of the tradition for local students and for domestic students.",
            "And that was actually always the same for every University.",
            "It was the same currency that they had to pay, and there since we only have a purely data driven approach, cannot solve the problem.",
            "So there we have to use some external knowledge may be or include the labels again.",
            "We had problems with the subscription relationship.",
            "So for example if we had genre and music genre.",
            "We sometimes could not identify them as not being synonym.",
            "And what is a little bit difficult as we had relationships where the domain and range was similar.",
            "So for example, we had relationships South and North.",
            "They are from DB pedia, where you say that the city is North of or South of another city and they were often identified as synonym even though they have more or less nothing to do with each other.",
            "Well they have something to do with each other.",
            "They're sharing a lot of entities, but their semantics is quite different and the models could not really figure that out.",
            "Which is.",
            "Quite a big problem I guess.",
            "So."
        ],
        [
            "Anne.",
            "I think I have to hurry up so summary so we build a purely data driven approach for synonym detection.",
            "We make no assumptions on the data we achieve.",
            "High precision, but recall is only average I would say, but we still could outperform the baseline.",
            "And I think we can improve it by some semi automatic approach.",
            "I think to boost the precision again, but there's still a lot of problems that we detected when we looked at the results, and I think that means we have a lot of future work to do."
        ],
        [
            "And what we think what we can do is we can boost the results by using the transitivity of synonyms.",
            "So if we know that birthplace is synonym to birthplace Prime birthplace Prime is synonym to birthplace double prime we of course now that birthdays is also synonym to birthplace, double prime and we did not use that knowledge yet.",
            "And I think that can boost results alot.",
            "Then one of the reviewers proposed to use.",
            "A different idea with Association rule mining.",
            "I think he actually misunderstood the baseline description in our paper, but came up with a different idea that I think looks promising right now.",
            "So we're doing some experiments, but we have some performance problems there and we can could also extend our approach to detect inverse or subsample relationships OK?"
        ],
        [
            "Are there any questions?",
            "Thanks ideas maybe.",
            "OK, we have just a couple of minutes for questions, any question.",
            "Yeah, other.",
            "Shop.",
            "No, I did not use lexical information at all.",
            "I tried to do it purely without lexical information.",
            "Yeah, I think that would help.",
            "Yes, Yep, definitely.",
            "Any other question?",
            "So I have a quick question.",
            "While the next speaker can set up so knowledge graphs change overtime, how is your?",
            "Embedding model.",
            "Robust to change in the sense.",
            "Can you update the model or you need to rebuild it from scratch if you have changes in the ground in the Knowledge Graph, I think you have to update the model so performance wise that is really a problem.",
            "So there's actually some work on what embeddings that look at the changes.",
            "Changes of embedding so you can retrain them and compare them and compare the semantics.",
            "So currently you're looking a little bit into the evolution of the semantics of relationships overtime which might be also interesting maybe.",
            "Can also be used to track changes and I guess it might happen that maybe some relationships semantics get closer together overtime or get further away.",
            "So I'm currently looking into that because I think that might be quite interesting problem, but you have to retrain it and that costs a lot of time.",
            "OK, let's thank our speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly thank you John Luca and thanks everyone for being here and listening to my talk.",
                    "label": 0
                },
                {
                    "sent": "So I will tell you something about finding synonymous relationships in knowledge, graphs and 1st of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All I will introduce to you ontology alignment, but I guess most of you know, But ontology alignment is and might have even visited the workshop on Saturday.",
                    "label": 0
                },
                {
                    "sent": "At least I see a couple of phases here that I saw also there and then I will tell you what the differences of my approach to knowledge ontology alignment is.",
                    "label": 0
                },
                {
                    "sent": "So into ontology alignment.",
                    "label": 0
                },
                {
                    "sent": "You usually have two or more ontologies or knowledge graphs.",
                    "label": 1
                },
                {
                    "sent": "And you want to find corresponding concepts in both of them.",
                    "label": 0
                },
                {
                    "sent": "So that means that you want to identify, for example, instances in both knowledge graphs that have the same view world.",
                    "label": 0
                },
                {
                    "sent": "Meaning that might in this example, for example be Albert Einstein in the green Knowledge graph to Einstein, Albert in the red Knowledge Graph or for example the relationship one into birthplace.",
                    "label": 0
                },
                {
                    "sent": "And there's several methods that can solve that problem, so there's string based methods that look at the labels, their structural methods, and some machine learning techniques that can help there.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at the little bit different problem problem.",
                    "label": 0
                },
                {
                    "sent": "We look at single knowledge graphs because we notice that there's a lot of heterogeneous data also in single knowledge graphs, so duplicate information applicant concepts that also can be joined and should be joined together.",
                    "label": 1
                },
                {
                    "sent": "In this case we only look at relationships, but all of our methods can also be transferred to instances and classes.",
                    "label": 0
                },
                {
                    "sent": "So what we want to find us when we're looking at the screen on that graph.",
                    "label": 0
                },
                {
                    "sent": "Here we want to find out that born in is synonym a synonym of birthplace.",
                    "label": 0
                },
                {
                    "sent": "So it's quite.",
                    "label": 0
                },
                {
                    "sent": "Easy task, but it's actually a big problem in today's or in several of today's knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "And this problem comes from either bad data integration.",
                    "label": 1
                },
                {
                    "sent": "So if you take several knowledge graphs or databases, integrate them into one large knowledge graph, you might miss some duplicates and you end up with problems and with duplicate information in there or when you have open collaborative knowledge graphs like the PDF for example, where several people can contribute.",
                    "label": 0
                },
                {
                    "sent": "There you might end up with more information about the same relationship for DB Pedia.",
                    "label": 0
                },
                {
                    "sent": "For example, I found that there are at least 27 different birthdays relationships in there with typos in it with in different languages, and nobody really sticks to the same relationship.",
                    "label": 1
                },
                {
                    "sent": "And that is a really big problem when you want to query the data of course, or it comes from open information extraction.",
                    "label": 0
                },
                {
                    "sent": "So when you take some natural language texts you want to extract triples from there, put that into some RDF store.",
                    "label": 0
                },
                {
                    "sent": "And do not stick to a fixed vocabulary.",
                    "label": 0
                },
                {
                    "sent": "You might end up with some synonyms and.",
                    "label": 0
                },
                {
                    "sent": "We want to solve their problems somehow.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's actually not that easy.",
                    "label": 0
                },
                {
                    "sent": "You cannot just take ontology alignment tools as they are there becausw.",
                    "label": 0
                },
                {
                    "sent": "You have some.",
                    "label": 0
                },
                {
                    "sent": "Well, you have different data actually.",
                    "label": 0
                },
                {
                    "sent": "In our first example on my first slide, you saw that there was the birthplace relationship for Max Planck and the Born in relationship for Albert Einstein know both file.",
                    "label": 0
                },
                {
                    "sent": "But Einstein and you could just use the instance data so much.",
                    "label": 0
                },
                {
                    "sent": "Albert Einstein from one database to Einstein, Albert to the other one and then use this overlap in the instance data to match relationships.",
                    "label": 0
                },
                {
                    "sent": "And that is actually something that many, many ontology alignment tools are doing today to match classes and to match relationships.",
                    "label": 0
                },
                {
                    "sent": "But that does not work in our case becausw.",
                    "label": 1
                },
                {
                    "sent": "Many of these synonyms do not have an overlap in their extension, so they're not sharing subjects.",
                    "label": 1
                },
                {
                    "sent": "They're not sharing objects, or only few of them.",
                    "label": 0
                },
                {
                    "sent": "Another problem that we saw is that we did not want to use string metrics because we saw that.",
                    "label": 0
                },
                {
                    "sent": "They just restrict the whole problem.",
                    "label": 0
                },
                {
                    "sent": "The solution too much because we saw that we have a lot of relationships that don't have labels or have some cryptic your eyes as identifiers and their string metrics do not really work.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to have something that is purely data driven and does not make any assumptions on the data and so we also try to include that into our problem and.",
                    "label": 0
                },
                {
                    "sent": "We had another problem that is that we don't have training data because it is currently quite popular to just use some deep learning approach or future engineers.",
                    "label": 0
                },
                {
                    "sent": "Something use some classifier and then solve some problem.",
                    "label": 1
                },
                {
                    "sent": "In that case we thought that well there is a data cleaning problem where we usually don't have much training data or no trainer training data at all.",
                    "label": 0
                },
                {
                    "sent": "So using a machine, pure machine learning approach does not simply work in that case.",
                    "label": 0
                },
                {
                    "sent": "And that's why we decided for looking.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At Knowledge Graph embeddings, I hope, or I guess most of you have heard of, knowledge graph embeddings.",
                    "label": 0
                },
                {
                    "sent": "There has been just a session and before lunch about knowledge graph embedding's and they're quite popular right now 'cause they're used for predicting new knowledge.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is you take triples and you transfer them into some vector space, meaning that you use the entities that you have that you make a vector out of them or matrix out of them.",
                    "label": 0
                },
                {
                    "sent": "Use the relationships with the same.",
                    "label": 0
                },
                {
                    "sent": "And the classes, for example two and then you can use some vector operations and use that to predict new knowledge.",
                    "label": 0
                },
                {
                    "sent": "Recently there has also been some work on where this vector representations has been used to do entity alignments.",
                    "label": 0
                },
                {
                    "sent": "So I saw a poster and I think there's also a paper on that on this conference where people used entity alignments to do to measure the similarity of entities and use that.",
                    "label": 0
                },
                {
                    "sent": "To do entity alignments.",
                    "label": 0
                },
                {
                    "sent": "And that is an idea that we transferred.",
                    "label": 0
                },
                {
                    "sent": "And two relationships because they are, in my opinion there is no work that analyzes the suitability of relationship alignment relationship vectors to measure semantic similarity, and we try to evaluate that for synonym detection and that's what we did in this work.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the basic idea is we take a knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "We train the sum knowledge graph, embedding arbitrary knowledge graph embedding on it.",
                    "label": 0
                },
                {
                    "sent": "You take the relationship representation, so that might either be a vector matrix or multiple matrices, and they use some similarity matrix on it.",
                    "label": 0
                },
                {
                    "sent": "So in our case we use that one metric and cosine similarity.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                },
                {
                    "sent": "Then came compute similarity histograms for every single relationship where we measure the similarity from one relationship to all the other relationships in the Knowledge graph, and then we can come up with such a histogram where we have the similarity on the X axis.",
                    "label": 1
                },
                {
                    "sent": "And the number of relationships with this similarity on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "And when we look here at the first diagram on the left hand side, we see that most relationships have.",
                    "label": 0
                },
                {
                    "sent": "That's an average similarity of around 7:00 to our relationship that we're currently really looking at.",
                    "label": 0
                },
                {
                    "sent": "But there's one single relationship that is an outlier on the left hand side, and these are usually the relationships that are synonym.",
                    "label": 0
                },
                {
                    "sent": "Am I yeah, mostly asking them and then we can use a simple outlier detection in this histogram.",
                    "label": 0
                },
                {
                    "sent": "Figure out that there's one relationship which is extremely similar, or.",
                    "label": 0
                },
                {
                    "sent": "Quite identical and then detect this one as a synonym.",
                    "label": 0
                },
                {
                    "sent": "Then we have cases that are a little bit more complex, like the one here in the middle where we have again the mass.",
                    "label": 0
                },
                {
                    "sent": "Of relationships having similarity of around 7:00 or 8, but we have a couple of relational relationships that have.",
                    "label": 0
                },
                {
                    "sent": "I'm smaller relationship similarity values, so having a similarity of two and three is quite frequent here and that.",
                    "label": 0
                },
                {
                    "sent": "Well, in that case it could either be that we have multiple synonyms so that we just have around 4 synonyms.",
                    "label": 0
                },
                {
                    "sent": "So everything maybe around with their similarity value over 5, or we only have one or none of them.",
                    "label": 0
                },
                {
                    "sent": "So that is quite difficult case and I will come to that later and then we have the easy cases on the right hand side so we have no outlier here, at least not on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "So we have no similar relationship at all.",
                    "label": 0
                },
                {
                    "sent": "And this means we never.",
                    "label": 0
                },
                {
                    "sent": "We usually don't have a synonym here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's what we usually moreles did we did an outlier detection.",
                    "label": 0
                },
                {
                    "sent": "On this histograms, and then evaluated that on several.",
                    "label": 0
                },
                {
                    "sent": "Am datasets.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we actually used eight state of the art embeddings, so from the basic trends E model that you might now to newer models and we evaluated it against the baseline that was published at SWC in 2013 that was built on Association rule mining.",
                    "label": 1
                },
                {
                    "sent": "And we evaluate it on large scale real world knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "So the problem here is that we don't have a gold standard for that and that it is.",
                    "label": 0
                },
                {
                    "sent": "It would usually mean that you all have to evaluate it on your own.",
                    "label": 0
                },
                {
                    "sent": "You have need a lot of manual work to go through the data.",
                    "label": 0
                },
                {
                    "sent": "Also the results and identify synonyms on yourself, but that would be too much work.",
                    "label": 0
                },
                {
                    "sent": "Becausw Freebase for example has more than 10,000 and relationships, DP, Pedia.",
                    "label": 0
                },
                {
                    "sent": "Two wiki data has a couple of thousands 2.",
                    "label": 0
                },
                {
                    "sent": "So we came up with a different idea so we said OK, we only make a manual evaluation for DB pedia.",
                    "label": 0
                },
                {
                    "sent": "But for the other ones we make synthetics introduce syntactic synonyms into the data.",
                    "label": 0
                },
                {
                    "sent": "I know that this is a little bit problematic, but at least we can analyze.",
                    "label": 0
                },
                {
                    "sent": "The performance at a little bit with that idea.",
                    "label": 0
                },
                {
                    "sent": "What we did is.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "If we have an existing synonym.",
                    "label": 0
                },
                {
                    "sent": "Birthplace for example.",
                    "label": 0
                },
                {
                    "sent": "We take all the triples with birthplace in it and just rename some of them into birthplace Prime, and then let our approach we identify that birthplace is synonym to birthplace Prime.",
                    "label": 0
                },
                {
                    "sent": "Sounds quite easy, but it is actually quite good idea to evaluate the different approaches and we did that for.",
                    "label": 0
                },
                {
                    "sent": "FP15K that is a popular data set that is useful.",
                    "label": 0
                },
                {
                    "sent": "Knowledge, embeddings and evaluating knowledge embeddings.",
                    "label": 0
                },
                {
                    "sent": "Then we did it on a sample of wiki data.",
                    "label": 0
                },
                {
                    "sent": "I'm at that covers all relationships in Big Data, But the problem is that it is performance wise it's not possible to train knowledge embeddings on complete data because it's just too large.",
                    "label": 0
                },
                {
                    "sent": "And we did it on the PDF.",
                    "label": 0
                },
                {
                    "sent": "The October version 2016.",
                    "label": 0
                },
                {
                    "sent": "Here we didn't manual evaluation for precision at K, so we took the top 500 results for all of our approaches and evaluated them manually with two persons and.",
                    "label": 0
                },
                {
                    "sent": "Came up with a list of around 3000 synonyms.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and.",
                    "label": 0
                },
                {
                    "sent": "Wait?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's our first graph, so I'm only showing two of them, because otherwise it would be a little bit too much.",
                    "label": 0
                },
                {
                    "sent": "So these are is a president recall diagram.",
                    "label": 0
                },
                {
                    "sent": "I hope all of you know how to read it and it's for weekly data.",
                    "label": 0
                },
                {
                    "sent": "And let's first focus on the baseline results.",
                    "label": 0
                },
                {
                    "sent": "So that is the black dotted line down here.",
                    "label": 0
                },
                {
                    "sent": "What you can see here is that the Association rule based baseline is having very low precision, so it is not really going above 20%.",
                    "label": 0
                },
                {
                    "sent": "And another interesting point is maybe that you that it does not exceed a record of .3.",
                    "label": 0
                },
                {
                    "sent": "That is because it is based on some minimum support value and it if you set that lower you get higher recall values, but for much much lower precision values.",
                    "label": 0
                },
                {
                    "sent": "So we optimized for F1 measure here and that is actually the best we could get out of it, so it's not getting any better there.",
                    "label": 0
                },
                {
                    "sent": "Well, on the other hand side we have the eight embedding models here in various colors.",
                    "label": 0
                },
                {
                    "sent": "Maybe we just focus on the blue line because I think that's more or less the best one measure wise.",
                    "label": 0
                },
                {
                    "sent": "That is also not great, so we have maybe a precision of around 7060% until you get to have a recall of .4, then it slowly decreases and ends down here at recall at .9, meaning that we actually never find.",
                    "label": 0
                },
                {
                    "sent": "We never fight.",
                    "label": 0
                },
                {
                    "sent": "We cannot find all synonyms here because we're not going to recall it 100%.",
                    "label": 0
                },
                {
                    "sent": "We having a very very low precision here, which means that there seems to be some problems with these embedding models, and they're not really capturing the synonym semantics in Ricky data, even though they're only synthetic synonyms and not real synonyms that are in the data.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now results for our manual evaluation, so here we evaluated precision at K, so we put out a ranked list of classified results for all our methods, and again we can start with the baseline.",
                    "label": 0
                },
                {
                    "sent": "So it's down here again and we again see Kate has lost precision, that's why.",
                    "label": 0
                },
                {
                    "sent": "It's the baseline, but the main problem here is you see that it's increasing with increasing K, which is usually not the case.",
                    "label": 0
                },
                {
                    "sent": "It should be the other way around.",
                    "label": 0
                },
                {
                    "sent": "That is because the baseline is making some assumptions on the data that are used for the ranking of their results, which are actually not true for data set.",
                    "label": 0
                },
                {
                    "sent": "So that's why it looks like that and you now might think that you can draw this line further out of the diagram, but that's actually not happen because of this minimum support value again.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the best that we could get out of it and we see again.",
                    "label": 0
                },
                {
                    "sent": "Adding approaches here having quite good results, but also.",
                    "label": 0
                },
                {
                    "sent": "Decreasing at.",
                    "label": 0
                },
                {
                    "sent": "Well, if we had top 500, we only have a precision of 70% left OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what did not work?",
                    "label": 0
                },
                {
                    "sent": "That's quite interesting thing here.",
                    "label": 0
                },
                {
                    "sent": "So first of all, embedding models are extremely bad for working with rare relationships.",
                    "label": 1
                },
                {
                    "sent": "So if we only have 10 or 20 triples prior relationship there, they represent the semantics very badly.",
                    "label": 0
                },
                {
                    "sent": "So there they don't work.",
                    "label": 1
                },
                {
                    "sent": "Then we have relationships that have very high overlap in the extension, meaning they're always appear for same subject and object triples.",
                    "label": 0
                },
                {
                    "sent": "We for example in freebase funds relationships.",
                    "label": 0
                },
                {
                    "sent": "For the currency of the tradition for local students and for domestic students.",
                    "label": 0
                },
                {
                    "sent": "And that was actually always the same for every University.",
                    "label": 0
                },
                {
                    "sent": "It was the same currency that they had to pay, and there since we only have a purely data driven approach, cannot solve the problem.",
                    "label": 0
                },
                {
                    "sent": "So there we have to use some external knowledge may be or include the labels again.",
                    "label": 0
                },
                {
                    "sent": "We had problems with the subscription relationship.",
                    "label": 0
                },
                {
                    "sent": "So for example if we had genre and music genre.",
                    "label": 0
                },
                {
                    "sent": "We sometimes could not identify them as not being synonym.",
                    "label": 0
                },
                {
                    "sent": "And what is a little bit difficult as we had relationships where the domain and range was similar.",
                    "label": 1
                },
                {
                    "sent": "So for example, we had relationships South and North.",
                    "label": 0
                },
                {
                    "sent": "They are from DB pedia, where you say that the city is North of or South of another city and they were often identified as synonym even though they have more or less nothing to do with each other.",
                    "label": 0
                },
                {
                    "sent": "Well they have something to do with each other.",
                    "label": 0
                },
                {
                    "sent": "They're sharing a lot of entities, but their semantics is quite different and the models could not really figure that out.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "Quite a big problem I guess.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I think I have to hurry up so summary so we build a purely data driven approach for synonym detection.",
                    "label": 0
                },
                {
                    "sent": "We make no assumptions on the data we achieve.",
                    "label": 1
                },
                {
                    "sent": "High precision, but recall is only average I would say, but we still could outperform the baseline.",
                    "label": 0
                },
                {
                    "sent": "And I think we can improve it by some semi automatic approach.",
                    "label": 0
                },
                {
                    "sent": "I think to boost the precision again, but there's still a lot of problems that we detected when we looked at the results, and I think that means we have a lot of future work to do.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we think what we can do is we can boost the results by using the transitivity of synonyms.",
                    "label": 1
                },
                {
                    "sent": "So if we know that birthplace is synonym to birthplace Prime birthplace Prime is synonym to birthplace double prime we of course now that birthdays is also synonym to birthplace, double prime and we did not use that knowledge yet.",
                    "label": 0
                },
                {
                    "sent": "And I think that can boost results alot.",
                    "label": 1
                },
                {
                    "sent": "Then one of the reviewers proposed to use.",
                    "label": 0
                },
                {
                    "sent": "A different idea with Association rule mining.",
                    "label": 0
                },
                {
                    "sent": "I think he actually misunderstood the baseline description in our paper, but came up with a different idea that I think looks promising right now.",
                    "label": 0
                },
                {
                    "sent": "So we're doing some experiments, but we have some performance problems there and we can could also extend our approach to detect inverse or subsample relationships OK?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "Thanks ideas maybe.",
                    "label": 0
                },
                {
                    "sent": "OK, we have just a couple of minutes for questions, any question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, other.",
                    "label": 0
                },
                {
                    "sent": "Shop.",
                    "label": 0
                },
                {
                    "sent": "No, I did not use lexical information at all.",
                    "label": 0
                },
                {
                    "sent": "I tried to do it purely without lexical information.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that would help.",
                    "label": 0
                },
                {
                    "sent": "Yes, Yep, definitely.",
                    "label": 0
                },
                {
                    "sent": "Any other question?",
                    "label": 0
                },
                {
                    "sent": "So I have a quick question.",
                    "label": 0
                },
                {
                    "sent": "While the next speaker can set up so knowledge graphs change overtime, how is your?",
                    "label": 1
                },
                {
                    "sent": "Embedding model.",
                    "label": 0
                },
                {
                    "sent": "Robust to change in the sense.",
                    "label": 0
                },
                {
                    "sent": "Can you update the model or you need to rebuild it from scratch if you have changes in the ground in the Knowledge Graph, I think you have to update the model so performance wise that is really a problem.",
                    "label": 0
                },
                {
                    "sent": "So there's actually some work on what embeddings that look at the changes.",
                    "label": 0
                },
                {
                    "sent": "Changes of embedding so you can retrain them and compare them and compare the semantics.",
                    "label": 0
                },
                {
                    "sent": "So currently you're looking a little bit into the evolution of the semantics of relationships overtime which might be also interesting maybe.",
                    "label": 0
                },
                {
                    "sent": "Can also be used to track changes and I guess it might happen that maybe some relationships semantics get closer together overtime or get further away.",
                    "label": 0
                },
                {
                    "sent": "So I'm currently looking into that because I think that might be quite interesting problem, but you have to retrain it and that costs a lot of time.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank our speaker again.",
                    "label": 0
                }
            ]
        }
    }
}