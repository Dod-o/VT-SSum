{
    "id": "xugi4qij7j4uj5gyzhxbo66yuqx75hfa",
    "title": "Regret-based Online Ranking for a Growing Digital Library",
    "info": {
        "author": [
            "Erick Delage, Stanford University"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning"
        ]
    },
    "url": "http://videolectures.net/kdd09_delage_rborgdl/",
    "segmentation": [
        [
            "My name is Eric, diligent from shifts in Moria.",
            "Yeah, I recently graduated from Stanford University and this is independent work that I did there at the end of my PhD, the talk will focus on regret based online ranking with applications being growing digital libraries, so I cannot put a title like this without."
        ],
        [
            "Putting up their most popular digital library on Earth, which is the World Wide Web and how we interface with this library currently.",
            "So put up in the query bar of Google, the Knowledge Discovery Conference 2009, press search and got the list.",
            "This ranked list of web pages that discuss gives information about this conference and typically I would click on that third link in order to find out at what time I'm giving this talk today.",
            "And it's."
        ],
        [
            "Important problem to be ranking libraries like the Internet like the World Wide Web.",
            "Very difficult problem to solve in general.",
            "Many prior work as discussed, ways of handling this task.",
            "For instance, there's a couple of options when you're trying to develop an algorithm for this.",
            "This type of task, first the source of the data, could be a set of expert which are hand labeling some relevance of different items in the library, or could actually be clickthrough data.",
            "This is data that is highly abundant currently for these types of data sets.",
            "And the current difficulties with handling with this data are that there can be noisy and subject to manipulation.",
            "Yet I will assume for now that I'm not discussing these type of issues that arise with click through data because I feel that there's an equally important question to answer with this type of data.",
            "The other type of algorithm ways of algorithm handle the information that's available about ranking is through offline, when, for instance we take a historical data set.",
            "Try to find what is the most appropriate ranking function or policy for that data set.",
            "Or use data as it as it is generated by users and these two click through data.",
            "An online ranking will be the main focus of this talk and as I said I'll assume that clickthrough data is truthful so that I'm really trying to capture what's the information about the best ranking policy for click through data generated."
        ],
        [
            "Users.",
            "With this assumption, the ranking model becomes quite simple, yet I'll get after these description of the model.",
            "The important question that I'm addressing first, we have three entities.",
            "First, users are coming to our data set or a digital library in the sequential fashion, and at each time time step, user T submits a query Q FT."
        ],
        [
            "The ranking mechanism, which has for task to provide an order and ordering of the items in the library in order to provide the user with a ranked list which should put the item that's Morse relevant to his query high in."
        ],
        [
            "List in forward the information that gets back to the ranking mechanism after providing this ranked list has two aspects.",
            "First, pretty much it's restrict restricted to the item that the user clicked on, yet this information also captures some sort of satisfaction measure on the user's point of view.",
            "Here it's represented by a cost function which depends both on the item that is clicked and the ranked list that was presented to him and the cost function should be higher.",
            "If the item was low in that list, for instance.",
            "And the ranking mechanism in this context, As for goal to accumulate and long in the long run at low cost average cost over."
        ],
        [
            "Users of visits.",
            "So the question that I'm answering the remain of the stock is for is.",
            "As users are visiting this digital library and providing their queries and clicks and as the library evolves, does there exist a ranking algorithm or ranking policy update that can use the click through information in a way that the long term performance of its ranks will be comperable to the best chosen ranking app posterior.",
            "So we'll compare the performance that is met by the engine to the performance that the engine could have.",
            "Achieved if it had access to the future history of users and which I think it will click on.",
            "I'll make the details of how we compare this a bit later cured."
        ],
        [
            "Later, as a derivative of this work will also showing our empirical section greedy variance of an algorithm that satisfies the first question which actually outperforms many popular ranking algorithms on the data."
        ],
        [
            "That we used.",
            "So the remaining of this top is separated in three parts.",
            "The 1st I'll describe the scale, rank, model, bit clearer.",
            "Then I'll present what I mean by currying performance to the best performance achievable with hindsight.",
            "And finally, I'll show some experimental results with citation database that I did not online."
        ],
        [
            "So the first question that we answered when developing this model is, should we choose a deterministic ranking policy or non deterministic one?",
            "Typically the algorithms that are available currently will formulate the score function like is like is expressed here linear score function which depends both on some para meters of the policy and features that are extracted and supposed to represent the relationship between the item and the query that was.",
            "Submitted and typically a deterministic ranking will just order the items according to their score.",
            "That's like vanilla way of doing."
        ],
        [
            "Thinking.",
            "In this work we favor non deterministic ranking.",
            "I'll explain more precise precisely later why this nondeterministic ranking refers to the fact that our ranking won't be fully determined based on the query and the state of the parimeter data or the score function.",
            "For instance, the policy that were the set of policy that we're examining more closely is the following, following one where a random ranking is generated given a query.",
            "And a para meters state for the policy based on the following process will generate this random ranking by sampling first the first item of the list and the likelihood of getting a given item first is proportional to the score of that item.",
            "Instead of putting the highest score first in the list like the deterministic ranking would do here, I'll just sample more frequently, the first the highest score as the first item of the list.",
            "And after sampling the first item of the list, I'll recursively address the other items in my rank list.",
            "After discarding the items that I have."
        ],
        [
            "Assembled.",
            "So things that can be already noted is that the nondeterministic ranking policy can closely replicate any deterministic policy by appropriate values for the different parameters, and in both cases, generation of the rank is not is pretty much the same cost computationally.",
            "We have a way of generating these random ranking, which costs an order of N log in."
        ],
        [
            "So why did I introduce this nondeterministic ranking?",
            "Well, major difficulty with ranking currently is that if you start with a deterministic policy, then the decision model becomes non continuous decision models.",
            "So finding the direction in which the data parameter should change in order to decrease costs is hard to do because of the fact that the score doesn't influence in the is not influence, does not influence in there.",
            "Continuous way the actual rank of the item I list here.",
            "Common cost model that I've been that are used and I won't go into detail, especially of the normalized normalized discounted gain which appeared in Lian ET al in 2008.",
            "But all for all these these cost model which will see in the experimental section later on the decision model of finding the optimal deterministic ranking is a hard problem."
        ],
        [
            "Yet when we consider non deterministic ranking already we reduce the difficulty of continuity in their decision objectives.",
            "If we consider it instead of the cost itself, we would consider the expected cost and then the expected cars become continuous in terms of the pyramid."
        ],
        [
            "Hours of this core function.",
            "More importantly, if we use this new cost function, which I refer to as scale cost, the problem becomes a convex optimization problem.",
            "To describe this cost, which we believe is a very natural one for this type of family of policy, the cost measures actually the Cal divergance between the random ranking that is generated, the distribution of the random ranking and the set of distribution which would have put the item that was clicked first in the list.",
            "So the Gale divergences between this random policy.",
            "Distribution mu still the Python data and the distribution which and the closest distribution with respect to this divergent measure?",
            "So this this in this form it's a complicated objective function to to evaluate, yet you can easily reduce this expression to something that's much more."
        ],
        [
            "Easy to evaluate, tractable.",
            "So at this point I've completely the original picture of the model that we're working with, and we can already suggest heuristic, for instance, of how to generate new random rankings.",
            "We could use when Barty users that visited the search engine, we could use the historic historical data to find the optimal parimeter that minimize the KL cost.",
            "The total KL costs and choose as the next random ranking distribution.",
            "One that experiment that has perimeter assigned to an argument that minimizes this historical cost.",
            "Yet, although we would expect this to do good in practice, we have no guarantees about how closed."
        ],
        [
            "It will be to the actual optimal solution of the problem.",
            "More specifically, we can measure closeness to optimal ranking by the expression here, which is called regret referred as regret in the literature, and compares the total cost accumulated by our updates or the sequence of random rankings that were proposed to the minimal cost that would be achieved by this policy, chosen with full knowledge of the sequence of users.",
            "This is expressed here by the right hand side of the regret equation, and here I expressed the fact that this is not known right now for such a juristic.",
            "If you apply it in practice."
        ],
        [
            "So our goal in the the remaining section is to propose a update rules that will actually."
        ],
        [
            "If this I achieve have some guarantees with respect to this value, because the problem that I described was a convex optimization problem and be composed over every time step, we can use theory of online convex optimization to obtain the following guarantees, but given that we employ an update rule which is called greedy projection, really projection is somewhat there will look familiar, it's a.",
            "It's a rule that does a gradient descent of one step at a given distance and project back on the set of feasible policies.",
            "If we employ this greedy projection algorithm and we can guarantee that the feature function is bounded and the feature set of perimetre is also bounded, then we are guaranteed by the derivation proof presented in Zinkevich in 2003 that regret will, lated regret will grow at a rate of the square root of T, which means that if we want to analyze the average regret that is obtained using this update rule.",
            "We'll get there.",
            "An average regret which goes to zero at the rate of 1 / sqrt T and these orders are not asymptotic order for a given number of of time step.",
            "We're actually guaranteed that counts given constant Times Square root of T is achieved in term of regret."
        ],
        [
            "Yet when we're dealing with the World Wide Web or the set of books in the Library of Congress, we cannot use the result that we presented earlier.",
            "Since these library as users are visiting these libraries, then the library also grows with new volumes with new items 'cause these things happen simultaneously.",
            "Therefore, we spent some time extending the results that were presented in zinc.",
            "If it by zinkevich to this case where we need to after a certain number of time step, we will want to add new features to consider when building our random policy.",
            "Or random ranking?",
            "To do so, we also need to modify the definition of regret because we cannot anymore compare the best random ranking chosen with full knowledge.",
            "At the given time step, if if the set of feasible policy that were considered at this time step is not the same as the final one.",
            "So here we added this, we need to add this projection term in the comparison of the right hand side expression, which make sure that when we look in hindsight, what was the optimal cost that could be achieved, we put ourselves back in the shoes of our ranking algorithm at the time that this user visit users visited."
        ],
        [
            "So in this case, as I said, the extension of the work that was presented earlier.",
            "And suggests using greedy projection with exponential restart.",
            "This is an algorithm that I won't describe in details.",
            "It really it refer.",
            "It relates very closely to the."
        ],
        [
            "Greedy projection algorithm.",
            "If the feasible set for these fitness."
        ],
        [
            "Meters is bounded by a ball, which scales the order of 1 T to the one four 4th an with time.",
            "Then we have these regret."
        ],
        [
            "Guarantees we compared our algorithm to a set of other options.",
            "The algorithm that I presented earlier, we called, no regret KL rank.",
            "In fact, we I announced that I would present on an alternativ algorithms which doesn't give the same guarantees in terms of regret, but we can consider it as an opportunistic version of the KL rank approach, which follows the steps that are done in KL rank.",
            "Yet to win comes a time to to present.",
            "The ranking will present the ranking according to the score directly and not."
        ],
        [
            "Randomly.",
            "We compared the algorithms to four ranking algorithms that are currently popular and that have made significant improvement on prior ones.",
            "I'll just briefly through mentioning Rank Net which was presented by Virgil and his collaborator and page rank and hits are also graph based algorithms that were."
        ],
        [
            "Compared in our analysis so quickly, I have this table comparing the scores for the three different cost functions that I presented earlier.",
            "For simplicity, I put it in parentesis.",
            "I put the ordering of the different algorithms for these costs and we see that there might be a price to pay for having it for using an algorithm that has no regret overall.",
            "The reason for this is somewhat related to the fact that.",
            "Looking at these results, we might think that it's not a good algorithm, yet the guarantees tells us that if we apply the algorithm in that totally different context, or if we don't believe these users truly represents what the users will do tomorrow or a year from now, there no regret algorithm will provide the same guarantees.",
            "While these are results that are strictly related to the historical data that is in this test.",
            "On the other hand, if you are someone that believes that.",
            "If you see a curve you believe in that curve, then greedy Cal rank is something that outperforms all the other methods that I listed here."
        ],
        [
            "So, just to summarize quickly, and I mentioned earlier how click today.",
            "There was a band abundant for this type of application and how the I hope I convinced you that the Cal rank model was a good fit for this type of task of ranking for users that are visiting a search engine.",
            "The no regret Cal rank was the first to provide guarantees with respect to the long term performance in terms of regret and actually the performance guarantees do not depend on any assumption about.",
            "The users that are visiting it's a general guarantee for the for the algorithm and for arbitrary sequence of users.",
            "Finally, the analysis did provide guidance with respect to how to choose the features that would be added to.",
            "To the feature set to create new policies without having over over over fitting, occur.",
            "And finally we showed how greedy coloring might be a good approach in a case where you want tangible proof that things are working well with your environment, but."
        ],
        [
            "It's a case by case analysis that needs to be performed and in the future work.",
            "I still believe that greedy CLO Rang might have similar guarantees as the no regret one, which could be further investigated.",
            "And of course, there's I mentioned earlier.",
            "The other trouble that people have with click through data which is biases that are due to the ordering itself, and also the fact that clickthrough can be manipulated if someone wants to see his web page hierarchy is going to click a lot of time on this item and.",
            "This that this analysis didn't account for any of that, yet it's a it's.",
            "It's an analysis of robustness of these."
        ],
        [
            "Kittens if you have any questions, I'll be happy to answer them now or at the poster session or at the break.",
            "Thank you.",
            "Could you go back to the phone?",
            "Question does.",
            "So.",
            "1st.",
            "My understanding is that you need to kind of realization.",
            "You don't want to be.",
            "Why did you choose this form?",
            "I wanted to compare the random ranking distribution to some other distribution using Kyle Direct KL divergences.",
            "But in the choice of the distribution, this seems to be this set seemed to be the set of any distribution which would have pleased the user as long as this item is first in the list, he doesn't really care what are the other, how the other items are ordered afterwards.",
            "So this is somewhat of a distance between a set an appoint.",
            "And then I have another question.",
            "The fact that the greedy behaves much better than that, no regret approach.",
            "Come from practices.",
            "The problem is not the shuddering.",
            "But also the taste of preferences.",
            "Read what I have a feeling that so the question was that the greedy policy might do better because we are in a non stationary environment, yet I feel like both should be vulnerable to non stationary environment 'cause they both follow the same update rules.",
            "It's just that greedy variant is more blind when applying the process but it is something that's right now for nonstationary context.",
            "It seems like.",
            "When the algorithm restarts, it should adapt to the nonstationarity.",
            "But yeah, so you need exponentially more users too, as the as the process changes.",
            "No, it's an interesting question to improve on.",
            "Thank you.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Eric, diligent from shifts in Moria.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I recently graduated from Stanford University and this is independent work that I did there at the end of my PhD, the talk will focus on regret based online ranking with applications being growing digital libraries, so I cannot put a title like this without.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Putting up their most popular digital library on Earth, which is the World Wide Web and how we interface with this library currently.",
                    "label": 0
                },
                {
                    "sent": "So put up in the query bar of Google, the Knowledge Discovery Conference 2009, press search and got the list.",
                    "label": 0
                },
                {
                    "sent": "This ranked list of web pages that discuss gives information about this conference and typically I would click on that third link in order to find out at what time I'm giving this talk today.",
                    "label": 0
                },
                {
                    "sent": "And it's.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Important problem to be ranking libraries like the Internet like the World Wide Web.",
                    "label": 0
                },
                {
                    "sent": "Very difficult problem to solve in general.",
                    "label": 0
                },
                {
                    "sent": "Many prior work as discussed, ways of handling this task.",
                    "label": 0
                },
                {
                    "sent": "For instance, there's a couple of options when you're trying to develop an algorithm for this.",
                    "label": 0
                },
                {
                    "sent": "This type of task, first the source of the data, could be a set of expert which are hand labeling some relevance of different items in the library, or could actually be clickthrough data.",
                    "label": 0
                },
                {
                    "sent": "This is data that is highly abundant currently for these types of data sets.",
                    "label": 1
                },
                {
                    "sent": "And the current difficulties with handling with this data are that there can be noisy and subject to manipulation.",
                    "label": 1
                },
                {
                    "sent": "Yet I will assume for now that I'm not discussing these type of issues that arise with click through data because I feel that there's an equally important question to answer with this type of data.",
                    "label": 0
                },
                {
                    "sent": "The other type of algorithm ways of algorithm handle the information that's available about ranking is through offline, when, for instance we take a historical data set.",
                    "label": 0
                },
                {
                    "sent": "Try to find what is the most appropriate ranking function or policy for that data set.",
                    "label": 0
                },
                {
                    "sent": "Or use data as it as it is generated by users and these two click through data.",
                    "label": 1
                },
                {
                    "sent": "An online ranking will be the main focus of this talk and as I said I'll assume that clickthrough data is truthful so that I'm really trying to capture what's the information about the best ranking policy for click through data generated.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Users.",
                    "label": 0
                },
                {
                    "sent": "With this assumption, the ranking model becomes quite simple, yet I'll get after these description of the model.",
                    "label": 0
                },
                {
                    "sent": "The important question that I'm addressing first, we have three entities.",
                    "label": 0
                },
                {
                    "sent": "First, users are coming to our data set or a digital library in the sequential fashion, and at each time time step, user T submits a query Q FT.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The ranking mechanism, which has for task to provide an order and ordering of the items in the library in order to provide the user with a ranked list which should put the item that's Morse relevant to his query high in.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "List in forward the information that gets back to the ranking mechanism after providing this ranked list has two aspects.",
                    "label": 1
                },
                {
                    "sent": "First, pretty much it's restrict restricted to the item that the user clicked on, yet this information also captures some sort of satisfaction measure on the user's point of view.",
                    "label": 0
                },
                {
                    "sent": "Here it's represented by a cost function which depends both on the item that is clicked and the ranked list that was presented to him and the cost function should be higher.",
                    "label": 0
                },
                {
                    "sent": "If the item was low in that list, for instance.",
                    "label": 0
                },
                {
                    "sent": "And the ranking mechanism in this context, As for goal to accumulate and long in the long run at low cost average cost over.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Users of visits.",
                    "label": 0
                },
                {
                    "sent": "So the question that I'm answering the remain of the stock is for is.",
                    "label": 0
                },
                {
                    "sent": "As users are visiting this digital library and providing their queries and clicks and as the library evolves, does there exist a ranking algorithm or ranking policy update that can use the click through information in a way that the long term performance of its ranks will be comperable to the best chosen ranking app posterior.",
                    "label": 1
                },
                {
                    "sent": "So we'll compare the performance that is met by the engine to the performance that the engine could have.",
                    "label": 0
                },
                {
                    "sent": "Achieved if it had access to the future history of users and which I think it will click on.",
                    "label": 0
                },
                {
                    "sent": "I'll make the details of how we compare this a bit later cured.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later, as a derivative of this work will also showing our empirical section greedy variance of an algorithm that satisfies the first question which actually outperforms many popular ranking algorithms on the data.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we used.",
                    "label": 0
                },
                {
                    "sent": "So the remaining of this top is separated in three parts.",
                    "label": 0
                },
                {
                    "sent": "The 1st I'll describe the scale, rank, model, bit clearer.",
                    "label": 0
                },
                {
                    "sent": "Then I'll present what I mean by currying performance to the best performance achievable with hindsight.",
                    "label": 0
                },
                {
                    "sent": "And finally, I'll show some experimental results with citation database that I did not online.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first question that we answered when developing this model is, should we choose a deterministic ranking policy or non deterministic one?",
                    "label": 0
                },
                {
                    "sent": "Typically the algorithms that are available currently will formulate the score function like is like is expressed here linear score function which depends both on some para meters of the policy and features that are extracted and supposed to represent the relationship between the item and the query that was.",
                    "label": 0
                },
                {
                    "sent": "Submitted and typically a deterministic ranking will just order the items according to their score.",
                    "label": 1
                },
                {
                    "sent": "That's like vanilla way of doing.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thinking.",
                    "label": 0
                },
                {
                    "sent": "In this work we favor non deterministic ranking.",
                    "label": 1
                },
                {
                    "sent": "I'll explain more precise precisely later why this nondeterministic ranking refers to the fact that our ranking won't be fully determined based on the query and the state of the parimeter data or the score function.",
                    "label": 1
                },
                {
                    "sent": "For instance, the policy that were the set of policy that we're examining more closely is the following, following one where a random ranking is generated given a query.",
                    "label": 0
                },
                {
                    "sent": "And a para meters state for the policy based on the following process will generate this random ranking by sampling first the first item of the list and the likelihood of getting a given item first is proportional to the score of that item.",
                    "label": 0
                },
                {
                    "sent": "Instead of putting the highest score first in the list like the deterministic ranking would do here, I'll just sample more frequently, the first the highest score as the first item of the list.",
                    "label": 0
                },
                {
                    "sent": "And after sampling the first item of the list, I'll recursively address the other items in my rank list.",
                    "label": 0
                },
                {
                    "sent": "After discarding the items that I have.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Assembled.",
                    "label": 0
                },
                {
                    "sent": "So things that can be already noted is that the nondeterministic ranking policy can closely replicate any deterministic policy by appropriate values for the different parameters, and in both cases, generation of the rank is not is pretty much the same cost computationally.",
                    "label": 1
                },
                {
                    "sent": "We have a way of generating these random ranking, which costs an order of N log in.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why did I introduce this nondeterministic ranking?",
                    "label": 0
                },
                {
                    "sent": "Well, major difficulty with ranking currently is that if you start with a deterministic policy, then the decision model becomes non continuous decision models.",
                    "label": 0
                },
                {
                    "sent": "So finding the direction in which the data parameter should change in order to decrease costs is hard to do because of the fact that the score doesn't influence in the is not influence, does not influence in there.",
                    "label": 0
                },
                {
                    "sent": "Continuous way the actual rank of the item I list here.",
                    "label": 0
                },
                {
                    "sent": "Common cost model that I've been that are used and I won't go into detail, especially of the normalized normalized discounted gain which appeared in Lian ET al in 2008.",
                    "label": 1
                },
                {
                    "sent": "But all for all these these cost model which will see in the experimental section later on the decision model of finding the optimal deterministic ranking is a hard problem.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yet when we consider non deterministic ranking already we reduce the difficulty of continuity in their decision objectives.",
                    "label": 0
                },
                {
                    "sent": "If we consider it instead of the cost itself, we would consider the expected cost and then the expected cars become continuous in terms of the pyramid.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hours of this core function.",
                    "label": 0
                },
                {
                    "sent": "More importantly, if we use this new cost function, which I refer to as scale cost, the problem becomes a convex optimization problem.",
                    "label": 1
                },
                {
                    "sent": "To describe this cost, which we believe is a very natural one for this type of family of policy, the cost measures actually the Cal divergance between the random ranking that is generated, the distribution of the random ranking and the set of distribution which would have put the item that was clicked first in the list.",
                    "label": 1
                },
                {
                    "sent": "So the Gale divergences between this random policy.",
                    "label": 0
                },
                {
                    "sent": "Distribution mu still the Python data and the distribution which and the closest distribution with respect to this divergent measure?",
                    "label": 0
                },
                {
                    "sent": "So this this in this form it's a complicated objective function to to evaluate, yet you can easily reduce this expression to something that's much more.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Easy to evaluate, tractable.",
                    "label": 0
                },
                {
                    "sent": "So at this point I've completely the original picture of the model that we're working with, and we can already suggest heuristic, for instance, of how to generate new random rankings.",
                    "label": 0
                },
                {
                    "sent": "We could use when Barty users that visited the search engine, we could use the historic historical data to find the optimal parimeter that minimize the KL cost.",
                    "label": 0
                },
                {
                    "sent": "The total KL costs and choose as the next random ranking distribution.",
                    "label": 0
                },
                {
                    "sent": "One that experiment that has perimeter assigned to an argument that minimizes this historical cost.",
                    "label": 0
                },
                {
                    "sent": "Yet, although we would expect this to do good in practice, we have no guarantees about how closed.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It will be to the actual optimal solution of the problem.",
                    "label": 0
                },
                {
                    "sent": "More specifically, we can measure closeness to optimal ranking by the expression here, which is called regret referred as regret in the literature, and compares the total cost accumulated by our updates or the sequence of random rankings that were proposed to the minimal cost that would be achieved by this policy, chosen with full knowledge of the sequence of users.",
                    "label": 0
                },
                {
                    "sent": "This is expressed here by the right hand side of the regret equation, and here I expressed the fact that this is not known right now for such a juristic.",
                    "label": 0
                },
                {
                    "sent": "If you apply it in practice.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our goal in the the remaining section is to propose a update rules that will actually.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If this I achieve have some guarantees with respect to this value, because the problem that I described was a convex optimization problem and be composed over every time step, we can use theory of online convex optimization to obtain the following guarantees, but given that we employ an update rule which is called greedy projection, really projection is somewhat there will look familiar, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a rule that does a gradient descent of one step at a given distance and project back on the set of feasible policies.",
                    "label": 0
                },
                {
                    "sent": "If we employ this greedy projection algorithm and we can guarantee that the feature function is bounded and the feature set of perimetre is also bounded, then we are guaranteed by the derivation proof presented in Zinkevich in 2003 that regret will, lated regret will grow at a rate of the square root of T, which means that if we want to analyze the average regret that is obtained using this update rule.",
                    "label": 0
                },
                {
                    "sent": "We'll get there.",
                    "label": 0
                },
                {
                    "sent": "An average regret which goes to zero at the rate of 1 / sqrt T and these orders are not asymptotic order for a given number of of time step.",
                    "label": 1
                },
                {
                    "sent": "We're actually guaranteed that counts given constant Times Square root of T is achieved in term of regret.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yet when we're dealing with the World Wide Web or the set of books in the Library of Congress, we cannot use the result that we presented earlier.",
                    "label": 0
                },
                {
                    "sent": "Since these library as users are visiting these libraries, then the library also grows with new volumes with new items 'cause these things happen simultaneously.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we spent some time extending the results that were presented in zinc.",
                    "label": 0
                },
                {
                    "sent": "If it by zinkevich to this case where we need to after a certain number of time step, we will want to add new features to consider when building our random policy.",
                    "label": 0
                },
                {
                    "sent": "Or random ranking?",
                    "label": 0
                },
                {
                    "sent": "To do so, we also need to modify the definition of regret because we cannot anymore compare the best random ranking chosen with full knowledge.",
                    "label": 0
                },
                {
                    "sent": "At the given time step, if if the set of feasible policy that were considered at this time step is not the same as the final one.",
                    "label": 0
                },
                {
                    "sent": "So here we added this, we need to add this projection term in the comparison of the right hand side expression, which make sure that when we look in hindsight, what was the optimal cost that could be achieved, we put ourselves back in the shoes of our ranking algorithm at the time that this user visit users visited.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this case, as I said, the extension of the work that was presented earlier.",
                    "label": 0
                },
                {
                    "sent": "And suggests using greedy projection with exponential restart.",
                    "label": 1
                },
                {
                    "sent": "This is an algorithm that I won't describe in details.",
                    "label": 0
                },
                {
                    "sent": "It really it refer.",
                    "label": 0
                },
                {
                    "sent": "It relates very closely to the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Greedy projection algorithm.",
                    "label": 0
                },
                {
                    "sent": "If the feasible set for these fitness.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Meters is bounded by a ball, which scales the order of 1 T to the one four 4th an with time.",
                    "label": 0
                },
                {
                    "sent": "Then we have these regret.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guarantees we compared our algorithm to a set of other options.",
                    "label": 0
                },
                {
                    "sent": "The algorithm that I presented earlier, we called, no regret KL rank.",
                    "label": 0
                },
                {
                    "sent": "In fact, we I announced that I would present on an alternativ algorithms which doesn't give the same guarantees in terms of regret, but we can consider it as an opportunistic version of the KL rank approach, which follows the steps that are done in KL rank.",
                    "label": 0
                },
                {
                    "sent": "Yet to win comes a time to to present.",
                    "label": 0
                },
                {
                    "sent": "The ranking will present the ranking according to the score directly and not.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Randomly.",
                    "label": 0
                },
                {
                    "sent": "We compared the algorithms to four ranking algorithms that are currently popular and that have made significant improvement on prior ones.",
                    "label": 0
                },
                {
                    "sent": "I'll just briefly through mentioning Rank Net which was presented by Virgil and his collaborator and page rank and hits are also graph based algorithms that were.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compared in our analysis so quickly, I have this table comparing the scores for the three different cost functions that I presented earlier.",
                    "label": 0
                },
                {
                    "sent": "For simplicity, I put it in parentesis.",
                    "label": 0
                },
                {
                    "sent": "I put the ordering of the different algorithms for these costs and we see that there might be a price to pay for having it for using an algorithm that has no regret overall.",
                    "label": 0
                },
                {
                    "sent": "The reason for this is somewhat related to the fact that.",
                    "label": 0
                },
                {
                    "sent": "Looking at these results, we might think that it's not a good algorithm, yet the guarantees tells us that if we apply the algorithm in that totally different context, or if we don't believe these users truly represents what the users will do tomorrow or a year from now, there no regret algorithm will provide the same guarantees.",
                    "label": 0
                },
                {
                    "sent": "While these are results that are strictly related to the historical data that is in this test.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you are someone that believes that.",
                    "label": 0
                },
                {
                    "sent": "If you see a curve you believe in that curve, then greedy Cal rank is something that outperforms all the other methods that I listed here.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, just to summarize quickly, and I mentioned earlier how click today.",
                    "label": 0
                },
                {
                    "sent": "There was a band abundant for this type of application and how the I hope I convinced you that the Cal rank model was a good fit for this type of task of ranking for users that are visiting a search engine.",
                    "label": 0
                },
                {
                    "sent": "The no regret Cal rank was the first to provide guarantees with respect to the long term performance in terms of regret and actually the performance guarantees do not depend on any assumption about.",
                    "label": 1
                },
                {
                    "sent": "The users that are visiting it's a general guarantee for the for the algorithm and for arbitrary sequence of users.",
                    "label": 0
                },
                {
                    "sent": "Finally, the analysis did provide guidance with respect to how to choose the features that would be added to.",
                    "label": 0
                },
                {
                    "sent": "To the feature set to create new policies without having over over over fitting, occur.",
                    "label": 0
                },
                {
                    "sent": "And finally we showed how greedy coloring might be a good approach in a case where you want tangible proof that things are working well with your environment, but.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a case by case analysis that needs to be performed and in the future work.",
                    "label": 0
                },
                {
                    "sent": "I still believe that greedy CLO Rang might have similar guarantees as the no regret one, which could be further investigated.",
                    "label": 0
                },
                {
                    "sent": "And of course, there's I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "The other trouble that people have with click through data which is biases that are due to the ordering itself, and also the fact that clickthrough can be manipulated if someone wants to see his web page hierarchy is going to click a lot of time on this item and.",
                    "label": 0
                },
                {
                    "sent": "This that this analysis didn't account for any of that, yet it's a it's.",
                    "label": 0
                },
                {
                    "sent": "It's an analysis of robustness of these.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kittens if you have any questions, I'll be happy to answer them now or at the poster session or at the break.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Could you go back to the phone?",
                    "label": 0
                },
                {
                    "sent": "Question does.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "1st.",
                    "label": 0
                },
                {
                    "sent": "My understanding is that you need to kind of realization.",
                    "label": 0
                },
                {
                    "sent": "You don't want to be.",
                    "label": 0
                },
                {
                    "sent": "Why did you choose this form?",
                    "label": 0
                },
                {
                    "sent": "I wanted to compare the random ranking distribution to some other distribution using Kyle Direct KL divergences.",
                    "label": 0
                },
                {
                    "sent": "But in the choice of the distribution, this seems to be this set seemed to be the set of any distribution which would have pleased the user as long as this item is first in the list, he doesn't really care what are the other, how the other items are ordered afterwards.",
                    "label": 0
                },
                {
                    "sent": "So this is somewhat of a distance between a set an appoint.",
                    "label": 0
                },
                {
                    "sent": "And then I have another question.",
                    "label": 0
                },
                {
                    "sent": "The fact that the greedy behaves much better than that, no regret approach.",
                    "label": 0
                },
                {
                    "sent": "Come from practices.",
                    "label": 0
                },
                {
                    "sent": "The problem is not the shuddering.",
                    "label": 0
                },
                {
                    "sent": "But also the taste of preferences.",
                    "label": 0
                },
                {
                    "sent": "Read what I have a feeling that so the question was that the greedy policy might do better because we are in a non stationary environment, yet I feel like both should be vulnerable to non stationary environment 'cause they both follow the same update rules.",
                    "label": 0
                },
                {
                    "sent": "It's just that greedy variant is more blind when applying the process but it is something that's right now for nonstationary context.",
                    "label": 0
                },
                {
                    "sent": "It seems like.",
                    "label": 0
                },
                {
                    "sent": "When the algorithm restarts, it should adapt to the nonstationarity.",
                    "label": 0
                },
                {
                    "sent": "But yeah, so you need exponentially more users too, as the as the process changes.",
                    "label": 0
                },
                {
                    "sent": "No, it's an interesting question to improve on.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}