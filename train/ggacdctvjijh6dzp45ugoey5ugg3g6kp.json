{
    "id": "ggacdctvjijh6dzp45ugoey5ugg3g6kp",
    "title": "Spherical Embedding and Classification",
    "info": {
        "author": [
            "Richard Wilson, Department of Computer Science, University of York"
        ],
        "published": "Sept. 13, 2010",
        "recorded": "August 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ssspr2010_wilson_sec/",
    "segmentation": [
        [
            "OK, so my talk will be slightly related to the previous one.",
            "It might have some relevance to some of the questions that the audience were asking asking there.",
            "So they're talking.",
            "My talk is fariko embedding and classification, but I'll talk mainly about the embedding part and just touch briefly on the idea of classification on spheres."
        ],
        [
            "OK, so to start with maybe a little bit of background.",
            "Probably heard a little bit of this already in their talks about dissimilarity this morning, but just to go through it kind of briefly again, dis similarities are quite a common starting point in pattern recognition.",
            "You usually get them when you're dealing with non vectorial data, so you might not have a vector representation of your objects, but you do have some way of measuring the similarity or dissimilarity between your different objects, so that's quite typical when you have.",
            "Structural representations you want to compare them.",
            "You might have to do some kind of alignment of the objects before you actually work, how similar they are to each other.",
            "So the starting point is a dissimilarity matrix.",
            "So we start from a dissimilarity matrix.",
            "We can then move to a similarity matrix by using this double centering operation to find the equivalent set of similarities so that original set of dissimilarity's.",
            "So this is just a simple step going from dissimilarity dissimilarity.",
            "Of course, you may measure the similarity directly rather than similarities first.",
            "So it's well known that if this matrix here similarity matrix is symmetric and positive semidefinite, then S is in fact a kernel matrix.",
            "So that means that the objects that you represent in this similarity matrix S can be represented by a kernel, and therefore you can use your favorite kernel machine on the data.",
            "So kernel PCA or support vector machine whatever you like.",
            "Or Alternatively, you can take the points and you can find an embedding like this in Euclidean space.",
            "So if you have a true Colonel then you'll guarantee that you can factorize this matrix using the eigen decomposition and cause these eigen values here in London positive and take the square root.",
            "We can split this kernel matrix up into two parts and we can find this equivalent embedding of the points into Euclidean space.",
            "So once you have a set of vectors representing the objects then you can use all of your techniques from.",
            "Standard vector based statistical pattern recognition and deal with the data.",
            "So this is quite a nice theory if you have this positive semidefinite matrix becausw you can identify this distance matrix dissimilarity matrix D with a distance and they'll be the Euclidean distances between the points which you found from your embedding.",
            "So that means we have this relationship between our original December act is measured and then the distance squared distance in Euclidean space and the inner product between the vectors that represent."
        ],
        [
            "The points.",
            "So unfortunately, when comparing structural representations in particular, we have this alignment process commonly between structures, and that means that the similarity matrix we get is usually not positive semidefinite.",
            "Usually it's indefinite, which means there negative eigenvalues in this similarity matrix, so we get negative eigenvalues as Lisa just saying if we try to factorize this as a kernel matrix then we can't take the square root here because we have negative eigenvalues.",
            "And therefore we can't find a Euclidean embedding of these points.",
            "So essentially what this means is that these are sets of objects that we can't represent as points in Euclidean space.",
            "An faithfully preserve the distances as dis similarities between those objects.",
            "OK, so once you get to that point you have two basic ways to move forward.",
            "So the first approach is to try and modify your dis similarities to make them Euclidean and then use the Euclidean space to represent the objects so that this was the approach that Eliza was just talking about to try and remove this non Euclidean part of the data and find a faithful embedding or as close as possible to the original data in Euclidean space.",
            "Of course it's inevitable if you do this first process here that you're going to throw away some of the information in the distances, so it may be that that information is noise and you can get away with throwing away, but certainly for some types of dissimilarity's it seems that if you throw away that information that you're actually losing something and you get much worse classification results.",
            "So the other way to go is to actually use a non Euclidean space to represent the data.",
            "So instead of saying that we want things in Euclidian space, we can find a different space where we can faithfully preserve these dissimilarities between the data and still do our operations without throwing things away.",
            "So if we're going to go down that route, we don't want to take just any space.",
            "We want a nice space in some sense, so we'd certainly like our space to be metric so we can do kind of geometric operations in it, and we'd like it to be feasible for us to compute distances in, so we'd like it to be fairly straightforward for us to compute the distances between two points embedded in this space."
        ],
        [
            "OK, so one of the reasons why we might want to embed the points into space in the 1st place is so that we can do statistical operations on them and in effect if we want to compute statistics and points, then we need to have a metric distance.",
            "So if we don't preserve the metric properties, then we're likely to be in big trouble in terms of computing statistics.",
            "So for example, if we violate the triangle inequality and essentially violating locality, and you can't say with certainty whether.",
            "A point that's close to one point will also be distant from another point that's far away, so points can be close to each other and then close to another point, but those first 2 points could be very far away from each other.",
            "So it's important that the distance measure that we have is metric.",
            "We know it with this place can't be Euclidean because we know we can't find the Euclidean embedding of the points.",
            "So that leads us to consider the class of Romanian spaces which fulfill these requirements for us.",
            "So in Romanian space we've heard a bit about it from alyzon from Fatty yesterday, so space is curved.",
            "The distances are not going to be Euclidean, and that means that we can use a curved space to represent indefinite similarities.",
            "So points embedded in a curved space will naturally give you an indefinite similarity matrix.",
            "These spaces are metric, so we have all the metric properties preserved and distance in these spaces are measured by geodesics.",
            "So essentially a geodesic is the shortest curve which joins two points in space which passes only through the manifold.",
            "So Judy's Excel, essentially the equivalent of straight lines, so the shortest distance between two points, but in kind of arbitrary remaining spaces it can be very difficult to compute what these drinks are, so essentially involves solving a set of differential equations in order computer desks.",
            "So we'd like also to use a space where it's easy to compute these geodesics.",
            "So those criterion lead us to look at the choice of constant curvature spaces as our embedding space."
        ],
        [
            "OK, so the space I'm particularly focused on is the spherical space.",
            "It's very familiar to us, 'cause of course it's represented by the surface of the Earth.",
            "So this is generally referred to as the elliptic manifold.",
            "We can visualize it as being the surface of a hypersphere, which is embedded in Euclidean space.",
            "So this is a 2 dimensional example of this manifold.",
            "It's the surface of a sphere, and then this sphere is embedded in three dimensional Euclidean space, which is external space of the manifold.",
            "So if we use that embedding of the manifold in Euclidean space, then we can define an embedding equation which looks like this.",
            "So it just says that all the points are a distance R away from the center of the sphere.",
            "It also gives us positive definite metric tensor for the surface of the manifolds.",
            "That means that we can measure distances and we're confident they'll be metric.",
            "And it has a constant curvature on the surface, so the curvature everywhere on the surface.",
            "This fear is the same, and the curvature sectional curvature, essentially the Gaussian curvature is given by one over the radius squared.",
            "So this way here is a well known example of this type of space, so I've given kind of the embedding of this fear here.",
            "These hypothetical spaces also have this nice parameterisation that we can use as well.",
            "So this is the Classic 3 dimensional spherical coordinate system.",
            "This can be extended quite straightforwardly up to any number of dimensions we want for hyper sphere, and we have this metric tensor here, so this quantity here is always positive and therefore we can guarantee that we can measure distances properly on the surface.",
            "And because this fear is embedded in a Euclidean space, we also have fact we can measure inner products in this space using the standard formula.",
            "So standard formula for an inner product.",
            "OK, so the interesting thing about the surface of the sphere, it's it's a very different from the standard Euclidean geometry.",
            "So just give you 2 examples of this.",
            "So if I started this point here and I move away from this point, perpendicular directions at right angles to each other if I did the same thing in Euclidean space, then we essentially move away.",
            "These points move away from each other and the distance would keep getting larger and larger, but on the surface of the sphere the distance starts increasing between these two points.",
            "But then when we get to this.",
            "Point it reaches a maximum and if I would continue down here, the distance will start getting smaller again.",
            "So on the surface of the sphere as you move away, distances are perhaps smaller than you'd expect in Euclidean space between two points.",
            "The second interesting thing about this manifold is that it's finite and close, so there's a maximum distance you can have between any two points.",
            "So if I select a point up here in a point on the opposite point at the bottom of the sphere here, that's the maximum distance I can have between any two points.",
            "So those two problems alone make it very different from Euclidean manifold."
        ],
        [
            "OK, so there's been quite a lot of previous work going back a fairway on Soquel manifolds, so I found one going back to the 1978.",
            "This is really looking at the problem of psychological data such as data where people rank the similarity of objects or colors or that kind of thing.",
            "And inevitably when they do that, they don't give you back a Euclidean set of similarities between those objects.",
            "So the idea here was to try and.",
            "Take some this psychological similarity data.",
            "An embedded on the surface of a sphere or on a hyperbolic space.",
            "So the hyperbolic space is the equivalent of sphere but has negative curvature everywhere rather than positive curvature.",
            "So they developed an optimization method which worked on small datasets.",
            "Because these psychological datasets, inevitably quite small because you have to have people there to rank the similarities of the data.",
            "So 1991 Cox and Cox kind of formalized the idea of embedding on non Euclidean manifolds.",
            "They define this idea of the stress of the configuration points, particularly on the surface of the sphere and then you can take this stress and you can try and minimize that stress to find the embedding of the points on the surface of sphere.",
            "So this stress on the surface of sphere actually gives you a difficult optimization problem.",
            "Turns out not to be very practical thing to do on large datasets.",
            "So when the number of points are interested in gets quite large, it's going to take a very long time to take that approach.",
            "I've also highlighted one much more recent thing here, so from 2008 this is interesting 'cause it's looking at the embedding of Internet connectivity, so the connectivity between nodes on the Internet and they look to embedding into hyperbolic space.",
            "So space of negative curvature.",
            "So in hyperbolic space as you move away, you expect the distances to be larger than they would be in Euclidean space, and they use their physics based simulation.",
            "It was essentially based on some kind of particle dynamics, so again, this is.",
            "This gives you quite an expensive optimization problem, so the goal of what we're trying to do here is to actually try and find an optimization problem which is approximate but much simpler to operate, and therefore we can operate on large point sets, large sets of data."
        ],
        [
            "OK, so the geodesics on on the surface this very very important to what we want to do.",
            "So just say a little bit about those so the geodesic curve on a manifold is a curve of shortest length which joins two points.",
            "OK, so we got an example here on the surface of the sphere.",
            "So we have the origin of our Euclidean embedding system at the center of the sphere and the surface of the sphere defined by those equations I showed you in previous slide.",
            "So if I take 2 points I&J on the surface this fear.",
            "Then the geodesic is the curve shortest length, which joins these two points on this surface.",
            "So in the case of this fear, this geodis exactly an arc of a great circle, a sphere which goes round with maximum radius around the sphere.",
            "So it's very simple to workout the length on the surface between these two points.",
            "It's just the radius sphere times the angle that these two points subtend at the center of the sphere.",
            "So this is my formula for computing distances on the surface.",
            "This fear the distance between two points is just our theater, so that gives me my property.",
            "I was looking forward to start, which is a very simple way to compute distances and because we have this inner product that we can use in the embedding space where you can find out the angle from this inner product and then find out the distance in terms of the original point positions in quite a straightforward way.",
            "So the only problem with this is we have this cost to minus one here, which makes the whole thing nonlinear."
        ],
        [
            "OK, so onto my problem.",
            "So what I want to do is what I want to find a set of points on the surface of a hyper sphere such that the geodesic distances between those points on the surface of the sphere are given by my original dissimilarity matrix D. So I'm going to formulate the problem as follow, so I want to minimize the squared differences between the sorry, the differences between the square distances squared, so it's a least squares problem between the square distances.",
            "Unfortunately, should be some in there so that summed over all the points pairs of points that we have.",
            "So this is the optimization problem I want to solve.",
            "And of course we have a constraint that my points have to lie on the surface of the sphere.",
            "Distance is the IJ here are defined by this geodesic equation here, which rates the position of the points to the distances between them.",
            "So this is kind of my main optimization problem, so this is a difficult problem to solve because it's a nonlinear constrained optimization problem.",
            "So of course there are methods available to solve this type of problem.",
            "But if we're talking about large numbers of points, so in some of my datasets I have between 2 and 3000 points, and trying to solve this kind of constrained optimization problem here can be very computationally expensive.",
            "So what I really want to do is try and find a way of simplifying this optimization problem and I'm going to do that first of all by updating the position of each point separately.",
            "So I'm going to fix all the other points, then try and find a good position for a single point in the problem."
        ],
        [
            "OK, so the next tool I need the exponential map, so we should all be experts on the exponential map.",
            "After that it's very nice talk on Wednesday.",
            "So what I'm going to do is use the exponential map in order to try and minimize the problem on the tangent plane of the surface.",
            "So I'll go through this briefly because we've heard something about it already.",
            "But essentially the tangent plane to surface is clearly in subspace, so if we have some manifold curved manifold here.",
            "Represented by this curved line.",
            "Then the tangent space here is tangent to the surface at some point M and it's flat.",
            "So it's a Euclidean space in effect.",
            "So we can use the exponential map to move from the manifold onto the tangent plane and back again.",
            "If I have some point why here on the surface of the manifold then I can use the log map to go to this point X on the tangent space and then I can use the exponential map to go from the point X on the tangent space back onto the manifold again.",
            "So I have this one to one mapping between points on the manifold, important space and this applies though only locally to this point M. So provider in a reasonably local position around this point M. Then we can do this mapping process.",
            "OK, so this is the notation we use here, so this is not an actual logo X function.",
            "This is just notation to actually denote these Maps, so log means log means move from the manifold onto the tangent plane around a point M. So M is the center of our exponential map and similar effects function.",
            "So the reason why they called Logan Max please.",
            "I think because they actually correspond to Logan X functions for certain types of manifold.",
            "But in this case they correspond to something different."
        ],
        [
            "K so the idea for a sphere is as follows.",
            "So we want to take points X on the sphere and we want to map them onto this tangent plane like this, and so effectively what we do is we take this curve and we unwrap it onto this point on the tangent plane.",
            "So this has the nice property that the distance between the origin of this Mount, the center here and this point X primed on the tangent plane is the same distance as between the origin and this point X on the surface of the sphere.",
            "So that's a property of the exponential map that its distance preserving around the center of the projection.",
            "So we can do this mapping process.",
            "Take my points off the search of this fear onto the tangent plane and then we can optimize my problem in the tangent plane.",
            "So the reason for doing that is because this is a Euclidean space, so we have this nice flat unconstrained space in which we can do the optimization."
        ],
        [
            "OK, so these are actual mechanics of the process, so given some Centerpoint M we can take point X on the sphere an we can project onto the tangent plane using this formula and the inverse function back onto the search.",
            "Is this fear again?",
            "And of course the tangent plane is flat, so the distance is measured on the tangent plane or just given by the standard Euclidean distance formula.",
            "So if we choose the center of the map to be one of our points XI, then the distances to the other points on the tangent plane will be exact, so the distances from the center XI to any of the other points on that project on the tangent plane will be the same as the distances between that point and the corresponding points on the manifold.",
            "So that means that if I project them using that center, then I can use those distances directly on the manifold in my optimization problem and then I can compute the gradient of the embedding error on the tangent plane for that particular point and finally update this point position XI to a better location."
        ],
        [
            "So this is the updating procedure, so this is my there is no error function and I can project these points onto the tangent plane workout the gradient of the error and Now it turns out very straightforward and then update my point position by simple gradient descent formula.",
            "So the reason why we use this and not some more sophisticated quasi Newton optimization method is because we are interested in large datasets so it becomes very computation expensive to compute all the second derivatives necessary for quasi Newton method.",
            "So we stick to a simple gradient descent method merely for reasons of speed.",
            "But we can choose this step size here in an optimal way.",
            "So it turns out if you go through the calculations you actually get a cubic for this property to the smallest root of that cube.",
            "It will give us the optimal.",
            "Step size for updates."
        ],
        [
            "OK, so because this is this kind of local gradient descent procedure, we need a good initialization of our points is no good starting points in a random position because they will converge to some bad local minimum in all probability.",
            "So we need to have a good starting point for optimization.",
            "So here we use the method which recently presented in CPR.",
            "I wouldn't really say much about it other than we basically operated by forming this matrix Z, which depends on the radius of the hypersphere from the original dissimilarity's, and then we can optimize over the smallest eigenvalue of this matrix in order to choose a good radius from my hopeless fear.",
            "And once we found out then we can do this.",
            "Actually, an embedding of the points onto the surface is fair, so it turns out that if our minimum eigenvalue that we can get here by changing the radius is 0, then the result is exact.",
            "So this embedding result gives you actually the correct points in the correct distances on the surface of a hypersphere.",
            "But generally speaking, you can't find a radius which gives you a zero eigenvalue of this matrix.",
            "Generally speaking, it's negative, so you still have these residual on Euclidean elements.",
            "So if it's the case, this is not zero, then it's a good starting point for optimization.",
            "We're going to run this optimization procedure."
        ],
        [
            "OK, so this is just to summarize the algorithm, so we start off with a set of dissimilarity's.",
            "Then we minimize ahead of our overall to find optimal radius for hyper sphere and initial embedding of the points and then for each of the points in this embedding.",
            "Then we map all of the points onto the tangent space around this point, where then use this optimization procedure on the tangent plane and then after we've done optimize the new point position here and we can map those points.",
            "Back onto the original manifold to get an updated point position and then we have to iterate over this a number of times in order to converge to final solution."
        ],
        [
            "So I'll just briefly mention classifiers in elliptical space, so one of the reasons for putting this data onto a manifold is that we can do statistical procedures on it, and one of the statistical procedures might want to do is to define some kind of classifier on the surface of the sphere so it turns out this is quite a tricky thing to do, and I think it's no problem for us at the moment.",
            "Exactly how to define classifieds correctly on the surface of a sphere, we obviously we can do things like the nearest neighbors classifier 'cause we have the distances.",
            "So that's fairly trivial.",
            "We can also compute the nearest mean classifier, so there's a well defined procedure for computing the mean on the surface of a sphere or other manifolds was essentially is an iterative process which involves projecting onto the tangent plane, taking the mean and then back onto the surface of the sphere.",
            "So essentially this solves this generalized mean problem of finding a mean value of a set of points, so we can use this generalized mean, define a mean for a set of different classes on the manifold, and use the nearest mean classifier to classify the points."
        ],
        [
            "OK, so have a look at a couple of embeddings and results, so this is the chicken pieces data set that Lisa showed in her talk, so the idea is to take this out of the similarities of the shapes of these objects and then project them as points onto the surface of a sphere.",
            "So this is an example of what you get if you use a 2 dimensional Euclidean embedding other points.",
            "So this is taking the two.",
            "Essentially the two principal components from the multidimensional scaling of those dissimilarity's.",
            "So you can see it does preserve quite a lot of the structure of the points, but I think you can see just looking at the shape of these points that this data seems to want to move onto a curved manifold.",
            "We certainly have some curved structures in this.",
            "If we project onto the surface this fear using our method will get results look like this and you can see the nature of these classes now.",
            "Looks quite different from what we got here, particularly for something like this red set of points.",
            "Here the structure is much more loosely packed than it was here, and these endpoints of these classes here.",
            "Why they separated in this Euclidean embedding?",
            "But in this embedding here they come much closer together.",
            "Because you have this curve surface, which means the points can join up again at the bottom."
        ],
        [
            "So, just briefly some more comprehensive results, so we have a large set of data.",
            "Dissimilarity data which we've seen in some other talks already, so all this data here has this property of having indefinite similarity matrices.",
            "So we embed the points on the surface of sphere and this is the error we get.",
            "So this is essentially the root mean square error of the normalized similarities.",
            "So we normalize the similarities with respect to the mean, and then we compute the root mean square error of the embedding.",
            "So for a lot of these datasets we get quite good embedding, so best one is about 2% different from the original dissimilarity's.",
            "And when we get down to here, we've got about 8 or 9%.",
            "We have a couple of datasets down here which don't give very good embeddings on the surface of the sphere.",
            "So clearly these are not datasets we can actually represent in that kind of space.",
            "In terms of classification, once we've embedded in the surface of the sphere, the results are quite variable.",
            "So it seems that some datasets do inherently have this kind of spherical manifold structure, and some of them don't.",
            "Not clear from the date of looked at why that is, or how to tell whether or not they have this spherical property, but we can see that some of these datasets over here certainly get better results if we use the nearest mean classifier is also an open problem for us.",
            "Exactly why this is for this data set, it's clearly something unusual about this data set.",
            "An projecting onto a spherical space seems to throw away a lot of the noise that's present in the original data there."
        ],
        [
            "OK, so just to conclude, so we can use Romanian spaces to represent data from dissimilarity measures when it's not possible to represent them in Euclidean space.",
            "So either non Euclidean in the 1st place or if removing the non Euclidean part will give us worse results than we can use our Romanian curved space to embed these points and try and preserve those distances.",
            "So I showed an efficient method for embedding those points onto elliptical space which will work on large datasets.",
            "So my largest data set and the results showed you there had, I think, 2600 points, so it's a reasonably sized data set and it produces embeddings of low distortion.",
            "So we can define simple classifiers on the surface of these on this vehicle manifold, but I think this is this is one of our open problems about how we can extend this to more complicated geometric classifiers.",
            "So, for example, we might be interested in how to formulate something that looks like a support vector machine on the surface of a sphere.",
            "So we need to extend this work to more sophisticated geometric classifiers.",
            "OK, that's all, thank you."
        ],
        [
            "Thank you very much questions.",
            "So you know all these.",
            "See, you're talking the talk before this.",
            "Guess the the best one can expect this, whatever the advantages to be gained by globally increasing the complexity of the surface you know.",
            "In other words, instead of having a Canadian space, have the.",
            "You just experiment remaining space and.",
            "And then.",
            "Let's go for the advantage that gives it, because otherwise anything extra that you derive depends upon where you're selected.",
            "Point is where you're placing the tangent plane.",
            "Yeah, so we have to construct the tangent plane around a specific point and then update that point because the distances which are not relative to central distorted.",
            "So when you do the embedding so you can get a lot by by being located at one point, but then that is that is going to be a problem for some other points which are away from that.",
            "Area and you will distort them in ways that are hard to.",
            "Follow so so the I guess the lesson 1 takes from this is that.",
            "You can gain a definite advantage by by going to sphere globally.",
            "To the extent that you have accommodated the non Euclidean character of the distances.",
            "By doing so, of course, we don't know whether that's going to how much that's going to yield depends upon what the nature of the distance is.",
            "The question is, is it possible that?",
            "Data is such that you actually will lose by going to sphere.",
            "Or any other specific choice over this.",
            "Reading the data the way it is.",
            "Is it possible to lose?",
            "Yes, I think it's the answer.",
            "The simple answer that I mean you can.",
            "There are methods which had another talk switch.",
            "Work with the original dissimilarity's so you don't have to construct or throw away anything there.",
            "If you choose a particular embedding space then your data doesn't really conform to the shape of that space.",
            "Then you're going to be throwing something away.",
            "The big problem we have at the moment is we don't have any way a priority to tell what shape the space should be, so we don't know if a sphere is the right shape or a hyperbolic space.",
            "Or simply using Euclidean space and throw away whatever's left, so I think that's that's also problem that we're interested in, but we haven't solved yet how to actually tell from the original dissimilarity's what the appropriate space would be for the embedding.",
            "The fact that the ship that is being used is more complicated than the.",
            "Euclidean flat space.",
            "That alone does not guarantee that you're going to.",
            "Infinite, well, it's more complicated in the sense that the shape is faster, more complicated, but in doing this embedding process, we lose one of the original dimensions.",
            "So in effect, the space is one dimension less, but we have an extra variable of the curvature of the space, so it's approximately the same amount of of information that we can put on the manifold.",
            "Yeah OK, one more question.",
            "Actually, I enjoy the talk very much and a comment to your question.",
            "The labels are also important, not just the shape of the kind of the main fault.",
            "It could be the case that all the bank, all class of the points, same class could be concentrated in one side and you want to learn a classifier on it, right?",
            "So simple embedded might work might but, but I was wondering how you decide to dimensional too hyper sphere.",
            "Do you work on the same dimensions or go higher dimension?",
            "So let me just skip back a couple of slides, so that's something I didn't talk about, which is in this this work we presented CPR so this this original embedding process so implies that we take the original dimensionality of the data.",
            "So the number of points and we lose one of the dimensions in determine the radius.",
            "The optimal radius of the sphere.",
            "So the embedding this fear is in one less than the original number of dimensions.",
            "But you can increase the dimensionality, right?",
            "We could yes.",
            "But then we have an under constrained problem I think, which would again introduce difficulties.",
            "So if the data is intrinsically less as less dimension on the surface of the sphere than the original data set, then we discovered that in this process 'cause we get more than 1 zero eigenvalue in here and we're outta discard those.",
            "But we don't have any way of saying how much bigger we could make it and still get a good result.",
            "Any other question?",
            "If not, then thank you very much and thanks to all the speakers and the session is closed."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so my talk will be slightly related to the previous one.",
                    "label": 0
                },
                {
                    "sent": "It might have some relevance to some of the questions that the audience were asking asking there.",
                    "label": 0
                },
                {
                    "sent": "So they're talking.",
                    "label": 0
                },
                {
                    "sent": "My talk is fariko embedding and classification, but I'll talk mainly about the embedding part and just touch briefly on the idea of classification on spheres.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to start with maybe a little bit of background.",
                    "label": 0
                },
                {
                    "sent": "Probably heard a little bit of this already in their talks about dissimilarity this morning, but just to go through it kind of briefly again, dis similarities are quite a common starting point in pattern recognition.",
                    "label": 1
                },
                {
                    "sent": "You usually get them when you're dealing with non vectorial data, so you might not have a vector representation of your objects, but you do have some way of measuring the similarity or dissimilarity between your different objects, so that's quite typical when you have.",
                    "label": 0
                },
                {
                    "sent": "Structural representations you want to compare them.",
                    "label": 0
                },
                {
                    "sent": "You might have to do some kind of alignment of the objects before you actually work, how similar they are to each other.",
                    "label": 0
                },
                {
                    "sent": "So the starting point is a dissimilarity matrix.",
                    "label": 0
                },
                {
                    "sent": "So we start from a dissimilarity matrix.",
                    "label": 0
                },
                {
                    "sent": "We can then move to a similarity matrix by using this double centering operation to find the equivalent set of similarities so that original set of dissimilarity's.",
                    "label": 0
                },
                {
                    "sent": "So this is just a simple step going from dissimilarity dissimilarity.",
                    "label": 0
                },
                {
                    "sent": "Of course, you may measure the similarity directly rather than similarities first.",
                    "label": 1
                },
                {
                    "sent": "So it's well known that if this matrix here similarity matrix is symmetric and positive semidefinite, then S is in fact a kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "So that means that the objects that you represent in this similarity matrix S can be represented by a kernel, and therefore you can use your favorite kernel machine on the data.",
                    "label": 0
                },
                {
                    "sent": "So kernel PCA or support vector machine whatever you like.",
                    "label": 0
                },
                {
                    "sent": "Or Alternatively, you can take the points and you can find an embedding like this in Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So if you have a true Colonel then you'll guarantee that you can factorize this matrix using the eigen decomposition and cause these eigen values here in London positive and take the square root.",
                    "label": 0
                },
                {
                    "sent": "We can split this kernel matrix up into two parts and we can find this equivalent embedding of the points into Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So once you have a set of vectors representing the objects then you can use all of your techniques from.",
                    "label": 0
                },
                {
                    "sent": "Standard vector based statistical pattern recognition and deal with the data.",
                    "label": 0
                },
                {
                    "sent": "So this is quite a nice theory if you have this positive semidefinite matrix becausw you can identify this distance matrix dissimilarity matrix D with a distance and they'll be the Euclidean distances between the points which you found from your embedding.",
                    "label": 1
                },
                {
                    "sent": "So that means we have this relationship between our original December act is measured and then the distance squared distance in Euclidean space and the inner product between the vectors that represent.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The points.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately, when comparing structural representations in particular, we have this alignment process commonly between structures, and that means that the similarity matrix we get is usually not positive semidefinite.",
                    "label": 1
                },
                {
                    "sent": "Usually it's indefinite, which means there negative eigenvalues in this similarity matrix, so we get negative eigenvalues as Lisa just saying if we try to factorize this as a kernel matrix then we can't take the square root here because we have negative eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "And therefore we can't find a Euclidean embedding of these points.",
                    "label": 1
                },
                {
                    "sent": "So essentially what this means is that these are sets of objects that we can't represent as points in Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "An faithfully preserve the distances as dis similarities between those objects.",
                    "label": 0
                },
                {
                    "sent": "OK, so once you get to that point you have two basic ways to move forward.",
                    "label": 0
                },
                {
                    "sent": "So the first approach is to try and modify your dis similarities to make them Euclidean and then use the Euclidean space to represent the objects so that this was the approach that Eliza was just talking about to try and remove this non Euclidean part of the data and find a faithful embedding or as close as possible to the original data in Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "Of course it's inevitable if you do this first process here that you're going to throw away some of the information in the distances, so it may be that that information is noise and you can get away with throwing away, but certainly for some types of dissimilarity's it seems that if you throw away that information that you're actually losing something and you get much worse classification results.",
                    "label": 0
                },
                {
                    "sent": "So the other way to go is to actually use a non Euclidean space to represent the data.",
                    "label": 0
                },
                {
                    "sent": "So instead of saying that we want things in Euclidian space, we can find a different space where we can faithfully preserve these dissimilarities between the data and still do our operations without throwing things away.",
                    "label": 0
                },
                {
                    "sent": "So if we're going to go down that route, we don't want to take just any space.",
                    "label": 0
                },
                {
                    "sent": "We want a nice space in some sense, so we'd certainly like our space to be metric so we can do kind of geometric operations in it, and we'd like it to be feasible for us to compute distances in, so we'd like it to be fairly straightforward for us to compute the distances between two points embedded in this space.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so one of the reasons why we might want to embed the points into space in the 1st place is so that we can do statistical operations on them and in effect if we want to compute statistics and points, then we need to have a metric distance.",
                    "label": 1
                },
                {
                    "sent": "So if we don't preserve the metric properties, then we're likely to be in big trouble in terms of computing statistics.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we violate the triangle inequality and essentially violating locality, and you can't say with certainty whether.",
                    "label": 0
                },
                {
                    "sent": "A point that's close to one point will also be distant from another point that's far away, so points can be close to each other and then close to another point, but those first 2 points could be very far away from each other.",
                    "label": 0
                },
                {
                    "sent": "So it's important that the distance measure that we have is metric.",
                    "label": 0
                },
                {
                    "sent": "We know it with this place can't be Euclidean because we know we can't find the Euclidean embedding of the points.",
                    "label": 0
                },
                {
                    "sent": "So that leads us to consider the class of Romanian spaces which fulfill these requirements for us.",
                    "label": 0
                },
                {
                    "sent": "So in Romanian space we've heard a bit about it from alyzon from Fatty yesterday, so space is curved.",
                    "label": 1
                },
                {
                    "sent": "The distances are not going to be Euclidean, and that means that we can use a curved space to represent indefinite similarities.",
                    "label": 1
                },
                {
                    "sent": "So points embedded in a curved space will naturally give you an indefinite similarity matrix.",
                    "label": 1
                },
                {
                    "sent": "These spaces are metric, so we have all the metric properties preserved and distance in these spaces are measured by geodesics.",
                    "label": 0
                },
                {
                    "sent": "So essentially a geodesic is the shortest curve which joins two points in space which passes only through the manifold.",
                    "label": 0
                },
                {
                    "sent": "So Judy's Excel, essentially the equivalent of straight lines, so the shortest distance between two points, but in kind of arbitrary remaining spaces it can be very difficult to compute what these drinks are, so essentially involves solving a set of differential equations in order computer desks.",
                    "label": 0
                },
                {
                    "sent": "So we'd like also to use a space where it's easy to compute these geodesics.",
                    "label": 0
                },
                {
                    "sent": "So those criterion lead us to look at the choice of constant curvature spaces as our embedding space.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the space I'm particularly focused on is the spherical space.",
                    "label": 0
                },
                {
                    "sent": "It's very familiar to us, 'cause of course it's represented by the surface of the Earth.",
                    "label": 0
                },
                {
                    "sent": "So this is generally referred to as the elliptic manifold.",
                    "label": 1
                },
                {
                    "sent": "We can visualize it as being the surface of a hypersphere, which is embedded in Euclidean space.",
                    "label": 1
                },
                {
                    "sent": "So this is a 2 dimensional example of this manifold.",
                    "label": 1
                },
                {
                    "sent": "It's the surface of a sphere, and then this sphere is embedded in three dimensional Euclidean space, which is external space of the manifold.",
                    "label": 0
                },
                {
                    "sent": "So if we use that embedding of the manifold in Euclidean space, then we can define an embedding equation which looks like this.",
                    "label": 1
                },
                {
                    "sent": "So it just says that all the points are a distance R away from the center of the sphere.",
                    "label": 0
                },
                {
                    "sent": "It also gives us positive definite metric tensor for the surface of the manifolds.",
                    "label": 0
                },
                {
                    "sent": "That means that we can measure distances and we're confident they'll be metric.",
                    "label": 0
                },
                {
                    "sent": "And it has a constant curvature on the surface, so the curvature everywhere on the surface.",
                    "label": 0
                },
                {
                    "sent": "This fear is the same, and the curvature sectional curvature, essentially the Gaussian curvature is given by one over the radius squared.",
                    "label": 0
                },
                {
                    "sent": "So this way here is a well known example of this type of space, so I've given kind of the embedding of this fear here.",
                    "label": 0
                },
                {
                    "sent": "These hypothetical spaces also have this nice parameterisation that we can use as well.",
                    "label": 0
                },
                {
                    "sent": "So this is the Classic 3 dimensional spherical coordinate system.",
                    "label": 0
                },
                {
                    "sent": "This can be extended quite straightforwardly up to any number of dimensions we want for hyper sphere, and we have this metric tensor here, so this quantity here is always positive and therefore we can guarantee that we can measure distances properly on the surface.",
                    "label": 0
                },
                {
                    "sent": "And because this fear is embedded in a Euclidean space, we also have fact we can measure inner products in this space using the standard formula.",
                    "label": 0
                },
                {
                    "sent": "So standard formula for an inner product.",
                    "label": 0
                },
                {
                    "sent": "OK, so the interesting thing about the surface of the sphere, it's it's a very different from the standard Euclidean geometry.",
                    "label": 0
                },
                {
                    "sent": "So just give you 2 examples of this.",
                    "label": 0
                },
                {
                    "sent": "So if I started this point here and I move away from this point, perpendicular directions at right angles to each other if I did the same thing in Euclidean space, then we essentially move away.",
                    "label": 0
                },
                {
                    "sent": "These points move away from each other and the distance would keep getting larger and larger, but on the surface of the sphere the distance starts increasing between these two points.",
                    "label": 0
                },
                {
                    "sent": "But then when we get to this.",
                    "label": 0
                },
                {
                    "sent": "Point it reaches a maximum and if I would continue down here, the distance will start getting smaller again.",
                    "label": 0
                },
                {
                    "sent": "So on the surface of the sphere as you move away, distances are perhaps smaller than you'd expect in Euclidean space between two points.",
                    "label": 0
                },
                {
                    "sent": "The second interesting thing about this manifold is that it's finite and close, so there's a maximum distance you can have between any two points.",
                    "label": 0
                },
                {
                    "sent": "So if I select a point up here in a point on the opposite point at the bottom of the sphere here, that's the maximum distance I can have between any two points.",
                    "label": 0
                },
                {
                    "sent": "So those two problems alone make it very different from Euclidean manifold.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there's been quite a lot of previous work going back a fairway on Soquel manifolds, so I found one going back to the 1978.",
                    "label": 0
                },
                {
                    "sent": "This is really looking at the problem of psychological data such as data where people rank the similarity of objects or colors or that kind of thing.",
                    "label": 0
                },
                {
                    "sent": "And inevitably when they do that, they don't give you back a Euclidean set of similarities between those objects.",
                    "label": 0
                },
                {
                    "sent": "So the idea here was to try and.",
                    "label": 0
                },
                {
                    "sent": "Take some this psychological similarity data.",
                    "label": 1
                },
                {
                    "sent": "An embedded on the surface of a sphere or on a hyperbolic space.",
                    "label": 0
                },
                {
                    "sent": "So the hyperbolic space is the equivalent of sphere but has negative curvature everywhere rather than positive curvature.",
                    "label": 0
                },
                {
                    "sent": "So they developed an optimization method which worked on small datasets.",
                    "label": 0
                },
                {
                    "sent": "Because these psychological datasets, inevitably quite small because you have to have people there to rank the similarities of the data.",
                    "label": 1
                },
                {
                    "sent": "So 1991 Cox and Cox kind of formalized the idea of embedding on non Euclidean manifolds.",
                    "label": 0
                },
                {
                    "sent": "They define this idea of the stress of the configuration points, particularly on the surface of the sphere and then you can take this stress and you can try and minimize that stress to find the embedding of the points on the surface of sphere.",
                    "label": 1
                },
                {
                    "sent": "So this stress on the surface of sphere actually gives you a difficult optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Turns out not to be very practical thing to do on large datasets.",
                    "label": 0
                },
                {
                    "sent": "So when the number of points are interested in gets quite large, it's going to take a very long time to take that approach.",
                    "label": 0
                },
                {
                    "sent": "I've also highlighted one much more recent thing here, so from 2008 this is interesting 'cause it's looking at the embedding of Internet connectivity, so the connectivity between nodes on the Internet and they look to embedding into hyperbolic space.",
                    "label": 1
                },
                {
                    "sent": "So space of negative curvature.",
                    "label": 0
                },
                {
                    "sent": "So in hyperbolic space as you move away, you expect the distances to be larger than they would be in Euclidean space, and they use their physics based simulation.",
                    "label": 0
                },
                {
                    "sent": "It was essentially based on some kind of particle dynamics, so again, this is.",
                    "label": 0
                },
                {
                    "sent": "This gives you quite an expensive optimization problem, so the goal of what we're trying to do here is to actually try and find an optimization problem which is approximate but much simpler to operate, and therefore we can operate on large point sets, large sets of data.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the geodesics on on the surface this very very important to what we want to do.",
                    "label": 0
                },
                {
                    "sent": "So just say a little bit about those so the geodesic curve on a manifold is a curve of shortest length which joins two points.",
                    "label": 1
                },
                {
                    "sent": "OK, so we got an example here on the surface of the sphere.",
                    "label": 0
                },
                {
                    "sent": "So we have the origin of our Euclidean embedding system at the center of the sphere and the surface of the sphere defined by those equations I showed you in previous slide.",
                    "label": 0
                },
                {
                    "sent": "So if I take 2 points I&J on the surface this fear.",
                    "label": 1
                },
                {
                    "sent": "Then the geodesic is the curve shortest length, which joins these two points on this surface.",
                    "label": 0
                },
                {
                    "sent": "So in the case of this fear, this geodis exactly an arc of a great circle, a sphere which goes round with maximum radius around the sphere.",
                    "label": 0
                },
                {
                    "sent": "So it's very simple to workout the length on the surface between these two points.",
                    "label": 1
                },
                {
                    "sent": "It's just the radius sphere times the angle that these two points subtend at the center of the sphere.",
                    "label": 0
                },
                {
                    "sent": "So this is my formula for computing distances on the surface.",
                    "label": 0
                },
                {
                    "sent": "This fear the distance between two points is just our theater, so that gives me my property.",
                    "label": 0
                },
                {
                    "sent": "I was looking forward to start, which is a very simple way to compute distances and because we have this inner product that we can use in the embedding space where you can find out the angle from this inner product and then find out the distance in terms of the original point positions in quite a straightforward way.",
                    "label": 0
                },
                {
                    "sent": "So the only problem with this is we have this cost to minus one here, which makes the whole thing nonlinear.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so onto my problem.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do is what I want to find a set of points on the surface of a hyper sphere such that the geodesic distances between those points on the surface of the sphere are given by my original dissimilarity matrix D. So I'm going to formulate the problem as follow, so I want to minimize the squared differences between the sorry, the differences between the square distances squared, so it's a least squares problem between the square distances.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately, should be some in there so that summed over all the points pairs of points that we have.",
                    "label": 0
                },
                {
                    "sent": "So this is the optimization problem I want to solve.",
                    "label": 0
                },
                {
                    "sent": "And of course we have a constraint that my points have to lie on the surface of the sphere.",
                    "label": 0
                },
                {
                    "sent": "Distance is the IJ here are defined by this geodesic equation here, which rates the position of the points to the distances between them.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of my main optimization problem, so this is a difficult problem to solve because it's a nonlinear constrained optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So of course there are methods available to solve this type of problem.",
                    "label": 0
                },
                {
                    "sent": "But if we're talking about large numbers of points, so in some of my datasets I have between 2 and 3000 points, and trying to solve this kind of constrained optimization problem here can be very computationally expensive.",
                    "label": 0
                },
                {
                    "sent": "So what I really want to do is try and find a way of simplifying this optimization problem and I'm going to do that first of all by updating the position of each point separately.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to fix all the other points, then try and find a good position for a single point in the problem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the next tool I need the exponential map, so we should all be experts on the exponential map.",
                    "label": 0
                },
                {
                    "sent": "After that it's very nice talk on Wednesday.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is use the exponential map in order to try and minimize the problem on the tangent plane of the surface.",
                    "label": 1
                },
                {
                    "sent": "So I'll go through this briefly because we've heard something about it already.",
                    "label": 0
                },
                {
                    "sent": "But essentially the tangent plane to surface is clearly in subspace, so if we have some manifold curved manifold here.",
                    "label": 0
                },
                {
                    "sent": "Represented by this curved line.",
                    "label": 0
                },
                {
                    "sent": "Then the tangent space here is tangent to the surface at some point M and it's flat.",
                    "label": 1
                },
                {
                    "sent": "So it's a Euclidean space in effect.",
                    "label": 0
                },
                {
                    "sent": "So we can use the exponential map to move from the manifold onto the tangent plane and back again.",
                    "label": 0
                },
                {
                    "sent": "If I have some point why here on the surface of the manifold then I can use the log map to go to this point X on the tangent space and then I can use the exponential map to go from the point X on the tangent space back onto the manifold again.",
                    "label": 1
                },
                {
                    "sent": "So I have this one to one mapping between points on the manifold, important space and this applies though only locally to this point M. So provider in a reasonably local position around this point M. Then we can do this mapping process.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the notation we use here, so this is not an actual logo X function.",
                    "label": 0
                },
                {
                    "sent": "This is just notation to actually denote these Maps, so log means log means move from the manifold onto the tangent plane around a point M. So M is the center of our exponential map and similar effects function.",
                    "label": 0
                },
                {
                    "sent": "So the reason why they called Logan Max please.",
                    "label": 0
                },
                {
                    "sent": "I think because they actually correspond to Logan X functions for certain types of manifold.",
                    "label": 0
                },
                {
                    "sent": "But in this case they correspond to something different.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K so the idea for a sphere is as follows.",
                    "label": 0
                },
                {
                    "sent": "So we want to take points X on the sphere and we want to map them onto this tangent plane like this, and so effectively what we do is we take this curve and we unwrap it onto this point on the tangent plane.",
                    "label": 1
                },
                {
                    "sent": "So this has the nice property that the distance between the origin of this Mount, the center here and this point X primed on the tangent plane is the same distance as between the origin and this point X on the surface of the sphere.",
                    "label": 0
                },
                {
                    "sent": "So that's a property of the exponential map that its distance preserving around the center of the projection.",
                    "label": 0
                },
                {
                    "sent": "So we can do this mapping process.",
                    "label": 0
                },
                {
                    "sent": "Take my points off the search of this fear onto the tangent plane and then we can optimize my problem in the tangent plane.",
                    "label": 1
                },
                {
                    "sent": "So the reason for doing that is because this is a Euclidean space, so we have this nice flat unconstrained space in which we can do the optimization.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so these are actual mechanics of the process, so given some Centerpoint M we can take point X on the sphere an we can project onto the tangent plane using this formula and the inverse function back onto the search.",
                    "label": 1
                },
                {
                    "sent": "Is this fear again?",
                    "label": 0
                },
                {
                    "sent": "And of course the tangent plane is flat, so the distance is measured on the tangent plane or just given by the standard Euclidean distance formula.",
                    "label": 1
                },
                {
                    "sent": "So if we choose the center of the map to be one of our points XI, then the distances to the other points on the tangent plane will be exact, so the distances from the center XI to any of the other points on that project on the tangent plane will be the same as the distances between that point and the corresponding points on the manifold.",
                    "label": 1
                },
                {
                    "sent": "So that means that if I project them using that center, then I can use those distances directly on the manifold in my optimization problem and then I can compute the gradient of the embedding error on the tangent plane for that particular point and finally update this point position XI to a better location.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the updating procedure, so this is my there is no error function and I can project these points onto the tangent plane workout the gradient of the error and Now it turns out very straightforward and then update my point position by simple gradient descent formula.",
                    "label": 0
                },
                {
                    "sent": "So the reason why we use this and not some more sophisticated quasi Newton optimization method is because we are interested in large datasets so it becomes very computation expensive to compute all the second derivatives necessary for quasi Newton method.",
                    "label": 1
                },
                {
                    "sent": "So we stick to a simple gradient descent method merely for reasons of speed.",
                    "label": 1
                },
                {
                    "sent": "But we can choose this step size here in an optimal way.",
                    "label": 1
                },
                {
                    "sent": "So it turns out if you go through the calculations you actually get a cubic for this property to the smallest root of that cube.",
                    "label": 0
                },
                {
                    "sent": "It will give us the optimal.",
                    "label": 0
                },
                {
                    "sent": "Step size for updates.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so because this is this kind of local gradient descent procedure, we need a good initialization of our points is no good starting points in a random position because they will converge to some bad local minimum in all probability.",
                    "label": 0
                },
                {
                    "sent": "So we need to have a good starting point for optimization.",
                    "label": 1
                },
                {
                    "sent": "So here we use the method which recently presented in CPR.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't really say much about it other than we basically operated by forming this matrix Z, which depends on the radius of the hypersphere from the original dissimilarity's, and then we can optimize over the smallest eigenvalue of this matrix in order to choose a good radius from my hopeless fear.",
                    "label": 0
                },
                {
                    "sent": "And once we found out then we can do this.",
                    "label": 1
                },
                {
                    "sent": "Actually, an embedding of the points onto the surface is fair, so it turns out that if our minimum eigenvalue that we can get here by changing the radius is 0, then the result is exact.",
                    "label": 0
                },
                {
                    "sent": "So this embedding result gives you actually the correct points in the correct distances on the surface of a hypersphere.",
                    "label": 0
                },
                {
                    "sent": "But generally speaking, you can't find a radius which gives you a zero eigenvalue of this matrix.",
                    "label": 0
                },
                {
                    "sent": "Generally speaking, it's negative, so you still have these residual on Euclidean elements.",
                    "label": 0
                },
                {
                    "sent": "So if it's the case, this is not zero, then it's a good starting point for optimization.",
                    "label": 0
                },
                {
                    "sent": "We're going to run this optimization procedure.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is just to summarize the algorithm, so we start off with a set of dissimilarity's.",
                    "label": 0
                },
                {
                    "sent": "Then we minimize ahead of our overall to find optimal radius for hyper sphere and initial embedding of the points and then for each of the points in this embedding.",
                    "label": 1
                },
                {
                    "sent": "Then we map all of the points onto the tangent space around this point, where then use this optimization procedure on the tangent plane and then after we've done optimize the new point position here and we can map those points.",
                    "label": 0
                },
                {
                    "sent": "Back onto the original manifold to get an updated point position and then we have to iterate over this a number of times in order to converge to final solution.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll just briefly mention classifiers in elliptical space, so one of the reasons for putting this data onto a manifold is that we can do statistical procedures on it, and one of the statistical procedures might want to do is to define some kind of classifier on the surface of the sphere so it turns out this is quite a tricky thing to do, and I think it's no problem for us at the moment.",
                    "label": 1
                },
                {
                    "sent": "Exactly how to define classifieds correctly on the surface of a sphere, we obviously we can do things like the nearest neighbors classifier 'cause we have the distances.",
                    "label": 1
                },
                {
                    "sent": "So that's fairly trivial.",
                    "label": 0
                },
                {
                    "sent": "We can also compute the nearest mean classifier, so there's a well defined procedure for computing the mean on the surface of a sphere or other manifolds was essentially is an iterative process which involves projecting onto the tangent plane, taking the mean and then back onto the surface of the sphere.",
                    "label": 1
                },
                {
                    "sent": "So essentially this solves this generalized mean problem of finding a mean value of a set of points, so we can use this generalized mean, define a mean for a set of different classes on the manifold, and use the nearest mean classifier to classify the points.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so have a look at a couple of embeddings and results, so this is the chicken pieces data set that Lisa showed in her talk, so the idea is to take this out of the similarities of the shapes of these objects and then project them as points onto the surface of a sphere.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of what you get if you use a 2 dimensional Euclidean embedding other points.",
                    "label": 0
                },
                {
                    "sent": "So this is taking the two.",
                    "label": 0
                },
                {
                    "sent": "Essentially the two principal components from the multidimensional scaling of those dissimilarity's.",
                    "label": 0
                },
                {
                    "sent": "So you can see it does preserve quite a lot of the structure of the points, but I think you can see just looking at the shape of these points that this data seems to want to move onto a curved manifold.",
                    "label": 0
                },
                {
                    "sent": "We certainly have some curved structures in this.",
                    "label": 0
                },
                {
                    "sent": "If we project onto the surface this fear using our method will get results look like this and you can see the nature of these classes now.",
                    "label": 0
                },
                {
                    "sent": "Looks quite different from what we got here, particularly for something like this red set of points.",
                    "label": 0
                },
                {
                    "sent": "Here the structure is much more loosely packed than it was here, and these endpoints of these classes here.",
                    "label": 0
                },
                {
                    "sent": "Why they separated in this Euclidean embedding?",
                    "label": 0
                },
                {
                    "sent": "But in this embedding here they come much closer together.",
                    "label": 0
                },
                {
                    "sent": "Because you have this curve surface, which means the points can join up again at the bottom.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, just briefly some more comprehensive results, so we have a large set of data.",
                    "label": 0
                },
                {
                    "sent": "Dissimilarity data which we've seen in some other talks already, so all this data here has this property of having indefinite similarity matrices.",
                    "label": 0
                },
                {
                    "sent": "So we embed the points on the surface of sphere and this is the error we get.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially the root mean square error of the normalized similarities.",
                    "label": 0
                },
                {
                    "sent": "So we normalize the similarities with respect to the mean, and then we compute the root mean square error of the embedding.",
                    "label": 0
                },
                {
                    "sent": "So for a lot of these datasets we get quite good embedding, so best one is about 2% different from the original dissimilarity's.",
                    "label": 0
                },
                {
                    "sent": "And when we get down to here, we've got about 8 or 9%.",
                    "label": 0
                },
                {
                    "sent": "We have a couple of datasets down here which don't give very good embeddings on the surface of the sphere.",
                    "label": 0
                },
                {
                    "sent": "So clearly these are not datasets we can actually represent in that kind of space.",
                    "label": 0
                },
                {
                    "sent": "In terms of classification, once we've embedded in the surface of the sphere, the results are quite variable.",
                    "label": 0
                },
                {
                    "sent": "So it seems that some datasets do inherently have this kind of spherical manifold structure, and some of them don't.",
                    "label": 0
                },
                {
                    "sent": "Not clear from the date of looked at why that is, or how to tell whether or not they have this spherical property, but we can see that some of these datasets over here certainly get better results if we use the nearest mean classifier is also an open problem for us.",
                    "label": 0
                },
                {
                    "sent": "Exactly why this is for this data set, it's clearly something unusual about this data set.",
                    "label": 0
                },
                {
                    "sent": "An projecting onto a spherical space seems to throw away a lot of the noise that's present in the original data there.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to conclude, so we can use Romanian spaces to represent data from dissimilarity measures when it's not possible to represent them in Euclidean space.",
                    "label": 1
                },
                {
                    "sent": "So either non Euclidean in the 1st place or if removing the non Euclidean part will give us worse results than we can use our Romanian curved space to embed these points and try and preserve those distances.",
                    "label": 1
                },
                {
                    "sent": "So I showed an efficient method for embedding those points onto elliptical space which will work on large datasets.",
                    "label": 0
                },
                {
                    "sent": "So my largest data set and the results showed you there had, I think, 2600 points, so it's a reasonably sized data set and it produces embeddings of low distortion.",
                    "label": 0
                },
                {
                    "sent": "So we can define simple classifiers on the surface of these on this vehicle manifold, but I think this is this is one of our open problems about how we can extend this to more complicated geometric classifiers.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we might be interested in how to formulate something that looks like a support vector machine on the surface of a sphere.",
                    "label": 1
                },
                {
                    "sent": "So we need to extend this work to more sophisticated geometric classifiers.",
                    "label": 0
                },
                {
                    "sent": "OK, that's all, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much questions.",
                    "label": 0
                },
                {
                    "sent": "So you know all these.",
                    "label": 0
                },
                {
                    "sent": "See, you're talking the talk before this.",
                    "label": 0
                },
                {
                    "sent": "Guess the the best one can expect this, whatever the advantages to be gained by globally increasing the complexity of the surface you know.",
                    "label": 0
                },
                {
                    "sent": "In other words, instead of having a Canadian space, have the.",
                    "label": 0
                },
                {
                    "sent": "You just experiment remaining space and.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Let's go for the advantage that gives it, because otherwise anything extra that you derive depends upon where you're selected.",
                    "label": 0
                },
                {
                    "sent": "Point is where you're placing the tangent plane.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we have to construct the tangent plane around a specific point and then update that point because the distances which are not relative to central distorted.",
                    "label": 0
                },
                {
                    "sent": "So when you do the embedding so you can get a lot by by being located at one point, but then that is that is going to be a problem for some other points which are away from that.",
                    "label": 0
                },
                {
                    "sent": "Area and you will distort them in ways that are hard to.",
                    "label": 0
                },
                {
                    "sent": "Follow so so the I guess the lesson 1 takes from this is that.",
                    "label": 0
                },
                {
                    "sent": "You can gain a definite advantage by by going to sphere globally.",
                    "label": 0
                },
                {
                    "sent": "To the extent that you have accommodated the non Euclidean character of the distances.",
                    "label": 0
                },
                {
                    "sent": "By doing so, of course, we don't know whether that's going to how much that's going to yield depends upon what the nature of the distance is.",
                    "label": 0
                },
                {
                    "sent": "The question is, is it possible that?",
                    "label": 0
                },
                {
                    "sent": "Data is such that you actually will lose by going to sphere.",
                    "label": 0
                },
                {
                    "sent": "Or any other specific choice over this.",
                    "label": 0
                },
                {
                    "sent": "Reading the data the way it is.",
                    "label": 0
                },
                {
                    "sent": "Is it possible to lose?",
                    "label": 0
                },
                {
                    "sent": "Yes, I think it's the answer.",
                    "label": 0
                },
                {
                    "sent": "The simple answer that I mean you can.",
                    "label": 0
                },
                {
                    "sent": "There are methods which had another talk switch.",
                    "label": 0
                },
                {
                    "sent": "Work with the original dissimilarity's so you don't have to construct or throw away anything there.",
                    "label": 0
                },
                {
                    "sent": "If you choose a particular embedding space then your data doesn't really conform to the shape of that space.",
                    "label": 0
                },
                {
                    "sent": "Then you're going to be throwing something away.",
                    "label": 0
                },
                {
                    "sent": "The big problem we have at the moment is we don't have any way a priority to tell what shape the space should be, so we don't know if a sphere is the right shape or a hyperbolic space.",
                    "label": 0
                },
                {
                    "sent": "Or simply using Euclidean space and throw away whatever's left, so I think that's that's also problem that we're interested in, but we haven't solved yet how to actually tell from the original dissimilarity's what the appropriate space would be for the embedding.",
                    "label": 0
                },
                {
                    "sent": "The fact that the ship that is being used is more complicated than the.",
                    "label": 0
                },
                {
                    "sent": "Euclidean flat space.",
                    "label": 0
                },
                {
                    "sent": "That alone does not guarantee that you're going to.",
                    "label": 0
                },
                {
                    "sent": "Infinite, well, it's more complicated in the sense that the shape is faster, more complicated, but in doing this embedding process, we lose one of the original dimensions.",
                    "label": 0
                },
                {
                    "sent": "So in effect, the space is one dimension less, but we have an extra variable of the curvature of the space, so it's approximately the same amount of of information that we can put on the manifold.",
                    "label": 0
                },
                {
                    "sent": "Yeah OK, one more question.",
                    "label": 0
                },
                {
                    "sent": "Actually, I enjoy the talk very much and a comment to your question.",
                    "label": 0
                },
                {
                    "sent": "The labels are also important, not just the shape of the kind of the main fault.",
                    "label": 0
                },
                {
                    "sent": "It could be the case that all the bank, all class of the points, same class could be concentrated in one side and you want to learn a classifier on it, right?",
                    "label": 0
                },
                {
                    "sent": "So simple embedded might work might but, but I was wondering how you decide to dimensional too hyper sphere.",
                    "label": 0
                },
                {
                    "sent": "Do you work on the same dimensions or go higher dimension?",
                    "label": 0
                },
                {
                    "sent": "So let me just skip back a couple of slides, so that's something I didn't talk about, which is in this this work we presented CPR so this this original embedding process so implies that we take the original dimensionality of the data.",
                    "label": 0
                },
                {
                    "sent": "So the number of points and we lose one of the dimensions in determine the radius.",
                    "label": 0
                },
                {
                    "sent": "The optimal radius of the sphere.",
                    "label": 0
                },
                {
                    "sent": "So the embedding this fear is in one less than the original number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "But you can increase the dimensionality, right?",
                    "label": 0
                },
                {
                    "sent": "We could yes.",
                    "label": 0
                },
                {
                    "sent": "But then we have an under constrained problem I think, which would again introduce difficulties.",
                    "label": 0
                },
                {
                    "sent": "So if the data is intrinsically less as less dimension on the surface of the sphere than the original data set, then we discovered that in this process 'cause we get more than 1 zero eigenvalue in here and we're outta discard those.",
                    "label": 0
                },
                {
                    "sent": "But we don't have any way of saying how much bigger we could make it and still get a good result.",
                    "label": 0
                },
                {
                    "sent": "Any other question?",
                    "label": 0
                },
                {
                    "sent": "If not, then thank you very much and thanks to all the speakers and the session is closed.",
                    "label": 0
                }
            ]
        }
    }
}