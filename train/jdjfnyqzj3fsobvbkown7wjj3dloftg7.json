{
    "id": "jdjfnyqzj3fsobvbkown7wjj3dloftg7",
    "title": "Fast projections onto l1,q-norm balls for grouped feature selection",
    "info": {
        "author": [
            "Suvrit Sra, Max Planck Institute for Intelligent Systems, Max Planck Institute"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Feature Selection"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_sra_fast/",
    "segmentation": [
        [
            "I'm just going to give a very high level kind of summary of what I'm doing and kind of give you a summary of some results.",
            "So because there are many small technical details involved, but you know they're not that important to getting the main idea.",
            "So let's see."
        ],
        [
            "So many of us see this regularised optimization.",
            "You have some loss function and you have some kind of regularizer.",
            "You can have these problems coming into flavors.",
            "You could have it with that additive regularization, or you could have the regularization available as a constraint.",
            "So."
        ],
        [
            "Probably these are some of the common areas where you see these problems coming up like inverse problems in applied mathematics or in image reconstruction, medical imaging, etc.",
            "Or maybe more familiar in computational statistics and machine learning, or even in compressed sensing where you.",
            "Get such problems.",
            "I'll mention a few words soon enough so of course we want to."
        ],
        [
            "Minimize this loss plus the regularizer or the constraint version.",
            "And for today's talk.",
            "I'm just going to concentrate on.",
            "A very simple approach to.",
            "Solve the second version so we have some differentiable loss function.",
            "It has to be differentiable and you have some maybe non differentiable function which models your constraints.",
            "Some keywords where associated with these types of problems.",
            "So you see this with you know last so group Lasso, multi task, lasso image reconstruction."
        ],
        [
            "Total variation, reconstruction statistics, and so there's just A tag cloud of things where that problem comes up."
        ],
        [
            "K. And I already mentioned this, that what we can handle is some differentiable function.",
            "It can be non convex but.",
            "Today means the examples that I'll show.",
            "I will use actually a convex loss function, but as far as the simplistic algorithmic approach I'm going to show you, you can just you just need differentiability.",
            "Maybe one or two other simple restrictions, but you just need differentiability and the kind of loss.",
            "Sorry the these constraint these regularising functions.",
            "That we are interested in.",
            "Are usually non differentiable, they are convex so I just do it.",
            "That's why it's a convex function.",
            "But with the corner because it's non differentiable.",
            "And."
        ],
        [
            "These are the kinds of problems that we're going to look at, and now will straightaway show you how I'm optimizing it so.",
            "Just so that you don't lose track, just fix in mind that.",
            "Differentiable loss function and some constraint.",
            "That's the basic problem.",
            "Let's look at how to solve that well.",
            "We use a very simple iteration.",
            "So what is this iteration?",
            "So just you know you start at some X0.",
            "Alpha is some scalar and this is the gradient of the loss function and.",
            "\u03a0 is the projection.",
            "I'll make that detail, so there are three components here.",
            "There's a projection.",
            "There's a step size, and there's a gradient.",
            "Well, we assume the loss function is differentiable, so we assume that we know how to compute the gradient.",
            "Not going to talk about that.",
            "In some applications it's quite complex to even compute the gradient, but assume we can compute the gradient and.",
            "The step size so in."
        ],
        [
            "Paper I showed the details what step size to use, so ordinarily if you would open a textbook style approach, maybe you do projected gradient or some such method, But there you really need to figure out a step size and if you start searching for a step size, how much should I?",
            "Step in the direction of the negative gradient to decrease things that searching for the step size can be expensive.",
            "So what we then instead do is we use this method.",
            "Called Spectral projected gradients.",
            "It's kind of popular because it's simple, I just used it 'cause you know it's a available off the shelf.",
            "This is not really the best method for such problems yet, but that's a separate talk anyhow.",
            "So the nice thing about this spectral projected gradient method is that the step size Alpha is given to you in closed form.",
            "It's a very simple formula, very easy to compute, so that's all I'm going to tell you about spectral projected gradients.",
            "You can.",
            "Search for that keyword online and learn more about it.",
            "If you want to more interesting for us is."
        ],
        [
            "This operator Pi, which computes a projection, so it computes an orthogonal projection.",
            "So orthogonal projection just means it minimizes.",
            "So.",
            "The vector Y is projected onto this constraint, so because this constraint is convex constraint, you can compute this nicely and that's what the projection operator does and.",
            "Often, not always.",
            "Computing the projection requires hard work, so at least here we got the gradients from somewhere and the spectral projected gradient method pretty much out of the box.",
            "So we have to supply something and that is the projection operator.",
            "So how do we compute the probe?"
        ],
        [
            "Action well in general you know you can have.",
            "You can have an arbitrarily complicated regularizer R, so of course in general computing the projection itself can be as hard.",
            "Almost as hard as solving the entire problem, so let's look at a simple, maybe more tractable, but still useful class.",
            "Of regularizers where can projection is somewhat easier to compute?",
            "So I will look at only this class called mixed norms.",
            "So how is it defined?",
            "So imagine X is a long vector.",
            "You chop it into subvectors.",
            "They can be of different length, doesn't matter, but important thing is it's chopped partitioned into this sub vectors.",
            "And the one, Q norm is just defined.",
            "As you know, the one norm of the Q norms of the sub vectors whatever.",
            "So Q is some real number bigger than one bigger than equal to 1."
        ],
        [
            "And these norms are kind of popular to enforce sparsity, so enforcing sparsity if you enforce sparsity on feature, just like you know, that gets you the feature selection.",
            "So of course you recognize if you put Q equal to 1.",
            "This just turns into the ordinary L1 norm.",
            "For ordinary sparsity, if Q is equal to two, that is the kind of thing that comes up when you do group plus so.",
            "Four cube bigger than two but smaller than Infinity.",
            "It's not really clear if it is very beneficial, because numerically things get highly nonlinear and very tricky, but if Q becomes Infinity then things are again tractable, and I'll talk more about the Q equal to Infinity case.",
            "In this talk I just mentioned some applications where this comes up, like multi task lasso block, compressed sensing, etc.",
            "So OK, let's see what we got.",
            "Gotta do so we have to."
        ],
        [
            "Solve the projection right so this is what I said was the projection.",
            "So we have to minimize X -- Y ^2 and we have that constraint.",
            "So how do we do that minimization?",
            "So the minimization we invoke, you can say theory of duality.",
            "So you invoke some Lagrangian duality after some simple algebra."
        ],
        [
            "You can show that you can solve that by actually solving.",
            "It's related problem.",
            "So this one has no constraints.",
            "Everything is in the objective function, but it requires you to know this Theta star.",
            "So if you knew Theta star, this problem is easier and why?"
        ],
        [
            "Is it easier because I very comfortablly assumed.",
            "That this thing is just a sum of.",
            "Sub vectors.",
            "So because it's just a sum of sub vectors.",
            "And here you have just sum of squares.",
            "Things separate out, so bigger problem turns into a set of several smaller problems, so you can solve them fairly easily.",
            "So the smaller problem looks like this Theta star is still in there for Q is equal to 1.",
            "It is the classical soft thresholding that you see in signal processing Q&Q equal to Infinity is the case we are interested in that requires some reformulation.",
            "And.",
            "It's a very very well studied problem, but still you have to implement it carefully to be able to make it scale, and I'm not describing now in my talk how I solve this sub problem with equal to Infinity.",
            "The details are in the paper, but assume we can do it very well."
        ],
        [
            "OK. How about Theta star?",
            "So I said if we knew Theta star we can solve the problem.",
            "But we don't know Theta star.",
            "So what do we do?",
            "Well, we would do something maybe fairly obvious.",
            "Search for that."
        ],
        [
            "Star, right?",
            "How do you search for Theta star?",
            "Well again, I'm skipping all the details.",
            "You don't have to really worry about the details.",
            "The point is.",
            "Theta star can be searched for by solving a certain nonlinear equation which.",
            "Falls out again from convex duality, but the details are not important, so the point is that Theta star can be solved for by getting.",
            "But solving this nonlinear equation.",
            "And.",
            "The interesting thing is.",
            "You can show that this nonlinear equation has a Theta star that lies in this interval.",
            "To get that accurate interval for everyone Q norm, you have to again do some more convex analysis, but you can show it there is a lower and upper bound.",
            "Well, as soon as you hit a star is a real number, you have a lower and upper bound.",
            "You can say I'll do by section.",
            "You can do by section or you can do something more sophisticated by using inverse quadratic interpolation, secant method and whatnot.",
            "So that's kind of the root finding that I."
        ],
        [
            "Doing the paper to find Theta star theoretically, then it shows you you know that within certain number of iterations you get to an epsilon accurate solution.",
            "Again, it's an iterative method, so you can only guarantee epsilon accuracy.",
            "Fine, so that's pretty much what was done.",
            "And now an example.",
            "So I apply this routefinding idea.",
            "To projection onto this one."
        ],
        [
            "Infinity Choice, so now imagine that you have matrices.",
            "So instead of a long vector, you can just think now maybe a matrix with several rows columns.",
            "And.",
            "This is the sum of F, just stands for Frobenius norm, and we want to project onto this set.",
            "So there previously there has been a method that was a nice simple 2009.",
            "I'll call it QP.",
            "And now here is a new method from this paper that I'll just call that FP.",
            "So all I'm trying to show here now is that.",
            "If you try to do it carefully, you can gain some speed.",
            "And.",
            "So this is just some.",
            "This is actually a really large matrix you currently you don't see such large matrices for this projection task yet.",
            "So the reason is it's a dense 10,000 by 50,000 matrix.",
            "It's a fairly large matrix.",
            "If you had sparse data, this would translate into, you know, million by several million matrix.",
            "But this is a dense matrix.",
            "To really do a stress test, so this just kind of the first column denotes."
        ],
        [
            "How stringent your sparsity constraint is.",
            "So the.",
            "Solving that subproblem.",
            "The previous method takes say about 60 times longer than this method."
        ],
        [
            "And as you increase the requirement, the astringency on sparsity, the time difference changes dramatically.",
            "So here this is, I think, about 800 times faster, so again."
        ],
        [
            "This thing so this is just one subproblem right projection, but we're trying to solve the overall regularised optimization problem.",
            "That means you have to repeatedly do projection so it pays off to optimize this projection task so that it is no longer a bottleneck.",
            "An example for this application was in multi task lasso.",
            "So I'll just remind you what multi task lasso does.",
            "Fairly simple problem.",
            "So suppose you have several different tasks, so you have data matrix 123 and so on.",
            "And we so for each data matrix you could solve a separate lasso type task.",
            "For each loss or task, you will have some solution vector W. You can collect all of them into matrix W. So essentially double user parameter matrix that you're trying to find each column corresponds to a task that you're solving and the rows of this matrix they correspond to the features.",
            "So here now you have data from several tasks, all of them share the same features.",
            "Now you're trying to simultaneously solve these tasks to kind of identify features that are simultaneously important across are.",
            "Series of tasks.",
            "And so you regularize essentially using this norm.",
            "So this is this is the loss function and this is the constraint.",
            "And this is we solve this using the spectral projected gradient method.",
            "And again, this is this is some synthetic data, you know, fairly large matrix, but I'm not sure."
        ],
        [
            "Thank you, how many tasks?",
            "How many features becausw the ultimate scalability depends on that.",
            "But you can see that.",
            "Here the data was synthetic and overly simple, so within 16 projections only the spectral projected gradient method managed to solve this to fairly to medium accuracy.",
            "And.",
            "So the."
        ],
        [
            "Runtimes are written."
        ],
        [
            "Here, so for smaller data where more projections are needed, the sort of scalability differences are bigger anyhow, so the."
        ],
        [
            "Point is now OK, I'll just show you one real data and then tell you what the point is here.",
            "So here on a more realistic maybe sparse matrix, not a dense matrix.",
            "So you took this classical, you know CMU newsgroup data.",
            "Broke it into five simultaneous tasks, corresponding to different news groups, but they shared the same sparse set of features.",
            "Which is the same underlying vocabulary across all.",
            "So after whatever cleaning we had about 54,000 features and five different tasks, each with about 3000 documents.",
            "So this is like kind of the data size.",
            "And so with that, if you really.",
            "Have a very sparse task.",
            "This also medium sparse task.",
            "These are kind of the runtimes to solve.",
            "This feature selection task, with this data set and you can see there is a lot of difference but now so previously when we just solved the projection the differences were much larger going up."
        ],
        [
            "800 times suddenly here the differences are now only said 20 to 30 times.",
            "Why is that essentially here it is, so the first method.",
            "It's the SPG algorithm spends 96% of its time just doing projections, so projections are the bottleneck there.",
            "But after some improvement, now the SPG method is only spending 20% time doing projections.",
            "Projections are no longer the bottleneck, which is why I said SPG is not yet the best method now tomorrow you come along with some method which requires which can do the overall task faster.",
            "Then you will see further benefit.",
            "So that's pretty much it, and with that I guess I'll just summarize for."
        ],
        [
            "Do what I said.",
            "I actually repeated that often enough, but I'll still mention that, you know.",
            "Explicitly, I showed you efficient projections on these mixed norm balls and empirical results were interesting and several open issues remain.",
            "But I mentioned those in the paper, including theoretical issues.",
            "For example, the previous algorithm that I compared against their method actually theoretically.",
            "Is better because theoretically it gives you you know it's a non iterative method, so it actually gives you if you had infinite precision it would give you the correct answer.",
            "While the method I show it is iterative but you know infinite precision doesn't happen on computers but still.",
            "But for people interested in theoretical issues, there are several interesting things remaining in there.",
            "The most interesting to me is now their projections have been spread up.",
            "Can we speed up actually the overall algorithm?",
            "But that's a very challenging question, so with that I'll end my talk and."
        ],
        [
            "Thanks for listening.",
            "Hi nice work so actually I'm Chevy Carreras.",
            "I'm one of the authors of the of the L1 Infinity projection.",
            "You compare two and I'd like to discuss the empirical comparison you've done OK. And make actually one important clarification if you if you can go back to your result.",
            "So what you show.",
            "So you basically run our code.",
            "What you show is that for a fixed matrix, as you yeah.",
            "EXPAND."
        ],
        [
            "So here in this table.",
            "Wait, these are different datasets.",
            "This is the size of the data set, but you have another slide where for a fee."
        ],
        [
            "This matrix, yes one.",
            "So here as you increase the density, the yeah right?",
            "If you upgrade yourself becomes less strict.",
            "That means you do not value sparsity as much.",
            "Yeah.",
            "I sorry, I called it density.",
            "At some point you showed that the performance in terms of time, well, I know which one you mean, so let me come back to that right?",
            "So you probably mean."
        ],
        [
            "This right this one so here, as long as you as you make the the Bolt tighter, so the performance of our algorithm here dramatically gets it gets horrible.",
            "So that's really that's really surprising when you see that our analysis because our method is an log North where N is the size of the matrix, and so this, and there's nothing in our analysis that says that by changing the radius of the ball, the performance will get worse.",
            "So we just knew about we saw this yesterday and we say that there must be something wrong here.",
            "And in full.",
            "Yeah, in fact, in fact we went to our implementation and there is something wrong there, so that's quite.",
            "That's quite unfortunate because our code that we published, so it turns out that the cost is not N log N, it's actually unlock N + N ^2, so this end end times the square.",
            "So this makes this really, really bad.",
            "But we this morning we corrected that because it's really a trivial change, and in fact our our running time while projection is independent of the radius.",
            "So on some experiments.",
            "This morning on my laptop for for this size of problem, I think that your problem is fifty 50,000 * * 1010 thousand.",
            "No 10,000, I thought in your paper.",
            "I think there's a small.",
            "There's a small typo because in two places you say 50,000,000.",
            "In any case, our projection, the running time of our the correct code is independent of the sea.",
            "Let's say margin.",
            "My my except for some variations that are really not that dramatical.",
            "So OK, we will put the new code so that you can so I can breathe in my experiments, yeah, so that's good.",
            "Yeah, it's it's a bit.",
            "About the implementation was an optimal, so that's why I didn't run in again, but it's actually when we fix it and it is Logan.",
            "Sure, our algorithm, the only real causes sorting the matrix right?",
            "So it should never be.",
            "I mean it because there was something wrong, right?",
            "So all I can say to the end log is I avoid sorting and that is 1 speed up, so it is empirically order N actually and one of the open issues is.",
            "Proving a lower bound actually of log North.",
            "So it is because the things are not comparing, but this gets into some technical issues 'cause you know.",
            "Like I said, your method is exact because it's based on sorting.",
            "My method is iterative because it's based on root finding, so actually theoretically so so you see this is our empirically order North, but you know I think N log N is a lower bound, but it's not clear.",
            "Blue cheese paper and one yeah, but I think that so that they don't do exact sorting and they also get into order end.",
            "You could do our battery no, no, no.",
            "There's something there's a lower bound.",
            "It's not doable.",
            "So anyways, I am intimately familiar with blue cheese work, which actually was solved by a mathematician in Poland in 1997 with a much better implementation also.",
            "So it's a classical problem from 1950s in operations research.",
            "No, no no, no linear, but you know when it comes down to high performance, the constant inside the big or really matters.",
            "So it means, but all I'm saying is that case is very well studied.",
            "But in any case, I think that in the paper there is a number of claims that say that there were not high performance, highly scalable methods for that, so I don't know if your claims are really based on the experimentation.",
            "If that's the case, I think you should revisit that.",
            "No, not happy.",
            "So you are forgetting there is also the spectral projected gradient method overall method in there without which because projection is just one small part.",
            "You also have to be able to solve the overall problem right?",
            "Alright, so it's not just so.",
            "Projection is a key part, but all I'm trying to do is.",
            "Make it not the bottleneck, that's all."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm just going to give a very high level kind of summary of what I'm doing and kind of give you a summary of some results.",
                    "label": 0
                },
                {
                    "sent": "So because there are many small technical details involved, but you know they're not that important to getting the main idea.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So many of us see this regularised optimization.",
                    "label": 0
                },
                {
                    "sent": "You have some loss function and you have some kind of regularizer.",
                    "label": 0
                },
                {
                    "sent": "You can have these problems coming into flavors.",
                    "label": 0
                },
                {
                    "sent": "You could have it with that additive regularization, or you could have the regularization available as a constraint.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Probably these are some of the common areas where you see these problems coming up like inverse problems in applied mathematics or in image reconstruction, medical imaging, etc.",
                    "label": 1
                },
                {
                    "sent": "Or maybe more familiar in computational statistics and machine learning, or even in compressed sensing where you.",
                    "label": 1
                },
                {
                    "sent": "Get such problems.",
                    "label": 0
                },
                {
                    "sent": "I'll mention a few words soon enough so of course we want to.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Minimize this loss plus the regularizer or the constraint version.",
                    "label": 0
                },
                {
                    "sent": "And for today's talk.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to concentrate on.",
                    "label": 0
                },
                {
                    "sent": "A very simple approach to.",
                    "label": 0
                },
                {
                    "sent": "Solve the second version so we have some differentiable loss function.",
                    "label": 0
                },
                {
                    "sent": "It has to be differentiable and you have some maybe non differentiable function which models your constraints.",
                    "label": 0
                },
                {
                    "sent": "Some keywords where associated with these types of problems.",
                    "label": 0
                },
                {
                    "sent": "So you see this with you know last so group Lasso, multi task, lasso image reconstruction.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Total variation, reconstruction statistics, and so there's just A tag cloud of things where that problem comes up.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "K. And I already mentioned this, that what we can handle is some differentiable function.",
                    "label": 0
                },
                {
                    "sent": "It can be non convex but.",
                    "label": 0
                },
                {
                    "sent": "Today means the examples that I'll show.",
                    "label": 0
                },
                {
                    "sent": "I will use actually a convex loss function, but as far as the simplistic algorithmic approach I'm going to show you, you can just you just need differentiability.",
                    "label": 0
                },
                {
                    "sent": "Maybe one or two other simple restrictions, but you just need differentiability and the kind of loss.",
                    "label": 0
                },
                {
                    "sent": "Sorry the these constraint these regularising functions.",
                    "label": 0
                },
                {
                    "sent": "That we are interested in.",
                    "label": 0
                },
                {
                    "sent": "Are usually non differentiable, they are convex so I just do it.",
                    "label": 0
                },
                {
                    "sent": "That's why it's a convex function.",
                    "label": 0
                },
                {
                    "sent": "But with the corner because it's non differentiable.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the kinds of problems that we're going to look at, and now will straightaway show you how I'm optimizing it so.",
                    "label": 0
                },
                {
                    "sent": "Just so that you don't lose track, just fix in mind that.",
                    "label": 0
                },
                {
                    "sent": "Differentiable loss function and some constraint.",
                    "label": 0
                },
                {
                    "sent": "That's the basic problem.",
                    "label": 0
                },
                {
                    "sent": "Let's look at how to solve that well.",
                    "label": 0
                },
                {
                    "sent": "We use a very simple iteration.",
                    "label": 0
                },
                {
                    "sent": "So what is this iteration?",
                    "label": 0
                },
                {
                    "sent": "So just you know you start at some X0.",
                    "label": 0
                },
                {
                    "sent": "Alpha is some scalar and this is the gradient of the loss function and.",
                    "label": 0
                },
                {
                    "sent": "\u03a0 is the projection.",
                    "label": 0
                },
                {
                    "sent": "I'll make that detail, so there are three components here.",
                    "label": 0
                },
                {
                    "sent": "There's a projection.",
                    "label": 0
                },
                {
                    "sent": "There's a step size, and there's a gradient.",
                    "label": 0
                },
                {
                    "sent": "Well, we assume the loss function is differentiable, so we assume that we know how to compute the gradient.",
                    "label": 0
                },
                {
                    "sent": "Not going to talk about that.",
                    "label": 0
                },
                {
                    "sent": "In some applications it's quite complex to even compute the gradient, but assume we can compute the gradient and.",
                    "label": 0
                },
                {
                    "sent": "The step size so in.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paper I showed the details what step size to use, so ordinarily if you would open a textbook style approach, maybe you do projected gradient or some such method, But there you really need to figure out a step size and if you start searching for a step size, how much should I?",
                    "label": 0
                },
                {
                    "sent": "Step in the direction of the negative gradient to decrease things that searching for the step size can be expensive.",
                    "label": 0
                },
                {
                    "sent": "So what we then instead do is we use this method.",
                    "label": 0
                },
                {
                    "sent": "Called Spectral projected gradients.",
                    "label": 0
                },
                {
                    "sent": "It's kind of popular because it's simple, I just used it 'cause you know it's a available off the shelf.",
                    "label": 0
                },
                {
                    "sent": "This is not really the best method for such problems yet, but that's a separate talk anyhow.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing about this spectral projected gradient method is that the step size Alpha is given to you in closed form.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple formula, very easy to compute, so that's all I'm going to tell you about spectral projected gradients.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Search for that keyword online and learn more about it.",
                    "label": 0
                },
                {
                    "sent": "If you want to more interesting for us is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This operator Pi, which computes a projection, so it computes an orthogonal projection.",
                    "label": 0
                },
                {
                    "sent": "So orthogonal projection just means it minimizes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The vector Y is projected onto this constraint, so because this constraint is convex constraint, you can compute this nicely and that's what the projection operator does and.",
                    "label": 0
                },
                {
                    "sent": "Often, not always.",
                    "label": 0
                },
                {
                    "sent": "Computing the projection requires hard work, so at least here we got the gradients from somewhere and the spectral projected gradient method pretty much out of the box.",
                    "label": 0
                },
                {
                    "sent": "So we have to supply something and that is the projection operator.",
                    "label": 0
                },
                {
                    "sent": "So how do we compute the probe?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Action well in general you know you can have.",
                    "label": 0
                },
                {
                    "sent": "You can have an arbitrarily complicated regularizer R, so of course in general computing the projection itself can be as hard.",
                    "label": 0
                },
                {
                    "sent": "Almost as hard as solving the entire problem, so let's look at a simple, maybe more tractable, but still useful class.",
                    "label": 0
                },
                {
                    "sent": "Of regularizers where can projection is somewhat easier to compute?",
                    "label": 0
                },
                {
                    "sent": "So I will look at only this class called mixed norms.",
                    "label": 1
                },
                {
                    "sent": "So how is it defined?",
                    "label": 0
                },
                {
                    "sent": "So imagine X is a long vector.",
                    "label": 0
                },
                {
                    "sent": "You chop it into subvectors.",
                    "label": 0
                },
                {
                    "sent": "They can be of different length, doesn't matter, but important thing is it's chopped partitioned into this sub vectors.",
                    "label": 0
                },
                {
                    "sent": "And the one, Q norm is just defined.",
                    "label": 0
                },
                {
                    "sent": "As you know, the one norm of the Q norms of the sub vectors whatever.",
                    "label": 0
                },
                {
                    "sent": "So Q is some real number bigger than one bigger than equal to 1.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these norms are kind of popular to enforce sparsity, so enforcing sparsity if you enforce sparsity on feature, just like you know, that gets you the feature selection.",
                    "label": 0
                },
                {
                    "sent": "So of course you recognize if you put Q equal to 1.",
                    "label": 0
                },
                {
                    "sent": "This just turns into the ordinary L1 norm.",
                    "label": 0
                },
                {
                    "sent": "For ordinary sparsity, if Q is equal to two, that is the kind of thing that comes up when you do group plus so.",
                    "label": 0
                },
                {
                    "sent": "Four cube bigger than two but smaller than Infinity.",
                    "label": 0
                },
                {
                    "sent": "It's not really clear if it is very beneficial, because numerically things get highly nonlinear and very tricky, but if Q becomes Infinity then things are again tractable, and I'll talk more about the Q equal to Infinity case.",
                    "label": 0
                },
                {
                    "sent": "In this talk I just mentioned some applications where this comes up, like multi task lasso block, compressed sensing, etc.",
                    "label": 1
                },
                {
                    "sent": "So OK, let's see what we got.",
                    "label": 0
                },
                {
                    "sent": "Gotta do so we have to.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solve the projection right so this is what I said was the projection.",
                    "label": 0
                },
                {
                    "sent": "So we have to minimize X -- Y ^2 and we have that constraint.",
                    "label": 0
                },
                {
                    "sent": "So how do we do that minimization?",
                    "label": 0
                },
                {
                    "sent": "So the minimization we invoke, you can say theory of duality.",
                    "label": 0
                },
                {
                    "sent": "So you invoke some Lagrangian duality after some simple algebra.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can show that you can solve that by actually solving.",
                    "label": 0
                },
                {
                    "sent": "It's related problem.",
                    "label": 0
                },
                {
                    "sent": "So this one has no constraints.",
                    "label": 0
                },
                {
                    "sent": "Everything is in the objective function, but it requires you to know this Theta star.",
                    "label": 0
                },
                {
                    "sent": "So if you knew Theta star, this problem is easier and why?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is it easier because I very comfortablly assumed.",
                    "label": 0
                },
                {
                    "sent": "That this thing is just a sum of.",
                    "label": 0
                },
                {
                    "sent": "Sub vectors.",
                    "label": 0
                },
                {
                    "sent": "So because it's just a sum of sub vectors.",
                    "label": 0
                },
                {
                    "sent": "And here you have just sum of squares.",
                    "label": 0
                },
                {
                    "sent": "Things separate out, so bigger problem turns into a set of several smaller problems, so you can solve them fairly easily.",
                    "label": 0
                },
                {
                    "sent": "So the smaller problem looks like this Theta star is still in there for Q is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "It is the classical soft thresholding that you see in signal processing Q&Q equal to Infinity is the case we are interested in that requires some reformulation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It's a very very well studied problem, but still you have to implement it carefully to be able to make it scale, and I'm not describing now in my talk how I solve this sub problem with equal to Infinity.",
                    "label": 0
                },
                {
                    "sent": "The details are in the paper, but assume we can do it very well.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. How about Theta star?",
                    "label": 0
                },
                {
                    "sent": "So I said if we knew Theta star we can solve the problem.",
                    "label": 0
                },
                {
                    "sent": "But we don't know Theta star.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "Well, we would do something maybe fairly obvious.",
                    "label": 0
                },
                {
                    "sent": "Search for that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Star, right?",
                    "label": 0
                },
                {
                    "sent": "How do you search for Theta star?",
                    "label": 0
                },
                {
                    "sent": "Well again, I'm skipping all the details.",
                    "label": 0
                },
                {
                    "sent": "You don't have to really worry about the details.",
                    "label": 0
                },
                {
                    "sent": "The point is.",
                    "label": 0
                },
                {
                    "sent": "Theta star can be searched for by solving a certain nonlinear equation which.",
                    "label": 0
                },
                {
                    "sent": "Falls out again from convex duality, but the details are not important, so the point is that Theta star can be solved for by getting.",
                    "label": 0
                },
                {
                    "sent": "But solving this nonlinear equation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is.",
                    "label": 0
                },
                {
                    "sent": "You can show that this nonlinear equation has a Theta star that lies in this interval.",
                    "label": 0
                },
                {
                    "sent": "To get that accurate interval for everyone Q norm, you have to again do some more convex analysis, but you can show it there is a lower and upper bound.",
                    "label": 0
                },
                {
                    "sent": "Well, as soon as you hit a star is a real number, you have a lower and upper bound.",
                    "label": 0
                },
                {
                    "sent": "You can say I'll do by section.",
                    "label": 0
                },
                {
                    "sent": "You can do by section or you can do something more sophisticated by using inverse quadratic interpolation, secant method and whatnot.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of the root finding that I.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing the paper to find Theta star theoretically, then it shows you you know that within certain number of iterations you get to an epsilon accurate solution.",
                    "label": 0
                },
                {
                    "sent": "Again, it's an iterative method, so you can only guarantee epsilon accuracy.",
                    "label": 0
                },
                {
                    "sent": "Fine, so that's pretty much what was done.",
                    "label": 0
                },
                {
                    "sent": "And now an example.",
                    "label": 0
                },
                {
                    "sent": "So I apply this routefinding idea.",
                    "label": 0
                },
                {
                    "sent": "To projection onto this one.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Infinity Choice, so now imagine that you have matrices.",
                    "label": 0
                },
                {
                    "sent": "So instead of a long vector, you can just think now maybe a matrix with several rows columns.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is the sum of F, just stands for Frobenius norm, and we want to project onto this set.",
                    "label": 0
                },
                {
                    "sent": "So there previously there has been a method that was a nice simple 2009.",
                    "label": 0
                },
                {
                    "sent": "I'll call it QP.",
                    "label": 0
                },
                {
                    "sent": "And now here is a new method from this paper that I'll just call that FP.",
                    "label": 0
                },
                {
                    "sent": "So all I'm trying to show here now is that.",
                    "label": 0
                },
                {
                    "sent": "If you try to do it carefully, you can gain some speed.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So this is just some.",
                    "label": 0
                },
                {
                    "sent": "This is actually a really large matrix you currently you don't see such large matrices for this projection task yet.",
                    "label": 0
                },
                {
                    "sent": "So the reason is it's a dense 10,000 by 50,000 matrix.",
                    "label": 0
                },
                {
                    "sent": "It's a fairly large matrix.",
                    "label": 0
                },
                {
                    "sent": "If you had sparse data, this would translate into, you know, million by several million matrix.",
                    "label": 0
                },
                {
                    "sent": "But this is a dense matrix.",
                    "label": 0
                },
                {
                    "sent": "To really do a stress test, so this just kind of the first column denotes.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How stringent your sparsity constraint is.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Solving that subproblem.",
                    "label": 0
                },
                {
                    "sent": "The previous method takes say about 60 times longer than this method.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as you increase the requirement, the astringency on sparsity, the time difference changes dramatically.",
                    "label": 0
                },
                {
                    "sent": "So here this is, I think, about 800 times faster, so again.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This thing so this is just one subproblem right projection, but we're trying to solve the overall regularised optimization problem.",
                    "label": 0
                },
                {
                    "sent": "That means you have to repeatedly do projection so it pays off to optimize this projection task so that it is no longer a bottleneck.",
                    "label": 0
                },
                {
                    "sent": "An example for this application was in multi task lasso.",
                    "label": 0
                },
                {
                    "sent": "So I'll just remind you what multi task lasso does.",
                    "label": 0
                },
                {
                    "sent": "Fairly simple problem.",
                    "label": 0
                },
                {
                    "sent": "So suppose you have several different tasks, so you have data matrix 123 and so on.",
                    "label": 0
                },
                {
                    "sent": "And we so for each data matrix you could solve a separate lasso type task.",
                    "label": 0
                },
                {
                    "sent": "For each loss or task, you will have some solution vector W. You can collect all of them into matrix W. So essentially double user parameter matrix that you're trying to find each column corresponds to a task that you're solving and the rows of this matrix they correspond to the features.",
                    "label": 0
                },
                {
                    "sent": "So here now you have data from several tasks, all of them share the same features.",
                    "label": 0
                },
                {
                    "sent": "Now you're trying to simultaneously solve these tasks to kind of identify features that are simultaneously important across are.",
                    "label": 0
                },
                {
                    "sent": "Series of tasks.",
                    "label": 0
                },
                {
                    "sent": "And so you regularize essentially using this norm.",
                    "label": 0
                },
                {
                    "sent": "So this is this is the loss function and this is the constraint.",
                    "label": 0
                },
                {
                    "sent": "And this is we solve this using the spectral projected gradient method.",
                    "label": 0
                },
                {
                    "sent": "And again, this is this is some synthetic data, you know, fairly large matrix, but I'm not sure.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you, how many tasks?",
                    "label": 0
                },
                {
                    "sent": "How many features becausw the ultimate scalability depends on that.",
                    "label": 0
                },
                {
                    "sent": "But you can see that.",
                    "label": 0
                },
                {
                    "sent": "Here the data was synthetic and overly simple, so within 16 projections only the spectral projected gradient method managed to solve this to fairly to medium accuracy.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Runtimes are written.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, so for smaller data where more projections are needed, the sort of scalability differences are bigger anyhow, so the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Point is now OK, I'll just show you one real data and then tell you what the point is here.",
                    "label": 0
                },
                {
                    "sent": "So here on a more realistic maybe sparse matrix, not a dense matrix.",
                    "label": 0
                },
                {
                    "sent": "So you took this classical, you know CMU newsgroup data.",
                    "label": 0
                },
                {
                    "sent": "Broke it into five simultaneous tasks, corresponding to different news groups, but they shared the same sparse set of features.",
                    "label": 1
                },
                {
                    "sent": "Which is the same underlying vocabulary across all.",
                    "label": 0
                },
                {
                    "sent": "So after whatever cleaning we had about 54,000 features and five different tasks, each with about 3000 documents.",
                    "label": 0
                },
                {
                    "sent": "So this is like kind of the data size.",
                    "label": 0
                },
                {
                    "sent": "And so with that, if you really.",
                    "label": 0
                },
                {
                    "sent": "Have a very sparse task.",
                    "label": 0
                },
                {
                    "sent": "This also medium sparse task.",
                    "label": 0
                },
                {
                    "sent": "These are kind of the runtimes to solve.",
                    "label": 1
                },
                {
                    "sent": "This feature selection task, with this data set and you can see there is a lot of difference but now so previously when we just solved the projection the differences were much larger going up.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "800 times suddenly here the differences are now only said 20 to 30 times.",
                    "label": 0
                },
                {
                    "sent": "Why is that essentially here it is, so the first method.",
                    "label": 0
                },
                {
                    "sent": "It's the SPG algorithm spends 96% of its time just doing projections, so projections are the bottleneck there.",
                    "label": 1
                },
                {
                    "sent": "But after some improvement, now the SPG method is only spending 20% time doing projections.",
                    "label": 1
                },
                {
                    "sent": "Projections are no longer the bottleneck, which is why I said SPG is not yet the best method now tomorrow you come along with some method which requires which can do the overall task faster.",
                    "label": 0
                },
                {
                    "sent": "Then you will see further benefit.",
                    "label": 0
                },
                {
                    "sent": "So that's pretty much it, and with that I guess I'll just summarize for.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do what I said.",
                    "label": 0
                },
                {
                    "sent": "I actually repeated that often enough, but I'll still mention that, you know.",
                    "label": 0
                },
                {
                    "sent": "Explicitly, I showed you efficient projections on these mixed norm balls and empirical results were interesting and several open issues remain.",
                    "label": 1
                },
                {
                    "sent": "But I mentioned those in the paper, including theoretical issues.",
                    "label": 0
                },
                {
                    "sent": "For example, the previous algorithm that I compared against their method actually theoretically.",
                    "label": 0
                },
                {
                    "sent": "Is better because theoretically it gives you you know it's a non iterative method, so it actually gives you if you had infinite precision it would give you the correct answer.",
                    "label": 0
                },
                {
                    "sent": "While the method I show it is iterative but you know infinite precision doesn't happen on computers but still.",
                    "label": 0
                },
                {
                    "sent": "But for people interested in theoretical issues, there are several interesting things remaining in there.",
                    "label": 0
                },
                {
                    "sent": "The most interesting to me is now their projections have been spread up.",
                    "label": 0
                },
                {
                    "sent": "Can we speed up actually the overall algorithm?",
                    "label": 0
                },
                {
                    "sent": "But that's a very challenging question, so with that I'll end my talk and.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks for listening.",
                    "label": 0
                },
                {
                    "sent": "Hi nice work so actually I'm Chevy Carreras.",
                    "label": 0
                },
                {
                    "sent": "I'm one of the authors of the of the L1 Infinity projection.",
                    "label": 0
                },
                {
                    "sent": "You compare two and I'd like to discuss the empirical comparison you've done OK. And make actually one important clarification if you if you can go back to your result.",
                    "label": 0
                },
                {
                    "sent": "So what you show.",
                    "label": 0
                },
                {
                    "sent": "So you basically run our code.",
                    "label": 0
                },
                {
                    "sent": "What you show is that for a fixed matrix, as you yeah.",
                    "label": 0
                },
                {
                    "sent": "EXPAND.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here in this table.",
                    "label": 0
                },
                {
                    "sent": "Wait, these are different datasets.",
                    "label": 0
                },
                {
                    "sent": "This is the size of the data set, but you have another slide where for a fee.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This matrix, yes one.",
                    "label": 0
                },
                {
                    "sent": "So here as you increase the density, the yeah right?",
                    "label": 0
                },
                {
                    "sent": "If you upgrade yourself becomes less strict.",
                    "label": 0
                },
                {
                    "sent": "That means you do not value sparsity as much.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I sorry, I called it density.",
                    "label": 0
                },
                {
                    "sent": "At some point you showed that the performance in terms of time, well, I know which one you mean, so let me come back to that right?",
                    "label": 0
                },
                {
                    "sent": "So you probably mean.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This right this one so here, as long as you as you make the the Bolt tighter, so the performance of our algorithm here dramatically gets it gets horrible.",
                    "label": 0
                },
                {
                    "sent": "So that's really that's really surprising when you see that our analysis because our method is an log North where N is the size of the matrix, and so this, and there's nothing in our analysis that says that by changing the radius of the ball, the performance will get worse.",
                    "label": 0
                },
                {
                    "sent": "So we just knew about we saw this yesterday and we say that there must be something wrong here.",
                    "label": 0
                },
                {
                    "sent": "And in full.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in fact, in fact we went to our implementation and there is something wrong there, so that's quite.",
                    "label": 0
                },
                {
                    "sent": "That's quite unfortunate because our code that we published, so it turns out that the cost is not N log N, it's actually unlock N + N ^2, so this end end times the square.",
                    "label": 0
                },
                {
                    "sent": "So this makes this really, really bad.",
                    "label": 0
                },
                {
                    "sent": "But we this morning we corrected that because it's really a trivial change, and in fact our our running time while projection is independent of the radius.",
                    "label": 0
                },
                {
                    "sent": "So on some experiments.",
                    "label": 0
                },
                {
                    "sent": "This morning on my laptop for for this size of problem, I think that your problem is fifty 50,000 * * 1010 thousand.",
                    "label": 0
                },
                {
                    "sent": "No 10,000, I thought in your paper.",
                    "label": 0
                },
                {
                    "sent": "I think there's a small.",
                    "label": 0
                },
                {
                    "sent": "There's a small typo because in two places you say 50,000,000.",
                    "label": 0
                },
                {
                    "sent": "In any case, our projection, the running time of our the correct code is independent of the sea.",
                    "label": 0
                },
                {
                    "sent": "Let's say margin.",
                    "label": 0
                },
                {
                    "sent": "My my except for some variations that are really not that dramatical.",
                    "label": 0
                },
                {
                    "sent": "So OK, we will put the new code so that you can so I can breathe in my experiments, yeah, so that's good.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's it's a bit.",
                    "label": 0
                },
                {
                    "sent": "About the implementation was an optimal, so that's why I didn't run in again, but it's actually when we fix it and it is Logan.",
                    "label": 0
                },
                {
                    "sent": "Sure, our algorithm, the only real causes sorting the matrix right?",
                    "label": 0
                },
                {
                    "sent": "So it should never be.",
                    "label": 0
                },
                {
                    "sent": "I mean it because there was something wrong, right?",
                    "label": 0
                },
                {
                    "sent": "So all I can say to the end log is I avoid sorting and that is 1 speed up, so it is empirically order N actually and one of the open issues is.",
                    "label": 0
                },
                {
                    "sent": "Proving a lower bound actually of log North.",
                    "label": 0
                },
                {
                    "sent": "So it is because the things are not comparing, but this gets into some technical issues 'cause you know.",
                    "label": 0
                },
                {
                    "sent": "Like I said, your method is exact because it's based on sorting.",
                    "label": 0
                },
                {
                    "sent": "My method is iterative because it's based on root finding, so actually theoretically so so you see this is our empirically order North, but you know I think N log N is a lower bound, but it's not clear.",
                    "label": 0
                },
                {
                    "sent": "Blue cheese paper and one yeah, but I think that so that they don't do exact sorting and they also get into order end.",
                    "label": 0
                },
                {
                    "sent": "You could do our battery no, no, no.",
                    "label": 0
                },
                {
                    "sent": "There's something there's a lower bound.",
                    "label": 0
                },
                {
                    "sent": "It's not doable.",
                    "label": 0
                },
                {
                    "sent": "So anyways, I am intimately familiar with blue cheese work, which actually was solved by a mathematician in Poland in 1997 with a much better implementation also.",
                    "label": 0
                },
                {
                    "sent": "So it's a classical problem from 1950s in operations research.",
                    "label": 0
                },
                {
                    "sent": "No, no no, no linear, but you know when it comes down to high performance, the constant inside the big or really matters.",
                    "label": 0
                },
                {
                    "sent": "So it means, but all I'm saying is that case is very well studied.",
                    "label": 0
                },
                {
                    "sent": "But in any case, I think that in the paper there is a number of claims that say that there were not high performance, highly scalable methods for that, so I don't know if your claims are really based on the experimentation.",
                    "label": 0
                },
                {
                    "sent": "If that's the case, I think you should revisit that.",
                    "label": 0
                },
                {
                    "sent": "No, not happy.",
                    "label": 0
                },
                {
                    "sent": "So you are forgetting there is also the spectral projected gradient method overall method in there without which because projection is just one small part.",
                    "label": 0
                },
                {
                    "sent": "You also have to be able to solve the overall problem right?",
                    "label": 0
                },
                {
                    "sent": "Alright, so it's not just so.",
                    "label": 0
                },
                {
                    "sent": "Projection is a key part, but all I'm trying to do is.",
                    "label": 0
                },
                {
                    "sent": "Make it not the bottleneck, that's all.",
                    "label": 0
                }
            ]
        }
    }
}