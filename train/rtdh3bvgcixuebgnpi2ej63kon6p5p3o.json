{
    "id": "rtdh3bvgcixuebgnpi2ej63kon6p5p3o",
    "title": "Kernel-based learning of hierarchial multilabel classification models",
    "info": {
        "author": [
            "Juho Rousu, Department of Computer Science, University of Helsinki"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "March 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/mlsvmlso05_rousu_kblhm/",
    "segmentation": [
        [
            "Albertsons, Mattix, and she learning.",
            "So you can ask him about that later.",
            "Pirate.",
            "OK, thank you.",
            "Yes, the work I'm going to present this joint work with Craig Sanders Sanders said Mark and John Chuck Taylor, or from Southampton.",
            "OK.",
            "So the problem."
        ],
        [
            "To talk about is hierarchical multi label classification.",
            "And the idea is that we have some documents like this newsarticle here and we have some classification hierarchy.",
            "Like this?",
            "And we want to learn to classify that news article with respect.",
            "It is this article and I kind of made up this classification here.",
            "You could think that this article is about football and music.",
            "OK, we might argue that though it's about music but but OK. Never mind that OK and the model is kind of unique and a partial part.",
            "Model's idea is that classification labels do not need to be leaves so articles can be assigned to internal nodes and also.",
            "So since we have more than one glass label.",
            "We have kind of a. Subtree.",
            "From the from the root.",
            "So several parts partial parts in the tree that are correspond to this correct labeling of the article.",
            "OK, so."
        ],
        [
            "How to learn this kind of hierarchal multi labels?",
            "Traditionally, too simple hierarchy strategies have been used.",
            "Both based on.",
            "Putting a classifier to learn it's node of the hierarchy, kind of.",
            "You will suffer a loss of 1 exactly."
        ],
        [
            "It doesn't matter how many mistakes you make once you make one, you pay one.",
            "OK, that's one way, but.",
            "You don't get any kind of creating of your how bad your prediction actually was, so so that's that's not perhaps the.",
            "Best last function, symmetric difference losses, maybe a little bit.",
            "My mic OK.",
            "So symmetric difference loss there you have.",
            "Take a test some over your your.",
            "Micro labels or mistakes in the micro labels.",
            "So you'll get some kind of a creating with respect to.",
            "With respect to how bad your your predictions were.",
            "OK.",
            "But in hierarchical classification, you could think that that.",
            "OK, not all your mistakes are.",
            "But as as.",
            "Others could think that that maybe a mistake in the root is more kind of severe than mistake make made.",
            "Further down the tree, or indeed if your parent node made a mistake, maybe the side node shouldn't be penalized for.",
            "Making a mistake anymore.",
            "So this idea is encoded in this hierarchical loss function by case of pianki.",
            "OK, so.",
            "So what happens there is that.",
            "Only the first mistake along along a part is penalized.",
            "Each part is penalized, so here what I'm trying to show in this pictures I have.",
            "My prediction is this yellow part of the.",
            "Part of the tree and the true true one is this.",
            "This part here.",
            "So in this hierarchical loss, I pay for four.",
            "4 not including this that was correct.",
            "One in the prediction.",
            "And this one, but I don't.",
            "I don't pay for for let's say this.",
            "No.",
            "This one becausw because I already my grandparent made a mistake, so this I don't pay anymore.",
            "So that's the idea in hierarchical.",
            "Loss functions.",
            "Then OK. We think about a little bit more simplified loss function that is hierarchical loss.",
            "Very only look at the edge.",
            "So we penalize the mistake in the child if the parent was was correct.",
            "So for example here you will pay for this mistake because the parent was correct, so it's a little bit.",
            "It's not as kind of.",
            "This would be the ideal I think way of penalizing.",
            "But we cannot define it in terms of edges and this we want to do for computational reasons.",
            "So we are looking at this more simplified loss function.",
            "Sony J. Yeah, I'm on next slide.",
            "I will tell you about that.",
            "So I'm just coming, yeah so.",
            "So we."
        ],
        [
            "And also scale their loss with respect to the tree.",
            "So the idea is that the further down in the trailer go, the less you should be penalized for your mistakes.",
            "So what you can do is that you have.",
            "Put this scaling coefficient one for root and then when you go down the tree, you.",
            "Kind of gradually decrease it all the time.",
            "So like this Internode Ju look what is the C for the parent?",
            "And divide PC equally among the children, so that's that's the approach used by Nikola Tesla, Piankhi and coauthors.",
            "We are also thinking about a little bit different scaling, where your scale pilot size of the subtree subtree bilotta node.",
            "So so here also to root pacewon, but Internode pays relative to the size of the subtree rooted.",
            "After node, so this is the whole tree and this is the subtree rooted at at node J, so it also kind of scales down.",
            "Down to losses.",
            "Examples, for example, decision tree by very large amount, but they no longer really counting the decisions.",
            "So so you know there might be some critical examples you need critical.",
            "Yeah, yeah, that's that's supposed to plot this moment.",
            "I don't have kind of a definite answer, that's just kind of playing around with this loss functions.",
            "We don't really.",
            "Really, no.",
            "Yeah, that's that's good question.",
            "Yeah.",
            "It might change.",
            "Number of number of the notes.",
            "Including the leaves.",
            "OK. Yeah.",
            "Number.",
            "Yes, like that.",
            "Stores please place in some other part of this is a huge subsidy.",
            "But if you have a large subtree you want to do well in that subtree, so you don't want to make a mistake at the root of a lot subtree.",
            "So you want to have.",
            "Quite large penalty.",
            "For that, so that's kind of the intuition in that scale that you want to kind of say that OK, you got to get this correct, because this subtree below it below it is depending on this parent to be correct in a way, so that's kind of the intuition.",
            "Right?",
            "OK."
        ],
        [
            "So the classification model we we use is the one.",
            "Kind of variant of the one introduced by Hoffman I'm Tascar.",
            "So.",
            "We make the hierarchy as a graphical model, so it's kind of a Markov tree.",
            "By putting an exponential family over the edges, so this is the kind of models.",
            "Martin Wainwright was was talking about in his tutorial.",
            "Two days ago.",
            "So.",
            "So this model is going over.",
            "You have a.",
            "Product of factors.",
            "For the edges and then you have some normalization.",
            "And that gives you your probability of the labeling given the document and your.",
            "Wait a second or so you have here.",
            "Here wait vector and some feature vector.",
            "I will explain a minute what the feature vector looks like.",
            "OK.",
            "So what we what we have here?",
            "OK, so some notation.",
            "By this kind of notation I need mean restriction of the whole multi label to the one edge.",
            "So if I take.",
            "I take from this vector Y only to components.",
            "Hope that edge.",
            "So that's the notice tonight.",
            "I mean by.",
            "OK, and this.",
            "File file file eases joint feature Map 4 four pair.",
            "The document and labeling of the edge.",
            "OK, so W is the weight vector that we want to learn.",
            "I'm not like I said said is the normalization factor.",
            "OK. OK."
        ],
        [
            "The feature vectors we use look like this.",
            "We have a.",
            "Fat blocks R. Or features where we have some feature vector for the document.",
            "For example pack of birds, substring spectrum, whatever.",
            "Where you can want to represent documents in vector form.",
            "And you you prefix it by an indicator so.",
            "So you have.",
            "In this block belonging to this.",
            "Edge labeling.",
            "You will only put the pack of words vector in if in that if in the labeling of this document.",
            "That matches matches this labeling of this block, so you have quite sparse feature vectors.",
            "You only put kind of.",
            "Samuel, OK, since you put put this back of words or or whatever feature vector you have.",
            "So the whole feature fixed vector would be like.",
            "Concatenation of the blocks and.",
            "For edges four, it says that you have a blog and.",
            "This edge block still divides into into blocks.",
            "Responding to different labelings of the edge and or only in the correct labeling of of this document on that edge, you will have the feature vector, so it's quite quite sparse sparse feature.",
            "Representation.",
            "OK, but what we can get from this is that.",
            "We can.",
            "Learn different with feature weights for this document feature vector.",
            "In different contexts, so the context is its edge labeling.",
            "But at the same time, it's computationally quite efficient because we can.",
            "We don't need to the kernel that results from these feature vectors.",
            "It doesn't need to be explicitly represented in memory.",
            "We can always rely on the fact that we are doing it like like this and just do some clever tricks to not to actually.",
            "Represent the whole kernel in memory.",
            "OK."
        ],
        [
            "So.",
            "I guess when you when you keep this kind of probabilistic formalize of our learning problem, you would think about learning the maximum likelihood.",
            "Assignment for the para meters.",
            "So.",
            "So this W. But this of course is quite hard because you need to compute this partition function or this normalization factor.",
            "Four for each.",
            "Kind of put in so W indeed's example and that's kind of exponential sum.",
            "And so it's it's quite quite hard and we don't want to do it.",
            "So in this emotional margin approach is what is done instead.",
            "People look at the races of probabilities and then you need to cancel out the partition function from from there so you only have this this quantity here.",
            "And you could take the logs.",
            "You will have have a kind of a linear accuracy.",
            "OK. OK, if you want to maximize this race overall incorrect pseudo examples.",
            "So we call pseudo example.",
            "Any pair where we have a document and some potential labeling for a document.",
            "So we don't.",
            "We haven't seen.",
            "It's one of these pseudo examples.",
            "We only have seen this training.",
            "Example, but we we need to consider all these potential labelings.",
            "So we want to maximize this race for all kind of incorrect.",
            "Potential labelings?",
            "That's equivalent of maximizing the minimum margin.",
            "So if you think about.",
            "This this is the margin.",
            "So this this is what it is.",
            "Turning into.",
            "Right?",
            "So not."
        ],
        [
            "Only we want to do kind of maximize the margin.",
            "We would also want that we could push, kind of.",
            "Incorrectly labeling's or incorrect pseudo examples far from the correct one.",
            "So here I have this correct labeling for the document in the hierarchy.",
            "And this is the feature space I would like.",
            "At the picture of this coin, where.",
            "Kind of the highest one is the correct.",
            "Labeling and then.",
            "Gradually, the more mistakes you make in the labeling, for example, here you have one mistake.",
            "So it should be.",
            "Little bit further down, but not not a lot.",
            "Or then make more and more mistakes like like here you make make two mistakes in the label you want, push it further and so on.",
            "So you will get through this kind of creating of the fields of space.",
            "So it's kind of almost like a kind of regressing.",
            "In a way.",
            "OK, and what is I think quite interesting is that.",
            "Now you actually can have margin violations later is quite far from the front.",
            "Your best.",
            "Kind of.",
            "Model.",
            "So that's kind of a. I think it's kind of a regularization thinking, but I'm not sure whether it's good or bad actually, but.",
            "I'd say nobody knows at the moment.",
            "OK it."
        ],
        [
            "Now write down this optimization problem using this scale scaled margins, you will get broken like this.",
            "So you have you want to minimize the normal of the way to vector.",
            "You have some flux, so this is exactly.",
            "Like an SVM where constraints are much more complicated you have you have this kind of.",
            "Requirement of marching.",
            "Being at least your loss minus some slack.",
            "On this constraint, you right for each.",
            "Potential labeling of of the document and for each document so you have.",
            "Have Lord of these constraints.",
            "OK. And you, yeah, you know this terrorist one slack for, for example.",
            "So not select for it's it's labeling an example pair so.",
            "It's 1, one thing to point out as well.",
            "OK, so this is.",
            "Quite a big problem and the dual."
        ],
        [
            "Like this?",
            "So it's a quadratic program.",
            "Where you have here you have the losses.",
            "And here here we have a kernel where you're kernel indices are are.",
            "Pseudo examples so you have.",
            "XXYY pair an X XI prime XY prime pair.",
            "So it's possible labeling.",
            "You have a separate entry in the kernel matrix.",
            "OK, so you have of course dual variable for its constraints in a primal, so you have quite a lot of term exponential number.",
            "But you have only one one box constraint for training example, so you don't have that many constraints in the door.",
            "But of course this is."
        ],
        [
            "Completely intractable problem to solve, so you have exponential number number of variables in a dual and constraints in the.",
            "Primal and you get completely silly serial numbers.",
            "The problem size is.",
            "So we want to avoid solve this to this full problem.",
            "There are two approaches, main approaches.",
            "How people have done it.",
            "Half months group.",
            "They swear by this working setup process where it's just.",
            "Put a set of pseudo examples in your training sets.",
            "You're trying to find.",
            "Find the best possible subset and they have shown that actually you can do with polynomial number of pseudo examples and you can get approximate solution.",
            "Tuskers Group did a different thing.",
            "They daymark make this market margin license centric.",
            "Of the dual, so you could actually can compress the dual into polynomial size.",
            "By utilizing Markov structure and we're doing a similar thing.",
            "Here.",
            "OK, so."
        ],
        [
            "What is marginalization means that they want to express the optimization problem in terms of marginal do our variables?",
            "So this marginal variables are just some of the original variables taken risk with respect to some.",
            "Edge and labeling of the edge.",
            "So you compute the sum over alphas.",
            "Such stepped.",
            "The part of this labeling vector U matches.",
            "Matt says you're what year?",
            "So that's that's your marginal well, variables.",
            "And we need to express the kernel Lausanne constraint in turn of the edges.",
            "So this is the how it.",
            "How the kernel is represented?",
            "So we have a block diagonal kernel and that's why it works.",
            "So the blocks are by the edges, so This is why we can actually write it as a sum over the edges.",
            "Losses, it depends on the loss function whether you can write it in terms of the edges so.",
            "We cannot write 01 loss or this hierarchical loss that pinella his first mistake along the path in terms of edges, but we can write this symmetric difference loss that says computes the number of micro label mistakes and also we can.",
            "Write this simplified hierarchical list that only looks at one edge.",
            "So for these losses you can do this kind of decomposition.",
            "And the box constraint is very easy, it's just.",
            "Look, it's just.",
            "Pull this definite book this definition in a new cat get box constraint, so nothing.",
            "Very fancy, happens there."
        ],
        [
            "OK, but there is 1 catch.",
            "Still you need to ensure that your marginal dual variables correspond to some valid Alpha.",
            "I'm if you remember Martin Wainwrights tutorial for three topologies.",
            "It's official stuff.",
            "We enforce local consistency.",
            "We don't need to care about the whole tree.",
            "We can look at the neighborhood of an edge.",
            "So so if you have two edges that are adjacent to each other, it's sufficient that we make their kind of marginals match at the note that they share.",
            "And what we do we?",
            "Pair it's edge.",
            "Up with his parents, so that's the way we do this, despairing?",
            "And that's sufficient to get get consistent news.",
            "So we get this kind of conference now.",
            "Now in the problem.",
            "That just come 'cause we need to."
        ],
        [
            "At constant marginal.",
            "So this is how the whole marginals program problem looks like, so we have here some of over the edge.",
            "See some of the training.",
            "Points.",
            "Potential edge labelings.",
            "And here is the loss for the edge.",
            "OK, and there is the quadratic part.",
            "We have some of the edges.",
            "An OK sum over all pairs of.",
            "Training point.",
            "Edge labeling pairs.",
            "Here is the Foxconn strain, and here are the consistency.",
            "Concentrate OK, so now we're talking about.",
            "Kind of almost reasonable sized problems, so we have a Reuters data data we have.",
            "330,000 marginal variables in another data set we have slightly over 1,000,000.",
            "OK, but still need to do something.",
            "It's too big to put in the QB.",
            "Silver OK?",
            "OK, but there is some."
        ],
        [
            "Structure that we can can.",
            "Take advantage.",
            "If you look at the constraints.",
            "They only talk about a single training example."
        ],
        [
            "At the time, so this by I refers to training a sample here so.",
            "If there is no kind of cross training example constraints.",
            "In the kernel, of course there are crosstraining example kernel values."
        ],
        [
            "So, but if you look at the kind of gradient phrase based approach as we can.",
            "We can decompose the problem.",
            "Here I have used this sort hands that this vector mu I contains all the marginal dual variables talking about example XI.",
            "Same thing for loss and kij is kind of kernel Clock between example XY and XX J so OK.",
            "So the idea is that if we solve the subproblem for XI, we need to obtain the initial gradient.",
            "It looks like this.",
            "You need to evaluate all other.",
            "Dual variables.",
            "So the whole.",
            "Whole big cities is involved.",
            "And this correspond bonding slice of the kernel kernel matrix.",
            "But when we do then go on and update only to dual variables of of example I we have much cheaper gradient update.",
            "So that's the kind of.",
            "Kee Kee folder efficiency.",
            "So the algorithm looks like."
        ],
        [
            "The main idea is this.",
            "They have some kind of working set of examples.",
            "We make one optimization path forward examples in the working ship and this.",
            "We need to 1st compute this initial gradient global gradient of XI with respect the whole whole training set, But anyway.",
            "Or update this.",
            "Do all variables of example I.",
            "Doing kondel conditional gradient gradient steps.",
            "But we are not fully optimizing.",
            "Kind of because we have we are not.",
            "Going to code the global optimum if we even if we cut off.",
            "Go.",
            "Optimizing this to look through the four.",
            "Instead we do.",
            "Couple of item, race and move on to the next example and so on.",
            "So I iterate over to over to working set.",
            "OK, after one pass we compute the cake at a condition selects and duality gap.",
            "If you are small enough that you stop, otherwise we iterate this process so.",
            "Quite simple loop loop iterating.",
            "OK.",
            "So the."
        ],
        [
            "Only condition on gradient ascent or descent.",
            "So the idea?",
            "Is the following.",
            "We want to find the best feasible points.",
            "With respect to the gradient.",
            "So, and that's kind of so kind of a.",
            "It's a linear approximation of the quartering objective, and we want the best point on this.",
            "Along this linear approximation.",
            "So we need to solve a linear program like this.",
            "In order to find the best feasible point.",
            "OK, but this best piece piece of point is not one that necessarily maximized the quadratic kind of objective.",
            "Instead of it.",
            "Now look along this direction and find a saddle point of the quadratic objective.",
            "Go there.",
            "It turns out this this finding a saddle point.",
            "This kind of cost is not big, so we can afford to step exactly through the best point along that Ray, we don't need to make some kind of guess how much.",
            "How big the steps would be?",
            "OK, and in order to solve this linear program, we have test used to matlab's linear interpoint solver.",
            "So quite.",
            "Sorry.",
            "States ministrations makes a state of the question.",
            "Any variables that will help consumers, then you programming stuff around.",
            "Well.",
            "OK, but I have missed out.",
            "You update this crazy and so that's kind of your.",
            "What you're doing you move.",
            "I will serve an animation so.",
            "So what happens?",
            "So here's kind of what will happen.",
            "You'll start from here."
        ],
        [
            "Here is your quadric objective.",
            "Computer gradient.",
            "OK, then you look for the."
        ],
        [
            "Best visible point.",
            "Along this crazy and so you have this point there.",
            "Then your code is."
        ],
        [
            "Battle Point of this.",
            "Because you're restricted to go along this line, the saddle point of this quadratic or checked, it is flat.",
            "Then you iterate.",
            "Computer gradient tool."
        ],
        [
            "Exploring find the conditional gradient.",
            "Find the saddle point."
        ],
        [
            "And repeat so this is kind of the search we do and we."
        ],
        [
            "Like I said, I will only do a few iterations.",
            "We are not going to continue this very long.",
            "We're going to move to the next example and OK in future.",
            "Come back to that example an update, so that's kind of we want to be quite greedy in our optimization, so we want to kind of."
        ],
        [
            "I don't want to spend too much time of really optimizing this.",
            "One example, we rather code with other ones.",
            "OK, something about working."
        ],
        [
            "Maintenance this is just.",
            "Kind of heuristic.",
            "What we have kind of learned.",
            "So one is an observation that.",
            "Most of our training data will be active at the optimal, so we will not have very sparse solution, so keeping the.",
            "Working set small is not the strategy that works.",
            "You want.",
            "You want to be efficient that way so.",
            "So we're kind of surrendering to looking up the whole training set set.",
            "Immediately, so we're not looking at the small subset.",
            "OK, so yeah, so this is the criteria if it's active so head task non 0 dual variables or or it violates violates its margins.",
            "It will be in the working set but we do something try to be a little bit clever.",
            "May want to give more iterations.",
            "2 examples that look kind of problematic that have to seem to constitute the duality cap alot so they will be spent more time on in order to kind of.",
            "Try to decrease the duality as fast as possible.",
            "But this is kind of just the heuristics.",
            "It's not, not.",
            "Kind of very, very deeply founded.",
            "OK, so this is basically what we do.",
            "Done some."
        ],
        [
            "Experiments.",
            "Two datasets, Reuters corpus.",
            "Where we have 34 Michael labels, three maximum credit is 3.",
            "Actually it's very very very shallow tree.",
            "The root has kind of almost half of the micro labels are child children of the root.",
            "OK. Use 200, two 1500 documents for training 5000 for testing.",
            "Viper all far, is apparent database there we have 188 micro labels.",
            "The hierarchy is of depth 4.",
            "And it's like quite a balanced hierarchy, so it's a real hierarchy.",
            "They've used.",
            "This is actually what type of gives it already gives us.",
            "Ready made, split the training and testing sets so we are using exactly what what they take care.",
            "OK, so we have a couple of.",
            "Different algorithms our that we named hierarchical maximal margin Markov networks according to task cards, kind of.",
            "Naming scheme comparison.",
            "Flat SVM so predicate smartlabel independently.",
            "SVM trained hierarchically, so you so to note only part of the training examples.",
            "On hierarchical regularised least quest, that's Nicola Castle paintings.",
            "No new algorithm.",
            "Very recently introduced.",
            "So everything implemented in Matlab.",
            "And like I said, the lifts also lower used as the solar folder for a conditional trading and OK. And here's the.",
            "Point GB RAM install.",
            "That's what happens.",
            "This is why for Alpha."
        ],
        [
            "Learning curve trained with this simplified hierarchical loss.",
            "We're optimizing one 1,000,000 little more dual variables.",
            "Like I said, majority of them are non zero at optimum.",
            "OK, here I have the objective and this optimum is in 100, so it's scale that way.",
            "Saying error here, testing error here and this is 01 loss.",
            "So it's like how many of the documents are.",
            "Contain one or more errors, so it's kind of.",
            "Kind of the strictest.",
            "Plus you can enforce well you can.",
            "You can see it.",
            "It's that.",
            "The test error actually bottom out out out first, so after seems like 2 hours now.",
            "It's their very according to care training error.",
            "Little bit slower going up.",
            "It's like training error followed.",
            "Follows the objective kind of.",
            "Kind of mirrors gross stuff.",
            "OK.",
            "But you could.",
            "You could say that OK?",
            "Well, we are 95% or something like that of the maximum here.",
            "The training error will be.",
            "The bottom.",
            "So you can think there's some kind of early stopping maybe.",
            "Possible here.",
            "And you don't see much.",
            "Overfitting, it's like when we consider this cervicals a little bit, but.",
            "It doesn't go up or down, it says.",
            "Quiet quiet level.",
            "OK, some prediction.",
            "Errors."
        ],
        [
            "So here right here I have 01 loss for Reuters data set.",
            "Here is my profile, so it's 01 loss symmetric difference.",
            "Lausanne, this hierarchical loss and algorithms, SVM, Harkless, PM.",
            "Oracle recognize least squares on our two.",
            "Two different training losses, so this is trained with symmetric different loss.",
            "This is trained with this simplified hierarchical loss.",
            "So what you can see is that we're doing pretty well in terms of the 01 loss, so these are the lowest figures.",
            "Flat Esbian loses quite quite clearly.",
            "Both here and here the same thing as the lowest zero and errors are with this with our method.",
            "But then it's much more mixed when you think about the other other loss funds.",
            "So since we're doing OK in terms of the symmetric difference loss as well, so these figures are not with that and especially here we are.",
            "Quite good.",
            "OK, one thing to note here is this wipe off of East Side.",
            "Here we really have a.",
            "Four level hierarchy and that's.",
            "Quite quite a balanced one and this process hierarchy is quite shallow and unbalanced, so you can actually see that even the flat SVM does pretty well.",
            "In this hierarchal losses because there's no law hierarchy to benefit from, so it's like.",
            "On but yes, you can see here we have real hierarchy, so flat SV and starts to.",
            "Fall behind.",
            "OK, here are my my results.",
            "Some conclusions so."
        ],
        [
            "So we present the kernel based uploads for hierarchical.",
            "Text classification where we allowed the documents belong to more than one class at a time.",
            "So we utilized this dependency structure of the micro labels.",
            "By imposing a mark of structure over the hierarchy.",
            "OK, and we get decent prediction accuracy.",
            "And especially when we have a little bit of a hierarchy like in the Bible, Alpha digested.",
            "OK, and we have a feasible optimization scheme for this.",
            "Quite quite a big.",
            "Problem.",
            "And this this working horse there is described conditional gradient search in.",
            "Subproblem subram problems.",
            "OK, so this is all I have to say, so thank you for your attention.",
            "Questions.",
            "That's very nice work, I would just add I was wondering about your only motivation that the partition function was hard to evaluate that at three is a case in which the partition function principle is relatively easy.",
            "Yeah, yeah.",
            "Problem there, would that be another possible way of attacking the problem, yeah?",
            "Possibly.",
            "The thing is, I don't know how to deal with the fact that it's dependent on the weights vector I'm learning, so I have infinite amount of.",
            "These guys to available because I'm learning and continuous later, so that's.",
            "Maybe there's a way to do it, though I'm not aware of.",
            "So that's yeah.",
            "Summation and then say calculated derivative.",
            "Yeah, that would be another evaluation.",
            "So that is.",
            "Yeah yeah, that's kind of.",
            "Yeah, that's some kind of sometimes entered my mind.",
            "That is it really that hard disk partition function?",
            "But I never have really gone that problem lies not in the summations so much, but in the dependency.",
            "Yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Albertsons, Mattix, and she learning.",
                    "label": 0
                },
                {
                    "sent": "So you can ask him about that later.",
                    "label": 0
                },
                {
                    "sent": "Pirate.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes, the work I'm going to present this joint work with Craig Sanders Sanders said Mark and John Chuck Taylor, or from Southampton.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the problem.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To talk about is hierarchical multi label classification.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that we have some documents like this newsarticle here and we have some classification hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "And we want to learn to classify that news article with respect.",
                    "label": 1
                },
                {
                    "sent": "It is this article and I kind of made up this classification here.",
                    "label": 0
                },
                {
                    "sent": "You could think that this article is about football and music.",
                    "label": 0
                },
                {
                    "sent": "OK, we might argue that though it's about music but but OK. Never mind that OK and the model is kind of unique and a partial part.",
                    "label": 0
                },
                {
                    "sent": "Model's idea is that classification labels do not need to be leaves so articles can be assigned to internal nodes and also.",
                    "label": 1
                },
                {
                    "sent": "So since we have more than one glass label.",
                    "label": 0
                },
                {
                    "sent": "We have kind of a. Subtree.",
                    "label": 0
                },
                {
                    "sent": "From the from the root.",
                    "label": 0
                },
                {
                    "sent": "So several parts partial parts in the tree that are correspond to this correct labeling of the article.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How to learn this kind of hierarchal multi labels?",
                    "label": 0
                },
                {
                    "sent": "Traditionally, too simple hierarchy strategies have been used.",
                    "label": 0
                },
                {
                    "sent": "Both based on.",
                    "label": 0
                },
                {
                    "sent": "Putting a classifier to learn it's node of the hierarchy, kind of.",
                    "label": 1
                },
                {
                    "sent": "You will suffer a loss of 1 exactly.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It doesn't matter how many mistakes you make once you make one, you pay one.",
                    "label": 0
                },
                {
                    "sent": "OK, that's one way, but.",
                    "label": 0
                },
                {
                    "sent": "You don't get any kind of creating of your how bad your prediction actually was, so so that's that's not perhaps the.",
                    "label": 0
                },
                {
                    "sent": "Best last function, symmetric difference losses, maybe a little bit.",
                    "label": 0
                },
                {
                    "sent": "My mic OK.",
                    "label": 0
                },
                {
                    "sent": "So symmetric difference loss there you have.",
                    "label": 1
                },
                {
                    "sent": "Take a test some over your your.",
                    "label": 0
                },
                {
                    "sent": "Micro labels or mistakes in the micro labels.",
                    "label": 0
                },
                {
                    "sent": "So you'll get some kind of a creating with respect to.",
                    "label": 0
                },
                {
                    "sent": "With respect to how bad your your predictions were.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But in hierarchical classification, you could think that that.",
                    "label": 0
                },
                {
                    "sent": "OK, not all your mistakes are.",
                    "label": 0
                },
                {
                    "sent": "But as as.",
                    "label": 0
                },
                {
                    "sent": "Others could think that that maybe a mistake in the root is more kind of severe than mistake make made.",
                    "label": 0
                },
                {
                    "sent": "Further down the tree, or indeed if your parent node made a mistake, maybe the side node shouldn't be penalized for.",
                    "label": 0
                },
                {
                    "sent": "Making a mistake anymore.",
                    "label": 0
                },
                {
                    "sent": "So this idea is encoded in this hierarchical loss function by case of pianki.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So what happens there is that.",
                    "label": 0
                },
                {
                    "sent": "Only the first mistake along along a part is penalized.",
                    "label": 1
                },
                {
                    "sent": "Each part is penalized, so here what I'm trying to show in this pictures I have.",
                    "label": 0
                },
                {
                    "sent": "My prediction is this yellow part of the.",
                    "label": 0
                },
                {
                    "sent": "Part of the tree and the true true one is this.",
                    "label": 1
                },
                {
                    "sent": "This part here.",
                    "label": 0
                },
                {
                    "sent": "So in this hierarchical loss, I pay for four.",
                    "label": 0
                },
                {
                    "sent": "4 not including this that was correct.",
                    "label": 0
                },
                {
                    "sent": "One in the prediction.",
                    "label": 0
                },
                {
                    "sent": "And this one, but I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't pay for for let's say this.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "This one becausw because I already my grandparent made a mistake, so this I don't pay anymore.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea in hierarchical.",
                    "label": 0
                },
                {
                    "sent": "Loss functions.",
                    "label": 0
                },
                {
                    "sent": "Then OK. We think about a little bit more simplified loss function that is hierarchical loss.",
                    "label": 0
                },
                {
                    "sent": "Very only look at the edge.",
                    "label": 0
                },
                {
                    "sent": "So we penalize the mistake in the child if the parent was was correct.",
                    "label": 1
                },
                {
                    "sent": "So for example here you will pay for this mistake because the parent was correct, so it's a little bit.",
                    "label": 0
                },
                {
                    "sent": "It's not as kind of.",
                    "label": 0
                },
                {
                    "sent": "This would be the ideal I think way of penalizing.",
                    "label": 0
                },
                {
                    "sent": "But we cannot define it in terms of edges and this we want to do for computational reasons.",
                    "label": 0
                },
                {
                    "sent": "So we are looking at this more simplified loss function.",
                    "label": 0
                },
                {
                    "sent": "Sony J. Yeah, I'm on next slide.",
                    "label": 0
                },
                {
                    "sent": "I will tell you about that.",
                    "label": 0
                },
                {
                    "sent": "So I'm just coming, yeah so.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also scale their loss with respect to the tree.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that the further down in the trailer go, the less you should be penalized for your mistakes.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is that you have.",
                    "label": 0
                },
                {
                    "sent": "Put this scaling coefficient one for root and then when you go down the tree, you.",
                    "label": 0
                },
                {
                    "sent": "Kind of gradually decrease it all the time.",
                    "label": 0
                },
                {
                    "sent": "So like this Internode Ju look what is the C for the parent?",
                    "label": 0
                },
                {
                    "sent": "And divide PC equally among the children, so that's that's the approach used by Nikola Tesla, Piankhi and coauthors.",
                    "label": 1
                },
                {
                    "sent": "We are also thinking about a little bit different scaling, where your scale pilot size of the subtree subtree bilotta node.",
                    "label": 0
                },
                {
                    "sent": "So so here also to root pacewon, but Internode pays relative to the size of the subtree rooted.",
                    "label": 1
                },
                {
                    "sent": "After node, so this is the whole tree and this is the subtree rooted at at node J, so it also kind of scales down.",
                    "label": 0
                },
                {
                    "sent": "Down to losses.",
                    "label": 0
                },
                {
                    "sent": "Examples, for example, decision tree by very large amount, but they no longer really counting the decisions.",
                    "label": 0
                },
                {
                    "sent": "So so you know there might be some critical examples you need critical.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, that's that's supposed to plot this moment.",
                    "label": 0
                },
                {
                    "sent": "I don't have kind of a definite answer, that's just kind of playing around with this loss functions.",
                    "label": 0
                },
                {
                    "sent": "We don't really.",
                    "label": 0
                },
                {
                    "sent": "Really, no.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's good question.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 1
                },
                {
                    "sent": "It might change.",
                    "label": 0
                },
                {
                    "sent": "Number of number of the notes.",
                    "label": 0
                },
                {
                    "sent": "Including the leaves.",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah.",
                    "label": 0
                },
                {
                    "sent": "Number.",
                    "label": 0
                },
                {
                    "sent": "Yes, like that.",
                    "label": 0
                },
                {
                    "sent": "Stores please place in some other part of this is a huge subsidy.",
                    "label": 0
                },
                {
                    "sent": "But if you have a large subtree you want to do well in that subtree, so you don't want to make a mistake at the root of a lot subtree.",
                    "label": 0
                },
                {
                    "sent": "So you want to have.",
                    "label": 0
                },
                {
                    "sent": "Quite large penalty.",
                    "label": 0
                },
                {
                    "sent": "For that, so that's kind of the intuition in that scale that you want to kind of say that OK, you got to get this correct, because this subtree below it below it is depending on this parent to be correct in a way, so that's kind of the intuition.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the classification model we we use is the one.",
                    "label": 1
                },
                {
                    "sent": "Kind of variant of the one introduced by Hoffman I'm Tascar.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We make the hierarchy as a graphical model, so it's kind of a Markov tree.",
                    "label": 1
                },
                {
                    "sent": "By putting an exponential family over the edges, so this is the kind of models.",
                    "label": 0
                },
                {
                    "sent": "Martin Wainwright was was talking about in his tutorial.",
                    "label": 0
                },
                {
                    "sent": "Two days ago.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this model is going over.",
                    "label": 0
                },
                {
                    "sent": "You have a.",
                    "label": 1
                },
                {
                    "sent": "Product of factors.",
                    "label": 0
                },
                {
                    "sent": "For the edges and then you have some normalization.",
                    "label": 0
                },
                {
                    "sent": "And that gives you your probability of the labeling given the document and your.",
                    "label": 0
                },
                {
                    "sent": "Wait a second or so you have here.",
                    "label": 0
                },
                {
                    "sent": "Here wait vector and some feature vector.",
                    "label": 0
                },
                {
                    "sent": "I will explain a minute what the feature vector looks like.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what we what we have here?",
                    "label": 1
                },
                {
                    "sent": "OK, so some notation.",
                    "label": 0
                },
                {
                    "sent": "By this kind of notation I need mean restriction of the whole multi label to the one edge.",
                    "label": 0
                },
                {
                    "sent": "So if I take.",
                    "label": 0
                },
                {
                    "sent": "I take from this vector Y only to components.",
                    "label": 0
                },
                {
                    "sent": "Hope that edge.",
                    "label": 0
                },
                {
                    "sent": "So that's the notice tonight.",
                    "label": 1
                },
                {
                    "sent": "I mean by.",
                    "label": 1
                },
                {
                    "sent": "OK, and this.",
                    "label": 0
                },
                {
                    "sent": "File file file eases joint feature Map 4 four pair.",
                    "label": 0
                },
                {
                    "sent": "The document and labeling of the edge.",
                    "label": 0
                },
                {
                    "sent": "OK, so W is the weight vector that we want to learn.",
                    "label": 0
                },
                {
                    "sent": "I'm not like I said said is the normalization factor.",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The feature vectors we use look like this.",
                    "label": 1
                },
                {
                    "sent": "We have a.",
                    "label": 0
                },
                {
                    "sent": "Fat blocks R. Or features where we have some feature vector for the document.",
                    "label": 0
                },
                {
                    "sent": "For example pack of birds, substring spectrum, whatever.",
                    "label": 0
                },
                {
                    "sent": "Where you can want to represent documents in vector form.",
                    "label": 0
                },
                {
                    "sent": "And you you prefix it by an indicator so.",
                    "label": 0
                },
                {
                    "sent": "So you have.",
                    "label": 0
                },
                {
                    "sent": "In this block belonging to this.",
                    "label": 0
                },
                {
                    "sent": "Edge labeling.",
                    "label": 0
                },
                {
                    "sent": "You will only put the pack of words vector in if in that if in the labeling of this document.",
                    "label": 0
                },
                {
                    "sent": "That matches matches this labeling of this block, so you have quite sparse feature vectors.",
                    "label": 0
                },
                {
                    "sent": "You only put kind of.",
                    "label": 0
                },
                {
                    "sent": "Samuel, OK, since you put put this back of words or or whatever feature vector you have.",
                    "label": 0
                },
                {
                    "sent": "So the whole feature fixed vector would be like.",
                    "label": 0
                },
                {
                    "sent": "Concatenation of the blocks and.",
                    "label": 0
                },
                {
                    "sent": "For edges four, it says that you have a blog and.",
                    "label": 0
                },
                {
                    "sent": "This edge block still divides into into blocks.",
                    "label": 0
                },
                {
                    "sent": "Responding to different labelings of the edge and or only in the correct labeling of of this document on that edge, you will have the feature vector, so it's quite quite sparse sparse feature.",
                    "label": 0
                },
                {
                    "sent": "Representation.",
                    "label": 0
                },
                {
                    "sent": "OK, but what we can get from this is that.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "Learn different with feature weights for this document feature vector.",
                    "label": 1
                },
                {
                    "sent": "In different contexts, so the context is its edge labeling.",
                    "label": 0
                },
                {
                    "sent": "But at the same time, it's computationally quite efficient because we can.",
                    "label": 1
                },
                {
                    "sent": "We don't need to the kernel that results from these feature vectors.",
                    "label": 0
                },
                {
                    "sent": "It doesn't need to be explicitly represented in memory.",
                    "label": 1
                },
                {
                    "sent": "We can always rely on the fact that we are doing it like like this and just do some clever tricks to not to actually.",
                    "label": 0
                },
                {
                    "sent": "Represent the whole kernel in memory.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I guess when you when you keep this kind of probabilistic formalize of our learning problem, you would think about learning the maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Assignment for the para meters.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this W. But this of course is quite hard because you need to compute this partition function or this normalization factor.",
                    "label": 1
                },
                {
                    "sent": "Four for each.",
                    "label": 0
                },
                {
                    "sent": "Kind of put in so W indeed's example and that's kind of exponential sum.",
                    "label": 0
                },
                {
                    "sent": "And so it's it's quite quite hard and we don't want to do it.",
                    "label": 0
                },
                {
                    "sent": "So in this emotional margin approach is what is done instead.",
                    "label": 0
                },
                {
                    "sent": "People look at the races of probabilities and then you need to cancel out the partition function from from there so you only have this this quantity here.",
                    "label": 0
                },
                {
                    "sent": "And you could take the logs.",
                    "label": 0
                },
                {
                    "sent": "You will have have a kind of a linear accuracy.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, if you want to maximize this race overall incorrect pseudo examples.",
                    "label": 0
                },
                {
                    "sent": "So we call pseudo example.",
                    "label": 0
                },
                {
                    "sent": "Any pair where we have a document and some potential labeling for a document.",
                    "label": 0
                },
                {
                    "sent": "So we don't.",
                    "label": 0
                },
                {
                    "sent": "We haven't seen.",
                    "label": 0
                },
                {
                    "sent": "It's one of these pseudo examples.",
                    "label": 0
                },
                {
                    "sent": "We only have seen this training.",
                    "label": 0
                },
                {
                    "sent": "Example, but we we need to consider all these potential labelings.",
                    "label": 0
                },
                {
                    "sent": "So we want to maximize this race for all kind of incorrect.",
                    "label": 0
                },
                {
                    "sent": "Potential labelings?",
                    "label": 0
                },
                {
                    "sent": "That's equivalent of maximizing the minimum margin.",
                    "label": 1
                },
                {
                    "sent": "So if you think about.",
                    "label": 0
                },
                {
                    "sent": "This this is the margin.",
                    "label": 0
                },
                {
                    "sent": "So this this is what it is.",
                    "label": 0
                },
                {
                    "sent": "Turning into.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So not.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Only we want to do kind of maximize the margin.",
                    "label": 1
                },
                {
                    "sent": "We would also want that we could push, kind of.",
                    "label": 0
                },
                {
                    "sent": "Incorrectly labeling's or incorrect pseudo examples far from the correct one.",
                    "label": 1
                },
                {
                    "sent": "So here I have this correct labeling for the document in the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "And this is the feature space I would like.",
                    "label": 1
                },
                {
                    "sent": "At the picture of this coin, where.",
                    "label": 0
                },
                {
                    "sent": "Kind of the highest one is the correct.",
                    "label": 0
                },
                {
                    "sent": "Labeling and then.",
                    "label": 0
                },
                {
                    "sent": "Gradually, the more mistakes you make in the labeling, for example, here you have one mistake.",
                    "label": 0
                },
                {
                    "sent": "So it should be.",
                    "label": 0
                },
                {
                    "sent": "Little bit further down, but not not a lot.",
                    "label": 0
                },
                {
                    "sent": "Or then make more and more mistakes like like here you make make two mistakes in the label you want, push it further and so on.",
                    "label": 0
                },
                {
                    "sent": "So you will get through this kind of creating of the fields of space.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of almost like a kind of regressing.",
                    "label": 0
                },
                {
                    "sent": "In a way.",
                    "label": 0
                },
                {
                    "sent": "OK, and what is I think quite interesting is that.",
                    "label": 0
                },
                {
                    "sent": "Now you actually can have margin violations later is quite far from the front.",
                    "label": 0
                },
                {
                    "sent": "Your best.",
                    "label": 0
                },
                {
                    "sent": "Kind of.",
                    "label": 0
                },
                {
                    "sent": "Model.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a. I think it's kind of a regularization thinking, but I'm not sure whether it's good or bad actually, but.",
                    "label": 0
                },
                {
                    "sent": "I'd say nobody knows at the moment.",
                    "label": 0
                },
                {
                    "sent": "OK it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now write down this optimization problem using this scale scaled margins, you will get broken like this.",
                    "label": 1
                },
                {
                    "sent": "So you have you want to minimize the normal of the way to vector.",
                    "label": 0
                },
                {
                    "sent": "You have some flux, so this is exactly.",
                    "label": 0
                },
                {
                    "sent": "Like an SVM where constraints are much more complicated you have you have this kind of.",
                    "label": 0
                },
                {
                    "sent": "Requirement of marching.",
                    "label": 0
                },
                {
                    "sent": "Being at least your loss minus some slack.",
                    "label": 0
                },
                {
                    "sent": "On this constraint, you right for each.",
                    "label": 0
                },
                {
                    "sent": "Potential labeling of of the document and for each document so you have.",
                    "label": 1
                },
                {
                    "sent": "Have Lord of these constraints.",
                    "label": 0
                },
                {
                    "sent": "OK. And you, yeah, you know this terrorist one slack for, for example.",
                    "label": 0
                },
                {
                    "sent": "So not select for it's it's labeling an example pair so.",
                    "label": 0
                },
                {
                    "sent": "It's 1, one thing to point out as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "Quite a big problem and the dual.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "So it's a quadratic program.",
                    "label": 0
                },
                {
                    "sent": "Where you have here you have the losses.",
                    "label": 0
                },
                {
                    "sent": "And here here we have a kernel where you're kernel indices are are.",
                    "label": 0
                },
                {
                    "sent": "Pseudo examples so you have.",
                    "label": 0
                },
                {
                    "sent": "XXYY pair an X XI prime XY prime pair.",
                    "label": 0
                },
                {
                    "sent": "So it's possible labeling.",
                    "label": 0
                },
                {
                    "sent": "You have a separate entry in the kernel matrix.",
                    "label": 1
                },
                {
                    "sent": "OK, so you have of course dual variable for its constraints in a primal, so you have quite a lot of term exponential number.",
                    "label": 0
                },
                {
                    "sent": "But you have only one one box constraint for training example, so you don't have that many constraints in the door.",
                    "label": 1
                },
                {
                    "sent": "But of course this is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Completely intractable problem to solve, so you have exponential number number of variables in a dual and constraints in the.",
                    "label": 1
                },
                {
                    "sent": "Primal and you get completely silly serial numbers.",
                    "label": 0
                },
                {
                    "sent": "The problem size is.",
                    "label": 0
                },
                {
                    "sent": "So we want to avoid solve this to this full problem.",
                    "label": 0
                },
                {
                    "sent": "There are two approaches, main approaches.",
                    "label": 0
                },
                {
                    "sent": "How people have done it.",
                    "label": 0
                },
                {
                    "sent": "Half months group.",
                    "label": 0
                },
                {
                    "sent": "They swear by this working setup process where it's just.",
                    "label": 0
                },
                {
                    "sent": "Put a set of pseudo examples in your training sets.",
                    "label": 0
                },
                {
                    "sent": "You're trying to find.",
                    "label": 0
                },
                {
                    "sent": "Find the best possible subset and they have shown that actually you can do with polynomial number of pseudo examples and you can get approximate solution.",
                    "label": 0
                },
                {
                    "sent": "Tuskers Group did a different thing.",
                    "label": 0
                },
                {
                    "sent": "They daymark make this market margin license centric.",
                    "label": 1
                },
                {
                    "sent": "Of the dual, so you could actually can compress the dual into polynomial size.",
                    "label": 0
                },
                {
                    "sent": "By utilizing Markov structure and we're doing a similar thing.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is marginalization means that they want to express the optimization problem in terms of marginal do our variables?",
                    "label": 1
                },
                {
                    "sent": "So this marginal variables are just some of the original variables taken risk with respect to some.",
                    "label": 0
                },
                {
                    "sent": "Edge and labeling of the edge.",
                    "label": 0
                },
                {
                    "sent": "So you compute the sum over alphas.",
                    "label": 0
                },
                {
                    "sent": "Such stepped.",
                    "label": 0
                },
                {
                    "sent": "The part of this labeling vector U matches.",
                    "label": 0
                },
                {
                    "sent": "Matt says you're what year?",
                    "label": 0
                },
                {
                    "sent": "So that's that's your marginal well, variables.",
                    "label": 1
                },
                {
                    "sent": "And we need to express the kernel Lausanne constraint in turn of the edges.",
                    "label": 0
                },
                {
                    "sent": "So this is the how it.",
                    "label": 0
                },
                {
                    "sent": "How the kernel is represented?",
                    "label": 0
                },
                {
                    "sent": "So we have a block diagonal kernel and that's why it works.",
                    "label": 1
                },
                {
                    "sent": "So the blocks are by the edges, so This is why we can actually write it as a sum over the edges.",
                    "label": 0
                },
                {
                    "sent": "Losses, it depends on the loss function whether you can write it in terms of the edges so.",
                    "label": 0
                },
                {
                    "sent": "We cannot write 01 loss or this hierarchical loss that pinella his first mistake along the path in terms of edges, but we can write this symmetric difference loss that says computes the number of micro label mistakes and also we can.",
                    "label": 1
                },
                {
                    "sent": "Write this simplified hierarchical list that only looks at one edge.",
                    "label": 0
                },
                {
                    "sent": "So for these losses you can do this kind of decomposition.",
                    "label": 0
                },
                {
                    "sent": "And the box constraint is very easy, it's just.",
                    "label": 0
                },
                {
                    "sent": "Look, it's just.",
                    "label": 0
                },
                {
                    "sent": "Pull this definite book this definition in a new cat get box constraint, so nothing.",
                    "label": 0
                },
                {
                    "sent": "Very fancy, happens there.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but there is 1 catch.",
                    "label": 0
                },
                {
                    "sent": "Still you need to ensure that your marginal dual variables correspond to some valid Alpha.",
                    "label": 1
                },
                {
                    "sent": "I'm if you remember Martin Wainwrights tutorial for three topologies.",
                    "label": 0
                },
                {
                    "sent": "It's official stuff.",
                    "label": 0
                },
                {
                    "sent": "We enforce local consistency.",
                    "label": 0
                },
                {
                    "sent": "We don't need to care about the whole tree.",
                    "label": 1
                },
                {
                    "sent": "We can look at the neighborhood of an edge.",
                    "label": 0
                },
                {
                    "sent": "So so if you have two edges that are adjacent to each other, it's sufficient that we make their kind of marginals match at the note that they share.",
                    "label": 0
                },
                {
                    "sent": "And what we do we?",
                    "label": 0
                },
                {
                    "sent": "Pair it's edge.",
                    "label": 0
                },
                {
                    "sent": "Up with his parents, so that's the way we do this, despairing?",
                    "label": 1
                },
                {
                    "sent": "And that's sufficient to get get consistent news.",
                    "label": 0
                },
                {
                    "sent": "So we get this kind of conference now.",
                    "label": 0
                },
                {
                    "sent": "Now in the problem.",
                    "label": 1
                },
                {
                    "sent": "That just come 'cause we need to.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At constant marginal.",
                    "label": 0
                },
                {
                    "sent": "So this is how the whole marginals program problem looks like, so we have here some of over the edge.",
                    "label": 0
                },
                {
                    "sent": "See some of the training.",
                    "label": 0
                },
                {
                    "sent": "Points.",
                    "label": 0
                },
                {
                    "sent": "Potential edge labelings.",
                    "label": 0
                },
                {
                    "sent": "And here is the loss for the edge.",
                    "label": 0
                },
                {
                    "sent": "OK, and there is the quadratic part.",
                    "label": 0
                },
                {
                    "sent": "We have some of the edges.",
                    "label": 0
                },
                {
                    "sent": "An OK sum over all pairs of.",
                    "label": 0
                },
                {
                    "sent": "Training point.",
                    "label": 0
                },
                {
                    "sent": "Edge labeling pairs.",
                    "label": 0
                },
                {
                    "sent": "Here is the Foxconn strain, and here are the consistency.",
                    "label": 0
                },
                {
                    "sent": "Concentrate OK, so now we're talking about.",
                    "label": 0
                },
                {
                    "sent": "Kind of almost reasonable sized problems, so we have a Reuters data data we have.",
                    "label": 1
                },
                {
                    "sent": "330,000 marginal variables in another data set we have slightly over 1,000,000.",
                    "label": 1
                },
                {
                    "sent": "OK, but still need to do something.",
                    "label": 0
                },
                {
                    "sent": "It's too big to put in the QB.",
                    "label": 0
                },
                {
                    "sent": "Silver OK?",
                    "label": 0
                },
                {
                    "sent": "OK, but there is some.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structure that we can can.",
                    "label": 0
                },
                {
                    "sent": "Take advantage.",
                    "label": 0
                },
                {
                    "sent": "If you look at the constraints.",
                    "label": 0
                },
                {
                    "sent": "They only talk about a single training example.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the time, so this by I refers to training a sample here so.",
                    "label": 0
                },
                {
                    "sent": "If there is no kind of cross training example constraints.",
                    "label": 0
                },
                {
                    "sent": "In the kernel, of course there are crosstraining example kernel values.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but if you look at the kind of gradient phrase based approach as we can.",
                    "label": 0
                },
                {
                    "sent": "We can decompose the problem.",
                    "label": 1
                },
                {
                    "sent": "Here I have used this sort hands that this vector mu I contains all the marginal dual variables talking about example XI.",
                    "label": 0
                },
                {
                    "sent": "Same thing for loss and kij is kind of kernel Clock between example XY and XX J so OK.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that if we solve the subproblem for XI, we need to obtain the initial gradient.",
                    "label": 1
                },
                {
                    "sent": "It looks like this.",
                    "label": 0
                },
                {
                    "sent": "You need to evaluate all other.",
                    "label": 0
                },
                {
                    "sent": "Dual variables.",
                    "label": 0
                },
                {
                    "sent": "So the whole.",
                    "label": 1
                },
                {
                    "sent": "Whole big cities is involved.",
                    "label": 1
                },
                {
                    "sent": "And this correspond bonding slice of the kernel kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "But when we do then go on and update only to dual variables of of example I we have much cheaper gradient update.",
                    "label": 0
                },
                {
                    "sent": "So that's the kind of.",
                    "label": 0
                },
                {
                    "sent": "Kee Kee folder efficiency.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm looks like.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main idea is this.",
                    "label": 1
                },
                {
                    "sent": "They have some kind of working set of examples.",
                    "label": 1
                },
                {
                    "sent": "We make one optimization path forward examples in the working ship and this.",
                    "label": 0
                },
                {
                    "sent": "We need to 1st compute this initial gradient global gradient of XI with respect the whole whole training set, But anyway.",
                    "label": 0
                },
                {
                    "sent": "Or update this.",
                    "label": 1
                },
                {
                    "sent": "Do all variables of example I.",
                    "label": 0
                },
                {
                    "sent": "Doing kondel conditional gradient gradient steps.",
                    "label": 0
                },
                {
                    "sent": "But we are not fully optimizing.",
                    "label": 0
                },
                {
                    "sent": "Kind of because we have we are not.",
                    "label": 0
                },
                {
                    "sent": "Going to code the global optimum if we even if we cut off.",
                    "label": 0
                },
                {
                    "sent": "Go.",
                    "label": 1
                },
                {
                    "sent": "Optimizing this to look through the four.",
                    "label": 0
                },
                {
                    "sent": "Instead we do.",
                    "label": 1
                },
                {
                    "sent": "Couple of item, race and move on to the next example and so on.",
                    "label": 0
                },
                {
                    "sent": "So I iterate over to over to working set.",
                    "label": 0
                },
                {
                    "sent": "OK, after one pass we compute the cake at a condition selects and duality gap.",
                    "label": 0
                },
                {
                    "sent": "If you are small enough that you stop, otherwise we iterate this process so.",
                    "label": 0
                },
                {
                    "sent": "Quite simple loop loop iterating.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Only condition on gradient ascent or descent.",
                    "label": 1
                },
                {
                    "sent": "So the idea?",
                    "label": 0
                },
                {
                    "sent": "Is the following.",
                    "label": 0
                },
                {
                    "sent": "We want to find the best feasible points.",
                    "label": 0
                },
                {
                    "sent": "With respect to the gradient.",
                    "label": 0
                },
                {
                    "sent": "So, and that's kind of so kind of a.",
                    "label": 0
                },
                {
                    "sent": "It's a linear approximation of the quartering objective, and we want the best point on this.",
                    "label": 0
                },
                {
                    "sent": "Along this linear approximation.",
                    "label": 0
                },
                {
                    "sent": "So we need to solve a linear program like this.",
                    "label": 0
                },
                {
                    "sent": "In order to find the best feasible point.",
                    "label": 0
                },
                {
                    "sent": "OK, but this best piece piece of point is not one that necessarily maximized the quadratic kind of objective.",
                    "label": 0
                },
                {
                    "sent": "Instead of it.",
                    "label": 0
                },
                {
                    "sent": "Now look along this direction and find a saddle point of the quadratic objective.",
                    "label": 0
                },
                {
                    "sent": "Go there.",
                    "label": 0
                },
                {
                    "sent": "It turns out this this finding a saddle point.",
                    "label": 0
                },
                {
                    "sent": "This kind of cost is not big, so we can afford to step exactly through the best point along that Ray, we don't need to make some kind of guess how much.",
                    "label": 0
                },
                {
                    "sent": "How big the steps would be?",
                    "label": 0
                },
                {
                    "sent": "OK, and in order to solve this linear program, we have test used to matlab's linear interpoint solver.",
                    "label": 0
                },
                {
                    "sent": "So quite.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "States ministrations makes a state of the question.",
                    "label": 0
                },
                {
                    "sent": "Any variables that will help consumers, then you programming stuff around.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "OK, but I have missed out.",
                    "label": 0
                },
                {
                    "sent": "You update this crazy and so that's kind of your.",
                    "label": 0
                },
                {
                    "sent": "What you're doing you move.",
                    "label": 0
                },
                {
                    "sent": "I will serve an animation so.",
                    "label": 0
                },
                {
                    "sent": "So what happens?",
                    "label": 0
                },
                {
                    "sent": "So here's kind of what will happen.",
                    "label": 0
                },
                {
                    "sent": "You'll start from here.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is your quadric objective.",
                    "label": 0
                },
                {
                    "sent": "Computer gradient.",
                    "label": 0
                },
                {
                    "sent": "OK, then you look for the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Best visible point.",
                    "label": 0
                },
                {
                    "sent": "Along this crazy and so you have this point there.",
                    "label": 0
                },
                {
                    "sent": "Then your code is.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Battle Point of this.",
                    "label": 0
                },
                {
                    "sent": "Because you're restricted to go along this line, the saddle point of this quadratic or checked, it is flat.",
                    "label": 0
                },
                {
                    "sent": "Then you iterate.",
                    "label": 0
                },
                {
                    "sent": "Computer gradient tool.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exploring find the conditional gradient.",
                    "label": 0
                },
                {
                    "sent": "Find the saddle point.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And repeat so this is kind of the search we do and we.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like I said, I will only do a few iterations.",
                    "label": 0
                },
                {
                    "sent": "We are not going to continue this very long.",
                    "label": 0
                },
                {
                    "sent": "We're going to move to the next example and OK in future.",
                    "label": 0
                },
                {
                    "sent": "Come back to that example an update, so that's kind of we want to be quite greedy in our optimization, so we want to kind of.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't want to spend too much time of really optimizing this.",
                    "label": 0
                },
                {
                    "sent": "One example, we rather code with other ones.",
                    "label": 0
                },
                {
                    "sent": "OK, something about working.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maintenance this is just.",
                    "label": 0
                },
                {
                    "sent": "Kind of heuristic.",
                    "label": 0
                },
                {
                    "sent": "What we have kind of learned.",
                    "label": 0
                },
                {
                    "sent": "So one is an observation that.",
                    "label": 0
                },
                {
                    "sent": "Most of our training data will be active at the optimal, so we will not have very sparse solution, so keeping the.",
                    "label": 0
                },
                {
                    "sent": "Working set small is not the strategy that works.",
                    "label": 1
                },
                {
                    "sent": "You want.",
                    "label": 0
                },
                {
                    "sent": "You want to be efficient that way so.",
                    "label": 0
                },
                {
                    "sent": "So we're kind of surrendering to looking up the whole training set set.",
                    "label": 0
                },
                {
                    "sent": "Immediately, so we're not looking at the small subset.",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, so this is the criteria if it's active so head task non 0 dual variables or or it violates violates its margins.",
                    "label": 0
                },
                {
                    "sent": "It will be in the working set but we do something try to be a little bit clever.",
                    "label": 1
                },
                {
                    "sent": "May want to give more iterations.",
                    "label": 0
                },
                {
                    "sent": "2 examples that look kind of problematic that have to seem to constitute the duality cap alot so they will be spent more time on in order to kind of.",
                    "label": 0
                },
                {
                    "sent": "Try to decrease the duality as fast as possible.",
                    "label": 0
                },
                {
                    "sent": "But this is kind of just the heuristics.",
                    "label": 0
                },
                {
                    "sent": "It's not, not.",
                    "label": 0
                },
                {
                    "sent": "Kind of very, very deeply founded.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is basically what we do.",
                    "label": 0
                },
                {
                    "sent": "Done some.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiments.",
                    "label": 0
                },
                {
                    "sent": "Two datasets, Reuters corpus.",
                    "label": 0
                },
                {
                    "sent": "Where we have 34 Michael labels, three maximum credit is 3.",
                    "label": 0
                },
                {
                    "sent": "Actually it's very very very shallow tree.",
                    "label": 0
                },
                {
                    "sent": "The root has kind of almost half of the micro labels are child children of the root.",
                    "label": 0
                },
                {
                    "sent": "OK. Use 200, two 1500 documents for training 5000 for testing.",
                    "label": 1
                },
                {
                    "sent": "Viper all far, is apparent database there we have 188 micro labels.",
                    "label": 0
                },
                {
                    "sent": "The hierarchy is of depth 4.",
                    "label": 0
                },
                {
                    "sent": "And it's like quite a balanced hierarchy, so it's a real hierarchy.",
                    "label": 0
                },
                {
                    "sent": "They've used.",
                    "label": 0
                },
                {
                    "sent": "This is actually what type of gives it already gives us.",
                    "label": 0
                },
                {
                    "sent": "Ready made, split the training and testing sets so we are using exactly what what they take care.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a couple of.",
                    "label": 1
                },
                {
                    "sent": "Different algorithms our that we named hierarchical maximal margin Markov networks according to task cards, kind of.",
                    "label": 0
                },
                {
                    "sent": "Naming scheme comparison.",
                    "label": 0
                },
                {
                    "sent": "Flat SVM so predicate smartlabel independently.",
                    "label": 0
                },
                {
                    "sent": "SVM trained hierarchically, so you so to note only part of the training examples.",
                    "label": 0
                },
                {
                    "sent": "On hierarchical regularised least quest, that's Nicola Castle paintings.",
                    "label": 0
                },
                {
                    "sent": "No new algorithm.",
                    "label": 0
                },
                {
                    "sent": "Very recently introduced.",
                    "label": 0
                },
                {
                    "sent": "So everything implemented in Matlab.",
                    "label": 0
                },
                {
                    "sent": "And like I said, the lifts also lower used as the solar folder for a conditional trading and OK. And here's the.",
                    "label": 0
                },
                {
                    "sent": "Point GB RAM install.",
                    "label": 0
                },
                {
                    "sent": "That's what happens.",
                    "label": 0
                },
                {
                    "sent": "This is why for Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning curve trained with this simplified hierarchical loss.",
                    "label": 0
                },
                {
                    "sent": "We're optimizing one 1,000,000 little more dual variables.",
                    "label": 0
                },
                {
                    "sent": "Like I said, majority of them are non zero at optimum.",
                    "label": 0
                },
                {
                    "sent": "OK, here I have the objective and this optimum is in 100, so it's scale that way.",
                    "label": 0
                },
                {
                    "sent": "Saying error here, testing error here and this is 01 loss.",
                    "label": 0
                },
                {
                    "sent": "So it's like how many of the documents are.",
                    "label": 0
                },
                {
                    "sent": "Contain one or more errors, so it's kind of.",
                    "label": 0
                },
                {
                    "sent": "Kind of the strictest.",
                    "label": 0
                },
                {
                    "sent": "Plus you can enforce well you can.",
                    "label": 0
                },
                {
                    "sent": "You can see it.",
                    "label": 0
                },
                {
                    "sent": "It's that.",
                    "label": 0
                },
                {
                    "sent": "The test error actually bottom out out out first, so after seems like 2 hours now.",
                    "label": 0
                },
                {
                    "sent": "It's their very according to care training error.",
                    "label": 0
                },
                {
                    "sent": "Little bit slower going up.",
                    "label": 0
                },
                {
                    "sent": "It's like training error followed.",
                    "label": 0
                },
                {
                    "sent": "Follows the objective kind of.",
                    "label": 0
                },
                {
                    "sent": "Kind of mirrors gross stuff.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But you could.",
                    "label": 0
                },
                {
                    "sent": "You could say that OK?",
                    "label": 0
                },
                {
                    "sent": "Well, we are 95% or something like that of the maximum here.",
                    "label": 0
                },
                {
                    "sent": "The training error will be.",
                    "label": 0
                },
                {
                    "sent": "The bottom.",
                    "label": 0
                },
                {
                    "sent": "So you can think there's some kind of early stopping maybe.",
                    "label": 0
                },
                {
                    "sent": "Possible here.",
                    "label": 0
                },
                {
                    "sent": "And you don't see much.",
                    "label": 0
                },
                {
                    "sent": "Overfitting, it's like when we consider this cervicals a little bit, but.",
                    "label": 0
                },
                {
                    "sent": "It doesn't go up or down, it says.",
                    "label": 0
                },
                {
                    "sent": "Quiet quiet level.",
                    "label": 0
                },
                {
                    "sent": "OK, some prediction.",
                    "label": 0
                },
                {
                    "sent": "Errors.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here right here I have 01 loss for Reuters data set.",
                    "label": 0
                },
                {
                    "sent": "Here is my profile, so it's 01 loss symmetric difference.",
                    "label": 0
                },
                {
                    "sent": "Lausanne, this hierarchical loss and algorithms, SVM, Harkless, PM.",
                    "label": 0
                },
                {
                    "sent": "Oracle recognize least squares on our two.",
                    "label": 0
                },
                {
                    "sent": "Two different training losses, so this is trained with symmetric different loss.",
                    "label": 0
                },
                {
                    "sent": "This is trained with this simplified hierarchical loss.",
                    "label": 0
                },
                {
                    "sent": "So what you can see is that we're doing pretty well in terms of the 01 loss, so these are the lowest figures.",
                    "label": 0
                },
                {
                    "sent": "Flat Esbian loses quite quite clearly.",
                    "label": 0
                },
                {
                    "sent": "Both here and here the same thing as the lowest zero and errors are with this with our method.",
                    "label": 0
                },
                {
                    "sent": "But then it's much more mixed when you think about the other other loss funds.",
                    "label": 0
                },
                {
                    "sent": "So since we're doing OK in terms of the symmetric difference loss as well, so these figures are not with that and especially here we are.",
                    "label": 0
                },
                {
                    "sent": "Quite good.",
                    "label": 0
                },
                {
                    "sent": "OK, one thing to note here is this wipe off of East Side.",
                    "label": 0
                },
                {
                    "sent": "Here we really have a.",
                    "label": 0
                },
                {
                    "sent": "Four level hierarchy and that's.",
                    "label": 0
                },
                {
                    "sent": "Quite quite a balanced one and this process hierarchy is quite shallow and unbalanced, so you can actually see that even the flat SVM does pretty well.",
                    "label": 0
                },
                {
                    "sent": "In this hierarchal losses because there's no law hierarchy to benefit from, so it's like.",
                    "label": 0
                },
                {
                    "sent": "On but yes, you can see here we have real hierarchy, so flat SV and starts to.",
                    "label": 0
                },
                {
                    "sent": "Fall behind.",
                    "label": 0
                },
                {
                    "sent": "OK, here are my my results.",
                    "label": 0
                },
                {
                    "sent": "Some conclusions so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we present the kernel based uploads for hierarchical.",
                    "label": 1
                },
                {
                    "sent": "Text classification where we allowed the documents belong to more than one class at a time.",
                    "label": 1
                },
                {
                    "sent": "So we utilized this dependency structure of the micro labels.",
                    "label": 1
                },
                {
                    "sent": "By imposing a mark of structure over the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "OK, and we get decent prediction accuracy.",
                    "label": 0
                },
                {
                    "sent": "And especially when we have a little bit of a hierarchy like in the Bible, Alpha digested.",
                    "label": 0
                },
                {
                    "sent": "OK, and we have a feasible optimization scheme for this.",
                    "label": 0
                },
                {
                    "sent": "Quite quite a big.",
                    "label": 1
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "And this this working horse there is described conditional gradient search in.",
                    "label": 0
                },
                {
                    "sent": "Subproblem subram problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is all I have to say, so thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "That's very nice work, I would just add I was wondering about your only motivation that the partition function was hard to evaluate that at three is a case in which the partition function principle is relatively easy.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Problem there, would that be another possible way of attacking the problem, yeah?",
                    "label": 0
                },
                {
                    "sent": "Possibly.",
                    "label": 0
                },
                {
                    "sent": "The thing is, I don't know how to deal with the fact that it's dependent on the weights vector I'm learning, so I have infinite amount of.",
                    "label": 0
                },
                {
                    "sent": "These guys to available because I'm learning and continuous later, so that's.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's a way to do it, though I'm not aware of.",
                    "label": 0
                },
                {
                    "sent": "So that's yeah.",
                    "label": 0
                },
                {
                    "sent": "Summation and then say calculated derivative.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that would be another evaluation.",
                    "label": 0
                },
                {
                    "sent": "So that is.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, that's kind of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's some kind of sometimes entered my mind.",
                    "label": 0
                },
                {
                    "sent": "That is it really that hard disk partition function?",
                    "label": 0
                },
                {
                    "sent": "But I never have really gone that problem lies not in the summations so much, but in the dependency.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        }
    }
}