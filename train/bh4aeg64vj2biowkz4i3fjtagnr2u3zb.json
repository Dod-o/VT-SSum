{
    "id": "bh4aeg64vj2biowkz4i3fjtagnr2u3zb",
    "title": "Generalization and Exploration via Value Function Randomization",
    "info": {
        "author": [
            "Ben Van Roy, Management Science and Engineering Department, Stanford University"
        ],
        "published": "July 28, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Artificial Intelligence",
            "Top->Social Sciences->Psychology",
            "Top->Social Sciences->Economics",
            "Top->Medicine->Neuroscience",
            "Top->Technology->Engineering->Electrical Engineering->Control Engineering"
        ]
    },
    "url": "http://videolectures.net/rldm2015_van_roy_function_randomization/",
    "segmentation": [
        [
            "It's great to be here among.",
            "All these reinforcement learning researchers, many of whom I have known for decades.",
            "After a hiatus from the field, I started working on reinforcement learning again.",
            "About four years ago.",
            "And over this time I've worked with a great group of students.",
            "And what I want to share with you today is things we've learned over these four years.",
            "Hopefully some of it is new and interesting to you."
        ],
        [
            "Right before talking about the broader reinforcement learning problem, I wanted to.",
            "Discuss the online optimization problem.",
            "An what I mean by online optimization?"
        ],
        [
            "And is really.",
            "Reinforcement learning without delayed consequences.",
            "So in particular, the situation is an agent interact with an environment through a sequence of one shot engagements.",
            "Each time.",
            "Taking an action 80 and observing an outcome YT at the agent get some reward based on what he observes.",
            "Now, this context is simpler than the general reinforcement learning context, but it does bring to the fore this exploration exploitation tradeoff.",
            "In a simpler setting."
        ],
        [
            "So let me start by.",
            "Going through a few heuristics that are commonly used to deal with this tradeoff."
        ],
        [
            "Wanted dithering.",
            "And in dithering.",
            "We essentially say, Let's sometimes exploit that is take the action that we think is best given what we know and sometimes explore that is just take a random action.",
            "And so, with dithering, you assign a probability to each action you assign a high probability to be actually think is optimal.",
            "And then you distribute probabilities among other actions.",
            "And this is a simple way to tradeoff between exploration and exploitation, but it tends not to be particularly efficient.",
            "And in particular, it fails to write off bad actions.",
            "So suppose you learn that action one.",
            "Is really a bad action where you learn that there's no way action one is going to be the best action.",
            "Well, with dithering you keep trying action one regardless.",
            "Just because you sprinkle around probability among actions that aren't optimal."
        ],
        [
            "Another heuristic that's a bit more sophisticated is the upper confidence bound heuristic.",
            "And here what you do is you maximize an optimistic estimate of reward.",
            "So to each action you assign a expected reward and then you come up with an upper confidence bound.",
            "That tells you, given your uncertainty.",
            "About the reward you learn from that action.",
            "You know what's the highest statistically plausible reward to it?",
            "Assign that action.",
            "And then among those upper confidence bounds, you choose the one that's largest.",
            "So in this case it would be action for an.",
            "You take that action.",
            "OK, so this upper confidence bound approach is another heuristic for exploration.",
            "And in some cases you can show this is near optimal.",
            "In terms of how it manages exploration exploitation tradeoff, but you might ask in general, how?",
            "How well this works?",
            "Cannot."
        ],
        [
            "Class of heuristics that have received a lot of attention lately.",
            "Goes under the name of Thompson sampling.",
            "And in Thompson sampling, what you do?",
            "Is you sample an action at random according to the probability that that action is optimal?",
            "OK, so you come up with some distribution over actions assigned to each.",
            "The probability that you think that action is optimal given what you've seen so far.",
            "And then you just take a random draw from that distribution.",
            "An one question is whether this is better than UCB.",
            "There's a common narrative in the literature that suggests this is much better than UCB, and I'll talk more about that."
        ],
        [
            "Alright, just to give an example of a problem in online optimization where you could apply these sorts of eristics, let's take a look at online linear programming, 'cause it's a simple context."
        ],
        [
            "So nonlinear programming.",
            "What you want to do is select an action from a polytope, so there's some fixed polytope.",
            "And your action 80 comes from that polytope."
        ],
        [
            "And then your reward.",
            "Your feedback is your reward in this context.",
            "Yet that's also known as bandit feedback when the only feedback you get as a reward.",
            "OK, and your reward is an inner product between some unknown.",
            "Parameter vector Theta.",
            "And your action.",
            "Which is a vector in this polytope plus some white Gaussian noise."
        ],
        [
            "Alright, Anne.",
            "You don't know what Theta is, and that's what makes this problem challenging.",
            "You're trying to learn Theta so that you can make better decisions.",
            "Decisions that lead to greater rewards and one important issue is how you represent what you know about Theta.",
            "So one approach would be to represent a posterior distribution for Theta.",
            "In this case, if your prior over Theta is Gaussian, your post here is going to be Gaussian because of the structure of the reward.",
            "And so at anytime your posterior is represented by a mean vector, muti and a covariance matrix.",
            "Sigma T. Another approach might be to represent uncertainty in terms of confidence.",
            "Set Capital Theta of T that represents statistically plausible.",
            "A range of statistically possible vectors theater.",
            "OK, and in Thompson sampling it typically would represent uncertainty loops in terms of a probability distribution.",
            "While in."
        ],
        [
            "In ECB, approaches would represent it in terms of confidence set.",
            "Anna Natural confidence that to choose here is, of course in the ellipsoid because that's the level set for the posterior."
        ],
        [
            "Alright, so question.",
            "Is Thompson sampling better than UCP?"
        ],
        [
            "And as I mentioned, a lot of their literature suggests yes, and here's an example.",
            "Here's some results from.",
            "Running UCB and Thompson sampling.",
            "This online linear programming problem.",
            "And these results are averaged over many simulations and which plotted is the per.",
            "Regret average, instantaneous regret.",
            "Over the number of time periods we've learned.",
            "OK, so in each case of course the per.",
            "Regret goes down as we learn more.",
            "These are two upper confidence bound algorithms taken from these papers, and if you run Thompson sampling on the same problem."
        ],
        [
            "Looks much better.",
            "OK, so a lot of people say because of this, Thompson sampling is much better than UCB.",
            "But one thing that I think is important to note is if you."
        ],
        [
            "You take UCB and tweak it a bit in order to get an algorithm that works well for this problem.",
            "It actually works a little bit better than Thompson sampling.",
            "Not much better, but a little bit better.",
            "And this is our experience across many problem classes, right?",
            "UCB.",
            "If you tune the parameters right, works a little bit better than Thompson sampling, but not much."
        ],
        [
            "And I think the issue here is that in the UCB literature.",
            "For example, in this case you need to decide how big of an ellipsoid you're going to use.",
            "Alright, and in the UCB literature people are interested in proving theorems showing that you get efficient learning an and to do so, you need to come up with some formula that gives you size for this ellipsoid.",
            "This confidence set in this case.",
            "And and those formulas are designed for theorem proving when you apply them to practical problem, they tend not to work very well.",
            "So if you take a UCB algorithm from a paper and apply it in practice tends not to work well, you need to do some tuning of what confidence that you use.",
            "OK, so that's the real issue.",
            "One nice thing about Thompson sampling though is you don't do that tuning right.",
            "There's no confidence that doesn't play a role in Thompson sampling.",
            "You're just sampling from the posterior distribution, so there isn't this extra parameter you have to tune.",
            "So that's an interesting thing.",
            "We sort of learned about about the comparison between these algorithms."
        ],
        [
            "Another important thing that we came to realize is that.",
            "Thompson sampling is often.",
            "Computationally more convenient.",
            "And in particular, in many contexts, UCB is intractable."
        ],
        [
            "Again, let's consider the online linear programming problem.",
            "So in this case, if we wanted to."
        ],
        [
            "Run Thompson sampling.",
            "What you do in each time step.",
            "If you sample eh, parameter vector Theta hat T from the posterior, that's easier to do your sampling from a posterior Gaussian distribution.",
            "OK, and then we optimize.",
            "We choose an action that optimizes given the sample parameter.",
            "That's easy to do.",
            "We're just solving a linear program.",
            "OK.",
            "Very easy."
        ],
        [
            "If you do, you see what you have to do is solve this maximization problem, where we're simultaneously maximizing over actions.",
            "And parameter vectors."
        ],
        [
            "That that's NP hard."
        ],
        [
            "An.",
            "It is, however, tractable if you have a problem with a small number of vertices that you can iterate through, but linear programs in general don't have small numbers of vertices, but in order to generate, for example, the results on the previous slide.",
            "I chose problems with small numbers of vertices, but in general this is an NP hard and it's not clear how to solve this problem.",
            "OK, so so in many cases we've looked at constant sampling is very easy to do.",
            "UCB is hard to do."
        ],
        [
            "Alright now.",
            "One way people have gone around this in the literature.",
            "And in the UCB literature, before Thompson sampling picked up momentum, is that people would use.",
            "Very different sorts of confidence sets.",
            "So for example, in this work here, people said, well, we can't solve this problem.",
            "Where we have an ellipsoidal confidence set, so let's use a hyper rectangular confidence set.",
            "'cause that makes the problem tractable.",
            "And then you can show that if you use a hyper rectangular confidence set, the regret you get.",
            "From UCB scales up by a factor of D, where D is the dimension of the parameter space, that's a huge factor, right?",
            "As a dimension grows, but you know in the design of UCB algorithms, sometimes it's necessary to cut corners in order to attain tractability.",
            "You can.",
            "Get the same kind of performance, often without cutting such corners by using Thompson sampling.",
            "OK."
        ],
        [
            "Alright, so to summarize what I've said."
        ],
        [
            "What we found is Thompson sampling outperforms UCB design for analysis.",
            "Thompson sampling is slightly underperforms.",
            "Well tuned UCB algorithms where you choose the right confidence sets.",
            "Thompson sampling is often tractable and UCB isn't.",
            "And Thompson sampling outperforms UCB where confidence sets we should've choosing the wrong confidence sets in order to make ECB tractable."
        ],
        [
            "More broadly, if you're interested in our findings and along these lines which you pointed to a few papers.",
            "One, actually a pair of papers in 2013 that I wrote with Dan Russo.",
            "I think are worth looking at.",
            "One is a posterior sampling in the title and one has eluded dimension in the title.",
            "But this paper does a few things.",
            "First, it formally connects UCB and Thompson sampling.",
            "It essentially shows that Thompson sampling is a randomized approximation to UCB.",
            "OK. An the result.",
            "It shows that can be used to translate UCB regret bounds from the UCB literature too.",
            "Expected regret bounds for Thompson sampling.",
            "So you automatically get all the results from the literature.",
            "But in going through and translating those results from UCB to Thompson sampling, we got kind of frustrated because UCB literature has a long list of results.",
            "And we felt like, you know people were proving like 1 result per problem and Eusebian there's gotta be a cleaner way to do this.",
            "So what we also did in this paper we unified these results so that you just have one result that specializes to all these UCB problems.",
            "And that's where this notion of uludere dimension comes in.",
            "It's sort of a measure of complexity of the individual problems that you plug into the regret bound to get to recover all these UCB regret bounds.",
            "So you know, hopefully we clean that up a bit.",
            "So if you just look at the UCB literature, hopefully we cleaned up even that letter.",
            "Shared bit here.",
            "Now I should mention that this paper is limited to the case of bandit feedback.",
            "Although it's quite general in terms of you could have any reward model, for example, any nature of dependencies among arms in the multi arm bandit problem.",
            "And also we set up the model so that it's general in some ways so that things like contextual bandit models, cautious bandit models, adversarial bandit models are all special cases that you can get regret bounds for those from our one result as well.",
            "Anyways, if you're interested in this stuff, you should take a look at.",
            "This pair of."
        ],
        [
            "Nurse.",
            "One thing we realized when we were doing all this analysis is that.",
            "It's actually possible to do all of this using information theory, and so we ended up writing another paper with an analysis based on information theory.",
            "And it turns out to be quite elegant.",
            "You can prove all these regret bounds.",
            "You know, and in some way stronger, using information theory.",
            "And in particular, one benefit of the information theoretic framework here is that we can handle general general feedback information structures, not just structures where you have bandit feedback, but where you have any sort of feedback.",
            "OK, so this paper sort of handles this broader class of problems in case you're interested in that."
        ],
        [
            "Alright.",
            "So I've talked about.",
            "Upper confidence bound algorithms and Thompson sampling, and we looked at that because we are interested in reinforcement learning and try it wanting to understand different ways of doing exploration.",
            "Um?",
            "But as we were looking at these two algorithms, which seemed to be the two favourites for for the online optimization, we also realized that these approaches aren't perfect.",
            "So there are some shortcomings to these two approaches that I should mention.",
            "Of course, these are heuristics, another Bayes optimal.",
            "So one might ask, are they close enough?",
            "Are they close enough to optimal?",
            "And there's a few ways in which they are not.",
            "For starters, these algorithms are, in a sense, design for large, very large time horizons.",
            "OK, so if what matters to you are short time horizons, these algorithms may not.",
            "Be good for you.",
            "There may be ways to tweak these algorithms to deal with short horizons, like for example in UCB.",
            "You might shrink the confidence sets in Thompson sampling.",
            "You might take multiple samples at once and optimize averages rather than individual samples, but that all has to be explored and I think that area is not all not totally sorted out.",
            "As they stand right now, the algorithms are really designed for very long time horizons.",
            "Also, you know UCB is very sensitive to the sizing of the confidence that we've seen, and it's hard to do.",
            "It's hard to come up with the right formula to size or confident steps and the wrong formula may sacrifice factor of two 345 ten in regret, and that's just the nature of UCB.",
            "Thompson sampling tends to work almost as well as UCB with the right confidence set.",
            "But it does give a factor of 2.",
            "Typically due to randomization.",
            "So in a sense, UCB always is optimistic and guides the optimistic action.",
            "Thompson sampling is random, sometimes it's optimistic, sometimes it's not, and you're sort of counting on it being optimistic at least half the time.",
            "And that's where you lose a factor of two different randomization.",
            "OK, so that said, if.",
            "If you know temporal preferences.",
            "Don't matter much to you.",
            "And if moderate constant like losing a factor of two and regret doesn't matter much to you, there are a broad variety of problems for UCB and Thompson.",
            "Sampling are about as good as you can do it.",
            "Pretty close to optimal.",
            "However, in thinking this through, we also found examples where UCB and Thompson sampling just do completely the wrong thing.",
            "They just fail miserably.",
            "And so I thought I should share with you.",
            "These examples and how one might address that.",
            "OK, so here's one case where they fail.",
            "It's a sparse linear bandit problem."
        ],
        [
            "So suppose we're dealing with this linear optimization kind of problem, but where where the reward is linear in the action.",
            "OK, let's take a really simple case where there's no noise and just say the reward is linear in the action.",
            "And let's take the one sparse case.",
            "So in this case Theta.",
            "This is going to be binary vector.",
            "Where one components equal to 1, the other components each equal to zero.",
            "We just don't know which components equal to 1.",
            "And let's say our prior assigns equal probability to each component.",
            "Um?",
            "And let's say our actions allow us to query averages over subsets of the components, so you can choose an action that says give me the average overall components.",
            "You can choose an action that says give me an average over the first half of the components, or any of the above."
        ],
        [
            "Now.",
            "UCB and Thompson sampling require order D. Samples to identify which component of Theta is equal to 1.",
            "Essentially UCB and Thompson sampling rule out one component at a time.",
            "And they do this because in a sense, each of these heuristics hypothesizes that one of the components is the one and then sort of tries to figure out whether that hypothesis is correct or not, and if it's not corrected, rules that out.",
            "This ruling out one component at a time."
        ],
        [
            "On the other hand, for this problem you can obviously.",
            "Figure out which component is 1 in log D time.",
            "Through, for example, binary search.",
            "OK, so log D versus D is a huge difference, and UCB and Thompson sampling are clearly doing the wrong thing here."
        ],
        [
            "Another example is.",
            "Assortment optimization So suppose we're dealing with a problem where there are different customer types in the world.",
            "And we have.",
            "Many products.",
            "Each geared to a particular customer type.",
            "So the blue product here are designed for blue customers, the green for green, Brown for Brown.",
            "OK and then.",
            "Suppose we."
        ],
        [
            "We are faced by customer of unknown type.",
            "What we want to do is through repeated interactions.",
            "Figure out what type is so that we can best serve him.",
            "Right and then these repeated interactions we get to present him with.",
            "Assortments of products.",
            "Let's say we get to present three products at a time, and then we see if he chooses one or.",
            "Just doesn't choose any."
        ],
        [
            "OK, so in each time step we.",
            "Show MM products.",
            "An through sequence of interactions where either selects one product or passes.",
            "We learned about him and do a better job of presenting with him in the product with him.",
            "With the right products in the future."
        ],
        [
            "It turns out that UCB and Thompson sampling.",
            "In each engagement.",
            "We'll present 11 type of product.",
            "So in each engagement it'll sort of guess.",
            "It'll say, hey, let's guess that this is a blue customer.",
            "If it's a blue customer, we should present all blue products and let's see what happens.",
            "OK, so in each engagement you present one color.",
            "And that turns out to be the wrong way to go."
        ],
        [
            "Because you can show easily that by diversifying your portfolio of products by diversifying your assortments.",
            "You can accelerate learning by a factor of M. OK, Sam could be huge.",
            "It's a clearly again in this case.",
            "You see in Thompson sampling.",
            "Completely wrong."
        ],
        [
            "Alright, so.",
            "One approach, one way of dealing with this.",
            "Is through information directed sampling, which the new algorithmic concept we proposed.",
            "The idea here is to minimize this thing called.",
            "We call it an information ratio, but what it really is is the ratio between the squared expected regret over the next over the current engagement divided by the mutual information between the action.",
            "We sorry between the optimal action and the.",
            "Outcome we may observe given the action we choose, and I'm not going to spend much time on this, but it's sort of a new single.",
            "Objective that you could apply."
        ],
        [
            "These problems.",
            "Turns out that optimizing this kind of objective kills UCB and Thompson sampling in the aforementioned examples, where these algorithms don't work well.",
            "We're quite surprised that you actually also get a slight improvement in the cases where UCB and Thompson sampling or near optimal.",
            "OK, so it seems to work better across the board.",
            "And you know, we have strong and very general regret regret bounds for this kind of heuristic."
        ],
        [
            "Um, few caveats.",
            "One is that this kind of algorithm touristic is tractable in some sense, but.",
            "Not practical, requires too much computation, and so I think there's a lot of.",
            "Algorithmic work to be done here.",
            "If one wants to apply this kind of method across a broad variety of problems.",
            "Second is 1 might ask whether what's used here is the right measure of information, where there is the right way to trade off between information and regret.",
            "And I don't know.",
            "It solves the problems we've looked at, but maybe there's a better way of doing things, but.",
            "But that's something to be sorted out."
        ],
        [
            "Alright anyways, if you're interested in information directed sampling, there's a paper.",
            "That we wrote on this topic.",
            "Alright, so let me move on to reinforcement learning."
        ],
        [
            "So the difference in reinforce."
        ],
        [
            "Learning is that you can have delayed consequences and that really complicates things."
        ],
        [
            "And in particular, to do effective reinforcement learning, you may need to do.",
            "What my group has come to call deep exploration.",
            "Deep exploration.",
            "What is deep exploration?",
            "Well, simplest to understand through a decision tree.",
            "So, so here's the decision tree.",
            "Suppose you have a problem with the actions, or 01 and rewards, or zero or one.",
            "Well, you take an action 01 and then suppose you take action Zero.",
            "You might realize rewards error one and then you work your way down the tree."
        ],
        [
            "Other actions and other rewards.",
            "Well, suppose you're in a situation where."
        ],
        [
            "You know?",
            "You look down this decision tree, you know you're over here and there's one branch down here.",
            "Where unsern.",
            "About what's going to happen in the future.",
            "But you know about everything else in this decision tree.",
            "Well.",
            "Deep exploration is where you take that in account and you figure, gosh, I want to make a beeline to learn about this red part of the tree.",
            "OK, it's.",
            "It's insufficient.",
            "To be myopic and just look one step ahead.",
            "Right, and it's insufficient to dither you really gotta look ahead.",
            "Multiple steps.",
            "Figure out where you want to get to to learn something you don't know.",
            "That's deep exploration."
        ],
        [
            "Ideas to invest now to learn something downstream and this only."
        ],
        [
            "Becomes important when delayed consequences matter.",
            "Now, deep exploration of the new term, but the concept really isn't."
        ],
        [
            "And.",
            "There's this whole literature that I think of as being about deep exploration."
        ],
        [
            "Really got started by current sensing.",
            "Where one interpretation of this paper they wrote.",
            "Is that deep exploration enables polynomial time reinforcement learning.",
            "If you don't do deep exploration, you end up with exponential time reinforcement learning in the worst case.",
            "You do deep exploration.",
            "You can achieve polynomial time OK, and that paper really."
        ],
        [
            "He started a. Trajectory of research where people develop better and better algorithms and their understanding and regret bounds.",
            "For.",
            "Reinforcement learning algorithms that do deep exploration."
        ],
        [
            "Now, the focus of this literature has been on the tabular rasa case case of tabular rasa MDP's, where you know the time you need to learn scales with the number of states in the number of actions and all that."
        ],
        [
            "And for this reason, some people find this line of work practically useless."
        ],
        [
            "Because realistic problems require generalization, you can't.",
            "Address realistic problems using a type of larossa framework.",
            "I'm.",
            "Turns out that realistic problems also sometimes.",
            "Require deep exploration, but that's not always the case.",
            "There are problems where you do enough exploration naturally anyways, so that even though you don't do deep exploration, you do well, but there are very few problems where you don't have to do generalization, right?",
            "So it seems like generalization is the most important thing, and there's a bunch of problems where you also need to do deep exploration.",
            "OK, so because because of this, deep exploration hasn't.",
            "Really caught on in a way that's practically useful."
        ],
        [
            "An innocence, you know, this may be a oversimplify."
        ],
        [
            "Asian butt?",
            "I see two cultures in reinforcement learning.",
            "That's separated partly by language, but.",
            "But here's the culture that really focuses on deep exploration and tabular rasa learning.",
            "And here the guys that actually try to get things working.",
            "Alright."
        ],
        [
            "And I think a really important direction for the field is to sort of merge these.",
            "And to design practical reinforcement, learning algorithms that combine deep exploration and generalization."
        ],
        [
            "All right, there has been some work on this over the past few years."
        ],
        [
            "And for example, there is a.",
            "Bunch of work on model based approaches that generalize and explore one issue with this strand of literature is that the methods are quite specialized to specific classes of models.",
            "And also, they're generally computationally intractable."
        ],
        [
            "Then there's this recent paper which is quite nice by posits and par.",
            "Which does deep exploration with interpolative value function generalization?",
            "And so interpolative value function generalization is nice.",
            "Because projections when you interpolate.",
            "Our maximum norm, non expensive and somehow that makes it easier to work with dynamic programming and so everything works out nicely.",
            "So this is a very nice piece of work, but for practical algorithms I think it's also important to extrapolate.",
            "So imagine, for example, that your state space consists of binary vectors.",
            "In other words, how much time do I have?",
            "Done already, I think we started late, didn't we?",
            "But I think we started late.",
            "OK good OK. Well anyways let me."
        ],
        [
            "Faster.",
            "OK, So what I want to talk about is new approach.",
            "To tell this called value function randomized."
        ],
        [
            "Anne."
        ],
        [
            "Anne."
        ],
        [
            "I don't have much time to talk."
        ],
        [
            "Gosh, how did I run out of time?"
        ],
        [
            "OK, so let me join."
        ],
        [
            "To the punch line."
        ],
        [
            "So."
        ],
        [
            "So."
        ],
        [
            "The idea here."
        ],
        [
            "There is."
        ],
        [
            "To use something like Thompson sampling to sample among plausibel value functions.",
            "OK, and we're going to talk about with an algorithm that's very similar to least squares value iteration LS VI.",
            "Except it randomizes in the process of working your way backwards.",
            "Doing value iterations.",
            "Can it randomizes in a way that achieves efficient exploration.",
            "OK, another sanity check.",
            "To all this."
        ],
        [
            "We've done analysis for tabular rasa learning.",
            "And in this analysis, what we do is, we assume a prior.",
            "Over over title rossem DPS.",
            "And then we look at expected regret for this algorithm given that prior.",
            "And you get a bound."
        ],
        [
            "Like this?",
            "Sort of interesting actually, 'cause it bound."
        ],
        [
            "Better than the bound that you get using analysis in literature that for UCR L2 and all these other algorithms.",
            "But we're still digesting all this and making sure this doesn't isn't because of prior.",
            "We use and all that."
        ],
        [
            "But anyway, so one one punch line here is that RLS VI is doing deep algorithm I mentioned is doing.",
            "Deep exploration and it's because you can see that because you have to be doing deep exploration to get a polynomial bound here."
        ],
        [
            "Regret analysis for something like this with generalization remains an open issue."
        ],
        [
            "Just a quick example.",
            "It's easy to find to design problems.",
            "Where when you do random value function randomization, you very quickly even with.",
            "Even with value function approximation, when you do, when you do this kind of randomization, you very quickly get optimal to optimal policy, whereas with something like Boltzmann exploration.",
            "It looks like you're searching for a needle in a haystack, and here's an example of a problem where the optimal policy gives you reward of 1 per timestep.",
            "And this algorithm are less VI the randomized thing.",
            "Gives you a reward of 1 per timestep, starting after about 150 timesteps, Boltzmann takes longer."
        ],
        [
            "And in fact, if you.",
            "Expand The X axis out to a billion.",
            "In a Boltzmann is still going looks like Bolton is searching for a needle in a haystack.",
            "But something like Arlis VI gets you the answer very quickly."
        ],
        [
            "OK, so let."
        ],
        [
            "Jump to the end here."
        ],
        [
            "So, um.",
            "One question is.",
            "Have interesting is how could you apply value function randomization.",
            "With nonlinear parameterisations like a neural network."
        ],
        [
            "For."
        ],
        [
            "Ocular how do you combine deep exploration with deep learning?",
            "And it turns out that there are ways to do this."
        ],
        [
            "One good approach is to use something like the bootstrap.",
            "Bootstrap is an approach to.",
            "Approximating the distribution of parameters when you don't want to construct that distribution explicitly, but you have data sampled from it."
        ],
        [
            "And in a sense, this is equivalent to doing something like a scattered version of experience replay.",
            "And by doing this you can apply.",
            "An appropriate version of value function randomization.",
            "So how does what you call deep exploration compare with simple optimistic initialization 'cause we know there are domains where you can just act Gridley if you have optimistic or optimistic values and that will take care of all the exploration.",
            "So how does deep exploration?",
            "So?",
            "I mean optimistic approach, you have to be more careful.",
            "It's not just a matter of initialization.",
            "But you can do something like UCL 2 which is essentially optimistic and it makes sure that you don't become pessimistic.",
            "So.",
            "So there's two things.",
            "One is something like ECL 2 does not extend well to the case where you want to generalize.",
            "Right, so the key here is we want a way to do deep exploration.",
            "That works together with broad ways of doing generalization across the state space.",
            "So that's one thing the other is, as I mentioned, you may actually save a square root of X Factor in the regret by doing something like.",
            "What I talked about and I can.",
            "Give some more intuition for why that might be the case, but it will take awhile.",
            "Hi, thanks for the great talk.",
            "Trying to get a bit more intuition into how you actually do the randomization of the value function.",
            "So for example it feels like 1 issue might be trying to estimate these sort of unknown unknowns.",
            "So if you have some kind of randomization over the value function, it feels like you have to have at least visited the state space enough to have computed the uncertainty in your value function to even drive you towards those unknown states.",
            "But perhaps you might have some room that you just never visited before at all and so even estimating the fact that you are uncertain about that room.",
            "In order to drive you towards it becomes very problematic, so I was just wondering if you've got any thoughts on how to deal with these issues.",
            "So are you thinking?",
            "What does your question apply even in the top of Dresser case?",
            "I think it's about generalization.",
            "So the question is if you have some.",
            "So the question is.",
            "How can you even estimate the uncertainty of your value function sufficiently well to drive you towards exploring parts of the state space which you know nothing about, which is something which the myopic approach is naive as they are occasionally will take you to some completely unknown part of the space, and I guess the question is how do you ensure that you even find out enough about these unknown unknowns in order to to drive you in that direction?",
            "Yeah, so.",
            "I know there are aspects of this that I don't understand perfectly yet, but in the case of.",
            "Suppose your representation can perfectly capture the true value function.",
            "Right?",
            "Then in your parameters, right, there's always going to be some uncertainty.",
            "In dimensions that.",
            "Are related to the part of the state space you haven't explored yet.",
            "Right, if your generalization of that part of this database is incorrect.",
            "So this is that that randomness will lead you there.",
            "No, things had more complicated.",
            "When?",
            "Your representation cannot perfectly capture.",
            "The value function.",
            "Then you can be thrown off.",
            "And there I suspect you can't get strong guarantees, but I think that's something really interesting to study.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's great to be here among.",
                    "label": 0
                },
                {
                    "sent": "All these reinforcement learning researchers, many of whom I have known for decades.",
                    "label": 0
                },
                {
                    "sent": "After a hiatus from the field, I started working on reinforcement learning again.",
                    "label": 0
                },
                {
                    "sent": "About four years ago.",
                    "label": 0
                },
                {
                    "sent": "And over this time I've worked with a great group of students.",
                    "label": 0
                },
                {
                    "sent": "And what I want to share with you today is things we've learned over these four years.",
                    "label": 0
                },
                {
                    "sent": "Hopefully some of it is new and interesting to you.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right before talking about the broader reinforcement learning problem, I wanted to.",
                    "label": 0
                },
                {
                    "sent": "Discuss the online optimization problem.",
                    "label": 1
                },
                {
                    "sent": "An what I mean by online optimization?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And is really.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning without delayed consequences.",
                    "label": 0
                },
                {
                    "sent": "So in particular, the situation is an agent interact with an environment through a sequence of one shot engagements.",
                    "label": 0
                },
                {
                    "sent": "Each time.",
                    "label": 0
                },
                {
                    "sent": "Taking an action 80 and observing an outcome YT at the agent get some reward based on what he observes.",
                    "label": 0
                },
                {
                    "sent": "Now, this context is simpler than the general reinforcement learning context, but it does bring to the fore this exploration exploitation tradeoff.",
                    "label": 0
                },
                {
                    "sent": "In a simpler setting.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me start by.",
                    "label": 0
                },
                {
                    "sent": "Going through a few heuristics that are commonly used to deal with this tradeoff.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wanted dithering.",
                    "label": 0
                },
                {
                    "sent": "And in dithering.",
                    "label": 0
                },
                {
                    "sent": "We essentially say, Let's sometimes exploit that is take the action that we think is best given what we know and sometimes explore that is just take a random action.",
                    "label": 0
                },
                {
                    "sent": "And so, with dithering, you assign a probability to each action you assign a high probability to be actually think is optimal.",
                    "label": 0
                },
                {
                    "sent": "And then you distribute probabilities among other actions.",
                    "label": 0
                },
                {
                    "sent": "And this is a simple way to tradeoff between exploration and exploitation, but it tends not to be particularly efficient.",
                    "label": 0
                },
                {
                    "sent": "And in particular, it fails to write off bad actions.",
                    "label": 1
                },
                {
                    "sent": "So suppose you learn that action one.",
                    "label": 0
                },
                {
                    "sent": "Is really a bad action where you learn that there's no way action one is going to be the best action.",
                    "label": 0
                },
                {
                    "sent": "Well, with dithering you keep trying action one regardless.",
                    "label": 0
                },
                {
                    "sent": "Just because you sprinkle around probability among actions that aren't optimal.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another heuristic that's a bit more sophisticated is the upper confidence bound heuristic.",
                    "label": 0
                },
                {
                    "sent": "And here what you do is you maximize an optimistic estimate of reward.",
                    "label": 0
                },
                {
                    "sent": "So to each action you assign a expected reward and then you come up with an upper confidence bound.",
                    "label": 0
                },
                {
                    "sent": "That tells you, given your uncertainty.",
                    "label": 0
                },
                {
                    "sent": "About the reward you learn from that action.",
                    "label": 0
                },
                {
                    "sent": "You know what's the highest statistically plausible reward to it?",
                    "label": 0
                },
                {
                    "sent": "Assign that action.",
                    "label": 0
                },
                {
                    "sent": "And then among those upper confidence bounds, you choose the one that's largest.",
                    "label": 0
                },
                {
                    "sent": "So in this case it would be action for an.",
                    "label": 0
                },
                {
                    "sent": "You take that action.",
                    "label": 0
                },
                {
                    "sent": "OK, so this upper confidence bound approach is another heuristic for exploration.",
                    "label": 0
                },
                {
                    "sent": "And in some cases you can show this is near optimal.",
                    "label": 0
                },
                {
                    "sent": "In terms of how it manages exploration exploitation tradeoff, but you might ask in general, how?",
                    "label": 0
                },
                {
                    "sent": "How well this works?",
                    "label": 0
                },
                {
                    "sent": "Cannot.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Class of heuristics that have received a lot of attention lately.",
                    "label": 0
                },
                {
                    "sent": "Goes under the name of Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "And in Thompson sampling, what you do?",
                    "label": 0
                },
                {
                    "sent": "Is you sample an action at random according to the probability that that action is optimal?",
                    "label": 0
                },
                {
                    "sent": "OK, so you come up with some distribution over actions assigned to each.",
                    "label": 0
                },
                {
                    "sent": "The probability that you think that action is optimal given what you've seen so far.",
                    "label": 0
                },
                {
                    "sent": "And then you just take a random draw from that distribution.",
                    "label": 0
                },
                {
                    "sent": "An one question is whether this is better than UCB.",
                    "label": 1
                },
                {
                    "sent": "There's a common narrative in the literature that suggests this is much better than UCB, and I'll talk more about that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, just to give an example of a problem in online optimization where you could apply these sorts of eristics, let's take a look at online linear programming, 'cause it's a simple context.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So nonlinear programming.",
                    "label": 0
                },
                {
                    "sent": "What you want to do is select an action from a polytope, so there's some fixed polytope.",
                    "label": 0
                },
                {
                    "sent": "And your action 80 comes from that polytope.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then your reward.",
                    "label": 0
                },
                {
                    "sent": "Your feedback is your reward in this context.",
                    "label": 0
                },
                {
                    "sent": "Yet that's also known as bandit feedback when the only feedback you get as a reward.",
                    "label": 1
                },
                {
                    "sent": "OK, and your reward is an inner product between some unknown.",
                    "label": 0
                },
                {
                    "sent": "Parameter vector Theta.",
                    "label": 0
                },
                {
                    "sent": "And your action.",
                    "label": 0
                },
                {
                    "sent": "Which is a vector in this polytope plus some white Gaussian noise.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, Anne.",
                    "label": 0
                },
                {
                    "sent": "You don't know what Theta is, and that's what makes this problem challenging.",
                    "label": 0
                },
                {
                    "sent": "You're trying to learn Theta so that you can make better decisions.",
                    "label": 0
                },
                {
                    "sent": "Decisions that lead to greater rewards and one important issue is how you represent what you know about Theta.",
                    "label": 0
                },
                {
                    "sent": "So one approach would be to represent a posterior distribution for Theta.",
                    "label": 0
                },
                {
                    "sent": "In this case, if your prior over Theta is Gaussian, your post here is going to be Gaussian because of the structure of the reward.",
                    "label": 0
                },
                {
                    "sent": "And so at anytime your posterior is represented by a mean vector, muti and a covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Sigma T. Another approach might be to represent uncertainty in terms of confidence.",
                    "label": 0
                },
                {
                    "sent": "Set Capital Theta of T that represents statistically plausible.",
                    "label": 0
                },
                {
                    "sent": "A range of statistically possible vectors theater.",
                    "label": 0
                },
                {
                    "sent": "OK, and in Thompson sampling it typically would represent uncertainty loops in terms of a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "While in.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In ECB, approaches would represent it in terms of confidence set.",
                    "label": 0
                },
                {
                    "sent": "Anna Natural confidence that to choose here is, of course in the ellipsoid because that's the level set for the posterior.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so question.",
                    "label": 0
                },
                {
                    "sent": "Is Thompson sampling better than UCP?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as I mentioned, a lot of their literature suggests yes, and here's an example.",
                    "label": 0
                },
                {
                    "sent": "Here's some results from.",
                    "label": 0
                },
                {
                    "sent": "Running UCB and Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "This online linear programming problem.",
                    "label": 0
                },
                {
                    "sent": "And these results are averaged over many simulations and which plotted is the per.",
                    "label": 0
                },
                {
                    "sent": "Regret average, instantaneous regret.",
                    "label": 0
                },
                {
                    "sent": "Over the number of time periods we've learned.",
                    "label": 0
                },
                {
                    "sent": "OK, so in each case of course the per.",
                    "label": 0
                },
                {
                    "sent": "Regret goes down as we learn more.",
                    "label": 0
                },
                {
                    "sent": "These are two upper confidence bound algorithms taken from these papers, and if you run Thompson sampling on the same problem.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looks much better.",
                    "label": 0
                },
                {
                    "sent": "OK, so a lot of people say because of this, Thompson sampling is much better than UCB.",
                    "label": 1
                },
                {
                    "sent": "But one thing that I think is important to note is if you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You take UCB and tweak it a bit in order to get an algorithm that works well for this problem.",
                    "label": 0
                },
                {
                    "sent": "It actually works a little bit better than Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "Not much better, but a little bit better.",
                    "label": 0
                },
                {
                    "sent": "And this is our experience across many problem classes, right?",
                    "label": 0
                },
                {
                    "sent": "UCB.",
                    "label": 0
                },
                {
                    "sent": "If you tune the parameters right, works a little bit better than Thompson sampling, but not much.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I think the issue here is that in the UCB literature.",
                    "label": 0
                },
                {
                    "sent": "For example, in this case you need to decide how big of an ellipsoid you're going to use.",
                    "label": 0
                },
                {
                    "sent": "Alright, and in the UCB literature people are interested in proving theorems showing that you get efficient learning an and to do so, you need to come up with some formula that gives you size for this ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "This confidence set in this case.",
                    "label": 0
                },
                {
                    "sent": "And and those formulas are designed for theorem proving when you apply them to practical problem, they tend not to work very well.",
                    "label": 0
                },
                {
                    "sent": "So if you take a UCB algorithm from a paper and apply it in practice tends not to work well, you need to do some tuning of what confidence that you use.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the real issue.",
                    "label": 0
                },
                {
                    "sent": "One nice thing about Thompson sampling though is you don't do that tuning right.",
                    "label": 0
                },
                {
                    "sent": "There's no confidence that doesn't play a role in Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "You're just sampling from the posterior distribution, so there isn't this extra parameter you have to tune.",
                    "label": 0
                },
                {
                    "sent": "So that's an interesting thing.",
                    "label": 0
                },
                {
                    "sent": "We sort of learned about about the comparison between these algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another important thing that we came to realize is that.",
                    "label": 0
                },
                {
                    "sent": "Thompson sampling is often.",
                    "label": 0
                },
                {
                    "sent": "Computationally more convenient.",
                    "label": 0
                },
                {
                    "sent": "And in particular, in many contexts, UCB is intractable.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, let's consider the online linear programming problem.",
                    "label": 0
                },
                {
                    "sent": "So in this case, if we wanted to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Run Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "What you do in each time step.",
                    "label": 0
                },
                {
                    "sent": "If you sample eh, parameter vector Theta hat T from the posterior, that's easier to do your sampling from a posterior Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we optimize.",
                    "label": 0
                },
                {
                    "sent": "We choose an action that optimizes given the sample parameter.",
                    "label": 0
                },
                {
                    "sent": "That's easy to do.",
                    "label": 0
                },
                {
                    "sent": "We're just solving a linear program.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Very easy.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you do, you see what you have to do is solve this maximization problem, where we're simultaneously maximizing over actions.",
                    "label": 0
                },
                {
                    "sent": "And parameter vectors.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That that's NP hard.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An.",
                    "label": 0
                },
                {
                    "sent": "It is, however, tractable if you have a problem with a small number of vertices that you can iterate through, but linear programs in general don't have small numbers of vertices, but in order to generate, for example, the results on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "I chose problems with small numbers of vertices, but in general this is an NP hard and it's not clear how to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so so in many cases we've looked at constant sampling is very easy to do.",
                    "label": 0
                },
                {
                    "sent": "UCB is hard to do.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright now.",
                    "label": 0
                },
                {
                    "sent": "One way people have gone around this in the literature.",
                    "label": 0
                },
                {
                    "sent": "And in the UCB literature, before Thompson sampling picked up momentum, is that people would use.",
                    "label": 0
                },
                {
                    "sent": "Very different sorts of confidence sets.",
                    "label": 0
                },
                {
                    "sent": "So for example, in this work here, people said, well, we can't solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Where we have an ellipsoidal confidence set, so let's use a hyper rectangular confidence set.",
                    "label": 0
                },
                {
                    "sent": "'cause that makes the problem tractable.",
                    "label": 0
                },
                {
                    "sent": "And then you can show that if you use a hyper rectangular confidence set, the regret you get.",
                    "label": 0
                },
                {
                    "sent": "From UCB scales up by a factor of D, where D is the dimension of the parameter space, that's a huge factor, right?",
                    "label": 1
                },
                {
                    "sent": "As a dimension grows, but you know in the design of UCB algorithms, sometimes it's necessary to cut corners in order to attain tractability.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Get the same kind of performance, often without cutting such corners by using Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so to summarize what I've said.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we found is Thompson sampling outperforms UCB design for analysis.",
                    "label": 1
                },
                {
                    "sent": "Thompson sampling is slightly underperforms.",
                    "label": 0
                },
                {
                    "sent": "Well tuned UCB algorithms where you choose the right confidence sets.",
                    "label": 0
                },
                {
                    "sent": "Thompson sampling is often tractable and UCB isn't.",
                    "label": 0
                },
                {
                    "sent": "And Thompson sampling outperforms UCB where confidence sets we should've choosing the wrong confidence sets in order to make ECB tractable.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More broadly, if you're interested in our findings and along these lines which you pointed to a few papers.",
                    "label": 0
                },
                {
                    "sent": "One, actually a pair of papers in 2013 that I wrote with Dan Russo.",
                    "label": 0
                },
                {
                    "sent": "I think are worth looking at.",
                    "label": 0
                },
                {
                    "sent": "One is a posterior sampling in the title and one has eluded dimension in the title.",
                    "label": 1
                },
                {
                    "sent": "But this paper does a few things.",
                    "label": 0
                },
                {
                    "sent": "First, it formally connects UCB and Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "It essentially shows that Thompson sampling is a randomized approximation to UCB.",
                    "label": 1
                },
                {
                    "sent": "OK. An the result.",
                    "label": 0
                },
                {
                    "sent": "It shows that can be used to translate UCB regret bounds from the UCB literature too.",
                    "label": 0
                },
                {
                    "sent": "Expected regret bounds for Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "So you automatically get all the results from the literature.",
                    "label": 0
                },
                {
                    "sent": "But in going through and translating those results from UCB to Thompson sampling, we got kind of frustrated because UCB literature has a long list of results.",
                    "label": 0
                },
                {
                    "sent": "And we felt like, you know people were proving like 1 result per problem and Eusebian there's gotta be a cleaner way to do this.",
                    "label": 0
                },
                {
                    "sent": "So what we also did in this paper we unified these results so that you just have one result that specializes to all these UCB problems.",
                    "label": 0
                },
                {
                    "sent": "And that's where this notion of uludere dimension comes in.",
                    "label": 0
                },
                {
                    "sent": "It's sort of a measure of complexity of the individual problems that you plug into the regret bound to get to recover all these UCB regret bounds.",
                    "label": 0
                },
                {
                    "sent": "So you know, hopefully we clean that up a bit.",
                    "label": 0
                },
                {
                    "sent": "So if you just look at the UCB literature, hopefully we cleaned up even that letter.",
                    "label": 1
                },
                {
                    "sent": "Shared bit here.",
                    "label": 1
                },
                {
                    "sent": "Now I should mention that this paper is limited to the case of bandit feedback.",
                    "label": 0
                },
                {
                    "sent": "Although it's quite general in terms of you could have any reward model, for example, any nature of dependencies among arms in the multi arm bandit problem.",
                    "label": 0
                },
                {
                    "sent": "And also we set up the model so that it's general in some ways so that things like contextual bandit models, cautious bandit models, adversarial bandit models are all special cases that you can get regret bounds for those from our one result as well.",
                    "label": 0
                },
                {
                    "sent": "Anyways, if you're interested in this stuff, you should take a look at.",
                    "label": 0
                },
                {
                    "sent": "This pair of.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nurse.",
                    "label": 0
                },
                {
                    "sent": "One thing we realized when we were doing all this analysis is that.",
                    "label": 0
                },
                {
                    "sent": "It's actually possible to do all of this using information theory, and so we ended up writing another paper with an analysis based on information theory.",
                    "label": 0
                },
                {
                    "sent": "And it turns out to be quite elegant.",
                    "label": 0
                },
                {
                    "sent": "You can prove all these regret bounds.",
                    "label": 0
                },
                {
                    "sent": "You know, and in some way stronger, using information theory.",
                    "label": 0
                },
                {
                    "sent": "And in particular, one benefit of the information theoretic framework here is that we can handle general general feedback information structures, not just structures where you have bandit feedback, but where you have any sort of feedback.",
                    "label": 0
                },
                {
                    "sent": "OK, so this paper sort of handles this broader class of problems in case you're interested in that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So I've talked about.",
                    "label": 0
                },
                {
                    "sent": "Upper confidence bound algorithms and Thompson sampling, and we looked at that because we are interested in reinforcement learning and try it wanting to understand different ways of doing exploration.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But as we were looking at these two algorithms, which seemed to be the two favourites for for the online optimization, we also realized that these approaches aren't perfect.",
                    "label": 0
                },
                {
                    "sent": "So there are some shortcomings to these two approaches that I should mention.",
                    "label": 0
                },
                {
                    "sent": "Of course, these are heuristics, another Bayes optimal.",
                    "label": 0
                },
                {
                    "sent": "So one might ask, are they close enough?",
                    "label": 0
                },
                {
                    "sent": "Are they close enough to optimal?",
                    "label": 0
                },
                {
                    "sent": "And there's a few ways in which they are not.",
                    "label": 0
                },
                {
                    "sent": "For starters, these algorithms are, in a sense, design for large, very large time horizons.",
                    "label": 0
                },
                {
                    "sent": "OK, so if what matters to you are short time horizons, these algorithms may not.",
                    "label": 0
                },
                {
                    "sent": "Be good for you.",
                    "label": 0
                },
                {
                    "sent": "There may be ways to tweak these algorithms to deal with short horizons, like for example in UCB.",
                    "label": 0
                },
                {
                    "sent": "You might shrink the confidence sets in Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "You might take multiple samples at once and optimize averages rather than individual samples, but that all has to be explored and I think that area is not all not totally sorted out.",
                    "label": 0
                },
                {
                    "sent": "As they stand right now, the algorithms are really designed for very long time horizons.",
                    "label": 0
                },
                {
                    "sent": "Also, you know UCB is very sensitive to the sizing of the confidence that we've seen, and it's hard to do.",
                    "label": 0
                },
                {
                    "sent": "It's hard to come up with the right formula to size or confident steps and the wrong formula may sacrifice factor of two 345 ten in regret, and that's just the nature of UCB.",
                    "label": 0
                },
                {
                    "sent": "Thompson sampling tends to work almost as well as UCB with the right confidence set.",
                    "label": 0
                },
                {
                    "sent": "But it does give a factor of 2.",
                    "label": 0
                },
                {
                    "sent": "Typically due to randomization.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, UCB always is optimistic and guides the optimistic action.",
                    "label": 0
                },
                {
                    "sent": "Thompson sampling is random, sometimes it's optimistic, sometimes it's not, and you're sort of counting on it being optimistic at least half the time.",
                    "label": 0
                },
                {
                    "sent": "And that's where you lose a factor of two different randomization.",
                    "label": 0
                },
                {
                    "sent": "OK, so that said, if.",
                    "label": 0
                },
                {
                    "sent": "If you know temporal preferences.",
                    "label": 0
                },
                {
                    "sent": "Don't matter much to you.",
                    "label": 0
                },
                {
                    "sent": "And if moderate constant like losing a factor of two and regret doesn't matter much to you, there are a broad variety of problems for UCB and Thompson.",
                    "label": 0
                },
                {
                    "sent": "Sampling are about as good as you can do it.",
                    "label": 0
                },
                {
                    "sent": "Pretty close to optimal.",
                    "label": 0
                },
                {
                    "sent": "However, in thinking this through, we also found examples where UCB and Thompson sampling just do completely the wrong thing.",
                    "label": 0
                },
                {
                    "sent": "They just fail miserably.",
                    "label": 0
                },
                {
                    "sent": "And so I thought I should share with you.",
                    "label": 0
                },
                {
                    "sent": "These examples and how one might address that.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's one case where they fail.",
                    "label": 0
                },
                {
                    "sent": "It's a sparse linear bandit problem.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So suppose we're dealing with this linear optimization kind of problem, but where where the reward is linear in the action.",
                    "label": 0
                },
                {
                    "sent": "OK, let's take a really simple case where there's no noise and just say the reward is linear in the action.",
                    "label": 0
                },
                {
                    "sent": "And let's take the one sparse case.",
                    "label": 0
                },
                {
                    "sent": "So in this case Theta.",
                    "label": 0
                },
                {
                    "sent": "This is going to be binary vector.",
                    "label": 0
                },
                {
                    "sent": "Where one components equal to 1, the other components each equal to zero.",
                    "label": 0
                },
                {
                    "sent": "We just don't know which components equal to 1.",
                    "label": 0
                },
                {
                    "sent": "And let's say our prior assigns equal probability to each component.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And let's say our actions allow us to query averages over subsets of the components, so you can choose an action that says give me the average overall components.",
                    "label": 0
                },
                {
                    "sent": "You can choose an action that says give me an average over the first half of the components, or any of the above.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "UCB and Thompson sampling require order D. Samples to identify which component of Theta is equal to 1.",
                    "label": 1
                },
                {
                    "sent": "Essentially UCB and Thompson sampling rule out one component at a time.",
                    "label": 1
                },
                {
                    "sent": "And they do this because in a sense, each of these heuristics hypothesizes that one of the components is the one and then sort of tries to figure out whether that hypothesis is correct or not, and if it's not corrected, rules that out.",
                    "label": 0
                },
                {
                    "sent": "This ruling out one component at a time.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, for this problem you can obviously.",
                    "label": 0
                },
                {
                    "sent": "Figure out which component is 1 in log D time.",
                    "label": 0
                },
                {
                    "sent": "Through, for example, binary search.",
                    "label": 0
                },
                {
                    "sent": "OK, so log D versus D is a huge difference, and UCB and Thompson sampling are clearly doing the wrong thing here.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another example is.",
                    "label": 0
                },
                {
                    "sent": "Assortment optimization So suppose we're dealing with a problem where there are different customer types in the world.",
                    "label": 1
                },
                {
                    "sent": "And we have.",
                    "label": 0
                },
                {
                    "sent": "Many products.",
                    "label": 0
                },
                {
                    "sent": "Each geared to a particular customer type.",
                    "label": 0
                },
                {
                    "sent": "So the blue product here are designed for blue customers, the green for green, Brown for Brown.",
                    "label": 0
                },
                {
                    "sent": "OK and then.",
                    "label": 0
                },
                {
                    "sent": "Suppose we.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are faced by customer of unknown type.",
                    "label": 1
                },
                {
                    "sent": "What we want to do is through repeated interactions.",
                    "label": 0
                },
                {
                    "sent": "Figure out what type is so that we can best serve him.",
                    "label": 0
                },
                {
                    "sent": "Right and then these repeated interactions we get to present him with.",
                    "label": 0
                },
                {
                    "sent": "Assortments of products.",
                    "label": 0
                },
                {
                    "sent": "Let's say we get to present three products at a time, and then we see if he chooses one or.",
                    "label": 0
                },
                {
                    "sent": "Just doesn't choose any.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in each time step we.",
                    "label": 0
                },
                {
                    "sent": "Show MM products.",
                    "label": 0
                },
                {
                    "sent": "An through sequence of interactions where either selects one product or passes.",
                    "label": 1
                },
                {
                    "sent": "We learned about him and do a better job of presenting with him in the product with him.",
                    "label": 0
                },
                {
                    "sent": "With the right products in the future.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out that UCB and Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "In each engagement.",
                    "label": 0
                },
                {
                    "sent": "We'll present 11 type of product.",
                    "label": 0
                },
                {
                    "sent": "So in each engagement it'll sort of guess.",
                    "label": 0
                },
                {
                    "sent": "It'll say, hey, let's guess that this is a blue customer.",
                    "label": 0
                },
                {
                    "sent": "If it's a blue customer, we should present all blue products and let's see what happens.",
                    "label": 0
                },
                {
                    "sent": "OK, so in each engagement you present one color.",
                    "label": 0
                },
                {
                    "sent": "And that turns out to be the wrong way to go.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because you can show easily that by diversifying your portfolio of products by diversifying your assortments.",
                    "label": 0
                },
                {
                    "sent": "You can accelerate learning by a factor of M. OK, Sam could be huge.",
                    "label": 1
                },
                {
                    "sent": "It's a clearly again in this case.",
                    "label": 0
                },
                {
                    "sent": "You see in Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "Completely wrong.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "One approach, one way of dealing with this.",
                    "label": 0
                },
                {
                    "sent": "Is through information directed sampling, which the new algorithmic concept we proposed.",
                    "label": 0
                },
                {
                    "sent": "The idea here is to minimize this thing called.",
                    "label": 0
                },
                {
                    "sent": "We call it an information ratio, but what it really is is the ratio between the squared expected regret over the next over the current engagement divided by the mutual information between the action.",
                    "label": 1
                },
                {
                    "sent": "We sorry between the optimal action and the.",
                    "label": 0
                },
                {
                    "sent": "Outcome we may observe given the action we choose, and I'm not going to spend much time on this, but it's sort of a new single.",
                    "label": 0
                },
                {
                    "sent": "Objective that you could apply.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These problems.",
                    "label": 0
                },
                {
                    "sent": "Turns out that optimizing this kind of objective kills UCB and Thompson sampling in the aforementioned examples, where these algorithms don't work well.",
                    "label": 0
                },
                {
                    "sent": "We're quite surprised that you actually also get a slight improvement in the cases where UCB and Thompson sampling or near optimal.",
                    "label": 1
                },
                {
                    "sent": "OK, so it seems to work better across the board.",
                    "label": 1
                },
                {
                    "sent": "And you know, we have strong and very general regret regret bounds for this kind of heuristic.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, few caveats.",
                    "label": 0
                },
                {
                    "sent": "One is that this kind of algorithm touristic is tractable in some sense, but.",
                    "label": 0
                },
                {
                    "sent": "Not practical, requires too much computation, and so I think there's a lot of.",
                    "label": 0
                },
                {
                    "sent": "Algorithmic work to be done here.",
                    "label": 0
                },
                {
                    "sent": "If one wants to apply this kind of method across a broad variety of problems.",
                    "label": 0
                },
                {
                    "sent": "Second is 1 might ask whether what's used here is the right measure of information, where there is the right way to trade off between information and regret.",
                    "label": 0
                },
                {
                    "sent": "And I don't know.",
                    "label": 0
                },
                {
                    "sent": "It solves the problems we've looked at, but maybe there's a better way of doing things, but.",
                    "label": 0
                },
                {
                    "sent": "But that's something to be sorted out.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright anyways, if you're interested in information directed sampling, there's a paper.",
                    "label": 0
                },
                {
                    "sent": "That we wrote on this topic.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let me move on to reinforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the difference in reinforce.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning is that you can have delayed consequences and that really complicates things.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in particular, to do effective reinforcement learning, you may need to do.",
                    "label": 0
                },
                {
                    "sent": "What my group has come to call deep exploration.",
                    "label": 1
                },
                {
                    "sent": "Deep exploration.",
                    "label": 0
                },
                {
                    "sent": "What is deep exploration?",
                    "label": 0
                },
                {
                    "sent": "Well, simplest to understand through a decision tree.",
                    "label": 0
                },
                {
                    "sent": "So, so here's the decision tree.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have a problem with the actions, or 01 and rewards, or zero or one.",
                    "label": 0
                },
                {
                    "sent": "Well, you take an action 01 and then suppose you take action Zero.",
                    "label": 0
                },
                {
                    "sent": "You might realize rewards error one and then you work your way down the tree.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other actions and other rewards.",
                    "label": 0
                },
                {
                    "sent": "Well, suppose you're in a situation where.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "You look down this decision tree, you know you're over here and there's one branch down here.",
                    "label": 0
                },
                {
                    "sent": "Where unsern.",
                    "label": 0
                },
                {
                    "sent": "About what's going to happen in the future.",
                    "label": 0
                },
                {
                    "sent": "But you know about everything else in this decision tree.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Deep exploration is where you take that in account and you figure, gosh, I want to make a beeline to learn about this red part of the tree.",
                    "label": 1
                },
                {
                    "sent": "OK, it's.",
                    "label": 0
                },
                {
                    "sent": "It's insufficient.",
                    "label": 0
                },
                {
                    "sent": "To be myopic and just look one step ahead.",
                    "label": 0
                },
                {
                    "sent": "Right, and it's insufficient to dither you really gotta look ahead.",
                    "label": 0
                },
                {
                    "sent": "Multiple steps.",
                    "label": 0
                },
                {
                    "sent": "Figure out where you want to get to to learn something you don't know.",
                    "label": 0
                },
                {
                    "sent": "That's deep exploration.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ideas to invest now to learn something downstream and this only.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Becomes important when delayed consequences matter.",
                    "label": 0
                },
                {
                    "sent": "Now, deep exploration of the new term, but the concept really isn't.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "There's this whole literature that I think of as being about deep exploration.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really got started by current sensing.",
                    "label": 0
                },
                {
                    "sent": "Where one interpretation of this paper they wrote.",
                    "label": 0
                },
                {
                    "sent": "Is that deep exploration enables polynomial time reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "If you don't do deep exploration, you end up with exponential time reinforcement learning in the worst case.",
                    "label": 0
                },
                {
                    "sent": "You do deep exploration.",
                    "label": 0
                },
                {
                    "sent": "You can achieve polynomial time OK, and that paper really.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He started a. Trajectory of research where people develop better and better algorithms and their understanding and regret bounds.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning algorithms that do deep exploration.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, the focus of this literature has been on the tabular rasa case case of tabular rasa MDP's, where you know the time you need to learn scales with the number of states in the number of actions and all that.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for this reason, some people find this line of work practically useless.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because realistic problems require generalization, you can't.",
                    "label": 0
                },
                {
                    "sent": "Address realistic problems using a type of larossa framework.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Turns out that realistic problems also sometimes.",
                    "label": 0
                },
                {
                    "sent": "Require deep exploration, but that's not always the case.",
                    "label": 0
                },
                {
                    "sent": "There are problems where you do enough exploration naturally anyways, so that even though you don't do deep exploration, you do well, but there are very few problems where you don't have to do generalization, right?",
                    "label": 0
                },
                {
                    "sent": "So it seems like generalization is the most important thing, and there's a bunch of problems where you also need to do deep exploration.",
                    "label": 0
                },
                {
                    "sent": "OK, so because because of this, deep exploration hasn't.",
                    "label": 0
                },
                {
                    "sent": "Really caught on in a way that's practically useful.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An innocence, you know, this may be a oversimplify.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Asian butt?",
                    "label": 0
                },
                {
                    "sent": "I see two cultures in reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "That's separated partly by language, but.",
                    "label": 0
                },
                {
                    "sent": "But here's the culture that really focuses on deep exploration and tabular rasa learning.",
                    "label": 0
                },
                {
                    "sent": "And here the guys that actually try to get things working.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I think a really important direction for the field is to sort of merge these.",
                    "label": 0
                },
                {
                    "sent": "And to design practical reinforcement, learning algorithms that combine deep exploration and generalization.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All right, there has been some work on this over the past few years.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for example, there is a.",
                    "label": 0
                },
                {
                    "sent": "Bunch of work on model based approaches that generalize and explore one issue with this strand of literature is that the methods are quite specialized to specific classes of models.",
                    "label": 0
                },
                {
                    "sent": "And also, they're generally computationally intractable.",
                    "label": 1
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then there's this recent paper which is quite nice by posits and par.",
                    "label": 0
                },
                {
                    "sent": "Which does deep exploration with interpolative value function generalization?",
                    "label": 1
                },
                {
                    "sent": "And so interpolative value function generalization is nice.",
                    "label": 0
                },
                {
                    "sent": "Because projections when you interpolate.",
                    "label": 0
                },
                {
                    "sent": "Our maximum norm, non expensive and somehow that makes it easier to work with dynamic programming and so everything works out nicely.",
                    "label": 0
                },
                {
                    "sent": "So this is a very nice piece of work, but for practical algorithms I think it's also important to extrapolate.",
                    "label": 0
                },
                {
                    "sent": "So imagine, for example, that your state space consists of binary vectors.",
                    "label": 0
                },
                {
                    "sent": "In other words, how much time do I have?",
                    "label": 0
                },
                {
                    "sent": "Done already, I think we started late, didn't we?",
                    "label": 0
                },
                {
                    "sent": "But I think we started late.",
                    "label": 0
                },
                {
                    "sent": "OK good OK. Well anyways let me.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Faster.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I want to talk about is new approach.",
                    "label": 0
                },
                {
                    "sent": "To tell this called value function randomized.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't have much time to talk.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gosh, how did I run out of time?",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me join.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the punch line.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea here.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To use something like Thompson sampling to sample among plausibel value functions.",
                    "label": 1
                },
                {
                    "sent": "OK, and we're going to talk about with an algorithm that's very similar to least squares value iteration LS VI.",
                    "label": 0
                },
                {
                    "sent": "Except it randomizes in the process of working your way backwards.",
                    "label": 0
                },
                {
                    "sent": "Doing value iterations.",
                    "label": 0
                },
                {
                    "sent": "Can it randomizes in a way that achieves efficient exploration.",
                    "label": 0
                },
                {
                    "sent": "OK, another sanity check.",
                    "label": 0
                },
                {
                    "sent": "To all this.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've done analysis for tabular rasa learning.",
                    "label": 0
                },
                {
                    "sent": "And in this analysis, what we do is, we assume a prior.",
                    "label": 0
                },
                {
                    "sent": "Over over title rossem DPS.",
                    "label": 0
                },
                {
                    "sent": "And then we look at expected regret for this algorithm given that prior.",
                    "label": 0
                },
                {
                    "sent": "And you get a bound.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "Sort of interesting actually, 'cause it bound.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better than the bound that you get using analysis in literature that for UCR L2 and all these other algorithms.",
                    "label": 0
                },
                {
                    "sent": "But we're still digesting all this and making sure this doesn't isn't because of prior.",
                    "label": 0
                },
                {
                    "sent": "We use and all that.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But anyway, so one one punch line here is that RLS VI is doing deep algorithm I mentioned is doing.",
                    "label": 0
                },
                {
                    "sent": "Deep exploration and it's because you can see that because you have to be doing deep exploration to get a polynomial bound here.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regret analysis for something like this with generalization remains an open issue.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a quick example.",
                    "label": 0
                },
                {
                    "sent": "It's easy to find to design problems.",
                    "label": 0
                },
                {
                    "sent": "Where when you do random value function randomization, you very quickly even with.",
                    "label": 0
                },
                {
                    "sent": "Even with value function approximation, when you do, when you do this kind of randomization, you very quickly get optimal to optimal policy, whereas with something like Boltzmann exploration.",
                    "label": 0
                },
                {
                    "sent": "It looks like you're searching for a needle in a haystack, and here's an example of a problem where the optimal policy gives you reward of 1 per timestep.",
                    "label": 0
                },
                {
                    "sent": "And this algorithm are less VI the randomized thing.",
                    "label": 0
                },
                {
                    "sent": "Gives you a reward of 1 per timestep, starting after about 150 timesteps, Boltzmann takes longer.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in fact, if you.",
                    "label": 0
                },
                {
                    "sent": "Expand The X axis out to a billion.",
                    "label": 0
                },
                {
                    "sent": "In a Boltzmann is still going looks like Bolton is searching for a needle in a haystack.",
                    "label": 0
                },
                {
                    "sent": "But something like Arlis VI gets you the answer very quickly.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jump to the end here.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "One question is.",
                    "label": 0
                },
                {
                    "sent": "Have interesting is how could you apply value function randomization.",
                    "label": 0
                },
                {
                    "sent": "With nonlinear parameterisations like a neural network.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ocular how do you combine deep exploration with deep learning?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that there are ways to do this.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One good approach is to use something like the bootstrap.",
                    "label": 0
                },
                {
                    "sent": "Bootstrap is an approach to.",
                    "label": 0
                },
                {
                    "sent": "Approximating the distribution of parameters when you don't want to construct that distribution explicitly, but you have data sampled from it.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in a sense, this is equivalent to doing something like a scattered version of experience replay.",
                    "label": 0
                },
                {
                    "sent": "And by doing this you can apply.",
                    "label": 0
                },
                {
                    "sent": "An appropriate version of value function randomization.",
                    "label": 1
                },
                {
                    "sent": "So how does what you call deep exploration compare with simple optimistic initialization 'cause we know there are domains where you can just act Gridley if you have optimistic or optimistic values and that will take care of all the exploration.",
                    "label": 1
                },
                {
                    "sent": "So how does deep exploration?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "I mean optimistic approach, you have to be more careful.",
                    "label": 0
                },
                {
                    "sent": "It's not just a matter of initialization.",
                    "label": 0
                },
                {
                    "sent": "But you can do something like UCL 2 which is essentially optimistic and it makes sure that you don't become pessimistic.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So there's two things.",
                    "label": 0
                },
                {
                    "sent": "One is something like ECL 2 does not extend well to the case where you want to generalize.",
                    "label": 0
                },
                {
                    "sent": "Right, so the key here is we want a way to do deep exploration.",
                    "label": 0
                },
                {
                    "sent": "That works together with broad ways of doing generalization across the state space.",
                    "label": 0
                },
                {
                    "sent": "So that's one thing the other is, as I mentioned, you may actually save a square root of X Factor in the regret by doing something like.",
                    "label": 0
                },
                {
                    "sent": "What I talked about and I can.",
                    "label": 0
                },
                {
                    "sent": "Give some more intuition for why that might be the case, but it will take awhile.",
                    "label": 0
                },
                {
                    "sent": "Hi, thanks for the great talk.",
                    "label": 0
                },
                {
                    "sent": "Trying to get a bit more intuition into how you actually do the randomization of the value function.",
                    "label": 0
                },
                {
                    "sent": "So for example it feels like 1 issue might be trying to estimate these sort of unknown unknowns.",
                    "label": 0
                },
                {
                    "sent": "So if you have some kind of randomization over the value function, it feels like you have to have at least visited the state space enough to have computed the uncertainty in your value function to even drive you towards those unknown states.",
                    "label": 0
                },
                {
                    "sent": "But perhaps you might have some room that you just never visited before at all and so even estimating the fact that you are uncertain about that room.",
                    "label": 0
                },
                {
                    "sent": "In order to drive you towards it becomes very problematic, so I was just wondering if you've got any thoughts on how to deal with these issues.",
                    "label": 0
                },
                {
                    "sent": "So are you thinking?",
                    "label": 0
                },
                {
                    "sent": "What does your question apply even in the top of Dresser case?",
                    "label": 0
                },
                {
                    "sent": "I think it's about generalization.",
                    "label": 0
                },
                {
                    "sent": "So the question is if you have some.",
                    "label": 0
                },
                {
                    "sent": "So the question is.",
                    "label": 0
                },
                {
                    "sent": "How can you even estimate the uncertainty of your value function sufficiently well to drive you towards exploring parts of the state space which you know nothing about, which is something which the myopic approach is naive as they are occasionally will take you to some completely unknown part of the space, and I guess the question is how do you ensure that you even find out enough about these unknown unknowns in order to to drive you in that direction?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so.",
                    "label": 0
                },
                {
                    "sent": "I know there are aspects of this that I don't understand perfectly yet, but in the case of.",
                    "label": 0
                },
                {
                    "sent": "Suppose your representation can perfectly capture the true value function.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Then in your parameters, right, there's always going to be some uncertainty.",
                    "label": 0
                },
                {
                    "sent": "In dimensions that.",
                    "label": 0
                },
                {
                    "sent": "Are related to the part of the state space you haven't explored yet.",
                    "label": 0
                },
                {
                    "sent": "Right, if your generalization of that part of this database is incorrect.",
                    "label": 0
                },
                {
                    "sent": "So this is that that randomness will lead you there.",
                    "label": 0
                },
                {
                    "sent": "No, things had more complicated.",
                    "label": 0
                },
                {
                    "sent": "When?",
                    "label": 0
                },
                {
                    "sent": "Your representation cannot perfectly capture.",
                    "label": 0
                },
                {
                    "sent": "The value function.",
                    "label": 0
                },
                {
                    "sent": "Then you can be thrown off.",
                    "label": 0
                },
                {
                    "sent": "And there I suspect you can't get strong guarantees, but I think that's something really interesting to study.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}