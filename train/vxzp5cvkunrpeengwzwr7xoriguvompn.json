{
    "id": "vxzp5cvkunrpeengwzwr7xoriguvompn",
    "title": "Query-Level Stability and Generalization in Learning to Rank",
    "info": {
        "author": [
            "Tie-Yan Liu, Microsoft Research"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Information Retrieval"
        ]
    },
    "url": "http://videolectures.net/icml08_liu_qls/",
    "segmentation": [
        [
            "OK.",
            "Thank you very much.",
            "So I'm getting with the session chair because I will present the papers.",
            "We can save a lot of time of exchanging machines, so you have more time to ask me questions.",
            "If you like, you like.",
            "So this is the title of this paper is query level stability and generalization in learning to rank.",
            "This is also a Co work with my student.",
            "Yeah in line and some of my colleagues.",
            "And once again, this paper is concerned with learning to rank because they have already introduced the background of."
        ],
        [
            "I will skip this slide.",
            "OK.",
            "I also maybe I would like to."
        ],
        [
            "Give another slide.",
            "But here I want to, I want you to really pay attention that many different kind of approaches have been used and basically regression classification or linear regression preference, learning and the so-called listwise ranking help all be used to solve the problem of learning to rank.",
            "So as you can see, there are many algorithms proposed in the literature.",
            "But I have to say most of the algorithms actually pay more attention to the design and the empirical verification of the method.",
            "But the corresponding circular theoretical theoretical analysis, for example, the generalization ability analysis is not sufficient.",
            "But as everybody knows.",
            "It is quite meaningful to understand this regular property of algorithm before we really use it in."
        ],
        [
            "So here is the question, how about the generalization ability of all these learning to rank algorithms?",
            "When we argue that since the pointwise approach and pairwise approach are based on some how convenient conventional machine learning technologies such as regression and classification, so there are a rich pool of theories that can be used to analyze their generalization ability.",
            "But I I want to point out that for some other approach, for example, this right approach to learning to rank, which is a little bit different from the regression and classification this theoretical support is not sufficient.",
            "So maybe some new theory need to be defined to really analyze their generalization ability.",
            "And.",
            "Furthermore, you know, even for the pointwise and pairwise approach.",
            "We still have some question about their generalizability.",
            "Because in these approaches, if we regard learning to rank as a problem of classification or regression, what we have seen as a basic learning unit is a document or document pair.",
            "It is not.",
            "Is not clear whether such kind of analysis is really meaningful for real applications, I will."
        ],
        [
            "Show more about this issue in this slide.",
            "Here I take information retrieval as an example.",
            "As we can see, information retrieval query plays a very important role in defining all the problem.",
            "For example, the widely used evaluation measures such as map and ECG.",
            "All of them are defined at the query level.",
            "That is, you can see from the definition of NDC G at position N, 1st of all, we will compute measure for the query Q.",
            "This measure we call, also called N DCG, is bounded between zero and one.",
            "And then we will.",
            "Compute the average of the measure over all the queries.",
            "So there is a.",
            "There are two step, one is to compute a matter for each particular query and then they will do an average of all the queries.",
            "So in this.",
            "Scenario, we can see that the query level error makes more sense to the users because in the air task, each of the task is really identified by a query submitted by the user.",
            "User will never see a document pair a single document.",
            "How much error you have made their what they care about is the success of the entire IR task, which is featured by the query.",
            "So the conclusion is evaluation.",
            "IR is usually conducted at query level, so.",
            "In real applications, like really don't doesn't care the errors at the document or document paradigm.",
            "And based on."
        ],
        [
            "This observation.",
            "It makes sense that we also conduct generalization ability at the query level, but not as a document or docking pair level.",
            "So this is a.",
            "Basic idea of this paper.",
            "So in this sense, existing works on the generalizability of learning Frank cannot answer the entire problem.",
            "They can only provide partial answer to the problem, so we think new theory needs to be developed to really count the query level generalization analysis."
        ],
        [
            "So this is outline of the paper and you can see from the outline how we really make this task done step by step.",
            "First of all, we propose a two layer probabilistic framework for ranking and this framework very explicitly introduced the query.",
            "The concept of query into the definition of ranking.",
            "And then we will give the definition of query level loss and character level risk.",
            "And based on this definition, we will propose a theory which we call a query level stability theory which can give a bound acquirable bound to any learning strong algorithm.",
            "At last we will use experimental results to really validate whether the proposed theoretical Funding Circle result is is correct."
        ],
        [
            "OK, so first of all, let's have a look at the two layer probability ranking model.",
            "So in our model we have two layers.",
            "As I mentioned, the first layer is about query.",
            "So here Q stands for query.",
            "We view every query as a random variable sampled from a query space Q.",
            "And this sampling is according to a known probability distribution PQ.",
            "So in other words, we assume the IID distribution among queries and also the ID distribution among all their tasks featured by the queries.",
            "And then for each query we have.",
            "Is associated WQ and the corresponding ground truth label.",
            "And we also viewed associate and the ground Truth label as a random variable sampled from this product space according to an unknown probability distribution, DQ."
        ],
        [
            "And for different approach.",
            "The associate may represent different things.",
            "For example, for the pointwise approach document is associated and the corresponding ground truth is relevant for a class label.",
            "And for the pairwise approach document there is a so called associate and the ground truth.",
            "Is there partial order?",
            "And for the list wise approach, a set of document is the associate and corresponding ground.",
            "Truth is a permutation of the objects.",
            "So these are how we defined associate and how to really make the two layer probability model more meaningful."
        ],
        [
            "OK, so with that earlier model in mind, let's see what the training data looks like.",
            "Actually we have a training set.",
            "There are queries in it and it S actually their associates and corresponding ground truth level.",
            "OK.",
            "So based on our assumption, the queries are IID distributed.",
            "And all the associated on their ground truth are also added distributed.",
            "And based on the."
        ],
        [
            "Definition we can easily define the query level loss and character level risks accordingly.",
            "The query level loss is actually defined for each query and you can see we take expectations or Irish or other sources with the query.",
            "This is a curable loss.",
            "And if we further take expectation or average over all the queries, we will get the queryable risk.",
            "So then the."
        ],
        [
            "Going to run task is actually to minimize the current level risk.",
            "The expected query level risk.",
            "But in practice, as you know, the distribution is unknown.",
            "So when you really minimize the empirical risk.",
            "But this is also the queryable risk, as we defined in the previous slide.",
            "Then it is interesting to know about the difference between this expected risk and empirical risk.",
            "So this is just what the term.",
            "Query level generates inbound, actually quantifies.",
            "So this up to here we have introduced the whole story.",
            "What is the query level risk that earlier model and what is the true meaning of acquired level generalization bound?"
        ],
        [
            "Then for the next step we will try to give a theory which can really give acquired level bound.",
            "The theory is based on the new concept named query level stability.",
            "And I think some of you may be familiar with the stability, and here's a new concept is parallel stability, which actually represents a degree of change in the loss of prediction when we randomly remove one query from the training set.",
            "OK, so the key point is here we really regard the query and all is associated as a learning instance.",
            "So if we want to remove instance from the training set, we need to remove the query and always associated.",
            "So this is a mathematical definition of this query level stability.",
            "And this is a.",
            "Ranking function trained using the original training set and this is a new ranking function with a trained using the set where we removed one query from it.",
            "And these are the associated and ground truth.",
            "For any testing query.",
            "And then this tower is a so called squirrel stability coefficient.",
            "So this is just a definition for a particular algorithm.",
            "We can really conduct some deductions and get its corresponding stability coefficient, so this is a common common practice in using stability to analyze the general."
        ],
        [
            "Inbound.",
            "OK, so based on the concept of query level stability we can further give a theory which quantifies the difference between the.",
            "Empirical query level risk and expected query level risk.",
            "So this is the corresponding bound.",
            "And from this bond we can see that the bond here is related to the number of training.",
            "Queries are only but not related to the number of documents associated with each query.",
            "And also it is related to the.",
            "The ability coefficient are.",
            "So if one the number of training query approaches Infinity, is the tower, the stability coefficient can be?",
            "There is more actually approaches 0.",
            "Then maybe we can know that this generation bond will converge to 0, or in other words, this generalization bound will be reasonable.",
            "Then for particular learning to rank algorithm, we would really check whether this condition satisfied when the number of training queries increase, whether the stability coefficient decrease accordingly."
        ],
        [
            "OK, so now we will take 2 examples.",
            "Actually they are very popular ranking algorithm in the literature where we use them as examples to demonstrate how we can apply the query level stability theory to conduct general generalization ability analysis.",
            "The first algorithm is ranking SVM and the other one is IRS form.",
            "Oil filter definition."
        ],
        [
            "Slide.",
            "So it is clear ranking as some users about support vector machines technology to minimize the pairwise hinge loss.",
            "And I swear I'm actually introduced.",
            "Query we normalize it here and I is a number of a document pairs for the S query.",
            "OK, so there is a query level normalization and by using this normalization we can expect that the loss of each query will become very similar to each other because they have been normalized.",
            "So then we will use the query level stability theory to analyze these two algorithms."
        ],
        [
            "OK, so after some deductions we can get that this stability coefficient for ranking aspect is like this.",
            "OK.",
            "So the Lambda is the.",
            "Efficient coefficient here the regularization parameter.",
            "And the new and Sigma are the mean and variance of the number of document pairs per query.",
            "If we regard that number as a random variable from the training set, then we can compute mu and Sigma.",
            "And from this results we can see clearly.",
            "This term will be larger than one, especially when the variance is very large.",
            "So this is very common.",
            "We have seen this result in many previous papers.",
            "If we count the number of document pairs for query in any benchmark data set, this number of pair will actually follow a power law distribution.",
            "So usually you know the variance will be quite large.",
            "And the result is that the stability coefficient of ranking SM will be much larger than I expect.",
            "Now if we substitute this stability coefficient."
        ],
        [
            "Into the theory.",
            "We will get these two point 2 pounds.",
            "And once again, it is very easy to verify this bound for ranking as well is much larger than the one for ASM.",
            "So based on these formulas, we can have some further discussions on."
        ],
        [
            "Generalizing bound, which I showed them in this slide.",
            "So the first one is that when the number of training queries are tends to Infinity, actually the the generalization bound of."
        ],
        [
            "I I swam this one."
        ],
        [
            "Will approaches zero and the convergence rate."
        ],
        [
            "It is in the order of 1 / R square root, so this is a result."
        ],
        [
            "And for the ranking SVM, when the number of training query tends to be Infinity, the bond will become a constant.",
            "It is quite interesting."
        ],
        [
            "So this bond will eventually when our approach is in this Infinity.",
            "This bond will."
        ],
        [
            "Become a constant which is not a number of the number of training queries.",
            "So at first glance this will be quite quite difficult to understand, but actually, if we really think about the problem.",
            "You know?",
            "In the learning process of ranking as well, there is no clear concept of query, so they really optimize for pairwise classification error.",
            "This is not necessarily what we care about in choir level evaluation.",
            "So when the ranking SM optimized towards direction a, but we evaluate it at a direction B, then there will be obviously a gap between these two objectives, so this is not difficult to understand.",
            "Why the bond will not be 0?",
            "In other words, the training process will not converge to the query level risks."
        ],
        [
            "So, OK, that's all the theoretical results, and we have also conducted some experiments to validate these theoretical findings.",
            "So for this purpose we have collected more than 1000 queries.",
            "And when construct training set a validation set of test set.",
            "So this first experiment will validate these query level stability of the two algorithms.",
            "First of all we will train 2 models using both ranking as well and I swam respectively and we denote their model as F0F zero prime.",
            "Then we will randomly remove a query from the training set.",
            "Then we will retrain ranking as well and I will get 2 new models.",
            "If we do this for multiple times, we will get multiple F&F prime.",
            "Then this data actually specifies the change of the prediction loss when we really remove one query from the training set.",
            "So we actually did."
        ],
        [
            "430 times and we can get this table.",
            "And from the table you can see."
        ],
        [
            "These are."
        ],
        [
            "This data I is the.",
            "Change of loss prediction for ranking as well and this third eye prime is for as well and we can clearly see this.",
            "Third I prime is always much smaller than third I so this shows that I am is more stable than ranking SVM at the query level."
        ],
        [
            "So the second experiment is conducted on the real data set and sorry on the same data set and the evaluation measure we use is on DCG and precision on the test set.",
            "So usually the performance on the test set can partially reflect the generation ability.",
            "So from this figure we can see clearly the performance of RSM is much better than ranking as well, so partially reflecting that the generalization ability of it is better than that of ranking as well."
        ],
        [
            "OK, so here is the conclusion.",
            "So in this paper we have argued that the generalization ability.",
            "Analysis For learning to rank algorithm should be conducted at the query level but not document or document their level.",
            "Then for this purpose we have proposed to layer probabilistic framework for the ranking problem and we have given the concept of a kernel loss parallel risk quarter stability and then we will propose quarrel stability theory to close the loop where you can use it too.",
            "Analyze the generation bound of each learning to rank algorithms."
        ],
        [
            "This is the future work.",
            "As you can see in this presentation, we just use a support vector machine based methods such as ranking SM ASM to perform the analysis and the experimental verification.",
            "So it will make sense if we can really try other algorithms such as those algorithms based on boosting and other technologies.",
            "And the second one is that you know you may have already mentioned that the both ranking as well and analyze them can be categorized as a pairwise approach.",
            "So they basically deal with the document pairs to perform the training.",
            "Then it is interesting whether we can extend the case study to other kinds of approaches.",
            "For example, the pointwise approach and this was approach.",
            "So this these things will be my future work.",
            "Also, in our mind to really propose a generalization, theory is now the eventual goal we want to use this generalization ability analysis process to guide us to select better algorithms, so we will also try to see whether we can derive some new algorithms with the help of the whole theory we proposed today."
        ],
        [
            "OK, thank you very much and before my clothes I close my talk.",
            "I would like to thank Lily Wang Professor in Peking University for his very helpful discussions with us.",
            "And also this a little advertisement.",
            "You know we will have a tutorial and a workshop on this learning topic at cigar this year.",
            "So if you really have interest in them.",
            "You are welcome to join us.",
            "OK, thank you very much.",
            "Yeah.",
            "Qualification.",
            "Valuation.",
            "You mean?",
            "Sorry man, is experiments or OK?"
        ],
        [
            "True.",
            "Angel.",
            "Series yes.",
            "What do you have to clear?",
            "OK, so in this case actually when we did the training set, the details about the training set is like this.",
            "So we have a large query log and we sampled.",
            "Here 1000 query.",
            "Sorry I cannot tell you which the the particular query is but we have them.",
            "Yes, I I won't mention it later with this query.",
            "Actually we have asked the human labor to give five level ratings for each of the document.",
            "They could be perfect, excellent, good, fair or bad and based on this kind of ground truth one can construct a lot of pairs.",
            "If the two documents with different levels, they will construct the pair.",
            "OK then, with all these pairs, we will form the training set, the testing, set the validation set.",
            "So this is a basic setting of this experiment.",
            "It seems like the performance gap is decreasing as you increase the truncation level.",
            "I mean, so did you try?"
        ],
        [
            "Larger, larger rank positions to calculate in this city or precision like a 10 or 15, is the gap still decreasing as you increase an?",
            "I think your question is is very interesting enough, because you know if we really increase the position to Infinity.",
            "Maybe, at least for some precision and ECG, the difference will be even not observable because we.",
            "OK.",
            "So we have considered more documents and many of them are irrelevant, but in that case, if you do this admission to the average, the difference will be be really reduced.",
            "But if you look at the top positions, the difference will be larger, but usually you know in our tasks we will pay more attention to the top results.",
            "That's always true.",
            "If you search engine, you will maybe look at the first page.",
            "You only 10 results there, so we will check precision DCDS position one to 10.",
            "So here for the space issue.",
            "I only list one to five and we actually have the result for the.",
            "Up to the position 10.",
            "Yeah, as just a similar to what you have mentioned, the difference will be smaller, but even though the errors realm is better than working in that case.",
            "Thanks to young one more time.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So I'm getting with the session chair because I will present the papers.",
                    "label": 0
                },
                {
                    "sent": "We can save a lot of time of exchanging machines, so you have more time to ask me questions.",
                    "label": 0
                },
                {
                    "sent": "If you like, you like.",
                    "label": 0
                },
                {
                    "sent": "So this is the title of this paper is query level stability and generalization in learning to rank.",
                    "label": 1
                },
                {
                    "sent": "This is also a Co work with my student.",
                    "label": 0
                },
                {
                    "sent": "Yeah in line and some of my colleagues.",
                    "label": 0
                },
                {
                    "sent": "And once again, this paper is concerned with learning to rank because they have already introduced the background of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will skip this slide.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I also maybe I would like to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give another slide.",
                    "label": 0
                },
                {
                    "sent": "But here I want to, I want you to really pay attention that many different kind of approaches have been used and basically regression classification or linear regression preference, learning and the so-called listwise ranking help all be used to solve the problem of learning to rank.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, there are many algorithms proposed in the literature.",
                    "label": 0
                },
                {
                    "sent": "But I have to say most of the algorithms actually pay more attention to the design and the empirical verification of the method.",
                    "label": 0
                },
                {
                    "sent": "But the corresponding circular theoretical theoretical analysis, for example, the generalization ability analysis is not sufficient.",
                    "label": 0
                },
                {
                    "sent": "But as everybody knows.",
                    "label": 0
                },
                {
                    "sent": "It is quite meaningful to understand this regular property of algorithm before we really use it in.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the question, how about the generalization ability of all these learning to rank algorithms?",
                    "label": 1
                },
                {
                    "sent": "When we argue that since the pointwise approach and pairwise approach are based on some how convenient conventional machine learning technologies such as regression and classification, so there are a rich pool of theories that can be used to analyze their generalization ability.",
                    "label": 0
                },
                {
                    "sent": "But I I want to point out that for some other approach, for example, this right approach to learning to rank, which is a little bit different from the regression and classification this theoretical support is not sufficient.",
                    "label": 0
                },
                {
                    "sent": "So maybe some new theory need to be defined to really analyze their generalization ability.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, you know, even for the pointwise and pairwise approach.",
                    "label": 0
                },
                {
                    "sent": "We still have some question about their generalizability.",
                    "label": 0
                },
                {
                    "sent": "Because in these approaches, if we regard learning to rank as a problem of classification or regression, what we have seen as a basic learning unit is a document or document pair.",
                    "label": 0
                },
                {
                    "sent": "It is not.",
                    "label": 0
                },
                {
                    "sent": "Is not clear whether such kind of analysis is really meaningful for real applications, I will.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Show more about this issue in this slide.",
                    "label": 0
                },
                {
                    "sent": "Here I take information retrieval as an example.",
                    "label": 1
                },
                {
                    "sent": "As we can see, information retrieval query plays a very important role in defining all the problem.",
                    "label": 0
                },
                {
                    "sent": "For example, the widely used evaluation measures such as map and ECG.",
                    "label": 1
                },
                {
                    "sent": "All of them are defined at the query level.",
                    "label": 0
                },
                {
                    "sent": "That is, you can see from the definition of NDC G at position N, 1st of all, we will compute measure for the query Q.",
                    "label": 0
                },
                {
                    "sent": "This measure we call, also called N DCG, is bounded between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And then we will.",
                    "label": 0
                },
                {
                    "sent": "Compute the average of the measure over all the queries.",
                    "label": 0
                },
                {
                    "sent": "So there is a.",
                    "label": 0
                },
                {
                    "sent": "There are two step, one is to compute a matter for each particular query and then they will do an average of all the queries.",
                    "label": 0
                },
                {
                    "sent": "So in this.",
                    "label": 1
                },
                {
                    "sent": "Scenario, we can see that the query level error makes more sense to the users because in the air task, each of the task is really identified by a query submitted by the user.",
                    "label": 0
                },
                {
                    "sent": "User will never see a document pair a single document.",
                    "label": 0
                },
                {
                    "sent": "How much error you have made their what they care about is the success of the entire IR task, which is featured by the query.",
                    "label": 1
                },
                {
                    "sent": "So the conclusion is evaluation.",
                    "label": 0
                },
                {
                    "sent": "IR is usually conducted at query level, so.",
                    "label": 0
                },
                {
                    "sent": "In real applications, like really don't doesn't care the errors at the document or document paradigm.",
                    "label": 1
                },
                {
                    "sent": "And based on.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This observation.",
                    "label": 0
                },
                {
                    "sent": "It makes sense that we also conduct generalization ability at the query level, but not as a document or docking pair level.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "Basic idea of this paper.",
                    "label": 0
                },
                {
                    "sent": "So in this sense, existing works on the generalizability of learning Frank cannot answer the entire problem.",
                    "label": 0
                },
                {
                    "sent": "They can only provide partial answer to the problem, so we think new theory needs to be developed to really count the query level generalization analysis.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is outline of the paper and you can see from the outline how we really make this task done step by step.",
                    "label": 0
                },
                {
                    "sent": "First of all, we propose a two layer probabilistic framework for ranking and this framework very explicitly introduced the query.",
                    "label": 1
                },
                {
                    "sent": "The concept of query into the definition of ranking.",
                    "label": 0
                },
                {
                    "sent": "And then we will give the definition of query level loss and character level risk.",
                    "label": 0
                },
                {
                    "sent": "And based on this definition, we will propose a theory which we call a query level stability theory which can give a bound acquirable bound to any learning strong algorithm.",
                    "label": 0
                },
                {
                    "sent": "At last we will use experimental results to really validate whether the proposed theoretical Funding Circle result is is correct.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so first of all, let's have a look at the two layer probability ranking model.",
                    "label": 0
                },
                {
                    "sent": "So in our model we have two layers.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, the first layer is about query.",
                    "label": 0
                },
                {
                    "sent": "So here Q stands for query.",
                    "label": 1
                },
                {
                    "sent": "We view every query as a random variable sampled from a query space Q.",
                    "label": 1
                },
                {
                    "sent": "And this sampling is according to a known probability distribution PQ.",
                    "label": 0
                },
                {
                    "sent": "So in other words, we assume the IID distribution among queries and also the ID distribution among all their tasks featured by the queries.",
                    "label": 0
                },
                {
                    "sent": "And then for each query we have.",
                    "label": 0
                },
                {
                    "sent": "Is associated WQ and the corresponding ground truth label.",
                    "label": 0
                },
                {
                    "sent": "And we also viewed associate and the ground Truth label as a random variable sampled from this product space according to an unknown probability distribution, DQ.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for different approach.",
                    "label": 0
                },
                {
                    "sent": "The associate may represent different things.",
                    "label": 0
                },
                {
                    "sent": "For example, for the pointwise approach document is associated and the corresponding ground truth is relevant for a class label.",
                    "label": 0
                },
                {
                    "sent": "And for the pairwise approach document there is a so called associate and the ground truth.",
                    "label": 0
                },
                {
                    "sent": "Is there partial order?",
                    "label": 0
                },
                {
                    "sent": "And for the list wise approach, a set of document is the associate and corresponding ground.",
                    "label": 0
                },
                {
                    "sent": "Truth is a permutation of the objects.",
                    "label": 0
                },
                {
                    "sent": "So these are how we defined associate and how to really make the two layer probability model more meaningful.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so with that earlier model in mind, let's see what the training data looks like.",
                    "label": 1
                },
                {
                    "sent": "Actually we have a training set.",
                    "label": 0
                },
                {
                    "sent": "There are queries in it and it S actually their associates and corresponding ground truth level.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So based on our assumption, the queries are IID distributed.",
                    "label": 0
                },
                {
                    "sent": "And all the associated on their ground truth are also added distributed.",
                    "label": 0
                },
                {
                    "sent": "And based on the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Definition we can easily define the query level loss and character level risks accordingly.",
                    "label": 1
                },
                {
                    "sent": "The query level loss is actually defined for each query and you can see we take expectations or Irish or other sources with the query.",
                    "label": 0
                },
                {
                    "sent": "This is a curable loss.",
                    "label": 0
                },
                {
                    "sent": "And if we further take expectation or average over all the queries, we will get the queryable risk.",
                    "label": 0
                },
                {
                    "sent": "So then the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to run task is actually to minimize the current level risk.",
                    "label": 0
                },
                {
                    "sent": "The expected query level risk.",
                    "label": 1
                },
                {
                    "sent": "But in practice, as you know, the distribution is unknown.",
                    "label": 1
                },
                {
                    "sent": "So when you really minimize the empirical risk.",
                    "label": 0
                },
                {
                    "sent": "But this is also the queryable risk, as we defined in the previous slide.",
                    "label": 1
                },
                {
                    "sent": "Then it is interesting to know about the difference between this expected risk and empirical risk.",
                    "label": 0
                },
                {
                    "sent": "So this is just what the term.",
                    "label": 0
                },
                {
                    "sent": "Query level generates inbound, actually quantifies.",
                    "label": 0
                },
                {
                    "sent": "So this up to here we have introduced the whole story.",
                    "label": 0
                },
                {
                    "sent": "What is the query level risk that earlier model and what is the true meaning of acquired level generalization bound?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then for the next step we will try to give a theory which can really give acquired level bound.",
                    "label": 0
                },
                {
                    "sent": "The theory is based on the new concept named query level stability.",
                    "label": 0
                },
                {
                    "sent": "And I think some of you may be familiar with the stability, and here's a new concept is parallel stability, which actually represents a degree of change in the loss of prediction when we randomly remove one query from the training set.",
                    "label": 1
                },
                {
                    "sent": "OK, so the key point is here we really regard the query and all is associated as a learning instance.",
                    "label": 1
                },
                {
                    "sent": "So if we want to remove instance from the training set, we need to remove the query and always associated.",
                    "label": 0
                },
                {
                    "sent": "So this is a mathematical definition of this query level stability.",
                    "label": 0
                },
                {
                    "sent": "And this is a.",
                    "label": 0
                },
                {
                    "sent": "Ranking function trained using the original training set and this is a new ranking function with a trained using the set where we removed one query from it.",
                    "label": 0
                },
                {
                    "sent": "And these are the associated and ground truth.",
                    "label": 0
                },
                {
                    "sent": "For any testing query.",
                    "label": 0
                },
                {
                    "sent": "And then this tower is a so called squirrel stability coefficient.",
                    "label": 0
                },
                {
                    "sent": "So this is just a definition for a particular algorithm.",
                    "label": 0
                },
                {
                    "sent": "We can really conduct some deductions and get its corresponding stability coefficient, so this is a common common practice in using stability to analyze the general.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inbound.",
                    "label": 0
                },
                {
                    "sent": "OK, so based on the concept of query level stability we can further give a theory which quantifies the difference between the.",
                    "label": 0
                },
                {
                    "sent": "Empirical query level risk and expected query level risk.",
                    "label": 0
                },
                {
                    "sent": "So this is the corresponding bound.",
                    "label": 0
                },
                {
                    "sent": "And from this bond we can see that the bond here is related to the number of training.",
                    "label": 1
                },
                {
                    "sent": "Queries are only but not related to the number of documents associated with each query.",
                    "label": 0
                },
                {
                    "sent": "And also it is related to the.",
                    "label": 0
                },
                {
                    "sent": "The ability coefficient are.",
                    "label": 0
                },
                {
                    "sent": "So if one the number of training query approaches Infinity, is the tower, the stability coefficient can be?",
                    "label": 0
                },
                {
                    "sent": "There is more actually approaches 0.",
                    "label": 0
                },
                {
                    "sent": "Then maybe we can know that this generation bond will converge to 0, or in other words, this generalization bound will be reasonable.",
                    "label": 1
                },
                {
                    "sent": "Then for particular learning to rank algorithm, we would really check whether this condition satisfied when the number of training queries increase, whether the stability coefficient decrease accordingly.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we will take 2 examples.",
                    "label": 0
                },
                {
                    "sent": "Actually they are very popular ranking algorithm in the literature where we use them as examples to demonstrate how we can apply the query level stability theory to conduct general generalization ability analysis.",
                    "label": 1
                },
                {
                    "sent": "The first algorithm is ranking SVM and the other one is IRS form.",
                    "label": 0
                },
                {
                    "sent": "Oil filter definition.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slide.",
                    "label": 0
                },
                {
                    "sent": "So it is clear ranking as some users about support vector machines technology to minimize the pairwise hinge loss.",
                    "label": 0
                },
                {
                    "sent": "And I swear I'm actually introduced.",
                    "label": 0
                },
                {
                    "sent": "Query we normalize it here and I is a number of a document pairs for the S query.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is a query level normalization and by using this normalization we can expect that the loss of each query will become very similar to each other because they have been normalized.",
                    "label": 0
                },
                {
                    "sent": "So then we will use the query level stability theory to analyze these two algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so after some deductions we can get that this stability coefficient for ranking aspect is like this.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the Lambda is the.",
                    "label": 0
                },
                {
                    "sent": "Efficient coefficient here the regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "And the new and Sigma are the mean and variance of the number of document pairs per query.",
                    "label": 1
                },
                {
                    "sent": "If we regard that number as a random variable from the training set, then we can compute mu and Sigma.",
                    "label": 0
                },
                {
                    "sent": "And from this results we can see clearly.",
                    "label": 0
                },
                {
                    "sent": "This term will be larger than one, especially when the variance is very large.",
                    "label": 0
                },
                {
                    "sent": "So this is very common.",
                    "label": 0
                },
                {
                    "sent": "We have seen this result in many previous papers.",
                    "label": 0
                },
                {
                    "sent": "If we count the number of document pairs for query in any benchmark data set, this number of pair will actually follow a power law distribution.",
                    "label": 0
                },
                {
                    "sent": "So usually you know the variance will be quite large.",
                    "label": 0
                },
                {
                    "sent": "And the result is that the stability coefficient of ranking SM will be much larger than I expect.",
                    "label": 0
                },
                {
                    "sent": "Now if we substitute this stability coefficient.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into the theory.",
                    "label": 0
                },
                {
                    "sent": "We will get these two point 2 pounds.",
                    "label": 0
                },
                {
                    "sent": "And once again, it is very easy to verify this bound for ranking as well is much larger than the one for ASM.",
                    "label": 0
                },
                {
                    "sent": "So based on these formulas, we can have some further discussions on.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generalizing bound, which I showed them in this slide.",
                    "label": 0
                },
                {
                    "sent": "So the first one is that when the number of training queries are tends to Infinity, actually the the generalization bound of.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I I swam this one.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will approaches zero and the convergence rate.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is in the order of 1 / R square root, so this is a result.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the ranking SVM, when the number of training query tends to be Infinity, the bond will become a constant.",
                    "label": 0
                },
                {
                    "sent": "It is quite interesting.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this bond will eventually when our approach is in this Infinity.",
                    "label": 0
                },
                {
                    "sent": "This bond will.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Become a constant which is not a number of the number of training queries.",
                    "label": 1
                },
                {
                    "sent": "So at first glance this will be quite quite difficult to understand, but actually, if we really think about the problem.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "In the learning process of ranking as well, there is no clear concept of query, so they really optimize for pairwise classification error.",
                    "label": 0
                },
                {
                    "sent": "This is not necessarily what we care about in choir level evaluation.",
                    "label": 0
                },
                {
                    "sent": "So when the ranking SM optimized towards direction a, but we evaluate it at a direction B, then there will be obviously a gap between these two objectives, so this is not difficult to understand.",
                    "label": 0
                },
                {
                    "sent": "Why the bond will not be 0?",
                    "label": 1
                },
                {
                    "sent": "In other words, the training process will not converge to the query level risks.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, OK, that's all the theoretical results, and we have also conducted some experiments to validate these theoretical findings.",
                    "label": 0
                },
                {
                    "sent": "So for this purpose we have collected more than 1000 queries.",
                    "label": 0
                },
                {
                    "sent": "And when construct training set a validation set of test set.",
                    "label": 0
                },
                {
                    "sent": "So this first experiment will validate these query level stability of the two algorithms.",
                    "label": 0
                },
                {
                    "sent": "First of all we will train 2 models using both ranking as well and I swam respectively and we denote their model as F0F zero prime.",
                    "label": 0
                },
                {
                    "sent": "Then we will randomly remove a query from the training set.",
                    "label": 1
                },
                {
                    "sent": "Then we will retrain ranking as well and I will get 2 new models.",
                    "label": 0
                },
                {
                    "sent": "If we do this for multiple times, we will get multiple F&F prime.",
                    "label": 0
                },
                {
                    "sent": "Then this data actually specifies the change of the prediction loss when we really remove one query from the training set.",
                    "label": 0
                },
                {
                    "sent": "So we actually did.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "430 times and we can get this table.",
                    "label": 0
                },
                {
                    "sent": "And from the table you can see.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This data I is the.",
                    "label": 0
                },
                {
                    "sent": "Change of loss prediction for ranking as well and this third eye prime is for as well and we can clearly see this.",
                    "label": 0
                },
                {
                    "sent": "Third I prime is always much smaller than third I so this shows that I am is more stable than ranking SVM at the query level.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the second experiment is conducted on the real data set and sorry on the same data set and the evaluation measure we use is on DCG and precision on the test set.",
                    "label": 0
                },
                {
                    "sent": "So usually the performance on the test set can partially reflect the generation ability.",
                    "label": 0
                },
                {
                    "sent": "So from this figure we can see clearly the performance of RSM is much better than ranking as well, so partially reflecting that the generalization ability of it is better than that of ranking as well.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here is the conclusion.",
                    "label": 0
                },
                {
                    "sent": "So in this paper we have argued that the generalization ability.",
                    "label": 1
                },
                {
                    "sent": "Analysis For learning to rank algorithm should be conducted at the query level but not document or document their level.",
                    "label": 1
                },
                {
                    "sent": "Then for this purpose we have proposed to layer probabilistic framework for the ranking problem and we have given the concept of a kernel loss parallel risk quarter stability and then we will propose quarrel stability theory to close the loop where you can use it too.",
                    "label": 0
                },
                {
                    "sent": "Analyze the generation bound of each learning to rank algorithms.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the future work.",
                    "label": 1
                },
                {
                    "sent": "As you can see in this presentation, we just use a support vector machine based methods such as ranking SM ASM to perform the analysis and the experimental verification.",
                    "label": 1
                },
                {
                    "sent": "So it will make sense if we can really try other algorithms such as those algorithms based on boosting and other technologies.",
                    "label": 1
                },
                {
                    "sent": "And the second one is that you know you may have already mentioned that the both ranking as well and analyze them can be categorized as a pairwise approach.",
                    "label": 0
                },
                {
                    "sent": "So they basically deal with the document pairs to perform the training.",
                    "label": 0
                },
                {
                    "sent": "Then it is interesting whether we can extend the case study to other kinds of approaches.",
                    "label": 1
                },
                {
                    "sent": "For example, the pointwise approach and this was approach.",
                    "label": 0
                },
                {
                    "sent": "So this these things will be my future work.",
                    "label": 0
                },
                {
                    "sent": "Also, in our mind to really propose a generalization, theory is now the eventual goal we want to use this generalization ability analysis process to guide us to select better algorithms, so we will also try to see whether we can derive some new algorithms with the help of the whole theory we proposed today.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, thank you very much and before my clothes I close my talk.",
                    "label": 0
                },
                {
                    "sent": "I would like to thank Lily Wang Professor in Peking University for his very helpful discussions with us.",
                    "label": 0
                },
                {
                    "sent": "And also this a little advertisement.",
                    "label": 0
                },
                {
                    "sent": "You know we will have a tutorial and a workshop on this learning topic at cigar this year.",
                    "label": 1
                },
                {
                    "sent": "So if you really have interest in them.",
                    "label": 0
                },
                {
                    "sent": "You are welcome to join us.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Qualification.",
                    "label": 0
                },
                {
                    "sent": "Valuation.",
                    "label": 0
                },
                {
                    "sent": "You mean?",
                    "label": 0
                },
                {
                    "sent": "Sorry man, is experiments or OK?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "True.",
                    "label": 0
                },
                {
                    "sent": "Angel.",
                    "label": 0
                },
                {
                    "sent": "Series yes.",
                    "label": 0
                },
                {
                    "sent": "What do you have to clear?",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case actually when we did the training set, the details about the training set is like this.",
                    "label": 0
                },
                {
                    "sent": "So we have a large query log and we sampled.",
                    "label": 0
                },
                {
                    "sent": "Here 1000 query.",
                    "label": 0
                },
                {
                    "sent": "Sorry I cannot tell you which the the particular query is but we have them.",
                    "label": 0
                },
                {
                    "sent": "Yes, I I won't mention it later with this query.",
                    "label": 0
                },
                {
                    "sent": "Actually we have asked the human labor to give five level ratings for each of the document.",
                    "label": 0
                },
                {
                    "sent": "They could be perfect, excellent, good, fair or bad and based on this kind of ground truth one can construct a lot of pairs.",
                    "label": 0
                },
                {
                    "sent": "If the two documents with different levels, they will construct the pair.",
                    "label": 0
                },
                {
                    "sent": "OK then, with all these pairs, we will form the training set, the testing, set the validation set.",
                    "label": 0
                },
                {
                    "sent": "So this is a basic setting of this experiment.",
                    "label": 0
                },
                {
                    "sent": "It seems like the performance gap is decreasing as you increase the truncation level.",
                    "label": 0
                },
                {
                    "sent": "I mean, so did you try?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Larger, larger rank positions to calculate in this city or precision like a 10 or 15, is the gap still decreasing as you increase an?",
                    "label": 0
                },
                {
                    "sent": "I think your question is is very interesting enough, because you know if we really increase the position to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Maybe, at least for some precision and ECG, the difference will be even not observable because we.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we have considered more documents and many of them are irrelevant, but in that case, if you do this admission to the average, the difference will be be really reduced.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the top positions, the difference will be larger, but usually you know in our tasks we will pay more attention to the top results.",
                    "label": 0
                },
                {
                    "sent": "That's always true.",
                    "label": 0
                },
                {
                    "sent": "If you search engine, you will maybe look at the first page.",
                    "label": 0
                },
                {
                    "sent": "You only 10 results there, so we will check precision DCDS position one to 10.",
                    "label": 0
                },
                {
                    "sent": "So here for the space issue.",
                    "label": 0
                },
                {
                    "sent": "I only list one to five and we actually have the result for the.",
                    "label": 0
                },
                {
                    "sent": "Up to the position 10.",
                    "label": 0
                },
                {
                    "sent": "Yeah, as just a similar to what you have mentioned, the difference will be smaller, but even though the errors realm is better than working in that case.",
                    "label": 0
                },
                {
                    "sent": "Thanks to young one more time.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}