{
    "id": "4ofrtdn7jsk7wve7xqco5qr4lw75fh77",
    "title": "PLAL: Cluster-based active learning",
    "info": {
        "author": [
            "Ruth Urner, Computer Science Department, Carnegie Mellon University"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_urner_learning/",
    "segmentation": [
        [
            "Yeah, so I'll talk about active learning and in particular cluster based active learning is joint work with Schreiben Leviton with showing Wolf who's currently PhD student in Zurich at Ath.",
            "So as I'm the first present to talk about active learning, learning are very shortly I'm introduced the setting."
        ],
        [
            "So, so we do binary classification usually.",
            "You get the learner is assumed to get.",
            "The task is modelled SM distribution.",
            "The learner gets a fully labeled sample.",
            "It then produce some function and the goal is to produce function of small error."
        ],
        [
            "Um, and very often we would like to save labels, so labeling is often very expensive.",
            "We need.",
            "Humans need to pay people to do that.",
            "So active learning is one of the learning settings that aims to save label supervision and it does it by as follows.",
            "The learner gets an unlabeled sample in the beginning and then the learner gets to choose which points of the unlabeled sample to.",
            "To label so it can, it can do label queries and then the goal of the learner is to still learn a good classifier.",
            "Obviously, while making as few queries as possible."
        ],
        [
            "So this is just the former model.",
            "Is the former model of in our work also, so we always assume that they did that the domain is the unit.",
            "Cube labels are binary.",
            "Um?",
            "And we have a labeling function from X to 01."
        ],
        [
            "So yeah, so there are several challenges that one has to overcome when doing active learning or when trying to gain something by by active learning.",
            "So the first thing is the sampling bias.",
            "So what happens is that if you get an unlabeled IID sample from the distribution, and the learner chooses which which points to label the resulting label set is not necessarily still a good representation of the distribution.",
            "So any active learner has to somehow deal with this with this issue.",
            "Another point is that they are lower bounds on the label complexity of active learning, but in the worst case correspond to the lower bounds for passive learning.",
            "So what it means is that we cannot.",
            "We cannot hope for provable label reductions in general, so we whenever we want to prove such a thing, we will have to make some kind of assumption on the on the task and the most prominent assumption so far.",
            "This is the disagreement coefficient that was introduced by Chanukah much of the previous work on active learning.",
            "Use this this assumption.",
            "We will introduce a new data assumption to the active learning research area."
        ],
        [
            "So yeah, obviously there's some.",
            "There's lots of previous work on active learning.",
            "Many people have worked on this.",
            "Many of those people are here.",
            "So I am not listing everything and I'm I cannot describe everything.",
            "But yeah, there are these lower bounds that I've mentioned already.",
            "There's been working the realisable case so.",
            "The most encouraging results probably were first in the realisable case and then it has been.",
            "We then generalize to some degree, to be ignostic to be agnostic case.",
            "Um and what I want to emphasize is 1 work here.",
            "That was bison send riders group down Daniel Schorr.",
            "They they suggested to two.",
            "To do active learning based on a hierarchical clustering, so that is kind of a little bit of an outsider.",
            "In comparison to much of the other work, because much of the other work has has used active queries to summer search efficiently through our hypothesis space.",
            "In this work does something else so and we will build heavily on the ideas of this work, so let me explain what they do."
        ],
        [
            "Um?",
            "I stole these images from from a survey by Sunjoy.",
            "OK, so the idea is as follows.",
            "The learner gets an unlabeled sample and in the end it will output a fully labeling of the sample.",
            "And then then this can be fed to to another learning procedure as a procedure.",
            "So what we and they learn altogether.",
            "Hierarchy gets Iraqi clustering of this of the unlabeled sample.",
            "So it's in kind of a tree in the beginning.",
            "It's the whole data set.",
            "And then as you go down the tree you get partitions of the of the unlabeled sample, and the idea is like this.",
            "We request on the tree whenever you add some cluster, you sample randomly from this.",
            "From this cluster you see some labels and you will decide whether.",
            "This cluster is labeled homogeneous or label heterogeneous.",
            "Buy some buy some measure and then decide what to do.",
            "So if they if they label is the class label, hit Rd genius then you will want to split this this class according to the tree if."
        ],
        [
            "It's homogeneous, then.",
            "You might want to just label everything in the cluster with this one label.",
            "Um?"
        ],
        [
            "So the nice insight about this is that it avoids the the sampling bias because the hierarchical clustering is fixed before we see any labels.",
            "So this is kind of one way to get around the sampling bias, and the proposal is to use this as a pre procedure for any other.",
            "Learning algorithm."
        ],
        [
            "So what do we do in this work?",
            "So we so this is this is a fairly general framework and what we do is we look at a specific version of it and we suggest a specific version and then we analyze this with this version of the general paradigm we call it fly.",
            "Um?",
            "So and then what we show is we show that this version of it probably probably reduces the label complexity under some general mildness, or cluster.",
            "Billy assumption of the data distribution and we identify various learning settings where we actually get label complexity reductions."
        ],
        [
            "So yeah, this is.",
            "Our version this is the hierarchical clustering that we use.",
            "So yeah, so it's just demos demonstrated by this picture I guess.",
            "So we use as a hierarchical clustering, spatial trees unique took its success successively divided into into cells.",
            "We go through it level by level query so and so many points per per cluster and then if everything that we've seen has the same level, we declared homogeneous label.",
            "Everything with this label.",
            "Otherwise we split."
        ],
        [
            "OK, so these are the things that I'm now going to talk about that we show in this work.",
            "First of all, we proven error bound for our version of this framework.",
            "Then we also want to bound the number of queries that we may make with this.",
            "Um, after that, I'll talk about for which.",
            "Which type of learning algorithms you can actually use this pre procedure for and in the end I'll present a couple of settings where we can show that we actually reduce the label complexity if we use our labeling procedure as a pre procedure.",
            "OK.",
            "So I start with this with the error bound."
        ],
        [
            "So yeah, this is this is what we show.",
            "Um?",
            "So yeah, domainers unit cubes.",
            "Um?",
            "So we show that if you feed if you feed the algorithm a sample of size M, then with high probability at most an epsilon fraction of the points will end up with the wrong label.",
            "So.",
            "This is."
        ],
        [
            "First thing and then.",
            "Maybe why is Interestingly, we bound the number of queries that the algorithm will make.",
            "During this."
        ],
        [
            "So yeah, so the first thing to notice that this this error bound that I just mentioned that will always be that will always hold.",
            "It'll always be fine.",
            "We don't need to make any extra data assumptions.",
            "But now, in order to to bound the number of queries that be that this procedure will make, we need to assume that the distribution is not behaving to wildly in some sense.",
            "Um, so intuitively, our algorithm will make few queries of dense areas are labeled homogeneous.",
            "So this is some kind of a cluster assumption and.",
            "Let me show."
        ],
        [
            "You what is the formalization of this class assumption that we use?",
            "So one thing to notice.",
            "If the if the labeling function of the offered distribution satisfies the Lipschitz condition.",
            "It kind of forces the data to sit in nicely separated homogeneous clusters, so if.",
            "This is the case then you can say OK, this labeling function satisfies the Lipschitz condition.",
            "This is apparently very strong assumption."
        ],
        [
            "We know would like to generalize this or to weaken this assumption.",
            "So what we suggest is to say.",
            "This is what we called probabilistic Lipschitz Ness, so we only require quite Lipschitz NIS or rebound the rebound.",
            "The fraction of points for which we acquired.",
            "Maybe let me give you a re phrasing of this definition because of my not so easy to parse.",
            "So what we do is we assume that we have some function Phi.",
            "Um?",
            "And now we bound.",
            "We assume that the mass of points for which the ball around this point is labeled heterogeneous is bounded by a function of the radius of this wall.",
            "In this function.",
            "Is this fire of Lambda?",
            "So in this work, and I mean this is very like to fairly generic assumption.",
            "You can plug in all kind of functions for fire and see what this gives you.",
            "So in this work we assume that Fire Flame does some kind of a polynomial.",
            "Of Lambda."
        ],
        [
            "So just to give you an example, if.",
            "The distribution is uniform and the.",
            "Labelings linear separator, then you have.",
            "Then you then you have linear linear Lipschitz Ness."
        ],
        [
            "Obviously it depends on the marginal.",
            "If the marginal kind of forms clusters then this could hope that it will actually satisfy stronger versions of the... Ness.",
            "For example, a polynomial of higher degree or even exponential.",
            "So we can construct examples for this by making the marginal distribution.",
            "Amor.",
            "I say that yeah, more clustered linear for any dimension there for the right right.",
            "Right?"
        ],
        [
            "OK, so under this assumption we can bound the number of queries that our labeling procedure will make.",
            "So what we use for this is we use that as we go down in this tree, the diameter of the cells of the cluster shrinks.",
            "So assuming.",
            "That the diameter of cell at level K is bounded by some Lambda K. We get this bound on the number of queries, so feed it, feed it as a sample, unlabeled sample of size M. Then this is a bound on the expected number of queries that the algorithm will make.",
            "So this is right now, fairly general, works for any FI.",
            "Um?",
            "Maybe let me explain to you where this bound comes from."
        ],
        [
            "Um?",
            "So what we do is we so.",
            "This bonds hold for any holds for any level K. So what we want to do is we found the number of queries that we will make up to level K and we want the number of queries that we make from level KO.",
            "So up to level K there they are two to the K. Many classes at most.",
            "Hopefully there will be less because we hope that we will have finished some of the classes and have declared them homogeneous earlier.",
            "But in the worst case they at most two to the K. Many cells and then QK is the number of queries that we will make per cluster.",
            "So this is definitely a bound on the number of queries up to level K and then formula from level K on.",
            "We can bound the expected number of points that lie still lying, heterogeneous certain, or the the expected number of sample points.",
            "There's still line hit Regina cell by 5 Lambda K. So these two together.",
            "So up to let up to level K, you make this many queries from.",
            "Levoca on you make it most this many queries so this holds for any K and now if we."
        ],
        [
            "African Creed.",
            "Um?",
            "And function for the Lipschitz Ness we can compute for which K these to meet and get actual bounce on the number of queries that we make.",
            "So if, for example, if the... is fire is Lambda to the end, we get this bound.",
            "And this.",
            "This implies that if the labeling label complexity of your learning task is one over epsilon to the Alpha for some Alpha.",
            "Then this label complexity will get reduced by using plot whenever Alpha is larger than one.",
            "But obviously this label complexity will need to hold under the same assumption under the probabilistic lipshitz assumption also."
        ],
        [
            "OK, so now I would like to talk about which OK.",
            "Which which learning algorithms we can actually?",
            "Feed the sample that we have now labeled 2."
        ],
        [
            "So, so we show that you can.",
            "I mean you can use it for any armor like erm, arlem learner for any actually for any statistical learning algorithm.",
            "Also for nearest neighbor learning.",
            "We argue that you can use it.",
            "And what we need to show is that the algorithm that we want to feed the sample tool will be robust to the type of type of labeling error that we introduce, or that we might introduce with this plan.",
            "OK."
        ],
        [
            "OK, so we have some kind of formalization of robustness.",
            "Basically it says if you feed it, an algorithm is robust.",
            "Is this robust?",
            "If you when you feed it with a sample that is epsilon corrupted, the error will.",
            "Get worse at most Byetta, so such algorithms.",
            "For such for arguments that are robust in the sense, we can definitely use plan."
        ],
        [
            "And it's easy to see that this is the case for ERM or farlim learners, because.",
            "They use the empirical empirical error, they minimize the empirical error of a function or empirical error, plus some regularizer and the empirical error will not change by too much.",
            "Um, if you use plus so therefore I mean we can, we can put numbers on this whole past day and it's safe to use.",
            "Apply for this and it's."
        ],
        [
            "Actually, more more generally.",
            "It's safe to use ply for any kind of statistical algorithm for you, OK?"
        ],
        [
            "Um?",
            "And then we also show that you can.",
            "You can use it for nearest neighbor learning.",
            "You have to to modify the nearest neighbor algorithm a little bit.",
            "We say that instead of labeling everything by the by the nearest neighbor, you label a point by the nearest neighbor in the cell that the produced.",
            "So there's a slight modification, but then it's fine to use.",
            "Um plan is a pre labeling procedure."
        ],
        [
            "And I'm almost done."
        ],
        [
            "So yes, so finally we actually present several learning settings where we can show that the label complexity gets reduced."
        ],
        [
            "In particular, we have examples for proper learning of basic class for unrestricted a, not necessarily proper learning, and for nearest neighbor learning.",
            "So for each of these, each of these settings, what we do is we establish lower bounds for for the usual passive learning that are higher than the active, the upper bounds that we get for active learning with plug.",
            "So we construct examples for this and then what we get."
        ],
        [
            "Is.",
            "Is this so for proper learning?",
            "The sample complexity, even with probabilistic lips, this is still one of epsilon square gets reduced to this, and this is obviously it's better the larger Ennis, so the larger NN is the the degree of the polynomial and the Lipschitz Ness, so the larger end, the stronger the assumption, the better the reduction.",
            "And for unrestricted learning, we construct an example where we so need this.",
            "Many labels gets reduced to one of epsilon and this is for nearest neighbor learning.",
            "OK."
        ],
        [
            "So this is my summary.",
            "So we've analyzed the specific version of the of the frame that was suggested by this group to show we analyze the error.",
            "The number of flavor queries, and we show that under the cluster ability assumption that I rarely find we get label complexity reductions for.",
            "These three types of learning proper learning generally learning MVC class and for nearest neighbor.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so I'll talk about active learning and in particular cluster based active learning is joint work with Schreiben Leviton with showing Wolf who's currently PhD student in Zurich at Ath.",
                    "label": 0
                },
                {
                    "sent": "So as I'm the first present to talk about active learning, learning are very shortly I'm introduced the setting.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so we do binary classification usually.",
                    "label": 0
                },
                {
                    "sent": "You get the learner is assumed to get.",
                    "label": 0
                },
                {
                    "sent": "The task is modelled SM distribution.",
                    "label": 0
                },
                {
                    "sent": "The learner gets a fully labeled sample.",
                    "label": 0
                },
                {
                    "sent": "It then produce some function and the goal is to produce function of small error.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um, and very often we would like to save labels, so labeling is often very expensive.",
                    "label": 0
                },
                {
                    "sent": "We need.",
                    "label": 0
                },
                {
                    "sent": "Humans need to pay people to do that.",
                    "label": 0
                },
                {
                    "sent": "So active learning is one of the learning settings that aims to save label supervision and it does it by as follows.",
                    "label": 1
                },
                {
                    "sent": "The learner gets an unlabeled sample in the beginning and then the learner gets to choose which points of the unlabeled sample to.",
                    "label": 0
                },
                {
                    "sent": "To label so it can, it can do label queries and then the goal of the learner is to still learn a good classifier.",
                    "label": 0
                },
                {
                    "sent": "Obviously, while making as few queries as possible.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just the former model.",
                    "label": 0
                },
                {
                    "sent": "Is the former model of in our work also, so we always assume that they did that the domain is the unit.",
                    "label": 0
                },
                {
                    "sent": "Cube labels are binary.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And we have a labeling function from X to 01.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, so there are several challenges that one has to overcome when doing active learning or when trying to gain something by by active learning.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is the sampling bias.",
                    "label": 0
                },
                {
                    "sent": "So what happens is that if you get an unlabeled IID sample from the distribution, and the learner chooses which which points to label the resulting label set is not necessarily still a good representation of the distribution.",
                    "label": 1
                },
                {
                    "sent": "So any active learner has to somehow deal with this with this issue.",
                    "label": 1
                },
                {
                    "sent": "Another point is that they are lower bounds on the label complexity of active learning, but in the worst case correspond to the lower bounds for passive learning.",
                    "label": 0
                },
                {
                    "sent": "So what it means is that we cannot.",
                    "label": 0
                },
                {
                    "sent": "We cannot hope for provable label reductions in general, so we whenever we want to prove such a thing, we will have to make some kind of assumption on the on the task and the most prominent assumption so far.",
                    "label": 0
                },
                {
                    "sent": "This is the disagreement coefficient that was introduced by Chanukah much of the previous work on active learning.",
                    "label": 0
                },
                {
                    "sent": "Use this this assumption.",
                    "label": 0
                },
                {
                    "sent": "We will introduce a new data assumption to the active learning research area.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, obviously there's some.",
                    "label": 0
                },
                {
                    "sent": "There's lots of previous work on active learning.",
                    "label": 1
                },
                {
                    "sent": "Many people have worked on this.",
                    "label": 0
                },
                {
                    "sent": "Many of those people are here.",
                    "label": 0
                },
                {
                    "sent": "So I am not listing everything and I'm I cannot describe everything.",
                    "label": 1
                },
                {
                    "sent": "But yeah, there are these lower bounds that I've mentioned already.",
                    "label": 0
                },
                {
                    "sent": "There's been working the realisable case so.",
                    "label": 1
                },
                {
                    "sent": "The most encouraging results probably were first in the realisable case and then it has been.",
                    "label": 0
                },
                {
                    "sent": "We then generalize to some degree, to be ignostic to be agnostic case.",
                    "label": 0
                },
                {
                    "sent": "Um and what I want to emphasize is 1 work here.",
                    "label": 0
                },
                {
                    "sent": "That was bison send riders group down Daniel Schorr.",
                    "label": 0
                },
                {
                    "sent": "They they suggested to two.",
                    "label": 0
                },
                {
                    "sent": "To do active learning based on a hierarchical clustering, so that is kind of a little bit of an outsider.",
                    "label": 0
                },
                {
                    "sent": "In comparison to much of the other work, because much of the other work has has used active queries to summer search efficiently through our hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "In this work does something else so and we will build heavily on the ideas of this work, so let me explain what they do.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I stole these images from from a survey by Sunjoy.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is as follows.",
                    "label": 0
                },
                {
                    "sent": "The learner gets an unlabeled sample and in the end it will output a fully labeling of the sample.",
                    "label": 0
                },
                {
                    "sent": "And then then this can be fed to to another learning procedure as a procedure.",
                    "label": 0
                },
                {
                    "sent": "So what we and they learn altogether.",
                    "label": 0
                },
                {
                    "sent": "Hierarchy gets Iraqi clustering of this of the unlabeled sample.",
                    "label": 1
                },
                {
                    "sent": "So it's in kind of a tree in the beginning.",
                    "label": 0
                },
                {
                    "sent": "It's the whole data set.",
                    "label": 0
                },
                {
                    "sent": "And then as you go down the tree you get partitions of the of the unlabeled sample, and the idea is like this.",
                    "label": 1
                },
                {
                    "sent": "We request on the tree whenever you add some cluster, you sample randomly from this.",
                    "label": 0
                },
                {
                    "sent": "From this cluster you see some labels and you will decide whether.",
                    "label": 1
                },
                {
                    "sent": "This cluster is labeled homogeneous or label heterogeneous.",
                    "label": 0
                },
                {
                    "sent": "Buy some buy some measure and then decide what to do.",
                    "label": 0
                },
                {
                    "sent": "So if they if they label is the class label, hit Rd genius then you will want to split this this class according to the tree if.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's homogeneous, then.",
                    "label": 0
                },
                {
                    "sent": "You might want to just label everything in the cluster with this one label.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the nice insight about this is that it avoids the the sampling bias because the hierarchical clustering is fixed before we see any labels.",
                    "label": 1
                },
                {
                    "sent": "So this is kind of one way to get around the sampling bias, and the proposal is to use this as a pre procedure for any other.",
                    "label": 1
                },
                {
                    "sent": "Learning algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we do in this work?",
                    "label": 0
                },
                {
                    "sent": "So we so this is this is a fairly general framework and what we do is we look at a specific version of it and we suggest a specific version and then we analyze this with this version of the general paradigm we call it fly.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So and then what we show is we show that this version of it probably probably reduces the label complexity under some general mildness, or cluster.",
                    "label": 1
                },
                {
                    "sent": "Billy assumption of the data distribution and we identify various learning settings where we actually get label complexity reductions.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, this is.",
                    "label": 0
                },
                {
                    "sent": "Our version this is the hierarchical clustering that we use.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so it's just demos demonstrated by this picture I guess.",
                    "label": 0
                },
                {
                    "sent": "So we use as a hierarchical clustering, spatial trees unique took its success successively divided into into cells.",
                    "label": 1
                },
                {
                    "sent": "We go through it level by level query so and so many points per per cluster and then if everything that we've seen has the same level, we declared homogeneous label.",
                    "label": 1
                },
                {
                    "sent": "Everything with this label.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we split.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so these are the things that I'm now going to talk about that we show in this work.",
                    "label": 0
                },
                {
                    "sent": "First of all, we proven error bound for our version of this framework.",
                    "label": 0
                },
                {
                    "sent": "Then we also want to bound the number of queries that we may make with this.",
                    "label": 1
                },
                {
                    "sent": "Um, after that, I'll talk about for which.",
                    "label": 1
                },
                {
                    "sent": "Which type of learning algorithms you can actually use this pre procedure for and in the end I'll present a couple of settings where we can show that we actually reduce the label complexity if we use our labeling procedure as a pre procedure.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I start with this with the error bound.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, this is this is what we show.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So yeah, domainers unit cubes.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we show that if you feed if you feed the algorithm a sample of size M, then with high probability at most an epsilon fraction of the points will end up with the wrong label.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First thing and then.",
                    "label": 0
                },
                {
                    "sent": "Maybe why is Interestingly, we bound the number of queries that the algorithm will make.",
                    "label": 1
                },
                {
                    "sent": "During this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, so the first thing to notice that this this error bound that I just mentioned that will always be that will always hold.",
                    "label": 0
                },
                {
                    "sent": "It'll always be fine.",
                    "label": 0
                },
                {
                    "sent": "We don't need to make any extra data assumptions.",
                    "label": 0
                },
                {
                    "sent": "But now, in order to to bound the number of queries that be that this procedure will make, we need to assume that the distribution is not behaving to wildly in some sense.",
                    "label": 1
                },
                {
                    "sent": "Um, so intuitively, our algorithm will make few queries of dense areas are labeled homogeneous.",
                    "label": 0
                },
                {
                    "sent": "So this is some kind of a cluster assumption and.",
                    "label": 0
                },
                {
                    "sent": "Let me show.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You what is the formalization of this class assumption that we use?",
                    "label": 0
                },
                {
                    "sent": "So one thing to notice.",
                    "label": 0
                },
                {
                    "sent": "If the if the labeling function of the offered distribution satisfies the Lipschitz condition.",
                    "label": 1
                },
                {
                    "sent": "It kind of forces the data to sit in nicely separated homogeneous clusters, so if.",
                    "label": 0
                },
                {
                    "sent": "This is the case then you can say OK, this labeling function satisfies the Lipschitz condition.",
                    "label": 0
                },
                {
                    "sent": "This is apparently very strong assumption.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We know would like to generalize this or to weaken this assumption.",
                    "label": 0
                },
                {
                    "sent": "So what we suggest is to say.",
                    "label": 0
                },
                {
                    "sent": "This is what we called probabilistic Lipschitz Ness, so we only require quite Lipschitz NIS or rebound the rebound.",
                    "label": 0
                },
                {
                    "sent": "The fraction of points for which we acquired.",
                    "label": 0
                },
                {
                    "sent": "Maybe let me give you a re phrasing of this definition because of my not so easy to parse.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we assume that we have some function Phi.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And now we bound.",
                    "label": 0
                },
                {
                    "sent": "We assume that the mass of points for which the ball around this point is labeled heterogeneous is bounded by a function of the radius of this wall.",
                    "label": 0
                },
                {
                    "sent": "In this function.",
                    "label": 0
                },
                {
                    "sent": "Is this fire of Lambda?",
                    "label": 0
                },
                {
                    "sent": "So in this work, and I mean this is very like to fairly generic assumption.",
                    "label": 0
                },
                {
                    "sent": "You can plug in all kind of functions for fire and see what this gives you.",
                    "label": 0
                },
                {
                    "sent": "So in this work we assume that Fire Flame does some kind of a polynomial.",
                    "label": 0
                },
                {
                    "sent": "Of Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to give you an example, if.",
                    "label": 0
                },
                {
                    "sent": "The distribution is uniform and the.",
                    "label": 0
                },
                {
                    "sent": "Labelings linear separator, then you have.",
                    "label": 0
                },
                {
                    "sent": "Then you then you have linear linear Lipschitz Ness.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Obviously it depends on the marginal.",
                    "label": 0
                },
                {
                    "sent": "If the marginal kind of forms clusters then this could hope that it will actually satisfy stronger versions of the... Ness.",
                    "label": 1
                },
                {
                    "sent": "For example, a polynomial of higher degree or even exponential.",
                    "label": 1
                },
                {
                    "sent": "So we can construct examples for this by making the marginal distribution.",
                    "label": 0
                },
                {
                    "sent": "Amor.",
                    "label": 0
                },
                {
                    "sent": "I say that yeah, more clustered linear for any dimension there for the right right.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so under this assumption we can bound the number of queries that our labeling procedure will make.",
                    "label": 0
                },
                {
                    "sent": "So what we use for this is we use that as we go down in this tree, the diameter of the cells of the cluster shrinks.",
                    "label": 0
                },
                {
                    "sent": "So assuming.",
                    "label": 0
                },
                {
                    "sent": "That the diameter of cell at level K is bounded by some Lambda K. We get this bound on the number of queries, so feed it, feed it as a sample, unlabeled sample of size M. Then this is a bound on the expected number of queries that the algorithm will make.",
                    "label": 1
                },
                {
                    "sent": "So this is right now, fairly general, works for any FI.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Maybe let me explain to you where this bound comes from.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So what we do is we so.",
                    "label": 0
                },
                {
                    "sent": "This bonds hold for any holds for any level K. So what we want to do is we found the number of queries that we will make up to level K and we want the number of queries that we make from level KO.",
                    "label": 0
                },
                {
                    "sent": "So up to level K there they are two to the K. Many classes at most.",
                    "label": 0
                },
                {
                    "sent": "Hopefully there will be less because we hope that we will have finished some of the classes and have declared them homogeneous earlier.",
                    "label": 0
                },
                {
                    "sent": "But in the worst case they at most two to the K. Many cells and then QK is the number of queries that we will make per cluster.",
                    "label": 0
                },
                {
                    "sent": "So this is definitely a bound on the number of queries up to level K and then formula from level K on.",
                    "label": 1
                },
                {
                    "sent": "We can bound the expected number of points that lie still lying, heterogeneous certain, or the the expected number of sample points.",
                    "label": 0
                },
                {
                    "sent": "There's still line hit Regina cell by 5 Lambda K. So these two together.",
                    "label": 0
                },
                {
                    "sent": "So up to let up to level K, you make this many queries from.",
                    "label": 0
                },
                {
                    "sent": "Levoca on you make it most this many queries so this holds for any K and now if we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "African Creed.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And function for the Lipschitz Ness we can compute for which K these to meet and get actual bounce on the number of queries that we make.",
                    "label": 1
                },
                {
                    "sent": "So if, for example, if the... is fire is Lambda to the end, we get this bound.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                },
                {
                    "sent": "This implies that if the labeling label complexity of your learning task is one over epsilon to the Alpha for some Alpha.",
                    "label": 1
                },
                {
                    "sent": "Then this label complexity will get reduced by using plot whenever Alpha is larger than one.",
                    "label": 0
                },
                {
                    "sent": "But obviously this label complexity will need to hold under the same assumption under the probabilistic lipshitz assumption also.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I would like to talk about which OK.",
                    "label": 0
                },
                {
                    "sent": "Which which learning algorithms we can actually?",
                    "label": 0
                },
                {
                    "sent": "Feed the sample that we have now labeled 2.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so we show that you can.",
                    "label": 1
                },
                {
                    "sent": "I mean you can use it for any armor like erm, arlem learner for any actually for any statistical learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Also for nearest neighbor learning.",
                    "label": 1
                },
                {
                    "sent": "We argue that you can use it.",
                    "label": 0
                },
                {
                    "sent": "And what we need to show is that the algorithm that we want to feed the sample tool will be robust to the type of type of labeling error that we introduce, or that we might introduce with this plan.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we have some kind of formalization of robustness.",
                    "label": 0
                },
                {
                    "sent": "Basically it says if you feed it, an algorithm is robust.",
                    "label": 0
                },
                {
                    "sent": "Is this robust?",
                    "label": 0
                },
                {
                    "sent": "If you when you feed it with a sample that is epsilon corrupted, the error will.",
                    "label": 0
                },
                {
                    "sent": "Get worse at most Byetta, so such algorithms.",
                    "label": 0
                },
                {
                    "sent": "For such for arguments that are robust in the sense, we can definitely use plan.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's easy to see that this is the case for ERM or farlim learners, because.",
                    "label": 1
                },
                {
                    "sent": "They use the empirical empirical error, they minimize the empirical error of a function or empirical error, plus some regularizer and the empirical error will not change by too much.",
                    "label": 1
                },
                {
                    "sent": "Um, if you use plus so therefore I mean we can, we can put numbers on this whole past day and it's safe to use.",
                    "label": 0
                },
                {
                    "sent": "Apply for this and it's.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, more more generally.",
                    "label": 0
                },
                {
                    "sent": "It's safe to use ply for any kind of statistical algorithm for you, OK?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then we also show that you can.",
                    "label": 0
                },
                {
                    "sent": "You can use it for nearest neighbor learning.",
                    "label": 1
                },
                {
                    "sent": "You have to to modify the nearest neighbor algorithm a little bit.",
                    "label": 1
                },
                {
                    "sent": "We say that instead of labeling everything by the by the nearest neighbor, you label a point by the nearest neighbor in the cell that the produced.",
                    "label": 0
                },
                {
                    "sent": "So there's a slight modification, but then it's fine to use.",
                    "label": 0
                },
                {
                    "sent": "Um plan is a pre labeling procedure.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm almost done.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yes, so finally we actually present several learning settings where we can show that the label complexity gets reduced.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In particular, we have examples for proper learning of basic class for unrestricted a, not necessarily proper learning, and for nearest neighbor learning.",
                    "label": 1
                },
                {
                    "sent": "So for each of these, each of these settings, what we do is we establish lower bounds for for the usual passive learning that are higher than the active, the upper bounds that we get for active learning with plug.",
                    "label": 1
                },
                {
                    "sent": "So we construct examples for this and then what we get.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Is this so for proper learning?",
                    "label": 1
                },
                {
                    "sent": "The sample complexity, even with probabilistic lips, this is still one of epsilon square gets reduced to this, and this is obviously it's better the larger Ennis, so the larger NN is the the degree of the polynomial and the Lipschitz Ness, so the larger end, the stronger the assumption, the better the reduction.",
                    "label": 1
                },
                {
                    "sent": "And for unrestricted learning, we construct an example where we so need this.",
                    "label": 0
                },
                {
                    "sent": "Many labels gets reduced to one of epsilon and this is for nearest neighbor learning.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is my summary.",
                    "label": 0
                },
                {
                    "sent": "So we've analyzed the specific version of the of the frame that was suggested by this group to show we analyze the error.",
                    "label": 0
                },
                {
                    "sent": "The number of flavor queries, and we show that under the cluster ability assumption that I rarely find we get label complexity reductions for.",
                    "label": 1
                },
                {
                    "sent": "These three types of learning proper learning generally learning MVC class and for nearest neighbor.",
                    "label": 1
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}