{
    "id": "tlgcx2rpkj252tybz3mzthxtywdb5lfg",
    "title": "Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations",
    "info": {
        "author": [
            "Honglak Lee, Department of Electrical Engineering and Computer Science, University of Michigan"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_lee_cdb/",
    "segmentation": [
        [
            "I'm going to talk about convolutional deep belief networks for scalable on unsupervised learning of hierarchical representation.",
            "I'm likely from Stanford University.",
            "This is work with Roger Growth, Rajesh Ranganath and my advisor and ring."
        ],
        [
            "Here is the outline of the talk.",
            "First, talk about motivation of this research and the background, and describe our algorithm an show experimental results and finish with a summary."
        ],
        [
            "As a motivation of this research.",
            "We are generally interested in deep learning algorithm.",
            "Deep learning algorithms are inspired by hierarchical organization of the brain and they try to learn hierarchical feature representation by building high level features on top of simpler low level features.",
            "Deep learning algorithm can be used as training in mostly unsupervised way.",
            "an A single learning algorithm can be used to train multiple layers of hierarchy.",
            "Among many algorithms, we are interested in deep belief network, deep belief network is a hierarchical generative model and has been very promising.",
            "In this research, we are interested in interested in scaling of deep belief networks to learn generative models and to perform inference on very challenging problems."
        ],
        [
            "To begin with, I'll first talk about restricted Boltzmann machine.",
            "Restricted Boltzmann machine is an undirected bipartite graphical model with a set of visible nodes and a set of hidden nodes.",
            "The visible nodes denote the input data.",
            "And the hidden nodes will encode statistical relationship in the visible nodes.",
            "We can write down the energy function an define or joint probability distribution between visible no Dan hidden nodes.",
            "Since this is undirected bipartite graphical model.",
            "Suppose that we are given one layer.",
            "All the other nodes in one layer, then nose in the other layers are conditionally independent.",
            "Therefore we can perform blockage sampling for inference and learning.",
            "More specifically.",
            "We can train, we can train restricted Boltzmann machine using contrastive divergent approximation for maximum likelihood."
        ],
        [
            "Using this restricted Boltzmann machine as a building block, we can train deep belief network.",
            "The belief network is a hierarchical generative model.",
            "With one visible layer and many hidden layers.",
            "And we can train deep belief network and in our greedy layer wise training meaning using restricted Boltzmann training restricted Boltzmann machine one layer at a time.",
            "Deep belief network has.",
            "Been very successful in many applications, for example, recognizing handwritten digits or learning human motion capture data.",
            "Typical input dimension of depth network is around 1000.",
            "This roughly correspond to 30 by 30 image pixels.",
            "The challenge here that we want to address is to scale up the public network to more realistic image sizes, for example 200 by 200 pixels."
        ],
        [
            "One way to deal with this problem is using convolutional architectures.",
            "Typically, convolutional architectures alternate between detection and pooling layers.",
            "Detection layers have a set of filter weights that are shared between all image locations, and the computation can be done very efficiently using convolution.",
            "In the pooling layer, each polling unit computes the maximum of the neighboring block of.",
            "Neighboring units of the full detection unit.",
            "And this procedure is called Max pooling.",
            "Max pooling has two advantages.",
            "First, it shrinks the representation in the higher layers.",
            "And it makes the computation in the higher lace higher layers or efficient second, it provides invariants to local transformations.",
            "Typically Max pooling is used in a deterministic and feed for feedforward setting.",
            "However, in our approach we want to make this probabilistic so we will describe how to implement Max pooling in a probabilistic way, and it turns out that we can perform probabilistic inference by combining bottom up and top down in top down information."
        ],
        [
            "Now I'm going to describe our algorithms."
        ],
        [
            "1st, I'll talk about convolutional RBM.",
            "Convolutional RBM is an extension of RBM to a convolutional setting.",
            "Where?",
            "The visible nodes correspond to typically input images.",
            "And there are a set of weights filter weights.",
            "So here.",
            "Stop.",
            "This WK correspond to one filter weights that covers a small area in the image.",
            "And this weights are shared across all the locations in the hidden layers.",
            "And Moreover, we have a small group of detection units that are pulled together to Max pooling node.",
            "And since this is a probabilistic model, we can."
        ],
        [
            "Define joint probability probability distribution between the visible node and hit hidden detection nodes and we can simply write down energy function.",
            "So the energy function can be compactly written as.",
            "Some interaction between hidden nodes and visible nodes and weights using a convolution operator.",
            "In addition, we have a constraint that.",
            "That means that for each pulling unit.",
            "A group of detection units that are pulled together.",
            "At most one detection unit can be one.",
            "So don't worry about the details of the main idea here is that we extended the energy function of or regular restricted Boltzmann machine to a convolutional setting and also we have additional constraint that achieves probabilistic Max pooling and I'll describe it in more detail in later slides.",
            "So using this energy function, it turns out that we can perform blockage sampling which is just using convolution as a linear filtering an sample using multinomial distribution.",
            "To train this convolutional RBM we just used sparse RBM formulation."
        ],
        [
            "Now I'm going to talk in more detail about probabilistic Max pooling.",
            "Let's consider an example where we have four detection units that are pulled together to one pulling unit.",
            "Here one important distinction between our model and the convolutional neural network is that in convolutional neural network these XJ's are real valued and they are deterministic.",
            "However, in our case, these X Rays are stochastic binary variables and also due to the constraint these.",
            "Xrays are mutually exclusive.",
            "As a result, we have 5 configuration, 5 possible configuration from this example.",
            "In general, if we have N detection unit pulled together to pulling unit, then there are two to the end possible configuration without the mutual exclusion constraint.",
            "But with the mutual exclusion constraint we collapse this into N + 1 configuration and this allows us to perform probabilistic inference by combining bottom up and top down inference information."
        ],
        [
            "As more detailed example, I'll consider how we perform bottom up inference.",
            "So we have four detection units and pull together to a pulling unit an suppose that they are getting bottom up information, denoted as.",
            "I want to I4.",
            "Then using the energy function we can compute the conditional probability of activation, and it turns out that the probability of escaping one can be simply written as a softmax function.",
            "And due to Mr Exclusion constraint, the probability of wiping one is simply just summing over the order probability of X Rays being one and two sample from this distribution we can just sample X1 to X4 according to the multinomial distribution."
        ],
        [
            "So I described how to do inference and learning for the convolutional RBM's.",
            "Now I described how to construct convolutional deep belief networks.",
            "Convolutional deep belief networks can be constructed in a greedy layer wise training, meaning that we just use one retrain, one convolutional RBM, one at a time from bottom up.",
            "After we train all the parameters of the model, we can perform approximate inference.",
            "1 technical distinction between our model and the original Deep list network is that we use undirected connection for all layers.",
            "This means that we can perform lock if sampling or mean field for inference.",
            "And I'll show you in the next slide that it enables us to do hierarchical, probably probabilistic inference efficiently.",
            "And I will show you some experiment."
        ],
        [
            "Demonstrating this.",
            "So this is some illustrative example for hierarchical probabilistic inference.",
            "So in this example we have the same setting with the four detection units with one pulling unit.",
            "However, we consider a case that this pulling unit is receiving or top down information.",
            "Intuitively, if this top top down information denoted as T if T is large, that means that probability of Y being active is high.",
            "So using the energy function we can also write down the conditional probability that X shaping one, and it turns out that it can be simply written as softmax function.",
            "Similar way.",
            "Thought.",
            "We just add the top down and bottom up information together inside the exponential function.",
            "An we can sample from this conditional distribution by simply using multinomial distribution."
        ],
        [
            "So I described about convolutional deep belief network and."
        ],
        [
            "Going to.",
            "So experimental results first.",
            "We evaluated our model to handwritten digit data.",
            "We trained two layer convolutional deep belief network on unlabeled MNIST training data and it turns out that the first layer learns strokes and the second layer learns some groupings of strokes.",
            "And we computed the test error with different number of labeled examples.",
            "In summary, our model performs very competitively compared to other state of the art models."
        ],
        [
            "In the next experiment, we trained two layer convolutional Deep IP network representation from a set of natural images.",
            "And the resulting representation is that in the first layer it learns localized oriented edges, and in the second layer it combines this first layer representation into a richer set of features.",
            "For example, contours, corners and arxan surface boundaries."
        ],
        [
            "And we evaluate our representation too.",
            "Caltech.",
            "One Object classification task.",
            "First, we can see that our first layer representation gives fairly good result, but after combining the 1st and Secondly representation, the accuracy improved significantly.",
            "And our final result is 65% accuracy for 30 training images per class and this is very competitive result using other state of the art features such as SIFT.",
            "And considering that our representation was trained from natural image, which is completely unrelated to the Caltech 101 images, this shows that our convolutional DBN representation learned fairly generic informative representation for images."
        ],
        [
            "In the next experiment, we trained three layer convolutional deep belief network for multiple object category images.",
            "For example if we train from.",
            "Off images of faces, then the second layer representation tend to learn object part representations such as eyes or nose, and in the third layer representation it combines those part based.",
            "Secondly representation into the concept of full objects.",
            "And our model can do can discover a similar object part an object representation.",
            "For many other classes such as cars, elephants and chairs."
        ],
        [
            "In our quantitative evaluation.",
            "We measured how informative each feature representation in the second or third layer is."
        ],
        [
            "So going back to the previous slide, we have trained 40 features in the second layer and 24 features in the third."
        ],
        [
            "Third layer, so we pick just one second layer feature and then compute the activation for the non activation for the cars face and non face images.",
            "And we can compute the precision recall curve and compute the area.",
            "So I'll call this average precision.",
            "So we have 40 average precision values for the second layer, an 24 average precision values for the third layer.",
            "This is the distribution of average precision values.",
            "You can see that the first layer basis overall has fairly small average precision values.",
            "However, as a layer goes up, the second layer has significantly higher average precision and Thirdly, features even has even higher average precision.",
            "So this suggests that the higher layer feature learn learns are more informative for object class."
        ],
        [
            "We also tested if our representation can learn from some mixture of objects, so we train from images of faces, cars and motorbikes and airplanes.",
            "As a result, our secondary feature learns some mixture of object specific parts and shared parts.",
            "And in the third layer, by combining this secondary feature representation, it discovers more more object specific representation."
        ],
        [
            "As a quantitative evaluation, we measured conditional entropy.",
            "So conditional entropy give up class labels given that features are active.",
            "So.",
            "As in the previous experiment, we have 40 conditional entropy values for the second layer and 24 conditional entropy values for the third layer.",
            "And here are the distribution of conditional entropy values.",
            "Overall, we see that the first layer features have fairly high conditional entropy, which means that even though they are active, it's not very specific to objectclass.",
            "However, as the layer goes up, the distribution for the conditional entropy values become more closer to 1, meaning that when you know that the 1st when you know that the third level features are active and it is very specific to object class."
        ],
        [
            "As a final experiment, we tested hierarchical probabilistic inference.",
            "This experiment is inspired by Lee and Mumford Paper.",
            "As a motivating example, we can consider a situation where we observe our image of phase where we the left half of the face is very, very dark illumination.",
            "Humans are very good at recognizing the images of face an.",
            "Furthermore, fill out the other half of the face.",
            "So we tested if our model can do some similar task.",
            "So in the top row are the input images where we zeroed out the left half of the face.",
            "And we compare the two settings where we just use feedforward inference or we do four poster inference by multiple iterations of blockage sampling.",
            "As a result we see that by just doing a single feedforward sample, single feedforward influence the representation.",
            "The secondary representation doesn't activate much to the left half of the face.",
            "However, by combining the top down and bottom up influence, since the third layer representation knows that it's a face image, it gives top down information to fill out the rest half of the face.",
            "So overall, reconstruction gets significantly improved."
        ],
        [
            "As a summary, we presented convolutional restricted Boltzmann machine with probabilistic Max pooling and this enables us.",
            "Build convolutional deep belief network, which is scalable to realistic image sizes and it discovers hierarchical object part representation.",
            "Also, it gives excellent performance in object recognition task.",
            "Finally we show that our model can perform hierarchical probabilistic inference by combining bottom up and top down important information."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to talk about convolutional deep belief networks for scalable on unsupervised learning of hierarchical representation.",
                    "label": 1
                },
                {
                    "sent": "I'm likely from Stanford University.",
                    "label": 0
                },
                {
                    "sent": "This is work with Roger Growth, Rajesh Ranganath and my advisor and ring.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "First, talk about motivation of this research and the background, and describe our algorithm an show experimental results and finish with a summary.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a motivation of this research.",
                    "label": 0
                },
                {
                    "sent": "We are generally interested in deep learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Deep learning algorithms are inspired by hierarchical organization of the brain and they try to learn hierarchical feature representation by building high level features on top of simpler low level features.",
                    "label": 1
                },
                {
                    "sent": "Deep learning algorithm can be used as training in mostly unsupervised way.",
                    "label": 0
                },
                {
                    "sent": "an A single learning algorithm can be used to train multiple layers of hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Among many algorithms, we are interested in deep belief network, deep belief network is a hierarchical generative model and has been very promising.",
                    "label": 1
                },
                {
                    "sent": "In this research, we are interested in interested in scaling of deep belief networks to learn generative models and to perform inference on very challenging problems.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To begin with, I'll first talk about restricted Boltzmann machine.",
                    "label": 1
                },
                {
                    "sent": "Restricted Boltzmann machine is an undirected bipartite graphical model with a set of visible nodes and a set of hidden nodes.",
                    "label": 1
                },
                {
                    "sent": "The visible nodes denote the input data.",
                    "label": 0
                },
                {
                    "sent": "And the hidden nodes will encode statistical relationship in the visible nodes.",
                    "label": 0
                },
                {
                    "sent": "We can write down the energy function an define or joint probability distribution between visible no Dan hidden nodes.",
                    "label": 0
                },
                {
                    "sent": "Since this is undirected bipartite graphical model.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we are given one layer.",
                    "label": 0
                },
                {
                    "sent": "All the other nodes in one layer, then nose in the other layers are conditionally independent.",
                    "label": 1
                },
                {
                    "sent": "Therefore we can perform blockage sampling for inference and learning.",
                    "label": 0
                },
                {
                    "sent": "More specifically.",
                    "label": 1
                },
                {
                    "sent": "We can train, we can train restricted Boltzmann machine using contrastive divergent approximation for maximum likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using this restricted Boltzmann machine as a building block, we can train deep belief network.",
                    "label": 1
                },
                {
                    "sent": "The belief network is a hierarchical generative model.",
                    "label": 1
                },
                {
                    "sent": "With one visible layer and many hidden layers.",
                    "label": 0
                },
                {
                    "sent": "And we can train deep belief network and in our greedy layer wise training meaning using restricted Boltzmann training restricted Boltzmann machine one layer at a time.",
                    "label": 0
                },
                {
                    "sent": "Deep belief network has.",
                    "label": 0
                },
                {
                    "sent": "Been very successful in many applications, for example, recognizing handwritten digits or learning human motion capture data.",
                    "label": 1
                },
                {
                    "sent": "Typical input dimension of depth network is around 1000.",
                    "label": 0
                },
                {
                    "sent": "This roughly correspond to 30 by 30 image pixels.",
                    "label": 0
                },
                {
                    "sent": "The challenge here that we want to address is to scale up the public network to more realistic image sizes, for example 200 by 200 pixels.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One way to deal with this problem is using convolutional architectures.",
                    "label": 0
                },
                {
                    "sent": "Typically, convolutional architectures alternate between detection and pooling layers.",
                    "label": 1
                },
                {
                    "sent": "Detection layers have a set of filter weights that are shared between all image locations, and the computation can be done very efficiently using convolution.",
                    "label": 1
                },
                {
                    "sent": "In the pooling layer, each polling unit computes the maximum of the neighboring block of.",
                    "label": 0
                },
                {
                    "sent": "Neighboring units of the full detection unit.",
                    "label": 0
                },
                {
                    "sent": "And this procedure is called Max pooling.",
                    "label": 1
                },
                {
                    "sent": "Max pooling has two advantages.",
                    "label": 0
                },
                {
                    "sent": "First, it shrinks the representation in the higher layers.",
                    "label": 0
                },
                {
                    "sent": "And it makes the computation in the higher lace higher layers or efficient second, it provides invariants to local transformations.",
                    "label": 0
                },
                {
                    "sent": "Typically Max pooling is used in a deterministic and feed for feedforward setting.",
                    "label": 0
                },
                {
                    "sent": "However, in our approach we want to make this probabilistic so we will describe how to implement Max pooling in a probabilistic way, and it turns out that we can perform probabilistic inference by combining bottom up and top down in top down information.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm going to describe our algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st, I'll talk about convolutional RBM.",
                    "label": 0
                },
                {
                    "sent": "Convolutional RBM is an extension of RBM to a convolutional setting.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "The visible nodes correspond to typically input images.",
                    "label": 0
                },
                {
                    "sent": "And there are a set of weights filter weights.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "Stop.",
                    "label": 0
                },
                {
                    "sent": "This WK correspond to one filter weights that covers a small area in the image.",
                    "label": 0
                },
                {
                    "sent": "And this weights are shared across all the locations in the hidden layers.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, we have a small group of detection units that are pulled together to Max pooling node.",
                    "label": 0
                },
                {
                    "sent": "And since this is a probabilistic model, we can.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Define joint probability probability distribution between the visible node and hit hidden detection nodes and we can simply write down energy function.",
                    "label": 0
                },
                {
                    "sent": "So the energy function can be compactly written as.",
                    "label": 0
                },
                {
                    "sent": "Some interaction between hidden nodes and visible nodes and weights using a convolution operator.",
                    "label": 0
                },
                {
                    "sent": "In addition, we have a constraint that.",
                    "label": 0
                },
                {
                    "sent": "That means that for each pulling unit.",
                    "label": 0
                },
                {
                    "sent": "A group of detection units that are pulled together.",
                    "label": 0
                },
                {
                    "sent": "At most one detection unit can be one.",
                    "label": 0
                },
                {
                    "sent": "So don't worry about the details of the main idea here is that we extended the energy function of or regular restricted Boltzmann machine to a convolutional setting and also we have additional constraint that achieves probabilistic Max pooling and I'll describe it in more detail in later slides.",
                    "label": 0
                },
                {
                    "sent": "So using this energy function, it turns out that we can perform blockage sampling which is just using convolution as a linear filtering an sample using multinomial distribution.",
                    "label": 0
                },
                {
                    "sent": "To train this convolutional RBM we just used sparse RBM formulation.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I'm going to talk in more detail about probabilistic Max pooling.",
                    "label": 1
                },
                {
                    "sent": "Let's consider an example where we have four detection units that are pulled together to one pulling unit.",
                    "label": 0
                },
                {
                    "sent": "Here one important distinction between our model and the convolutional neural network is that in convolutional neural network these XJ's are real valued and they are deterministic.",
                    "label": 0
                },
                {
                    "sent": "However, in our case, these X Rays are stochastic binary variables and also due to the constraint these.",
                    "label": 0
                },
                {
                    "sent": "Xrays are mutually exclusive.",
                    "label": 0
                },
                {
                    "sent": "As a result, we have 5 configuration, 5 possible configuration from this example.",
                    "label": 0
                },
                {
                    "sent": "In general, if we have N detection unit pulled together to pulling unit, then there are two to the end possible configuration without the mutual exclusion constraint.",
                    "label": 0
                },
                {
                    "sent": "But with the mutual exclusion constraint we collapse this into N + 1 configuration and this allows us to perform probabilistic inference by combining bottom up and top down inference information.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As more detailed example, I'll consider how we perform bottom up inference.",
                    "label": 0
                },
                {
                    "sent": "So we have four detection units and pull together to a pulling unit an suppose that they are getting bottom up information, denoted as.",
                    "label": 0
                },
                {
                    "sent": "I want to I4.",
                    "label": 0
                },
                {
                    "sent": "Then using the energy function we can compute the conditional probability of activation, and it turns out that the probability of escaping one can be simply written as a softmax function.",
                    "label": 0
                },
                {
                    "sent": "And due to Mr Exclusion constraint, the probability of wiping one is simply just summing over the order probability of X Rays being one and two sample from this distribution we can just sample X1 to X4 according to the multinomial distribution.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I described how to do inference and learning for the convolutional RBM's.",
                    "label": 0
                },
                {
                    "sent": "Now I described how to construct convolutional deep belief networks.",
                    "label": 0
                },
                {
                    "sent": "Convolutional deep belief networks can be constructed in a greedy layer wise training, meaning that we just use one retrain, one convolutional RBM, one at a time from bottom up.",
                    "label": 1
                },
                {
                    "sent": "After we train all the parameters of the model, we can perform approximate inference.",
                    "label": 0
                },
                {
                    "sent": "1 technical distinction between our model and the original Deep list network is that we use undirected connection for all layers.",
                    "label": 0
                },
                {
                    "sent": "This means that we can perform lock if sampling or mean field for inference.",
                    "label": 0
                },
                {
                    "sent": "And I'll show you in the next slide that it enables us to do hierarchical, probably probabilistic inference efficiently.",
                    "label": 0
                },
                {
                    "sent": "And I will show you some experiment.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Demonstrating this.",
                    "label": 0
                },
                {
                    "sent": "So this is some illustrative example for hierarchical probabilistic inference.",
                    "label": 1
                },
                {
                    "sent": "So in this example we have the same setting with the four detection units with one pulling unit.",
                    "label": 0
                },
                {
                    "sent": "However, we consider a case that this pulling unit is receiving or top down information.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, if this top top down information denoted as T if T is large, that means that probability of Y being active is high.",
                    "label": 0
                },
                {
                    "sent": "So using the energy function we can also write down the conditional probability that X shaping one, and it turns out that it can be simply written as softmax function.",
                    "label": 0
                },
                {
                    "sent": "Similar way.",
                    "label": 0
                },
                {
                    "sent": "Thought.",
                    "label": 0
                },
                {
                    "sent": "We just add the top down and bottom up information together inside the exponential function.",
                    "label": 0
                },
                {
                    "sent": "An we can sample from this conditional distribution by simply using multinomial distribution.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I described about convolutional deep belief network and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to.",
                    "label": 0
                },
                {
                    "sent": "So experimental results first.",
                    "label": 0
                },
                {
                    "sent": "We evaluated our model to handwritten digit data.",
                    "label": 0
                },
                {
                    "sent": "We trained two layer convolutional deep belief network on unlabeled MNIST training data and it turns out that the first layer learns strokes and the second layer learns some groupings of strokes.",
                    "label": 1
                },
                {
                    "sent": "And we computed the test error with different number of labeled examples.",
                    "label": 0
                },
                {
                    "sent": "In summary, our model performs very competitively compared to other state of the art models.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the next experiment, we trained two layer convolutional Deep IP network representation from a set of natural images.",
                    "label": 0
                },
                {
                    "sent": "And the resulting representation is that in the first layer it learns localized oriented edges, and in the second layer it combines this first layer representation into a richer set of features.",
                    "label": 1
                },
                {
                    "sent": "For example, contours, corners and arxan surface boundaries.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we evaluate our representation too.",
                    "label": 0
                },
                {
                    "sent": "Caltech.",
                    "label": 0
                },
                {
                    "sent": "One Object classification task.",
                    "label": 0
                },
                {
                    "sent": "First, we can see that our first layer representation gives fairly good result, but after combining the 1st and Secondly representation, the accuracy improved significantly.",
                    "label": 0
                },
                {
                    "sent": "And our final result is 65% accuracy for 30 training images per class and this is very competitive result using other state of the art features such as SIFT.",
                    "label": 0
                },
                {
                    "sent": "And considering that our representation was trained from natural image, which is completely unrelated to the Caltech 101 images, this shows that our convolutional DBN representation learned fairly generic informative representation for images.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the next experiment, we trained three layer convolutional deep belief network for multiple object category images.",
                    "label": 0
                },
                {
                    "sent": "For example if we train from.",
                    "label": 0
                },
                {
                    "sent": "Off images of faces, then the second layer representation tend to learn object part representations such as eyes or nose, and in the third layer representation it combines those part based.",
                    "label": 0
                },
                {
                    "sent": "Secondly representation into the concept of full objects.",
                    "label": 0
                },
                {
                    "sent": "And our model can do can discover a similar object part an object representation.",
                    "label": 0
                },
                {
                    "sent": "For many other classes such as cars, elephants and chairs.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our quantitative evaluation.",
                    "label": 0
                },
                {
                    "sent": "We measured how informative each feature representation in the second or third layer is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So going back to the previous slide, we have trained 40 features in the second layer and 24 features in the third.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Third layer, so we pick just one second layer feature and then compute the activation for the non activation for the cars face and non face images.",
                    "label": 0
                },
                {
                    "sent": "And we can compute the precision recall curve and compute the area.",
                    "label": 1
                },
                {
                    "sent": "So I'll call this average precision.",
                    "label": 0
                },
                {
                    "sent": "So we have 40 average precision values for the second layer, an 24 average precision values for the third layer.",
                    "label": 1
                },
                {
                    "sent": "This is the distribution of average precision values.",
                    "label": 0
                },
                {
                    "sent": "You can see that the first layer basis overall has fairly small average precision values.",
                    "label": 0
                },
                {
                    "sent": "However, as a layer goes up, the second layer has significantly higher average precision and Thirdly, features even has even higher average precision.",
                    "label": 0
                },
                {
                    "sent": "So this suggests that the higher layer feature learn learns are more informative for object class.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also tested if our representation can learn from some mixture of objects, so we train from images of faces, cars and motorbikes and airplanes.",
                    "label": 0
                },
                {
                    "sent": "As a result, our secondary feature learns some mixture of object specific parts and shared parts.",
                    "label": 0
                },
                {
                    "sent": "And in the third layer, by combining this secondary feature representation, it discovers more more object specific representation.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a quantitative evaluation, we measured conditional entropy.",
                    "label": 1
                },
                {
                    "sent": "So conditional entropy give up class labels given that features are active.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As in the previous experiment, we have 40 conditional entropy values for the second layer and 24 conditional entropy values for the third layer.",
                    "label": 0
                },
                {
                    "sent": "And here are the distribution of conditional entropy values.",
                    "label": 0
                },
                {
                    "sent": "Overall, we see that the first layer features have fairly high conditional entropy, which means that even though they are active, it's not very specific to objectclass.",
                    "label": 0
                },
                {
                    "sent": "However, as the layer goes up, the distribution for the conditional entropy values become more closer to 1, meaning that when you know that the 1st when you know that the third level features are active and it is very specific to object class.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a final experiment, we tested hierarchical probabilistic inference.",
                    "label": 1
                },
                {
                    "sent": "This experiment is inspired by Lee and Mumford Paper.",
                    "label": 0
                },
                {
                    "sent": "As a motivating example, we can consider a situation where we observe our image of phase where we the left half of the face is very, very dark illumination.",
                    "label": 0
                },
                {
                    "sent": "Humans are very good at recognizing the images of face an.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, fill out the other half of the face.",
                    "label": 0
                },
                {
                    "sent": "So we tested if our model can do some similar task.",
                    "label": 0
                },
                {
                    "sent": "So in the top row are the input images where we zeroed out the left half of the face.",
                    "label": 0
                },
                {
                    "sent": "And we compare the two settings where we just use feedforward inference or we do four poster inference by multiple iterations of blockage sampling.",
                    "label": 0
                },
                {
                    "sent": "As a result we see that by just doing a single feedforward sample, single feedforward influence the representation.",
                    "label": 0
                },
                {
                    "sent": "The secondary representation doesn't activate much to the left half of the face.",
                    "label": 0
                },
                {
                    "sent": "However, by combining the top down and bottom up influence, since the third layer representation knows that it's a face image, it gives top down information to fill out the rest half of the face.",
                    "label": 0
                },
                {
                    "sent": "So overall, reconstruction gets significantly improved.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a summary, we presented convolutional restricted Boltzmann machine with probabilistic Max pooling and this enables us.",
                    "label": 1
                },
                {
                    "sent": "Build convolutional deep belief network, which is scalable to realistic image sizes and it discovers hierarchical object part representation.",
                    "label": 1
                },
                {
                    "sent": "Also, it gives excellent performance in object recognition task.",
                    "label": 0
                },
                {
                    "sent": "Finally we show that our model can perform hierarchical probabilistic inference by combining bottom up and top down important information.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}