{
    "id": "c6yytptmmffsfevz47f5zn7m6b2selcl",
    "title": "Theory and Applications of Boosting",
    "info": {
        "author": [
            "Robert Schapire, Department of Computer Science, Princeton University"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/mlss09us_schapire_tab/",
    "segmentation": [
        [
            "So this is our.",
            "5th morning of tutorials and we're very happy to have Rob Shapiro.",
            "And we'll talk about boosting.",
            "Thanks Martha.",
            "Alright, well good morning so thank you for getting up early to see a tutorial on boosting.",
            "So this is on theory and applications of boosting a little bit more theory than applications, but there."
        ],
        [
            "Both.",
            "So this is a tutorial for people who know little or nothing about boosting.",
            "So let me start at the beginning with a typical kind of problem.",
            "This is a problem I worked on when I was at AT&T.",
            "So the problem here is to automatically categorize the type of call requested by a phone customer.",
            "So the idea is that somebody would call up AT&T and maybe they want to make a collect call or a third number call or whatever it is.",
            "And the idea was to build a system where people could just speak naturally to this system, rather than if you want this, press one if you want that, press two and so on.",
            "So when the person says something, we need, the computer needs to be able to categorize what kind of requests the person is making.",
            "So if the person says yes, I'd like to place a collect call long distance please, that's a collect call I discovered recently that since everybody's cell phones Now there are a lot of.",
            "People younger than me who don't know what a collect call is in a third number call, and so on.",
            "So these are just kinds of calls.",
            "That's all you need to know.",
            "In the old days you would make a collect call and a third number calling someone.",
            "So if the person says operator, I need to make a call.",
            "But I need to bill it to my office that's being built to 1/3 number.",
            "So that's called 1/3 number call and so on.",
            "OK, so if you think about the problem like this, if you just look at examples like this."
        ],
        [
            "Then you immediately start to realize that there are rules of thumb that will often be correct.",
            "So for instance, if you just look at that very first example, I have a pointer.",
            "Is this long enough?",
            "OK, if you just look at this very first example.",
            "You know you can immediately think it well if the word collect appears in what was said, then predict that's a collect call.",
            "Or if the word card appears in what was said, predict it's a calling card call and so on, so you can immediately start thinking about these rules of thumb, which will be pretty good.",
            "They're not going to be perfect, but they should be better than just guessing at random.",
            "What's harder is to find just a single prediction rule, which will be very highly accurate.",
            "So the idea of boosting is to somehow use these weak rules of thumb in combination somehow, so that when you put them together you end up with a prediction rule, which is very highly accurate."
        ],
        [
            "OK, so here's the boosting approach at a very high level.",
            "I'll certainly be going into more detail, but at a very high level.",
            "Here's the idea of boosting.",
            "So since these rules of thumb are not so hard to find, we could think about coming up with a computer program that could look at data and derive rules of thumb from that data.",
            "Then we could apply that procedure to a subset of the examples that would give us a first rule of thumb.",
            "Then we could do this again, choose a second subset of the examples, get a second rule of thumb, and you could imagine doing this over and over again, repeating this T times.",
            "So there are some important details that are being left out here.",
            "So first of all, how should we choose the subset of examples on each one of these rounds?",
            "And Secondly, once we've gathered all these rules of thumb, how do we combine them into a single prediction rule that will be very highly accurate?"
        ],
        [
            "OK, so again those two questions.",
            "So the first question again is how should we choose the examples on each round to train this procedure on?",
            "And here the main idea of boosting is to concentrate on the hardest examples on every round.",
            "We want to concentrate on the hard examples and what are the hard examples where the hard examples are the one which are most often misclassified by the previous rules of thumb.",
            "The ones that we got wrong the most on the previous rounds up until that point.",
            "So that's the first main idea, and the first question we have to answer, and the second question again, is, how do we?",
            "Once we've gathered all these rules of thumb, how do we combine them into a single prediction rule that will be very highly accurate?",
            "And so here we just do something simple.",
            "We just take a majority vote or a weighted majority vote of the rules of thumb so.",
            "Given a new example, we evaluate all the rules of thumb.",
            "Each one makes a prediction of what the correct label should be, and then we just take a majority vote of those predictions."
        ],
        [
            "OK, so boosting is a general method of converting these rough rules of thumb into a highly accurate prediction rule, and also I'll be talking about it also has a more technical definition which is important to keep in mind.",
            "So boosting starts out with an assumption.",
            "Really any learning algorithm starts out with an assumption.",
            "You're always assuming something about your data, otherwise learning is impossible.",
            "So in boosting we start out with what's called the weak learning assumption.",
            "We assume that we've been given a weak learning algorithm, which can consistently find these weak classifiers or what I've been calling these rules of thumb, which are at least a little bit better than random guessing.",
            "Now, if you're just talking about a two class problem, most of the talk I'll be talking about two class problems and you guess randomly.",
            "You'll be right exactly half the time, so the week learning assumption says that the accuracy of these classifiers should be a little bit better than 50%, maybe 55% something.",
            "That's a little bit better than random guessing.",
            "And given that assumption and given enough data, whatever that means, a boosting algorithm is an algorithm that can prove obli construct a single classifier.",
            "With very high accuracy with accuracy, say 99%, so that's why it's called boosting.",
            "'cause you're boosting the accuracy from 55% to 99%."
        ],
        [
            "OK, so here's what I'm going to cover in the tutorial that was by way of introduction.",
            "So I'm first going to give a very brief background on boosting that will be rather short, and then the main part of the tutorial would be in three parts.",
            "So in the first part I'm going to talk about the basic Adaboost algorithm.",
            "Adaboost was the first practical boosting algorithm and its core theory focusing on analyzing the error of the algorithm.",
            "Then the second main part is on other ways of understanding boosting.",
            "So there are many, many ways of thinking about boosting that have been developed over the years, so I'm going to be talking about three of those, and then the last part will be focused on experiments, applications and extensions.",
            "OK, and I know it's a big group, but Needless to say you know, please do ask questions."
        ],
        [
            "Especially clarifying questions.",
            "OK, so brief background."
        ],
        [
            "OK, so boosting has its roots in a theoretical machine learning model called the pack learning model.",
            "Pack stands for probably approximately correct.",
            "It was introduced by Valiant.",
            "Back in the 80s, early 80s and in this pack model, the learning algorithm gets random examples from some unknown an arbitrary distribution.",
            "And in this strong pack learning out model.",
            "A strong pack learning algorithm is 1 so that for any distribution over the examples, any distribution generating the data that is with high probability given enough examples, were enough means polynomially many examples in polynomial time.",
            "The algorithm can find a classifier with arbitrarily small generalization error.",
            "Generalization error means the air on the true distribution which is generating the data.",
            "OK, so we can drive the air as small as we want."
        ],
        [
            "And in the Week PAC Learning model or a weak PAC learning algorithm rather is 1 which.",
            "Satisfies all these same requirements, but the generalization error only has to be a little bit better than random guessing.",
            "OK, so in this model you have to be able to drive the error below 1%, and here you only have to drive the error below 45% site.",
            "Ann Kearns and Valiant were the ones who first looked at week learning the week learning model, and they asked, does weak learnability implies strong learnability.",
            "If you can learn in this model and this week model, is there some way of boosting your accuracy so that you can also learn in this seemingly much stronger learning model?"
        ],
        [
            "So the first provable boosting algorithm was one that I came up with.",
            "His part of my PhD thesis, and you are throwing came up with a better algorithm, one which is optimal in a certain sense.",
            "It's called the boost by majority algorithm, and there were some early experiments with these algorithms, but they had certain.",
            "There were certain practical reasons why these algorithms really did not work so well in practice, or were difficult to use in practice.",
            "I'll talk about that a little bit later."
        ],
        [
            "So the first practical boosting algorithm is Adaboost, which I'll be focusing on.",
            "And again, I'll talk about those practical advantages later, and since then, there's been a lot of work, experimental work and theoretical work on boosting.",
            "Obviously not going to cover all of this, but.",
            "And this list is far from exhaustive, but I'll try to cover a good part of it.",
            "OK."
        ],
        [
            "So that was a little bit of historical background.",
            "So what I want to do now is introduce Adaboost and talk about how to analyze its training error.",
            "It's error rate on the training set and then talk about how to analyze its test air.",
            "It's error on a separate test set or its generalization error using something called the margins theory."
        ],
        [
            "OK, so I already introduced boosting at a high level.",
            "What I want to do now is go back and talk about it in more formal terms and introduce some mathematical notation.",
            "So as with any learning algorithm of this kind, we start out with the training set.",
            "So there are M training examples.",
            "Each training example is a pair xiy I.",
            "So XI is the object that we're trying to classify.",
            "So for instance, it's the utterance that was said by the person in the example I gave at the beginning and Capital X is the instant space.",
            "It's the space of all possible instances.",
            "And why is the label so?",
            "For instance, it's the correct classification of one of those utterances, and to make the math simple, we're only going to focus on the two class case where there are only two possible classes, and to make the math pretty, it's nice to assume that these two possible labels are plus one or minus one.",
            "So."
        ],
        [
            "As I said, boosting works in rounds, so there are capital T rounds altogether.",
            "An on each round, little T. What the boosting algorithm does is it constructs a distribution DT over the M training examples.",
            "These are the indices of the M training examples, so this is a little bit different than what I said before.",
            "Before I said that on every round we choose a subset of the examples and now instead I'm saying that we choose a distribution over the examples and they're sort of the same thing, so a subset can be viewed as a distribution over the examples that you picked, and Conversely, if you have a distribution.",
            "You can create a subset by just sampling according to that distribution.",
            "So in any case this distribution you can think of is just a set of weights over the M examples and the weight on any particular example is kind of the importance of getting that example correct on that round of boosting OK.",
            "So once we've constructed that distribution, we go off and we try to find what I'll be calling now a weak classifier when I was calling before a rule of thumb.",
            "So a weak classifier, which I'll denote HT.",
            "Is just a prediction rule.",
            "So for any instance in our space it gives a prediction plus one or minus one of what the correct label is.",
            "And the goal in finding that we classifier is that this week classifier should have small error.",
            "So we measure.",
            "In other words, we measure the goodness of one of these weak classifiers in terms of its error, which I'll be noting epsilon T with respect to the distribution on which was trained.",
            "So epsilon T is just the probability if we choose an example at random according to the distribution on which was trained of misclassifying the example of the prediction diffring from the correct label.",
            "OK, and so we're assuming that there's some weak learning algorithm that we've been given that we can use to find these weak classifiers by training it on this distribution DT.",
            "Hey and then?"
        ],
        [
            "We get all done after we've gathered all these weak classifiers, we need to combine them into a single combined classifier or final classifier, which I'll denote each final.",
            "OK, so this is a pretty generic view of boosting.",
            "What I haven't said is how we choose the distributions and how we take all those weak classifiers and combine them into the final classifier."
        ],
        [
            "So here's how Adaboost answers those questions.",
            "Again, work with your Freund.",
            "So first of all, on the very first round, we're kind of giving equal importance to all of the training examples because we don't have any reason to.",
            "To treat one example differently than any other example, so for the first distribution, when T equals one, we just give equal weight to all of the examples there M examples and it's a distribution, so we give them each weight 1 / M."
        ],
        [
            "Non round T we already have distribution DT and suppose we found a weak classifier HT and we need to compute the new distribution for the next round DT plus one.",
            "So what Adaboost does is for each example I it takes the old wait for that example and it multiplies by a number.",
            "So if the example was correctly classified by that weak classifier, then it multiplies by E to the minus Alpha T where Alpha T is a number which will be positive.",
            "So since Alpha T is positive, even the minus Alpha T is less than one, so it's cutting the weight of an example, which is correctly classified.",
            "And if the example was incorrectly classified, then we multiply by E to the Alpha T. Since Alpha T is positive, this will be bigger than one, so we're increasing the weight of the incorrectly classified examples.",
            "OK, so we're doing something pretty simple.",
            "We're just cutting the weight of the correctly classified examples because they're easier and we're increasing the weight of the incorrectly classified examples because they're harder, so the effect will be to put more weight on the harder examples, where harder means more often misclassified.",
            "Now, this little rule that I've written up here, because the wiser plus 1 -- 1 and the weak classifiers are plus 1 -- 1, we can just write it in this nicer form down here.",
            "So after we do that, after we go through and we multiply the weights of all the examples by this number we need to end up with a distribution.",
            "So we divide by a normalization factor, which I'll denote by ZT.",
            "OK, so we just divide by Z to normalize.",
            "And Alpha T Alpha T turns.",
            "Adaboost chooses Alpha T to be this formula in terms of the epsilon tease.",
            "So for the moment all you need to know about this formula is that this number is positive."
        ],
        [
            "OK, so before talking about the final classifier, let's look at an example.",
            "So in this little tiny toy example, there are five positive examples, five negative examples.",
            "Examples are just points in this little square.",
            "The weak classifiers are vertical or horizontal half planes.",
            "So in boosting you can use anything for the week learning algorithm and for the weak classifiers.",
            "In this case we're using vertical or horizontal half planes.",
            "You'll see what I mean in a second on the first round we give equal weight to all 10 examples."
        ],
        [
            "An we passed this off to our week learning algorithm which chooses a weak classifier.",
            "So maybe it chooses this one H1 which classifieds everything to the left of this line.",
            "Positive everything to the right of the line negative.",
            "So this week classifier misclassified three of the 10 examples, those three examples that I circled up there.",
            "They each have weight .1 because there are 10 examples.",
            "So the air is .3.",
            "In this case epsilon one is .3 and if you plug into that formula you get A1 equals about .442.",
            "So on the next round what we do is we increase the weight of those three examples which were misclassified on the first round and we cut the weight of the other seven examples which were correctly classified.",
            "And then we again ask for another week."
        ],
        [
            "Passafire with respect to this distribution, so maybe this time we get this one which classifieds everything to the left positive everything to the right negative this one misclassify's these three examples.",
            "Each of those has wait about .07 under the second distribution, so the overall error rate epsilon two is about .21, and if you plug into that formula at A2 is about .65.",
            "So on the third round we do it again.",
            "We increase the weight of these three examples, and we cut the weight of the other seven examples and now you can kind of see what's going on because these four examples have very low weight.",
            "And the reason they have very low weight is because they've been correctly classified on the previous two rounds, so we don't really care about them that much because those examples are somehow easier.",
            "So if you just ignore those four examples.",
            "You can see what kind of weak classifier we're going to get on the next."
        ],
        [
            "Round we're going to get one like this H3 which classifieds everything above that line.",
            "Positive everything below the line negative this one misclassified 3 examples again and again we get these error rate and Alpha value.",
            "So now let me go back.",
            "Now that we've gathered all these."
        ],
        [
            "The classifiers, how do we take them and combine them into a single classifier?",
            "So what we do is, given a new example, X.",
            "We evaluate all of the weak classifiers.",
            "Each one makes a prediction of plus one or minus one.",
            "We wait that prediction by Alpha T. The same Alpha T we used up here.",
            "Then we add them up.",
            "We take this at this weighted sum of the predictions of the weak classifiers.",
            "If that weighted sum is positive, then we predict it's a positive example.",
            "Otherwise we predict it's negative.",
            "OK, so that's just a very long winded complicated way of saying that we're taking a weighted majority vote of the weak classifiers.",
            "That's all we're doing.",
            "We're just taking a weighted majority vote of their predictions.",
            "So going back to this example."
        ],
        [
            "Going back to this example, here's H1H2 and H3.",
            "We wait them by A1A2A3, add them together.",
            "And then we take the sign of that weighted sum, and that gives us a prediction rule like this.",
            "Which classifieds everything in this funny blue region as positive and everything in the pink region is negative.",
            "So it correctly classifieds all ten of the training examples.",
            "So in this case we used only weak classifiers, each of which missed three of the 10 examples.",
            "30% of the examples, and in three rounds of boosting we were able to drive the training air down from 30% to 0.",
            "So.",
            "So this again, I mean, this is clearly just a contrived example.",
            "And so a natural question to ask is, well, does the training error always get driven to 0 this quickly?",
            "And the answer is."
        ],
        [
            "Is yes given the weak learning assumption.",
            "So here's what we can prove about the training error.",
            "So remember epsilon T is the air of the teeth.",
            "We classifier with respect to the distribution DTN which it was trained.",
            "And all I'm going to do is take epsilon T and rewrite it as 1/2 minus gamma T. You guess randomly, you'll be right exactly half the time, so gamma T is how much better than random guessing we're doing.",
            "So think of gamma T is a small positive number.",
            "Gamma T is also called the edge.",
            "It's the edge.",
            "OK, so here's what."
        ],
        [
            "We can prove and you'll notice there are no assumptions in this theorem.",
            "There are no assumptions whatsoever in this theorem.",
            "What we can prove is that the training error of the final combined classifier is at most the product overall.",
            "Rounds T of this expression in terms of epsilon T2 Times Square root epsilon T * 1 minus epsilon T and if we plug in the fact that epsilon T = 1/2 minus gamma T, we can write it in this form and then using a simple exponential upper bound.",
            "This is at most E to the minus two times the sum of the squares of the gamma tease.",
            "So what does this mean?"
        ],
        [
            "Well, suppose the weak learning assumption holds.",
            "That means that all of the gamma Tees are at least some positive number gamma, like at least .02.",
            "For instance, if all the years are at most 48%.",
            "So that means that the training error of the combined classifier according to this found will be at most E to the minus two times gamma squared times T the number of rounds of boosting.",
            "So it's saying that the training error is going down exponentially fast in the number of rounds, so the training error is going down very very quickly.",
            "And also this also brings us to the reason that Adaboost is practical.",
            "Adaboost is practical because it's adaptive it's adaptive.",
            "In the sense that we don't need to know a lower bound gamma on all of the edges ahead of time, and we also do not need to know how many rounds we're going to run the algorithm for ahead of time.",
            "And this is an important difference from the algorithms that came before Adaboost, and also if you look at these bounds, Adaboost is taking advantage of.",
            "Edges which are bigger than the smallest edge, so this bound holds in terms of the smallest edge.",
            "But if some of the edges are bigger than this bound will be even bigger better."
        ],
        [
            "OK, so this is the only proof I'm going to give in this tutorial.",
            "OK, it's not.",
            "It's not too hard to proof, but.",
            "Well, it's a simple proof and we're going to use it later in the tutorial, which is why I'm giving it.",
            "So let me write F of X which I use this notation throughout the tutorial.",
            "Let me write F of X for the weighted sum of the weak classifiers.",
            "So remember that's the same weighted sum which is computed by the final combined classifier.",
            "The final combined classifier computes that weighted sum and then takes its sign.",
            "So if you take that recurrence which defines Adaboost, go back to it."
        ],
        [
            "We go back to this recurrence right here.",
            "We can just unwrap that recurrence, unravel that recurrence.",
            "Sorry.",
            "We can just unravel that recurrence and what we get is that the."
        ],
        [
            "Final distribution on any example, I is equal to 1 / M * E to the minus Yi times.",
            "This weighted sum divided by the product of the normalization factors.",
            "And this weighted sum again is just equal to F of XI.",
            "So we can write it in this form.",
            "OK, so step one is just unwrapping the recurrence."
        ],
        [
            "There are three steps.",
            "Step 2 is to show that the training error of the final classifiers at most the product of these normalization constants.",
            "So you might have just thought all these normalization constants.",
            "They're just there as a convenience so we can end up with the distribution they actually turn out to be key to analyzing the algorithm."
        ],
        [
            "So here's a proof of this fact.",
            "So the training error of the classifier by definition, is just the fraction of examples which are misclassified.",
            "So 1 / M times the sum of all the examples of an indicator variable, which is one that's misclassified 0 otherwise."
        ],
        [
            "Because the final classifier is the sign of F of X, and because Yi is plus one or minus one.",
            "This condition is equivalent to why I disagreeing in sign with F of XI, which is the same as their product not being positive."
        ],
        [
            "And now we can take this indicator variable and upper bound it by an exponential.",
            "It's kind of a crazy loose.",
            "Approximation, But it's true.",
            "So the exponential is always bigger than zero, and if this is negative, then E to the minus.",
            "That same thing will be bigger than one.",
            "OK, so this is an upper bound on that indicator variable.",
            "And now we can just go back to step."
        ],
        [
            "One remember, this same expression appeared in step one and is equal to this down here.",
            "If we just plug that in.",
            "And now we're just adding up over a distribution, so the sum over a distribution is 1."
        ],
        [
            "Just get product of ziti.",
            "OK, in the third step we try."
        ],
        [
            "The upper bound.",
            "The training air."
        ],
        [
            "We already have a bound on the training here in terms of the product of the normalization factors."
        ],
        [
            "So now it turns out we can just compute the normalization constants.",
            "The normalization factors directly exactly."
        ],
        [
            "So to see this, we just write down the definition of the normalization constant.",
            "This is its definition.",
            "This is what you need to set it to so that the sum of the weights will be equal to 1.",
            "We can take this sum into two parts over the examples which are misclassified and correctly classified.",
            "And now if you just look at this sum.",
            "This is just the weighted air of the weak classifier, which is epsilon T, and likewise this sum is 1 minus epsilon T. And now if you plug in that choice of Alpha T, you get exactly this expression.",
            "So now I can answer the question where did this choice of Alpha T come from?",
            "Alpha T was chosen so as to minimize this expression.",
            "We want the tea to be small because ZTE, the product of the zipties, has an upper bound on the training error.",
            "So we choose Alpha T in this expression to make it as small as possible, and that gives the choice of Alpha T used by Adaboost, and when you plug it in, you get this expression.",
            "OK, so people are being extremely quiet."
        ],
        [
            "So please do interrupt me with questions.",
            "I know it's kind of scary because partly it's a big group and partly I'm way up here on this high stage so you know.",
            "Seems like I'll hit you with a Bolt of lightning or something if you ask a question, but I won't.",
            "OK. Alright, so here."
        ],
        [
            "I'll ask myself a question.",
            "So you just proved this theorem on training error.",
            "Who cares about training here?",
            "That's not what learning is about.",
            "Learning is about minimizing the test there, right?",
            "So what can you say about the tester?",
            "That's a great question."
        ],
        [
            "Actually usually say that's a great question.",
            "Once question, I have no idea what the answer is to, but OK, OK, so how do we expect the test error to behave?",
            "So here's a cartoon where I'm plotting the air of the combined classifier.",
            "The final combined classifier, not the weak classifiers as a function of the number of Browns as a function of capital T. So we just proved this theorem that says that the training error goes down exponentially fast, so we expect the training error to go down very very quickly exponentially fast.",
            "Now, what about the test there?",
            "Well, at 1st at first we're doing a better job of fitting the training data, so we expect the test air to also be dropping.",
            "But every time we run another round of boosting we add one new weak classifier to the combined classifier to the final classifier.",
            "So that combined classifier is getting bigger and bigger and more and more complicated.",
            "So at some point we expect overfitting to set, and we expect this test there.",
            "To start going up again, we expect to see overfitting because the combined classifier, like I said is becoming bigger and bigger and more and more complex.",
            "This is I'm sure you've heard about this in other tutorials.",
            "This is sometimes called Occam's razor.",
            "Occam's Razor is the idea that you have two explanations, all else being equal, you should prefer the simpler explanation, and in learning, that means that given 2.",
            "Classifiers which perform equally equally well on your training data, all else being equal, you should expect the simpler one to give better predictions.",
            "Now, over fitting is a huge problem.",
            "It's a big problem and it comes up all the time in machine learning.",
            "So in a case like this you really want to stop training right about here to get that best test error rate.",
            "But your according to your training set performance is getting better.",
            "That's why it's called overfitting, 'cause things seem to be getting better.",
            "You seem to be fitting the data better, when in fact things are getting worse and it makes it very hard to decide when to stop training.",
            "So that's what we expect."
        ],
        [
            "Here's an actual typical run, so this is using boosting on top of C 4.5, which is a decision tree learning algorithm on a benchmark data set called the letter data set, and again, we're plotting the air of the final combined classifier as a function of the number of rounds, but I've switched to a log rhythmic scale here.",
            "If you just run C 4.5 by itself, it gets a test error rate of about 13%.",
            "That's that line way up at the top there.",
            "As expected, the training error goes down very quickly.",
            "In fact, after only five rounds, the training air is zero.",
            "We perfectly fit the training set.",
            "What about the test air?",
            "Well, the test air also drops as expected, but it keeps on dropping, dropping, dropping, dropping even after 1000 rounds.",
            "The test air has not started to go up again, and this is kind of amazing because.",
            "After you've run this for 1000 rounds, you're talking about some more than two million decision tree nodes.",
            "So in terms of just raw number of parameters, this model is absolutely enormous.",
            "An extremely complicated and yet it's giving very very good performance, as we see over here.",
            "Now it's even more surprising is that the test error is continuing to drop even after the training error is 0.",
            "So if you look at the training error after five rounds, the training here is 0 and it stays at zero.",
            "Now if you think about what Occam's Razor predicts here, you have one model which consists of five decision trees.",
            "Perfect accuracy on the training set.",
            "Here's another model.",
            "It's 200 times bigger.",
            "It's 1000 decision trees also perfect accuracy on the training set.",
            "This one is 200 times more complex.",
            "Occam's Razor says this one should certainly give better test performance, but we see exactly the opposite happening.",
            "Instead, the tester is 8.4% here and about 3% here.",
            "So Occam's razor just seems wrong in this case.",
            "How could a question?",
            "I don't know how you measure that it's real data.",
            "This is actually.",
            "Well, it's data that's derived from OCR data.",
            "Does the article noted right out of college?",
            "The Oracle the.",
            "Right?",
            "Well, they're supposed to be the right answers, but usually with any training set, there are almost always some mistakes of some kind, so it's so nobody has gone in and deliberately added noise if that's what you're asking.",
            "But because it's real data, I would expect that there probably there might well be mistakes in it, or some ambiguous examples in it, but I don't know for sure.",
            "If it was a lot of noise.",
            "What order to organize that?",
            "He would make an order, but it is a right.",
            "All of them are right then after you get the desired amount of complexity would expected by the level of yeah, right?",
            "Well, so your intuition is right.",
            "So if you do an experiment I was going to talk about this later, but if you do an experiment where you deliberately add noise to the data.",
            "When you add that noise uniformly over your entire space.",
            "Then then that does cause problems for Adaboost, because what it does like you were getting at is that it focuses on the hardest example.",
            "So it's really just spinning its wheels on these really hard examples, and so that does happen, and there's been various research on how to try to make it more resistant to that kind of noise, more robust to that kind of noise.",
            "But on the other hand, you know that's an artificial experiment where you artificially add noise.",
            "On the other hand, on actual datasets.",
            "Which are certainly have some kind of noise in them.",
            "It's performing well, which suggests to me that our model of noise is not very good.",
            "The model of noise that says you have equal noise everywhere in your space probably is not realistic.",
            "Probably what's more realistic is that there's some examples near the true decision boundary, whatever that means.",
            "Which are more likely to be more susceptible to noise than ones which are far?",
            "Yeah.",
            "Yeah.",
            "OK epsilon T epsilon T is the air of the weak classifiers OK?",
            "The weak classifiers and this is the error of the combined classifier.",
            "So even after the combined classifier has training error zero.",
            "We classifiers.",
            "Still have significant air, so in this case the weak classifiers after you run this a few times, the weak classifiers will all have errors somewhere around 30%.",
            "OK, so that stays large so you can continue re weighting the examples and so on.",
            "Yeah.",
            "Works the same way with other classification methods other than decision tree.",
            "While neural networks neural networks are quite susceptible to overfitting, decision tree algorithms are susceptible to overfitting so.",
            "It depends, yeah, so other ensemble there are other ensemble algorithms which do not overfit.",
            "So for instance there's bagging and random forests.",
            "They will also tend not to overfit.",
            "You'll tend to get behaviors somewhat like this.",
            "So it depends on the algorithm, but but this?",
            "Does that answer your question?",
            "Oh, I see two different weak classifiers exhibit this behavior, I see.",
            "So.",
            "I believe with neural networks you'll tend to see this kind of behavior with.",
            "Actually, can I come back to that question?",
            "That is a good question, but can I come back to that question so I'm going to talk a little bit more about when this desert does not happen.",
            "OK."
        ],
        [
            "OK, good so.",
            "So this seems to be a paradox.",
            "At least it seems on the face of it, contradict Occam's razor.",
            "So here's a different explanation called the margins explanation.",
            "Work with you operating Peter Bartlett and we suddenly.",
            "So the basic idea is that the training error, which is all we were looking at, does not tell us the whole story.",
            "The training error only measures whether the classifications are right or wrong.",
            "So what we also should be looking at are the confidences of the classifications.",
            "So the idea of this explanation is that yes, after five rounds of boosting the training error zero and it stays at 0, nothing seems to be happening.",
            "But in fact after the training error zero the confidences.",
            "And the predictions that are being made by the combined classifier are increasing.",
            "It's becoming more confident in its own predictions and that that increase in confidence is leading to better performance on the test set.",
            "OK, so that's nice."
        ],
        [
            "And fuzzy to turn it into something precise, we need a way of measuring confidence.",
            "So how do we measure confidence in the predictions made by these combined classifier?",
            "Well, remember that this combined classifier is a weighted majority vote of the weak classifiers.",
            "It's voting the predictions of the weak classifiers, so it's like it takes an election every time it has to make a prediction.",
            "It takes an election among the weak classifiers.",
            "So in an ordinary election, how do we measure confidence in the outcome of an election?",
            "Well, if you think back state of Florida in 2000 or two Minnesota in the Senate race, which is still being argued about the problem there is that people have very low confidence in the outcome of the election because the margin was so low, the margin, the difference in the number of votes received by one candidate over the other candidate was so small.",
            "So we can measure confidence in the same way."
        ],
        [
            "We can measure confidence using that same margin.",
            "The strength of the vote, which is just the fraction.",
            "Or here we have take the weighted fraction of the weak classifiers voting for the correct label minus the weighted fraction.",
            "Voting for the incorrect label.",
            "So that's called the margin, and that's the measure of confidence that we use.",
            "So because this is the difference of two difference of two fractions.",
            "The margin will always be a real number between minus one and plus one.",
            "Margin.",
            "If the margin is positive, that means more of the weak classifiers were voting correctly than voting incorrectly.",
            "So when you take that majority vote that which is what the final classifier is doing, you'll get the correct prediction.",
            "So the margin is positive if and only if the final classifier is giving a correct prediction on that example and the absolute value the magnitude of the margin.",
            "That's what's really measuring the confidence.",
            "So if the margin is close to 0.",
            "So the outcome of the election is very close.",
            "Then we think of that as a low confidence prediction, and if it's rather high, if it's way out here or way out here, we regard that as a high confidence prediction, which might be correct.",
            "Or it might be incorrect."
        ],
        [
            "OK, So what is the evidence that margins have something to do with Adaboost performance?",
            "So first of all, there's empirical evidence.",
            "So we can look at specific datasets and we can look at the distribution of margins on the training set.",
            "So here's that same curve I had before with the training Aaron Test there for that same data set, and what we can do is we can plot the cumulative distribution margins on the training set.",
            "Remember, every example has a margin, so if you look at a data set then you get a distribution of margins and this is a plot of the cumulative distribution, which means that.",
            "Where the slope is steepest, that's where the most examples are.",
            "So this is plotting cumulative distribution after five rounds, 100 rounds which you can't see very well and 1000 rounds.",
            "And what we see is that as boosting continues, the training error remains at zero.",
            "But the these examples with relatively low margin are being pushed to the right.",
            "So this is saying that many examples have margins in here and way out here.",
            "But after 100 or 1000 rounds, nearly all of them have margin over in here.",
            "Or you can look at the numbers down here.",
            "Training Aaron Test area had before.",
            "If you look at the fraction of examples with margin below point.",
            "Five that's at this point.",
            "About 7.7% of them have margin below .5 after five rounds, but after 100 rounds, all of the examples have margin above .5 OK. And similarly you could look at the example with the smallest margin which is about .1 four after five rounds.",
            "After 100 rounds it's up to .5 two and then .55.",
            "So the claim is it's here.",
            "It's an empirical claim, not a theoretical claim.",
            "It's an empirical claim that this increase in margins.",
            "Is correlated with the continuing improvement in the test performance.",
            "So there's also."
        ],
        [
            "Theoretical evidence for the margins.",
            "Explanation of boosting.",
            "So we can analyze boosting in terms of these margins.",
            "So first of all, we can prove that the bigger the margins on the training set, the better the bound we can prove on the generalization error, and that bound is independent of the number of rounds of boosting.",
            "So this bound this theorem says that it doesn't matter how many rounds of boosting you run for, it doesn't matter how big capital T is, what matters is the margins on that."
        ],
        [
            "Training set.",
            "So the way we prove this is actually if you think about elections.",
            "Think about any election where you have maybe 100 million voters.",
            "Even if you have a huge huge electorate with a huge number of voters.",
            "It's possible to predict the outcome of the election simply by taking a survey simply by calling up 1000 random voters and asking them who they're going to vote for.",
            "But that only works if the margin is large.",
            "If the election is very close, then that won't tell you who's going to win the election.",
            "You need a large margin for that to work, so the same idea works here.",
            "So if all of the margins are large, then we can approximate the final classifier, which is an election over this huge electorate.",
            "By a much smaller classifier just by taking a random sample from that set of classifiers, weak classifiers.",
            "So that."
        ],
        [
            "The first part of the analysis.",
            "The other part of the analysis says that it's not just a coincidence what we saw in the last."
        ],
        [
            "Slide, it's not just a coincidence that these margins are being pushed to the right, in fact."
        ],
        [
            "We can prove that boosting tends to increase the margins of all of the training examples given the weak learning assumption, and in fact the larger the edges, the larger the edges of the weak classifiers.",
            "In other words, the better they are compared to random guessing, the larger will be these margins and the."
        ],
        [
            "Can be proved in a similar way to the training error proof that I gave, yeah?",
            "1st.",
            "OK so it is OK so up here we're assuming that the data is random, that we're getting IID data, so the training examples and the test examples are coming from the same data set.",
            "And then the bound that we prove is in terms of the distribution of the margins, the complexity of the weak classifiers themselves.",
            "And the number of training examples.",
            "Unlimited what?",
            "Oh well, the bound isn't."
        ],
        [
            "Let's say."
        ],
        [
            "I have a slide for you.",
            "OK, So what we can prove is that the generalization error is at most the fraction of training examples with margin below any level Theta that you pick plus some additional term, which is something like square root D / M, where D is the complexity of the weak classifiers, not the combined classifier.",
            "The weak classifiers divided by M, the number of training examples and Theta is your chosen margin level OK?"
        ],
        [
            "OK, so.",
            "Right, so putting this argument together, what we're saying is that the final classifier is getting bigger and more complex, but as that happens the margins are likely to be increasing by the 2nd theorem, so the final classifier is actually getting closer to a simpler classifier.",
            "By this argument up here.",
            "And that's actually driving down the tester.",
            "OK, so."
        ],
        [
            "OK, so this is a good place to stop.",
            "So why don't we take so?",
            "Let's say I notice that if people if they say 15 minutes then people are going 25 minutes and so let's say 10 minutes and then I know that will actually be 15 minutes.",
            "Does that sound good?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is our.",
                    "label": 0
                },
                {
                    "sent": "5th morning of tutorials and we're very happy to have Rob Shapiro.",
                    "label": 0
                },
                {
                    "sent": "And we'll talk about boosting.",
                    "label": 0
                },
                {
                    "sent": "Thanks Martha.",
                    "label": 0
                },
                {
                    "sent": "Alright, well good morning so thank you for getting up early to see a tutorial on boosting.",
                    "label": 0
                },
                {
                    "sent": "So this is on theory and applications of boosting a little bit more theory than applications, but there.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Both.",
                    "label": 0
                },
                {
                    "sent": "So this is a tutorial for people who know little or nothing about boosting.",
                    "label": 0
                },
                {
                    "sent": "So let me start at the beginning with a typical kind of problem.",
                    "label": 0
                },
                {
                    "sent": "This is a problem I worked on when I was at AT&T.",
                    "label": 0
                },
                {
                    "sent": "So the problem here is to automatically categorize the type of call requested by a phone customer.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that somebody would call up AT&T and maybe they want to make a collect call or a third number call or whatever it is.",
                    "label": 0
                },
                {
                    "sent": "And the idea was to build a system where people could just speak naturally to this system, rather than if you want this, press one if you want that, press two and so on.",
                    "label": 0
                },
                {
                    "sent": "So when the person says something, we need, the computer needs to be able to categorize what kind of requests the person is making.",
                    "label": 0
                },
                {
                    "sent": "So if the person says yes, I'd like to place a collect call long distance please, that's a collect call I discovered recently that since everybody's cell phones Now there are a lot of.",
                    "label": 1
                },
                {
                    "sent": "People younger than me who don't know what a collect call is in a third number call, and so on.",
                    "label": 0
                },
                {
                    "sent": "So these are just kinds of calls.",
                    "label": 0
                },
                {
                    "sent": "That's all you need to know.",
                    "label": 0
                },
                {
                    "sent": "In the old days you would make a collect call and a third number calling someone.",
                    "label": 1
                },
                {
                    "sent": "So if the person says operator, I need to make a call.",
                    "label": 1
                },
                {
                    "sent": "But I need to bill it to my office that's being built to 1/3 number.",
                    "label": 0
                },
                {
                    "sent": "So that's called 1/3 number call and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you think about the problem like this, if you just look at examples like this.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then you immediately start to realize that there are rules of thumb that will often be correct.",
                    "label": 1
                },
                {
                    "sent": "So for instance, if you just look at that very first example, I have a pointer.",
                    "label": 0
                },
                {
                    "sent": "Is this long enough?",
                    "label": 0
                },
                {
                    "sent": "OK, if you just look at this very first example.",
                    "label": 0
                },
                {
                    "sent": "You know you can immediately think it well if the word collect appears in what was said, then predict that's a collect call.",
                    "label": 1
                },
                {
                    "sent": "Or if the word card appears in what was said, predict it's a calling card call and so on, so you can immediately start thinking about these rules of thumb, which will be pretty good.",
                    "label": 0
                },
                {
                    "sent": "They're not going to be perfect, but they should be better than just guessing at random.",
                    "label": 1
                },
                {
                    "sent": "What's harder is to find just a single prediction rule, which will be very highly accurate.",
                    "label": 0
                },
                {
                    "sent": "So the idea of boosting is to somehow use these weak rules of thumb in combination somehow, so that when you put them together you end up with a prediction rule, which is very highly accurate.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's the boosting approach at a very high level.",
                    "label": 1
                },
                {
                    "sent": "I'll certainly be going into more detail, but at a very high level.",
                    "label": 0
                },
                {
                    "sent": "Here's the idea of boosting.",
                    "label": 1
                },
                {
                    "sent": "So since these rules of thumb are not so hard to find, we could think about coming up with a computer program that could look at data and derive rules of thumb from that data.",
                    "label": 0
                },
                {
                    "sent": "Then we could apply that procedure to a subset of the examples that would give us a first rule of thumb.",
                    "label": 1
                },
                {
                    "sent": "Then we could do this again, choose a second subset of the examples, get a second rule of thumb, and you could imagine doing this over and over again, repeating this T times.",
                    "label": 0
                },
                {
                    "sent": "So there are some important details that are being left out here.",
                    "label": 0
                },
                {
                    "sent": "So first of all, how should we choose the subset of examples on each one of these rounds?",
                    "label": 0
                },
                {
                    "sent": "And Secondly, once we've gathered all these rules of thumb, how do we combine them into a single prediction rule that will be very highly accurate?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so again those two questions.",
                    "label": 0
                },
                {
                    "sent": "So the first question again is how should we choose the examples on each round to train this procedure on?",
                    "label": 1
                },
                {
                    "sent": "And here the main idea of boosting is to concentrate on the hardest examples on every round.",
                    "label": 0
                },
                {
                    "sent": "We want to concentrate on the hard examples and what are the hard examples where the hard examples are the one which are most often misclassified by the previous rules of thumb.",
                    "label": 1
                },
                {
                    "sent": "The ones that we got wrong the most on the previous rounds up until that point.",
                    "label": 0
                },
                {
                    "sent": "So that's the first main idea, and the first question we have to answer, and the second question again, is, how do we?",
                    "label": 1
                },
                {
                    "sent": "Once we've gathered all these rules of thumb, how do we combine them into a single prediction rule that will be very highly accurate?",
                    "label": 0
                },
                {
                    "sent": "And so here we just do something simple.",
                    "label": 1
                },
                {
                    "sent": "We just take a majority vote or a weighted majority vote of the rules of thumb so.",
                    "label": 0
                },
                {
                    "sent": "Given a new example, we evaluate all the rules of thumb.",
                    "label": 0
                },
                {
                    "sent": "Each one makes a prediction of what the correct label should be, and then we just take a majority vote of those predictions.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so boosting is a general method of converting these rough rules of thumb into a highly accurate prediction rule, and also I'll be talking about it also has a more technical definition which is important to keep in mind.",
                    "label": 1
                },
                {
                    "sent": "So boosting starts out with an assumption.",
                    "label": 0
                },
                {
                    "sent": "Really any learning algorithm starts out with an assumption.",
                    "label": 0
                },
                {
                    "sent": "You're always assuming something about your data, otherwise learning is impossible.",
                    "label": 0
                },
                {
                    "sent": "So in boosting we start out with what's called the weak learning assumption.",
                    "label": 1
                },
                {
                    "sent": "We assume that we've been given a weak learning algorithm, which can consistently find these weak classifiers or what I've been calling these rules of thumb, which are at least a little bit better than random guessing.",
                    "label": 0
                },
                {
                    "sent": "Now, if you're just talking about a two class problem, most of the talk I'll be talking about two class problems and you guess randomly.",
                    "label": 0
                },
                {
                    "sent": "You'll be right exactly half the time, so the week learning assumption says that the accuracy of these classifiers should be a little bit better than 50%, maybe 55% something.",
                    "label": 1
                },
                {
                    "sent": "That's a little bit better than random guessing.",
                    "label": 1
                },
                {
                    "sent": "And given that assumption and given enough data, whatever that means, a boosting algorithm is an algorithm that can prove obli construct a single classifier.",
                    "label": 0
                },
                {
                    "sent": "With very high accuracy with accuracy, say 99%, so that's why it's called boosting.",
                    "label": 0
                },
                {
                    "sent": "'cause you're boosting the accuracy from 55% to 99%.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's what I'm going to cover in the tutorial that was by way of introduction.",
                    "label": 0
                },
                {
                    "sent": "So I'm first going to give a very brief background on boosting that will be rather short, and then the main part of the tutorial would be in three parts.",
                    "label": 0
                },
                {
                    "sent": "So in the first part I'm going to talk about the basic Adaboost algorithm.",
                    "label": 0
                },
                {
                    "sent": "Adaboost was the first practical boosting algorithm and its core theory focusing on analyzing the error of the algorithm.",
                    "label": 1
                },
                {
                    "sent": "Then the second main part is on other ways of understanding boosting.",
                    "label": 1
                },
                {
                    "sent": "So there are many, many ways of thinking about boosting that have been developed over the years, so I'm going to be talking about three of those, and then the last part will be focused on experiments, applications and extensions.",
                    "label": 0
                },
                {
                    "sent": "OK, and I know it's a big group, but Needless to say you know, please do ask questions.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Especially clarifying questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so brief background.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so boosting has its roots in a theoretical machine learning model called the pack learning model.",
                    "label": 0
                },
                {
                    "sent": "Pack stands for probably approximately correct.",
                    "label": 0
                },
                {
                    "sent": "It was introduced by Valiant.",
                    "label": 0
                },
                {
                    "sent": "Back in the 80s, early 80s and in this pack model, the learning algorithm gets random examples from some unknown an arbitrary distribution.",
                    "label": 0
                },
                {
                    "sent": "And in this strong pack learning out model.",
                    "label": 0
                },
                {
                    "sent": "A strong pack learning algorithm is 1 so that for any distribution over the examples, any distribution generating the data that is with high probability given enough examples, were enough means polynomially many examples in polynomial time.",
                    "label": 1
                },
                {
                    "sent": "The algorithm can find a classifier with arbitrarily small generalization error.",
                    "label": 0
                },
                {
                    "sent": "Generalization error means the air on the true distribution which is generating the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can drive the air as small as we want.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in the Week PAC Learning model or a weak PAC learning algorithm rather is 1 which.",
                    "label": 1
                },
                {
                    "sent": "Satisfies all these same requirements, but the generalization error only has to be a little bit better than random guessing.",
                    "label": 1
                },
                {
                    "sent": "OK, so in this model you have to be able to drive the error below 1%, and here you only have to drive the error below 45% site.",
                    "label": 0
                },
                {
                    "sent": "Ann Kearns and Valiant were the ones who first looked at week learning the week learning model, and they asked, does weak learnability implies strong learnability.",
                    "label": 0
                },
                {
                    "sent": "If you can learn in this model and this week model, is there some way of boosting your accuracy so that you can also learn in this seemingly much stronger learning model?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first provable boosting algorithm was one that I came up with.",
                    "label": 1
                },
                {
                    "sent": "His part of my PhD thesis, and you are throwing came up with a better algorithm, one which is optimal in a certain sense.",
                    "label": 0
                },
                {
                    "sent": "It's called the boost by majority algorithm, and there were some early experiments with these algorithms, but they had certain.",
                    "label": 0
                },
                {
                    "sent": "There were certain practical reasons why these algorithms really did not work so well in practice, or were difficult to use in practice.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about that a little bit later.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first practical boosting algorithm is Adaboost, which I'll be focusing on.",
                    "label": 0
                },
                {
                    "sent": "And again, I'll talk about those practical advantages later, and since then, there's been a lot of work, experimental work and theoretical work on boosting.",
                    "label": 0
                },
                {
                    "sent": "Obviously not going to cover all of this, but.",
                    "label": 0
                },
                {
                    "sent": "And this list is far from exhaustive, but I'll try to cover a good part of it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that was a little bit of historical background.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do now is introduce Adaboost and talk about how to analyze its training error.",
                    "label": 1
                },
                {
                    "sent": "It's error rate on the training set and then talk about how to analyze its test air.",
                    "label": 0
                },
                {
                    "sent": "It's error on a separate test set or its generalization error using something called the margins theory.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I already introduced boosting at a high level.",
                    "label": 0
                },
                {
                    "sent": "What I want to do now is go back and talk about it in more formal terms and introduce some mathematical notation.",
                    "label": 0
                },
                {
                    "sent": "So as with any learning algorithm of this kind, we start out with the training set.",
                    "label": 0
                },
                {
                    "sent": "So there are M training examples.",
                    "label": 0
                },
                {
                    "sent": "Each training example is a pair xiy I.",
                    "label": 0
                },
                {
                    "sent": "So XI is the object that we're trying to classify.",
                    "label": 0
                },
                {
                    "sent": "So for instance, it's the utterance that was said by the person in the example I gave at the beginning and Capital X is the instant space.",
                    "label": 0
                },
                {
                    "sent": "It's the space of all possible instances.",
                    "label": 0
                },
                {
                    "sent": "And why is the label so?",
                    "label": 0
                },
                {
                    "sent": "For instance, it's the correct classification of one of those utterances, and to make the math simple, we're only going to focus on the two class case where there are only two possible classes, and to make the math pretty, it's nice to assume that these two possible labels are plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I said, boosting works in rounds, so there are capital T rounds altogether.",
                    "label": 0
                },
                {
                    "sent": "An on each round, little T. What the boosting algorithm does is it constructs a distribution DT over the M training examples.",
                    "label": 1
                },
                {
                    "sent": "These are the indices of the M training examples, so this is a little bit different than what I said before.",
                    "label": 0
                },
                {
                    "sent": "Before I said that on every round we choose a subset of the examples and now instead I'm saying that we choose a distribution over the examples and they're sort of the same thing, so a subset can be viewed as a distribution over the examples that you picked, and Conversely, if you have a distribution.",
                    "label": 0
                },
                {
                    "sent": "You can create a subset by just sampling according to that distribution.",
                    "label": 0
                },
                {
                    "sent": "So in any case this distribution you can think of is just a set of weights over the M examples and the weight on any particular example is kind of the importance of getting that example correct on that round of boosting OK.",
                    "label": 0
                },
                {
                    "sent": "So once we've constructed that distribution, we go off and we try to find what I'll be calling now a weak classifier when I was calling before a rule of thumb.",
                    "label": 1
                },
                {
                    "sent": "So a weak classifier, which I'll denote HT.",
                    "label": 0
                },
                {
                    "sent": "Is just a prediction rule.",
                    "label": 0
                },
                {
                    "sent": "So for any instance in our space it gives a prediction plus one or minus one of what the correct label is.",
                    "label": 1
                },
                {
                    "sent": "And the goal in finding that we classifier is that this week classifier should have small error.",
                    "label": 0
                },
                {
                    "sent": "So we measure.",
                    "label": 0
                },
                {
                    "sent": "In other words, we measure the goodness of one of these weak classifiers in terms of its error, which I'll be noting epsilon T with respect to the distribution on which was trained.",
                    "label": 0
                },
                {
                    "sent": "So epsilon T is just the probability if we choose an example at random according to the distribution on which was trained of misclassifying the example of the prediction diffring from the correct label.",
                    "label": 0
                },
                {
                    "sent": "OK, and so we're assuming that there's some weak learning algorithm that we've been given that we can use to find these weak classifiers by training it on this distribution DT.",
                    "label": 0
                },
                {
                    "sent": "Hey and then?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We get all done after we've gathered all these weak classifiers, we need to combine them into a single combined classifier or final classifier, which I'll denote each final.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a pretty generic view of boosting.",
                    "label": 0
                },
                {
                    "sent": "What I haven't said is how we choose the distributions and how we take all those weak classifiers and combine them into the final classifier.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's how Adaboost answers those questions.",
                    "label": 0
                },
                {
                    "sent": "Again, work with your Freund.",
                    "label": 0
                },
                {
                    "sent": "So first of all, on the very first round, we're kind of giving equal importance to all of the training examples because we don't have any reason to.",
                    "label": 0
                },
                {
                    "sent": "To treat one example differently than any other example, so for the first distribution, when T equals one, we just give equal weight to all of the examples there M examples and it's a distribution, so we give them each weight 1 / M.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Non round T we already have distribution DT and suppose we found a weak classifier HT and we need to compute the new distribution for the next round DT plus one.",
                    "label": 0
                },
                {
                    "sent": "So what Adaboost does is for each example I it takes the old wait for that example and it multiplies by a number.",
                    "label": 0
                },
                {
                    "sent": "So if the example was correctly classified by that weak classifier, then it multiplies by E to the minus Alpha T where Alpha T is a number which will be positive.",
                    "label": 0
                },
                {
                    "sent": "So since Alpha T is positive, even the minus Alpha T is less than one, so it's cutting the weight of an example, which is correctly classified.",
                    "label": 0
                },
                {
                    "sent": "And if the example was incorrectly classified, then we multiply by E to the Alpha T. Since Alpha T is positive, this will be bigger than one, so we're increasing the weight of the incorrectly classified examples.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're doing something pretty simple.",
                    "label": 0
                },
                {
                    "sent": "We're just cutting the weight of the correctly classified examples because they're easier and we're increasing the weight of the incorrectly classified examples because they're harder, so the effect will be to put more weight on the harder examples, where harder means more often misclassified.",
                    "label": 0
                },
                {
                    "sent": "Now, this little rule that I've written up here, because the wiser plus 1 -- 1 and the weak classifiers are plus 1 -- 1, we can just write it in this nicer form down here.",
                    "label": 0
                },
                {
                    "sent": "So after we do that, after we go through and we multiply the weights of all the examples by this number we need to end up with a distribution.",
                    "label": 0
                },
                {
                    "sent": "So we divide by a normalization factor, which I'll denote by ZT.",
                    "label": 0
                },
                {
                    "sent": "OK, so we just divide by Z to normalize.",
                    "label": 0
                },
                {
                    "sent": "And Alpha T Alpha T turns.",
                    "label": 0
                },
                {
                    "sent": "Adaboost chooses Alpha T to be this formula in terms of the epsilon tease.",
                    "label": 0
                },
                {
                    "sent": "So for the moment all you need to know about this formula is that this number is positive.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so before talking about the final classifier, let's look at an example.",
                    "label": 0
                },
                {
                    "sent": "So in this little tiny toy example, there are five positive examples, five negative examples.",
                    "label": 0
                },
                {
                    "sent": "Examples are just points in this little square.",
                    "label": 0
                },
                {
                    "sent": "The weak classifiers are vertical or horizontal half planes.",
                    "label": 0
                },
                {
                    "sent": "So in boosting you can use anything for the week learning algorithm and for the weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "In this case we're using vertical or horizontal half planes.",
                    "label": 0
                },
                {
                    "sent": "You'll see what I mean in a second on the first round we give equal weight to all 10 examples.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An we passed this off to our week learning algorithm which chooses a weak classifier.",
                    "label": 0
                },
                {
                    "sent": "So maybe it chooses this one H1 which classifieds everything to the left of this line.",
                    "label": 0
                },
                {
                    "sent": "Positive everything to the right of the line negative.",
                    "label": 0
                },
                {
                    "sent": "So this week classifier misclassified three of the 10 examples, those three examples that I circled up there.",
                    "label": 0
                },
                {
                    "sent": "They each have weight .1 because there are 10 examples.",
                    "label": 0
                },
                {
                    "sent": "So the air is .3.",
                    "label": 0
                },
                {
                    "sent": "In this case epsilon one is .3 and if you plug into that formula you get A1 equals about .442.",
                    "label": 0
                },
                {
                    "sent": "So on the next round what we do is we increase the weight of those three examples which were misclassified on the first round and we cut the weight of the other seven examples which were correctly classified.",
                    "label": 0
                },
                {
                    "sent": "And then we again ask for another week.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Passafire with respect to this distribution, so maybe this time we get this one which classifieds everything to the left positive everything to the right negative this one misclassify's these three examples.",
                    "label": 0
                },
                {
                    "sent": "Each of those has wait about .07 under the second distribution, so the overall error rate epsilon two is about .21, and if you plug into that formula at A2 is about .65.",
                    "label": 0
                },
                {
                    "sent": "So on the third round we do it again.",
                    "label": 0
                },
                {
                    "sent": "We increase the weight of these three examples, and we cut the weight of the other seven examples and now you can kind of see what's going on because these four examples have very low weight.",
                    "label": 0
                },
                {
                    "sent": "And the reason they have very low weight is because they've been correctly classified on the previous two rounds, so we don't really care about them that much because those examples are somehow easier.",
                    "label": 0
                },
                {
                    "sent": "So if you just ignore those four examples.",
                    "label": 0
                },
                {
                    "sent": "You can see what kind of weak classifier we're going to get on the next.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Round we're going to get one like this H3 which classifieds everything above that line.",
                    "label": 0
                },
                {
                    "sent": "Positive everything below the line negative this one misclassified 3 examples again and again we get these error rate and Alpha value.",
                    "label": 0
                },
                {
                    "sent": "So now let me go back.",
                    "label": 0
                },
                {
                    "sent": "Now that we've gathered all these.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The classifiers, how do we take them and combine them into a single classifier?",
                    "label": 0
                },
                {
                    "sent": "So what we do is, given a new example, X.",
                    "label": 0
                },
                {
                    "sent": "We evaluate all of the weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "Each one makes a prediction of plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "We wait that prediction by Alpha T. The same Alpha T we used up here.",
                    "label": 0
                },
                {
                    "sent": "Then we add them up.",
                    "label": 0
                },
                {
                    "sent": "We take this at this weighted sum of the predictions of the weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "If that weighted sum is positive, then we predict it's a positive example.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we predict it's negative.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just a very long winded complicated way of saying that we're taking a weighted majority vote of the weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "That's all we're doing.",
                    "label": 0
                },
                {
                    "sent": "We're just taking a weighted majority vote of their predictions.",
                    "label": 0
                },
                {
                    "sent": "So going back to this example.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going back to this example, here's H1H2 and H3.",
                    "label": 0
                },
                {
                    "sent": "We wait them by A1A2A3, add them together.",
                    "label": 0
                },
                {
                    "sent": "And then we take the sign of that weighted sum, and that gives us a prediction rule like this.",
                    "label": 0
                },
                {
                    "sent": "Which classifieds everything in this funny blue region as positive and everything in the pink region is negative.",
                    "label": 0
                },
                {
                    "sent": "So it correctly classifieds all ten of the training examples.",
                    "label": 0
                },
                {
                    "sent": "So in this case we used only weak classifiers, each of which missed three of the 10 examples.",
                    "label": 0
                },
                {
                    "sent": "30% of the examples, and in three rounds of boosting we were able to drive the training air down from 30% to 0.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this again, I mean, this is clearly just a contrived example.",
                    "label": 0
                },
                {
                    "sent": "And so a natural question to ask is, well, does the training error always get driven to 0 this quickly?",
                    "label": 0
                },
                {
                    "sent": "And the answer is.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is yes given the weak learning assumption.",
                    "label": 0
                },
                {
                    "sent": "So here's what we can prove about the training error.",
                    "label": 1
                },
                {
                    "sent": "So remember epsilon T is the air of the teeth.",
                    "label": 0
                },
                {
                    "sent": "We classifier with respect to the distribution DTN which it was trained.",
                    "label": 0
                },
                {
                    "sent": "And all I'm going to do is take epsilon T and rewrite it as 1/2 minus gamma T. You guess randomly, you'll be right exactly half the time, so gamma T is how much better than random guessing we're doing.",
                    "label": 0
                },
                {
                    "sent": "So think of gamma T is a small positive number.",
                    "label": 0
                },
                {
                    "sent": "Gamma T is also called the edge.",
                    "label": 0
                },
                {
                    "sent": "It's the edge.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's what.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can prove and you'll notice there are no assumptions in this theorem.",
                    "label": 0
                },
                {
                    "sent": "There are no assumptions whatsoever in this theorem.",
                    "label": 0
                },
                {
                    "sent": "What we can prove is that the training error of the final combined classifier is at most the product overall.",
                    "label": 1
                },
                {
                    "sent": "Rounds T of this expression in terms of epsilon T2 Times Square root epsilon T * 1 minus epsilon T and if we plug in the fact that epsilon T = 1/2 minus gamma T, we can write it in this form and then using a simple exponential upper bound.",
                    "label": 0
                },
                {
                    "sent": "This is at most E to the minus two times the sum of the squares of the gamma tease.",
                    "label": 0
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, suppose the weak learning assumption holds.",
                    "label": 0
                },
                {
                    "sent": "That means that all of the gamma Tees are at least some positive number gamma, like at least .02.",
                    "label": 0
                },
                {
                    "sent": "For instance, if all the years are at most 48%.",
                    "label": 0
                },
                {
                    "sent": "So that means that the training error of the combined classifier according to this found will be at most E to the minus two times gamma squared times T the number of rounds of boosting.",
                    "label": 0
                },
                {
                    "sent": "So it's saying that the training error is going down exponentially fast in the number of rounds, so the training error is going down very very quickly.",
                    "label": 1
                },
                {
                    "sent": "And also this also brings us to the reason that Adaboost is practical.",
                    "label": 1
                },
                {
                    "sent": "Adaboost is practical because it's adaptive it's adaptive.",
                    "label": 1
                },
                {
                    "sent": "In the sense that we don't need to know a lower bound gamma on all of the edges ahead of time, and we also do not need to know how many rounds we're going to run the algorithm for ahead of time.",
                    "label": 0
                },
                {
                    "sent": "And this is an important difference from the algorithms that came before Adaboost, and also if you look at these bounds, Adaboost is taking advantage of.",
                    "label": 0
                },
                {
                    "sent": "Edges which are bigger than the smallest edge, so this bound holds in terms of the smallest edge.",
                    "label": 0
                },
                {
                    "sent": "But if some of the edges are bigger than this bound will be even bigger better.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the only proof I'm going to give in this tutorial.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not too hard to proof, but.",
                    "label": 0
                },
                {
                    "sent": "Well, it's a simple proof and we're going to use it later in the tutorial, which is why I'm giving it.",
                    "label": 0
                },
                {
                    "sent": "So let me write F of X which I use this notation throughout the tutorial.",
                    "label": 0
                },
                {
                    "sent": "Let me write F of X for the weighted sum of the weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "So remember that's the same weighted sum which is computed by the final combined classifier.",
                    "label": 0
                },
                {
                    "sent": "The final combined classifier computes that weighted sum and then takes its sign.",
                    "label": 0
                },
                {
                    "sent": "So if you take that recurrence which defines Adaboost, go back to it.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We go back to this recurrence right here.",
                    "label": 0
                },
                {
                    "sent": "We can just unwrap that recurrence, unravel that recurrence.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "We can just unravel that recurrence and what we get is that the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Final distribution on any example, I is equal to 1 / M * E to the minus Yi times.",
                    "label": 0
                },
                {
                    "sent": "This weighted sum divided by the product of the normalization factors.",
                    "label": 0
                },
                {
                    "sent": "And this weighted sum again is just equal to F of XI.",
                    "label": 0
                },
                {
                    "sent": "So we can write it in this form.",
                    "label": 0
                },
                {
                    "sent": "OK, so step one is just unwrapping the recurrence.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are three steps.",
                    "label": 0
                },
                {
                    "sent": "Step 2 is to show that the training error of the final classifiers at most the product of these normalization constants.",
                    "label": 0
                },
                {
                    "sent": "So you might have just thought all these normalization constants.",
                    "label": 0
                },
                {
                    "sent": "They're just there as a convenience so we can end up with the distribution they actually turn out to be key to analyzing the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a proof of this fact.",
                    "label": 0
                },
                {
                    "sent": "So the training error of the classifier by definition, is just the fraction of examples which are misclassified.",
                    "label": 0
                },
                {
                    "sent": "So 1 / M times the sum of all the examples of an indicator variable, which is one that's misclassified 0 otherwise.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because the final classifier is the sign of F of X, and because Yi is plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "This condition is equivalent to why I disagreeing in sign with F of XI, which is the same as their product not being positive.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we can take this indicator variable and upper bound it by an exponential.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a crazy loose.",
                    "label": 0
                },
                {
                    "sent": "Approximation, But it's true.",
                    "label": 0
                },
                {
                    "sent": "So the exponential is always bigger than zero, and if this is negative, then E to the minus.",
                    "label": 0
                },
                {
                    "sent": "That same thing will be bigger than one.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an upper bound on that indicator variable.",
                    "label": 0
                },
                {
                    "sent": "And now we can just go back to step.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One remember, this same expression appeared in step one and is equal to this down here.",
                    "label": 0
                },
                {
                    "sent": "If we just plug that in.",
                    "label": 0
                },
                {
                    "sent": "And now we're just adding up over a distribution, so the sum over a distribution is 1.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just get product of ziti.",
                    "label": 0
                },
                {
                    "sent": "OK, in the third step we try.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The upper bound.",
                    "label": 0
                },
                {
                    "sent": "The training air.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We already have a bound on the training here in terms of the product of the normalization factors.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now it turns out we can just compute the normalization constants.",
                    "label": 0
                },
                {
                    "sent": "The normalization factors directly exactly.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to see this, we just write down the definition of the normalization constant.",
                    "label": 0
                },
                {
                    "sent": "This is its definition.",
                    "label": 0
                },
                {
                    "sent": "This is what you need to set it to so that the sum of the weights will be equal to 1.",
                    "label": 0
                },
                {
                    "sent": "We can take this sum into two parts over the examples which are misclassified and correctly classified.",
                    "label": 0
                },
                {
                    "sent": "And now if you just look at this sum.",
                    "label": 0
                },
                {
                    "sent": "This is just the weighted air of the weak classifier, which is epsilon T, and likewise this sum is 1 minus epsilon T. And now if you plug in that choice of Alpha T, you get exactly this expression.",
                    "label": 0
                },
                {
                    "sent": "So now I can answer the question where did this choice of Alpha T come from?",
                    "label": 0
                },
                {
                    "sent": "Alpha T was chosen so as to minimize this expression.",
                    "label": 0
                },
                {
                    "sent": "We want the tea to be small because ZTE, the product of the zipties, has an upper bound on the training error.",
                    "label": 0
                },
                {
                    "sent": "So we choose Alpha T in this expression to make it as small as possible, and that gives the choice of Alpha T used by Adaboost, and when you plug it in, you get this expression.",
                    "label": 0
                },
                {
                    "sent": "OK, so people are being extremely quiet.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So please do interrupt me with questions.",
                    "label": 0
                },
                {
                    "sent": "I know it's kind of scary because partly it's a big group and partly I'm way up here on this high stage so you know.",
                    "label": 0
                },
                {
                    "sent": "Seems like I'll hit you with a Bolt of lightning or something if you ask a question, but I won't.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, so here.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll ask myself a question.",
                    "label": 0
                },
                {
                    "sent": "So you just proved this theorem on training error.",
                    "label": 0
                },
                {
                    "sent": "Who cares about training here?",
                    "label": 0
                },
                {
                    "sent": "That's not what learning is about.",
                    "label": 0
                },
                {
                    "sent": "Learning is about minimizing the test there, right?",
                    "label": 0
                },
                {
                    "sent": "So what can you say about the tester?",
                    "label": 0
                },
                {
                    "sent": "That's a great question.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually usually say that's a great question.",
                    "label": 0
                },
                {
                    "sent": "Once question, I have no idea what the answer is to, but OK, OK, so how do we expect the test error to behave?",
                    "label": 1
                },
                {
                    "sent": "So here's a cartoon where I'm plotting the air of the combined classifier.",
                    "label": 0
                },
                {
                    "sent": "The final combined classifier, not the weak classifiers as a function of the number of Browns as a function of capital T. So we just proved this theorem that says that the training error goes down exponentially fast, so we expect the training error to go down very very quickly exponentially fast.",
                    "label": 0
                },
                {
                    "sent": "Now, what about the test there?",
                    "label": 0
                },
                {
                    "sent": "Well, at 1st at first we're doing a better job of fitting the training data, so we expect the test air to also be dropping.",
                    "label": 0
                },
                {
                    "sent": "But every time we run another round of boosting we add one new weak classifier to the combined classifier to the final classifier.",
                    "label": 0
                },
                {
                    "sent": "So that combined classifier is getting bigger and bigger and more and more complicated.",
                    "label": 0
                },
                {
                    "sent": "So at some point we expect overfitting to set, and we expect this test there.",
                    "label": 0
                },
                {
                    "sent": "To start going up again, we expect to see overfitting because the combined classifier, like I said is becoming bigger and bigger and more and more complex.",
                    "label": 0
                },
                {
                    "sent": "This is I'm sure you've heard about this in other tutorials.",
                    "label": 1
                },
                {
                    "sent": "This is sometimes called Occam's razor.",
                    "label": 0
                },
                {
                    "sent": "Occam's Razor is the idea that you have two explanations, all else being equal, you should prefer the simpler explanation, and in learning, that means that given 2.",
                    "label": 0
                },
                {
                    "sent": "Classifiers which perform equally equally well on your training data, all else being equal, you should expect the simpler one to give better predictions.",
                    "label": 0
                },
                {
                    "sent": "Now, over fitting is a huge problem.",
                    "label": 0
                },
                {
                    "sent": "It's a big problem and it comes up all the time in machine learning.",
                    "label": 0
                },
                {
                    "sent": "So in a case like this you really want to stop training right about here to get that best test error rate.",
                    "label": 0
                },
                {
                    "sent": "But your according to your training set performance is getting better.",
                    "label": 0
                },
                {
                    "sent": "That's why it's called overfitting, 'cause things seem to be getting better.",
                    "label": 0
                },
                {
                    "sent": "You seem to be fitting the data better, when in fact things are getting worse and it makes it very hard to decide when to stop training.",
                    "label": 1
                },
                {
                    "sent": "So that's what we expect.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's an actual typical run, so this is using boosting on top of C 4.5, which is a decision tree learning algorithm on a benchmark data set called the letter data set, and again, we're plotting the air of the final combined classifier as a function of the number of rounds, but I've switched to a log rhythmic scale here.",
                    "label": 0
                },
                {
                    "sent": "If you just run C 4.5 by itself, it gets a test error rate of about 13%.",
                    "label": 0
                },
                {
                    "sent": "That's that line way up at the top there.",
                    "label": 0
                },
                {
                    "sent": "As expected, the training error goes down very quickly.",
                    "label": 0
                },
                {
                    "sent": "In fact, after only five rounds, the training air is zero.",
                    "label": 0
                },
                {
                    "sent": "We perfectly fit the training set.",
                    "label": 0
                },
                {
                    "sent": "What about the test air?",
                    "label": 0
                },
                {
                    "sent": "Well, the test air also drops as expected, but it keeps on dropping, dropping, dropping, dropping even after 1000 rounds.",
                    "label": 1
                },
                {
                    "sent": "The test air has not started to go up again, and this is kind of amazing because.",
                    "label": 0
                },
                {
                    "sent": "After you've run this for 1000 rounds, you're talking about some more than two million decision tree nodes.",
                    "label": 0
                },
                {
                    "sent": "So in terms of just raw number of parameters, this model is absolutely enormous.",
                    "label": 0
                },
                {
                    "sent": "An extremely complicated and yet it's giving very very good performance, as we see over here.",
                    "label": 0
                },
                {
                    "sent": "Now it's even more surprising is that the test error is continuing to drop even after the training error is 0.",
                    "label": 1
                },
                {
                    "sent": "So if you look at the training error after five rounds, the training here is 0 and it stays at zero.",
                    "label": 0
                },
                {
                    "sent": "Now if you think about what Occam's Razor predicts here, you have one model which consists of five decision trees.",
                    "label": 0
                },
                {
                    "sent": "Perfect accuracy on the training set.",
                    "label": 0
                },
                {
                    "sent": "Here's another model.",
                    "label": 0
                },
                {
                    "sent": "It's 200 times bigger.",
                    "label": 0
                },
                {
                    "sent": "It's 1000 decision trees also perfect accuracy on the training set.",
                    "label": 0
                },
                {
                    "sent": "This one is 200 times more complex.",
                    "label": 0
                },
                {
                    "sent": "Occam's Razor says this one should certainly give better test performance, but we see exactly the opposite happening.",
                    "label": 1
                },
                {
                    "sent": "Instead, the tester is 8.4% here and about 3% here.",
                    "label": 0
                },
                {
                    "sent": "So Occam's razor just seems wrong in this case.",
                    "label": 0
                },
                {
                    "sent": "How could a question?",
                    "label": 0
                },
                {
                    "sent": "I don't know how you measure that it's real data.",
                    "label": 0
                },
                {
                    "sent": "This is actually.",
                    "label": 0
                },
                {
                    "sent": "Well, it's data that's derived from OCR data.",
                    "label": 0
                },
                {
                    "sent": "Does the article noted right out of college?",
                    "label": 0
                },
                {
                    "sent": "The Oracle the.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Well, they're supposed to be the right answers, but usually with any training set, there are almost always some mistakes of some kind, so it's so nobody has gone in and deliberately added noise if that's what you're asking.",
                    "label": 0
                },
                {
                    "sent": "But because it's real data, I would expect that there probably there might well be mistakes in it, or some ambiguous examples in it, but I don't know for sure.",
                    "label": 0
                },
                {
                    "sent": "If it was a lot of noise.",
                    "label": 0
                },
                {
                    "sent": "What order to organize that?",
                    "label": 0
                },
                {
                    "sent": "He would make an order, but it is a right.",
                    "label": 0
                },
                {
                    "sent": "All of them are right then after you get the desired amount of complexity would expected by the level of yeah, right?",
                    "label": 0
                },
                {
                    "sent": "Well, so your intuition is right.",
                    "label": 0
                },
                {
                    "sent": "So if you do an experiment I was going to talk about this later, but if you do an experiment where you deliberately add noise to the data.",
                    "label": 0
                },
                {
                    "sent": "When you add that noise uniformly over your entire space.",
                    "label": 0
                },
                {
                    "sent": "Then then that does cause problems for Adaboost, because what it does like you were getting at is that it focuses on the hardest example.",
                    "label": 0
                },
                {
                    "sent": "So it's really just spinning its wheels on these really hard examples, and so that does happen, and there's been various research on how to try to make it more resistant to that kind of noise, more robust to that kind of noise.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, you know that's an artificial experiment where you artificially add noise.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, on actual datasets.",
                    "label": 0
                },
                {
                    "sent": "Which are certainly have some kind of noise in them.",
                    "label": 0
                },
                {
                    "sent": "It's performing well, which suggests to me that our model of noise is not very good.",
                    "label": 0
                },
                {
                    "sent": "The model of noise that says you have equal noise everywhere in your space probably is not realistic.",
                    "label": 0
                },
                {
                    "sent": "Probably what's more realistic is that there's some examples near the true decision boundary, whatever that means.",
                    "label": 0
                },
                {
                    "sent": "Which are more likely to be more susceptible to noise than ones which are far?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK epsilon T epsilon T is the air of the weak classifiers OK?",
                    "label": 0
                },
                {
                    "sent": "The weak classifiers and this is the error of the combined classifier.",
                    "label": 0
                },
                {
                    "sent": "So even after the combined classifier has training error zero.",
                    "label": 0
                },
                {
                    "sent": "We classifiers.",
                    "label": 0
                },
                {
                    "sent": "Still have significant air, so in this case the weak classifiers after you run this a few times, the weak classifiers will all have errors somewhere around 30%.",
                    "label": 0
                },
                {
                    "sent": "OK, so that stays large so you can continue re weighting the examples and so on.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Works the same way with other classification methods other than decision tree.",
                    "label": 0
                },
                {
                    "sent": "While neural networks neural networks are quite susceptible to overfitting, decision tree algorithms are susceptible to overfitting so.",
                    "label": 0
                },
                {
                    "sent": "It depends, yeah, so other ensemble there are other ensemble algorithms which do not overfit.",
                    "label": 0
                },
                {
                    "sent": "So for instance there's bagging and random forests.",
                    "label": 0
                },
                {
                    "sent": "They will also tend not to overfit.",
                    "label": 0
                },
                {
                    "sent": "You'll tend to get behaviors somewhat like this.",
                    "label": 0
                },
                {
                    "sent": "So it depends on the algorithm, but but this?",
                    "label": 0
                },
                {
                    "sent": "Does that answer your question?",
                    "label": 0
                },
                {
                    "sent": "Oh, I see two different weak classifiers exhibit this behavior, I see.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I believe with neural networks you'll tend to see this kind of behavior with.",
                    "label": 0
                },
                {
                    "sent": "Actually, can I come back to that question?",
                    "label": 0
                },
                {
                    "sent": "That is a good question, but can I come back to that question so I'm going to talk a little bit more about when this desert does not happen.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, good so.",
                    "label": 0
                },
                {
                    "sent": "So this seems to be a paradox.",
                    "label": 0
                },
                {
                    "sent": "At least it seems on the face of it, contradict Occam's razor.",
                    "label": 0
                },
                {
                    "sent": "So here's a different explanation called the margins explanation.",
                    "label": 1
                },
                {
                    "sent": "Work with you operating Peter Bartlett and we suddenly.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is that the training error, which is all we were looking at, does not tell us the whole story.",
                    "label": 0
                },
                {
                    "sent": "The training error only measures whether the classifications are right or wrong.",
                    "label": 1
                },
                {
                    "sent": "So what we also should be looking at are the confidences of the classifications.",
                    "label": 0
                },
                {
                    "sent": "So the idea of this explanation is that yes, after five rounds of boosting the training error zero and it stays at 0, nothing seems to be happening.",
                    "label": 0
                },
                {
                    "sent": "But in fact after the training error zero the confidences.",
                    "label": 0
                },
                {
                    "sent": "And the predictions that are being made by the combined classifier are increasing.",
                    "label": 0
                },
                {
                    "sent": "It's becoming more confident in its own predictions and that that increase in confidence is leading to better performance on the test set.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's nice.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And fuzzy to turn it into something precise, we need a way of measuring confidence.",
                    "label": 0
                },
                {
                    "sent": "So how do we measure confidence in the predictions made by these combined classifier?",
                    "label": 0
                },
                {
                    "sent": "Well, remember that this combined classifier is a weighted majority vote of the weak classifiers.",
                    "label": 1
                },
                {
                    "sent": "It's voting the predictions of the weak classifiers, so it's like it takes an election every time it has to make a prediction.",
                    "label": 0
                },
                {
                    "sent": "It takes an election among the weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "So in an ordinary election, how do we measure confidence in the outcome of an election?",
                    "label": 0
                },
                {
                    "sent": "Well, if you think back state of Florida in 2000 or two Minnesota in the Senate race, which is still being argued about the problem there is that people have very low confidence in the outcome of the election because the margin was so low, the margin, the difference in the number of votes received by one candidate over the other candidate was so small.",
                    "label": 0
                },
                {
                    "sent": "So we can measure confidence in the same way.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can measure confidence using that same margin.",
                    "label": 0
                },
                {
                    "sent": "The strength of the vote, which is just the fraction.",
                    "label": 1
                },
                {
                    "sent": "Or here we have take the weighted fraction of the weak classifiers voting for the correct label minus the weighted fraction.",
                    "label": 0
                },
                {
                    "sent": "Voting for the incorrect label.",
                    "label": 0
                },
                {
                    "sent": "So that's called the margin, and that's the measure of confidence that we use.",
                    "label": 0
                },
                {
                    "sent": "So because this is the difference of two difference of two fractions.",
                    "label": 0
                },
                {
                    "sent": "The margin will always be a real number between minus one and plus one.",
                    "label": 0
                },
                {
                    "sent": "Margin.",
                    "label": 0
                },
                {
                    "sent": "If the margin is positive, that means more of the weak classifiers were voting correctly than voting incorrectly.",
                    "label": 1
                },
                {
                    "sent": "So when you take that majority vote that which is what the final classifier is doing, you'll get the correct prediction.",
                    "label": 0
                },
                {
                    "sent": "So the margin is positive if and only if the final classifier is giving a correct prediction on that example and the absolute value the magnitude of the margin.",
                    "label": 0
                },
                {
                    "sent": "That's what's really measuring the confidence.",
                    "label": 0
                },
                {
                    "sent": "So if the margin is close to 0.",
                    "label": 0
                },
                {
                    "sent": "So the outcome of the election is very close.",
                    "label": 0
                },
                {
                    "sent": "Then we think of that as a low confidence prediction, and if it's rather high, if it's way out here or way out here, we regard that as a high confidence prediction, which might be correct.",
                    "label": 0
                },
                {
                    "sent": "Or it might be incorrect.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is the evidence that margins have something to do with Adaboost performance?",
                    "label": 0
                },
                {
                    "sent": "So first of all, there's empirical evidence.",
                    "label": 1
                },
                {
                    "sent": "So we can look at specific datasets and we can look at the distribution of margins on the training set.",
                    "label": 0
                },
                {
                    "sent": "So here's that same curve I had before with the training Aaron Test there for that same data set, and what we can do is we can plot the cumulative distribution margins on the training set.",
                    "label": 0
                },
                {
                    "sent": "Remember, every example has a margin, so if you look at a data set then you get a distribution of margins and this is a plot of the cumulative distribution, which means that.",
                    "label": 1
                },
                {
                    "sent": "Where the slope is steepest, that's where the most examples are.",
                    "label": 0
                },
                {
                    "sent": "So this is plotting cumulative distribution after five rounds, 100 rounds which you can't see very well and 1000 rounds.",
                    "label": 0
                },
                {
                    "sent": "And what we see is that as boosting continues, the training error remains at zero.",
                    "label": 0
                },
                {
                    "sent": "But the these examples with relatively low margin are being pushed to the right.",
                    "label": 0
                },
                {
                    "sent": "So this is saying that many examples have margins in here and way out here.",
                    "label": 0
                },
                {
                    "sent": "But after 100 or 1000 rounds, nearly all of them have margin over in here.",
                    "label": 0
                },
                {
                    "sent": "Or you can look at the numbers down here.",
                    "label": 0
                },
                {
                    "sent": "Training Aaron Test area had before.",
                    "label": 0
                },
                {
                    "sent": "If you look at the fraction of examples with margin below point.",
                    "label": 0
                },
                {
                    "sent": "Five that's at this point.",
                    "label": 0
                },
                {
                    "sent": "About 7.7% of them have margin below .5 after five rounds, but after 100 rounds, all of the examples have margin above .5 OK. And similarly you could look at the example with the smallest margin which is about .1 four after five rounds.",
                    "label": 0
                },
                {
                    "sent": "After 100 rounds it's up to .5 two and then .55.",
                    "label": 0
                },
                {
                    "sent": "So the claim is it's here.",
                    "label": 0
                },
                {
                    "sent": "It's an empirical claim, not a theoretical claim.",
                    "label": 0
                },
                {
                    "sent": "It's an empirical claim that this increase in margins.",
                    "label": 0
                },
                {
                    "sent": "Is correlated with the continuing improvement in the test performance.",
                    "label": 0
                },
                {
                    "sent": "So there's also.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Theoretical evidence for the margins.",
                    "label": 1
                },
                {
                    "sent": "Explanation of boosting.",
                    "label": 0
                },
                {
                    "sent": "So we can analyze boosting in terms of these margins.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we can prove that the bigger the margins on the training set, the better the bound we can prove on the generalization error, and that bound is independent of the number of rounds of boosting.",
                    "label": 1
                },
                {
                    "sent": "So this bound this theorem says that it doesn't matter how many rounds of boosting you run for, it doesn't matter how big capital T is, what matters is the margins on that.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Training set.",
                    "label": 0
                },
                {
                    "sent": "So the way we prove this is actually if you think about elections.",
                    "label": 0
                },
                {
                    "sent": "Think about any election where you have maybe 100 million voters.",
                    "label": 0
                },
                {
                    "sent": "Even if you have a huge huge electorate with a huge number of voters.",
                    "label": 0
                },
                {
                    "sent": "It's possible to predict the outcome of the election simply by taking a survey simply by calling up 1000 random voters and asking them who they're going to vote for.",
                    "label": 0
                },
                {
                    "sent": "But that only works if the margin is large.",
                    "label": 0
                },
                {
                    "sent": "If the election is very close, then that won't tell you who's going to win the election.",
                    "label": 0
                },
                {
                    "sent": "You need a large margin for that to work, so the same idea works here.",
                    "label": 0
                },
                {
                    "sent": "So if all of the margins are large, then we can approximate the final classifier, which is an election over this huge electorate.",
                    "label": 1
                },
                {
                    "sent": "By a much smaller classifier just by taking a random sample from that set of classifiers, weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first part of the analysis.",
                    "label": 0
                },
                {
                    "sent": "The other part of the analysis says that it's not just a coincidence what we saw in the last.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slide, it's not just a coincidence that these margins are being pushed to the right, in fact.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can prove that boosting tends to increase the margins of all of the training examples given the weak learning assumption, and in fact the larger the edges, the larger the edges of the weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "In other words, the better they are compared to random guessing, the larger will be these margins and the.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can be proved in a similar way to the training error proof that I gave, yeah?",
                    "label": 1
                },
                {
                    "sent": "1st.",
                    "label": 0
                },
                {
                    "sent": "OK so it is OK so up here we're assuming that the data is random, that we're getting IID data, so the training examples and the test examples are coming from the same data set.",
                    "label": 0
                },
                {
                    "sent": "And then the bound that we prove is in terms of the distribution of the margins, the complexity of the weak classifiers themselves.",
                    "label": 0
                },
                {
                    "sent": "And the number of training examples.",
                    "label": 1
                },
                {
                    "sent": "Unlimited what?",
                    "label": 0
                },
                {
                    "sent": "Oh well, the bound isn't.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's say.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have a slide for you.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we can prove is that the generalization error is at most the fraction of training examples with margin below any level Theta that you pick plus some additional term, which is something like square root D / M, where D is the complexity of the weak classifiers, not the combined classifier.",
                    "label": 1
                },
                {
                    "sent": "The weak classifiers divided by M, the number of training examples and Theta is your chosen margin level OK?",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Right, so putting this argument together, what we're saying is that the final classifier is getting bigger and more complex, but as that happens the margins are likely to be increasing by the 2nd theorem, so the final classifier is actually getting closer to a simpler classifier.",
                    "label": 1
                },
                {
                    "sent": "By this argument up here.",
                    "label": 1
                },
                {
                    "sent": "And that's actually driving down the tester.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is a good place to stop.",
                    "label": 0
                },
                {
                    "sent": "So why don't we take so?",
                    "label": 0
                },
                {
                    "sent": "Let's say I notice that if people if they say 15 minutes then people are going 25 minutes and so let's say 10 minutes and then I know that will actually be 15 minutes.",
                    "label": 0
                },
                {
                    "sent": "Does that sound good?",
                    "label": 0
                }
            ]
        }
    }
}