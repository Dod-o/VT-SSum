{
    "id": "efzvrod64avuxfeeb7h47ymhdsuhypqs",
    "title": "Graph neural networks for computational drug repurposing",
    "info": {
        "author": [
            "Marinka \u017ditnik, Computer Science Department, Stanford University"
        ],
        "published": "June 28, 2019",
        "recorded": "May 2019",
        "category": [
            "Top->Computer Science",
            "Top->Data Science",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/icgeb_zitnik_graph_neural_networks/",
    "segmentation": [
        [
            "It's great to be here.",
            "Thank you.",
            "Organizers for organizing such a nice workshop here in Slovenia.",
            "It's always great to visit to have an excuse to come here.",
            "OK, so."
        ],
        [
            "So I'll be giving two lectures, one today and one in the morning, and my goal is really to make these two lectures more like tutorial like so I will really drill down and explain the methodology from beginning to the end.",
            "Also provide codebases datasets an examples for yours and."
        ],
        [
            "So the two lectures that I have to or are the following.",
            "So today I will focus on network Embeddings's methodology and I will explain methods for doing that for doing for embedding networks in graph structured data I will provide some pointers to resources that we have developed in the form of datasets, tools and code breaker code bases and I will describe a few biomedical applications focused mostly on protein, protein interactions, disease pathways and tissue modeling.",
            "And tomorrow in the morning.",
            "I will focus more on graph neural networks, which are methods for learning embeddings, but with using multiple layer, not layers of nonlinear transformations.",
            "So what we will learn today will be really relevant and foundational for what we'll be discussing tomorrow.",
            "And again, I will provide to malls tomorrow as well.",
            "I will provide data sources, anile practical advise us and if there will be time will actually look at the code and run some demos so that you will actually see how how it works in practice and in terms of applications.",
            "Tomorrow I will focus on.",
            "Polypharmacy and Drug Purpose Inc, which is very closely related to the topics of this workshop.",
            "So to just give a preview to make you excited to, I'll give you 1 slide.",
            "Here's one slide.",
            "Previous slide of tomorrow's lecture, so the methods that I will describe today and tomorrow we use them to predict and model drug combinations and identify side effects an the site safety safety of drug combinations, and that was the first ever study where we looked at all possible combinations of drugs that are approved on the US market.",
            "That has led to an interesting follow-up research that we have on prostate cancers as well as validations in the clinic on real life patients with high risk medical school.",
            "So I will talk about it tomorrow and then.",
            "Second point is the method that I will describe.",
            "Also use them for repurposing existing drugs.",
            "On that roundup approved on the market for new diseases, which has led to interesting new metal development and also too.",
            "Has directly impacted drug repurposing at Stanford.",
            "And all these studies methodology the way we introduce it to domain expertise.",
            "By developing this large systems or data science pipeline is that the the core of those pipelines?",
            "Is the methodology that I will describe, and it allows them domain experts to interact with the system in this form of asking queries, get predictions, ask why certain predictions are the case.",
            "So too they get explanations and interpret ability or options, and that is really open.",
            "Opportunities to introduce these deeper models to domain experts."
        ],
        [
            "But today let's start simply hand.",
            "Let's start with network embeddings and the applications for today.",
            "So."
        ],
        [
            "To motivate this work at a very high level, I see that science crucially depends on the existence of scientific instruments.",
            "So in the past in this, in this kind of instruments were physical devices, and physical devices were those that have facilitated facilitated new discoveries in science.",
            "But now, with modern data intensive science is we need a new kind of instruments and you kind of tools that I envision will enlarge way depend on knowledge discovery from data."
        ],
        [
            "However, knowledge discovery from biomedical data presents some unique set of challenges, and one of the fundamental challenge that I see is that for any particular by medical problem that you have, there are a number of different data data sources, datasets, or data types that might be relevant for that particular problem, and those datasets go from the range of genomic information to seller.",
            "Al Petteway information.",
            "Various transcriptomic gene expression data all the way to up to behaviors.",
            "Lifestyles in vital signs.",
            "So it's probably it's really challenging how to bring this data together, especially because it's multi scale, so it describes our problem at the level of molecules in the up to individuals up to entire populations.",
            "It's heterogeneous in in the sense that it generated by different labs using different platforms, and it might get it until it is confirmed that in a number of different ways due to investigative biases or due to constraints of current by technological."
        ],
        [
            "This kind of data then means that the biomedical data often does not fit into one flat tabular data set, and I see that as being a major gap between prevailing machine learning settings so that is or machine learning models that are primarily designed for tabular, monolitic flat matrix like datasets and by medical problems that often have complex multi scale heterogeneous datasets and that is significantly significant gap between what machine learning can address today and what are real world biomedical problems.",
            "Require us that we develop new mathematical representations that can integrate biomedical datasets in their broader sense and then develop methods that can learn in all reason over such representations."
        ],
        [
            "So the outline of today's lecture will be the following.",
            "I will first talk off of by my described by made biological networks as a search mathematical representation that allow us to really look at various kinds of data, and I will present some motivations why networks or are very good resource to think to use for biomedical problems.",
            "In the second part I will talk about note embeddings and network embeddings and methods that can take networks an learn some low dimensional representations that can really then enable us.",
            "Do machine learning.",
            "Can graphs with some applications, and in the third part of the talk I will talk about heterogeneous networks, which is which is really what we need in many real world applications.",
            "So to look at different kinds of data using this principle of multi model, multi model or heterogeneous networks."
        ],
        [
            "OK, so let's start with the first part.",
            "So also sad netic networks us as a mechanism.",
            "Also general language that allow us to integrate biomedical data.",
            "For example, we can start at the level of populations where we might have different individuals that interact with each other in different ways based on their physical proximity based on social relationships based on family ties.",
            "And we can go down to the level of diseases or phenotypes that exist within an individual and all the way down to interactions between various components within a cell of that individual.",
            "So what networks then allow us to do is that then allow us to model the kind of interactions within each scale, and then those interactions across different scales so we can bring together multiple scales of information and we can bring together different kinds of interactions between the between those components."
        ],
        [
            "So why, why why networks and why now is a good time so networks have been around for a number of years and they have lead to very interesting, exciting research and results in the context of biology and medicine.",
            "And let me give you a few examples.",
            "So very early on or people were excited and still are about understanding.",
            "How are diseases related to each other?",
            "What are proteins associated with different diseases and how can we study that?",
            "And using network based approach?",
            "This has led to interesting findings.",
            "That have uncovered various relationships between how genes with similar genomic information lead to disease us that our fellow typically simila"
        ],
        [
            "In another example, networks have been used to really understand and even simulate a basic of kariyat Excel in here.",
            "In particular, I'm sending a very a paper that I I like a lot by tryied occurs group very recently where they have used the notion of cell hierarchy and encoded as an actual architecture for neural network to learn and simulator basic occur at Excel."
        ],
        [
            "Furthermore, moving beyond molecular biology networks are having very excited.",
            "It's interesting object to study and understand cancer heterogeneity to do things such as patient subtyping that you Sledge 2 new exciting results in terms of identifying various new cancer subtypes that could correlate well with patient survival, Anne."
        ],
        [
            "Not only in cancer biology, there's lots of networks an in the study of ecology to understand how the complexity of real ecosystem can be modeled and understood, and from from the systemic level."
        ],
        [
            "More recently, microbiome is another example of a exciting new data modality that has been successfully studied using methods of enough network size."
        ],
        [
            "So many data are networks as I as I demonstrated just now, we can think of patient properties and relationships and represent them as networks or hierarchies of South systems or disease pathways.",
            "Various kinds of genetic interaction networks, jinx pressure networks, South similarity networks in the life in the world of single single cell genomics."
        ],
        [
            "Once we have that network representations, then the main question that appears is how to do machine learning on such biomedical networks, especially if they are very complex in reach and what we are ultimately asking is.",
            "Especially in the context of current era of deep learning, how can we design models that can take the dis rich, highly irregular and complex structures as their input, and they can make exciting predictions that tell us something about properties of cells or patient outcomes.",
            "The discover, new kinds of associations in relationships."
        ],
        [
            "And all the networks are very powerful data representation that they are relatively challenging to work with for preventing machine learning models, primarily because prevailing models assume that we have this flat matrix like datasets where every rose separate ID example.",
            "So to make this more concrete.",
            "Premier primary primary paradigm of current models is that they are designed for grids or simple sequences.",
            "So what I mean by that is convolutional neural networks and various variants of those architectures are primarily designed and used for fixed size images, and you can think of every image as a as a graph where every note is a pixel and every note has exactly 4 neighbors, which is pixel to the top, bottom, left and right.",
            "And then multiple channels potentially, so these are very regular structures.",
            "Similarly, LSD models or recurrent neural networks are our most successful models for various kinds of sequence data.",
            "Going from language to logical sequences, and they again they operate on graphs, but no graphs are simply linear chains, where every word is a token that is connected to their to the proceeding.",
            "In the following quarter, or potentially DNA nucleotide nucleotide.",
            "So these models have really brought extraordinary gains in various fields of computer science, including computer vision, natural language processing, and robotics, but they are unable to consider interactions, which is the essence of biomedical networks in particular."
        ],
        [
            "That means is that what?",
            "What about when our data is not regular in the forms of greed or linear change, but it looks something like like what is shown in the left part of the slide or when they are even more complex and their highly heterogeneous as shown in the right slide and we have seen now several examples of networks that in that that that are just examples that otherwise provided by law medical research."
        ],
        [
            "So fundamentally, why is deep learning on graph structured data so so hard and challenging is because the networks have complex topographical structure, so there is no spatial locality, and in particular for every node in the network there is no in really we don't have the concept of what is known to the left to the right to the bottom there is there is a set of neighbors that every node has.",
            "And fundamentally dismiss that the methods that we will develop need to be invariant to various kinds of translations and rotations which make existing models not useful.",
            "In many cases we have very rich networks, meaning we have different kinds of entities, molecules, individuals.",
            "And that interact with each other in different ways.",
            "So we have reached networks with different kinds of nodes, different kinds of edges, and that is something that we want to take into account.",
            "So real world networks, such as the one shown on the left look very different from what prevailing models are designed for, which are structures are shown on the right.",
            "So what we need are methods that can generalize convolutions beyond this simple lattices and they can learn and reason over over such rich, rich data that that we represent with networks."
        ],
        [
            "OK, so so that was the motivation for why we need this new methods and why the classical methods might not work well and we will return to that in various applications.",
            "Now in the in the main part of the stock I will focus on describing a foundational methodology that allow us then to really think about learning and reasoning over networks."
        ],
        [
            "And these will be focused on not embeddings, and hear various pointers to papers and some of the tutorials that we had recently on this methodology in various venues."
        ],
        [
            "So when they say embedding, So what I mean is the follow.",
            "So we start with our network on the left side.",
            "This is representation of our particular domain and the goal is to take every noting that networks a note we and then map it through some function F and that function F will take note we in it will return some feature vector which we call embedding which is a vector of length D that has the real numbers.",
            "The objective now is that we want to map.",
            "Every note in the network today, the dimensional embedding space such that two nodes that have similar local topology in the network will be embedded close together in that embedding space.",
            "So why do we want to do that?",
            "Or we want to do that because graphs are inherently combinatorial objects, which might be difficult to work with.",
            "But once we have their embeddings and feature vectors, then we can immediately use those vectors as input to downstream machine learning models.",
            "Or we can.",
            "Analyze them in various different ways.",
            "So ultimately our goal is we want to learn this mapping function F such that when we apply it to our network we get embeddings and those embeddings should reflect topology of the input graph.",
            "Really the main question then becomes, well, how do we learn that mapping function?",
            "How do we learn that mapping function if?"
        ],
        [
            "So for example, let's say our input will be some form of a disease similarity network, so that would be a network where nodes are various diseases or phenotypes, and two phenotypes might be connected based on their symptomatic similarity or based on comorbidity indices.",
            "So that might be our input network.",
            "We want to learn the function F that will for every note in that network in the.",
            "In this example of disease, it will map it to some D dimensional space, perhaps just two dimensional space, and in that case we can just visualize it very simply in the plane.",
            "And here in particular in this, in this example what I'm showing on the right is an example where diseases were embedded based on how frequently they call pier in patients that take particular set of drugs and that can lead to various interesting questions.",
            "So really next will focus on really learning.",
            "In talking I'll talk about methods that allow us to specify those embedding functions."
        ],
        [
            "So what is our set up or setup is the following.",
            "We start very simply will assume that we have one graph.",
            "This is graph G. That graph is defined from some set of vertices V and the graph is represented or we will work with it in the form of adjacency matrix a simply for the.",
            "We will think of ape to be to be on binary adjacency matrix and later today and tomorrow I will talk.",
            "What happens when we have this rich types of graphs that might be weighted or have multiple types of.",
            "Address multiple and also various kind of site information about node features and that we might want to include."
        ],
        [
            "So sad that the goal or the goal that we have is that we want to map notes in the network in our input network to some D dimensional embedding space such that similarity for notes in the beddings space will approximate similarity of the nodes in the network space.",
            "For example, you might like, for example, what would be an application in biology that we will see later?",
            "We know that if our input network would be a protein protein interaction network, we know that in in that network.",
            "Do proteins that tend to that interact with each other are more likely to be associated with similar phenotype, which gives us directly idea for how to use this, how to use this principle because two parties that will interact with each other, all that will be close in the network will be close in the embedding space, and that's the principle that can then be used to discover new disease genes for example."
        ],
        [
            "OK, so mathematically this means that if we have these two notes U&V, we want to have.",
            "We want to have the method to that that we learned embeddings for notes V and for notes you envy.",
            "Those embeddings would be OZV such that in the simplest case their dot product would approximate the similarity in the network.",
            "So."
        ],
        [
            "So what we do we need to define here what what we need to define are two basic things.",
            "So first we need to really define what is similarity between U&V in our network right?",
            "And then second we need to define what is this encoder encoding procedure that takes note each note in the network and in Maps it to the D dimensional space.",
            "So that's something we need to define.",
            "So as I said, an encoder a note similarity function which measure similarity, the network and once we have that similarity function and the the the encoding function, we just need to optimize parameters of the encoder such that the similarity between notes you envy approximates their dot product between the two between the two nodes.",
            "And we do that for every four for all notes.",
            "For all note pairs in the network."
        ],
        [
            "So the two key components are again encoders.",
            "Encoder is the procedure that will map a note to the dimensional space, so it will take a note in the input graph.",
            "We in it will return some D dimensional embeddings is AbbVie and then similarity function that will define how the relationship in the input network will map to some relationship into embedding space.",
            "And for the purpose of this talk, I will assume that this similarity, the embedding space into DOT product between the two embeddings.",
            "But we can, we can use some other approaches as well."
        ],
        [
            "So there's a class and methods in the literature that that follow this principle of thinking about methods that can take our input objects in networks and map them to something dimensional space and preserve some notion of similarity.",
            "And many methods you similar encoder.",
            "So recently the we heard some today about matrix factorizations that are very popular methods in other latent variable models.",
            "More recently, their methods such as Note, Awake, Deep Walk line structure, to make any kind of.",
            "Start to wake methods are basically operate on this principle of having some notion of similarity in our input data and some notion of encoder and this method use different notions of similarity, particular when our data is networks.",
            "We can say well, two nodes should have similar embeddings if they are directly connected by the by by an edge.",
            "So this is the fundamental principle that is used in matrix factorization algorithms, right?",
            "So we take adjacency matrix and we simply factorize it.",
            "Adjacency matrix encodes.",
            "Direct relationships edges between nodes which which means exactly that 2 notes that are related to each other, but there will be a one in the corresponding value in the matrix.",
            "They will have similar embeddings as learned by matrix factorization.",
            "We might have more advanced notions of what this means, that 2 notes should have similar embeddings.",
            "Perhaps to note, should have similar embeddings if they share many neighbors if they are connected to each other by many shortest by many short pets, or.",
            "Any kind of other other notion of similarity?"
        ],
        [
            "So next, what I will talk about is I will talk more concretely about a few prominent methods for networks that can generate and learn embeddings."
        ],
        [
            "And I called this matter shallow note embedding methods, so why shallow?",
            "Because they will they will.",
            "They will do us one layer of linear or nonlinear transformation on the network to learn embeddings.",
            "There also sometimes called direct direct encoding methods such as in May many matrix factorization methods fall under this area.",
            "I will talk about other types of challon automatics.",
            "Tomorrow I will talk about deep note embeddings and then about by then we'll see some applications in biomedicine.",
            "OK, so shallow note embeddings so."
        ],
        [
            "To set what we need is we need to 1st define what does it mean for us.",
            "For two nodes to be similar in the network.",
            "So the idea that we'll be using that turns out to be fairly flexible is will define node similarity function based on the structure of some K hop neighborhoods of nodes in the network.",
            "So if we have some note.",
            "Which is the one in the the the Red triangle.",
            "We will look at its direct neighbors.",
            "It's 2nd order neighbors.",
            "It's it's 3rd order neighbors, so and that is something that will define us.",
            "The representation of the note and it will be the structure that will be used to compare multiple neighborhoods to each other and learned embeddings.",
            "So in this case threat is the target note for K equal, once we get one hop neighbors which are directly encoded in adjacency matrix.",
            "And for cake with two, these are two hop neighbors and so on.",
            "So how to stochastically define these kind of higher order neighbors?",
            "Is something that we want to model next because?"
        ],
        [
            "Many real world networks are incomplete and noisy, so potentially.",
            "So potentially a note here with this wide triangle that is actually one through three hops away from the target node.",
            "Mahi Mcnaught might actually be a direct neighbor of target node, but we just don't know yet, because that interaction has not yet been tested, or just because our network, our our data is really noisy and income.",
            "So we want to have some stochastic notion of what does it means mean to be a direct number or some neighbor to do some K hops away?",
            "So."
        ],
        [
            "Our idea will be that really the the the question that we need to solve is what does it?",
            "What does it mean for a note to what does its neighbor?",
            "What is a neighborhood of a note?",
            "So given a particular notes say note, you will want to define what are nearby nodes and we will be fine when we will have this definition.",
            "Then we will optimize and learn embeddings in such a way that will save that for a pair of nodes that have similar neighbor nodes.",
            "Similar name, neighborhood.",
            "They should be embedded close together, so the notation that I'll be using is NR secured with two diff to denote the neighborhood of note you under some strategy are."
        ],
        [
            "So now more formally, optimization task would be we have this graph G. Our goal is to learn the mapping function and.",
            "That mapping function will be such that for for an embedding for four note you will be predictive of other nodes that are in the neighborhood of.",
            "Note you, so this is so called Skip Gram objective which is reminiscent of what some of you might know.",
            "A sword to work objective where the that are based on this idea of distributional distributional hypothesis that was proposed back in 50s where the where the the intuitively ladies the following that the embedding of the note.",
            "Should be indicative of embeddings of its nearby nodes.",
            "Similarly, similarly as the meaning of the word can be defined by the meaning of other words with which these words Co appears.",
            "So what will have here is the meaning or of the note will be defined based on the meaning of other nodes with which this node copiers, right.",
            "But now the meaning formally will be this D dimensional vector for us and this other node switch which denote called peers will define flexibly.",
            "So that will not only look at the direct neighbors, but some some some form of stochastic neighborhood for or not."
        ],
        [
            "OK.",
            "So mathematically dismissed, we have we.",
            "We want to find an embedding.",
            "This appeal for no tool that will predict its nearby notes, right?",
            "So what we want to maximize is there we want to maximize the following sum.",
            "We go through every four to go to all notes in the in our graph and want to maximize the probability the conditional probability of the neighborhood given the current embedding for note.",
            "And the assumption here that we will make is this because this is this the neighborhood of nodule is some set of nodes that are neighbors of you will assume that this conditional likelihood can be factorized in the form of.",
            "Through every going to old neighbors, forgiven neighbor of.",
            "Note you we want to say that embedding of new will be good for us if it is able to predict well its neighbor.",
            "So this is just mathematical way of.",
            "Encoding the distributional hypothesis that I described before."
        ],
        [
            "OK, so formally this means that what we will get, who will get embeddings for 4U and V such that the DOT product between the two embeddings will will represent the probability that a note U&V or Co occur in some random walk that is defined in the network."
        ],
        [
            "So this notion of using random walks on the network is exciting because it gives us two important.",
            "It has two important advantages.",
            "First, it's very flexible.",
            "It's a very stochastic definition of note.",
            "Similarity takes into account incompleteness, and eight in noisy networks, and it's very efficient in the sense that we do not need to consider all pairs of nodes which would make our method quadratic, but we consider only those on those pairs of nodes that Coker.",
            "In some random walks."
        ],
        [
            "So the random walk.",
            "So the random war or random book procedure is then the following for every note in the network.",
            "We will simulate some would simulate some number of short random walks that started the note and visit some some notes in the neighborhood.",
            "Then this.",
            "The result of this procedure will give us the neighborhood, the neighborhood of a note.",
            "So the neighborhood of note you will be defined as all nodes that are visited by random Walker that starts at Nokia.",
            "And then this will simply plugging this into our objective, which will be the objective.",
            "Based on this, on the principle that we want to learn the embeddings such that they can predict well, it's it's nearby nodes."
        ],
        [
            "OK, so to unpeg the formula that we have just seen, it's it's it is shown here in this slide so our loss function then if it has a few terms.",
            "So first this outer sum is the sum over all nodes, then for every node.",
            "So we go through all of the neighbors of the notes.",
            "The neighbors are obtained by the random Walker and then we want to say that while the.",
            "Embedding of we should be predicted forfeits neighbors, which means our formally just the softmax function, essentially saying that the dot product for you in V, where V is the neighbor of fuel should be high relative to the dot product between you and all other nodes in the network.",
            "Now the problem here is that here I said all other noted the network.",
            "So imagine how we have a network with 10,000,000 notes and here in order to compute this term we need to go through all other nodes in the network.",
            "And here we already need to go through all the notes, so that's already quadratic.",
            "So that's something we don't want to do."
        ],
        [
            "Or doing that naively is, is is really too expensive because we have to normalize this softmax function and that is really something that's not that is not specific to network embeddings, but is pervades various kind of meta methods that.",
            "Recently operate on.",
            "That operate on various kinds of data to learn low dimensional representations.",
            "So the problem is how to normalize this term in the softmax function and."
        ],
        [
            "Really dark, the solution to this problem is known as noise contrastive estimation or negative sampling originally proposed by Mykola at all in 2013, where the idea is well, we would want we would need to enumerate here all notes in the network and say that the dot product between a pair of nodes that are actually neighbors should be high relative to the dot product of all non neighbors because that is too expensive to do.",
            "What we will do.",
            "Instead we will randomly sample some fixed number of nodes and just say that the dot product.",
            "Of two nodes that are neighbors should be high relative to the dot product of.",
            "Random neighbors.",
            "And that is known as negative sampling, which means that instead of normalizing with respect to all notes, we normalized against some K random negative samples."
        ],
        [
            "OK, so.",
            "Just to overview this procedure again to get embeddings for the metaphor for the network, we will simulate many short random walks starting from every Nov starting from every note.",
            "This will give us give us neighbors and then once we have the neighbors for every note will optimize this objective function where we have just unpacked this particular term and use softmax function for that."
        ],
        [
            "So the only thing that's really left here is what does this strategy are mean?",
            "So what is this record here and so far the idea of what kind of strategy will be using I?",
            "I said that what was strategy will be that we will simulate random walks from every node in the network and those are other those nodes that are visited by random walks will define the neighborhood for the note.",
            "That is 1 strategy.",
            "Perhaps the question is.",
            "Can we do better?",
            "Perhaps we can be more intelligent about how to define note neighborhoods, and there are a number of methods that here that have aimed to do that, starting with note awake and then various kinds of awake methods."
        ],
        [
            "So.",
            "I will talk about 2 examples here.",
            "In the first snow, two equations biased random walks, so the idea is the following.",
            "Let's say we have such a network as our input and what we want to do is we want to define the neighbors of every note and for now let's say the target notice.",
            "Note you here.",
            "So the classic to classic strategies of saying what is the neighborhood of note you will be will just run breadth first search from starting from Nokia breadth first search is the one in Red would say that note S1X2X3 and S4 are the closest neighbors and then S5 would be the neighbor that is in a 7.",
            "Or not, just as five would be the neighbor that is 2 hops away and then others are three hops, and so on.",
            "And we would say, well, these are our neighbors.",
            "The other option is to do.",
            "The other extreme is depth first search where we would say, well, the neighbors of note, you are not really S1S2 and S3 and four the neighbors of Note U RS4 S 5 S 6 right so so we we would go further away from from No2 and would find that as a neighborhood.",
            "So these are two fundamentally different ideas because.",
            "The breakfast search ordeal will give us local make microscopic view and depth first search idea will give us global view in the networks."
        ],
        [
            "Be more concrete to make this.",
            "To give an example.",
            "Here is not.",
            "Here is Notific applied to those simple network so.",
            "This is a network where actually every note represents an XR2.",
            "Nodes are connected based on our core appearance of those networks.",
            "In the left view, we took that network as input.",
            "The network did not have any labels, and we applied note victory.",
            "It and we use this local view of the network where we say the neighborhood of every node are its direct neighbours or neighbors.",
            "Mmmkay hops away visited by breadth first search and we use that to with the note awake objective that really gives us embeddings an we for every note we cluster those embeddings.",
            "An nodes that are in the same cluster.",
            "Are colored on the left with the same color, So what we get from here from from with this principle is we get this local view.",
            "We can see that there are pockets of color of the network.",
            "So so here's one problem with this already, you're swamped.",
            "This is orange, so we we do this notion of similarity.",
            "The idea is that two nodes that have that are that are close or proximally.",
            "The network should have similar embeddings.",
            "There is a complementary view, which is if we would define neighborhoods based on depth first search where which is which would give us results like that.",
            "Again, we have the network.",
            "We run depth first search define neighborhoods of every note.",
            "We use those neighborhoods with the notebook objective to get embeddings.",
            "We cluster embeddings, colored the note and color denotes by cluster assignment, but all of a sudden here we see what red nodes are here, but also in this part of the network which is separated from this part right?",
            "And here again, some pocket the fret notes which are separate from this one.",
            "So this is because now the embeddings that we have learned are indicative of structural role in the network.",
            "So all these blue nodes are actually bridges.",
            "There are there act as bridges between different parts of network, like here's a blue note that is actually the bridge and this is the only note that connects these notes to the rest of the network.",
            "And again this note is again a bridge.",
            "It connects this set of nodes to the rest of the network and actually all blue notes are bridges and that is.",
            "That is what we have learned.",
            "That is what the information that is encoded in the beddings.",
            "But on the left the information that is encoded in the beddings is clustering information.",
            "So depending on a particular application that we have in biology, we might want to use different notion of not similarity to learn embeddings that might be useful for us.",
            "For example in the context of functional genomics, which I will show later.",
            "It has been demonstrated that it's it's it's not on.",
            "When predicting functions of proteins, it's not only important to look whether proteins directly interact.",
            "But if a bit also important to think whether proteins that are in completely different parts of protein interaction network might have similar roles in the network and similar functions, because their local interaction neighborhood is the same, so they might act as bridges, or they might act US made.",
            "They may have similar rules in different protein complexes."
        ],
        [
            "OK, so now the week really works well for this local view of the network.",
            "A different notion of of note similarity, which is called structure, which works really well for capturing structural similarity or for capturing this global structural offloading the network.",
            "So we struck too vague.",
            "The idea is the following.",
            "Our goal is the goal of structure, which is that.",
            "We want to learn embeddings for notes such that two nodes that have similar structure was in the network will have been be embedded close together.",
            "For example, imagine we have this.",
            "We have a gigantic network and there's some notes.",
            "Unv in-depth network you in VR separated from each other.",
            "They are really far away from each other, but if you look more closely, it seems that while you has three to hear it involved into triangles in V is involved in two triangles.",
            "In in in, even though they are far apart from each other, they are structurally similar.",
            "So what we want to do is then we want to learn embeddings that will be able to automatically find those to notes with structural similarity.",
            "Instructor we can do that."
        ],
        [
            "And it could do that by implementing tree.",
            "Using a project is contains 3 main steps.",
            "So the first step that struct awake implements is it defines similarity between nodes, which is called structural similarity.",
            "By looking at the K Hop neighborhoods and will see in the next like how this is done.",
            "To do actually do that, it constructs a new graph based on the input graph and then on that new slightly transformed graph.",
            "It runs random walks and those random walks give neighborhoods for, for, for.",
            "For notes that are then optimized using simple Skip gram objective in the same way as note topic."
        ],
        [
            "So in step one, but it means to construct a new.",
            "What is mean too much means to compute structural similarity between nodes.",
            "Is the following sort of let's?",
            "Let's start with the idea with.",
            "Let's assume that our notation is the 10 steps are lower case key of ur notes that are in some K Hop neighborhood.",
            "Of note you then we know there is slim line graph theory that says you NVR structurally equivalent.",
            "Considering the K hop neighborhoods if their key if there are.",
            "If the graphs induced in those neighborhoods are isomorphic, so which means that there the key?",
            "Hope for notes U&V there K hop neighborhoods look exactly the same, meaning exactly the same number of triangles, the same number of squares, the same number of neighbors that you have the same number of neighbors that other neighbors in key home neighborhood have.",
            "The problem is then testing if there keyhole neighborhoods are exactly the same is challenging, but there's a trick to use, and that trick is provided by this limit that says in order we don't need to really exhaustively compare or possible network patterns that might exist in K-Cup neighborhood.",
            "We can just look at the order degrees sequence of notes in the K-Cup neighborhood in checked.",
            "If those degrees are the same.",
            "So the idea then will be for a pair of nodes in the network.",
            "The method will compute their structural similarity, but really looking at the ordered degree sequence of other nodes that appear in the neighborhood of your envy."
        ],
        [
            "The way this is implemented is implemented by taking the input graph, constructing some among us knew multilayer graph or where in the first layer to wear in each layer of debt multilayer graph.",
            "The nodes are the regional nodes of the network and in the first layer two nodes are connected.",
            "If they are direct neighbors.",
            "In the second layers, two nodes are connected if they are.",
            "If if they are neighbors at two hops that are two hops away, and if they are neighbors that are three hops away, and so on.",
            "And."
        ],
        [
            "And once we have this multilayer graph, then automatically what we get with what we get it is that in in particular layer of that graph 2 notes will be connected if they are structurally similar with the respect of K hop neighborhood where K is the in that case.",
            "For example, for example tree and look examining this graph will give us the opportunity to get the neighborhood of four nodes in the network and once we have neighborhoods for nodes.",
            "We can immediately plug it into our optimization tasks to get embeddings that will be indicative of structural similarity."
        ],
        [
            "So to give an overview of of this method, we start with the input network that we have.",
            "We construct a multilayer graph that I described in the previous slide.",
            "We use it to simulate random walks on that graph from every note and that gives us the nodes that are visited by random walks, give us neighborhoods for, for, for, for notes, and then for once we have those, we learn.",
            "Embeddings Z.",
            "By using the same objective function as we have used for noting."
        ],
        [
            "So a quick experiment is here just to make things more concrete, so this is a small synthetic example which is actually called Barber graph.",
            "So what we see here?",
            "This is our network.",
            "We have one click here.",
            "One click here and we have a single pad that connects the two clicks.",
            "The notes of the colors of the notes here are structurally indicate structural equivalence, meaning all notes that have color blue.",
            "Are the same from the network point of view.",
            "This note is exactly the same as this note in the sense that it has the same 1st order numbers, the same second hop neighbors and so on.",
            "So here again we have noted that from structural point of view they are.",
            "They look the same, we cannot distinguish them.",
            "And for example here we have to recognize that outlook that that have the same role in the network to light blue nodes that have the same role in the network and so there are the color of these notes would be our ground truth information.",
            "So notes of the same color are structurally equivalent.",
            "What we want to ask now is if you take this network of course without note colors, use it on a strong, use it with struct awake will structure the traffic able to learn embeddings such that nodes that have the same structural or role will be embedded close together.",
            "Which means, no, that have the same color.",
            "Here should be embedded close together in the next slide.",
            "OK, so let's see that."
        ],
        [
            "So we have this Barber graph on the left, and here's the result when you struck too big to learn and padding for every note in that graph.",
            "And we project this into this 2 dimensional plane.",
            "What we see is really that old dark blue notes, which are structural equivalent are all in this part of the network.",
            "So so here is the method was actually indeed able to embed nodes that are structurally similar, very close together.",
            "So here are the two rentals that are structurally similar to real time zone.",
            "If you would take this network as inputs to note a week or some other of the shelf method, it would.",
            "The medics would look completely different because they will typically be based on him awfully, not structure occurrence."
        ],
        [
            "OK, beyond structurally there's a recent more recent meta graph waving in the code for all these methods is online, so you can simply download it and use them on your networks, and graph is a bit more advanced than structure weak in the sense that it scales to very large networks, and it's more accurate.",
            "So imagine we would start with another example graph where we have this Jerry or graph here and again the color of the notes indicate structural.",
            "Curious of the notes, meaning all powerful notes here all have the same role in libel.",
            "Notes have the same role in are the same from the network's point of view.",
            "If you take this network is simple to struct awake, learning bedrooms and visualize them.",
            "We get a few like that, which is good in the sense that we see that the notes of the same color are grouped together, but it's even better with graph wave, because what we get with graph which we get, we get this continuous gradients of notes that are not only structural equivalent, but are more structurally similar, so.",
            "Dark of secure like light blue is more similar to this even lighter blue than orange.",
            "And that's something that is learned automatically without having information on node labels."
        ],
        [
            "OK, so summary so far while I talked about so Far East, I described an approach to embed nodes in the network.",
            "Such debt performing algebraic operator or operations in the learned embedding space reflects topology of the graph.",
            "There are different notions of note similarity.",
            "We can simply have adjacency based.",
            "No similarity that would be matrix factorization we can have.",
            "Note similarities defined based on random walks.",
            "What we did so far in general, what we need to do is we need to define note similarities that work best for our application.",
            "So meaning when you have a new data set and you use a method that learns embeddings and that is true for networks or other types of data, you need to ask yourself.",
            "What is the situation when I want to for the two examples in my data set to be embedded close together?",
            "Under what conditions should those examples be embedded close together and that will give you information on what is the notion of similarity that you want to optimize for?"
        ],
        [
            "OK, so this concludes the overview of shallow, shallow North embedding methods."
        ],
        [
            "And now I will describe a few biomedical applications of those methods to real problems."
        ],
        [
            "In particular, I will describe 2 problems.",
            "In the first will be dispatched for detection problem, which is, well, relatively well defined problem in bioinformatics and computational biology where the goal is to identify proteins that are identify new proteins associated with the disease.",
            "In this, in the second problem will be the problem of predicting protein protein interactions, where the goal will be to identify pairs of proteins that physically interact."
        ],
        [
            "And let's start with the first problem, which is the problem of disease battery detection.",
            "So the data that will be we will be working with will be the human protein protein interaction network, which means we have a network where nodes are proteins.",
            "Edges indicate physical contact between those proteins."
        ],
        [
            "What has been discovered it all during the the core or the last 20 research in the last 20 years or so is that there is this key principle saying that proteins that interact with each other tend to overlay, tend to underlie the same phenotype, and we will use this principle to identify knew potential disease associated proteins."
        ],
        [
            "So this means that if when we look at the set of known proteins associated with a particular disease, which which, which in this case is what we will refer to us Pat way, and there are many different definitions of pathway.",
            "But in this example pathway for us will be then some network of proteins that are associated with the disease in our interactive.",
            "So we had the interactome for a particular disease.",
            "We say we look at what are currently known proteins associated with the disease, and that defies as a sub defines.",
            "US and gives us some network."
        ],
        [
            "Our prediction task in the in the disease Pathway detection task is then the following we we we have our protein interaction network on the left and we have for a given disease known disease associated proteins, which are those threat proteins.",
            "They might be as connected before my former connected subnetwork or not.",
            "So this is our disease pathway and then we want to have the method that will.",
            "Take this as input and what it will do.",
            "It will identify new disease proteins so it will identify that.",
            "Perhaps these two blue notes are.",
            "According to the method, new candidates that are likely to be also represent proteins that are likely to be associated with the disease."
        ],
        [
            "So in order to do that, we need to 1st the spectral data set.",
            "In our case, what will be using is a protein protein interaction network that is called from some.",
            "Some number of knowledge databases and it's also includes the some of the largest interactive mapping efforts from Marcus with Al Group from Harvard and others will use currently known information on protein disease associations that come from this Jeanette and other knowledge bases and the prediction test that we are one that we will be solving here is that of multi label node classification because every note in that network every protein can have can be associated with 01 or with multiple diseases.",
            "And that is something that we want to discover."
        ],
        [
            "So how can we use our note embedding methods that we have heard about minutes ago too for this problem we will do?",
            "We will.",
            "We will have the setup that has the following two stages and that's a fair, very standard way of using those.",
            "The shallow embedding embedding methods will take our PPI network will use notes too weak to learn embedding for every note, and then once we have those embeddings we will use them as input to a classifier.",
            "That will predict new disease proteins based on these proteins that are already known to be associated with the disease.",
            "Ultimately, the goal will be there to those classifiers.",
            "Poor would predict probability for a given node, 14 probability that they are associated with a given disease."
        ],
        [
            "So we can use different kinds of cross validation, train test, validation, split.",
            "I'm happy to talk more about it offline and compare a number of different methods.",
            "In the value of their ability to accurately predict disease associated proteins.",
            "So what I'm showing here on the.",
            "On the left are different plots that show ability of different methods, such as note, awake, or.",
            "Yeah, in this particular note, awake methods where every know where every point in the plot is a is a particular disease, and the value on the Y axis represents the heat rate at 100 to, meaning how many out of 100 most.",
            "High protein is that were predicted as most likely to proteins to be involved with the disease are actually known disease, associated proteins, and the best performing approaches for this problem.",
            "There are various kinds of network embeddings.",
            "Here I'm showing the results for note to weaken Beddings which were better than a method based on various latent variable models such as matrix completion or diamond with diamond, which is another network based method.",
            "The worst performer method here is neighbor scoring.",
            "So what is neighbors scoring?",
            "This is a simple majority vote on network where you have a note you look at its neighbors.",
            "Perhaps a note we have 5 neighbors in three out of five neighbors are already known to be associated with.",
            "Adrenal carcinoma, so then you would say, well, the likelihood that this note is associated with the dinner carcinoma is 3 / 5 right?",
            "So simply look at the neighbors and say I will use majority vote on my neighbors to pre identify how likely the neighbors involved with the disease and that would be neighbors scoring method.",
            "Which performs worse, but it's always good to look at.",
            "Look"
        ],
        [
            "Yes, sure.",
            "It is also allowed.",
            "Yes, yes.",
            "Yes, so that's a great question.",
            "So those that are familiar with submitted would say, well, this is not an end to end learning procedure in the the talk that I will have tomorrow, I will talk about end to end optimization where we will learn embedding such that they will immediately predict the particular outcome, such as a label of a node or an Association between two nodes.",
            "In this case we have two separate stages.",
            "First we learn embedding.",
            "Second, we use classifier.",
            "The classifier here was.",
            "The result that I'm trying to slide were obtained by logistic regression.",
            "One can use any other classifier that you would want.",
            "Yes, so here's here's yes.",
            "So the results that are shown here would be one logistic classifier for every disease.",
            "More results in papers that that is associated with their that is associated with this results are of course results were are obtained by methods that take multiple days that make predictions are for all diseases at the same time.",
            "But the principle here was really to study whether embeddings.",
            "Provide more useful information relative to some of the standard methods, so the focus was not on developing new method for predicting for doing multilabel classification task.",
            "But of course if you would give the goal would be to have the best performance or to optimize for performance.",
            "One would want to take into account dependencies between diseases since we know that two diseases that I felt typically similar have similar symptoms, they tend to have to tend to have similar genetic underpinnings.",
            "So it would make sense to have that in the leverage that correlation.",
            "Yes, exactly.",
            "Yeah."
        ],
        [
            "OK, so so this was the the first use case of the methods that are that I described where the task was that the task was to predict a particular property of nodes in the network and that is known as class node classification task and the property we were predicting was disease associations.",
            "In the second task we will we will we will apply methods to the problem of protein protein interaction prediction where the goal will be to predict a relationship or.",
            "Potential relationship between a pair of notes.",
            "So here now we are not only now interesting, only about predicting a property of one out, but the relationship between a pair of not."
        ],
        [
            "Which means what what the goal of our prediction will be for a pair of nodes.",
            "We want to predict whether they will physically interact with each other."
        ],
        [
            "The data that we will be using for that we will be again the bare minimum of data that we need for this, which is the human protein protein interaction network.",
            "And given this even this network, our goal is that for a particular pair of proteins we want to predict what is the probability that those proteins will interact."
        ],
        [
            "And so, and in order to answer this question, while what would be really useful to have is to have embeddings for edges, not only embeddings for notes, so how can we learn embeddings for edges seen so far?",
            "I describe methods that can learn embeddings for notes, and that was useful for previous tests, so our question will be how to address these tests involving pairs of notes and our simple idea will be that.",
            "Simple idea that that many new studies uses.",
            "Then given a pair of notes, common approaches to define some operator, let's call it G, that that will take the pair of notes and it generates an beddings for the pair of notes for your envy.",
            "So how to define that operate?"
        ],
        [
            "Sergey.",
            "Our goal is that this operator G needs to be defined for any pair of nodes.",
            "It can it can.",
            "We should also operate on a pair of nodes that are not directly connected with each other, and that's fundamental in order for us to make it really prediction.",
            "And what is commonly used in the literature?",
            "What you will find is fairly simple methods in for this problem.",
            "When one that you might think of it and get ideas fairly quickly, which is if you have a pair of notes and their embeddings, you will simply say that an embedding for any foreign edge that in connect UNV is simply the average of embeddings of the endpoints.",
            "Or perhaps it's a Hadamard product which means elementwise product.",
            "Or it might be awaited L1 distance or waited.",
            "Ultra distance and these are some of the fairly simple approaches that would be baseline approaches nowadays."
        ],
        [
            "So if you take this approach is an we applied to our data set.",
            "This means we do the following.",
            "We are given a protein protein interaction network with a certain fraction of edges in those.",
            "Net network removed which will represent our protein protein interaction network detar.",
            "In our test set.",
            "And again, the the experiment that will do will have two main stages.",
            "First will use note a week to learn an embedding for every note in the network, and then we will predict a score for every protein pair in the test set based on embeddings of based on embedding of an edge.",
            "Which means we take our operator GGR, allow us to take embedding of the notes, getting bending of the edge, and then we have a representation for every edge, and again fairly standard.",
            "One can use any standard machine learning model to define that as a classification task where where we're at embeddings that represent actual edges in the network are positive examples and embedding and embeddings that represent.",
            "Hi things they do not know that are currently represent non agency in the network are negative exam."
        ],
        [
            "OK, so so here are some results obtained.",
            "With with with the simple methods so the the data set that we are looking at as is in the second column and which is a 1440 direction data set.",
            "And what is interesting is that the embeddings that are learned by note awake and methods that use this stochastic definition of network neighborhoods based on random walks or multihop similarity rather than direct adjacency matrix drastically outperform some of the heuristic baselines which would be these green box here on the left.",
            "And they also outperform some of the earlier methods for network embeddings, such as.",
            "So such a spectral clustering deep walk in line."
        ],
        [
            "OK, so that was a very simple trick that I showed you for how to get edge embeddings or embeddings for for a pair of nodes from embeddings for individual nodes.",
            "And also this example highlighted how some of the shallow North embedding methods can be used for problems related to link prediction, where the goal is to predict knew to discover new edges in the network."
        ],
        [
            "So this concludes Biomedic tool by examples of biomedical applications for for today."
        ],
        [
            "And in particular also the this second part of the talk about notable things in the third part I will move forward with hetrogenous networks and explain how how, how some of network embedding methods that we have looked at so far can be can serve us to develop methodology for heterogeneous networks."
        ],
        [
            "OK, so so far we focus on homogeneous networks, which means those that paid careful attention.",
            "You probably notice that in all applications that I showed what we had was really simple networks in the sense that the notes are always proteins and then edges indicate physical interaction between proteins, right?",
            "One might, but we might might have different kinds of information, different kinds of relationship between proteins that might be interesting and useful to to consider, so really.",
            "The question here is can we embed heterogeneous networks or more generally, knowledge graph?"
        ],
        [
            "Knowledge graph so many networks in biology are actually heterogeneous networks, and here is just one example of different kinds of entities that might interact with different in different ways with each other, and we might want to consider that when learning representations for four for nodes.",
            "Formally, we often think of heterogeneous networks as US networks where we have a certain number of nodes types which we often call modes and different kinds of relations between between the notes.",
            "So here is a more abstract example where we have one.",
            "When we have six modes, meaning more notes of six different types, and then we have edges between nodes of each type and edges across different types of notes.",
            "So this will be the heterogeneous networks that will be working quit today, Ann.",
            "More much more in detail tomorrow when we will."
        ],
        [
            "About our deep models.",
            "The motivating problem for today will be the problem of predicting protein functions in different tissues.",
            "So it has been mentioned several times in the last few days.",
            "The importance of predicting protein functions and that is really one of the Canonical tasks in functional genomics.",
            "Since we know that proteins are working worker molecules in our cells and it's important to understand what their roles or their functions are.",
            "Of course, we know that those the rules or functions that proteins have depend on the tissue context, meaning that a particular protein might have different function in different issue.",
            "And potentially proteins that are active in similar tissues might have similar functions.",
            "So the goal really are that will have today is how to predict protein functions across different tissues and why."
        ],
        [
            "Is this problem challenging or it's hard?",
            "So there are a number of reasons, but let me mention three of them.",
            "The first is that teachers in human body are inherently organized in a hierarchical, multiscale manner, meaning they are related to each other in some way that will audit relationships.",
            "Organization tend to be hierarchical Ann, and current research has shown that proteins active in similar tissues tend to have similar properties and functions in those tissues.",
            "Second challenge is that.",
            "Many issues have no annotations, meaning that for certain tissues that is very hard to work with or impossible to probe them or obtain samples for, though for those issues that it's much less data available about what are the functions of proteins in the tissue, potentially those issues are completely unannotated, which means that or in machine learning language this means that there we will have some tissues for which there will be no label, no no labeled examples.",
            "Or just a handful of examples, so we really need to operate in that low data regime.",
            "And Thirdly, there is a set.",
            "Since this is a function, prediction is a Canonical problem.",
            "In functional genomics there's a number of methods that have been developed for the problem of developed for predicting protein functions.",
            "However, those methods are based on the Sumption that the functions are the same irrespective of the context.",
            "So that would mean that they could.",
            "Without, which means that functions in heart are same as functions of proteins in ski or functions in brain are the same as functions of proteins in the skin.",
            "These are the predictions that one would get by running current methods.",
            "We know that even that is that is not the case.",
            "So what we want to have is the methods that can make this tissue specific prediction."
        ],
        [
            "So our goal will be the following.",
            "Given a protein, given a tissue and function, so we have these three objects.",
            "Want to predict how likely it is that the protein has the particular function in that issue, and that is a much more precise prediction than simply saying that the protein has this function somewhere in the human body.",
            "So what this means at the high level we are solving the following test.",
            "So we start we start on the left with with the network perhaps or with a network of brain tissue.",
            "For the brain tissue, we have a network of water proteins and how those proteins interact with each other in brain tissue.",
            "For some of those proteins we have information on their functions and brake issues, such as perhaps the brain development and there is supporting known to be involved with the brain development in brain tissue or the protein involved in angiogenesis.",
            "We want to have a method that what will be able to do it.",
            "That method will allow us to label the remaining proteins in debt or narrow tissue brain network according to how likely they are to be involved.",
            "To be important for different functions.",
            "So the previously white notes on the left become red or blue notes here.",
            "So this is a toy example that you want to have."
        ],
        [
            "OK, So what realistically, how, what what we have is of course not just brain tissue network, but we might have a hierarchy of tissue specific networks, and that's what we will be working with here.",
            "So what we will have our multimodel tissue network, where for every tissue in human body we have a separate protein protein interaction network specific to that issue.",
            "We know how tissues are related to each other based on that issue hierarchy and run to take into account the tissue specific protein interaction network.",
            "Plus the soft tissue hierarchy to learn embeddings that are indicative of our interaction information as well as tissue context."
        ],
        [
            "So this means that what we want to do is, if you remember before we said that with the embedding approaches we want to learn, specify what are mapping functions are that take notes and learn and map them to the dimensional space in this particular context will have multiple such mapping functions, in particular, if our input will be teacher network with four with four 44 tissue specific networks.",
            "So G1 JK&L.",
            "We will learn a mapping functions that for for every for each graph as well As for each scale in the network.",
            "So we have.",
            "That means that for a for a particular note you, this is no that is active on three out of four networks.",
            "We will have three functions that mapping functions that will take note you an information on whether it's present in the first GIK or L, and depending on that will get an embedding for no tool for each of the graphs.",
            "As well as an embedding for nodule that captures the activity of this scale of the network, meaning that aggregates the topology of note you across these two networks, or at this scale, which means that will aggregate the topology courses Phoenix Open or do across these three networks or across all networks at the same time.",
            "So we have this multiscale embeddings that will help here that might be that that will be useful for us.",
            "So the question then is how to learn this mapping functions which are now much more complex than what we have seen before."
        ],
        [
            "So a bit more formally, are set up with the following.",
            "What we assume as input is that input.",
            "We have some number of graphs.",
            "All in these graphs are related to each other through some hierarchy, so those graphs might have might be are different from from each other, and there are different subsets of nodes that are present.",
            "In that hierarchy in that hierarchy, tell us how those graphs are related to each other.",
            "The assumption is that graphs are in the lives of that hierarchy, so here I would say we have this example.",
            "In this example, we have four graphs and GI&JRR according to our prior information are most similar to each other.",
            "That's why they are they are.",
            "They are neighbors in the hierarchy, and there are big, less related to GK and GS.",
            "So as I said, we will do a model.",
            "We want to have a multiscale models, which means that we will learn embeddings for notes for each in each of those graphs as well as at each of the internal notes of the hierarchy.",
            "So we learn what the most effective way to combat to learn a representation of a note for all for us.",
            "For a subset of networks or subset of graph, so the output will be note embeddings for every for each paragraph, and for each sub hierarchy."
        ],
        [
            "Our approach will have to create components and the first component is the component that will translate into our part in the objective function where we will model each graph separately.",
            "So we will say 444 notes in each graph.",
            "We want to encode and learn their embedding such that for notes in that graph 2 notes will be embedded close together if they have similar local topology.",
            "So that is first component that we want to take into account.",
            "The second component is that we want to encourage nodes that.",
            "Are active in similar graphs to also have similar embeddings, so this is how we will transfer or fuse information from one graph to the other."
        ],
        [
            "So this high level intuition then translates to an objective function fairly naturally.",
            "We have two components in our objective function.",
            "The first is the single graph objective where where what we do is the following.",
            "Intuitively, in each graph we will embed notes in the two dimensions and for a pair of notes unv will say well there are similar if they are similar according to that particular graph that we are currently looking at.",
            "And the notion of similarity that we will be using is based on the idea of random walks that I presented earlier in the talk.",
            "So we'll say forever for a particular note.",
            "You, the neighborhood of note you in this.",
            "In this graph, GI is defined based on.",
            "Notes that are visited by random walks when we simulate random walks in that graph.",
            "So if we start without you here note random Walker might visit these three other nodes in words.",
            "Or perhaps these and these three notes and it will say that this red notes are the neighbors of note you in the graph GI."
        ],
        [
            "So.",
            "This this.",
            "This means that our objective function is the following for a particular note you what you want to maximize, we want to maximize the probability that the embedding of note you in this in the ground that we're currently looking at graph GI is able to predict well its neighbors, and if it's able to predict its neighbors, that's an indication that the embedding of note you is good embedding in debt for for no GI.",
            "Given that we have multiple graphs, we want to maximize this.",
            "Note level objective across across all nodes in each graph in the ground to her GI and then across all graphs that we have in her in that we have in our input data."
        ],
        [
            "So the summary so far is that, well, that is great, but we have not yet considered the hierarchy M at all.",
            "Meaning that though if it would simply run the used objective function that I showed in the previous slide, we will learn embeddings for the notes and embeddings in each of the graphs would be run, learned completely independent, but that's not something we want.",
            "We want to encourage nodes that are that come from similar graphs to be similar as well.",
            "So how to model these interdependencies between graphs when we learn?"
        ],
        [
            "The embeddings.",
            "So recall that what we have is hierarchy of grass M. In our case would be given to us in the form of parent child relationships.",
            "So if we have our data set looks like that when we have four graphs in a hierarchy of these photographs, then two is apparent of GI&J and one is apparent.",
            "2G KNGL and this pie will be is just a formal representation of our hierarchy."
        ],
        [
            "And we will use that to define the cross graph objective.",
            "So the cross graph objective.",
            "What we'll do it will allow us to encode dependencies between between graphs and formally this will.",
            "This is something that we will achieve by using a recursive regularization, which means that embeddings at each level of the hierarchy will be encouraged, encouraged to be similar to embeddings in the parent of the hierarchy."
        ],
        [
            "Formally, this means that if we have a particular note, so we look, we are currently looking at.",
            "So we are looking at note you will say that when we are learning and embedding for no chewing graph GI.",
            "We want him banished for no chill to be to be similar to embedding of nodule in the parent of GI parental gispert.",
            "So in the simplest case this translates to this form of regularization when we say them begging for nodule in the current graph that we're looking at should be embedding for note you are the one.",
            "At one level up in our hiring.",
            "And this is the property that is repeated at every level of the hierarchy, and that we have."
        ],
        [
            "So if we put both parts of our objective function together, what we get is we need to solve the following maximum likelihood problem, whereas we said the goal is to find this mapping function that will make notes to the dimensional embeddings.",
            "We have two types of objective terms.",
            "The 11 type of.",
            "One important term in objective function says that for every graph that we're currently looking at learn embeddings that are indicative of local network structure in that graph, and the second part, which is this recursive regularization, says that for nodes active in similar graphs we want there in we encourage their embeddings to be similar as well."
        ],
        [
            "So what is the final algorithm that one can use so the final algorithm is the following?",
            "So for each graph that we have, we will first define the neighborhood of notes in those in the in the graph, and we do that using this fixed length.",
            "Random walks idea that we learned about when I was explaining to to work once.",
            "Once we have the neighbors for every noting in every graph, we optimized objective function that we have seen in the previous slide.",
            "That includes single graph and cross graph terms, and we can use that using stochastic gradient descent.",
            "So the reason why we have actually such schema for learning embeddings is because it this, because it makes our approach is very scalable.",
            "In particular, why this is the case is we do not need to do any kind of pairwise comparison of nodes across the graph.",
            "So if we would not have a hierarchy, then a naive approach would be that we we look at, we have a set of graphs and then we go across all possible pairs.",
            "Off notes across all graphs, and then we say, well, they should be similar somehow, but that would give us quadratic complexity in the number of nodes where this number of nodes is defined as the sum of the number of nodes across all graphs.",
            "Instead, what we do, what we get our is linear time complexity because we do not need to compare or pairs of graphs, but we only compare graph compare embeddings of notes to their embeddings in higher levels of the hierarchy, so this hierarchal structure.",
            "Makes the approach extremely scalable."
        ],
        [
            "OK, so enough about the details how to compute those embeddings left look at on a concrete application to biomedical data."
        ],
        [
            "And as I said, the problem that we will be tackling here is that of.",
            "Protein function prediction particular we want to have a model that will be able to answer questions of the form.",
            "Why does my protein do so?",
            "We want to have a model that, given a particular protein, a tissue and a function, it will be able to say how likely it is that the protein has the function in specific tissues."
        ],
        [
            "So the data that will be using for for that order.",
            "We compiled for for that is data that has 170 show graphs.",
            "So each graph here is supporting port Interaction network in specific human tissue and then tissue hierarchy that relates tissues to each other.",
            "That is that comes from Brenda Tissue Ontology.",
            "So this gives us a data set that looks like that.",
            "So this is the higher this is the hierarchy M for this problem.",
            "So the leaves of this hierarchy are blue notes.",
            "And then internal nodes are various hierarchical representations of tissues.",
            "So how tissues for multi tissues and organs and organ systems and eventually the root here would be entire human body.",
            "Blue, noting that the hierarchy which is hierarchy and the thing that we have seen in the.",
            "Methods part of this section, beliefs here.",
            "This blue notes are actually.",
            "This dish are actually tissue specific network.",
            "So actually the single rule out here in the hierarchy when you look close at it.",
            "What you get is a protein protein interaction network that is specific to this tissue human body.",
            "So, so this is the data that we have.",
            "Will be using for this problem.",
            "It's to tell us if it represents the hierarchy M and a set of graphs.",
            "GI that are that are 107 of them and the third represented by this blue notes here.",
            "In terms of note labels, what we have is some number of tissue specific protein functions, which is currently known.",
            "Informations about what are functions of proteins, specific human tissues.",
            "This largely came from a collaboration between Scala Lab at Princeton, or which is now this also publicly available.",
            "Data injection, giant or human based system that developed and maintained by flat tire.",
            "And the way these data comes from, it comes in the form of saying that it comes in the form of information on a function.",
            "For example, cortex development.",
            "But these are active in specific tissues.",
            "Rainer contact issue and then a set of proteins that are that are involved in quarters development in renal cortex, tissue or etc.",
            "Parties involved involved in archery, morphogenesis in artery tissue."
        ],
        [
            "OK, so experimental setup this the problem that we're holding solving here is that of multi label node classification task.",
            "So the query that we have an example query is does a gene.",
            "RPT one.",
            "Does it play a role in processing function angiogenesis in specific tissues in blood tissue?",
            "So that is a query that that we are able to answer with their system.",
            "Every node protein is assigned one or zero, one or multiple labels in specific graph and our setup will be the following first.",
            "Take it home constructing.",
            "First, we construct our multimodal tissue network using the hierarchy in tissue specific protein direction at work, and we learn features or embeddings for debt multimodal network.",
            "Second, we train a classifier for each function or multiple functions at the same time based on the embeddings and known label information.",
            "And 2nd we predict functions for proteins that and Thirdly we predict functions for new proteins.",
            "Here again, we can leverage various kinds of advancement that have been made recently by taking into account the hierarchal structure of protein functions that come from gene ontology and so on.",
            "Those that are familiar with this line of."
        ],
        [
            "So here's the results obtained by.",
            "Function level, cross validation and the methods that I have been discussing is called on it and the methods performs the methods that this method that can make this tissue specific protein function predictions turn out to be much more accurate than some of the other baseline methods.",
            "So what are the other baseline methods?",
            "So the first baseline method that one my task is to say is to use the state of the art method for 14 function prediction that ignores tissue context, which means the state of the art work at this time.",
            "These results show that it's really important to integrate this additional level of information on tissue context and contextual information on where in the human body proteins are active.",
            "When we are predicting functions.",
            "Of proteins and other the third line here.",
            "What it shows is the performance gain of Omnet relative to methods that would learn embeddings for networks, in dependently ignoring the hierarchal or structure in hierarchal similarities or interdependencies between graphs in different tissues which would mean for a given tissue specific network we use.",
            "Note awake.",
            "We learn embeddings, we we use those embeddings for.",
            "A simple classifier that makes predictions, ignoring all other tissue specific protein tissue, specific port interaction networks, and the final comparison here shows the improvement of this method over perhaps a fairly classic about well founded method based on things or decomposition where the idea was how does the method compare to the approach where we would construct a tensor where every mode of that answer would be.",
            "Protein protein interaction network in specific human tissue.",
            "So in the case when we had 107 tissue specific networks, we do have a tensor, which would be one which would have 107 modes.",
            "In each mode, then, would be one adjacency matrix of the networking that tissue and we would run.",
            "These are the composition on that things.",
            "So the reason why we see improvement in some of these methods over teams or the compositions is primarily because network embedding methods can can are much more flexible and can take into account not only direct neighbors but also indirect neighbors and have this stochastic flexible definition of network neighborhood."
        ],
        [
            "OK, so a small case study that we did here focused on brain tissues.",
            "So what I'm showing here on the left is a part of the hierarchy that contains 9 brain teachers and for each of those tissues shown here is this blue note.",
            "We have tissue brain tissue, specific protein direction and the hierarchy.",
            "Here follows what we know.",
            "Our autonomy, anatomical relationships between different brain tissues.",
            "So we did.",
            "This case study was to take to assume that our hierarchy and is given by this organizational brain tissues and we have nine brain tissue specific networks.",
            "Now let's learn and train on it on this network to learn embeddings for notes.",
            "We can learn embeddings for notes and then we can visualize them.",
            "So this is what happens when we visualized them.",
            "So.",
            "If I turn back here, you can see that this is this hierarchy had to has two levels, so there is the root of it, which in this case is brain.",
            "The brain has this five different tissues that are immediate children, and then it's brain stem, which further has four four different kinds of tissues.",
            "OK, So what we what we do, what we did, and we visualize embeddings at the brainstem level at end of the brain level.",
            "So."
        ],
        [
            "Or the brainstem level.",
            "Here we we, we get visualization that looks like like that.",
            "Every point here is a protein.",
            "Protein is colored by the tissue that it's active in.",
            "And what is important here is that the embeddings that were learned or learnt in such a way that that that that we get very naturally and automatically with any kind of information.",
            "Additional information we get is clear clusters.",
            "Of of the same color, representing 40 is active in the same tissue, and this happens even when we look at one level up at the level of brain.",
            "When we get this clear, clusters of proteins active in different tissues and the reason why we can do that at the level of brain is because we learn this multiscale embeddings."
        ],
        [
            "As we get, we get an embedding for the note.",
            "Well for us before before each graph.",
            "But also Allaire jesters of the hierarchy so we can visualize how to embedding's look like on each level of the hierarchy, such as each network or on each graph or the brainstem level or of the brain."
        ],
        [
            "And this shows that embeddings that we learned actually were such that they adhere to old assumptions that we are making in the data about proteins being active in a similar way having similar embeddings and proteins being active in similar tissues having similar imbalance."
        ],
        [
            "Finally, what we did.",
            "Here is is is using the methodology for the problem of transfer learning or annotating tissues for which none of the information is available currently so.",
            "The idea on the task the task here was to predict functions in a specific tissue without having access to any label information in the tissue.",
            "So really, the assumption is so that the simulation here is what happens when.",
            "Approach interaction network for a particular context is available.",
            "It's given to us and our goal is to say our goal is to label proteins in that issue.",
            "What can we say about functions of proteins in debt?",
            "Protein interaction in the newly generated 40 interaction network when none of the labels in debt network RFA.",
            "So what so this was the setting that was that was tested here.",
            "Here I'm showing performance in terms of AUC Roc for for this setting where in particular you can really stable as follows.",
            "Um so.",
            "Pete Rose represents a different target tissue, so the first draw.",
            "What he tells us is that one of the disposition was natural killer cell.",
            "And when we include information on natural killer cell, the.",
            "When we make prediction based on currently available annotations on natural Killer cell, this is the AUC Roc score that we get.",
            "However, when we pretend that there is none, information is available on natural killer cell and we want to annotate natural killer cell tissue.",
            "The aims Iraq is somewhat lower, but it's still very much different from what one would expect if there would be no predictive power.",
            "So these final roll the spinal column here, tell us the power of the methods to be to predict an annotate tissues, if no.",
            "Labels for death tissues are available.",
            "So the reason we can do that then how we actually do that is by taking into account the embeddings in the structure of the hierarchy.",
            "To predict, make predictions for, for, for a completely new tissues, which with no labels.",
            "Yes.",
            "Yes, that's that's something yes.",
            "Yes, yes we need to have its placement in the hierarchy which.",
            "Which I would argue is fairly reasonable assumption to some extent, some extent.",
            "This interesting that having this.",
            "This very small amount of information where the new tissue is placed in the hierarchy gives us lots of power already to be able to transfer annotations from from other tissues to date to proteins in the in the mutation."
        ],
        [
            "OK, so this concludes this first part of the talk for today on heterogeneous networks when I where I talked about how to embed it, true genius networks, an application on tissue specific protein function prediction.",
            "And in the talk in the lecture tomorrow we will build on this through Genius Network idea and specifically will be focused on graph neural networks for learning deep embeddings of notes.",
            "For for the most general representation of heterogeneous networks and knowledge graphs."
        ],
        [
            "Some of the resources that I would like to mention here that might be useful.",
            "Are the following.",
            "One of them is Mambo, which is a tool that we developed for multimodal biomedical network representation.",
            "So it's the tool that's that's that allow you to build, construct, represent, and analyze highly multimodal networks in the form of also solving some of the challenges with linking nodes that you might use different naming terminologies in different datasets and then achieving provenance of those data sources that are used as input to construct this multi model networks.",
            "In the two scales really well to very large networks, so the larger biomedical network that we use, this tool weight had 2.3 billion edges in 2000 nodes and was really very large knowledge graph of biomedical data.",
            "Second, for most of the projects that I outlined in this fight here in this lecture, the the network data is available at project websites of some of these studies and papers.",
            "Epstein, 40 do you slash projects for the more there is the new repository that we have started on.",
            "Biomedical network datasets, which is a repository of high quality preprocessed Clint biomedical networks that are linked so you can combine them in various different ways you would like with the goal of providing methods that can be used for algorithm development and benchmarking.",
            "And so that those kinds of biomedical datasets will be will be more often used in methods on new algorithms so that we can see how this biomedical, how new algorithms work on biomedical data, and that's called bio SNAP or Bio data.",
            "It's really."
        ],
        [
            "OK, so to conclude, this was today was the first part of the lecture.",
            "What we discussed is shallow net various methods for shallow network embeddings.",
            "Some of the takeaway messages are, but there exist many different notions of how different define a note similarity depending on that notion of note similarity, you can learn embeddings in different ways.",
            "And then finally, it's very important and useful to integrate different kinds of prior information in our model, which was demonstrated with Virginia's network comparing model and that will become more clear tomorrow when we will look at more general knowledge graphs.",
            "So tomorrow I will move away from shallow network embedding methods, focus on deep network models, and specifically describe how those models are useful for polypharmacy in computational direct repurposing.",
            "OK, and then I can.",
            "I can take more questions for this.",
            "Thank you America for the exciting lecture."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's great to be here.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Organizers for organizing such a nice workshop here in Slovenia.",
                    "label": 0
                },
                {
                    "sent": "It's always great to visit to have an excuse to come here.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll be giving two lectures, one today and one in the morning, and my goal is really to make these two lectures more like tutorial like so I will really drill down and explain the methodology from beginning to the end.",
                    "label": 0
                },
                {
                    "sent": "Also provide codebases datasets an examples for yours and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the two lectures that I have to or are the following.",
                    "label": 0
                },
                {
                    "sent": "So today I will focus on network Embeddings's methodology and I will explain methods for doing that for doing for embedding networks in graph structured data I will provide some pointers to resources that we have developed in the form of datasets, tools and code breaker code bases and I will describe a few biomedical applications focused mostly on protein, protein interactions, disease pathways and tissue modeling.",
                    "label": 0
                },
                {
                    "sent": "And tomorrow in the morning.",
                    "label": 0
                },
                {
                    "sent": "I will focus more on graph neural networks, which are methods for learning embeddings, but with using multiple layer, not layers of nonlinear transformations.",
                    "label": 0
                },
                {
                    "sent": "So what we will learn today will be really relevant and foundational for what we'll be discussing tomorrow.",
                    "label": 0
                },
                {
                    "sent": "And again, I will provide to malls tomorrow as well.",
                    "label": 0
                },
                {
                    "sent": "I will provide data sources, anile practical advise us and if there will be time will actually look at the code and run some demos so that you will actually see how how it works in practice and in terms of applications.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow I will focus on.",
                    "label": 0
                },
                {
                    "sent": "Polypharmacy and Drug Purpose Inc, which is very closely related to the topics of this workshop.",
                    "label": 0
                },
                {
                    "sent": "So to just give a preview to make you excited to, I'll give you 1 slide.",
                    "label": 0
                },
                {
                    "sent": "Here's one slide.",
                    "label": 0
                },
                {
                    "sent": "Previous slide of tomorrow's lecture, so the methods that I will describe today and tomorrow we use them to predict and model drug combinations and identify side effects an the site safety safety of drug combinations, and that was the first ever study where we looked at all possible combinations of drugs that are approved on the US market.",
                    "label": 1
                },
                {
                    "sent": "That has led to an interesting follow-up research that we have on prostate cancers as well as validations in the clinic on real life patients with high risk medical school.",
                    "label": 0
                },
                {
                    "sent": "So I will talk about it tomorrow and then.",
                    "label": 0
                },
                {
                    "sent": "Second point is the method that I will describe.",
                    "label": 0
                },
                {
                    "sent": "Also use them for repurposing existing drugs.",
                    "label": 1
                },
                {
                    "sent": "On that roundup approved on the market for new diseases, which has led to interesting new metal development and also too.",
                    "label": 0
                },
                {
                    "sent": "Has directly impacted drug repurposing at Stanford.",
                    "label": 0
                },
                {
                    "sent": "And all these studies methodology the way we introduce it to domain expertise.",
                    "label": 0
                },
                {
                    "sent": "By developing this large systems or data science pipeline is that the the core of those pipelines?",
                    "label": 0
                },
                {
                    "sent": "Is the methodology that I will describe, and it allows them domain experts to interact with the system in this form of asking queries, get predictions, ask why certain predictions are the case.",
                    "label": 0
                },
                {
                    "sent": "So too they get explanations and interpret ability or options, and that is really open.",
                    "label": 0
                },
                {
                    "sent": "Opportunities to introduce these deeper models to domain experts.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But today let's start simply hand.",
                    "label": 0
                },
                {
                    "sent": "Let's start with network embeddings and the applications for today.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To motivate this work at a very high level, I see that science crucially depends on the existence of scientific instruments.",
                    "label": 0
                },
                {
                    "sent": "So in the past in this, in this kind of instruments were physical devices, and physical devices were those that have facilitated facilitated new discoveries in science.",
                    "label": 0
                },
                {
                    "sent": "But now, with modern data intensive science is we need a new kind of instruments and you kind of tools that I envision will enlarge way depend on knowledge discovery from data.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, knowledge discovery from biomedical data presents some unique set of challenges, and one of the fundamental challenge that I see is that for any particular by medical problem that you have, there are a number of different data data sources, datasets, or data types that might be relevant for that particular problem, and those datasets go from the range of genomic information to seller.",
                    "label": 0
                },
                {
                    "sent": "Al Petteway information.",
                    "label": 0
                },
                {
                    "sent": "Various transcriptomic gene expression data all the way to up to behaviors.",
                    "label": 0
                },
                {
                    "sent": "Lifestyles in vital signs.",
                    "label": 0
                },
                {
                    "sent": "So it's probably it's really challenging how to bring this data together, especially because it's multi scale, so it describes our problem at the level of molecules in the up to individuals up to entire populations.",
                    "label": 0
                },
                {
                    "sent": "It's heterogeneous in in the sense that it generated by different labs using different platforms, and it might get it until it is confirmed that in a number of different ways due to investigative biases or due to constraints of current by technological.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of data then means that the biomedical data often does not fit into one flat tabular data set, and I see that as being a major gap between prevailing machine learning settings so that is or machine learning models that are primarily designed for tabular, monolitic flat matrix like datasets and by medical problems that often have complex multi scale heterogeneous datasets and that is significantly significant gap between what machine learning can address today and what are real world biomedical problems.",
                    "label": 0
                },
                {
                    "sent": "Require us that we develop new mathematical representations that can integrate biomedical datasets in their broader sense and then develop methods that can learn in all reason over such representations.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the outline of today's lecture will be the following.",
                    "label": 0
                },
                {
                    "sent": "I will first talk off of by my described by made biological networks as a search mathematical representation that allow us to really look at various kinds of data, and I will present some motivations why networks or are very good resource to think to use for biomedical problems.",
                    "label": 0
                },
                {
                    "sent": "In the second part I will talk about note embeddings and network embeddings and methods that can take networks an learn some low dimensional representations that can really then enable us.",
                    "label": 0
                },
                {
                    "sent": "Do machine learning.",
                    "label": 0
                },
                {
                    "sent": "Can graphs with some applications, and in the third part of the talk I will talk about heterogeneous networks, which is which is really what we need in many real world applications.",
                    "label": 0
                },
                {
                    "sent": "So to look at different kinds of data using this principle of multi model, multi model or heterogeneous networks.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's start with the first part.",
                    "label": 0
                },
                {
                    "sent": "So also sad netic networks us as a mechanism.",
                    "label": 0
                },
                {
                    "sent": "Also general language that allow us to integrate biomedical data.",
                    "label": 0
                },
                {
                    "sent": "For example, we can start at the level of populations where we might have different individuals that interact with each other in different ways based on their physical proximity based on social relationships based on family ties.",
                    "label": 0
                },
                {
                    "sent": "And we can go down to the level of diseases or phenotypes that exist within an individual and all the way down to interactions between various components within a cell of that individual.",
                    "label": 0
                },
                {
                    "sent": "So what networks then allow us to do is that then allow us to model the kind of interactions within each scale, and then those interactions across different scales so we can bring together multiple scales of information and we can bring together different kinds of interactions between the between those components.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why, why why networks and why now is a good time so networks have been around for a number of years and they have lead to very interesting, exciting research and results in the context of biology and medicine.",
                    "label": 0
                },
                {
                    "sent": "And let me give you a few examples.",
                    "label": 0
                },
                {
                    "sent": "So very early on or people were excited and still are about understanding.",
                    "label": 0
                },
                {
                    "sent": "How are diseases related to each other?",
                    "label": 0
                },
                {
                    "sent": "What are proteins associated with different diseases and how can we study that?",
                    "label": 0
                },
                {
                    "sent": "And using network based approach?",
                    "label": 0
                },
                {
                    "sent": "This has led to interesting findings.",
                    "label": 0
                },
                {
                    "sent": "That have uncovered various relationships between how genes with similar genomic information lead to disease us that our fellow typically simila",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In another example, networks have been used to really understand and even simulate a basic of kariyat Excel in here.",
                    "label": 0
                },
                {
                    "sent": "In particular, I'm sending a very a paper that I I like a lot by tryied occurs group very recently where they have used the notion of cell hierarchy and encoded as an actual architecture for neural network to learn and simulator basic occur at Excel.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Furthermore, moving beyond molecular biology networks are having very excited.",
                    "label": 0
                },
                {
                    "sent": "It's interesting object to study and understand cancer heterogeneity to do things such as patient subtyping that you Sledge 2 new exciting results in terms of identifying various new cancer subtypes that could correlate well with patient survival, Anne.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not only in cancer biology, there's lots of networks an in the study of ecology to understand how the complexity of real ecosystem can be modeled and understood, and from from the systemic level.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More recently, microbiome is another example of a exciting new data modality that has been successfully studied using methods of enough network size.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So many data are networks as I as I demonstrated just now, we can think of patient properties and relationships and represent them as networks or hierarchies of South systems or disease pathways.",
                    "label": 0
                },
                {
                    "sent": "Various kinds of genetic interaction networks, jinx pressure networks, South similarity networks in the life in the world of single single cell genomics.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once we have that network representations, then the main question that appears is how to do machine learning on such biomedical networks, especially if they are very complex in reach and what we are ultimately asking is.",
                    "label": 0
                },
                {
                    "sent": "Especially in the context of current era of deep learning, how can we design models that can take the dis rich, highly irregular and complex structures as their input, and they can make exciting predictions that tell us something about properties of cells or patient outcomes.",
                    "label": 0
                },
                {
                    "sent": "The discover, new kinds of associations in relationships.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And all the networks are very powerful data representation that they are relatively challenging to work with for preventing machine learning models, primarily because prevailing models assume that we have this flat matrix like datasets where every rose separate ID example.",
                    "label": 1
                },
                {
                    "sent": "So to make this more concrete.",
                    "label": 0
                },
                {
                    "sent": "Premier primary primary paradigm of current models is that they are designed for grids or simple sequences.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by that is convolutional neural networks and various variants of those architectures are primarily designed and used for fixed size images, and you can think of every image as a as a graph where every note is a pixel and every note has exactly 4 neighbors, which is pixel to the top, bottom, left and right.",
                    "label": 0
                },
                {
                    "sent": "And then multiple channels potentially, so these are very regular structures.",
                    "label": 0
                },
                {
                    "sent": "Similarly, LSD models or recurrent neural networks are our most successful models for various kinds of sequence data.",
                    "label": 0
                },
                {
                    "sent": "Going from language to logical sequences, and they again they operate on graphs, but no graphs are simply linear chains, where every word is a token that is connected to their to the proceeding.",
                    "label": 0
                },
                {
                    "sent": "In the following quarter, or potentially DNA nucleotide nucleotide.",
                    "label": 0
                },
                {
                    "sent": "So these models have really brought extraordinary gains in various fields of computer science, including computer vision, natural language processing, and robotics, but they are unable to consider interactions, which is the essence of biomedical networks in particular.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That means is that what?",
                    "label": 0
                },
                {
                    "sent": "What about when our data is not regular in the forms of greed or linear change, but it looks something like like what is shown in the left part of the slide or when they are even more complex and their highly heterogeneous as shown in the right slide and we have seen now several examples of networks that in that that that are just examples that otherwise provided by law medical research.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So fundamentally, why is deep learning on graph structured data so so hard and challenging is because the networks have complex topographical structure, so there is no spatial locality, and in particular for every node in the network there is no in really we don't have the concept of what is known to the left to the right to the bottom there is there is a set of neighbors that every node has.",
                    "label": 0
                },
                {
                    "sent": "And fundamentally dismiss that the methods that we will develop need to be invariant to various kinds of translations and rotations which make existing models not useful.",
                    "label": 0
                },
                {
                    "sent": "In many cases we have very rich networks, meaning we have different kinds of entities, molecules, individuals.",
                    "label": 0
                },
                {
                    "sent": "And that interact with each other in different ways.",
                    "label": 0
                },
                {
                    "sent": "So we have reached networks with different kinds of nodes, different kinds of edges, and that is something that we want to take into account.",
                    "label": 0
                },
                {
                    "sent": "So real world networks, such as the one shown on the left look very different from what prevailing models are designed for, which are structures are shown on the right.",
                    "label": 0
                },
                {
                    "sent": "So what we need are methods that can generalize convolutions beyond this simple lattices and they can learn and reason over over such rich, rich data that that we represent with networks.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so that was the motivation for why we need this new methods and why the classical methods might not work well and we will return to that in various applications.",
                    "label": 0
                },
                {
                    "sent": "Now in the in the main part of the stock I will focus on describing a foundational methodology that allow us then to really think about learning and reasoning over networks.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these will be focused on not embeddings, and hear various pointers to papers and some of the tutorials that we had recently on this methodology in various venues.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when they say embedding, So what I mean is the follow.",
                    "label": 0
                },
                {
                    "sent": "So we start with our network on the left side.",
                    "label": 0
                },
                {
                    "sent": "This is representation of our particular domain and the goal is to take every noting that networks a note we and then map it through some function F and that function F will take note we in it will return some feature vector which we call embedding which is a vector of length D that has the real numbers.",
                    "label": 0
                },
                {
                    "sent": "The objective now is that we want to map.",
                    "label": 0
                },
                {
                    "sent": "Every note in the network today, the dimensional embedding space such that two nodes that have similar local topology in the network will be embedded close together in that embedding space.",
                    "label": 0
                },
                {
                    "sent": "So why do we want to do that?",
                    "label": 0
                },
                {
                    "sent": "Or we want to do that because graphs are inherently combinatorial objects, which might be difficult to work with.",
                    "label": 0
                },
                {
                    "sent": "But once we have their embeddings and feature vectors, then we can immediately use those vectors as input to downstream machine learning models.",
                    "label": 0
                },
                {
                    "sent": "Or we can.",
                    "label": 0
                },
                {
                    "sent": "Analyze them in various different ways.",
                    "label": 0
                },
                {
                    "sent": "So ultimately our goal is we want to learn this mapping function F such that when we apply it to our network we get embeddings and those embeddings should reflect topology of the input graph.",
                    "label": 0
                },
                {
                    "sent": "Really the main question then becomes, well, how do we learn that mapping function?",
                    "label": 0
                },
                {
                    "sent": "How do we learn that mapping function if?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, let's say our input will be some form of a disease similarity network, so that would be a network where nodes are various diseases or phenotypes, and two phenotypes might be connected based on their symptomatic similarity or based on comorbidity indices.",
                    "label": 0
                },
                {
                    "sent": "So that might be our input network.",
                    "label": 0
                },
                {
                    "sent": "We want to learn the function F that will for every note in that network in the.",
                    "label": 0
                },
                {
                    "sent": "In this example of disease, it will map it to some D dimensional space, perhaps just two dimensional space, and in that case we can just visualize it very simply in the plane.",
                    "label": 0
                },
                {
                    "sent": "And here in particular in this, in this example what I'm showing on the right is an example where diseases were embedded based on how frequently they call pier in patients that take particular set of drugs and that can lead to various interesting questions.",
                    "label": 0
                },
                {
                    "sent": "So really next will focus on really learning.",
                    "label": 0
                },
                {
                    "sent": "In talking I'll talk about methods that allow us to specify those embedding functions.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is our set up or setup is the following.",
                    "label": 0
                },
                {
                    "sent": "We start very simply will assume that we have one graph.",
                    "label": 0
                },
                {
                    "sent": "This is graph G. That graph is defined from some set of vertices V and the graph is represented or we will work with it in the form of adjacency matrix a simply for the.",
                    "label": 0
                },
                {
                    "sent": "We will think of ape to be to be on binary adjacency matrix and later today and tomorrow I will talk.",
                    "label": 0
                },
                {
                    "sent": "What happens when we have this rich types of graphs that might be weighted or have multiple types of.",
                    "label": 0
                },
                {
                    "sent": "Address multiple and also various kind of site information about node features and that we might want to include.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So sad that the goal or the goal that we have is that we want to map notes in the network in our input network to some D dimensional embedding space such that similarity for notes in the beddings space will approximate similarity of the nodes in the network space.",
                    "label": 0
                },
                {
                    "sent": "For example, you might like, for example, what would be an application in biology that we will see later?",
                    "label": 0
                },
                {
                    "sent": "We know that if our input network would be a protein protein interaction network, we know that in in that network.",
                    "label": 0
                },
                {
                    "sent": "Do proteins that tend to that interact with each other are more likely to be associated with similar phenotype, which gives us directly idea for how to use this, how to use this principle because two parties that will interact with each other, all that will be close in the network will be close in the embedding space, and that's the principle that can then be used to discover new disease genes for example.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so mathematically this means that if we have these two notes U&V, we want to have.",
                    "label": 0
                },
                {
                    "sent": "We want to have the method to that that we learned embeddings for notes V and for notes you envy.",
                    "label": 0
                },
                {
                    "sent": "Those embeddings would be OZV such that in the simplest case their dot product would approximate the similarity in the network.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we do we need to define here what what we need to define are two basic things.",
                    "label": 0
                },
                {
                    "sent": "So first we need to really define what is similarity between U&V in our network right?",
                    "label": 0
                },
                {
                    "sent": "And then second we need to define what is this encoder encoding procedure that takes note each note in the network and in Maps it to the D dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So that's something we need to define.",
                    "label": 1
                },
                {
                    "sent": "So as I said, an encoder a note similarity function which measure similarity, the network and once we have that similarity function and the the the encoding function, we just need to optimize parameters of the encoder such that the similarity between notes you envy approximates their dot product between the two between the two nodes.",
                    "label": 0
                },
                {
                    "sent": "And we do that for every four for all notes.",
                    "label": 0
                },
                {
                    "sent": "For all note pairs in the network.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the two key components are again encoders.",
                    "label": 0
                },
                {
                    "sent": "Encoder is the procedure that will map a note to the dimensional space, so it will take a note in the input graph.",
                    "label": 0
                },
                {
                    "sent": "We in it will return some D dimensional embeddings is AbbVie and then similarity function that will define how the relationship in the input network will map to some relationship into embedding space.",
                    "label": 1
                },
                {
                    "sent": "And for the purpose of this talk, I will assume that this similarity, the embedding space into DOT product between the two embeddings.",
                    "label": 0
                },
                {
                    "sent": "But we can, we can use some other approaches as well.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a class and methods in the literature that that follow this principle of thinking about methods that can take our input objects in networks and map them to something dimensional space and preserve some notion of similarity.",
                    "label": 0
                },
                {
                    "sent": "And many methods you similar encoder.",
                    "label": 0
                },
                {
                    "sent": "So recently the we heard some today about matrix factorizations that are very popular methods in other latent variable models.",
                    "label": 0
                },
                {
                    "sent": "More recently, their methods such as Note, Awake, Deep Walk line structure, to make any kind of.",
                    "label": 0
                },
                {
                    "sent": "Start to wake methods are basically operate on this principle of having some notion of similarity in our input data and some notion of encoder and this method use different notions of similarity, particular when our data is networks.",
                    "label": 0
                },
                {
                    "sent": "We can say well, two nodes should have similar embeddings if they are directly connected by the by by an edge.",
                    "label": 0
                },
                {
                    "sent": "So this is the fundamental principle that is used in matrix factorization algorithms, right?",
                    "label": 0
                },
                {
                    "sent": "So we take adjacency matrix and we simply factorize it.",
                    "label": 0
                },
                {
                    "sent": "Adjacency matrix encodes.",
                    "label": 0
                },
                {
                    "sent": "Direct relationships edges between nodes which which means exactly that 2 notes that are related to each other, but there will be a one in the corresponding value in the matrix.",
                    "label": 0
                },
                {
                    "sent": "They will have similar embeddings as learned by matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "We might have more advanced notions of what this means, that 2 notes should have similar embeddings.",
                    "label": 0
                },
                {
                    "sent": "Perhaps to note, should have similar embeddings if they share many neighbors if they are connected to each other by many shortest by many short pets, or.",
                    "label": 0
                },
                {
                    "sent": "Any kind of other other notion of similarity?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So next, what I will talk about is I will talk more concretely about a few prominent methods for networks that can generate and learn embeddings.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I called this matter shallow note embedding methods, so why shallow?",
                    "label": 0
                },
                {
                    "sent": "Because they will they will.",
                    "label": 0
                },
                {
                    "sent": "They will do us one layer of linear or nonlinear transformation on the network to learn embeddings.",
                    "label": 0
                },
                {
                    "sent": "There also sometimes called direct direct encoding methods such as in May many matrix factorization methods fall under this area.",
                    "label": 0
                },
                {
                    "sent": "I will talk about other types of challon automatics.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow I will talk about deep note embeddings and then about by then we'll see some applications in biomedicine.",
                    "label": 0
                },
                {
                    "sent": "OK, so shallow note embeddings so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To set what we need is we need to 1st define what does it mean for us.",
                    "label": 0
                },
                {
                    "sent": "For two nodes to be similar in the network.",
                    "label": 0
                },
                {
                    "sent": "So the idea that we'll be using that turns out to be fairly flexible is will define node similarity function based on the structure of some K hop neighborhoods of nodes in the network.",
                    "label": 0
                },
                {
                    "sent": "So if we have some note.",
                    "label": 0
                },
                {
                    "sent": "Which is the one in the the the Red triangle.",
                    "label": 0
                },
                {
                    "sent": "We will look at its direct neighbors.",
                    "label": 0
                },
                {
                    "sent": "It's 2nd order neighbors.",
                    "label": 0
                },
                {
                    "sent": "It's it's 3rd order neighbors, so and that is something that will define us.",
                    "label": 0
                },
                {
                    "sent": "The representation of the note and it will be the structure that will be used to compare multiple neighborhoods to each other and learned embeddings.",
                    "label": 0
                },
                {
                    "sent": "So in this case threat is the target note for K equal, once we get one hop neighbors which are directly encoded in adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "And for cake with two, these are two hop neighbors and so on.",
                    "label": 0
                },
                {
                    "sent": "So how to stochastically define these kind of higher order neighbors?",
                    "label": 0
                },
                {
                    "sent": "Is something that we want to model next because?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Many real world networks are incomplete and noisy, so potentially.",
                    "label": 0
                },
                {
                    "sent": "So potentially a note here with this wide triangle that is actually one through three hops away from the target node.",
                    "label": 0
                },
                {
                    "sent": "Mahi Mcnaught might actually be a direct neighbor of target node, but we just don't know yet, because that interaction has not yet been tested, or just because our network, our our data is really noisy and income.",
                    "label": 0
                },
                {
                    "sent": "So we want to have some stochastic notion of what does it means mean to be a direct number or some neighbor to do some K hops away?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our idea will be that really the the the question that we need to solve is what does it?",
                    "label": 0
                },
                {
                    "sent": "What does it mean for a note to what does its neighbor?",
                    "label": 0
                },
                {
                    "sent": "What is a neighborhood of a note?",
                    "label": 0
                },
                {
                    "sent": "So given a particular notes say note, you will want to define what are nearby nodes and we will be fine when we will have this definition.",
                    "label": 1
                },
                {
                    "sent": "Then we will optimize and learn embeddings in such a way that will save that for a pair of nodes that have similar neighbor nodes.",
                    "label": 0
                },
                {
                    "sent": "Similar name, neighborhood.",
                    "label": 1
                },
                {
                    "sent": "They should be embedded close together, so the notation that I'll be using is NR secured with two diff to denote the neighborhood of note you under some strategy are.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now more formally, optimization task would be we have this graph G. Our goal is to learn the mapping function and.",
                    "label": 0
                },
                {
                    "sent": "That mapping function will be such that for for an embedding for four note you will be predictive of other nodes that are in the neighborhood of.",
                    "label": 1
                },
                {
                    "sent": "Note you, so this is so called Skip Gram objective which is reminiscent of what some of you might know.",
                    "label": 1
                },
                {
                    "sent": "A sword to work objective where the that are based on this idea of distributional distributional hypothesis that was proposed back in 50s where the where the the intuitively ladies the following that the embedding of the note.",
                    "label": 0
                },
                {
                    "sent": "Should be indicative of embeddings of its nearby nodes.",
                    "label": 1
                },
                {
                    "sent": "Similarly, similarly as the meaning of the word can be defined by the meaning of other words with which these words Co appears.",
                    "label": 0
                },
                {
                    "sent": "So what will have here is the meaning or of the note will be defined based on the meaning of other nodes with which this node copiers, right.",
                    "label": 0
                },
                {
                    "sent": "But now the meaning formally will be this D dimensional vector for us and this other node switch which denote called peers will define flexibly.",
                    "label": 0
                },
                {
                    "sent": "So that will not only look at the direct neighbors, but some some some form of stochastic neighborhood for or not.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So mathematically dismissed, we have we.",
                    "label": 0
                },
                {
                    "sent": "We want to find an embedding.",
                    "label": 1
                },
                {
                    "sent": "This appeal for no tool that will predict its nearby notes, right?",
                    "label": 0
                },
                {
                    "sent": "So what we want to maximize is there we want to maximize the following sum.",
                    "label": 0
                },
                {
                    "sent": "We go through every four to go to all notes in the in our graph and want to maximize the probability the conditional probability of the neighborhood given the current embedding for note.",
                    "label": 0
                },
                {
                    "sent": "And the assumption here that we will make is this because this is this the neighborhood of nodule is some set of nodes that are neighbors of you will assume that this conditional likelihood can be factorized in the form of.",
                    "label": 0
                },
                {
                    "sent": "Through every going to old neighbors, forgiven neighbor of.",
                    "label": 0
                },
                {
                    "sent": "Note you we want to say that embedding of new will be good for us if it is able to predict well its neighbor.",
                    "label": 0
                },
                {
                    "sent": "So this is just mathematical way of.",
                    "label": 0
                },
                {
                    "sent": "Encoding the distributional hypothesis that I described before.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so formally this means that what we will get, who will get embeddings for 4U and V such that the DOT product between the two embeddings will will represent the probability that a note U&V or Co occur in some random walk that is defined in the network.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this notion of using random walks on the network is exciting because it gives us two important.",
                    "label": 1
                },
                {
                    "sent": "It has two important advantages.",
                    "label": 0
                },
                {
                    "sent": "First, it's very flexible.",
                    "label": 0
                },
                {
                    "sent": "It's a very stochastic definition of note.",
                    "label": 0
                },
                {
                    "sent": "Similarity takes into account incompleteness, and eight in noisy networks, and it's very efficient in the sense that we do not need to consider all pairs of nodes which would make our method quadratic, but we consider only those on those pairs of nodes that Coker.",
                    "label": 0
                },
                {
                    "sent": "In some random walks.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the random walk.",
                    "label": 0
                },
                {
                    "sent": "So the random war or random book procedure is then the following for every note in the network.",
                    "label": 0
                },
                {
                    "sent": "We will simulate some would simulate some number of short random walks that started the note and visit some some notes in the neighborhood.",
                    "label": 0
                },
                {
                    "sent": "Then this.",
                    "label": 0
                },
                {
                    "sent": "The result of this procedure will give us the neighborhood, the neighborhood of a note.",
                    "label": 0
                },
                {
                    "sent": "So the neighborhood of note you will be defined as all nodes that are visited by random Walker that starts at Nokia.",
                    "label": 0
                },
                {
                    "sent": "And then this will simply plugging this into our objective, which will be the objective.",
                    "label": 0
                },
                {
                    "sent": "Based on this, on the principle that we want to learn the embeddings such that they can predict well, it's it's nearby nodes.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to unpeg the formula that we have just seen, it's it's it is shown here in this slide so our loss function then if it has a few terms.",
                    "label": 0
                },
                {
                    "sent": "So first this outer sum is the sum over all nodes, then for every node.",
                    "label": 0
                },
                {
                    "sent": "So we go through all of the neighbors of the notes.",
                    "label": 0
                },
                {
                    "sent": "The neighbors are obtained by the random Walker and then we want to say that while the.",
                    "label": 0
                },
                {
                    "sent": "Embedding of we should be predicted forfeits neighbors, which means our formally just the softmax function, essentially saying that the dot product for you in V, where V is the neighbor of fuel should be high relative to the dot product between you and all other nodes in the network.",
                    "label": 0
                },
                {
                    "sent": "Now the problem here is that here I said all other noted the network.",
                    "label": 0
                },
                {
                    "sent": "So imagine how we have a network with 10,000,000 notes and here in order to compute this term we need to go through all other nodes in the network.",
                    "label": 0
                },
                {
                    "sent": "And here we already need to go through all the notes, so that's already quadratic.",
                    "label": 0
                },
                {
                    "sent": "So that's something we don't want to do.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or doing that naively is, is is really too expensive because we have to normalize this softmax function and that is really something that's not that is not specific to network embeddings, but is pervades various kind of meta methods that.",
                    "label": 0
                },
                {
                    "sent": "Recently operate on.",
                    "label": 0
                },
                {
                    "sent": "That operate on various kinds of data to learn low dimensional representations.",
                    "label": 0
                },
                {
                    "sent": "So the problem is how to normalize this term in the softmax function and.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really dark, the solution to this problem is known as noise contrastive estimation or negative sampling originally proposed by Mykola at all in 2013, where the idea is well, we would want we would need to enumerate here all notes in the network and say that the dot product between a pair of nodes that are actually neighbors should be high relative to the dot product of all non neighbors because that is too expensive to do.",
                    "label": 1
                },
                {
                    "sent": "What we will do.",
                    "label": 0
                },
                {
                    "sent": "Instead we will randomly sample some fixed number of nodes and just say that the dot product.",
                    "label": 0
                },
                {
                    "sent": "Of two nodes that are neighbors should be high relative to the dot product of.",
                    "label": 0
                },
                {
                    "sent": "Random neighbors.",
                    "label": 0
                },
                {
                    "sent": "And that is known as negative sampling, which means that instead of normalizing with respect to all notes, we normalized against some K random negative samples.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Just to overview this procedure again to get embeddings for the metaphor for the network, we will simulate many short random walks starting from every Nov starting from every note.",
                    "label": 0
                },
                {
                    "sent": "This will give us give us neighbors and then once we have the neighbors for every note will optimize this objective function where we have just unpacked this particular term and use softmax function for that.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the only thing that's really left here is what does this strategy are mean?",
                    "label": 0
                },
                {
                    "sent": "So what is this record here and so far the idea of what kind of strategy will be using I?",
                    "label": 0
                },
                {
                    "sent": "I said that what was strategy will be that we will simulate random walks from every node in the network and those are other those nodes that are visited by random walks will define the neighborhood for the note.",
                    "label": 1
                },
                {
                    "sent": "That is 1 strategy.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the question is.",
                    "label": 0
                },
                {
                    "sent": "Can we do better?",
                    "label": 0
                },
                {
                    "sent": "Perhaps we can be more intelligent about how to define note neighborhoods, and there are a number of methods that here that have aimed to do that, starting with note awake and then various kinds of awake methods.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I will talk about 2 examples here.",
                    "label": 0
                },
                {
                    "sent": "In the first snow, two equations biased random walks, so the idea is the following.",
                    "label": 1
                },
                {
                    "sent": "Let's say we have such a network as our input and what we want to do is we want to define the neighbors of every note and for now let's say the target notice.",
                    "label": 0
                },
                {
                    "sent": "Note you here.",
                    "label": 1
                },
                {
                    "sent": "So the classic to classic strategies of saying what is the neighborhood of note you will be will just run breadth first search from starting from Nokia breadth first search is the one in Red would say that note S1X2X3 and S4 are the closest neighbors and then S5 would be the neighbor that is in a 7.",
                    "label": 0
                },
                {
                    "sent": "Or not, just as five would be the neighbor that is 2 hops away and then others are three hops, and so on.",
                    "label": 0
                },
                {
                    "sent": "And we would say, well, these are our neighbors.",
                    "label": 0
                },
                {
                    "sent": "The other option is to do.",
                    "label": 0
                },
                {
                    "sent": "The other extreme is depth first search where we would say, well, the neighbors of note, you are not really S1S2 and S3 and four the neighbors of Note U RS4 S 5 S 6 right so so we we would go further away from from No2 and would find that as a neighborhood.",
                    "label": 0
                },
                {
                    "sent": "So these are two fundamentally different ideas because.",
                    "label": 0
                },
                {
                    "sent": "The breakfast search ordeal will give us local make microscopic view and depth first search idea will give us global view in the networks.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be more concrete to make this.",
                    "label": 0
                },
                {
                    "sent": "To give an example.",
                    "label": 0
                },
                {
                    "sent": "Here is not.",
                    "label": 0
                },
                {
                    "sent": "Here is Notific applied to those simple network so.",
                    "label": 0
                },
                {
                    "sent": "This is a network where actually every note represents an XR2.",
                    "label": 0
                },
                {
                    "sent": "Nodes are connected based on our core appearance of those networks.",
                    "label": 0
                },
                {
                    "sent": "In the left view, we took that network as input.",
                    "label": 0
                },
                {
                    "sent": "The network did not have any labels, and we applied note victory.",
                    "label": 0
                },
                {
                    "sent": "It and we use this local view of the network where we say the neighborhood of every node are its direct neighbours or neighbors.",
                    "label": 0
                },
                {
                    "sent": "Mmmkay hops away visited by breadth first search and we use that to with the note awake objective that really gives us embeddings an we for every note we cluster those embeddings.",
                    "label": 0
                },
                {
                    "sent": "An nodes that are in the same cluster.",
                    "label": 0
                },
                {
                    "sent": "Are colored on the left with the same color, So what we get from here from from with this principle is we get this local view.",
                    "label": 0
                },
                {
                    "sent": "We can see that there are pockets of color of the network.",
                    "label": 0
                },
                {
                    "sent": "So so here's one problem with this already, you're swamped.",
                    "label": 0
                },
                {
                    "sent": "This is orange, so we we do this notion of similarity.",
                    "label": 0
                },
                {
                    "sent": "The idea is that two nodes that have that are that are close or proximally.",
                    "label": 0
                },
                {
                    "sent": "The network should have similar embeddings.",
                    "label": 0
                },
                {
                    "sent": "There is a complementary view, which is if we would define neighborhoods based on depth first search where which is which would give us results like that.",
                    "label": 0
                },
                {
                    "sent": "Again, we have the network.",
                    "label": 0
                },
                {
                    "sent": "We run depth first search define neighborhoods of every note.",
                    "label": 0
                },
                {
                    "sent": "We use those neighborhoods with the notebook objective to get embeddings.",
                    "label": 0
                },
                {
                    "sent": "We cluster embeddings, colored the note and color denotes by cluster assignment, but all of a sudden here we see what red nodes are here, but also in this part of the network which is separated from this part right?",
                    "label": 0
                },
                {
                    "sent": "And here again, some pocket the fret notes which are separate from this one.",
                    "label": 0
                },
                {
                    "sent": "So this is because now the embeddings that we have learned are indicative of structural role in the network.",
                    "label": 0
                },
                {
                    "sent": "So all these blue nodes are actually bridges.",
                    "label": 0
                },
                {
                    "sent": "There are there act as bridges between different parts of network, like here's a blue note that is actually the bridge and this is the only note that connects these notes to the rest of the network.",
                    "label": 0
                },
                {
                    "sent": "And again this note is again a bridge.",
                    "label": 0
                },
                {
                    "sent": "It connects this set of nodes to the rest of the network and actually all blue notes are bridges and that is.",
                    "label": 0
                },
                {
                    "sent": "That is what we have learned.",
                    "label": 0
                },
                {
                    "sent": "That is what the information that is encoded in the beddings.",
                    "label": 0
                },
                {
                    "sent": "But on the left the information that is encoded in the beddings is clustering information.",
                    "label": 0
                },
                {
                    "sent": "So depending on a particular application that we have in biology, we might want to use different notion of not similarity to learn embeddings that might be useful for us.",
                    "label": 0
                },
                {
                    "sent": "For example in the context of functional genomics, which I will show later.",
                    "label": 0
                },
                {
                    "sent": "It has been demonstrated that it's it's it's not on.",
                    "label": 0
                },
                {
                    "sent": "When predicting functions of proteins, it's not only important to look whether proteins directly interact.",
                    "label": 0
                },
                {
                    "sent": "But if a bit also important to think whether proteins that are in completely different parts of protein interaction network might have similar roles in the network and similar functions, because their local interaction neighborhood is the same, so they might act as bridges, or they might act US made.",
                    "label": 0
                },
                {
                    "sent": "They may have similar rules in different protein complexes.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now the week really works well for this local view of the network.",
                    "label": 0
                },
                {
                    "sent": "A different notion of of note similarity, which is called structure, which works really well for capturing structural similarity or for capturing this global structural offloading the network.",
                    "label": 0
                },
                {
                    "sent": "So we struck too vague.",
                    "label": 0
                },
                {
                    "sent": "The idea is the following.",
                    "label": 0
                },
                {
                    "sent": "Our goal is the goal of structure, which is that.",
                    "label": 0
                },
                {
                    "sent": "We want to learn embeddings for notes such that two nodes that have similar structure was in the network will have been be embedded close together.",
                    "label": 0
                },
                {
                    "sent": "For example, imagine we have this.",
                    "label": 0
                },
                {
                    "sent": "We have a gigantic network and there's some notes.",
                    "label": 0
                },
                {
                    "sent": "Unv in-depth network you in VR separated from each other.",
                    "label": 0
                },
                {
                    "sent": "They are really far away from each other, but if you look more closely, it seems that while you has three to hear it involved into triangles in V is involved in two triangles.",
                    "label": 0
                },
                {
                    "sent": "In in in, even though they are far apart from each other, they are structurally similar.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is then we want to learn embeddings that will be able to automatically find those to notes with structural similarity.",
                    "label": 0
                },
                {
                    "sent": "Instructor we can do that.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it could do that by implementing tree.",
                    "label": 0
                },
                {
                    "sent": "Using a project is contains 3 main steps.",
                    "label": 0
                },
                {
                    "sent": "So the first step that struct awake implements is it defines similarity between nodes, which is called structural similarity.",
                    "label": 1
                },
                {
                    "sent": "By looking at the K Hop neighborhoods and will see in the next like how this is done.",
                    "label": 1
                },
                {
                    "sent": "To do actually do that, it constructs a new graph based on the input graph and then on that new slightly transformed graph.",
                    "label": 0
                },
                {
                    "sent": "It runs random walks and those random walks give neighborhoods for, for, for.",
                    "label": 1
                },
                {
                    "sent": "For notes that are then optimized using simple Skip gram objective in the same way as note topic.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in step one, but it means to construct a new.",
                    "label": 1
                },
                {
                    "sent": "What is mean too much means to compute structural similarity between nodes.",
                    "label": 0
                },
                {
                    "sent": "Is the following sort of let's?",
                    "label": 0
                },
                {
                    "sent": "Let's start with the idea with.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that our notation is the 10 steps are lower case key of ur notes that are in some K Hop neighborhood.",
                    "label": 0
                },
                {
                    "sent": "Of note you then we know there is slim line graph theory that says you NVR structurally equivalent.",
                    "label": 0
                },
                {
                    "sent": "Considering the K hop neighborhoods if their key if there are.",
                    "label": 0
                },
                {
                    "sent": "If the graphs induced in those neighborhoods are isomorphic, so which means that there the key?",
                    "label": 0
                },
                {
                    "sent": "Hope for notes U&V there K hop neighborhoods look exactly the same, meaning exactly the same number of triangles, the same number of squares, the same number of neighbors that you have the same number of neighbors that other neighbors in key home neighborhood have.",
                    "label": 0
                },
                {
                    "sent": "The problem is then testing if there keyhole neighborhoods are exactly the same is challenging, but there's a trick to use, and that trick is provided by this limit that says in order we don't need to really exhaustively compare or possible network patterns that might exist in K-Cup neighborhood.",
                    "label": 0
                },
                {
                    "sent": "We can just look at the order degrees sequence of notes in the K-Cup neighborhood in checked.",
                    "label": 1
                },
                {
                    "sent": "If those degrees are the same.",
                    "label": 0
                },
                {
                    "sent": "So the idea then will be for a pair of nodes in the network.",
                    "label": 0
                },
                {
                    "sent": "The method will compute their structural similarity, but really looking at the ordered degree sequence of other nodes that appear in the neighborhood of your envy.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way this is implemented is implemented by taking the input graph, constructing some among us knew multilayer graph or where in the first layer to wear in each layer of debt multilayer graph.",
                    "label": 0
                },
                {
                    "sent": "The nodes are the regional nodes of the network and in the first layer two nodes are connected.",
                    "label": 0
                },
                {
                    "sent": "If they are direct neighbors.",
                    "label": 0
                },
                {
                    "sent": "In the second layers, two nodes are connected if they are.",
                    "label": 0
                },
                {
                    "sent": "If if they are neighbors at two hops that are two hops away, and if they are neighbors that are three hops away, and so on.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And once we have this multilayer graph, then automatically what we get with what we get it is that in in particular layer of that graph 2 notes will be connected if they are structurally similar with the respect of K hop neighborhood where K is the in that case.",
                    "label": 0
                },
                {
                    "sent": "For example, for example tree and look examining this graph will give us the opportunity to get the neighborhood of four nodes in the network and once we have neighborhoods for nodes.",
                    "label": 0
                },
                {
                    "sent": "We can immediately plug it into our optimization tasks to get embeddings that will be indicative of structural similarity.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to give an overview of of this method, we start with the input network that we have.",
                    "label": 0
                },
                {
                    "sent": "We construct a multilayer graph that I described in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "We use it to simulate random walks on that graph from every note and that gives us the nodes that are visited by random walks, give us neighborhoods for, for, for, for notes, and then for once we have those, we learn.",
                    "label": 0
                },
                {
                    "sent": "Embeddings Z.",
                    "label": 0
                },
                {
                    "sent": "By using the same objective function as we have used for noting.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a quick experiment is here just to make things more concrete, so this is a small synthetic example which is actually called Barber graph.",
                    "label": 0
                },
                {
                    "sent": "So what we see here?",
                    "label": 0
                },
                {
                    "sent": "This is our network.",
                    "label": 0
                },
                {
                    "sent": "We have one click here.",
                    "label": 0
                },
                {
                    "sent": "One click here and we have a single pad that connects the two clicks.",
                    "label": 0
                },
                {
                    "sent": "The notes of the colors of the notes here are structurally indicate structural equivalence, meaning all notes that have color blue.",
                    "label": 0
                },
                {
                    "sent": "Are the same from the network point of view.",
                    "label": 0
                },
                {
                    "sent": "This note is exactly the same as this note in the sense that it has the same 1st order numbers, the same second hop neighbors and so on.",
                    "label": 0
                },
                {
                    "sent": "So here again we have noted that from structural point of view they are.",
                    "label": 0
                },
                {
                    "sent": "They look the same, we cannot distinguish them.",
                    "label": 0
                },
                {
                    "sent": "And for example here we have to recognize that outlook that that have the same role in the network to light blue nodes that have the same role in the network and so there are the color of these notes would be our ground truth information.",
                    "label": 0
                },
                {
                    "sent": "So notes of the same color are structurally equivalent.",
                    "label": 0
                },
                {
                    "sent": "What we want to ask now is if you take this network of course without note colors, use it on a strong, use it with struct awake will structure the traffic able to learn embeddings such that nodes that have the same structural or role will be embedded close together.",
                    "label": 0
                },
                {
                    "sent": "Which means, no, that have the same color.",
                    "label": 0
                },
                {
                    "sent": "Here should be embedded close together in the next slide.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see that.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have this Barber graph on the left, and here's the result when you struck too big to learn and padding for every note in that graph.",
                    "label": 0
                },
                {
                    "sent": "And we project this into this 2 dimensional plane.",
                    "label": 0
                },
                {
                    "sent": "What we see is really that old dark blue notes, which are structural equivalent are all in this part of the network.",
                    "label": 0
                },
                {
                    "sent": "So so here is the method was actually indeed able to embed nodes that are structurally similar, very close together.",
                    "label": 1
                },
                {
                    "sent": "So here are the two rentals that are structurally similar to real time zone.",
                    "label": 0
                },
                {
                    "sent": "If you would take this network as inputs to note a week or some other of the shelf method, it would.",
                    "label": 0
                },
                {
                    "sent": "The medics would look completely different because they will typically be based on him awfully, not structure occurrence.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, beyond structurally there's a recent more recent meta graph waving in the code for all these methods is online, so you can simply download it and use them on your networks, and graph is a bit more advanced than structure weak in the sense that it scales to very large networks, and it's more accurate.",
                    "label": 0
                },
                {
                    "sent": "So imagine we would start with another example graph where we have this Jerry or graph here and again the color of the notes indicate structural.",
                    "label": 0
                },
                {
                    "sent": "Curious of the notes, meaning all powerful notes here all have the same role in libel.",
                    "label": 0
                },
                {
                    "sent": "Notes have the same role in are the same from the network's point of view.",
                    "label": 0
                },
                {
                    "sent": "If you take this network is simple to struct awake, learning bedrooms and visualize them.",
                    "label": 0
                },
                {
                    "sent": "We get a few like that, which is good in the sense that we see that the notes of the same color are grouped together, but it's even better with graph wave, because what we get with graph which we get, we get this continuous gradients of notes that are not only structural equivalent, but are more structurally similar, so.",
                    "label": 1
                },
                {
                    "sent": "Dark of secure like light blue is more similar to this even lighter blue than orange.",
                    "label": 0
                },
                {
                    "sent": "And that's something that is learned automatically without having information on node labels.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so summary so far while I talked about so Far East, I described an approach to embed nodes in the network.",
                    "label": 0
                },
                {
                    "sent": "Such debt performing algebraic operator or operations in the learned embedding space reflects topology of the graph.",
                    "label": 0
                },
                {
                    "sent": "There are different notions of note similarity.",
                    "label": 0
                },
                {
                    "sent": "We can simply have adjacency based.",
                    "label": 0
                },
                {
                    "sent": "No similarity that would be matrix factorization we can have.",
                    "label": 0
                },
                {
                    "sent": "Note similarities defined based on random walks.",
                    "label": 0
                },
                {
                    "sent": "What we did so far in general, what we need to do is we need to define note similarities that work best for our application.",
                    "label": 0
                },
                {
                    "sent": "So meaning when you have a new data set and you use a method that learns embeddings and that is true for networks or other types of data, you need to ask yourself.",
                    "label": 0
                },
                {
                    "sent": "What is the situation when I want to for the two examples in my data set to be embedded close together?",
                    "label": 0
                },
                {
                    "sent": "Under what conditions should those examples be embedded close together and that will give you information on what is the notion of similarity that you want to optimize for?",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this concludes the overview of shallow, shallow North embedding methods.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now I will describe a few biomedical applications of those methods to real problems.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In particular, I will describe 2 problems.",
                    "label": 0
                },
                {
                    "sent": "In the first will be dispatched for detection problem, which is, well, relatively well defined problem in bioinformatics and computational biology where the goal is to identify proteins that are identify new proteins associated with the disease.",
                    "label": 0
                },
                {
                    "sent": "In this, in the second problem will be the problem of predicting protein protein interactions, where the goal will be to identify pairs of proteins that physically interact.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let's start with the first problem, which is the problem of disease battery detection.",
                    "label": 0
                },
                {
                    "sent": "So the data that will be we will be working with will be the human protein protein interaction network, which means we have a network where nodes are proteins.",
                    "label": 0
                },
                {
                    "sent": "Edges indicate physical contact between those proteins.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What has been discovered it all during the the core or the last 20 research in the last 20 years or so is that there is this key principle saying that proteins that interact with each other tend to overlay, tend to underlie the same phenotype, and we will use this principle to identify knew potential disease associated proteins.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this means that if when we look at the set of known proteins associated with a particular disease, which which, which in this case is what we will refer to us Pat way, and there are many different definitions of pathway.",
                    "label": 0
                },
                {
                    "sent": "But in this example pathway for us will be then some network of proteins that are associated with the disease in our interactive.",
                    "label": 0
                },
                {
                    "sent": "So we had the interactome for a particular disease.",
                    "label": 0
                },
                {
                    "sent": "We say we look at what are currently known proteins associated with the disease, and that defies as a sub defines.",
                    "label": 0
                },
                {
                    "sent": "US and gives us some network.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our prediction task in the in the disease Pathway detection task is then the following we we we have our protein interaction network on the left and we have for a given disease known disease associated proteins, which are those threat proteins.",
                    "label": 0
                },
                {
                    "sent": "They might be as connected before my former connected subnetwork or not.",
                    "label": 0
                },
                {
                    "sent": "So this is our disease pathway and then we want to have the method that will.",
                    "label": 0
                },
                {
                    "sent": "Take this as input and what it will do.",
                    "label": 0
                },
                {
                    "sent": "It will identify new disease proteins so it will identify that.",
                    "label": 0
                },
                {
                    "sent": "Perhaps these two blue notes are.",
                    "label": 0
                },
                {
                    "sent": "According to the method, new candidates that are likely to be also represent proteins that are likely to be associated with the disease.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in order to do that, we need to 1st the spectral data set.",
                    "label": 0
                },
                {
                    "sent": "In our case, what will be using is a protein protein interaction network that is called from some.",
                    "label": 0
                },
                {
                    "sent": "Some number of knowledge databases and it's also includes the some of the largest interactive mapping efforts from Marcus with Al Group from Harvard and others will use currently known information on protein disease associations that come from this Jeanette and other knowledge bases and the prediction test that we are one that we will be solving here is that of multi label node classification because every note in that network every protein can have can be associated with 01 or with multiple diseases.",
                    "label": 0
                },
                {
                    "sent": "And that is something that we want to discover.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how can we use our note embedding methods that we have heard about minutes ago too for this problem we will do?",
                    "label": 0
                },
                {
                    "sent": "We will.",
                    "label": 0
                },
                {
                    "sent": "We will have the setup that has the following two stages and that's a fair, very standard way of using those.",
                    "label": 0
                },
                {
                    "sent": "The shallow embedding embedding methods will take our PPI network will use notes too weak to learn embedding for every note, and then once we have those embeddings we will use them as input to a classifier.",
                    "label": 0
                },
                {
                    "sent": "That will predict new disease proteins based on these proteins that are already known to be associated with the disease.",
                    "label": 0
                },
                {
                    "sent": "Ultimately, the goal will be there to those classifiers.",
                    "label": 0
                },
                {
                    "sent": "Poor would predict probability for a given node, 14 probability that they are associated with a given disease.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can use different kinds of cross validation, train test, validation, split.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to talk more about it offline and compare a number of different methods.",
                    "label": 0
                },
                {
                    "sent": "In the value of their ability to accurately predict disease associated proteins.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing here on the.",
                    "label": 0
                },
                {
                    "sent": "On the left are different plots that show ability of different methods, such as note, awake, or.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in this particular note, awake methods where every know where every point in the plot is a is a particular disease, and the value on the Y axis represents the heat rate at 100 to, meaning how many out of 100 most.",
                    "label": 0
                },
                {
                    "sent": "High protein is that were predicted as most likely to proteins to be involved with the disease are actually known disease, associated proteins, and the best performing approaches for this problem.",
                    "label": 0
                },
                {
                    "sent": "There are various kinds of network embeddings.",
                    "label": 0
                },
                {
                    "sent": "Here I'm showing the results for note to weaken Beddings which were better than a method based on various latent variable models such as matrix completion or diamond with diamond, which is another network based method.",
                    "label": 0
                },
                {
                    "sent": "The worst performer method here is neighbor scoring.",
                    "label": 0
                },
                {
                    "sent": "So what is neighbors scoring?",
                    "label": 0
                },
                {
                    "sent": "This is a simple majority vote on network where you have a note you look at its neighbors.",
                    "label": 0
                },
                {
                    "sent": "Perhaps a note we have 5 neighbors in three out of five neighbors are already known to be associated with.",
                    "label": 0
                },
                {
                    "sent": "Adrenal carcinoma, so then you would say, well, the likelihood that this note is associated with the dinner carcinoma is 3 / 5 right?",
                    "label": 0
                },
                {
                    "sent": "So simply look at the neighbors and say I will use majority vote on my neighbors to pre identify how likely the neighbors involved with the disease and that would be neighbors scoring method.",
                    "label": 0
                },
                {
                    "sent": "Which performs worse, but it's always good to look at.",
                    "label": 0
                },
                {
                    "sent": "Look",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, sure.",
                    "label": 0
                },
                {
                    "sent": "It is also allowed.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, so that's a great question.",
                    "label": 0
                },
                {
                    "sent": "So those that are familiar with submitted would say, well, this is not an end to end learning procedure in the the talk that I will have tomorrow, I will talk about end to end optimization where we will learn embedding such that they will immediately predict the particular outcome, such as a label of a node or an Association between two nodes.",
                    "label": 0
                },
                {
                    "sent": "In this case we have two separate stages.",
                    "label": 0
                },
                {
                    "sent": "First we learn embedding.",
                    "label": 0
                },
                {
                    "sent": "Second, we use classifier.",
                    "label": 0
                },
                {
                    "sent": "The classifier here was.",
                    "label": 0
                },
                {
                    "sent": "The result that I'm trying to slide were obtained by logistic regression.",
                    "label": 0
                },
                {
                    "sent": "One can use any other classifier that you would want.",
                    "label": 0
                },
                {
                    "sent": "Yes, so here's here's yes.",
                    "label": 0
                },
                {
                    "sent": "So the results that are shown here would be one logistic classifier for every disease.",
                    "label": 0
                },
                {
                    "sent": "More results in papers that that is associated with their that is associated with this results are of course results were are obtained by methods that take multiple days that make predictions are for all diseases at the same time.",
                    "label": 0
                },
                {
                    "sent": "But the principle here was really to study whether embeddings.",
                    "label": 0
                },
                {
                    "sent": "Provide more useful information relative to some of the standard methods, so the focus was not on developing new method for predicting for doing multilabel classification task.",
                    "label": 0
                },
                {
                    "sent": "But of course if you would give the goal would be to have the best performance or to optimize for performance.",
                    "label": 0
                },
                {
                    "sent": "One would want to take into account dependencies between diseases since we know that two diseases that I felt typically similar have similar symptoms, they tend to have to tend to have similar genetic underpinnings.",
                    "label": 0
                },
                {
                    "sent": "So it would make sense to have that in the leverage that correlation.",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so this was the the first use case of the methods that are that I described where the task was that the task was to predict a particular property of nodes in the network and that is known as class node classification task and the property we were predicting was disease associations.",
                    "label": 0
                },
                {
                    "sent": "In the second task we will we will we will apply methods to the problem of protein protein interaction prediction where the goal will be to predict a relationship or.",
                    "label": 0
                },
                {
                    "sent": "Potential relationship between a pair of notes.",
                    "label": 0
                },
                {
                    "sent": "So here now we are not only now interesting, only about predicting a property of one out, but the relationship between a pair of not.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which means what what the goal of our prediction will be for a pair of nodes.",
                    "label": 0
                },
                {
                    "sent": "We want to predict whether they will physically interact with each other.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data that we will be using for that we will be again the bare minimum of data that we need for this, which is the human protein protein interaction network.",
                    "label": 0
                },
                {
                    "sent": "And given this even this network, our goal is that for a particular pair of proteins we want to predict what is the probability that those proteins will interact.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so, and in order to answer this question, while what would be really useful to have is to have embeddings for edges, not only embeddings for notes, so how can we learn embeddings for edges seen so far?",
                    "label": 0
                },
                {
                    "sent": "I describe methods that can learn embeddings for notes, and that was useful for previous tests, so our question will be how to address these tests involving pairs of notes and our simple idea will be that.",
                    "label": 0
                },
                {
                    "sent": "Simple idea that that many new studies uses.",
                    "label": 0
                },
                {
                    "sent": "Then given a pair of notes, common approaches to define some operator, let's call it G, that that will take the pair of notes and it generates an beddings for the pair of notes for your envy.",
                    "label": 0
                },
                {
                    "sent": "So how to define that operate?",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sergey.",
                    "label": 0
                },
                {
                    "sent": "Our goal is that this operator G needs to be defined for any pair of nodes.",
                    "label": 1
                },
                {
                    "sent": "It can it can.",
                    "label": 0
                },
                {
                    "sent": "We should also operate on a pair of nodes that are not directly connected with each other, and that's fundamental in order for us to make it really prediction.",
                    "label": 0
                },
                {
                    "sent": "And what is commonly used in the literature?",
                    "label": 0
                },
                {
                    "sent": "What you will find is fairly simple methods in for this problem.",
                    "label": 0
                },
                {
                    "sent": "When one that you might think of it and get ideas fairly quickly, which is if you have a pair of notes and their embeddings, you will simply say that an embedding for any foreign edge that in connect UNV is simply the average of embeddings of the endpoints.",
                    "label": 1
                },
                {
                    "sent": "Or perhaps it's a Hadamard product which means elementwise product.",
                    "label": 0
                },
                {
                    "sent": "Or it might be awaited L1 distance or waited.",
                    "label": 0
                },
                {
                    "sent": "Ultra distance and these are some of the fairly simple approaches that would be baseline approaches nowadays.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you take this approach is an we applied to our data set.",
                    "label": 0
                },
                {
                    "sent": "This means we do the following.",
                    "label": 0
                },
                {
                    "sent": "We are given a protein protein interaction network with a certain fraction of edges in those.",
                    "label": 0
                },
                {
                    "sent": "Net network removed which will represent our protein protein interaction network detar.",
                    "label": 0
                },
                {
                    "sent": "In our test set.",
                    "label": 0
                },
                {
                    "sent": "And again, the the experiment that will do will have two main stages.",
                    "label": 0
                },
                {
                    "sent": "First will use note a week to learn an embedding for every note in the network, and then we will predict a score for every protein pair in the test set based on embeddings of based on embedding of an edge.",
                    "label": 0
                },
                {
                    "sent": "Which means we take our operator GGR, allow us to take embedding of the notes, getting bending of the edge, and then we have a representation for every edge, and again fairly standard.",
                    "label": 0
                },
                {
                    "sent": "One can use any standard machine learning model to define that as a classification task where where we're at embeddings that represent actual edges in the network are positive examples and embedding and embeddings that represent.",
                    "label": 0
                },
                {
                    "sent": "Hi things they do not know that are currently represent non agency in the network are negative exam.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so here are some results obtained.",
                    "label": 0
                },
                {
                    "sent": "With with with the simple methods so the the data set that we are looking at as is in the second column and which is a 1440 direction data set.",
                    "label": 1
                },
                {
                    "sent": "And what is interesting is that the embeddings that are learned by note awake and methods that use this stochastic definition of network neighborhoods based on random walks or multihop similarity rather than direct adjacency matrix drastically outperform some of the heuristic baselines which would be these green box here on the left.",
                    "label": 1
                },
                {
                    "sent": "And they also outperform some of the earlier methods for network embeddings, such as.",
                    "label": 0
                },
                {
                    "sent": "So such a spectral clustering deep walk in line.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that was a very simple trick that I showed you for how to get edge embeddings or embeddings for for a pair of nodes from embeddings for individual nodes.",
                    "label": 0
                },
                {
                    "sent": "And also this example highlighted how some of the shallow North embedding methods can be used for problems related to link prediction, where the goal is to predict knew to discover new edges in the network.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this concludes Biomedic tool by examples of biomedical applications for for today.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in particular also the this second part of the talk about notable things in the third part I will move forward with hetrogenous networks and explain how how, how some of network embedding methods that we have looked at so far can be can serve us to develop methodology for heterogeneous networks.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so far we focus on homogeneous networks, which means those that paid careful attention.",
                    "label": 0
                },
                {
                    "sent": "You probably notice that in all applications that I showed what we had was really simple networks in the sense that the notes are always proteins and then edges indicate physical interaction between proteins, right?",
                    "label": 0
                },
                {
                    "sent": "One might, but we might might have different kinds of information, different kinds of relationship between proteins that might be interesting and useful to to consider, so really.",
                    "label": 0
                },
                {
                    "sent": "The question here is can we embed heterogeneous networks or more generally, knowledge graph?",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Knowledge graph so many networks in biology are actually heterogeneous networks, and here is just one example of different kinds of entities that might interact with different in different ways with each other, and we might want to consider that when learning representations for four for nodes.",
                    "label": 0
                },
                {
                    "sent": "Formally, we often think of heterogeneous networks as US networks where we have a certain number of nodes types which we often call modes and different kinds of relations between between the notes.",
                    "label": 0
                },
                {
                    "sent": "So here is a more abstract example where we have one.",
                    "label": 0
                },
                {
                    "sent": "When we have six modes, meaning more notes of six different types, and then we have edges between nodes of each type and edges across different types of notes.",
                    "label": 0
                },
                {
                    "sent": "So this will be the heterogeneous networks that will be working quit today, Ann.",
                    "label": 1
                },
                {
                    "sent": "More much more in detail tomorrow when we will.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About our deep models.",
                    "label": 0
                },
                {
                    "sent": "The motivating problem for today will be the problem of predicting protein functions in different tissues.",
                    "label": 0
                },
                {
                    "sent": "So it has been mentioned several times in the last few days.",
                    "label": 0
                },
                {
                    "sent": "The importance of predicting protein functions and that is really one of the Canonical tasks in functional genomics.",
                    "label": 0
                },
                {
                    "sent": "Since we know that proteins are working worker molecules in our cells and it's important to understand what their roles or their functions are.",
                    "label": 0
                },
                {
                    "sent": "Of course, we know that those the rules or functions that proteins have depend on the tissue context, meaning that a particular protein might have different function in different issue.",
                    "label": 0
                },
                {
                    "sent": "And potentially proteins that are active in similar tissues might have similar functions.",
                    "label": 0
                },
                {
                    "sent": "So the goal really are that will have today is how to predict protein functions across different tissues and why.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is this problem challenging or it's hard?",
                    "label": 0
                },
                {
                    "sent": "So there are a number of reasons, but let me mention three of them.",
                    "label": 0
                },
                {
                    "sent": "The first is that teachers in human body are inherently organized in a hierarchical, multiscale manner, meaning they are related to each other in some way that will audit relationships.",
                    "label": 0
                },
                {
                    "sent": "Organization tend to be hierarchical Ann, and current research has shown that proteins active in similar tissues tend to have similar properties and functions in those tissues.",
                    "label": 0
                },
                {
                    "sent": "Second challenge is that.",
                    "label": 0
                },
                {
                    "sent": "Many issues have no annotations, meaning that for certain tissues that is very hard to work with or impossible to probe them or obtain samples for, though for those issues that it's much less data available about what are the functions of proteins in the tissue, potentially those issues are completely unannotated, which means that or in machine learning language this means that there we will have some tissues for which there will be no label, no no labeled examples.",
                    "label": 0
                },
                {
                    "sent": "Or just a handful of examples, so we really need to operate in that low data regime.",
                    "label": 0
                },
                {
                    "sent": "And Thirdly, there is a set.",
                    "label": 0
                },
                {
                    "sent": "Since this is a function, prediction is a Canonical problem.",
                    "label": 0
                },
                {
                    "sent": "In functional genomics there's a number of methods that have been developed for the problem of developed for predicting protein functions.",
                    "label": 0
                },
                {
                    "sent": "However, those methods are based on the Sumption that the functions are the same irrespective of the context.",
                    "label": 0
                },
                {
                    "sent": "So that would mean that they could.",
                    "label": 0
                },
                {
                    "sent": "Without, which means that functions in heart are same as functions of proteins in ski or functions in brain are the same as functions of proteins in the skin.",
                    "label": 1
                },
                {
                    "sent": "These are the predictions that one would get by running current methods.",
                    "label": 0
                },
                {
                    "sent": "We know that even that is that is not the case.",
                    "label": 0
                },
                {
                    "sent": "So what we want to have is the methods that can make this tissue specific prediction.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our goal will be the following.",
                    "label": 0
                },
                {
                    "sent": "Given a protein, given a tissue and function, so we have these three objects.",
                    "label": 1
                },
                {
                    "sent": "Want to predict how likely it is that the protein has the particular function in that issue, and that is a much more precise prediction than simply saying that the protein has this function somewhere in the human body.",
                    "label": 1
                },
                {
                    "sent": "So what this means at the high level we are solving the following test.",
                    "label": 0
                },
                {
                    "sent": "So we start we start on the left with with the network perhaps or with a network of brain tissue.",
                    "label": 1
                },
                {
                    "sent": "For the brain tissue, we have a network of water proteins and how those proteins interact with each other in brain tissue.",
                    "label": 1
                },
                {
                    "sent": "For some of those proteins we have information on their functions and brake issues, such as perhaps the brain development and there is supporting known to be involved with the brain development in brain tissue or the protein involved in angiogenesis.",
                    "label": 0
                },
                {
                    "sent": "We want to have a method that what will be able to do it.",
                    "label": 0
                },
                {
                    "sent": "That method will allow us to label the remaining proteins in debt or narrow tissue brain network according to how likely they are to be involved.",
                    "label": 0
                },
                {
                    "sent": "To be important for different functions.",
                    "label": 0
                },
                {
                    "sent": "So the previously white notes on the left become red or blue notes here.",
                    "label": 0
                },
                {
                    "sent": "So this is a toy example that you want to have.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what realistically, how, what what we have is of course not just brain tissue network, but we might have a hierarchy of tissue specific networks, and that's what we will be working with here.",
                    "label": 0
                },
                {
                    "sent": "So what we will have our multimodel tissue network, where for every tissue in human body we have a separate protein protein interaction network specific to that issue.",
                    "label": 0
                },
                {
                    "sent": "We know how tissues are related to each other based on that issue hierarchy and run to take into account the tissue specific protein interaction network.",
                    "label": 0
                },
                {
                    "sent": "Plus the soft tissue hierarchy to learn embeddings that are indicative of our interaction information as well as tissue context.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this means that what we want to do is, if you remember before we said that with the embedding approaches we want to learn, specify what are mapping functions are that take notes and learn and map them to the dimensional space in this particular context will have multiple such mapping functions, in particular, if our input will be teacher network with four with four 44 tissue specific networks.",
                    "label": 0
                },
                {
                    "sent": "So G1 JK&L.",
                    "label": 0
                },
                {
                    "sent": "We will learn a mapping functions that for for every for each graph as well As for each scale in the network.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "That means that for a for a particular note you, this is no that is active on three out of four networks.",
                    "label": 0
                },
                {
                    "sent": "We will have three functions that mapping functions that will take note you an information on whether it's present in the first GIK or L, and depending on that will get an embedding for no tool for each of the graphs.",
                    "label": 0
                },
                {
                    "sent": "As well as an embedding for nodule that captures the activity of this scale of the network, meaning that aggregates the topology of note you across these two networks, or at this scale, which means that will aggregate the topology courses Phoenix Open or do across these three networks or across all networks at the same time.",
                    "label": 0
                },
                {
                    "sent": "So we have this multiscale embeddings that will help here that might be that that will be useful for us.",
                    "label": 0
                },
                {
                    "sent": "So the question then is how to learn this mapping functions which are now much more complex than what we have seen before.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a bit more formally, are set up with the following.",
                    "label": 0
                },
                {
                    "sent": "What we assume as input is that input.",
                    "label": 0
                },
                {
                    "sent": "We have some number of graphs.",
                    "label": 0
                },
                {
                    "sent": "All in these graphs are related to each other through some hierarchy, so those graphs might have might be are different from from each other, and there are different subsets of nodes that are present.",
                    "label": 0
                },
                {
                    "sent": "In that hierarchy in that hierarchy, tell us how those graphs are related to each other.",
                    "label": 0
                },
                {
                    "sent": "The assumption is that graphs are in the lives of that hierarchy, so here I would say we have this example.",
                    "label": 0
                },
                {
                    "sent": "In this example, we have four graphs and GI&JRR according to our prior information are most similar to each other.",
                    "label": 0
                },
                {
                    "sent": "That's why they are they are.",
                    "label": 0
                },
                {
                    "sent": "They are neighbors in the hierarchy, and there are big, less related to GK and GS.",
                    "label": 0
                },
                {
                    "sent": "So as I said, we will do a model.",
                    "label": 0
                },
                {
                    "sent": "We want to have a multiscale models, which means that we will learn embeddings for notes for each in each of those graphs as well as at each of the internal notes of the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So we learn what the most effective way to combat to learn a representation of a note for all for us.",
                    "label": 0
                },
                {
                    "sent": "For a subset of networks or subset of graph, so the output will be note embeddings for every for each paragraph, and for each sub hierarchy.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our approach will have to create components and the first component is the component that will translate into our part in the objective function where we will model each graph separately.",
                    "label": 0
                },
                {
                    "sent": "So we will say 444 notes in each graph.",
                    "label": 0
                },
                {
                    "sent": "We want to encode and learn their embedding such that for notes in that graph 2 notes will be embedded close together if they have similar local topology.",
                    "label": 0
                },
                {
                    "sent": "So that is first component that we want to take into account.",
                    "label": 0
                },
                {
                    "sent": "The second component is that we want to encourage nodes that.",
                    "label": 0
                },
                {
                    "sent": "Are active in similar graphs to also have similar embeddings, so this is how we will transfer or fuse information from one graph to the other.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this high level intuition then translates to an objective function fairly naturally.",
                    "label": 0
                },
                {
                    "sent": "We have two components in our objective function.",
                    "label": 1
                },
                {
                    "sent": "The first is the single graph objective where where what we do is the following.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, in each graph we will embed notes in the two dimensions and for a pair of notes unv will say well there are similar if they are similar according to that particular graph that we are currently looking at.",
                    "label": 0
                },
                {
                    "sent": "And the notion of similarity that we will be using is based on the idea of random walks that I presented earlier in the talk.",
                    "label": 0
                },
                {
                    "sent": "So we'll say forever for a particular note.",
                    "label": 0
                },
                {
                    "sent": "You, the neighborhood of note you in this.",
                    "label": 0
                },
                {
                    "sent": "In this graph, GI is defined based on.",
                    "label": 0
                },
                {
                    "sent": "Notes that are visited by random walks when we simulate random walks in that graph.",
                    "label": 1
                },
                {
                    "sent": "So if we start without you here note random Walker might visit these three other nodes in words.",
                    "label": 0
                },
                {
                    "sent": "Or perhaps these and these three notes and it will say that this red notes are the neighbors of note you in the graph GI.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This this.",
                    "label": 0
                },
                {
                    "sent": "This means that our objective function is the following for a particular note you what you want to maximize, we want to maximize the probability that the embedding of note you in this in the ground that we're currently looking at graph GI is able to predict well its neighbors, and if it's able to predict its neighbors, that's an indication that the embedding of note you is good embedding in debt for for no GI.",
                    "label": 0
                },
                {
                    "sent": "Given that we have multiple graphs, we want to maximize this.",
                    "label": 0
                },
                {
                    "sent": "Note level objective across across all nodes in each graph in the ground to her GI and then across all graphs that we have in her in that we have in our input data.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the summary so far is that, well, that is great, but we have not yet considered the hierarchy M at all.",
                    "label": 0
                },
                {
                    "sent": "Meaning that though if it would simply run the used objective function that I showed in the previous slide, we will learn embeddings for the notes and embeddings in each of the graphs would be run, learned completely independent, but that's not something we want.",
                    "label": 0
                },
                {
                    "sent": "We want to encourage nodes that are that come from similar graphs to be similar as well.",
                    "label": 0
                },
                {
                    "sent": "So how to model these interdependencies between graphs when we learn?",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The embeddings.",
                    "label": 0
                },
                {
                    "sent": "So recall that what we have is hierarchy of grass M. In our case would be given to us in the form of parent child relationships.",
                    "label": 0
                },
                {
                    "sent": "So if we have our data set looks like that when we have four graphs in a hierarchy of these photographs, then two is apparent of GI&J and one is apparent.",
                    "label": 0
                },
                {
                    "sent": "2G KNGL and this pie will be is just a formal representation of our hierarchy.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we will use that to define the cross graph objective.",
                    "label": 0
                },
                {
                    "sent": "So the cross graph objective.",
                    "label": 0
                },
                {
                    "sent": "What we'll do it will allow us to encode dependencies between between graphs and formally this will.",
                    "label": 0
                },
                {
                    "sent": "This is something that we will achieve by using a recursive regularization, which means that embeddings at each level of the hierarchy will be encouraged, encouraged to be similar to embeddings in the parent of the hierarchy.",
                    "label": 1
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formally, this means that if we have a particular note, so we look, we are currently looking at.",
                    "label": 0
                },
                {
                    "sent": "So we are looking at note you will say that when we are learning and embedding for no chewing graph GI.",
                    "label": 0
                },
                {
                    "sent": "We want him banished for no chill to be to be similar to embedding of nodule in the parent of GI parental gispert.",
                    "label": 1
                },
                {
                    "sent": "So in the simplest case this translates to this form of regularization when we say them begging for nodule in the current graph that we're looking at should be embedding for note you are the one.",
                    "label": 0
                },
                {
                    "sent": "At one level up in our hiring.",
                    "label": 0
                },
                {
                    "sent": "And this is the property that is repeated at every level of the hierarchy, and that we have.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we put both parts of our objective function together, what we get is we need to solve the following maximum likelihood problem, whereas we said the goal is to find this mapping function that will make notes to the dimensional embeddings.",
                    "label": 0
                },
                {
                    "sent": "We have two types of objective terms.",
                    "label": 0
                },
                {
                    "sent": "The 11 type of.",
                    "label": 0
                },
                {
                    "sent": "One important term in objective function says that for every graph that we're currently looking at learn embeddings that are indicative of local network structure in that graph, and the second part, which is this recursive regularization, says that for nodes active in similar graphs we want there in we encourage their embeddings to be similar as well.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is the final algorithm that one can use so the final algorithm is the following?",
                    "label": 0
                },
                {
                    "sent": "So for each graph that we have, we will first define the neighborhood of notes in those in the in the graph, and we do that using this fixed length.",
                    "label": 0
                },
                {
                    "sent": "Random walks idea that we learned about when I was explaining to to work once.",
                    "label": 0
                },
                {
                    "sent": "Once we have the neighbors for every noting in every graph, we optimized objective function that we have seen in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "That includes single graph and cross graph terms, and we can use that using stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So the reason why we have actually such schema for learning embeddings is because it this, because it makes our approach is very scalable.",
                    "label": 0
                },
                {
                    "sent": "In particular, why this is the case is we do not need to do any kind of pairwise comparison of nodes across the graph.",
                    "label": 0
                },
                {
                    "sent": "So if we would not have a hierarchy, then a naive approach would be that we we look at, we have a set of graphs and then we go across all possible pairs.",
                    "label": 0
                },
                {
                    "sent": "Off notes across all graphs, and then we say, well, they should be similar somehow, but that would give us quadratic complexity in the number of nodes where this number of nodes is defined as the sum of the number of nodes across all graphs.",
                    "label": 0
                },
                {
                    "sent": "Instead, what we do, what we get our is linear time complexity because we do not need to compare or pairs of graphs, but we only compare graph compare embeddings of notes to their embeddings in higher levels of the hierarchy, so this hierarchal structure.",
                    "label": 0
                },
                {
                    "sent": "Makes the approach extremely scalable.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so enough about the details how to compute those embeddings left look at on a concrete application to biomedical data.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as I said, the problem that we will be tackling here is that of.",
                    "label": 0
                },
                {
                    "sent": "Protein function prediction particular we want to have a model that will be able to answer questions of the form.",
                    "label": 0
                },
                {
                    "sent": "Why does my protein do so?",
                    "label": 0
                },
                {
                    "sent": "We want to have a model that, given a particular protein, a tissue and a function, it will be able to say how likely it is that the protein has the function in specific tissues.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the data that will be using for for that order.",
                    "label": 0
                },
                {
                    "sent": "We compiled for for that is data that has 170 show graphs.",
                    "label": 0
                },
                {
                    "sent": "So each graph here is supporting port Interaction network in specific human tissue and then tissue hierarchy that relates tissues to each other.",
                    "label": 1
                },
                {
                    "sent": "That is that comes from Brenda Tissue Ontology.",
                    "label": 0
                },
                {
                    "sent": "So this gives us a data set that looks like that.",
                    "label": 0
                },
                {
                    "sent": "So this is the higher this is the hierarchy M for this problem.",
                    "label": 0
                },
                {
                    "sent": "So the leaves of this hierarchy are blue notes.",
                    "label": 0
                },
                {
                    "sent": "And then internal nodes are various hierarchical representations of tissues.",
                    "label": 0
                },
                {
                    "sent": "So how tissues for multi tissues and organs and organ systems and eventually the root here would be entire human body.",
                    "label": 0
                },
                {
                    "sent": "Blue, noting that the hierarchy which is hierarchy and the thing that we have seen in the.",
                    "label": 0
                },
                {
                    "sent": "Methods part of this section, beliefs here.",
                    "label": 0
                },
                {
                    "sent": "This blue notes are actually.",
                    "label": 0
                },
                {
                    "sent": "This dish are actually tissue specific network.",
                    "label": 0
                },
                {
                    "sent": "So actually the single rule out here in the hierarchy when you look close at it.",
                    "label": 0
                },
                {
                    "sent": "What you get is a protein protein interaction network that is specific to this tissue human body.",
                    "label": 1
                },
                {
                    "sent": "So, so this is the data that we have.",
                    "label": 0
                },
                {
                    "sent": "Will be using for this problem.",
                    "label": 1
                },
                {
                    "sent": "It's to tell us if it represents the hierarchy M and a set of graphs.",
                    "label": 1
                },
                {
                    "sent": "GI that are that are 107 of them and the third represented by this blue notes here.",
                    "label": 0
                },
                {
                    "sent": "In terms of note labels, what we have is some number of tissue specific protein functions, which is currently known.",
                    "label": 0
                },
                {
                    "sent": "Informations about what are functions of proteins, specific human tissues.",
                    "label": 1
                },
                {
                    "sent": "This largely came from a collaboration between Scala Lab at Princeton, or which is now this also publicly available.",
                    "label": 0
                },
                {
                    "sent": "Data injection, giant or human based system that developed and maintained by flat tire.",
                    "label": 0
                },
                {
                    "sent": "And the way these data comes from, it comes in the form of saying that it comes in the form of information on a function.",
                    "label": 0
                },
                {
                    "sent": "For example, cortex development.",
                    "label": 0
                },
                {
                    "sent": "But these are active in specific tissues.",
                    "label": 0
                },
                {
                    "sent": "Rainer contact issue and then a set of proteins that are that are involved in quarters development in renal cortex, tissue or etc.",
                    "label": 0
                },
                {
                    "sent": "Parties involved involved in archery, morphogenesis in artery tissue.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so experimental setup this the problem that we're holding solving here is that of multi label node classification task.",
                    "label": 0
                },
                {
                    "sent": "So the query that we have an example query is does a gene.",
                    "label": 0
                },
                {
                    "sent": "RPT one.",
                    "label": 0
                },
                {
                    "sent": "Does it play a role in processing function angiogenesis in specific tissues in blood tissue?",
                    "label": 0
                },
                {
                    "sent": "So that is a query that that we are able to answer with their system.",
                    "label": 0
                },
                {
                    "sent": "Every node protein is assigned one or zero, one or multiple labels in specific graph and our setup will be the following first.",
                    "label": 0
                },
                {
                    "sent": "Take it home constructing.",
                    "label": 0
                },
                {
                    "sent": "First, we construct our multimodal tissue network using the hierarchy in tissue specific protein direction at work, and we learn features or embeddings for debt multimodal network.",
                    "label": 0
                },
                {
                    "sent": "Second, we train a classifier for each function or multiple functions at the same time based on the embeddings and known label information.",
                    "label": 0
                },
                {
                    "sent": "And 2nd we predict functions for proteins that and Thirdly we predict functions for new proteins.",
                    "label": 0
                },
                {
                    "sent": "Here again, we can leverage various kinds of advancement that have been made recently by taking into account the hierarchal structure of protein functions that come from gene ontology and so on.",
                    "label": 0
                },
                {
                    "sent": "Those that are familiar with this line of.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the results obtained by.",
                    "label": 0
                },
                {
                    "sent": "Function level, cross validation and the methods that I have been discussing is called on it and the methods performs the methods that this method that can make this tissue specific protein function predictions turn out to be much more accurate than some of the other baseline methods.",
                    "label": 0
                },
                {
                    "sent": "So what are the other baseline methods?",
                    "label": 0
                },
                {
                    "sent": "So the first baseline method that one my task is to say is to use the state of the art method for 14 function prediction that ignores tissue context, which means the state of the art work at this time.",
                    "label": 0
                },
                {
                    "sent": "These results show that it's really important to integrate this additional level of information on tissue context and contextual information on where in the human body proteins are active.",
                    "label": 0
                },
                {
                    "sent": "When we are predicting functions.",
                    "label": 0
                },
                {
                    "sent": "Of proteins and other the third line here.",
                    "label": 1
                },
                {
                    "sent": "What it shows is the performance gain of Omnet relative to methods that would learn embeddings for networks, in dependently ignoring the hierarchal or structure in hierarchal similarities or interdependencies between graphs in different tissues which would mean for a given tissue specific network we use.",
                    "label": 0
                },
                {
                    "sent": "Note awake.",
                    "label": 0
                },
                {
                    "sent": "We learn embeddings, we we use those embeddings for.",
                    "label": 0
                },
                {
                    "sent": "A simple classifier that makes predictions, ignoring all other tissue specific protein tissue, specific port interaction networks, and the final comparison here shows the improvement of this method over perhaps a fairly classic about well founded method based on things or decomposition where the idea was how does the method compare to the approach where we would construct a tensor where every mode of that answer would be.",
                    "label": 0
                },
                {
                    "sent": "Protein protein interaction network in specific human tissue.",
                    "label": 0
                },
                {
                    "sent": "So in the case when we had 107 tissue specific networks, we do have a tensor, which would be one which would have 107 modes.",
                    "label": 0
                },
                {
                    "sent": "In each mode, then, would be one adjacency matrix of the networking that tissue and we would run.",
                    "label": 0
                },
                {
                    "sent": "These are the composition on that things.",
                    "label": 0
                },
                {
                    "sent": "So the reason why we see improvement in some of these methods over teams or the compositions is primarily because network embedding methods can can are much more flexible and can take into account not only direct neighbors but also indirect neighbors and have this stochastic flexible definition of network neighborhood.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so a small case study that we did here focused on brain tissues.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing here on the left is a part of the hierarchy that contains 9 brain teachers and for each of those tissues shown here is this blue note.",
                    "label": 0
                },
                {
                    "sent": "We have tissue brain tissue, specific protein direction and the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Here follows what we know.",
                    "label": 0
                },
                {
                    "sent": "Our autonomy, anatomical relationships between different brain tissues.",
                    "label": 0
                },
                {
                    "sent": "So we did.",
                    "label": 0
                },
                {
                    "sent": "This case study was to take to assume that our hierarchy and is given by this organizational brain tissues and we have nine brain tissue specific networks.",
                    "label": 0
                },
                {
                    "sent": "Now let's learn and train on it on this network to learn embeddings for notes.",
                    "label": 0
                },
                {
                    "sent": "We can learn embeddings for notes and then we can visualize them.",
                    "label": 0
                },
                {
                    "sent": "So this is what happens when we visualized them.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If I turn back here, you can see that this is this hierarchy had to has two levels, so there is the root of it, which in this case is brain.",
                    "label": 0
                },
                {
                    "sent": "The brain has this five different tissues that are immediate children, and then it's brain stem, which further has four four different kinds of tissues.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we what we do, what we did, and we visualize embeddings at the brainstem level at end of the brain level.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or the brainstem level.",
                    "label": 0
                },
                {
                    "sent": "Here we we, we get visualization that looks like like that.",
                    "label": 0
                },
                {
                    "sent": "Every point here is a protein.",
                    "label": 0
                },
                {
                    "sent": "Protein is colored by the tissue that it's active in.",
                    "label": 0
                },
                {
                    "sent": "And what is important here is that the embeddings that were learned or learnt in such a way that that that that we get very naturally and automatically with any kind of information.",
                    "label": 0
                },
                {
                    "sent": "Additional information we get is clear clusters.",
                    "label": 0
                },
                {
                    "sent": "Of of the same color, representing 40 is active in the same tissue, and this happens even when we look at one level up at the level of brain.",
                    "label": 0
                },
                {
                    "sent": "When we get this clear, clusters of proteins active in different tissues and the reason why we can do that at the level of brain is because we learn this multiscale embeddings.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we get, we get an embedding for the note.",
                    "label": 0
                },
                {
                    "sent": "Well for us before before each graph.",
                    "label": 0
                },
                {
                    "sent": "But also Allaire jesters of the hierarchy so we can visualize how to embedding's look like on each level of the hierarchy, such as each network or on each graph or the brainstem level or of the brain.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this shows that embeddings that we learned actually were such that they adhere to old assumptions that we are making in the data about proteins being active in a similar way having similar embeddings and proteins being active in similar tissues having similar imbalance.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, what we did.",
                    "label": 0
                },
                {
                    "sent": "Here is is is using the methodology for the problem of transfer learning or annotating tissues for which none of the information is available currently so.",
                    "label": 0
                },
                {
                    "sent": "The idea on the task the task here was to predict functions in a specific tissue without having access to any label information in the tissue.",
                    "label": 0
                },
                {
                    "sent": "So really, the assumption is so that the simulation here is what happens when.",
                    "label": 0
                },
                {
                    "sent": "Approach interaction network for a particular context is available.",
                    "label": 0
                },
                {
                    "sent": "It's given to us and our goal is to say our goal is to label proteins in that issue.",
                    "label": 0
                },
                {
                    "sent": "What can we say about functions of proteins in debt?",
                    "label": 0
                },
                {
                    "sent": "Protein interaction in the newly generated 40 interaction network when none of the labels in debt network RFA.",
                    "label": 0
                },
                {
                    "sent": "So what so this was the setting that was that was tested here.",
                    "label": 0
                },
                {
                    "sent": "Here I'm showing performance in terms of AUC Roc for for this setting where in particular you can really stable as follows.",
                    "label": 0
                },
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "Pete Rose represents a different target tissue, so the first draw.",
                    "label": 0
                },
                {
                    "sent": "What he tells us is that one of the disposition was natural killer cell.",
                    "label": 0
                },
                {
                    "sent": "And when we include information on natural killer cell, the.",
                    "label": 0
                },
                {
                    "sent": "When we make prediction based on currently available annotations on natural Killer cell, this is the AUC Roc score that we get.",
                    "label": 0
                },
                {
                    "sent": "However, when we pretend that there is none, information is available on natural killer cell and we want to annotate natural killer cell tissue.",
                    "label": 0
                },
                {
                    "sent": "The aims Iraq is somewhat lower, but it's still very much different from what one would expect if there would be no predictive power.",
                    "label": 0
                },
                {
                    "sent": "So these final roll the spinal column here, tell us the power of the methods to be to predict an annotate tissues, if no.",
                    "label": 0
                },
                {
                    "sent": "Labels for death tissues are available.",
                    "label": 0
                },
                {
                    "sent": "So the reason we can do that then how we actually do that is by taking into account the embeddings in the structure of the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "To predict, make predictions for, for, for a completely new tissues, which with no labels.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's that's something yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes we need to have its placement in the hierarchy which.",
                    "label": 0
                },
                {
                    "sent": "Which I would argue is fairly reasonable assumption to some extent, some extent.",
                    "label": 0
                },
                {
                    "sent": "This interesting that having this.",
                    "label": 0
                },
                {
                    "sent": "This very small amount of information where the new tissue is placed in the hierarchy gives us lots of power already to be able to transfer annotations from from other tissues to date to proteins in the in the mutation.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this concludes this first part of the talk for today on heterogeneous networks when I where I talked about how to embed it, true genius networks, an application on tissue specific protein function prediction.",
                    "label": 0
                },
                {
                    "sent": "And in the talk in the lecture tomorrow we will build on this through Genius Network idea and specifically will be focused on graph neural networks for learning deep embeddings of notes.",
                    "label": 0
                },
                {
                    "sent": "For for the most general representation of heterogeneous networks and knowledge graphs.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of the resources that I would like to mention here that might be useful.",
                    "label": 0
                },
                {
                    "sent": "Are the following.",
                    "label": 0
                },
                {
                    "sent": "One of them is Mambo, which is a tool that we developed for multimodal biomedical network representation.",
                    "label": 0
                },
                {
                    "sent": "So it's the tool that's that's that allow you to build, construct, represent, and analyze highly multimodal networks in the form of also solving some of the challenges with linking nodes that you might use different naming terminologies in different datasets and then achieving provenance of those data sources that are used as input to construct this multi model networks.",
                    "label": 0
                },
                {
                    "sent": "In the two scales really well to very large networks, so the larger biomedical network that we use, this tool weight had 2.3 billion edges in 2000 nodes and was really very large knowledge graph of biomedical data.",
                    "label": 0
                },
                {
                    "sent": "Second, for most of the projects that I outlined in this fight here in this lecture, the the network data is available at project websites of some of these studies and papers.",
                    "label": 0
                },
                {
                    "sent": "Epstein, 40 do you slash projects for the more there is the new repository that we have started on.",
                    "label": 0
                },
                {
                    "sent": "Biomedical network datasets, which is a repository of high quality preprocessed Clint biomedical networks that are linked so you can combine them in various different ways you would like with the goal of providing methods that can be used for algorithm development and benchmarking.",
                    "label": 0
                },
                {
                    "sent": "And so that those kinds of biomedical datasets will be will be more often used in methods on new algorithms so that we can see how this biomedical, how new algorithms work on biomedical data, and that's called bio SNAP or Bio data.",
                    "label": 0
                },
                {
                    "sent": "It's really.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to conclude, this was today was the first part of the lecture.",
                    "label": 0
                },
                {
                    "sent": "What we discussed is shallow net various methods for shallow network embeddings.",
                    "label": 0
                },
                {
                    "sent": "Some of the takeaway messages are, but there exist many different notions of how different define a note similarity depending on that notion of note similarity, you can learn embeddings in different ways.",
                    "label": 0
                },
                {
                    "sent": "And then finally, it's very important and useful to integrate different kinds of prior information in our model, which was demonstrated with Virginia's network comparing model and that will become more clear tomorrow when we will look at more general knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "So tomorrow I will move away from shallow network embedding methods, focus on deep network models, and specifically describe how those models are useful for polypharmacy in computational direct repurposing.",
                    "label": 0
                },
                {
                    "sent": "OK, and then I can.",
                    "label": 0
                },
                {
                    "sent": "I can take more questions for this.",
                    "label": 0
                },
                {
                    "sent": "Thank you America for the exciting lecture.",
                    "label": 0
                }
            ]
        }
    }
}