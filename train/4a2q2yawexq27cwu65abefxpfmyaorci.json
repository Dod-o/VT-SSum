{
    "id": "4a2q2yawexq27cwu65abefxpfmyaorci",
    "title": "Deep Learning",
    "info": {
        "author": [
            "Ruslan Salakhutdinov, Department of Statistical Sciences, University of Toronto"
        ],
        "published": "Oct. 9, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_salakhutdinov_deep_learning/",
    "segmentation": [
        [
            "My name is Russ.",
            "I'm with the Department of Computer Science and Department of Statistics at University of Toronto so and by the way, as I go through the tutorial, please feel free to ask questions.",
            "You know this is going to be, you know, partly informal, what I'm hoping to cover is that I'm hoping to cover a lot of different applications that we've seen.",
            "Some of the successful applications in deep learning, but also give you a little bit of theoretical foundations behind these algorithms, right?",
            "And particularly focus on.",
            "Graphical models and such, so hopefully what you'll get out of this tutorial is just a little bit of math behind these algorithms as well as different applications that."
        ],
        [
            "By using OK, let me just start by saying that you know my area of research is machine learning and it's a very exciting area and I know a lot of people here at KDD.",
            "Also experts in machine learning, but it's kind of very dynamic field, right?",
            "If you look at different areas like computer vision, natural language processing, information retrieval, speech recognition, robotics, computational biology, cognitive science is right.",
            "A lot of these fields are using."
        ],
        [
            "Machine learning, but what's exciting is that you know in the last few years if you look at the space of data that we have right, you can easily get access to images and videos or text to speech.",
            "If you look at various product recommendation companies like Netflix and Amazon, or if you look at social network data or even if you look at scientific data, right?",
            "That's a lot of data.",
            "But that would say that most of the data that we see today is unlabeled, right?",
            "It's basically that means that I can get millions of images of the web, but I might not have labels for those images, so it's precise description as to what's going on in those images.",
            "Right, so one of the things that we try to do in particularly deep learning community is is trying to develop models that can deal with unlabeled data or models that can discover structure or discover some statistical dependencies from the data and how that can be done in unsupervised without labels or semi supervised where you have partial labeled data.",
            "And also it's interesting you know in machine learning we sort of obsessed with developing algorithms that can work across multiple application domains.",
            "And I'll show you some of some of the examples of that."
        ],
        [
            "And would I would argue, is that you know building models that can do these things is 1 particular framework of doing that is building these deep hierarchical models and deep learning models models that can support inferences as well as discover structure at multiple levels of representation, right?",
            "And I'll make it more precise as I go through the tutorial of what I mean by that.",
            "But let me show you before I start going into, you know the definition of the models and you know go through the tutorial.",
            "Let me just show you some of the exam."
        ],
        [
            "Course, right?",
            "This is 1 particular model.",
            "It's called the Boltzmann machine model, and what you're trying to do here is you basically trying to model images of these.",
            "You're trying to model distribution over those images, right?",
            "So you're trying to ask.",
            "The model cannot recognize airplanes or cars or trucks and such, and because this is a generative model, what you can do is you can simulate from the model or generate data from the model.",
            "So if I'm asking the model to sort of.",
            "Tell me what it believes.",
            "Images of airplanes should look like.",
            "This is the model generating images of airplanes.",
            "In the flickering that you see at the top.",
            "These are the states of the latent variables.",
            "So hidden variables, right?",
            "So it's kind of like discovering what airplane should look like so you can see that it goes from one airplane to another plane and.",
            "Not argue that sort of has some understanding of what airplanes should look like now.",
            "The interesting thing about this model is that there is no image specific prior sitting in these models.",
            "The model just looks at 96 by 96 images and figures out what the airplane should look like if you."
        ],
        [
            "Look at other kinds of datasets, so this is an interesting data set.",
            "This data set was collected at MIT.",
            "You have about 25,000 characters from 50 different alphabets, and if you look at these characters you have Arabic.",
            "You have cereal.",
            "If you have Sanskrit.",
            "Right, and the model has about 3000 latent or hidden variables over 2 million parameters, and then you can ask the model to see what images of Sanskrit should look like right?",
            "And this is the model simulating what it believes images of Sanskrit should look like, and I typically ask the audience to see do these look like Sanskrit.",
            "Somewhat.",
            "Reasonable.",
            "I see, right?",
            "So Yep, I have no idea how it's supposed to look like, but this is what the most believes.",
            "Sanskrit images should look like, and this is useful for doing things like recognizing characters right or doing character recognition.",
            "You can also do it."
        ],
        [
            "Doing things like pattern completion, right?",
            "So for example, if I give you half of the input and I ask the model to simulate the remaining half right, this is what the model does, and again there is no image specific prime not telling the model that you know they should be strokes, they should be continuous.",
            "Lines right, the model just looks at 30,000 different characters and just figures out what characters should look like.",
            "So why why building these models is is?"
        ],
        [
            "Is challenging well if you look at the simplest possible model right, this is just a 28 by 28 image.",
            "It's binary image, right?",
            "But if you look at the space of all possible characters right, the space of all possible images you could generate?",
            "Well, let's do to the 28 by 28 or two to the 784 possible configurations.",
            "Right, so obviously doing it via the brute force method is going to be challenging, so we have to rely on some approximate algorithms and we'll see some of those algorithms."
        ],
        [
            "In this tutorial you can also the interesting thing about these models that you can also apply to other kinds of data.",
            "So if you're looking at bag of words representation, this is just taking.",
            "Documents and representing of them as word counts.",
            "If you take Reuters data set has about 800,000 web pages and you say, well, how can I extract structure from that data?",
            "If I look at the high level representations, this is what the model is discovering, right?",
            "So I'm just basically there are no labels.",
            "I'm just feeding the model bunch of different web pages and I'm asking it to.",
            "Discover the latent structure and this is what it's discovering, and it's interesting to see that you know sort of finds different groups of of these documents of these web pages.",
            "And the interesting thing here is that it sort of thinks that European Community economic stuff is next to disasters and accidents, right?",
            "So there's a lot of sort of finds a lot of correlations between between these two topics.",
            "I guess today those two lines would be even closer together, right?"
        ],
        [
            "So one of.",
            "One of the also exciting sort of developments recent developments in the deep learning communities.",
            "Convolutional models you probably heard about convolutional neural net style models, right?",
            "And this is kind of a neural net model, something that we'll talk about in this tutorial where you take an image.",
            "You design these sort of called C1 layers and then you're doing pulling you doing convolutions and pulling.",
            "It's fairly, you know, it's fairly straightforward model, but the Chiefs really remarkable results.",
            "Let me just.",
            "Show you a little bit of demos and it's always difficult to do life demos 'cause you know what.",
            "If they don't work right.",
            "So what I'm going to do?",
            "Is I'm going to show you so this is a website that we've been building and you know, for example, you can upload an image like this one.",
            "Right, so you can upload images like this one and the system will essentially tell you that it's 73%.",
            "It's a chimpanzee.",
            "Nine percent is hippo, so it's confusing it with heap or gorilla or such.",
            "So this is sort of an image recognition system.",
            "Again, it's one of the deep learning systems, and you know if you want to test the system, you're more than welcome to test it.",
            "But maybe I can just ask somebody from the audience to just think about any object.",
            "Anything?",
            "What would you like to recognize?",
            "Car OK. OK, let's pick a car.",
            "This Google images.",
            "So these are all sort of good looking cars, so these are easy cars.",
            "How about this guy here?",
            "Sexually right?",
            "So we can go to here, specify the URL and just ask the model to classify right?",
            "So let's see what it says.",
            "Thinks it's a car wheel, thinks it's a low motor tractor harvester, amphibian vehicle.",
            "Yeah, kind of looks like one of those, right?",
            "But obviously if you know if you want to recognize sort of cars like you know, we can.",
            "Obviously, how about this guy here?",
            "Right, it's sort of like a car.",
            "Upside down, well hopefully.",
            "The model will recognize it, so it takes about for this systems, let's see what it says.",
            "Sports card.",
            "There you go.",
            "Right?",
            "But so this is the system that can recognize 1000 different classes, and it's a pretty reasonable system, and I should also point out the code for training these systems and using them is online.",
            "It's all public public domain.",
            "You can interact with the system, you know, it can recognize different kinds of dogs, spiders, different animals, furniture, you know.",
            "We're trying to extend the system going to 20,000 classes, so.",
            "OK."
        ],
        [
            "Now the other thing that I'm going to talk as we go through the tutorial or these data.",
            "That's multi model.",
            "So this is the case where you have for example images in text right?",
            "So for example given an image like this one you can find different tags associated with images or given a particular word.",
            "Maybe you can do image retrieval right?",
            "So we'll see some of that in the second part of part of the tutorial."
        ],
        [
            "But the interesting thing is that you know you can do fun things like not only recognizing objects, but also sort of recognizing descriptions of objects, right?",
            "So like if you query the system is a fluffy sort of returns these images was a delicious determines these images, right?",
            "So I'm going to talk a little bit more about how we can work with words as well as images and sort of find semantic relationships between words and images."
        ],
        [
            "You know, there's another thing that has been done by other other people, so this is 1 particular example of trying to recognize roads in satellite images."
        ],
        [
            "Right, and this is again using in deep learning system that essentially figures out roads and images and you can imagine that's very useful for people who are trying to update the Maps of.",
            "Of the roads based on."
        ],
        [
            "No satellite imaging OK, so.",
            "I'm going to go through these topics in this tutorial, so I'm going to start with a little bit of introduction, sort of motivating deep loading and I'm going to talk about sparse coding in autoencoder models.",
            "Then I'm going to give you a brief introduction into graphical models and restricted Boltzmann machines.",
            "You know ways of trying to learn low level features as well as deep belief networks, and this is sort of trying to learn part based hierarchies and I'll give you a little bit of mathematical foundations behind behind these models.",
            "And at the second part of the tutorial, after the break, we're going to look at a little bit more advanced models.",
            "These would be Boltzmann machines, the Boltzmann machines, multimodal learning, as well as a little bit of language modeling.",
            "Um?",
            "And as I mentioned before, if you have any questions as a.",
            "As I go through the tutorial, you know.",
            "Feel free to ask me."
        ],
        [
            "OK, so one of the general ideas of deep learning is is trying to learn meaningful representations.",
            "One of the main motivations behind deep learning is trying to learn representations or trying to learn useful features right?",
            "If you look at a lot of traditional learning algorithms is you have, let's say, a data and you're applying some kind of learning algorithm to it.",
            "But if you're using for example pixel representation of the data, right?",
            "And you're trying to separate Segways from non Segways?",
            "Well the pixel space looks.",
            "Kind of ugly.",
            "It's very hard to do recognition based on pixels, right?",
            "On the other hand, if you able to construct meaningful representations like getting object or parts of the objects like handle and will."
        ],
        [
            "And you're getting to this feature representation than in the feature space.",
            "You can basically separate to be able to separate the two classes, right?",
            "So how can we do?",
            "How can we learn these repres?"
        ],
        [
            "Stations right, so for example, how is computer vision down?",
            "Computer perception is done.",
            "Well, we have some data.",
            "You find some features from the data and that's how it's been done in the last 20 years and you apply your favorite learning algorithm, right?",
            "Like support vector machines for example.",
            "You know, for object detection you have an image.",
            "You try to find some low level features and you get the recognition or for classification audio classification you can get some low level features and then you construct the algorithm."
        ],
        [
            "From it, and if you look at a space of possible features right in the way that computer vision was done is in the last 20 years is that people were trying to figure out what are the right representations to use based on images.",
            "So if you look at different features, there is things like Hog sifts, text ONS and all kinds of stuff that people have been developing."
        ],
        [
            "If you look at the audio features again this spectrograms MFC sees flux and again all kinds of different features to use."
        ],
        [
            "But one question that you know we are interested in and general deep learning communities interested in is that how can we do feature learning?",
            "Can we actually figure out what are the right features to use from this data?",
            "And it turns out that you know, I've shown you some of the examples of recognition.",
            "It was actually remarkable breakthrough, because by using these models and learning the features themselves right, you cannot perform basically the state of the art recognition systems that have been used.",
            "You know that the vision community has been using with speech recognition community have been using in the past 1015 years."
        ],
        [
            "So one particular model.",
            "This is sort of the oldest model for learning features is is sparse coding model, and it's been used a lot.",
            "That's a very popular models.",
            "One of the little building blocks of many deep learning algorithms.",
            "And the idea is the following.",
            "You have a set of input data vectors, X12 exam and you want to learn a Dictionary of basis and you want a Dictionary of basis such that you can reconstruct the data based on these bases, right?",
            "So it's a very simple formulation, but the key thing here is that what you'd like to do is you'd like these.",
            "These coefficients to be sparse, So what is essentially saying is that well, each data vector is going to be represented as a sparse linear combination of basis.",
            "So you can imagine that you have you know 10,000 basis, but you might want to say, well, only ten of these vectors should be used for reconstructing the data, right, that's?",
            "And that's the idea.",
            "And this belongs to the class of so called unsupervised learning algorithm, right?",
            "There is no label.",
            "So just trying to find the right basis so that can reconstruct the data as much as you can."
        ],
        [
            "So for example, for natural images, if I take the natural images and I take little patches from those natural images, then this is how these bases would look like.",
            "So you can see a lot of them look like little edges or a little Gabor like filters, and that seems like the right structure to extract from images, at least when you talk to.",
            "Neuro scientists.",
            "They believe that that's actually something that happens in the visual area.",
            "One human brain.",
            "If I give you a new example or a new Patch, then I can essentially.",
            "Reconstructed as maybe as some of these three patches, so you can sort of.",
            "See that you pick sub sparse number or few number of these bases in reconstructing example, and it turns out that these new representation that you get this sparse representation that you get is 1 particular way of doing feature representation.",
            "That turns out to be much more useful to deal with than the pixels themselves, right?",
            "So working in this basis, if you want, for example classify these images.",
            "Turns out to be much more.",
            "Has much better generalization."
        ],
        [
            "Performance.",
            "Right, so how would you do training of these models?",
            "Well, you have N inputs and you'd like to learn the basis.",
            "So you have two different terms.",
            "One term is essentially saying, well, you'd want to reconstruct the data's without introducing any errors.",
            "So you'd like to have a small reconstruction error.",
            "On the other hand, you also have a sparsity penalty, you say?",
            "Well, I don't want to use a lot of bases to reconstruct the data, I just want to use a few basis to reconstructed data so it's kind of like controls the complexity of the model, right?",
            "And the optimization itself can be done fairly easily.",
            "It turns out that you know for a fixed if you fixing the basis by why up to 5K and you solving for the coefficients A, then it turns out to be standard lasso program problem, right?",
            "So it's just squared loss plus L1 penalty.",
            "Let's just let's Sue and that can be solved fairly efficiently using quadratic programming, right?",
            "So that's standard machine learning 101.",
            "And then when you fix the activations, you optimize for the dictionary basis and then again that's a convex optimization problem.",
            "It's a quadratic programming problem and you can solve that so you can sort of do these alternating minimization, fixing the basis, finding the activations, fixing the activations, refitting the new basis."
        ],
        [
            "So that's a pretty straightforward and now at the test time.",
            "If I give you a test image.",
            "As well as the learned basis.",
            "Then what you'd like to get is you'd like to get a sparse representation, or these coefficients for a given image, and then you can represent this knew test batch again as as a linear combination of the learn basis, right?",
            "Now."
        ],
        [
            "You know, if you apply this model is sort of like this was done back in 2006 when the deep learning sort of was just, uh, you know, with stages.",
            "But for example, here's just one example.",
            "If I give you an input image and I give you these four different bases and I scan my image using these bases, then this is kind of representations that I'm learning right, using four different bases.",
            "And then you can take these representations.",
            "You can use a support vector machine or any other classification algorithm, and then you can basically get fairly reasonable performance, right?",
            "So you can see from the baseline and from PCA you can using just this simple sparse coding, a single layer model, you can improve recognition accuracy of."
        ],
        [
            "Of these images right now, the way to interpret sparse coding and we'll get into that more and more as we go through the tutorial is you can view it as.",
            "Cora decoder model.",
            "You have sparse features here.",
            "Right, and then you reconstructing the data so there is an explicit linear decoding stage, right?",
            "If I give you sparse features, I can reconstruct the data.",
            "But there is also an encoding scheme given an input X, you'd like to get the representation of sparse representation of these coefficients, but the encoding itself is implicit, and it's very nonlinear, right?",
            "It's implicit because you actually have to solve for these sparse coefficients."
        ],
        [
            "Right, but now this can be generalized to a general models called encoder decoder, model, autoencoder model and.",
            "The idea is that you get an input image, you encoding that image.",
            "You're getting some feature representations and then you decoding it to get back the original input.",
            "And the encoder is sometimes known as a feedforward or bottom up inference and the decoder is known as a feedback or generative or top down right?",
            "So you can imagine going from features all the way back to the back to the image.",
            "So when I was showing you these generative models like generating airplanes are generating characters.",
            "The generation part was the decoder part.",
            "Given some latent variables, some features you actually generate generate the data.",
            "And they details what's going inside encoding, decoding actually matter.",
            "It's been a lot of work across different people trying to figure out where the right.",
            "Pieces that would should go into encoding decoder and one thing that you'd like to do is you.",
            "Also, we need to put some constraints doing to essentially avoid learning identity like.",
            "Obviously if your feature representation is the same as the input dimensionality of the feature space is the same as dimensionality of the input space, you could just.",
            "Put the identity function right and you just say well don't do anything.",
            "So obviously we need to put some constraints."
        ],
        [
            "There.",
            "So one particular encoder decoder that's being used a lot is trying to come up with binary representation, so binary representations of features and we'll see a lot of that and then non linearity that you put is the sigmoid function.",
            "It's a fairly simple function, used a lot, but that way you're getting binary representations, and then the decoder is a linear decoder so you can reconstruct Becky original input.",
            "Right?",
            "So this is just one particular just formulation of how the encoding can be done with decoding can be done so this part is the encoder, you just take your data, you multiply by matrix W, you pass it through the sigmoid and the decoder is basically a linear reconstruction based on the latent features that you're learning.",
            "Right?"
        ],
        [
            "We can also determine the parameters WND from the data."
        ],
        [
            "By essentially trying to minimize the reconstruction error, and that's important because you know one question is how do you actually learn the right features, right?",
            "Even in the autoencoder model you can look at the reconstruction by basically saying, well, how can you encode your data in such a way that when you can reconstruct it back you don't suffer any reconstruction, sorry.",
            "Right, obviously if your reconstruction error is 0, then you have a perfect encoder, right?",
            "And in particular, if the number of hidden layers or the number of hidden features here is much smaller than the number of input dimensions, and you have perfect reconstruction, then you have perfect compression of the data, right?",
            "You can also view these kinds of models trying to compress the data."
        ],
        [
            "Now the interesting thing about these models is that if the hidden layer as well as the output layer linear.",
            "You're trying to minimize the reconstruction error.",
            "Then it turns out that you can recover PCA, right?",
            "You can recover principle components analysis.",
            "So if you look at these K hidden units, they will span the same space as the first gay principle components, so it's kind of interesting if you basically make this model be linear, then you're recovering PCA and many of you probably heard about PCA.",
            "It's like one of the heaviest use tools for, you know, across many many different domains is data compression.",
            "Algorithm so you can view autoencoders is nonlinear extension of PCA, which can be much more powerful than PCA."
        ],
        [
            "If your data is binary, then obviously you can also as a decoder in a decoder stage you can you also use nonlinear decoding function, in this case sigmoid function, and you know this is very also successfully used models and it actually relates to something that's called restricted Boltzmann machines or relates to restricted both machines and graphical models.",
            "OK, so one of the key components in a lot of these models is that you want to get sparse representations right.",
            "That turns out to be a very useful thing, and empirically people found that you know if you want to get good features you want to put some kind of sparsity on on your latent representations, and here, if even if you're trying to discover binary features you'd like them to be sparse."
        ],
        [
            "And that's exactly sort of the model that was done by NYU Group A few years ago.",
            "We again you're trying to minimize you have a decoder function that's attempting to minimize this reconstruction error, so this is this is the reconstruction error.",
            "This is the penalty on these latent features.",
            "You want these features to be sparse, so you put L1 penalty.",
            "That's a common thing to do, and this is the encoding function, right?",
            "It turns out that the encoding function is also very useful to have.",
            "In particular test time, right?",
            "It's very useful to have an explicit encoding function, because at the test time when I show you a new image or when I show you a new web page you'd like to get latent representations very quickly, right?",
            "And an explicit decoding encoding function allows you to do that."
        ],
        [
            "Now, one of the things that people realized you know five or six years ago is that, well, you can actually stack these things.",
            "OK, so you can learn some representations.",
            "You know, first layer features, then you can repeat the operation.",
            "You can say, well, can I actually take these latent features and encode them again and so forth and maybe at the top level, I'm going to have my classification model or whatever you want to use them for.",
            "Now when you look at the stacked autoencoders, you might ask why people haven't thought so.",
            "Autoencoders existed in the last 20 years, but only 5, four, five years ago.",
            "People figured out that you can actually stack them.",
            "The question is why people haven't thought of this thing before.",
            "That's a natural thing why people haven't tried doing that.",
            "And it turns out that you know if you look at the stacking here, it's not obvious whether it's a hack or whether there is some deep mathematical insight behind these kinds of models, and I'm going to show you.",
            "That stacking makes sense in terms of actually optimizing a certain kind of objective function, so it's not.",
            "It's not a hack."
        ],
        [
            "This is known as a greedy layer wise pretraining or greedy layer wise learning of."
        ],
        [
            "For the features.",
            "Now at the desk time you can remove the decoder if you just interested in recognizing what's going on in images or speech signal or web pages, and Justin code, given an input X, you just go through multiple layers of representations and then you you can use it for for classifying things.",
            "Um?",
            "In general.",
            "You know you can use standard.",
            "Encoder an that's sort of equivalent to neural network model, and you'll network architecture.",
            "You can use convolutional, which is again has to do with parameter sharing and the parameters of the entire model can be trained or can be tuned using back propagation algorithm and the backdrop algorithm.",
            "You know it's been.",
            "It's been around for quite some time."
        ],
        [
            "So one question you might want to ask is there top down bottom up, can we actually design algorithms that maybe have some kind of feedback connection?",
            "An is there a more rigorous mathematical formulation, right?",
            "Obviously a lot of these things are driven by empirical success.",
            "And yes, there is a lot of empirical success.",
            "But then the question is, is there a little bit something more we can say about these models?"
        ],
        [
            "OK, so let me give you a little bit of introduction to graphical model just so that we can get the notation right as we go through the rest of the tutorial.",
            "How many of you know about graphical models?",
            "OK, a lot so so I can go.",
            "I can go fairly quickly there.",
            "This is just primarily for set."
        ],
        [
            "Sing up for setting up the notation for us, right?",
            "So it's interesting if you look at the history of deep learning.",
            "I'd started with basically looking at these models from graphical model perspective.",
            "Right now there are a lot of different algorithms, like sparse coding and autoencoders, but the initial models were inspired by graphical models and doing inference in learning in graphical models.",
            "So graphical models.",
            "It's a very powerful framework for representing dependencies structure between random variables, right?",
            "Typically when we look at graphical models.",
            "The graph itself will contain a set of nodes, verticies that will represent random variables and will have links that tell us something about dependencies between those random variables.",
            "And the joint distribution of those random variables can be decomposed into the product of factors, right?",
            "Each one of those factors is looking at a subset of these.",
            "Subset of the nodes.",
            "Now.",
            "There are two types of models.",
            "There are directed graphical models.",
            "Something is called Bayesian networks and they've been around for quite some time, and they're also undirected graphical models, and these are known as Markov random fields of Boltzmann machines.",
            "In this tutorial, we'll be focusing a little bit more on undirected part.",
            "Just because a lot of models, lot of these models come from undirected graphical models, but there are also hybrid graphical models and we'll see some of those in hybrid graphical models.",
            "Models that combine both directed and undirected models together.",
            "So for example, you know deep belief networks.",
            "For those of you preferred about deep belief networks, these are actually hybrid graphical models.",
            "They have both undirected and directed parts to it.",
            "More like hierarchical deep models."
        ],
        [
            "And the idea behind directed graphical models, it's directed graphical models actually used a lot in the machine learning, and they're useful for expressing causal relationship between random variables.",
            "Right, so for example in this picture here, right, the joint distribution that specifies this particular is consistent with this particular graph is you can say, well, my joint distribution of these random variables is just going to be written as a product of note given as a conditional distributions of nodes.",
            "Given the parents of the nodes.",
            "Right, so for example, in this case, if I look at the joint distribution of these seven random variables, then it factorizes precisely this way.",
            "There is P of X1, P of X2 PX three.",
            "If you look at X, 4X4 has parents X, 2X1, and X3, so that's the number that I have here, right?",
            "And notice that the graphical model tells me something about how the distribution factorizes, right?",
            "And this is known as directed acyclic graph with."
        ],
        [
            "And it, you know, graphical models in particular directed graphical models are very useful.",
            "Been used a lot in.",
            "A lot of different domains, but you know maybe one example is just to show you as you know how would you generate an image?",
            "Well, you might have latent variables like object position, orientation, right, and then the all of these three random variables might have independent priors.",
            "And then the way you generating the image is, you say, well, if I know the object identity, the position and orientation, then I'm going to have a likelihood function here that essentially tells me what's the probability of the image given these two variables, right?",
            "So, given these three random variables, I can generate the image.",
            "You know this is sort of a Canonical way of how people in the computer graphics with computer vision communities are generating images.",
            "Now, inference in these models is a little bit tricky because what you'd like to do is you'd like to invert the errors, right?",
            "You'd like to say.",
            "Well, if you show me the image, can you tell me the object identity?",
            "Can you tell me the position?",
            "Can you tell me the orientation, right?",
            "And to do that you can use something that's called Bayes rule.",
            "So you inverting it.",
            "But the problem here is this term known as a marginal likelihood, writes the probability of the image.",
            "It's sometimes called normalizing constant and we'll see some of.",
            "Ways of dealing with that, and that becomes fairly difficult thing to do, so a lot of these inference algorithms you can specify the generative part, but actually doing inference is difficult, right?",
            "Obviously you'd like to show you test image and you'd like to tell me that that's a car and where is it in the image and such?",
            "That's difficult."
        ],
        [
            "This problem problem, Now Markov random fields, on the other hand, they don't have directed edges, right?",
            "What you have instead is you have potential functions in these potential functions, essentially a mapping from the joint configurations of these random variables to some non negative real real numbers.",
            "And the choice of these potential function is not really restricted to having some kind of probabilistic interpretation, right?",
            "So the difference is that you know the way to think about these things is that if I show you an image right and I'm trying to look at two pixels in the image, it's not like one pixel causes another pixel right?",
            "It's mostly just dependencies within two pixels to saying, well, they should probably be have the same value together because they are located nearby in the image, right?",
            "So it has a very different interpretation, but the potential functions sometimes are represented as exponential.",
            "It's a useful representation to have.",
            "So you can say well, the product of these potential functions can be written this way, it's just exponentiate them and you have these terms known as energy functions.",
            "So depending who you talk too, sometimes people like to talk about potential functions and representing probabilities, potential functions.",
            "If you talk to statistical physicists, they'd like to represent everything in terms of energies.",
            "And this is this is known as a Boltzmann distribution, and it's remarkable you know Markov random fields in these and directed graphical models really half half the roots of these models where basically developed in the statistical physics community, right?",
            "So we sort of inherit a lot of things like Boltzmann machines and energy functions and such.",
            "But one of the challenges again in looking at these models is that suppose you have binary random vectors, right?",
            "Just just access taking values plus one and minus one, right?",
            "If X has 100 dimensions, then you essentially in order to get the valid probability distribution you need to sum over 200 different configurations, right?",
            "If we want to say, what's the probability of that configuration?",
            "Right, that's exactly when I was showing you these.",
            "Generating these handwritten characters right.",
            "This is precisely roughly the model that I was using.",
            "And again, if you look at the space of all possible images, you could generate that space is exponential.",
            "So a lot of work in the machine learning and deep learning communities and statistics is dedicated to how can you sort of search over that exponential space efficiently?",
            "We'll see some of that, so computing these normalizing constant Lizette is sometimes called normalizing constant.",
            "Sometimes it's called partition function, sort of name, inherited from statistical physics.",
            "Um, that actually represents a major limitation of undirected graphical models.",
            "These are quite powerful models, but if you can compute these, normalizing constants."
        ],
        [
            "So let me just show you how maximum likelihood can be done in these models and we'll see some of that.",
            "Imagine I show you this small.",
            "It's a very simple model, but it's a very useful model that a lot of people in particularly computer vision communities, are using 'cause it tells you something about smoothness, right?",
            "It's essentially, this is the model that's telling you you know nearby random variables should take the same value, like when you when you look at images you'd like to say.",
            "Well nearby pixels should have the same value.",
            "Occasionally they have different values because there's an edge.",
            "But in general, you know if you look at images, these are just not random pixels.",
            "Turning on and off right?",
            "They have they have structure and that's a pretty good model for encoding that spatial structure.",
            "So given a set of ID images X one up to XN, you'd like to model the parameters of this model, figure out what these parameters should be, and you can use likelihood objective log likelihood objective.",
            "And the likelihood objective has a very sort of intuitive interpretation, right?",
            "It essentially is telling you well, if you show me the data, give me the data right?",
            "How can I find parameters of this model in such a way that the joint distribution of all of these are joint probability of all of these images is as high as possible?",
            "You'd like your model to be probable as possible.",
            "Sort of explain, explain the data, and that's the likelihood objective, right?",
            "And it's pretty easy to take, you know.",
            "The derivative this likelihood objective and you have these two terms right, and this is something that's called expected.",
            "Sufficient statistics driven by the data.",
            "What essentially does is it looks at the training data.",
            "And it basically says you know what's the correlation structure between nearby random variables.",
            "That's all what it says, right?",
            "Just looking at correlations between random variables and this thing.",
            "Here is the expected sufficient statistics by driven by the model, right?",
            "And it essentially says you know what are the correlations induced by the model according to the model with my correlations are and your maximum likelihood solution is essentially saying well try to match the two right whenever you're building them all, try to basically look at the correlations that the model is inducing in.",
            "You know saying you know those should match the data distribution, but unfortunately this term here is difficult to compute, 'cause they're exponentially many configurations, right?",
            "And will."
        ],
        [
            "In some ways of doing doing that, you know if you look at Markov random fields with latent variables, that becomes even more challenging and that's the challenge.",
            "When we start looking at deep learning models here, the X is composed both with something called visible variables, like pixels in the image of speech signal or words in a document as well as latent variables, and these latent variables.",
            "Can tell us something about, you know either the topics or semantic meaning of of the data.",
            "You can specify the probability in exactly the same way, But the problem is again you have this normalizing constant, but even more problematic you have this summation over the latent variables, right?",
            "So if you try to do inferences, these models, you basically want to say, well, what's the distribution of the latent variables that I'm trying to infer.",
            "Um?",
            "And the parameter learning becomes very very challenging tasks.",
            "So a lot of work and I'll show you some of that work in the deep learning community is trying to basically figure out how can you deal with that intractability.",
            "And as I go through tutorial, I'm going to make it all the more precise."
        ],
        [
            "So let me move into a class of models called restricted boss machines.",
            "In these kinds of models that are very useful for learning low level features.",
            "OK."
        ],
        [
            "These are kinds of models.",
            "Sometimes they called undirected graphical model.",
            "These are by part type.",
            "They have a bipartite structure in the simplest model you have stochastic binary visible variables.",
            "Again, think of them as pixels in your images and you have stochastic binary hidden variables.",
            "You can think of them as feature detectors, so feature detectors you know that can tell us something about maybe a semantic representation of documents or some kind of little high level features that you see in the data.",
            "Right, you can specify the energy of this configuration and the simplest way of specifying the energy is just using.",
            "Here is just a linear combination here.",
            "What this term is effectively doing is it's basically saying well, what's the correlation structure between each pixel?",
            "As well as each latent variable.",
            "OK, and the parameters Theta are just the parameters W as well as these bias terms, so these offset terms is a common."
        ],
        [
            "Thing to have now you can specify the probability of the joint distribution using a very standard definition, right?",
            "It's just.",
            "Each of the negative of the energy, or if you write it explicitly, you essentially have this term where you have pairwise.",
            "Sometimes these, called pairwise potentials, that again just modeling correlations between pixels and latent variables, and you have a normalizing constant, and these sometimes called.",
            "You know if you've heard terms like Markov random fields, Boltzmann machines log linear models this roughly.",
            "Correspond to the same, basically the same model, just different names."
        ],
        [
            "Now, why these models are called restricted right?",
            "The reason why they called restricted is because there is no connections between these hidden variables OK?",
            "And that has an advantage in disadvantage.",
            "The key advantage is that it turns out that computing the distribution over the latent variables can be done in closed form.",
            "Right, and this is a very useful thing to have.",
            "In particular, you know if I show you a new image, I can quickly tell you what features make up that image, or if I show you a document I can quickly tell you what topics make up that document, right?",
            "So it's easy to compute and in the same way the conditional probability of observing the image is again has a closed form solution."
        ],
        [
            "Right, so if you apply this model to handwritten characters, again, this is the kind of structure that it learns very closely related to sparse coding model, right?",
            "The one that we've seen it sort of detects little edges, and then you can say, well, if I show you a new image.",
            "Well, this new image can be written as a combination of these little edges, and it turns out to be useful thing to have, because you can represent a lot of different images just using a subset of these basis.",
            "So it's kind of like the natural is very simple.",
            "Basically tries to find basis decomposition of these images.",
            "Most hidden variables are going to be off and these numbers are given like these numbers are given by the conditional probability of a particular feature being on.",
            "And obviously you can see for this image you know observing a feature like this will have a very very small probability, right?",
            "But unlike in sparse coding model here, if I show you this new image, I can quickly tell you almost instantly tell you what are the features that make up this image, so I can do recognition very quickly.",
            "And then you can represent again this new image in terms of its conditional conditional."
        ],
        [
            "In terms of learning, you know it's the same story as in any type of graphical model.",
            "Give me a set of ID images, right then it turns out that if I want to look at the log likelihood objective, I can try to maximize the joint probability of observing all of these images.",
            "There is a regularization term.",
            "Typically people use could be a one."
        ],
        [
            "Be able to write and then there is a little bit of math behind these models.",
            "Not too difficult, it's just a little bit of algebra here, but again, you have the difference between these two expectations, right?",
            "You know?"
        ],
        [
            "The first expectation is easy to compute.",
            "And it turns out that's easy to compute.",
            "The reason why it's easy to compute is because when I look at the conditional probability that conditional probability is easy to compute and that has to do with the particular structure, the bipartite structure of this model.",
            "This term is difficult to compute 'cause you have to enumerate over all possible images, right?",
            "Again, that space is space is exponential, so it's difficult to compute, and one of the most dominant approaches to compute these expectations.",
            "Is is is using Markov chain Monte Carlo methods, right?",
            "And it turns out you know these MCMC methods are fairly easy to use in these models.",
            "In fact it's for these kind of model.",
            "Takes really 5 lines of MATLAB code, 5 lines of our code to actually code it up.",
            "Um?"
        ],
        [
            "You know, in terms of computing these expectations, you can essentially do something that's called deep sampling, right?",
            "It's a very commonly used Markov chain Monte Carlo algorithm, and the way it works is that you start at some configuration or you start at the data point and then you just repeat.",
            "You compute the probability of observed data given the latent variables, and then you can compute the conditional probability of latent variables given.",
            "Given the reconstructed data, so just doing alternating Gibbs sampling, it's very easy to do.",
            "You compute the hidden features, sample the hidden features given the data, then reconstruct the data.",
            "Given the hidden features and you proceed.",
            "And that's exactly what I was showing you when you saw these samples generating generating different Sanskrit, generating different airplanes, that's exactly the procedure that I was running an.",
            "Again, it's very easy to code."
        ],
        [
            "If you looking at maximum likelihood learning for these models right, you need to estimate these expectation and this is essentially how it looks like, right?",
            "And then you know what the theory is telling you is that if you go all the way up to Infinity, you will get the true expectations.",
            "Right, the Markov chain will converge eventually, and you'll get the.",
            "Unbiased estimate of."
        ],
        [
            "Of these expectations, now in practice people actually use something that's called contrastive divergent, so you probably heard the term contrastive divergent.",
            "And what it essentially does is you start at the training data you update all the hidden units, all the hidden variables, you update all the visible variables again, so you reconstruct the data and then you update all the hidden variables again and then you just update the parameters by looking at the difference between these two expectations, which you can compute.",
            "So it's kind of an interesting algorithm.",
            "It basically says instead of running your Markov chain all the way up to Infinity, just running for one step.",
            "That's all it says.",
            "And it turns out to be a very useful learning algorithm.",
            "It's very quick, it's biased, but you can learn."
        ],
        [
            "Good features.",
            "Using this algorithm.",
            "Now you can extend the beauty of these models is that you can extend these models to model in all kinds of distributions, right?",
            "So for example, if I if I'm interested in modeling images and I can think of images as being as vectors in real valued space, then with a little bit of modification of my energy function on my probability here, right?",
            "Again you have pairwise you have pairwise terms.",
            "You have unary terms.",
            "It turns out that just this simple modification in the code you can actually learn meaningful features from images.",
            "And in Matlab code there is only one line change in math lab code for learning these models, going from binary to real value."
        ],
        [
            "And again, the conditional probability he is just going is going to be given by the product of Gaussians, right?",
            "But you can learn is you can learn these kinds of features.",
            "So given here, you're looking at 4 million unlabeled images, just images random images downloaded from the web.",
            "This is the kind of structure that the model is discovering, right?",
            "And you can see it's sort of.",
            "Again, it discovers these edges, but also discovers these interesting things like little colors, dependencies, and again when I show these things to neuro scientists, they get excited because it's sort of looks a little bit possible from the neuroscience standpoint."
        ],
        [
            "And the way you can think again is again, it just finds basis.",
            "Find slightly different basis for four images.",
            "So if I show you a new image again, I can decompose it, or I can write it as a linear combination of these bases and these bases.",
            "Again, a very useful to have because you can use these bases to recognize what's going on.",
            "Image is much better than just using pixels."
        ],
        [
            "Now, one way to think about these models, maybe I just point this out is if if you look at the marginal probability probability of the data, you can write it as this way right?",
            "It's the conditional probability times the pryan you summing over these latent variables.",
            "Now what happens is that you can essentially interpret these models is mixture models.",
            "So in this case you can interpret them as mixture of Gaussians and you're probably familiar with mixture of Gaussians models right.",
            "But the beauty of these models, you can interpret them as a mixture of exponential number of Gaussians right and the way to think about this is that.",
            "Let's say I have three latent variables.",
            "Each one takes value 01, right?",
            "So if you think about this representation that two to three possible configurations, right?",
            "There are eight possible configurations.",
            "So it turns out that I can write this model as a mixture of eight Gaussians, right?",
            "But where the mean, then the covariances are going to be shared across different subsets of hidden variables.",
            "So in particularly typically if I have 100 or thousand of these latent variables hidden variables, there are 2000 possible configurations, right?",
            "So you can think of it as just 2000 possible mixture components.",
            "So these models in general for a lot of different problems.",
            "These models work much better than mixture Gaussians."
        ],
        [
            "You can also apply these models to again modeling other kinds of data, so here you're applying it to modeling count data that's very useful for modeling things like web pages or text bag of words representation.",
            "So here you can think of, you know, let's say you have different words in a document and K is the vocabulary size, so you know in English that might be 70,000.",
            "So K might be 70,000 and each particular webpage might contain, let's say A 1000 words.",
            "So deal would be 1000 here.",
            "And again, in terms of defining these models that are slight, some changes, but roughly again you have these pairwise terms and you have these unary terms, so there's nothing difficult of extending these models.",
            "The conditional distribution here is going to be given by something called softmax distribution.",
            "Is the distribution of a different words that can appear in a document.",
            "As."
        ],
        [
            "Fairly straightforward and these are the kind of latent variables latent topics that the model is discovering, right?",
            "So again, if you applied to the Reuters data set, you know it sort of figures out that there should be things like Russians and then US, and then the computers and such.",
            "So what I'm showing it.",
            "And again, the way to think about this model is that every single web page or document is given by some linear combination of these topics.",
            "Sparse linear combination of these topics.",
            "So few topics get activated and that's how.",
            "The data is generated, so it's sort of finds so you know the interesting thing here is that when you apply these models to images, you find these edges.",
            "When you apply these models 2 words, it's sort of finds these interesting, meaningful topics."
        ],
        [
            "Right, this is what it does in terms of reconstructions, right?",
            "So this was trained on the Flickr data set and the second half of the tour.",
            "I'm going to show you more of this, so this is if I if I give the model chocolate and cake, I encode chocolate and cake and look at the distribution of the hidden variables and then reconstruct back the data.",
            "Reconstruct reconstruct things like cake, chocolate, sweet dessert, cupcake food, sugar cream, right?",
            "So obviously it captures some semantic similarity between different words.",
            "It's also interesting that this guy flour high.",
            "And then the Japanese sign, and then it's flower High Japan soccer.",
            "And I don't know what these mean, but Blossom Tokyo, right?",
            "So it just picks up, you know these kinds of you know, to some extent, multilingual things just by looking at correlations between different between different words.",
            "It's very useful for encoding text."
        ],
        [
            "The other thing is that you can apply these models exact same model.",
            "You can apply to modeling social networking data, right?",
            "So this was done on the Netflix data set where you have users and you have movies and you can think of just encoding each user preference or the rating pattern as V here.",
            "So you can think of these being multinomial random variables that effectively tell you whether the user liked the movie.",
            "I didn't like the movie.",
            "You know these ratings 125.",
            "And the interesting thing about this model is that if you look at the latent representations that the model is discovering, you sort of discovering these learn genre, right?",
            "So you know, horror movies get grouped together.",
            "It's interesting Michael Moore's movies get grouped together, so just by looking at the patterns of how people rate the movies, you can basically figure out that either people really like his movies or they really hate his movies.",
            "Actually, somebody pointed to me that this is not Michael Moore's movie.",
            "But it should be very close I guess.",
            "I'm curious myself.",
            "Maybe I should watch that movie.",
            "So again, exactly the same kind of model, slight modification and you can apply to.",
            "You know social Nets or Netflix or product recommendation type of problems.",
            "So a lot of these different you can model a lot of these different modalities and from machine learning perspective that's very exciting, right?",
            "Because you can apply in a lot of different domains."
        ],
        [
            "One of the beauty behind these models is that you could.",
            "It's easy to infer the states of the hidden variables, right?",
            "So if you show me a particular pattern for a user, I can quickly tell you what movies he's he's going to like, or if you show me an image, I can quickly tell you the distribution of other features or topics that you see in the data, right?",
            "And that's very important for information retrieval.",
            "For for classification type of problems right then, the way to think about these models is just to give you an intuition is that you can think of them as product of experts.",
            "Sometimes people call these models product of experts.",
            "And the way to think about these models is that if I look at the marginal distribution over the data, it can be written as a product of these terms in.",
            "Sometimes people call them as products, right?",
            "So unlike traditional mixture models, right?",
            "You have product of bunch of things right?",
            "And maybe I can give you the intuition what that really means.",
            "Let's say I have these different topics that the model is discovering, right?",
            "If I activate topics like government corruption and oil, right?",
            "You have put in, will have very high probability under these three topics, right?",
            "If I'm going to tell you that this document is about government corruption and oil.",
            "Putin should go up right?",
            "This is unlike traditional mixture models right in the mixture model.",
            "You pick a topic and then you generate a word like traditional topic models like latent garishly allocation.",
            "If you've heard about these, these kinds of models so here you can make the distributions very precise because you're taking three different distributions, you multiplying them together and as soon as you take distributions and you look at the intersection, you can be very precise about what you're going to see in a document or what you're going to see in the data, and that's precisely what's going on with the.",
            "These models is a product based model, not a mixture based model."
        ],
        [
            "OK, and again, if you look at trying to do recognition or trying to do in this case information retrieval, you can do much better than traditional topic or mixture based models.",
            "If you're looking at speech, the same story holds, right?",
            "You know you can.",
            "This was done by group at Stanford, where if you look at the first layer basis you sort of discovering that kind of structure.",
            "And the remarkable thing is that what turns out is that if you, if you apply this."
        ],
        [
            "Model then it essentially discovers four names at the first level, right?",
            "So you know.",
            "This is sort of has a correspondence to a particular phoneme.",
            "Oy, it just has a correspondence between particular funding L. So here what I'm showing you.",
            "This is what the fanime the actual phoneme is, and this is what the model is learning.",
            "So by basically applying.",
            "Pretty much the same model.",
            "The model can basically discover phonemes which we know is a useful thing for doing speech recognition, right?",
            "And it sort of does it on its own, and you can see that there is a correspondence that the model is discovering, and I think that in you know this is not a speech community, but if you go to this speech conferences, you'll see a lot of these models deep learning kind of models basically making a huge impact just because you basically learning what are the right representations to extract from noisy signals.",
            "All noisy speech signals."
        ],
        [
            "Right, and maybe I can just point.",
            "Again, the difference between the two models is that you know you probably have seen nearest neighbors, or clustering or sort of local density estimators and the idea behind these models is that you essentially partitioning the space like clustering based models and mixture based models.",
            "You partitioning this space you finding these local regions an you finding parameters for each local region.",
            "Right, so number of regions is linear with the number of parameters.",
            "But if you look at sort of distributed type of models like restricted, both machines, factor models, PCA, sparse coding, deep models, they all sort of do something slightly different.",
            "And the idea is the following.",
            "Imagine I have a 2 dimensional data.",
            "The first hidden variable can partition the data into two of two planes, right?",
            "C0C1 if I introduce another hidden variable, I can partition the place again into.",
            "You know C21C20 right notice with two things I have four partitions.",
            "If I introduce another hit."
        ],
        [
            "Variable I can partition the plane as well.",
            "Right, so the interesting thing about these model."
        ],
        [
            "Is that each parameter, each hidden variable effects many regions, not just local regions, and the number of regions grows, roughly speaking exponential in number of parameters, right?",
            "So this is where you know.",
            "These models shine because they to some extent able to deal a little bit better with the curse of dimensionality.",
            "Write something that local models cannot deal, so that's the difference between between."
        ],
        [
            "Two models, so if you look at a lot of different domains like natural images, text, collaborative filtering, video applications, motion capture and I'm going to show you some of that more speech perception.",
            "The beauty of these models and what makes it exciting is that you have the same learning algorithm, but you have multiple domains, multiple input domains, right?",
            "And it does find interesting structure in every single domain.",
            "But obviously there are limitations, right?",
            "And there are limitations to the types of structures that can be extracted by just basically single layer of these nonlinear features, right?",
            "And as we've seen, you sort of able to extract things like little edges, right?",
            "Or if you're looking at words, you can extract little correlations between words or with speech you can extract little phonemes, just little parts of speech signal, right?",
            "And obviously there is a need to go beyond that, right?",
            "So people knew about this before, but.",
            "Up until five years ago, we didn't really have good learning algorithms for being able to learn multiple levels of representation, so a lot of focus has been focusing on just building these single layer models."
        ],
        [
            "OK.",
            "So now let me introduce you to the deep belief networks, and then we're going to take a half an hour break.",
            "Um?",
            "I'm just curious whether.",
            "Whether we're going to run out of coffee or not.",
            "Anyways, let me let me just speak for another 1520 minutes and then and then we're going to take a break.",
            "OK, so this is the exciting part.",
            "Deep belief networks."
        ],
        [
            "Right, so these are probabilistic generative models.",
            "Right, you have multiple layers of nonlinear representations and the beauty of these models is that there is a fast greedy layer wise pretraining algorithm and that pre training algorithm was discovered back in 2006 and basically opened up the space of deep learning, right?",
            "The interesting thing about this algorithm is that inferring the distribution of the latent variables in first is easy in these models and that was a breakthrough right?",
            "Because all of a sudden if I show you a new image, I can quickly tell you what a high level representations that make up this image.",
            "And you can do it very quickly.",
            "This is something that people couldn't couldn't do before.",
            "In particular, if you think about using directed model Bayesian networks and you have multiple layers, then you have to run complicated MCMC inference and it becomes fairly slow.",
            "And the idea?"
        ],
        [
            "Behind these models is that well if you look at restricted Boltzmann machines right, they capture sort of these little edges with these little primitive features.",
            "If we."
        ],
        [
            "So high up, right?",
            "And we're trying to look at some high level features.",
            "The hope is that we can discover things like combination Avengers or maybe part based representation that we see in images, right?",
            "And you're trying to basically look at combined simple features from into more complex ones, right?",
            "And the idea behind these models?",
            "Again, you can think of this latent variable is essentially modeling correlations between the data.",
            "But this latent variables essentially modeling correlation between these features, right?",
            "The hope is that.",
            "As you go up in the hierarchy, you can Start learning high and higher level representations of of the data."
        ],
        [
            "Right, So what is a deep belief network?",
            "You have hidden layers, so in this case you have H1H2H3.",
            "Those are your latent variables hidden variables.",
            "And then you have something that's called a restricted Boltzmann machine that sits at the top and something that's called a sigmoid belief network, which is a directed graphical model that basically generates the observed data.",
            "So it has this very strange kind of structure to it, right?",
            "It's a hybrid graphical model."
        ],
        [
            "But it was designed by construction and really comes out of a greedy layer wise pretraining of these models.",
            "I can write down the joint probability of the data right so I can right now in the joint probability we could say well is the probability of the top two layers followed by the conditional probability of H1 given H2 followed by the conditional probability of observed data given H1.",
            "Right, sigmoid belief Nets these kinds of models.",
            "Again, these are directed graphical model.",
            "These Bayes Nets they've been around for quite some time.",
            "So this isn't restricted.",
            "Boltzmann machine and the sigmoid belief Nets are just given by product of logistic functions, so there's nothing difficult about these models right now."
        ],
        [
            "In terms of the generative process, the way you generate the data is you can generate the top level representations using restricted Boltzmann machines, just alternating Gibbs sampling and then you can generate all the way down.",
            "In terms of inference or trying to infer the distribution of the latent variables, there's sort of an inference process that goes bottom up.",
            "Right, so given the data, there is an inference algorithm that basically tells you what the distribution of the latent variables is an it essentially becomes a neural network for this kind of model.",
            "Which is fast and very easy to do."
        ],
        [
            "Use.",
            "In terms of learning these models.",
            "The way you can design the learning algorithm and that was basically the breakthrough that happened in 2006 because people in particular Jeff Hinton figured out how you can gradually train these models right and the way you can do it is the following."
        ],
        [
            "First, you train a restricted Boltzmann machine.",
            "Then you use these approximate inference or the conditional probability as the data for training the next level Boltzmann machine.",
            "So your training one Boston machine, this machine, you infer the distribution of a hidden value chain is second bowl smush."
        ],
        [
            "You know when you proceed to training multiple layers, right?",
            "The question was that, why does it make sense right?",
            "This is just a hack.",
            "It turns out."
        ],
        [
            "You know there is a sort of.",
            "It.",
            "Turns out that when you do this, when you train these multiple layers greedily, what you effectively doing is you improving something that's called variational lower bound.",
            "I'm not going to get into too much detail about variational inference, but variational inference has been around in machine learning for quite sometime in the last 20 years, and the beauty of this algorithm is that that was the first mathematical proof that basically told us that as you stack multiple layers, you improving the variational bound on the likelihood objective.",
            "You're not improving the likelihood objective itself, but you improving the bound on the likelihood objective, right?",
            "So it's a meaningful thing to do and then."
        ],
        [
            "Cited a lot of people, particular people who were working variational inference and in machine learning.",
            "And the way you can think about the variational inference is you know.",
            "For any approximating distribution Q you tell me what that Q distribution should be.",
            "I can write down a low bound on the low probability of data, right?",
            "So for those of you who are familiar with expectation maximization algorithm, right, roughly speaking, you're doing something along those lines, except for.",
            "This is something that's called expected complete data log likelihood, and this is the entropy functional will get to this at the second half of the tutorial fairly quickly, but the idea is that you can show mathematically."
        ],
        [
            "That as you increase the number of layers and you start training the second layer.",
            "Then by training the second layer, you essentially pushing the variational lower bound up.",
            "Right, and by pushing the variational lower bound, the hope is that the likelihood function will improve as well.",
            "You know it's a little bit mathematically, it's a little bit nontrivial to explain it in a few minutes, but just I wanted to get the idea that there is an objective function of well defined objective function that sort of says that as you increase the number of layers, you're increasing their subjective function that relates to the maximum likelihood, and that was the first proof that basically inspired people to start looking into these models and start constructing these multiple levels of representations in greedy fashion."
        ],
        [
            "You can also do supervised learning with deep belief Nets.",
            "In particular, you can attach the label here and model the joint probability distribution.",
            "You can train these models greedily, and then you can maximize the conditional probability of the label given the data right?",
            "And that's a lot of sometimes people do that, in particular, for you know if you're interested in classification type of."
        ],
        [
            "In terms of sampling from this model, again, you can just run a Gibbs chain by simulating at the top two layers and then generating generating the data."
        ],
        [
            "In terms of the features that these models are learning.",
            "If you look at the first layer features right, this is kind of like edges that the model is discovering.",
            "This is done on digital data set, handwritten digit data set.",
            "If you look at the second layer features, you'll start seeing something that looks like high level parts of.",
            "Parts of."
        ],
        [
            "Images, you know, here's another way of looking at this model.",
            "If you train this model, images of faces right at the lower levels you discovering these little edges edge structure at the mid level representation you start discovering different parts of faces so you can see like their eyes and parts of noses at the high level third level you start discovering parts of faces.",
            "Right, so you can basically find in groups of parts and it turns out to be very useful for doing face recognition, for example.",
            "Right?"
        ],
        [
            "If you're looking at different.",
            "You know different like cars, faces, elephants and chairs.",
            "You can sort of see what happens in these models, right?",
            "So the first layer is shared at the second level, you start capturing more part based representation at the high levels you actually start capturing the objects themselves and that was."
        ],
        [
            "Have an exciting development.",
            "You know if you group everything together then you can see that you know at the low level you discovering edges at the mid level you start capturing part specific things and at the high levels you start capturing groups of parts right?",
            "And that's sort of representation that evolves as you learning these models and the beauty of these models.",
            "Again, you're just learning everything based on pixels based on the data itself, without designing what are the right features to use without designing hand designing that you should be detecting eyes and nose because that's what faces look like, right?",
            "You can just discover all of these levels based on the data in unsupervised fashion, so nobody is telling me that faces should contain eyes and nose.",
            "Is the model just figures out that that's the regularity is that you see in faces?"
        ],
        [
            "You can also do it for classifying digits right?",
            "If you pre training these models greedily and then you unrolling them into.",
            "Network then you're getting.",
            "Much better discrimination and it excited.",
            "A lot of people because these kinds of models can use unsupervised learning, right?",
            "And it sort of improves generalization because you're trying to model the distribution over the data itself.",
            "That turns out to be useful."
        ],
        [
            "Here is 1 particular example.",
            "Is it very interesting example?",
            "Imagine that I'm trying to predict the orientation of the face with a very simple task, right?",
            "So the training data contains multiple phases as well as the degree of orientation of the phase right at the test I'm I'm going to give you new people and the goal is to predict the orientation of the face for new people.",
            "Right, you can use something that's called Gaussian process, which is a fairly sophisticated regression model.",
            "Then you can predict basically based up to about 16 degrees."
        ],
        [
            "But now if you using deep belief Nets and you extracting you learning something about how faces should look like right and some dependencies, you know there should be eyes, they should be nose and such.",
            "You building the stack of these restrictive bolts machines.",
            "So extracting these features, and whenever you extracting features you not telling them all that you're going to be using it for predicting the orientation of the faces you just trying to figure out what are the regularity as you see in the data.",
            "Then it turns out that if you using.",
            "These latent variables latent features an you construct a Gaussian process on top of them.",
            "You can basically go all the way down to 6 degrees, so you can predict the orientation of a new phase up to 6 degrees versus 16 degrees.",
            "So obviously it's a very useful.",
            "Thing to have.",
            "You can also design these something.",
            "It's called deep autoencoders.",
            "And again this is the way of encoding the data through many, many layers of nonlinear processing and get some representation of the data."
        ],
        [
            "And this is you can train a stack of these restricted both machines.",
            "You can take the face.",
            "You can encode down to 30 dimensional space and then you can reconstruct the reconstructed back.",
            "And you can also update the parameters of this entire model using backpropagation algorithm, which is a very straightforward thing to do, and you can think of this as an."
        ],
        [
            "Pulling in generalization of PCA right?",
            "And this is indeed what's happening.",
            "This is the real data.",
            "This is the test images.",
            "This is what the auto encoder does.",
            "And this is what PCA does.",
            "So PCA notice doesn't capture.",
            "You know very specific.",
            "Like here, a lot of PCA things look like people like Snowman's, right?",
            "Where is the auto encoder is able to capture very precise?",
            "A little bit more crisp, representation of faces and again you can think of this is just an only extension of PCA and it's obviously the data is not linear here, so you can do."
        ],
        [
            "Much, much better.",
            "In terms of information retrieval, this is exactly what's happening.",
            "When I was showing you this image of the Reuters data set, this is again you can take bag of words representation encoded in the two dimensional space and then reconstruct it back.",
            "So for people who are working in information retrieval communities.",
            "Models like latent semantic analysis, PCA, version of it is just a linear version, so you can compare linear and nonlinear version and you can see that nonlinear version captures much more irregularities.",
            "You see?"
        ],
        [
            "The data.",
            "Anne."
        ],
        [
            "In terms of, you know, in terms of performance, these models do quite well.",
            "One other beautiful idea that came out of these models is something that's called semantic hashing, right?",
            "And that really is sort of.",
            "A way of doing information retrieval very quickly.",
            "The idea is that why don't we learn to map documents or images?",
            "To the binary space, this semantic binary space and why semantic binary space is because searching in binary space you can do much more efficiently.",
            "The search can be done much more efficiently, right?",
            "In fact, if you take a document and you map it to the binary space, you can map it to a particular address.",
            "Or you can create this sort of the address space, so documents that lie around in the similar address spaces are similar semantically.",
            "Right, so you can basically do retrieval without doing any kind of search.",
            "You just look up at the memory address, look at the memory address around that particular point and retrieve those documents of those images and that's."
        ],
        [
            "Inspired.",
            "Lot of people looking at these kinds of models, these PBM's stack of restricted Boltzmann machines, deep belief networks for image retrieval, right?",
            "And you can do it very very quickly by basically giving an image you achieve similar images based on the binary representation of that image.",
            "So now there's a lot of papers that you see in speech recognition in computer vision communities where you're trying to find what the binary representation should be."
        ],
        [
            "You can also do things like learning similarity metric, and after that we'll probably just going to take a break.",
            "Otherwise I'm worried that we're going to run out of coffee.",
            "So here the idea is that if you have two images, you can map them down to some semantic space such that you maximizing agreement between those images.",
            "So you telling the model, I think that this two and these two should be mapped to the same point in the latent space because they mean the same thing.",
            "Right, and it turns out that this is if you do that this is the representation that you can find.",
            "These are test images, map to that space and we'll see some of that going forward where one of the things could be an image.",
            "The other thing could be text.",
            "The other thing could be speech signal and you essentially mapping them to the same.",
            "The same semantic space."
        ],
        [
            "You know and compare it to something like PCA or something.",
            "A little cleaner discriminant analysis.",
            "You can learn much, much better Rep."
        ],
        [
            "Stations here the beauty of these kinds of representation is that you can also, if you look at some of the learn features right.",
            "If I change one of these features, one of these learn features, then three turns into five, which essentially means that you're learning some codes that are class specific, right?",
            "You basically say that this latent variable is very sensitive to the class boundary.",
            "So if I change it, threes will turn into 5.",
            "Some other latent variables modeling thickness of the ink.",
            "Right, so this three turns into this Lee as a wiggle, one of the Adelaide variables.",
            "So it has this beautiful property that you can start separating various degrees of variations that you see in the data, because some of them are more sensitive to classes and those are the ones you want to use for recognizing digits.",
            "Some of them are more sensitive to the thickness of the angle, how people write particular threads, and those you probably want to take away.",
            "If you want to do recognition."
        ],
        [
            "And let me just quickly point out, you know, there's been also some exciting developments in medical imaging for people who are working in this area in particular, trying to recognize different tumor cells.",
            "This was done by people at Berkeley, Berkeley National Lab."
        ],
        [
            "I think the idea is exactly the same.",
            "You basically constructing you're trying to get first level representation as well as second level representation.",
            "For finding these little."
        ],
        [
            "I guess you basically trying to recognize each one of those things.",
            "And you know, it's very new in the biomedical community or medical imaging community.",
            "These kinds of."
        ],
        [
            "Policy is a lot of opportunities there."
        ],
        [
            "And the idea is again you have a reconstruction error, so you're trying to basically reconstruct the data as much as possible at the same time you pulling."
        ],
        [
            "Pulling different units together and it turns out that you know the beauty of these kinds of models that they've shown that.",
            "That in particular, this is Marvin's group at Berkeley National Lab.",
            "They've shown that if you try to design features manually versus learning the features, you can do much, much better, right?",
            "So this is.",
            "On a particular, I guess this is just the classification accuracies, right?",
            "So it's sort of a lot of excitement happening in the biomedical communities as well.",
            "OK, let's take.",
            "Let's take a break."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Russ.",
                    "label": 0
                },
                {
                    "sent": "I'm with the Department of Computer Science and Department of Statistics at University of Toronto so and by the way, as I go through the tutorial, please feel free to ask questions.",
                    "label": 1
                },
                {
                    "sent": "You know this is going to be, you know, partly informal, what I'm hoping to cover is that I'm hoping to cover a lot of different applications that we've seen.",
                    "label": 0
                },
                {
                    "sent": "Some of the successful applications in deep learning, but also give you a little bit of theoretical foundations behind these algorithms, right?",
                    "label": 0
                },
                {
                    "sent": "And particularly focus on.",
                    "label": 0
                },
                {
                    "sent": "Graphical models and such, so hopefully what you'll get out of this tutorial is just a little bit of math behind these algorithms as well as different applications that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By using OK, let me just start by saying that you know my area of research is machine learning and it's a very exciting area and I know a lot of people here at KDD.",
                    "label": 0
                },
                {
                    "sent": "Also experts in machine learning, but it's kind of very dynamic field, right?",
                    "label": 0
                },
                {
                    "sent": "If you look at different areas like computer vision, natural language processing, information retrieval, speech recognition, robotics, computational biology, cognitive science is right.",
                    "label": 0
                },
                {
                    "sent": "A lot of these fields are using.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Machine learning, but what's exciting is that you know in the last few years if you look at the space of data that we have right, you can easily get access to images and videos or text to speech.",
                    "label": 0
                },
                {
                    "sent": "If you look at various product recommendation companies like Netflix and Amazon, or if you look at social network data or even if you look at scientific data, right?",
                    "label": 0
                },
                {
                    "sent": "That's a lot of data.",
                    "label": 1
                },
                {
                    "sent": "But that would say that most of the data that we see today is unlabeled, right?",
                    "label": 0
                },
                {
                    "sent": "It's basically that means that I can get millions of images of the web, but I might not have labels for those images, so it's precise description as to what's going on in those images.",
                    "label": 0
                },
                {
                    "sent": "Right, so one of the things that we try to do in particularly deep learning community is is trying to develop models that can deal with unlabeled data or models that can discover structure or discover some statistical dependencies from the data and how that can be done in unsupervised without labels or semi supervised where you have partial labeled data.",
                    "label": 1
                },
                {
                    "sent": "And also it's interesting you know in machine learning we sort of obsessed with developing algorithms that can work across multiple application domains.",
                    "label": 0
                },
                {
                    "sent": "And I'll show you some of some of the examples of that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And would I would argue, is that you know building models that can do these things is 1 particular framework of doing that is building these deep hierarchical models and deep learning models models that can support inferences as well as discover structure at multiple levels of representation, right?",
                    "label": 1
                },
                {
                    "sent": "And I'll make it more precise as I go through the tutorial of what I mean by that.",
                    "label": 0
                },
                {
                    "sent": "But let me show you before I start going into, you know the definition of the models and you know go through the tutorial.",
                    "label": 0
                },
                {
                    "sent": "Let me just show you some of the exam.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Course, right?",
                    "label": 0
                },
                {
                    "sent": "This is 1 particular model.",
                    "label": 0
                },
                {
                    "sent": "It's called the Boltzmann machine model, and what you're trying to do here is you basically trying to model images of these.",
                    "label": 0
                },
                {
                    "sent": "You're trying to model distribution over those images, right?",
                    "label": 0
                },
                {
                    "sent": "So you're trying to ask.",
                    "label": 0
                },
                {
                    "sent": "The model cannot recognize airplanes or cars or trucks and such, and because this is a generative model, what you can do is you can simulate from the model or generate data from the model.",
                    "label": 0
                },
                {
                    "sent": "So if I'm asking the model to sort of.",
                    "label": 0
                },
                {
                    "sent": "Tell me what it believes.",
                    "label": 0
                },
                {
                    "sent": "Images of airplanes should look like.",
                    "label": 0
                },
                {
                    "sent": "This is the model generating images of airplanes.",
                    "label": 0
                },
                {
                    "sent": "In the flickering that you see at the top.",
                    "label": 0
                },
                {
                    "sent": "These are the states of the latent variables.",
                    "label": 1
                },
                {
                    "sent": "So hidden variables, right?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of like discovering what airplane should look like so you can see that it goes from one airplane to another plane and.",
                    "label": 0
                },
                {
                    "sent": "Not argue that sort of has some understanding of what airplanes should look like now.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing about this model is that there is no image specific prior sitting in these models.",
                    "label": 0
                },
                {
                    "sent": "The model just looks at 96 by 96 images and figures out what the airplane should look like if you.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at other kinds of datasets, so this is an interesting data set.",
                    "label": 0
                },
                {
                    "sent": "This data set was collected at MIT.",
                    "label": 0
                },
                {
                    "sent": "You have about 25,000 characters from 50 different alphabets, and if you look at these characters you have Arabic.",
                    "label": 1
                },
                {
                    "sent": "You have cereal.",
                    "label": 0
                },
                {
                    "sent": "If you have Sanskrit.",
                    "label": 0
                },
                {
                    "sent": "Right, and the model has about 3000 latent or hidden variables over 2 million parameters, and then you can ask the model to see what images of Sanskrit should look like right?",
                    "label": 1
                },
                {
                    "sent": "And this is the model simulating what it believes images of Sanskrit should look like, and I typically ask the audience to see do these look like Sanskrit.",
                    "label": 0
                },
                {
                    "sent": "Somewhat.",
                    "label": 0
                },
                {
                    "sent": "Reasonable.",
                    "label": 0
                },
                {
                    "sent": "I see, right?",
                    "label": 0
                },
                {
                    "sent": "So Yep, I have no idea how it's supposed to look like, but this is what the most believes.",
                    "label": 0
                },
                {
                    "sent": "Sanskrit images should look like, and this is useful for doing things like recognizing characters right or doing character recognition.",
                    "label": 0
                },
                {
                    "sent": "You can also do it.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing things like pattern completion, right?",
                    "label": 0
                },
                {
                    "sent": "So for example, if I give you half of the input and I ask the model to simulate the remaining half right, this is what the model does, and again there is no image specific prime not telling the model that you know they should be strokes, they should be continuous.",
                    "label": 0
                },
                {
                    "sent": "Lines right, the model just looks at 30,000 different characters and just figures out what characters should look like.",
                    "label": 0
                },
                {
                    "sent": "So why why building these models is is?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is challenging well if you look at the simplest possible model right, this is just a 28 by 28 image.",
                    "label": 0
                },
                {
                    "sent": "It's binary image, right?",
                    "label": 0
                },
                {
                    "sent": "But if you look at the space of all possible characters right, the space of all possible images you could generate?",
                    "label": 1
                },
                {
                    "sent": "Well, let's do to the 28 by 28 or two to the 784 possible configurations.",
                    "label": 0
                },
                {
                    "sent": "Right, so obviously doing it via the brute force method is going to be challenging, so we have to rely on some approximate algorithms and we'll see some of those algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this tutorial you can also the interesting thing about these models that you can also apply to other kinds of data.",
                    "label": 0
                },
                {
                    "sent": "So if you're looking at bag of words representation, this is just taking.",
                    "label": 1
                },
                {
                    "sent": "Documents and representing of them as word counts.",
                    "label": 0
                },
                {
                    "sent": "If you take Reuters data set has about 800,000 web pages and you say, well, how can I extract structure from that data?",
                    "label": 0
                },
                {
                    "sent": "If I look at the high level representations, this is what the model is discovering, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm just basically there are no labels.",
                    "label": 0
                },
                {
                    "sent": "I'm just feeding the model bunch of different web pages and I'm asking it to.",
                    "label": 0
                },
                {
                    "sent": "Discover the latent structure and this is what it's discovering, and it's interesting to see that you know sort of finds different groups of of these documents of these web pages.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing here is that it sort of thinks that European Community economic stuff is next to disasters and accidents, right?",
                    "label": 1
                },
                {
                    "sent": "So there's a lot of sort of finds a lot of correlations between between these two topics.",
                    "label": 0
                },
                {
                    "sent": "I guess today those two lines would be even closer together, right?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of.",
                    "label": 0
                },
                {
                    "sent": "One of the also exciting sort of developments recent developments in the deep learning communities.",
                    "label": 0
                },
                {
                    "sent": "Convolutional models you probably heard about convolutional neural net style models, right?",
                    "label": 0
                },
                {
                    "sent": "And this is kind of a neural net model, something that we'll talk about in this tutorial where you take an image.",
                    "label": 0
                },
                {
                    "sent": "You design these sort of called C1 layers and then you're doing pulling you doing convolutions and pulling.",
                    "label": 0
                },
                {
                    "sent": "It's fairly, you know, it's fairly straightforward model, but the Chiefs really remarkable results.",
                    "label": 0
                },
                {
                    "sent": "Let me just.",
                    "label": 0
                },
                {
                    "sent": "Show you a little bit of demos and it's always difficult to do life demos 'cause you know what.",
                    "label": 0
                },
                {
                    "sent": "If they don't work right.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do?",
                    "label": 0
                },
                {
                    "sent": "Is I'm going to show you so this is a website that we've been building and you know, for example, you can upload an image like this one.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can upload images like this one and the system will essentially tell you that it's 73%.",
                    "label": 0
                },
                {
                    "sent": "It's a chimpanzee.",
                    "label": 0
                },
                {
                    "sent": "Nine percent is hippo, so it's confusing it with heap or gorilla or such.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of an image recognition system.",
                    "label": 0
                },
                {
                    "sent": "Again, it's one of the deep learning systems, and you know if you want to test the system, you're more than welcome to test it.",
                    "label": 0
                },
                {
                    "sent": "But maybe I can just ask somebody from the audience to just think about any object.",
                    "label": 0
                },
                {
                    "sent": "Anything?",
                    "label": 0
                },
                {
                    "sent": "What would you like to recognize?",
                    "label": 0
                },
                {
                    "sent": "Car OK. OK, let's pick a car.",
                    "label": 0
                },
                {
                    "sent": "This Google images.",
                    "label": 0
                },
                {
                    "sent": "So these are all sort of good looking cars, so these are easy cars.",
                    "label": 0
                },
                {
                    "sent": "How about this guy here?",
                    "label": 0
                },
                {
                    "sent": "Sexually right?",
                    "label": 0
                },
                {
                    "sent": "So we can go to here, specify the URL and just ask the model to classify right?",
                    "label": 0
                },
                {
                    "sent": "So let's see what it says.",
                    "label": 0
                },
                {
                    "sent": "Thinks it's a car wheel, thinks it's a low motor tractor harvester, amphibian vehicle.",
                    "label": 0
                },
                {
                    "sent": "Yeah, kind of looks like one of those, right?",
                    "label": 0
                },
                {
                    "sent": "But obviously if you know if you want to recognize sort of cars like you know, we can.",
                    "label": 0
                },
                {
                    "sent": "Obviously, how about this guy here?",
                    "label": 0
                },
                {
                    "sent": "Right, it's sort of like a car.",
                    "label": 0
                },
                {
                    "sent": "Upside down, well hopefully.",
                    "label": 0
                },
                {
                    "sent": "The model will recognize it, so it takes about for this systems, let's see what it says.",
                    "label": 0
                },
                {
                    "sent": "Sports card.",
                    "label": 0
                },
                {
                    "sent": "There you go.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "But so this is the system that can recognize 1000 different classes, and it's a pretty reasonable system, and I should also point out the code for training these systems and using them is online.",
                    "label": 0
                },
                {
                    "sent": "It's all public public domain.",
                    "label": 0
                },
                {
                    "sent": "You can interact with the system, you know, it can recognize different kinds of dogs, spiders, different animals, furniture, you know.",
                    "label": 0
                },
                {
                    "sent": "We're trying to extend the system going to 20,000 classes, so.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the other thing that I'm going to talk as we go through the tutorial or these data.",
                    "label": 0
                },
                {
                    "sent": "That's multi model.",
                    "label": 0
                },
                {
                    "sent": "So this is the case where you have for example images in text right?",
                    "label": 0
                },
                {
                    "sent": "So for example given an image like this one you can find different tags associated with images or given a particular word.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can do image retrieval right?",
                    "label": 0
                },
                {
                    "sent": "So we'll see some of that in the second part of part of the tutorial.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the interesting thing is that you know you can do fun things like not only recognizing objects, but also sort of recognizing descriptions of objects, right?",
                    "label": 0
                },
                {
                    "sent": "So like if you query the system is a fluffy sort of returns these images was a delicious determines these images, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk a little bit more about how we can work with words as well as images and sort of find semantic relationships between words and images.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, there's another thing that has been done by other other people, so this is 1 particular example of trying to recognize roads in satellite images.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and this is again using in deep learning system that essentially figures out roads and images and you can imagine that's very useful for people who are trying to update the Maps of.",
                    "label": 0
                },
                {
                    "sent": "Of the roads based on.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No satellite imaging OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to go through these topics in this tutorial, so I'm going to start with a little bit of introduction, sort of motivating deep loading and I'm going to talk about sparse coding in autoencoder models.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to give you a brief introduction into graphical models and restricted Boltzmann machines.",
                    "label": 1
                },
                {
                    "sent": "You know ways of trying to learn low level features as well as deep belief networks, and this is sort of trying to learn part based hierarchies and I'll give you a little bit of mathematical foundations behind behind these models.",
                    "label": 0
                },
                {
                    "sent": "And at the second part of the tutorial, after the break, we're going to look at a little bit more advanced models.",
                    "label": 0
                },
                {
                    "sent": "These would be Boltzmann machines, the Boltzmann machines, multimodal learning, as well as a little bit of language modeling.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned before, if you have any questions as a.",
                    "label": 0
                },
                {
                    "sent": "As I go through the tutorial, you know.",
                    "label": 0
                },
                {
                    "sent": "Feel free to ask me.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so one of the general ideas of deep learning is is trying to learn meaningful representations.",
                    "label": 0
                },
                {
                    "sent": "One of the main motivations behind deep learning is trying to learn representations or trying to learn useful features right?",
                    "label": 0
                },
                {
                    "sent": "If you look at a lot of traditional learning algorithms is you have, let's say, a data and you're applying some kind of learning algorithm to it.",
                    "label": 0
                },
                {
                    "sent": "But if you're using for example pixel representation of the data, right?",
                    "label": 0
                },
                {
                    "sent": "And you're trying to separate Segways from non Segways?",
                    "label": 0
                },
                {
                    "sent": "Well the pixel space looks.",
                    "label": 0
                },
                {
                    "sent": "Kind of ugly.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to do recognition based on pixels, right?",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you able to construct meaningful representations like getting object or parts of the objects like handle and will.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you're getting to this feature representation than in the feature space.",
                    "label": 1
                },
                {
                    "sent": "You can basically separate to be able to separate the two classes, right?",
                    "label": 0
                },
                {
                    "sent": "So how can we do?",
                    "label": 0
                },
                {
                    "sent": "How can we learn these repres?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stations right, so for example, how is computer vision down?",
                    "label": 1
                },
                {
                    "sent": "Computer perception is done.",
                    "label": 0
                },
                {
                    "sent": "Well, we have some data.",
                    "label": 0
                },
                {
                    "sent": "You find some features from the data and that's how it's been done in the last 20 years and you apply your favorite learning algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "Like support vector machines for example.",
                    "label": 0
                },
                {
                    "sent": "You know, for object detection you have an image.",
                    "label": 0
                },
                {
                    "sent": "You try to find some low level features and you get the recognition or for classification audio classification you can get some low level features and then you construct the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From it, and if you look at a space of possible features right in the way that computer vision was done is in the last 20 years is that people were trying to figure out what are the right representations to use based on images.",
                    "label": 0
                },
                {
                    "sent": "So if you look at different features, there is things like Hog sifts, text ONS and all kinds of stuff that people have been developing.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the audio features again this spectrograms MFC sees flux and again all kinds of different features to use.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But one question that you know we are interested in and general deep learning communities interested in is that how can we do feature learning?",
                    "label": 1
                },
                {
                    "sent": "Can we actually figure out what are the right features to use from this data?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that you know, I've shown you some of the examples of recognition.",
                    "label": 0
                },
                {
                    "sent": "It was actually remarkable breakthrough, because by using these models and learning the features themselves right, you cannot perform basically the state of the art recognition systems that have been used.",
                    "label": 0
                },
                {
                    "sent": "You know that the vision community has been using with speech recognition community have been using in the past 1015 years.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one particular model.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the oldest model for learning features is is sparse coding model, and it's been used a lot.",
                    "label": 0
                },
                {
                    "sent": "That's a very popular models.",
                    "label": 0
                },
                {
                    "sent": "One of the little building blocks of many deep learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "And the idea is the following.",
                    "label": 0
                },
                {
                    "sent": "You have a set of input data vectors, X12 exam and you want to learn a Dictionary of basis and you want a Dictionary of basis such that you can reconstruct the data based on these bases, right?",
                    "label": 1
                },
                {
                    "sent": "So it's a very simple formulation, but the key thing here is that what you'd like to do is you'd like these.",
                    "label": 0
                },
                {
                    "sent": "These coefficients to be sparse, So what is essentially saying is that well, each data vector is going to be represented as a sparse linear combination of basis.",
                    "label": 1
                },
                {
                    "sent": "So you can imagine that you have you know 10,000 basis, but you might want to say, well, only ten of these vectors should be used for reconstructing the data, right, that's?",
                    "label": 0
                },
                {
                    "sent": "And that's the idea.",
                    "label": 0
                },
                {
                    "sent": "And this belongs to the class of so called unsupervised learning algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "There is no label.",
                    "label": 0
                },
                {
                    "sent": "So just trying to find the right basis so that can reconstruct the data as much as you can.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, for natural images, if I take the natural images and I take little patches from those natural images, then this is how these bases would look like.",
                    "label": 1
                },
                {
                    "sent": "So you can see a lot of them look like little edges or a little Gabor like filters, and that seems like the right structure to extract from images, at least when you talk to.",
                    "label": 0
                },
                {
                    "sent": "Neuro scientists.",
                    "label": 0
                },
                {
                    "sent": "They believe that that's actually something that happens in the visual area.",
                    "label": 0
                },
                {
                    "sent": "One human brain.",
                    "label": 0
                },
                {
                    "sent": "If I give you a new example or a new Patch, then I can essentially.",
                    "label": 1
                },
                {
                    "sent": "Reconstructed as maybe as some of these three patches, so you can sort of.",
                    "label": 0
                },
                {
                    "sent": "See that you pick sub sparse number or few number of these bases in reconstructing example, and it turns out that these new representation that you get this sparse representation that you get is 1 particular way of doing feature representation.",
                    "label": 0
                },
                {
                    "sent": "That turns out to be much more useful to deal with than the pixels themselves, right?",
                    "label": 0
                },
                {
                    "sent": "So working in this basis, if you want, for example classify these images.",
                    "label": 0
                },
                {
                    "sent": "Turns out to be much more.",
                    "label": 0
                },
                {
                    "sent": "Has much better generalization.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Performance.",
                    "label": 0
                },
                {
                    "sent": "Right, so how would you do training of these models?",
                    "label": 0
                },
                {
                    "sent": "Well, you have N inputs and you'd like to learn the basis.",
                    "label": 0
                },
                {
                    "sent": "So you have two different terms.",
                    "label": 0
                },
                {
                    "sent": "One term is essentially saying, well, you'd want to reconstruct the data's without introducing any errors.",
                    "label": 0
                },
                {
                    "sent": "So you'd like to have a small reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you also have a sparsity penalty, you say?",
                    "label": 0
                },
                {
                    "sent": "Well, I don't want to use a lot of bases to reconstruct the data, I just want to use a few basis to reconstructed data so it's kind of like controls the complexity of the model, right?",
                    "label": 0
                },
                {
                    "sent": "And the optimization itself can be done fairly easily.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you know for a fixed if you fixing the basis by why up to 5K and you solving for the coefficients A, then it turns out to be standard lasso program problem, right?",
                    "label": 0
                },
                {
                    "sent": "So it's just squared loss plus L1 penalty.",
                    "label": 0
                },
                {
                    "sent": "Let's just let's Sue and that can be solved fairly efficiently using quadratic programming, right?",
                    "label": 0
                },
                {
                    "sent": "So that's standard machine learning 101.",
                    "label": 0
                },
                {
                    "sent": "And then when you fix the activations, you optimize for the dictionary basis and then again that's a convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It's a quadratic programming problem and you can solve that so you can sort of do these alternating minimization, fixing the basis, finding the activations, fixing the activations, refitting the new basis.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's a pretty straightforward and now at the test time.",
                    "label": 0
                },
                {
                    "sent": "If I give you a test image.",
                    "label": 0
                },
                {
                    "sent": "As well as the learned basis.",
                    "label": 0
                },
                {
                    "sent": "Then what you'd like to get is you'd like to get a sparse representation, or these coefficients for a given image, and then you can represent this knew test batch again as as a linear combination of the learn basis, right?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, if you apply this model is sort of like this was done back in 2006 when the deep learning sort of was just, uh, you know, with stages.",
                    "label": 0
                },
                {
                    "sent": "But for example, here's just one example.",
                    "label": 0
                },
                {
                    "sent": "If I give you an input image and I give you these four different bases and I scan my image using these bases, then this is kind of representations that I'm learning right, using four different bases.",
                    "label": 0
                },
                {
                    "sent": "And then you can take these representations.",
                    "label": 0
                },
                {
                    "sent": "You can use a support vector machine or any other classification algorithm, and then you can basically get fairly reasonable performance, right?",
                    "label": 0
                },
                {
                    "sent": "So you can see from the baseline and from PCA you can using just this simple sparse coding, a single layer model, you can improve recognition accuracy of.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of these images right now, the way to interpret sparse coding and we'll get into that more and more as we go through the tutorial is you can view it as.",
                    "label": 0
                },
                {
                    "sent": "Cora decoder model.",
                    "label": 0
                },
                {
                    "sent": "You have sparse features here.",
                    "label": 1
                },
                {
                    "sent": "Right, and then you reconstructing the data so there is an explicit linear decoding stage, right?",
                    "label": 1
                },
                {
                    "sent": "If I give you sparse features, I can reconstruct the data.",
                    "label": 0
                },
                {
                    "sent": "But there is also an encoding scheme given an input X, you'd like to get the representation of sparse representation of these coefficients, but the encoding itself is implicit, and it's very nonlinear, right?",
                    "label": 0
                },
                {
                    "sent": "It's implicit because you actually have to solve for these sparse coefficients.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, but now this can be generalized to a general models called encoder decoder, model, autoencoder model and.",
                    "label": 1
                },
                {
                    "sent": "The idea is that you get an input image, you encoding that image.",
                    "label": 1
                },
                {
                    "sent": "You're getting some feature representations and then you decoding it to get back the original input.",
                    "label": 0
                },
                {
                    "sent": "And the encoder is sometimes known as a feedforward or bottom up inference and the decoder is known as a feedback or generative or top down right?",
                    "label": 0
                },
                {
                    "sent": "So you can imagine going from features all the way back to the back to the image.",
                    "label": 0
                },
                {
                    "sent": "So when I was showing you these generative models like generating airplanes are generating characters.",
                    "label": 0
                },
                {
                    "sent": "The generation part was the decoder part.",
                    "label": 0
                },
                {
                    "sent": "Given some latent variables, some features you actually generate generate the data.",
                    "label": 0
                },
                {
                    "sent": "And they details what's going inside encoding, decoding actually matter.",
                    "label": 0
                },
                {
                    "sent": "It's been a lot of work across different people trying to figure out where the right.",
                    "label": 0
                },
                {
                    "sent": "Pieces that would should go into encoding decoder and one thing that you'd like to do is you.",
                    "label": 1
                },
                {
                    "sent": "Also, we need to put some constraints doing to essentially avoid learning identity like.",
                    "label": 0
                },
                {
                    "sent": "Obviously if your feature representation is the same as the input dimensionality of the feature space is the same as dimensionality of the input space, you could just.",
                    "label": 0
                },
                {
                    "sent": "Put the identity function right and you just say well don't do anything.",
                    "label": 0
                },
                {
                    "sent": "So obviously we need to put some constraints.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "So one particular encoder decoder that's being used a lot is trying to come up with binary representation, so binary representations of features and we'll see a lot of that and then non linearity that you put is the sigmoid function.",
                    "label": 0
                },
                {
                    "sent": "It's a fairly simple function, used a lot, but that way you're getting binary representations, and then the decoder is a linear decoder so you can reconstruct Becky original input.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So this is just one particular just formulation of how the encoding can be done with decoding can be done so this part is the encoder, you just take your data, you multiply by matrix W, you pass it through the sigmoid and the decoder is basically a linear reconstruction based on the latent features that you're learning.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also determine the parameters WND from the data.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By essentially trying to minimize the reconstruction error, and that's important because you know one question is how do you actually learn the right features, right?",
                    "label": 0
                },
                {
                    "sent": "Even in the autoencoder model you can look at the reconstruction by basically saying, well, how can you encode your data in such a way that when you can reconstruct it back you don't suffer any reconstruction, sorry.",
                    "label": 0
                },
                {
                    "sent": "Right, obviously if your reconstruction error is 0, then you have a perfect encoder, right?",
                    "label": 0
                },
                {
                    "sent": "And in particular, if the number of hidden layers or the number of hidden features here is much smaller than the number of input dimensions, and you have perfect reconstruction, then you have perfect compression of the data, right?",
                    "label": 0
                },
                {
                    "sent": "You can also view these kinds of models trying to compress the data.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the interesting thing about these models is that if the hidden layer as well as the output layer linear.",
                    "label": 0
                },
                {
                    "sent": "You're trying to minimize the reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "Then it turns out that you can recover PCA, right?",
                    "label": 0
                },
                {
                    "sent": "You can recover principle components analysis.",
                    "label": 0
                },
                {
                    "sent": "So if you look at these K hidden units, they will span the same space as the first gay principle components, so it's kind of interesting if you basically make this model be linear, then you're recovering PCA and many of you probably heard about PCA.",
                    "label": 1
                },
                {
                    "sent": "It's like one of the heaviest use tools for, you know, across many many different domains is data compression.",
                    "label": 0
                },
                {
                    "sent": "Algorithm so you can view autoencoders is nonlinear extension of PCA, which can be much more powerful than PCA.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If your data is binary, then obviously you can also as a decoder in a decoder stage you can you also use nonlinear decoding function, in this case sigmoid function, and you know this is very also successfully used models and it actually relates to something that's called restricted Boltzmann machines or relates to restricted both machines and graphical models.",
                    "label": 1
                },
                {
                    "sent": "OK, so one of the key components in a lot of these models is that you want to get sparse representations right.",
                    "label": 0
                },
                {
                    "sent": "That turns out to be a very useful thing, and empirically people found that you know if you want to get good features you want to put some kind of sparsity on on your latent representations, and here, if even if you're trying to discover binary features you'd like them to be sparse.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's exactly sort of the model that was done by NYU Group A few years ago.",
                    "label": 0
                },
                {
                    "sent": "We again you're trying to minimize you have a decoder function that's attempting to minimize this reconstruction error, so this is this is the reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "This is the penalty on these latent features.",
                    "label": 0
                },
                {
                    "sent": "You want these features to be sparse, so you put L1 penalty.",
                    "label": 0
                },
                {
                    "sent": "That's a common thing to do, and this is the encoding function, right?",
                    "label": 0
                },
                {
                    "sent": "It turns out that the encoding function is also very useful to have.",
                    "label": 0
                },
                {
                    "sent": "In particular test time, right?",
                    "label": 0
                },
                {
                    "sent": "It's very useful to have an explicit encoding function, because at the test time when I show you a new image or when I show you a new web page you'd like to get latent representations very quickly, right?",
                    "label": 0
                },
                {
                    "sent": "And an explicit decoding encoding function allows you to do that.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, one of the things that people realized you know five or six years ago is that, well, you can actually stack these things.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can learn some representations.",
                    "label": 0
                },
                {
                    "sent": "You know, first layer features, then you can repeat the operation.",
                    "label": 0
                },
                {
                    "sent": "You can say, well, can I actually take these latent features and encode them again and so forth and maybe at the top level, I'm going to have my classification model or whatever you want to use them for.",
                    "label": 0
                },
                {
                    "sent": "Now when you look at the stacked autoencoders, you might ask why people haven't thought so.",
                    "label": 1
                },
                {
                    "sent": "Autoencoders existed in the last 20 years, but only 5, four, five years ago.",
                    "label": 0
                },
                {
                    "sent": "People figured out that you can actually stack them.",
                    "label": 0
                },
                {
                    "sent": "The question is why people haven't thought of this thing before.",
                    "label": 0
                },
                {
                    "sent": "That's a natural thing why people haven't tried doing that.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that you know if you look at the stacking here, it's not obvious whether it's a hack or whether there is some deep mathematical insight behind these kinds of models, and I'm going to show you.",
                    "label": 0
                },
                {
                    "sent": "That stacking makes sense in terms of actually optimizing a certain kind of objective function, so it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not a hack.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is known as a greedy layer wise pretraining or greedy layer wise learning of.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the features.",
                    "label": 0
                },
                {
                    "sent": "Now at the desk time you can remove the decoder if you just interested in recognizing what's going on in images or speech signal or web pages, and Justin code, given an input X, you just go through multiple layers of representations and then you you can use it for for classifying things.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "You know you can use standard.",
                    "label": 0
                },
                {
                    "sent": "Encoder an that's sort of equivalent to neural network model, and you'll network architecture.",
                    "label": 1
                },
                {
                    "sent": "You can use convolutional, which is again has to do with parameter sharing and the parameters of the entire model can be trained or can be tuned using back propagation algorithm and the backdrop algorithm.",
                    "label": 0
                },
                {
                    "sent": "You know it's been.",
                    "label": 0
                },
                {
                    "sent": "It's been around for quite some time.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one question you might want to ask is there top down bottom up, can we actually design algorithms that maybe have some kind of feedback connection?",
                    "label": 0
                },
                {
                    "sent": "An is there a more rigorous mathematical formulation, right?",
                    "label": 1
                },
                {
                    "sent": "Obviously a lot of these things are driven by empirical success.",
                    "label": 0
                },
                {
                    "sent": "And yes, there is a lot of empirical success.",
                    "label": 0
                },
                {
                    "sent": "But then the question is, is there a little bit something more we can say about these models?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me give you a little bit of introduction to graphical model just so that we can get the notation right as we go through the rest of the tutorial.",
                    "label": 1
                },
                {
                    "sent": "How many of you know about graphical models?",
                    "label": 1
                },
                {
                    "sent": "OK, a lot so so I can go.",
                    "label": 0
                },
                {
                    "sent": "I can go fairly quickly there.",
                    "label": 0
                },
                {
                    "sent": "This is just primarily for set.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sing up for setting up the notation for us, right?",
                    "label": 0
                },
                {
                    "sent": "So it's interesting if you look at the history of deep learning.",
                    "label": 0
                },
                {
                    "sent": "I'd started with basically looking at these models from graphical model perspective.",
                    "label": 0
                },
                {
                    "sent": "Right now there are a lot of different algorithms, like sparse coding and autoencoders, but the initial models were inspired by graphical models and doing inference in learning in graphical models.",
                    "label": 0
                },
                {
                    "sent": "So graphical models.",
                    "label": 0
                },
                {
                    "sent": "It's a very powerful framework for representing dependencies structure between random variables, right?",
                    "label": 1
                },
                {
                    "sent": "Typically when we look at graphical models.",
                    "label": 1
                },
                {
                    "sent": "The graph itself will contain a set of nodes, verticies that will represent random variables and will have links that tell us something about dependencies between those random variables.",
                    "label": 1
                },
                {
                    "sent": "And the joint distribution of those random variables can be decomposed into the product of factors, right?",
                    "label": 0
                },
                {
                    "sent": "Each one of those factors is looking at a subset of these.",
                    "label": 0
                },
                {
                    "sent": "Subset of the nodes.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 1
                },
                {
                    "sent": "There are two types of models.",
                    "label": 0
                },
                {
                    "sent": "There are directed graphical models.",
                    "label": 0
                },
                {
                    "sent": "Something is called Bayesian networks and they've been around for quite some time, and they're also undirected graphical models, and these are known as Markov random fields of Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "In this tutorial, we'll be focusing a little bit more on undirected part.",
                    "label": 0
                },
                {
                    "sent": "Just because a lot of models, lot of these models come from undirected graphical models, but there are also hybrid graphical models and we'll see some of those in hybrid graphical models.",
                    "label": 1
                },
                {
                    "sent": "Models that combine both directed and undirected models together.",
                    "label": 0
                },
                {
                    "sent": "So for example, you know deep belief networks.",
                    "label": 0
                },
                {
                    "sent": "For those of you preferred about deep belief networks, these are actually hybrid graphical models.",
                    "label": 0
                },
                {
                    "sent": "They have both undirected and directed parts to it.",
                    "label": 0
                },
                {
                    "sent": "More like hierarchical deep models.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the idea behind directed graphical models, it's directed graphical models actually used a lot in the machine learning, and they're useful for expressing causal relationship between random variables.",
                    "label": 1
                },
                {
                    "sent": "Right, so for example in this picture here, right, the joint distribution that specifies this particular is consistent with this particular graph is you can say, well, my joint distribution of these random variables is just going to be written as a product of note given as a conditional distributions of nodes.",
                    "label": 0
                },
                {
                    "sent": "Given the parents of the nodes.",
                    "label": 0
                },
                {
                    "sent": "Right, so for example, in this case, if I look at the joint distribution of these seven random variables, then it factorizes precisely this way.",
                    "label": 0
                },
                {
                    "sent": "There is P of X1, P of X2 PX three.",
                    "label": 0
                },
                {
                    "sent": "If you look at X, 4X4 has parents X, 2X1, and X3, so that's the number that I have here, right?",
                    "label": 0
                },
                {
                    "sent": "And notice that the graphical model tells me something about how the distribution factorizes, right?",
                    "label": 1
                },
                {
                    "sent": "And this is known as directed acyclic graph with.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it, you know, graphical models in particular directed graphical models are very useful.",
                    "label": 1
                },
                {
                    "sent": "Been used a lot in.",
                    "label": 0
                },
                {
                    "sent": "A lot of different domains, but you know maybe one example is just to show you as you know how would you generate an image?",
                    "label": 0
                },
                {
                    "sent": "Well, you might have latent variables like object position, orientation, right, and then the all of these three random variables might have independent priors.",
                    "label": 0
                },
                {
                    "sent": "And then the way you generating the image is, you say, well, if I know the object identity, the position and orientation, then I'm going to have a likelihood function here that essentially tells me what's the probability of the image given these two variables, right?",
                    "label": 0
                },
                {
                    "sent": "So, given these three random variables, I can generate the image.",
                    "label": 0
                },
                {
                    "sent": "You know this is sort of a Canonical way of how people in the computer graphics with computer vision communities are generating images.",
                    "label": 0
                },
                {
                    "sent": "Now, inference in these models is a little bit tricky because what you'd like to do is you'd like to invert the errors, right?",
                    "label": 0
                },
                {
                    "sent": "You'd like to say.",
                    "label": 1
                },
                {
                    "sent": "Well, if you show me the image, can you tell me the object identity?",
                    "label": 0
                },
                {
                    "sent": "Can you tell me the position?",
                    "label": 0
                },
                {
                    "sent": "Can you tell me the orientation, right?",
                    "label": 0
                },
                {
                    "sent": "And to do that you can use something that's called Bayes rule.",
                    "label": 1
                },
                {
                    "sent": "So you inverting it.",
                    "label": 0
                },
                {
                    "sent": "But the problem here is this term known as a marginal likelihood, writes the probability of the image.",
                    "label": 0
                },
                {
                    "sent": "It's sometimes called normalizing constant and we'll see some of.",
                    "label": 0
                },
                {
                    "sent": "Ways of dealing with that, and that becomes fairly difficult thing to do, so a lot of these inference algorithms you can specify the generative part, but actually doing inference is difficult, right?",
                    "label": 0
                },
                {
                    "sent": "Obviously you'd like to show you test image and you'd like to tell me that that's a car and where is it in the image and such?",
                    "label": 0
                },
                {
                    "sent": "That's difficult.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This problem problem, Now Markov random fields, on the other hand, they don't have directed edges, right?",
                    "label": 1
                },
                {
                    "sent": "What you have instead is you have potential functions in these potential functions, essentially a mapping from the joint configurations of these random variables to some non negative real real numbers.",
                    "label": 1
                },
                {
                    "sent": "And the choice of these potential function is not really restricted to having some kind of probabilistic interpretation, right?",
                    "label": 1
                },
                {
                    "sent": "So the difference is that you know the way to think about these things is that if I show you an image right and I'm trying to look at two pixels in the image, it's not like one pixel causes another pixel right?",
                    "label": 0
                },
                {
                    "sent": "It's mostly just dependencies within two pixels to saying, well, they should probably be have the same value together because they are located nearby in the image, right?",
                    "label": 0
                },
                {
                    "sent": "So it has a very different interpretation, but the potential functions sometimes are represented as exponential.",
                    "label": 0
                },
                {
                    "sent": "It's a useful representation to have.",
                    "label": 0
                },
                {
                    "sent": "So you can say well, the product of these potential functions can be written this way, it's just exponentiate them and you have these terms known as energy functions.",
                    "label": 0
                },
                {
                    "sent": "So depending who you talk too, sometimes people like to talk about potential functions and representing probabilities, potential functions.",
                    "label": 0
                },
                {
                    "sent": "If you talk to statistical physicists, they'd like to represent everything in terms of energies.",
                    "label": 0
                },
                {
                    "sent": "And this is this is known as a Boltzmann distribution, and it's remarkable you know Markov random fields in these and directed graphical models really half half the roots of these models where basically developed in the statistical physics community, right?",
                    "label": 0
                },
                {
                    "sent": "So we sort of inherit a lot of things like Boltzmann machines and energy functions and such.",
                    "label": 0
                },
                {
                    "sent": "But one of the challenges again in looking at these models is that suppose you have binary random vectors, right?",
                    "label": 0
                },
                {
                    "sent": "Just just access taking values plus one and minus one, right?",
                    "label": 1
                },
                {
                    "sent": "If X has 100 dimensions, then you essentially in order to get the valid probability distribution you need to sum over 200 different configurations, right?",
                    "label": 0
                },
                {
                    "sent": "If we want to say, what's the probability of that configuration?",
                    "label": 0
                },
                {
                    "sent": "Right, that's exactly when I was showing you these.",
                    "label": 0
                },
                {
                    "sent": "Generating these handwritten characters right.",
                    "label": 0
                },
                {
                    "sent": "This is precisely roughly the model that I was using.",
                    "label": 0
                },
                {
                    "sent": "And again, if you look at the space of all possible images, you could generate that space is exponential.",
                    "label": 0
                },
                {
                    "sent": "So a lot of work in the machine learning and deep learning communities and statistics is dedicated to how can you sort of search over that exponential space efficiently?",
                    "label": 0
                },
                {
                    "sent": "We'll see some of that, so computing these normalizing constant Lizette is sometimes called normalizing constant.",
                    "label": 1
                },
                {
                    "sent": "Sometimes it's called partition function, sort of name, inherited from statistical physics.",
                    "label": 0
                },
                {
                    "sent": "Um, that actually represents a major limitation of undirected graphical models.",
                    "label": 0
                },
                {
                    "sent": "These are quite powerful models, but if you can compute these, normalizing constants.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me just show you how maximum likelihood can be done in these models and we'll see some of that.",
                    "label": 0
                },
                {
                    "sent": "Imagine I show you this small.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple model, but it's a very useful model that a lot of people in particularly computer vision communities, are using 'cause it tells you something about smoothness, right?",
                    "label": 0
                },
                {
                    "sent": "It's essentially, this is the model that's telling you you know nearby random variables should take the same value, like when you when you look at images you'd like to say.",
                    "label": 0
                },
                {
                    "sent": "Well nearby pixels should have the same value.",
                    "label": 0
                },
                {
                    "sent": "Occasionally they have different values because there's an edge.",
                    "label": 0
                },
                {
                    "sent": "But in general, you know if you look at images, these are just not random pixels.",
                    "label": 0
                },
                {
                    "sent": "Turning on and off right?",
                    "label": 0
                },
                {
                    "sent": "They have they have structure and that's a pretty good model for encoding that spatial structure.",
                    "label": 0
                },
                {
                    "sent": "So given a set of ID images X one up to XN, you'd like to model the parameters of this model, figure out what these parameters should be, and you can use likelihood objective log likelihood objective.",
                    "label": 1
                },
                {
                    "sent": "And the likelihood objective has a very sort of intuitive interpretation, right?",
                    "label": 0
                },
                {
                    "sent": "It essentially is telling you well, if you show me the data, give me the data right?",
                    "label": 0
                },
                {
                    "sent": "How can I find parameters of this model in such a way that the joint distribution of all of these are joint probability of all of these images is as high as possible?",
                    "label": 0
                },
                {
                    "sent": "You'd like your model to be probable as possible.",
                    "label": 0
                },
                {
                    "sent": "Sort of explain, explain the data, and that's the likelihood objective, right?",
                    "label": 0
                },
                {
                    "sent": "And it's pretty easy to take, you know.",
                    "label": 0
                },
                {
                    "sent": "The derivative this likelihood objective and you have these two terms right, and this is something that's called expected.",
                    "label": 0
                },
                {
                    "sent": "Sufficient statistics driven by the data.",
                    "label": 0
                },
                {
                    "sent": "What essentially does is it looks at the training data.",
                    "label": 0
                },
                {
                    "sent": "And it basically says you know what's the correlation structure between nearby random variables.",
                    "label": 0
                },
                {
                    "sent": "That's all what it says, right?",
                    "label": 0
                },
                {
                    "sent": "Just looking at correlations between random variables and this thing.",
                    "label": 0
                },
                {
                    "sent": "Here is the expected sufficient statistics by driven by the model, right?",
                    "label": 0
                },
                {
                    "sent": "And it essentially says you know what are the correlations induced by the model according to the model with my correlations are and your maximum likelihood solution is essentially saying well try to match the two right whenever you're building them all, try to basically look at the correlations that the model is inducing in.",
                    "label": 1
                },
                {
                    "sent": "You know saying you know those should match the data distribution, but unfortunately this term here is difficult to compute, 'cause they're exponentially many configurations, right?",
                    "label": 0
                },
                {
                    "sent": "And will.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In some ways of doing doing that, you know if you look at Markov random fields with latent variables, that becomes even more challenging and that's the challenge.",
                    "label": 1
                },
                {
                    "sent": "When we start looking at deep learning models here, the X is composed both with something called visible variables, like pixels in the image of speech signal or words in a document as well as latent variables, and these latent variables.",
                    "label": 0
                },
                {
                    "sent": "Can tell us something about, you know either the topics or semantic meaning of of the data.",
                    "label": 0
                },
                {
                    "sent": "You can specify the probability in exactly the same way, But the problem is again you have this normalizing constant, but even more problematic you have this summation over the latent variables, right?",
                    "label": 0
                },
                {
                    "sent": "So if you try to do inferences, these models, you basically want to say, well, what's the distribution of the latent variables that I'm trying to infer.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And the parameter learning becomes very very challenging tasks.",
                    "label": 1
                },
                {
                    "sent": "So a lot of work and I'll show you some of that work in the deep learning community is trying to basically figure out how can you deal with that intractability.",
                    "label": 0
                },
                {
                    "sent": "And as I go through tutorial, I'm going to make it all the more precise.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me move into a class of models called restricted boss machines.",
                    "label": 0
                },
                {
                    "sent": "In these kinds of models that are very useful for learning low level features.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are kinds of models.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they called undirected graphical model.",
                    "label": 1
                },
                {
                    "sent": "These are by part type.",
                    "label": 0
                },
                {
                    "sent": "They have a bipartite structure in the simplest model you have stochastic binary visible variables.",
                    "label": 1
                },
                {
                    "sent": "Again, think of them as pixels in your images and you have stochastic binary hidden variables.",
                    "label": 0
                },
                {
                    "sent": "You can think of them as feature detectors, so feature detectors you know that can tell us something about maybe a semantic representation of documents or some kind of little high level features that you see in the data.",
                    "label": 0
                },
                {
                    "sent": "Right, you can specify the energy of this configuration and the simplest way of specifying the energy is just using.",
                    "label": 0
                },
                {
                    "sent": "Here is just a linear combination here.",
                    "label": 0
                },
                {
                    "sent": "What this term is effectively doing is it's basically saying well, what's the correlation structure between each pixel?",
                    "label": 0
                },
                {
                    "sent": "As well as each latent variable.",
                    "label": 0
                },
                {
                    "sent": "OK, and the parameters Theta are just the parameters W as well as these bias terms, so these offset terms is a common.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing to have now you can specify the probability of the joint distribution using a very standard definition, right?",
                    "label": 0
                },
                {
                    "sent": "It's just.",
                    "label": 0
                },
                {
                    "sent": "Each of the negative of the energy, or if you write it explicitly, you essentially have this term where you have pairwise.",
                    "label": 0
                },
                {
                    "sent": "Sometimes these, called pairwise potentials, that again just modeling correlations between pixels and latent variables, and you have a normalizing constant, and these sometimes called.",
                    "label": 0
                },
                {
                    "sent": "You know if you've heard terms like Markov random fields, Boltzmann machines log linear models this roughly.",
                    "label": 1
                },
                {
                    "sent": "Correspond to the same, basically the same model, just different names.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, why these models are called restricted right?",
                    "label": 0
                },
                {
                    "sent": "The reason why they called restricted is because there is no connections between these hidden variables OK?",
                    "label": 1
                },
                {
                    "sent": "And that has an advantage in disadvantage.",
                    "label": 1
                },
                {
                    "sent": "The key advantage is that it turns out that computing the distribution over the latent variables can be done in closed form.",
                    "label": 0
                },
                {
                    "sent": "Right, and this is a very useful thing to have.",
                    "label": 0
                },
                {
                    "sent": "In particular, you know if I show you a new image, I can quickly tell you what features make up that image, or if I show you a document I can quickly tell you what topics make up that document, right?",
                    "label": 0
                },
                {
                    "sent": "So it's easy to compute and in the same way the conditional probability of observing the image is again has a closed form solution.",
                    "label": 1
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so if you apply this model to handwritten characters, again, this is the kind of structure that it learns very closely related to sparse coding model, right?",
                    "label": 0
                },
                {
                    "sent": "The one that we've seen it sort of detects little edges, and then you can say, well, if I show you a new image.",
                    "label": 0
                },
                {
                    "sent": "Well, this new image can be written as a combination of these little edges, and it turns out to be useful thing to have, because you can represent a lot of different images just using a subset of these basis.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of like the natural is very simple.",
                    "label": 0
                },
                {
                    "sent": "Basically tries to find basis decomposition of these images.",
                    "label": 0
                },
                {
                    "sent": "Most hidden variables are going to be off and these numbers are given like these numbers are given by the conditional probability of a particular feature being on.",
                    "label": 1
                },
                {
                    "sent": "And obviously you can see for this image you know observing a feature like this will have a very very small probability, right?",
                    "label": 0
                },
                {
                    "sent": "But unlike in sparse coding model here, if I show you this new image, I can quickly tell you almost instantly tell you what are the features that make up this image, so I can do recognition very quickly.",
                    "label": 0
                },
                {
                    "sent": "And then you can represent again this new image in terms of its conditional conditional.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of learning, you know it's the same story as in any type of graphical model.",
                    "label": 0
                },
                {
                    "sent": "Give me a set of ID images, right then it turns out that if I want to look at the log likelihood objective, I can try to maximize the joint probability of observing all of these images.",
                    "label": 1
                },
                {
                    "sent": "There is a regularization term.",
                    "label": 0
                },
                {
                    "sent": "Typically people use could be a one.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be able to write and then there is a little bit of math behind these models.",
                    "label": 0
                },
                {
                    "sent": "Not too difficult, it's just a little bit of algebra here, but again, you have the difference between these two expectations, right?",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first expectation is easy to compute.",
                    "label": 1
                },
                {
                    "sent": "And it turns out that's easy to compute.",
                    "label": 0
                },
                {
                    "sent": "The reason why it's easy to compute is because when I look at the conditional probability that conditional probability is easy to compute and that has to do with the particular structure, the bipartite structure of this model.",
                    "label": 0
                },
                {
                    "sent": "This term is difficult to compute 'cause you have to enumerate over all possible images, right?",
                    "label": 0
                },
                {
                    "sent": "Again, that space is space is exponential, so it's difficult to compute, and one of the most dominant approaches to compute these expectations.",
                    "label": 1
                },
                {
                    "sent": "Is is is using Markov chain Monte Carlo methods, right?",
                    "label": 0
                },
                {
                    "sent": "And it turns out you know these MCMC methods are fairly easy to use in these models.",
                    "label": 0
                },
                {
                    "sent": "In fact it's for these kind of model.",
                    "label": 0
                },
                {
                    "sent": "Takes really 5 lines of MATLAB code, 5 lines of our code to actually code it up.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, in terms of computing these expectations, you can essentially do something that's called deep sampling, right?",
                    "label": 0
                },
                {
                    "sent": "It's a very commonly used Markov chain Monte Carlo algorithm, and the way it works is that you start at some configuration or you start at the data point and then you just repeat.",
                    "label": 0
                },
                {
                    "sent": "You compute the probability of observed data given the latent variables, and then you can compute the conditional probability of latent variables given.",
                    "label": 0
                },
                {
                    "sent": "Given the reconstructed data, so just doing alternating Gibbs sampling, it's very easy to do.",
                    "label": 0
                },
                {
                    "sent": "You compute the hidden features, sample the hidden features given the data, then reconstruct the data.",
                    "label": 0
                },
                {
                    "sent": "Given the hidden features and you proceed.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what I was showing you when you saw these samples generating generating different Sanskrit, generating different airplanes, that's exactly the procedure that I was running an.",
                    "label": 0
                },
                {
                    "sent": "Again, it's very easy to code.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you looking at maximum likelihood learning for these models right, you need to estimate these expectation and this is essentially how it looks like, right?",
                    "label": 0
                },
                {
                    "sent": "And then you know what the theory is telling you is that if you go all the way up to Infinity, you will get the true expectations.",
                    "label": 0
                },
                {
                    "sent": "Right, the Markov chain will converge eventually, and you'll get the.",
                    "label": 1
                },
                {
                    "sent": "Unbiased estimate of.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of these expectations, now in practice people actually use something that's called contrastive divergent, so you probably heard the term contrastive divergent.",
                    "label": 0
                },
                {
                    "sent": "And what it essentially does is you start at the training data you update all the hidden units, all the hidden variables, you update all the visible variables again, so you reconstruct the data and then you update all the hidden variables again and then you just update the parameters by looking at the difference between these two expectations, which you can compute.",
                    "label": 1
                },
                {
                    "sent": "So it's kind of an interesting algorithm.",
                    "label": 0
                },
                {
                    "sent": "It basically says instead of running your Markov chain all the way up to Infinity, just running for one step.",
                    "label": 0
                },
                {
                    "sent": "That's all it says.",
                    "label": 0
                },
                {
                    "sent": "And it turns out to be a very useful learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's very quick, it's biased, but you can learn.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good features.",
                    "label": 0
                },
                {
                    "sent": "Using this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now you can extend the beauty of these models is that you can extend these models to model in all kinds of distributions, right?",
                    "label": 0
                },
                {
                    "sent": "So for example, if I if I'm interested in modeling images and I can think of images as being as vectors in real valued space, then with a little bit of modification of my energy function on my probability here, right?",
                    "label": 0
                },
                {
                    "sent": "Again you have pairwise you have pairwise terms.",
                    "label": 0
                },
                {
                    "sent": "You have unary terms.",
                    "label": 0
                },
                {
                    "sent": "It turns out that just this simple modification in the code you can actually learn meaningful features from images.",
                    "label": 0
                },
                {
                    "sent": "And in Matlab code there is only one line change in math lab code for learning these models, going from binary to real value.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And again, the conditional probability he is just going is going to be given by the product of Gaussians, right?",
                    "label": 0
                },
                {
                    "sent": "But you can learn is you can learn these kinds of features.",
                    "label": 0
                },
                {
                    "sent": "So given here, you're looking at 4 million unlabeled images, just images random images downloaded from the web.",
                    "label": 1
                },
                {
                    "sent": "This is the kind of structure that the model is discovering, right?",
                    "label": 0
                },
                {
                    "sent": "And you can see it's sort of.",
                    "label": 0
                },
                {
                    "sent": "Again, it discovers these edges, but also discovers these interesting things like little colors, dependencies, and again when I show these things to neuro scientists, they get excited because it's sort of looks a little bit possible from the neuroscience standpoint.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the way you can think again is again, it just finds basis.",
                    "label": 0
                },
                {
                    "sent": "Find slightly different basis for four images.",
                    "label": 0
                },
                {
                    "sent": "So if I show you a new image again, I can decompose it, or I can write it as a linear combination of these bases and these bases.",
                    "label": 0
                },
                {
                    "sent": "Again, a very useful to have because you can use these bases to recognize what's going on.",
                    "label": 0
                },
                {
                    "sent": "Image is much better than just using pixels.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, one way to think about these models, maybe I just point this out is if if you look at the marginal probability probability of the data, you can write it as this way right?",
                    "label": 0
                },
                {
                    "sent": "It's the conditional probability times the pryan you summing over these latent variables.",
                    "label": 0
                },
                {
                    "sent": "Now what happens is that you can essentially interpret these models is mixture models.",
                    "label": 0
                },
                {
                    "sent": "So in this case you can interpret them as mixture of Gaussians and you're probably familiar with mixture of Gaussians models right.",
                    "label": 0
                },
                {
                    "sent": "But the beauty of these models, you can interpret them as a mixture of exponential number of Gaussians right and the way to think about this is that.",
                    "label": 1
                },
                {
                    "sent": "Let's say I have three latent variables.",
                    "label": 0
                },
                {
                    "sent": "Each one takes value 01, right?",
                    "label": 0
                },
                {
                    "sent": "So if you think about this representation that two to three possible configurations, right?",
                    "label": 0
                },
                {
                    "sent": "There are eight possible configurations.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that I can write this model as a mixture of eight Gaussians, right?",
                    "label": 0
                },
                {
                    "sent": "But where the mean, then the covariances are going to be shared across different subsets of hidden variables.",
                    "label": 0
                },
                {
                    "sent": "So in particularly typically if I have 100 or thousand of these latent variables hidden variables, there are 2000 possible configurations, right?",
                    "label": 0
                },
                {
                    "sent": "So you can think of it as just 2000 possible mixture components.",
                    "label": 0
                },
                {
                    "sent": "So these models in general for a lot of different problems.",
                    "label": 0
                },
                {
                    "sent": "These models work much better than mixture Gaussians.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also apply these models to again modeling other kinds of data, so here you're applying it to modeling count data that's very useful for modeling things like web pages or text bag of words representation.",
                    "label": 0
                },
                {
                    "sent": "So here you can think of, you know, let's say you have different words in a document and K is the vocabulary size, so you know in English that might be 70,000.",
                    "label": 0
                },
                {
                    "sent": "So K might be 70,000 and each particular webpage might contain, let's say A 1000 words.",
                    "label": 0
                },
                {
                    "sent": "So deal would be 1000 here.",
                    "label": 0
                },
                {
                    "sent": "And again, in terms of defining these models that are slight, some changes, but roughly again you have these pairwise terms and you have these unary terms, so there's nothing difficult of extending these models.",
                    "label": 0
                },
                {
                    "sent": "The conditional distribution here is going to be given by something called softmax distribution.",
                    "label": 0
                },
                {
                    "sent": "Is the distribution of a different words that can appear in a document.",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fairly straightforward and these are the kind of latent variables latent topics that the model is discovering, right?",
                    "label": 0
                },
                {
                    "sent": "So again, if you applied to the Reuters data set, you know it sort of figures out that there should be things like Russians and then US, and then the computers and such.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing it.",
                    "label": 0
                },
                {
                    "sent": "And again, the way to think about this model is that every single web page or document is given by some linear combination of these topics.",
                    "label": 0
                },
                {
                    "sent": "Sparse linear combination of these topics.",
                    "label": 0
                },
                {
                    "sent": "So few topics get activated and that's how.",
                    "label": 0
                },
                {
                    "sent": "The data is generated, so it's sort of finds so you know the interesting thing here is that when you apply these models to images, you find these edges.",
                    "label": 0
                },
                {
                    "sent": "When you apply these models 2 words, it's sort of finds these interesting, meaningful topics.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, this is what it does in terms of reconstructions, right?",
                    "label": 0
                },
                {
                    "sent": "So this was trained on the Flickr data set and the second half of the tour.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you more of this, so this is if I if I give the model chocolate and cake, I encode chocolate and cake and look at the distribution of the hidden variables and then reconstruct back the data.",
                    "label": 0
                },
                {
                    "sent": "Reconstruct reconstruct things like cake, chocolate, sweet dessert, cupcake food, sugar cream, right?",
                    "label": 0
                },
                {
                    "sent": "So obviously it captures some semantic similarity between different words.",
                    "label": 0
                },
                {
                    "sent": "It's also interesting that this guy flour high.",
                    "label": 0
                },
                {
                    "sent": "And then the Japanese sign, and then it's flower High Japan soccer.",
                    "label": 0
                },
                {
                    "sent": "And I don't know what these mean, but Blossom Tokyo, right?",
                    "label": 0
                },
                {
                    "sent": "So it just picks up, you know these kinds of you know, to some extent, multilingual things just by looking at correlations between different between different words.",
                    "label": 0
                },
                {
                    "sent": "It's very useful for encoding text.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other thing is that you can apply these models exact same model.",
                    "label": 0
                },
                {
                    "sent": "You can apply to modeling social networking data, right?",
                    "label": 0
                },
                {
                    "sent": "So this was done on the Netflix data set where you have users and you have movies and you can think of just encoding each user preference or the rating pattern as V here.",
                    "label": 0
                },
                {
                    "sent": "So you can think of these being multinomial random variables that effectively tell you whether the user liked the movie.",
                    "label": 0
                },
                {
                    "sent": "I didn't like the movie.",
                    "label": 0
                },
                {
                    "sent": "You know these ratings 125.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing about this model is that if you look at the latent representations that the model is discovering, you sort of discovering these learn genre, right?",
                    "label": 0
                },
                {
                    "sent": "So you know, horror movies get grouped together.",
                    "label": 0
                },
                {
                    "sent": "It's interesting Michael Moore's movies get grouped together, so just by looking at the patterns of how people rate the movies, you can basically figure out that either people really like his movies or they really hate his movies.",
                    "label": 0
                },
                {
                    "sent": "Actually, somebody pointed to me that this is not Michael Moore's movie.",
                    "label": 0
                },
                {
                    "sent": "But it should be very close I guess.",
                    "label": 0
                },
                {
                    "sent": "I'm curious myself.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should watch that movie.",
                    "label": 0
                },
                {
                    "sent": "So again, exactly the same kind of model, slight modification and you can apply to.",
                    "label": 0
                },
                {
                    "sent": "You know social Nets or Netflix or product recommendation type of problems.",
                    "label": 0
                },
                {
                    "sent": "So a lot of these different you can model a lot of these different modalities and from machine learning perspective that's very exciting, right?",
                    "label": 0
                },
                {
                    "sent": "Because you can apply in a lot of different domains.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the beauty behind these models is that you could.",
                    "label": 0
                },
                {
                    "sent": "It's easy to infer the states of the hidden variables, right?",
                    "label": 1
                },
                {
                    "sent": "So if you show me a particular pattern for a user, I can quickly tell you what movies he's he's going to like, or if you show me an image, I can quickly tell you the distribution of other features or topics that you see in the data, right?",
                    "label": 0
                },
                {
                    "sent": "And that's very important for information retrieval.",
                    "label": 0
                },
                {
                    "sent": "For for classification type of problems right then, the way to think about these models is just to give you an intuition is that you can think of them as product of experts.",
                    "label": 0
                },
                {
                    "sent": "Sometimes people call these models product of experts.",
                    "label": 0
                },
                {
                    "sent": "And the way to think about these models is that if I look at the marginal distribution over the data, it can be written as a product of these terms in.",
                    "label": 0
                },
                {
                    "sent": "Sometimes people call them as products, right?",
                    "label": 0
                },
                {
                    "sent": "So unlike traditional mixture models, right?",
                    "label": 0
                },
                {
                    "sent": "You have product of bunch of things right?",
                    "label": 0
                },
                {
                    "sent": "And maybe I can give you the intuition what that really means.",
                    "label": 0
                },
                {
                    "sent": "Let's say I have these different topics that the model is discovering, right?",
                    "label": 0
                },
                {
                    "sent": "If I activate topics like government corruption and oil, right?",
                    "label": 0
                },
                {
                    "sent": "You have put in, will have very high probability under these three topics, right?",
                    "label": 0
                },
                {
                    "sent": "If I'm going to tell you that this document is about government corruption and oil.",
                    "label": 0
                },
                {
                    "sent": "Putin should go up right?",
                    "label": 0
                },
                {
                    "sent": "This is unlike traditional mixture models right in the mixture model.",
                    "label": 0
                },
                {
                    "sent": "You pick a topic and then you generate a word like traditional topic models like latent garishly allocation.",
                    "label": 0
                },
                {
                    "sent": "If you've heard about these, these kinds of models so here you can make the distributions very precise because you're taking three different distributions, you multiplying them together and as soon as you take distributions and you look at the intersection, you can be very precise about what you're going to see in a document or what you're going to see in the data, and that's precisely what's going on with the.",
                    "label": 0
                },
                {
                    "sent": "These models is a product based model, not a mixture based model.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and again, if you look at trying to do recognition or trying to do in this case information retrieval, you can do much better than traditional topic or mixture based models.",
                    "label": 0
                },
                {
                    "sent": "If you're looking at speech, the same story holds, right?",
                    "label": 0
                },
                {
                    "sent": "You know you can.",
                    "label": 0
                },
                {
                    "sent": "This was done by group at Stanford, where if you look at the first layer basis you sort of discovering that kind of structure.",
                    "label": 0
                },
                {
                    "sent": "And the remarkable thing is that what turns out is that if you, if you apply this.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model then it essentially discovers four names at the first level, right?",
                    "label": 0
                },
                {
                    "sent": "So you know.",
                    "label": 0
                },
                {
                    "sent": "This is sort of has a correspondence to a particular phoneme.",
                    "label": 0
                },
                {
                    "sent": "Oy, it just has a correspondence between particular funding L. So here what I'm showing you.",
                    "label": 0
                },
                {
                    "sent": "This is what the fanime the actual phoneme is, and this is what the model is learning.",
                    "label": 0
                },
                {
                    "sent": "So by basically applying.",
                    "label": 0
                },
                {
                    "sent": "Pretty much the same model.",
                    "label": 0
                },
                {
                    "sent": "The model can basically discover phonemes which we know is a useful thing for doing speech recognition, right?",
                    "label": 0
                },
                {
                    "sent": "And it sort of does it on its own, and you can see that there is a correspondence that the model is discovering, and I think that in you know this is not a speech community, but if you go to this speech conferences, you'll see a lot of these models deep learning kind of models basically making a huge impact just because you basically learning what are the right representations to extract from noisy signals.",
                    "label": 0
                },
                {
                    "sent": "All noisy speech signals.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, and maybe I can just point.",
                    "label": 0
                },
                {
                    "sent": "Again, the difference between the two models is that you know you probably have seen nearest neighbors, or clustering or sort of local density estimators and the idea behind these models is that you essentially partitioning the space like clustering based models and mixture based models.",
                    "label": 0
                },
                {
                    "sent": "You partitioning this space you finding these local regions an you finding parameters for each local region.",
                    "label": 0
                },
                {
                    "sent": "Right, so number of regions is linear with the number of parameters.",
                    "label": 1
                },
                {
                    "sent": "But if you look at sort of distributed type of models like restricted, both machines, factor models, PCA, sparse coding, deep models, they all sort of do something slightly different.",
                    "label": 0
                },
                {
                    "sent": "And the idea is the following.",
                    "label": 0
                },
                {
                    "sent": "Imagine I have a 2 dimensional data.",
                    "label": 0
                },
                {
                    "sent": "The first hidden variable can partition the data into two of two planes, right?",
                    "label": 0
                },
                {
                    "sent": "C0C1 if I introduce another hidden variable, I can partition the place again into.",
                    "label": 0
                },
                {
                    "sent": "You know C21C20 right notice with two things I have four partitions.",
                    "label": 0
                },
                {
                    "sent": "If I introduce another hit.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variable I can partition the plane as well.",
                    "label": 0
                },
                {
                    "sent": "Right, so the interesting thing about these model.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that each parameter, each hidden variable effects many regions, not just local regions, and the number of regions grows, roughly speaking exponential in number of parameters, right?",
                    "label": 1
                },
                {
                    "sent": "So this is where you know.",
                    "label": 0
                },
                {
                    "sent": "These models shine because they to some extent able to deal a little bit better with the curse of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Write something that local models cannot deal, so that's the difference between between.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two models, so if you look at a lot of different domains like natural images, text, collaborative filtering, video applications, motion capture and I'm going to show you some of that more speech perception.",
                    "label": 0
                },
                {
                    "sent": "The beauty of these models and what makes it exciting is that you have the same learning algorithm, but you have multiple domains, multiple input domains, right?",
                    "label": 1
                },
                {
                    "sent": "And it does find interesting structure in every single domain.",
                    "label": 0
                },
                {
                    "sent": "But obviously there are limitations, right?",
                    "label": 0
                },
                {
                    "sent": "And there are limitations to the types of structures that can be extracted by just basically single layer of these nonlinear features, right?",
                    "label": 1
                },
                {
                    "sent": "And as we've seen, you sort of able to extract things like little edges, right?",
                    "label": 0
                },
                {
                    "sent": "Or if you're looking at words, you can extract little correlations between words or with speech you can extract little phonemes, just little parts of speech signal, right?",
                    "label": 0
                },
                {
                    "sent": "And obviously there is a need to go beyond that, right?",
                    "label": 0
                },
                {
                    "sent": "So people knew about this before, but.",
                    "label": 0
                },
                {
                    "sent": "Up until five years ago, we didn't really have good learning algorithms for being able to learn multiple levels of representation, so a lot of focus has been focusing on just building these single layer models.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now let me introduce you to the deep belief networks, and then we're going to take a half an hour break.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I'm just curious whether.",
                    "label": 0
                },
                {
                    "sent": "Whether we're going to run out of coffee or not.",
                    "label": 0
                },
                {
                    "sent": "Anyways, let me let me just speak for another 1520 minutes and then and then we're going to take a break.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the exciting part.",
                    "label": 0
                },
                {
                    "sent": "Deep belief networks.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so these are probabilistic generative models.",
                    "label": 0
                },
                {
                    "sent": "Right, you have multiple layers of nonlinear representations and the beauty of these models is that there is a fast greedy layer wise pretraining algorithm and that pre training algorithm was discovered back in 2006 and basically opened up the space of deep learning, right?",
                    "label": 1
                },
                {
                    "sent": "The interesting thing about this algorithm is that inferring the distribution of the latent variables in first is easy in these models and that was a breakthrough right?",
                    "label": 1
                },
                {
                    "sent": "Because all of a sudden if I show you a new image, I can quickly tell you what a high level representations that make up this image.",
                    "label": 0
                },
                {
                    "sent": "And you can do it very quickly.",
                    "label": 0
                },
                {
                    "sent": "This is something that people couldn't couldn't do before.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you think about using directed model Bayesian networks and you have multiple layers, then you have to run complicated MCMC inference and it becomes fairly slow.",
                    "label": 0
                },
                {
                    "sent": "And the idea?",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Behind these models is that well if you look at restricted Boltzmann machines right, they capture sort of these little edges with these little primitive features.",
                    "label": 0
                },
                {
                    "sent": "If we.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So high up, right?",
                    "label": 0
                },
                {
                    "sent": "And we're trying to look at some high level features.",
                    "label": 0
                },
                {
                    "sent": "The hope is that we can discover things like combination Avengers or maybe part based representation that we see in images, right?",
                    "label": 0
                },
                {
                    "sent": "And you're trying to basically look at combined simple features from into more complex ones, right?",
                    "label": 0
                },
                {
                    "sent": "And the idea behind these models?",
                    "label": 0
                },
                {
                    "sent": "Again, you can think of this latent variable is essentially modeling correlations between the data.",
                    "label": 0
                },
                {
                    "sent": "But this latent variables essentially modeling correlation between these features, right?",
                    "label": 0
                },
                {
                    "sent": "The hope is that.",
                    "label": 0
                },
                {
                    "sent": "As you go up in the hierarchy, you can Start learning high and higher level representations of of the data.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, So what is a deep belief network?",
                    "label": 1
                },
                {
                    "sent": "You have hidden layers, so in this case you have H1H2H3.",
                    "label": 0
                },
                {
                    "sent": "Those are your latent variables hidden variables.",
                    "label": 0
                },
                {
                    "sent": "And then you have something that's called a restricted Boltzmann machine that sits at the top and something that's called a sigmoid belief network, which is a directed graphical model that basically generates the observed data.",
                    "label": 0
                },
                {
                    "sent": "So it has this very strange kind of structure to it, right?",
                    "label": 0
                },
                {
                    "sent": "It's a hybrid graphical model.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But it was designed by construction and really comes out of a greedy layer wise pretraining of these models.",
                    "label": 0
                },
                {
                    "sent": "I can write down the joint probability of the data right so I can right now in the joint probability we could say well is the probability of the top two layers followed by the conditional probability of H1 given H2 followed by the conditional probability of observed data given H1.",
                    "label": 0
                },
                {
                    "sent": "Right, sigmoid belief Nets these kinds of models.",
                    "label": 1
                },
                {
                    "sent": "Again, these are directed graphical model.",
                    "label": 0
                },
                {
                    "sent": "These Bayes Nets they've been around for quite some time.",
                    "label": 0
                },
                {
                    "sent": "So this isn't restricted.",
                    "label": 1
                },
                {
                    "sent": "Boltzmann machine and the sigmoid belief Nets are just given by product of logistic functions, so there's nothing difficult about these models right now.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of the generative process, the way you generate the data is you can generate the top level representations using restricted Boltzmann machines, just alternating Gibbs sampling and then you can generate all the way down.",
                    "label": 0
                },
                {
                    "sent": "In terms of inference or trying to infer the distribution of the latent variables, there's sort of an inference process that goes bottom up.",
                    "label": 0
                },
                {
                    "sent": "Right, so given the data, there is an inference algorithm that basically tells you what the distribution of the latent variables is an it essentially becomes a neural network for this kind of model.",
                    "label": 0
                },
                {
                    "sent": "Which is fast and very easy to do.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use.",
                    "label": 0
                },
                {
                    "sent": "In terms of learning these models.",
                    "label": 0
                },
                {
                    "sent": "The way you can design the learning algorithm and that was basically the breakthrough that happened in 2006 because people in particular Jeff Hinton figured out how you can gradually train these models right and the way you can do it is the following.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, you train a restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "Then you use these approximate inference or the conditional probability as the data for training the next level Boltzmann machine.",
                    "label": 1
                },
                {
                    "sent": "So your training one Boston machine, this machine, you infer the distribution of a hidden value chain is second bowl smush.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know when you proceed to training multiple layers, right?",
                    "label": 0
                },
                {
                    "sent": "The question was that, why does it make sense right?",
                    "label": 0
                },
                {
                    "sent": "This is just a hack.",
                    "label": 0
                },
                {
                    "sent": "It turns out.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know there is a sort of.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Turns out that when you do this, when you train these multiple layers greedily, what you effectively doing is you improving something that's called variational lower bound.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to get into too much detail about variational inference, but variational inference has been around in machine learning for quite sometime in the last 20 years, and the beauty of this algorithm is that that was the first mathematical proof that basically told us that as you stack multiple layers, you improving the variational bound on the likelihood objective.",
                    "label": 0
                },
                {
                    "sent": "You're not improving the likelihood objective itself, but you improving the bound on the likelihood objective, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a meaningful thing to do and then.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cited a lot of people, particular people who were working variational inference and in machine learning.",
                    "label": 0
                },
                {
                    "sent": "And the way you can think about the variational inference is you know.",
                    "label": 0
                },
                {
                    "sent": "For any approximating distribution Q you tell me what that Q distribution should be.",
                    "label": 1
                },
                {
                    "sent": "I can write down a low bound on the low probability of data, right?",
                    "label": 0
                },
                {
                    "sent": "So for those of you who are familiar with expectation maximization algorithm, right, roughly speaking, you're doing something along those lines, except for.",
                    "label": 0
                },
                {
                    "sent": "This is something that's called expected complete data log likelihood, and this is the entropy functional will get to this at the second half of the tutorial fairly quickly, but the idea is that you can show mathematically.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That as you increase the number of layers and you start training the second layer.",
                    "label": 0
                },
                {
                    "sent": "Then by training the second layer, you essentially pushing the variational lower bound up.",
                    "label": 1
                },
                {
                    "sent": "Right, and by pushing the variational lower bound, the hope is that the likelihood function will improve as well.",
                    "label": 0
                },
                {
                    "sent": "You know it's a little bit mathematically, it's a little bit nontrivial to explain it in a few minutes, but just I wanted to get the idea that there is an objective function of well defined objective function that sort of says that as you increase the number of layers, you're increasing their subjective function that relates to the maximum likelihood, and that was the first proof that basically inspired people to start looking into these models and start constructing these multiple levels of representations in greedy fashion.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can also do supervised learning with deep belief Nets.",
                    "label": 1
                },
                {
                    "sent": "In particular, you can attach the label here and model the joint probability distribution.",
                    "label": 1
                },
                {
                    "sent": "You can train these models greedily, and then you can maximize the conditional probability of the label given the data right?",
                    "label": 0
                },
                {
                    "sent": "And that's a lot of sometimes people do that, in particular, for you know if you're interested in classification type of.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of sampling from this model, again, you can just run a Gibbs chain by simulating at the top two layers and then generating generating the data.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of the features that these models are learning.",
                    "label": 0
                },
                {
                    "sent": "If you look at the first layer features right, this is kind of like edges that the model is discovering.",
                    "label": 0
                },
                {
                    "sent": "This is done on digital data set, handwritten digit data set.",
                    "label": 0
                },
                {
                    "sent": "If you look at the second layer features, you'll start seeing something that looks like high level parts of.",
                    "label": 0
                },
                {
                    "sent": "Parts of.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Images, you know, here's another way of looking at this model.",
                    "label": 0
                },
                {
                    "sent": "If you train this model, images of faces right at the lower levels you discovering these little edges edge structure at the mid level representation you start discovering different parts of faces so you can see like their eyes and parts of noses at the high level third level you start discovering parts of faces.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can basically find in groups of parts and it turns out to be very useful for doing face recognition, for example.",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you're looking at different.",
                    "label": 0
                },
                {
                    "sent": "You know different like cars, faces, elephants and chairs.",
                    "label": 0
                },
                {
                    "sent": "You can sort of see what happens in these models, right?",
                    "label": 0
                },
                {
                    "sent": "So the first layer is shared at the second level, you start capturing more part based representation at the high levels you actually start capturing the objects themselves and that was.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have an exciting development.",
                    "label": 0
                },
                {
                    "sent": "You know if you group everything together then you can see that you know at the low level you discovering edges at the mid level you start capturing part specific things and at the high levels you start capturing groups of parts right?",
                    "label": 1
                },
                {
                    "sent": "And that's sort of representation that evolves as you learning these models and the beauty of these models.",
                    "label": 0
                },
                {
                    "sent": "Again, you're just learning everything based on pixels based on the data itself, without designing what are the right features to use without designing hand designing that you should be detecting eyes and nose because that's what faces look like, right?",
                    "label": 0
                },
                {
                    "sent": "You can just discover all of these levels based on the data in unsupervised fashion, so nobody is telling me that faces should contain eyes and nose.",
                    "label": 0
                },
                {
                    "sent": "Is the model just figures out that that's the regularity is that you see in faces?",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also do it for classifying digits right?",
                    "label": 0
                },
                {
                    "sent": "If you pre training these models greedily and then you unrolling them into.",
                    "label": 0
                },
                {
                    "sent": "Network then you're getting.",
                    "label": 0
                },
                {
                    "sent": "Much better discrimination and it excited.",
                    "label": 0
                },
                {
                    "sent": "A lot of people because these kinds of models can use unsupervised learning, right?",
                    "label": 0
                },
                {
                    "sent": "And it sort of improves generalization because you're trying to model the distribution over the data itself.",
                    "label": 0
                },
                {
                    "sent": "That turns out to be useful.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is 1 particular example.",
                    "label": 0
                },
                {
                    "sent": "Is it very interesting example?",
                    "label": 0
                },
                {
                    "sent": "Imagine that I'm trying to predict the orientation of the face with a very simple task, right?",
                    "label": 0
                },
                {
                    "sent": "So the training data contains multiple phases as well as the degree of orientation of the phase right at the test I'm I'm going to give you new people and the goal is to predict the orientation of the face for new people.",
                    "label": 0
                },
                {
                    "sent": "Right, you can use something that's called Gaussian process, which is a fairly sophisticated regression model.",
                    "label": 0
                },
                {
                    "sent": "Then you can predict basically based up to about 16 degrees.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now if you using deep belief Nets and you extracting you learning something about how faces should look like right and some dependencies, you know there should be eyes, they should be nose and such.",
                    "label": 0
                },
                {
                    "sent": "You building the stack of these restrictive bolts machines.",
                    "label": 0
                },
                {
                    "sent": "So extracting these features, and whenever you extracting features you not telling them all that you're going to be using it for predicting the orientation of the faces you just trying to figure out what are the regularity as you see in the data.",
                    "label": 0
                },
                {
                    "sent": "Then it turns out that if you using.",
                    "label": 0
                },
                {
                    "sent": "These latent variables latent features an you construct a Gaussian process on top of them.",
                    "label": 0
                },
                {
                    "sent": "You can basically go all the way down to 6 degrees, so you can predict the orientation of a new phase up to 6 degrees versus 16 degrees.",
                    "label": 0
                },
                {
                    "sent": "So obviously it's a very useful.",
                    "label": 0
                },
                {
                    "sent": "Thing to have.",
                    "label": 0
                },
                {
                    "sent": "You can also design these something.",
                    "label": 0
                },
                {
                    "sent": "It's called deep autoencoders.",
                    "label": 0
                },
                {
                    "sent": "And again this is the way of encoding the data through many, many layers of nonlinear processing and get some representation of the data.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is you can train a stack of these restricted both machines.",
                    "label": 0
                },
                {
                    "sent": "You can take the face.",
                    "label": 0
                },
                {
                    "sent": "You can encode down to 30 dimensional space and then you can reconstruct the reconstructed back.",
                    "label": 0
                },
                {
                    "sent": "And you can also update the parameters of this entire model using backpropagation algorithm, which is a very straightforward thing to do, and you can think of this as an.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pulling in generalization of PCA right?",
                    "label": 0
                },
                {
                    "sent": "And this is indeed what's happening.",
                    "label": 0
                },
                {
                    "sent": "This is the real data.",
                    "label": 0
                },
                {
                    "sent": "This is the test images.",
                    "label": 0
                },
                {
                    "sent": "This is what the auto encoder does.",
                    "label": 0
                },
                {
                    "sent": "And this is what PCA does.",
                    "label": 0
                },
                {
                    "sent": "So PCA notice doesn't capture.",
                    "label": 0
                },
                {
                    "sent": "You know very specific.",
                    "label": 0
                },
                {
                    "sent": "Like here, a lot of PCA things look like people like Snowman's, right?",
                    "label": 0
                },
                {
                    "sent": "Where is the auto encoder is able to capture very precise?",
                    "label": 0
                },
                {
                    "sent": "A little bit more crisp, representation of faces and again you can think of this is just an only extension of PCA and it's obviously the data is not linear here, so you can do.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Much, much better.",
                    "label": 0
                },
                {
                    "sent": "In terms of information retrieval, this is exactly what's happening.",
                    "label": 0
                },
                {
                    "sent": "When I was showing you this image of the Reuters data set, this is again you can take bag of words representation encoded in the two dimensional space and then reconstruct it back.",
                    "label": 1
                },
                {
                    "sent": "So for people who are working in information retrieval communities.",
                    "label": 0
                },
                {
                    "sent": "Models like latent semantic analysis, PCA, version of it is just a linear version, so you can compare linear and nonlinear version and you can see that nonlinear version captures much more irregularities.",
                    "label": 0
                },
                {
                    "sent": "You see?",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of, you know, in terms of performance, these models do quite well.",
                    "label": 0
                },
                {
                    "sent": "One other beautiful idea that came out of these models is something that's called semantic hashing, right?",
                    "label": 1
                },
                {
                    "sent": "And that really is sort of.",
                    "label": 0
                },
                {
                    "sent": "A way of doing information retrieval very quickly.",
                    "label": 0
                },
                {
                    "sent": "The idea is that why don't we learn to map documents or images?",
                    "label": 1
                },
                {
                    "sent": "To the binary space, this semantic binary space and why semantic binary space is because searching in binary space you can do much more efficiently.",
                    "label": 0
                },
                {
                    "sent": "The search can be done much more efficiently, right?",
                    "label": 0
                },
                {
                    "sent": "In fact, if you take a document and you map it to the binary space, you can map it to a particular address.",
                    "label": 0
                },
                {
                    "sent": "Or you can create this sort of the address space, so documents that lie around in the similar address spaces are similar semantically.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can basically do retrieval without doing any kind of search.",
                    "label": 0
                },
                {
                    "sent": "You just look up at the memory address, look at the memory address around that particular point and retrieve those documents of those images and that's.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inspired.",
                    "label": 0
                },
                {
                    "sent": "Lot of people looking at these kinds of models, these PBM's stack of restricted Boltzmann machines, deep belief networks for image retrieval, right?",
                    "label": 0
                },
                {
                    "sent": "And you can do it very very quickly by basically giving an image you achieve similar images based on the binary representation of that image.",
                    "label": 0
                },
                {
                    "sent": "So now there's a lot of papers that you see in speech recognition in computer vision communities where you're trying to find what the binary representation should be.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also do things like learning similarity metric, and after that we'll probably just going to take a break.",
                    "label": 0
                },
                {
                    "sent": "Otherwise I'm worried that we're going to run out of coffee.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is that if you have two images, you can map them down to some semantic space such that you maximizing agreement between those images.",
                    "label": 0
                },
                {
                    "sent": "So you telling the model, I think that this two and these two should be mapped to the same point in the latent space because they mean the same thing.",
                    "label": 0
                },
                {
                    "sent": "Right, and it turns out that this is if you do that this is the representation that you can find.",
                    "label": 0
                },
                {
                    "sent": "These are test images, map to that space and we'll see some of that going forward where one of the things could be an image.",
                    "label": 0
                },
                {
                    "sent": "The other thing could be text.",
                    "label": 0
                },
                {
                    "sent": "The other thing could be speech signal and you essentially mapping them to the same.",
                    "label": 0
                },
                {
                    "sent": "The same semantic space.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know and compare it to something like PCA or something.",
                    "label": 0
                },
                {
                    "sent": "A little cleaner discriminant analysis.",
                    "label": 0
                },
                {
                    "sent": "You can learn much, much better Rep.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stations here the beauty of these kinds of representation is that you can also, if you look at some of the learn features right.",
                    "label": 0
                },
                {
                    "sent": "If I change one of these features, one of these learn features, then three turns into five, which essentially means that you're learning some codes that are class specific, right?",
                    "label": 0
                },
                {
                    "sent": "You basically say that this latent variable is very sensitive to the class boundary.",
                    "label": 0
                },
                {
                    "sent": "So if I change it, threes will turn into 5.",
                    "label": 0
                },
                {
                    "sent": "Some other latent variables modeling thickness of the ink.",
                    "label": 0
                },
                {
                    "sent": "Right, so this three turns into this Lee as a wiggle, one of the Adelaide variables.",
                    "label": 0
                },
                {
                    "sent": "So it has this beautiful property that you can start separating various degrees of variations that you see in the data, because some of them are more sensitive to classes and those are the ones you want to use for recognizing digits.",
                    "label": 0
                },
                {
                    "sent": "Some of them are more sensitive to the thickness of the angle, how people write particular threads, and those you probably want to take away.",
                    "label": 0
                },
                {
                    "sent": "If you want to do recognition.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let me just quickly point out, you know, there's been also some exciting developments in medical imaging for people who are working in this area in particular, trying to recognize different tumor cells.",
                    "label": 0
                },
                {
                    "sent": "This was done by people at Berkeley, Berkeley National Lab.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think the idea is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "You basically constructing you're trying to get first level representation as well as second level representation.",
                    "label": 0
                },
                {
                    "sent": "For finding these little.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I guess you basically trying to recognize each one of those things.",
                    "label": 0
                },
                {
                    "sent": "And you know, it's very new in the biomedical community or medical imaging community.",
                    "label": 0
                },
                {
                    "sent": "These kinds of.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Policy is a lot of opportunities there.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the idea is again you have a reconstruction error, so you're trying to basically reconstruct the data as much as possible at the same time you pulling.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pulling different units together and it turns out that you know the beauty of these kinds of models that they've shown that.",
                    "label": 0
                },
                {
                    "sent": "That in particular, this is Marvin's group at Berkeley National Lab.",
                    "label": 0
                },
                {
                    "sent": "They've shown that if you try to design features manually versus learning the features, you can do much, much better, right?",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "On a particular, I guess this is just the classification accuracies, right?",
                    "label": 0
                },
                {
                    "sent": "So it's sort of a lot of excitement happening in the biomedical communities as well.",
                    "label": 0
                },
                {
                    "sent": "OK, let's take.",
                    "label": 0
                },
                {
                    "sent": "Let's take a break.",
                    "label": 0
                }
            ]
        }
    }
}