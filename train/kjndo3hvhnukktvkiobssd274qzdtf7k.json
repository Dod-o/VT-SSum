{
    "id": "kjndo3hvhnukktvkiobssd274qzdtf7k",
    "title": "Regularized Off-Policy TD-Learning",
    "info": {
        "author": [
            "Bo Liu, Department of Computer Science, University of Massachusetts Amherst"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Science->Cognitive Science",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/machine_liu_learning/",
    "segmentation": [
        [
            "This work considers reinforcement learning in high dimensional space, wherein sparsity model, simplicity and feature selection play important role.",
            "Our policy training is training data from one policy in order to learn the value of another one.",
            "It is important and of wider applications than on policy approaches since they are able to learn while executing an exploratory policy.",
            "Learn from demonstrations and learn multiple tasks in parallel through a single sensory actuator.",
            "Interaction with the Environment TD Learning, which is the most widely used reinforcement learning algorithm, may diverge in our policies things since they do not have convergence guarantee there to overcome this drawback, certain proposed with gradient correction algorithm CDC, which is an all policy convergent.",
            "Our algorithm adding on gradient correction term on the gradient updates of regular TD algorithm.",
            "Regularization is helpful in improving the stability of the methods and admits feature selection.",
            "Meanwhile, linear complexity is desired for reinforcement learning algorithms to scale up to real world problems motivated by these factors we propose wrote the algorithm, which is the first regularised out policy convergence of the algorithm with linear computation complexity."
        ],
        [
            "Healthy, healthy essence of roti algorithm.",
            "The objective function is inspired by the TDC algorithm which in essence is solving linear equation using stochastic gradient.",
            "The objective function is proposed to obtain the L1 regularizer.",
            "Approximate solution of linear equations from by TC Plus an actual sparsity penalty term which is convex but non smooth.",
            "Then with the recent advancements in stochastic optimization theory, we can turn this objective function into its equivalent setpoint by linear representation, which admits the classical regularization.",
            "This regularization has two methods which differ in their objective function and the way to reach sparsity.",
            "The first method uses proximal gradient method to reach sparsity and the other one uses L Infinity projection.",
            "Meanwhile, linear complexity is attained over the sample size and the number of features where matrix decomposition.",
            "The algorithm also have this control.",
            "Learning Extension road JQ Lambda with eligibility traces and temporal abstraction predictions."
        ],
        [
            "The performance evaluation validates that role today has off policy convergence as well as.",
            "Feature selection capability on the famous Star Trek benchmark domain, where today diverges monotonically and identity cannot be applied.",
            "We can see that Taecyeon wrote it converges pretty well.",
            "The control learning performance and feature selection capability of Jake Rd, Columbia is also tested on various domains in comparison with many TD based reinforcement learning algorithms such as large DTC and LSD.",
            "In all this work is attempt of integrating state of art, stochastic optimization and recent reinforcement learning achievements.",
            "For more information, please visit our professor at W. 88 thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This work considers reinforcement learning in high dimensional space, wherein sparsity model, simplicity and feature selection play important role.",
                    "label": 0
                },
                {
                    "sent": "Our policy training is training data from one policy in order to learn the value of another one.",
                    "label": 1
                },
                {
                    "sent": "It is important and of wider applications than on policy approaches since they are able to learn while executing an exploratory policy.",
                    "label": 0
                },
                {
                    "sent": "Learn from demonstrations and learn multiple tasks in parallel through a single sensory actuator.",
                    "label": 0
                },
                {
                    "sent": "Interaction with the Environment TD Learning, which is the most widely used reinforcement learning algorithm, may diverge in our policies things since they do not have convergence guarantee there to overcome this drawback, certain proposed with gradient correction algorithm CDC, which is an all policy convergent.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm adding on gradient correction term on the gradient updates of regular TD algorithm.",
                    "label": 0
                },
                {
                    "sent": "Regularization is helpful in improving the stability of the methods and admits feature selection.",
                    "label": 0
                },
                {
                    "sent": "Meanwhile, linear complexity is desired for reinforcement learning algorithms to scale up to real world problems motivated by these factors we propose wrote the algorithm, which is the first regularised out policy convergence of the algorithm with linear computation complexity.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Healthy, healthy essence of roti algorithm.",
                    "label": 1
                },
                {
                    "sent": "The objective function is inspired by the TDC algorithm which in essence is solving linear equation using stochastic gradient.",
                    "label": 1
                },
                {
                    "sent": "The objective function is proposed to obtain the L1 regularizer.",
                    "label": 0
                },
                {
                    "sent": "Approximate solution of linear equations from by TC Plus an actual sparsity penalty term which is convex but non smooth.",
                    "label": 1
                },
                {
                    "sent": "Then with the recent advancements in stochastic optimization theory, we can turn this objective function into its equivalent setpoint by linear representation, which admits the classical regularization.",
                    "label": 0
                },
                {
                    "sent": "This regularization has two methods which differ in their objective function and the way to reach sparsity.",
                    "label": 0
                },
                {
                    "sent": "The first method uses proximal gradient method to reach sparsity and the other one uses L Infinity projection.",
                    "label": 1
                },
                {
                    "sent": "Meanwhile, linear complexity is attained over the sample size and the number of features where matrix decomposition.",
                    "label": 0
                },
                {
                    "sent": "The algorithm also have this control.",
                    "label": 0
                },
                {
                    "sent": "Learning Extension road JQ Lambda with eligibility traces and temporal abstraction predictions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The performance evaluation validates that role today has off policy convergence as well as.",
                    "label": 0
                },
                {
                    "sent": "Feature selection capability on the famous Star Trek benchmark domain, where today diverges monotonically and identity cannot be applied.",
                    "label": 0
                },
                {
                    "sent": "We can see that Taecyeon wrote it converges pretty well.",
                    "label": 0
                },
                {
                    "sent": "The control learning performance and feature selection capability of Jake Rd, Columbia is also tested on various domains in comparison with many TD based reinforcement learning algorithms such as large DTC and LSD.",
                    "label": 1
                },
                {
                    "sent": "In all this work is attempt of integrating state of art, stochastic optimization and recent reinforcement learning achievements.",
                    "label": 1
                },
                {
                    "sent": "For more information, please visit our professor at W. 88 thanks.",
                    "label": 0
                }
            ]
        }
    }
}