{
    "id": "t3vffz5wzte3elkglqcqrcouloi5bw6d",
    "title": "Mining Commonsense Knowledge From Personal Stories in Internet Weblogs",
    "info": {
        "author": [
            "Andrew S. Gordon, Institute for Creative Technologies, University of Southern California"
        ],
        "published": "June 7, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Information Extraction"
        ]
    },
    "url": "http://videolectures.net/akbc2010_gordon_mckfp/",
    "segmentation": [
        [
            "My name is Andrew Gordon from the University of Southern California.",
            "This Institute for Creative Technologies.",
            "So thanks for inviting me to this conference.",
            "I've been doing work in common sense reasoning for a long time and that are never intersected with this community, so it's a great chance to for me to learn kind of techniques you guys are using, and so forth.",
            "I'm very interested in common sense reasoning of the source you would.",
            "You would common sense knowledge of the sort you would find in things like."
        ],
        [
            "Like so, let's live it.",
            "Look at some some formal common sense reasoning axioms here that you would put.",
            "So here's some common sense psychology knowledge.",
            "If you believe that something would cause the thing that you want, then this causes you to want something that something as well.",
            "Or if you believe that something you want exists in some upcoming time, maybe you're going to be happy about this anticipated happiness of succeeding in your goal.",
            "And you can see immediately there's tons of problems with this kind of knowledge representation.",
            "Work for the purpose of our Community care or this community.",
            "Here one is that this isn't the type of knowledge that she would typically find on the web explicitly in this format.",
            "Norwood, Norwood.",
            "It potentially be useful for the kind of automated knowledge based that are that you guys are working on, because this is stuff that people already know this.",
            "So why would you put it in a big knowledge base so that you could so you could search on it or reason about it.",
            "But in fact there is a lot of reasons that you might want to have this knowledge.",
            "In a knowledge base, but not for human consumption necessarily, but for automated common sense reasoning.",
            "If you want to get a robot to have a good conversation with you or plan or interact with people in the world, then this kind of knowledge would be very useful.",
            "But probably the bigger problem here is that it takes a lot of work to engineer the kind of precision that you need to do.",
            "Automated common sense.",
            "Inferences drawn to prediction or explanation.",
            "Then you need an expert to author this kind of things, and they often get it wrong.",
            "It will be buggy and break and so forth.",
            "So the question is well.",
            "Can we somehow automate this process of collecting this kind of common sense?",
            "Knowledge about to enable automated prediction and explanation and planning and this kind of stuff from web data?",
            "So the thought is?"
        ],
        [
            "Yeah, this really that we're going to have people out there on the web and we're going to suck their brains and we're going to inject that knowledge somehow into the robots that we build, and these may be actual physical robots.",
            "Or maybe their virtual characters in virtual reality environments and that sort of thing.",
            "OK, so let's just and one of the things I want to do today is present.",
            "You the approach that we've taken to this problem and then really get your reactions on it.",
            "And if you had this same problem, how would you apply the techniques that you're familiar with to achieve the same goal?",
            "So the approach that we've taken at the University of Southern California is to take a story driven approach so."
        ],
        [
            "Very interested in storytelling and Internet weblogs.",
            "There's a long tradition in artificial intelligence about how even some have gone to say that all knowledge is stories so that none of the knowledge that you guys are collecting your knowledge bases consist knowledge because all knowledge is experiential and stories.",
            "I believe that actually so but but I'm very interested, so if that's true then there's tons of knowledge out there.",
            "It's in the case of it's in the form of narrative.",
            "OK, so if you look at surveys that have been done my life and experiences is the number one topic in peoples weblogs.",
            "But there's a million new non spam web blog posts each day, so there's plenty of data out there.",
            "For example, there's a great web log aggregator namespinner.com that will give you this kind of data.",
            "In our analysis of that kind of data, we found that you know there's really millions of stories of everyday human experience out there that you could be used as a knowledge base, right?",
            "So not all of the posts that people write to our stories or not, narratives about people's lives, most of the time they're not there often, like commentary on news articles and other opinions, even recipes and things like that.",
            "But roughly 5% of the web blog posts that people actually generate our narratives about experiences in their lives were talking about another day at school, another day at the office.",
            "Something other road trip that they take occasionally get some amazing experience, so I'll leave this.",
            "I live at 5:00 PM on Monday evening and quickly made my way to the hotel.",
            "My electric razor is now in the hands of whoever took it out of my check suitcase, but the jokes on them because I left the plug at home so they might get a weeks worth of shaves out of it.",
            "Not to mention the whole voltage difference.",
            "I was able to purchase a disposable razor at an auto service.",
            "These are little convenience stores mostly run by Koreans down in Paraguay, so it takes a lot of influence even to figure out what's going on this.",
            "On here, but there's a lot of good knowledge here.",
            "We should be able to take advantage of this knowledge for automated reasoning, right?",
            "So we decided."
        ],
        [
            "Let's collect all the stories out there on the web that people are posting on their Internet weblogs.",
            "So we start with the feet of Spinner.",
            "We, of course, we.",
            "You know we sample a large sample random sample of the posts.",
            "We annotate them a story nonstory build a classifier, and then while we have a giant story corpus, so and I work, we've been using linear classifier so that speed up is not a problem.",
            "We over the years, we've gotten better and better precision and recall.",
            "It's not to the accuracy of part of speech, tagging or other types of things that you might see in natural language processing, but the value of doing this kind of work is that you end up with the classifier that's general.",
            "It doesn't matter what domain it's in, your capturing genre of a personal story, and then you can apply that classifier to extremely large corpora.",
            "So the International Conference on Weblogs and social media.",
            "Two years ago now has put out a database of 66 million blog posts.",
            "You can apply this algorithm to those posts and get nearly a million English stories.",
            "Or if you apply it to the daily spinner feed that comes off the web, then you get about 35,000 stories a day in English.",
            "If you apply this kind of English, OK, there's plenty of data out there."
        ],
        [
            "How do we use it for this?",
            "How do we use story as stories as common sense?",
            "As a common sense, knowledge base and kind of what you want is something that does this that you can put in sort of any antecedent that you care about, like he lost control of the car and the system.",
            "Using the stories will do some reasoning and then will output some some inferred consequent, some causal reasoning.",
            "So like he smashed into a tree that's perfectly reasonable.",
            "So over the years different students have been working on this problem with me first using probabilistic language modeling techniques.",
            "Most recently, using discourse relations, that's our current approach actually going to start by talking about the middle one.",
            "Here graduate student read Swanson who just graduated, turn this into a case based reasoning problem.",
            "So he said, look these, just treat these stories as as sequences of sentence is in a total linear order and treat this as a case based retrieval problem.",
            "If you, if he lost control of the car that is the input, then you simply just find the closest sentence in that database that matches that sentence.",
            "That antecedents based on textual similarity.",
            "And then just give the user the back the next sentence that happens in that story as the that's the consequence.",
            "OK, seems like pretty reasonable approach.",
            "So he tried this.",
            "I don't know what this is about.",
            "That's OK, but this is the kind of you know.",
            "He got a lot of interesting inferences, but it wasn't.",
            "It wasn't exactly what we were looking for, from, and in fact it was only really good for this kind of interactive storytelling."
        ],
        [
            "Application I'm going to show you this this story that was sort of generated using this technique and you get an idea of what's going on so.",
            "The weather broke, so we saw a lot of the harbor.",
            "As Victoria grew nearer, the waves grew larger and we ferrals and for sale and turned to run.",
            "We sailed about 9 knots and good trim, but the storm eventually caught up with us.",
            "With its big open cockpit and heavy knows, I didn't like its chances.",
            "That kind of see get almost continually that time of year.",
            "Sure enough, the boat was completely interactive, adequate and we were tossed into the cold ocean.",
            "Everyone in our group of seven tourists, 5 locals, then in Japanese couple were pretty excited about the experience.",
            "The Japanese couple were the ones that saved us, though with our expert swimming abilities.",
            "As far as that goes, there was just the four of us.",
            "The last tourist was lost at sea, never to be found, drowned or murdered.",
            "The bloated, stinking bodies that turn up by the hundreds will look much the same, such as the way with storms like that.",
            "So this is a great story.",
            "It's actually half written by the computer.",
            "OK, So what the user did was write the first sentence and then the system says predicts that the next sentence is what happens after that.",
            "Based using this case based retrieval approach and then the user writes the next sentence and then the system predicts the next sentence after that.",
            "Using this case based retrieval approach.",
            "5 minutes OK.",
            "So if you know remarkably, this seems like there's something here.",
            "You get a lot of the kind of contextual knowledge that she would want.",
            "There's a lot of power in the sentence as a unit of retrieval, so there's some of the problems of disambiguation.",
            "Go away when you look at that sentence level at the multi million scale.",
            "So it seems like there's a lot to harness here for common sense reasoning, but it had some of the features that we don't want, namely that it's a case based reasoning paradigm.",
            "You're going from instance to instance, OK?",
            "And actually what we want here is to do common sense prediction.",
            "We actually want to generalize over all that knowledge of the database in order to make some general expectation of prediction for for new particular situation.",
            "So that's it's interesting.",
            "Now we just gotta figure out how to control control this kind of generative inference process that you get.",
            "So."
        ],
        [
            "Instead of interactive narrative applications, which is what the student focused on here, I'm interested in, then in this kind of common sense, knowledge validation.",
            "This is how we're going to evaluate how well we're doing OK, and it's going to be this kind of multiple choice tests that you're going to give to the computer.",
            "OK, so you'll get a quickie.",
            "I'll give an arbitrary antecedent, like there was only French food on the menu, and the computer has to pick which of these two are both plausible things that would follow from this.",
            "But which ones more possible?",
            "OK, that's the question.",
            "It's a choice of plausible alternatives, so there was only French food on the menu, so I ordered the chimi with me.",
            "So OK, well that's less plausable.",
            "Then I ordered crepes with a glass of wine.",
            "OK, that's that's common sense knowledge every year, so you can get extremely high interrater agreement on that question, but it's extremely difficult from the time of traditional statistical, or at least are word based frequencies approaches that you would get.",
            "Likewise, there was a.",
            "It was a short hike, mostly downhill, so I was exhausted at the end.",
            "OK, now actually would be better if you if you hardly broke a sweat because there's a lot of knowledge in there that you want.",
            "OK, so I didn't have a mobile phone service in France, so I called my office every 15 minutes.",
            "Well, it's probably more likely that I use email to stay in touch with my office.",
            "OK, so this is hard.",
            "This is knowledge that people can agree on, which is the more plausable consequences, but it's very difficult for an automated reasoning system that doesn't have a lot of knowledge, so we're going to use our.",
            "We're going to actually try to build a system that can actually accurately decide between these two options and score basically based on the agreement with the human writer.",
            "When human raters agree very highly on these kind of tasks, so will."
        ],
        [
            "This will be a nice metric to see how well we're doing in our in our.",
            "In our evaluation, so we instead of doing a straight narrative discourse order, which is what the student did in his his work, we started to look at.",
            "Discourse parsing.",
            "OK, so the idea is that we're still going to use text and sentences or phrases in this case, clauses as our unit of both retrieval and meaning, but we're going to instead not look at the linear order of the narration, but rather the temporal and causal order that you can extract from that.",
            "So you can search.",
            "There's certain to.",
            "This is the story from the beginning of this talk.",
            "If we, if we order all the events in the story in there.",
            "In their linear temporal time, then we get some sort of past tense in some state of affairs, as they are right now, and it's a sort of a prediction about the future state, and probably even more importantly, are these causal links.",
            "So if three seems to cause both four and eight, so three, my electric razor is now in the hands of whoever took it causes 4, which is that the joke is on them.",
            "OK, so that helps a little bit a little bit of knowledge also causes 8, which is I was able to purchase a disposable razor at an auto service.",
            "OK, this is the motivation for that.",
            "So the question that we're trying to do here is, if we do this over millions and millions of millions of web log entries, can we then use the both the temporal and the causal knowledge?",
            "That is that we get out of this kind of discourse parsing to do a better job of that automated common sense reasoning.",
            "So to achieve this kind of discourse parsing, we have to turn turn Tord against statistical text parsing techniques and many of the discourse parsers that are out there today that do this.",
            "Kind of like rhetorical structure theory or Penn discourse, treebank style, discourse parsing don't work all that well and they actually don't work nearly as well on web log text as they do on newspaper text.",
            "So there's a lot of technical challenges around both improving the accuracy of discourse parsing and then adapt and also adapting it to the.",
            "The genre, the domain of web log TXT.",
            "But you know, you can imagine that progress along those dimensions are going to give us millions and millions of these pairs of these causal pairs where where you have one clause causes another clause as a textual unit."
        ],
        [
            "So the question is, how do we then take the fact that stealing someone's razor causes you to buy something at an auto service in Paraguay and turn that into aggregate that evidence so that you can make a good prediction in an arbitrary forced multiple choice tests?",
            "So the idea is to do clustering of some sort clustering plus some Bayesian inference mechanism.",
            "So I quickly made my way to the hotel is a clause that then gets clustered with millions of other clauses that you're going to have in this database.",
            "And you can think you know, here's a hierarchical agglomerative clustering.",
            "So you could say it has.",
            "It has similar, you know, has similarity to a bunch of other clauses, but it's still a no.",
            "It's still a unit in the in this giant tree, right?",
            "So and it has as a unit in the tree, it has causal links and temporal links to other clauses in the database of the order of 10s of millions of these kinds of things so.",
            "But if we go up the hierarchy a little bit, we can start to think about aggregating some of those links, so that if we only look at two clauses up at that are clustered together.",
            "And we have two links going out of that.",
            "That database we go higher up the tree.",
            "We were aggregating evidence over more of these entities, and at some point as you go higher and higher up this tree and you have millions and millions of these things, then you start to have links that provide you more evidence for some things rather than others.",
            "OK, and you imagine if the if the depth of it or the height of the tree that you're going up is on the order of thousands or even 10s of thousands in this giant thing of 10s of millions of clauses, then you could actually think about approximating these kind of.",
            "Probabilistic inference is between causal nodes.",
            "OK, so the idea is to then aggregate.",
            "At a good level of abstraction that we'll have to evaluate that empirically and then used more traditional discourse process, our probabilistic techniques in order to do the reasoning.",
            "So I'm done our current approach, then, is too."
        ],
        [
            "Get every personal story that appears in web logs and we're on track to get 6 million of them in 2010.",
            "Our representations are clauses like by discourse connectives.",
            "We're going to aggregate using textual similarity between clauses are inference model is abrasion or other probabilistic approach rather than a case based reasoning approach in our evaluation.",
            "Is this choice of possible consequences?",
            "That's how we're going so."
        ],
        [
            "You very much, I'm Andrew Gordon again.",
            "So something like the discourse battery bank.",
            "Something like this?",
            "Is it the other discussed relations there, so specifically Newswire that now the the discourse discourse treebank is actually a little bit better.",
            "It's it's less fine grained than the rhetorical structure theory.",
            "That had been previously used my last paper that was listed there from Matt Gerber and who's a Michigan PhD student.",
            "He tried to apply the strata.",
            "RST rhetorical structure theory.",
            "Discourse parser to this data and performance was crummy both on the relations that were extracted in the evaluations and also on the inferences that you could then draw from them.",
            "But so the really is prompt us to do more work on genre adaptation and Co. Training and things like that in order to improve parsing.",
            "But right now we've enlisted the aid of some excellent discourse parsing, and especially dependencies, dependency style parsing researchers that to do just exactly this, but.",
            "At the clause level, dependency parsing out the clause level rather than at the word level.",
            "That's our current approach and to annotate massive data.",
            "OK, so we're crowdsourcing out.",
            "To generate an annotated training corpus of of 10s of 10s of thousands of discourse relationships within blog data to in order to identify what are the what are the statistical features that are most predictive of causal links?",
            "So I should take another question maybe.",
            "Are you?",
            "Yeah, so so.",
            "So is so so you had those examples where you have this discriminating path where you had one antecedent into possible consequences.",
            "Right?",
            "Ann is that kind of if you were to use that for crowdsourcing, how do you generate those candidates?",
            "OK, so the idea of using crowdsourcing here is in is to generate a annotated training corpus in order to do the discourse parsing correctly.",
            "OK once, once you have entered and a corpus of social you learn, then a discourse parser that you apply to millions and millions of stories.",
            "To do that I have by asking to anybody I have to ask something like that, right?",
            "Yes, and in fact I did generate those things to start with to give to the to my annotators to my.",
            "Oh right now so the annotators are actually they're not answering questions like this.",
            "They are marking up random web log stories.",
            "OK, personal stories that people right in their web logs, and there are identifying the discourse units or clauses in this case.",
            "And they're actually ordering them temporally and then doing the causal relationships between them.",
            "So that's the task for the web log Turkers that has to be broken down and in many many iterations in order to get high reliability on the on the Turkers, but.",
            "You know that's the challenge, that is how to decompose the task so that you get high accuracy, that's right."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Andrew Gordon from the University of Southern California.",
                    "label": 0
                },
                {
                    "sent": "This Institute for Creative Technologies.",
                    "label": 1
                },
                {
                    "sent": "So thanks for inviting me to this conference.",
                    "label": 0
                },
                {
                    "sent": "I've been doing work in common sense reasoning for a long time and that are never intersected with this community, so it's a great chance to for me to learn kind of techniques you guys are using, and so forth.",
                    "label": 0
                },
                {
                    "sent": "I'm very interested in common sense reasoning of the source you would.",
                    "label": 0
                },
                {
                    "sent": "You would common sense knowledge of the sort you would find in things like.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like so, let's live it.",
                    "label": 0
                },
                {
                    "sent": "Look at some some formal common sense reasoning axioms here that you would put.",
                    "label": 0
                },
                {
                    "sent": "So here's some common sense psychology knowledge.",
                    "label": 0
                },
                {
                    "sent": "If you believe that something would cause the thing that you want, then this causes you to want something that something as well.",
                    "label": 1
                },
                {
                    "sent": "Or if you believe that something you want exists in some upcoming time, maybe you're going to be happy about this anticipated happiness of succeeding in your goal.",
                    "label": 0
                },
                {
                    "sent": "And you can see immediately there's tons of problems with this kind of knowledge representation.",
                    "label": 0
                },
                {
                    "sent": "Work for the purpose of our Community care or this community.",
                    "label": 0
                },
                {
                    "sent": "Here one is that this isn't the type of knowledge that she would typically find on the web explicitly in this format.",
                    "label": 0
                },
                {
                    "sent": "Norwood, Norwood.",
                    "label": 0
                },
                {
                    "sent": "It potentially be useful for the kind of automated knowledge based that are that you guys are working on, because this is stuff that people already know this.",
                    "label": 0
                },
                {
                    "sent": "So why would you put it in a big knowledge base so that you could so you could search on it or reason about it.",
                    "label": 0
                },
                {
                    "sent": "But in fact there is a lot of reasons that you might want to have this knowledge.",
                    "label": 0
                },
                {
                    "sent": "In a knowledge base, but not for human consumption necessarily, but for automated common sense reasoning.",
                    "label": 0
                },
                {
                    "sent": "If you want to get a robot to have a good conversation with you or plan or interact with people in the world, then this kind of knowledge would be very useful.",
                    "label": 0
                },
                {
                    "sent": "But probably the bigger problem here is that it takes a lot of work to engineer the kind of precision that you need to do.",
                    "label": 0
                },
                {
                    "sent": "Automated common sense.",
                    "label": 0
                },
                {
                    "sent": "Inferences drawn to prediction or explanation.",
                    "label": 0
                },
                {
                    "sent": "Then you need an expert to author this kind of things, and they often get it wrong.",
                    "label": 0
                },
                {
                    "sent": "It will be buggy and break and so forth.",
                    "label": 0
                },
                {
                    "sent": "So the question is well.",
                    "label": 0
                },
                {
                    "sent": "Can we somehow automate this process of collecting this kind of common sense?",
                    "label": 0
                },
                {
                    "sent": "Knowledge about to enable automated prediction and explanation and planning and this kind of stuff from web data?",
                    "label": 0
                },
                {
                    "sent": "So the thought is?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, this really that we're going to have people out there on the web and we're going to suck their brains and we're going to inject that knowledge somehow into the robots that we build, and these may be actual physical robots.",
                    "label": 0
                },
                {
                    "sent": "Or maybe their virtual characters in virtual reality environments and that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's just and one of the things I want to do today is present.",
                    "label": 0
                },
                {
                    "sent": "You the approach that we've taken to this problem and then really get your reactions on it.",
                    "label": 0
                },
                {
                    "sent": "And if you had this same problem, how would you apply the techniques that you're familiar with to achieve the same goal?",
                    "label": 0
                },
                {
                    "sent": "So the approach that we've taken at the University of Southern California is to take a story driven approach so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very interested in storytelling and Internet weblogs.",
                    "label": 0
                },
                {
                    "sent": "There's a long tradition in artificial intelligence about how even some have gone to say that all knowledge is stories so that none of the knowledge that you guys are collecting your knowledge bases consist knowledge because all knowledge is experiential and stories.",
                    "label": 0
                },
                {
                    "sent": "I believe that actually so but but I'm very interested, so if that's true then there's tons of knowledge out there.",
                    "label": 0
                },
                {
                    "sent": "It's in the case of it's in the form of narrative.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you look at surveys that have been done my life and experiences is the number one topic in peoples weblogs.",
                    "label": 0
                },
                {
                    "sent": "But there's a million new non spam web blog posts each day, so there's plenty of data out there.",
                    "label": 0
                },
                {
                    "sent": "For example, there's a great web log aggregator namespinner.com that will give you this kind of data.",
                    "label": 0
                },
                {
                    "sent": "In our analysis of that kind of data, we found that you know there's really millions of stories of everyday human experience out there that you could be used as a knowledge base, right?",
                    "label": 0
                },
                {
                    "sent": "So not all of the posts that people write to our stories or not, narratives about people's lives, most of the time they're not there often, like commentary on news articles and other opinions, even recipes and things like that.",
                    "label": 0
                },
                {
                    "sent": "But roughly 5% of the web blog posts that people actually generate our narratives about experiences in their lives were talking about another day at school, another day at the office.",
                    "label": 0
                },
                {
                    "sent": "Something other road trip that they take occasionally get some amazing experience, so I'll leave this.",
                    "label": 0
                },
                {
                    "sent": "I live at 5:00 PM on Monday evening and quickly made my way to the hotel.",
                    "label": 1
                },
                {
                    "sent": "My electric razor is now in the hands of whoever took it out of my check suitcase, but the jokes on them because I left the plug at home so they might get a weeks worth of shaves out of it.",
                    "label": 1
                },
                {
                    "sent": "Not to mention the whole voltage difference.",
                    "label": 1
                },
                {
                    "sent": "I was able to purchase a disposable razor at an auto service.",
                    "label": 0
                },
                {
                    "sent": "These are little convenience stores mostly run by Koreans down in Paraguay, so it takes a lot of influence even to figure out what's going on this.",
                    "label": 0
                },
                {
                    "sent": "On here, but there's a lot of good knowledge here.",
                    "label": 0
                },
                {
                    "sent": "We should be able to take advantage of this knowledge for automated reasoning, right?",
                    "label": 0
                },
                {
                    "sent": "So we decided.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's collect all the stories out there on the web that people are posting on their Internet weblogs.",
                    "label": 0
                },
                {
                    "sent": "So we start with the feet of Spinner.",
                    "label": 0
                },
                {
                    "sent": "We, of course, we.",
                    "label": 0
                },
                {
                    "sent": "You know we sample a large sample random sample of the posts.",
                    "label": 0
                },
                {
                    "sent": "We annotate them a story nonstory build a classifier, and then while we have a giant story corpus, so and I work, we've been using linear classifier so that speed up is not a problem.",
                    "label": 0
                },
                {
                    "sent": "We over the years, we've gotten better and better precision and recall.",
                    "label": 0
                },
                {
                    "sent": "It's not to the accuracy of part of speech, tagging or other types of things that you might see in natural language processing, but the value of doing this kind of work is that you end up with the classifier that's general.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter what domain it's in, your capturing genre of a personal story, and then you can apply that classifier to extremely large corpora.",
                    "label": 0
                },
                {
                    "sent": "So the International Conference on Weblogs and social media.",
                    "label": 0
                },
                {
                    "sent": "Two years ago now has put out a database of 66 million blog posts.",
                    "label": 0
                },
                {
                    "sent": "You can apply this algorithm to those posts and get nearly a million English stories.",
                    "label": 0
                },
                {
                    "sent": "Or if you apply it to the daily spinner feed that comes off the web, then you get about 35,000 stories a day in English.",
                    "label": 0
                },
                {
                    "sent": "If you apply this kind of English, OK, there's plenty of data out there.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do we use it for this?",
                    "label": 0
                },
                {
                    "sent": "How do we use story as stories as common sense?",
                    "label": 1
                },
                {
                    "sent": "As a common sense, knowledge base and kind of what you want is something that does this that you can put in sort of any antecedent that you care about, like he lost control of the car and the system.",
                    "label": 1
                },
                {
                    "sent": "Using the stories will do some reasoning and then will output some some inferred consequent, some causal reasoning.",
                    "label": 1
                },
                {
                    "sent": "So like he smashed into a tree that's perfectly reasonable.",
                    "label": 1
                },
                {
                    "sent": "So over the years different students have been working on this problem with me first using probabilistic language modeling techniques.",
                    "label": 0
                },
                {
                    "sent": "Most recently, using discourse relations, that's our current approach actually going to start by talking about the middle one.",
                    "label": 0
                },
                {
                    "sent": "Here graduate student read Swanson who just graduated, turn this into a case based reasoning problem.",
                    "label": 0
                },
                {
                    "sent": "So he said, look these, just treat these stories as as sequences of sentence is in a total linear order and treat this as a case based retrieval problem.",
                    "label": 0
                },
                {
                    "sent": "If you, if he lost control of the car that is the input, then you simply just find the closest sentence in that database that matches that sentence.",
                    "label": 0
                },
                {
                    "sent": "That antecedents based on textual similarity.",
                    "label": 0
                },
                {
                    "sent": "And then just give the user the back the next sentence that happens in that story as the that's the consequence.",
                    "label": 0
                },
                {
                    "sent": "OK, seems like pretty reasonable approach.",
                    "label": 0
                },
                {
                    "sent": "So he tried this.",
                    "label": 0
                },
                {
                    "sent": "I don't know what this is about.",
                    "label": 0
                },
                {
                    "sent": "That's OK, but this is the kind of you know.",
                    "label": 0
                },
                {
                    "sent": "He got a lot of interesting inferences, but it wasn't.",
                    "label": 0
                },
                {
                    "sent": "It wasn't exactly what we were looking for, from, and in fact it was only really good for this kind of interactive storytelling.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Application I'm going to show you this this story that was sort of generated using this technique and you get an idea of what's going on so.",
                    "label": 0
                },
                {
                    "sent": "The weather broke, so we saw a lot of the harbor.",
                    "label": 1
                },
                {
                    "sent": "As Victoria grew nearer, the waves grew larger and we ferrals and for sale and turned to run.",
                    "label": 1
                },
                {
                    "sent": "We sailed about 9 knots and good trim, but the storm eventually caught up with us.",
                    "label": 1
                },
                {
                    "sent": "With its big open cockpit and heavy knows, I didn't like its chances.",
                    "label": 1
                },
                {
                    "sent": "That kind of see get almost continually that time of year.",
                    "label": 0
                },
                {
                    "sent": "Sure enough, the boat was completely interactive, adequate and we were tossed into the cold ocean.",
                    "label": 0
                },
                {
                    "sent": "Everyone in our group of seven tourists, 5 locals, then in Japanese couple were pretty excited about the experience.",
                    "label": 0
                },
                {
                    "sent": "The Japanese couple were the ones that saved us, though with our expert swimming abilities.",
                    "label": 1
                },
                {
                    "sent": "As far as that goes, there was just the four of us.",
                    "label": 0
                },
                {
                    "sent": "The last tourist was lost at sea, never to be found, drowned or murdered.",
                    "label": 1
                },
                {
                    "sent": "The bloated, stinking bodies that turn up by the hundreds will look much the same, such as the way with storms like that.",
                    "label": 1
                },
                {
                    "sent": "So this is a great story.",
                    "label": 0
                },
                {
                    "sent": "It's actually half written by the computer.",
                    "label": 0
                },
                {
                    "sent": "OK, So what the user did was write the first sentence and then the system says predicts that the next sentence is what happens after that.",
                    "label": 0
                },
                {
                    "sent": "Based using this case based retrieval approach and then the user writes the next sentence and then the system predicts the next sentence after that.",
                    "label": 0
                },
                {
                    "sent": "Using this case based retrieval approach.",
                    "label": 0
                },
                {
                    "sent": "5 minutes OK.",
                    "label": 0
                },
                {
                    "sent": "So if you know remarkably, this seems like there's something here.",
                    "label": 0
                },
                {
                    "sent": "You get a lot of the kind of contextual knowledge that she would want.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of power in the sentence as a unit of retrieval, so there's some of the problems of disambiguation.",
                    "label": 0
                },
                {
                    "sent": "Go away when you look at that sentence level at the multi million scale.",
                    "label": 0
                },
                {
                    "sent": "So it seems like there's a lot to harness here for common sense reasoning, but it had some of the features that we don't want, namely that it's a case based reasoning paradigm.",
                    "label": 0
                },
                {
                    "sent": "You're going from instance to instance, OK?",
                    "label": 0
                },
                {
                    "sent": "And actually what we want here is to do common sense prediction.",
                    "label": 0
                },
                {
                    "sent": "We actually want to generalize over all that knowledge of the database in order to make some general expectation of prediction for for new particular situation.",
                    "label": 0
                },
                {
                    "sent": "So that's it's interesting.",
                    "label": 0
                },
                {
                    "sent": "Now we just gotta figure out how to control control this kind of generative inference process that you get.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instead of interactive narrative applications, which is what the student focused on here, I'm interested in, then in this kind of common sense, knowledge validation.",
                    "label": 0
                },
                {
                    "sent": "This is how we're going to evaluate how well we're doing OK, and it's going to be this kind of multiple choice tests that you're going to give to the computer.",
                    "label": 0
                },
                {
                    "sent": "OK, so you'll get a quickie.",
                    "label": 0
                },
                {
                    "sent": "I'll give an arbitrary antecedent, like there was only French food on the menu, and the computer has to pick which of these two are both plausible things that would follow from this.",
                    "label": 0
                },
                {
                    "sent": "But which ones more possible?",
                    "label": 0
                },
                {
                    "sent": "OK, that's the question.",
                    "label": 0
                },
                {
                    "sent": "It's a choice of plausible alternatives, so there was only French food on the menu, so I ordered the chimi with me.",
                    "label": 1
                },
                {
                    "sent": "So OK, well that's less plausable.",
                    "label": 0
                },
                {
                    "sent": "Then I ordered crepes with a glass of wine.",
                    "label": 0
                },
                {
                    "sent": "OK, that's that's common sense knowledge every year, so you can get extremely high interrater agreement on that question, but it's extremely difficult from the time of traditional statistical, or at least are word based frequencies approaches that you would get.",
                    "label": 0
                },
                {
                    "sent": "Likewise, there was a.",
                    "label": 0
                },
                {
                    "sent": "It was a short hike, mostly downhill, so I was exhausted at the end.",
                    "label": 1
                },
                {
                    "sent": "OK, now actually would be better if you if you hardly broke a sweat because there's a lot of knowledge in there that you want.",
                    "label": 1
                },
                {
                    "sent": "OK, so I didn't have a mobile phone service in France, so I called my office every 15 minutes.",
                    "label": 0
                },
                {
                    "sent": "Well, it's probably more likely that I use email to stay in touch with my office.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is hard.",
                    "label": 0
                },
                {
                    "sent": "This is knowledge that people can agree on, which is the more plausable consequences, but it's very difficult for an automated reasoning system that doesn't have a lot of knowledge, so we're going to use our.",
                    "label": 0
                },
                {
                    "sent": "We're going to actually try to build a system that can actually accurately decide between these two options and score basically based on the agreement with the human writer.",
                    "label": 0
                },
                {
                    "sent": "When human raters agree very highly on these kind of tasks, so will.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This will be a nice metric to see how well we're doing in our in our.",
                    "label": 0
                },
                {
                    "sent": "In our evaluation, so we instead of doing a straight narrative discourse order, which is what the student did in his his work, we started to look at.",
                    "label": 0
                },
                {
                    "sent": "Discourse parsing.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is that we're still going to use text and sentences or phrases in this case, clauses as our unit of both retrieval and meaning, but we're going to instead not look at the linear order of the narration, but rather the temporal and causal order that you can extract from that.",
                    "label": 0
                },
                {
                    "sent": "So you can search.",
                    "label": 0
                },
                {
                    "sent": "There's certain to.",
                    "label": 0
                },
                {
                    "sent": "This is the story from the beginning of this talk.",
                    "label": 0
                },
                {
                    "sent": "If we, if we order all the events in the story in there.",
                    "label": 0
                },
                {
                    "sent": "In their linear temporal time, then we get some sort of past tense in some state of affairs, as they are right now, and it's a sort of a prediction about the future state, and probably even more importantly, are these causal links.",
                    "label": 0
                },
                {
                    "sent": "So if three seems to cause both four and eight, so three, my electric razor is now in the hands of whoever took it causes 4, which is that the joke is on them.",
                    "label": 1
                },
                {
                    "sent": "OK, so that helps a little bit a little bit of knowledge also causes 8, which is I was able to purchase a disposable razor at an auto service.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the motivation for that.",
                    "label": 0
                },
                {
                    "sent": "So the question that we're trying to do here is, if we do this over millions and millions of millions of web log entries, can we then use the both the temporal and the causal knowledge?",
                    "label": 0
                },
                {
                    "sent": "That is that we get out of this kind of discourse parsing to do a better job of that automated common sense reasoning.",
                    "label": 0
                },
                {
                    "sent": "So to achieve this kind of discourse parsing, we have to turn turn Tord against statistical text parsing techniques and many of the discourse parsers that are out there today that do this.",
                    "label": 0
                },
                {
                    "sent": "Kind of like rhetorical structure theory or Penn discourse, treebank style, discourse parsing don't work all that well and they actually don't work nearly as well on web log text as they do on newspaper text.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of technical challenges around both improving the accuracy of discourse parsing and then adapt and also adapting it to the.",
                    "label": 0
                },
                {
                    "sent": "The genre, the domain of web log TXT.",
                    "label": 0
                },
                {
                    "sent": "But you know, you can imagine that progress along those dimensions are going to give us millions and millions of these pairs of these causal pairs where where you have one clause causes another clause as a textual unit.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is, how do we then take the fact that stealing someone's razor causes you to buy something at an auto service in Paraguay and turn that into aggregate that evidence so that you can make a good prediction in an arbitrary forced multiple choice tests?",
                    "label": 0
                },
                {
                    "sent": "So the idea is to do clustering of some sort clustering plus some Bayesian inference mechanism.",
                    "label": 0
                },
                {
                    "sent": "So I quickly made my way to the hotel is a clause that then gets clustered with millions of other clauses that you're going to have in this database.",
                    "label": 1
                },
                {
                    "sent": "And you can think you know, here's a hierarchical agglomerative clustering.",
                    "label": 0
                },
                {
                    "sent": "So you could say it has.",
                    "label": 0
                },
                {
                    "sent": "It has similar, you know, has similarity to a bunch of other clauses, but it's still a no.",
                    "label": 0
                },
                {
                    "sent": "It's still a unit in the in this giant tree, right?",
                    "label": 0
                },
                {
                    "sent": "So and it has as a unit in the tree, it has causal links and temporal links to other clauses in the database of the order of 10s of millions of these kinds of things so.",
                    "label": 0
                },
                {
                    "sent": "But if we go up the hierarchy a little bit, we can start to think about aggregating some of those links, so that if we only look at two clauses up at that are clustered together.",
                    "label": 0
                },
                {
                    "sent": "And we have two links going out of that.",
                    "label": 0
                },
                {
                    "sent": "That database we go higher up the tree.",
                    "label": 0
                },
                {
                    "sent": "We were aggregating evidence over more of these entities, and at some point as you go higher and higher up this tree and you have millions and millions of these things, then you start to have links that provide you more evidence for some things rather than others.",
                    "label": 0
                },
                {
                    "sent": "OK, and you imagine if the if the depth of it or the height of the tree that you're going up is on the order of thousands or even 10s of thousands in this giant thing of 10s of millions of clauses, then you could actually think about approximating these kind of.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic inference is between causal nodes.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is to then aggregate.",
                    "label": 0
                },
                {
                    "sent": "At a good level of abstraction that we'll have to evaluate that empirically and then used more traditional discourse process, our probabilistic techniques in order to do the reasoning.",
                    "label": 0
                },
                {
                    "sent": "So I'm done our current approach, then, is too.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get every personal story that appears in web logs and we're on track to get 6 million of them in 2010.",
                    "label": 1
                },
                {
                    "sent": "Our representations are clauses like by discourse connectives.",
                    "label": 1
                },
                {
                    "sent": "We're going to aggregate using textual similarity between clauses are inference model is abrasion or other probabilistic approach rather than a case based reasoning approach in our evaluation.",
                    "label": 1
                },
                {
                    "sent": "Is this choice of possible consequences?",
                    "label": 0
                },
                {
                    "sent": "That's how we're going so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You very much, I'm Andrew Gordon again.",
                    "label": 0
                },
                {
                    "sent": "So something like the discourse battery bank.",
                    "label": 0
                },
                {
                    "sent": "Something like this?",
                    "label": 0
                },
                {
                    "sent": "Is it the other discussed relations there, so specifically Newswire that now the the discourse discourse treebank is actually a little bit better.",
                    "label": 0
                },
                {
                    "sent": "It's it's less fine grained than the rhetorical structure theory.",
                    "label": 0
                },
                {
                    "sent": "That had been previously used my last paper that was listed there from Matt Gerber and who's a Michigan PhD student.",
                    "label": 0
                },
                {
                    "sent": "He tried to apply the strata.",
                    "label": 0
                },
                {
                    "sent": "RST rhetorical structure theory.",
                    "label": 0
                },
                {
                    "sent": "Discourse parser to this data and performance was crummy both on the relations that were extracted in the evaluations and also on the inferences that you could then draw from them.",
                    "label": 0
                },
                {
                    "sent": "But so the really is prompt us to do more work on genre adaptation and Co. Training and things like that in order to improve parsing.",
                    "label": 0
                },
                {
                    "sent": "But right now we've enlisted the aid of some excellent discourse parsing, and especially dependencies, dependency style parsing researchers that to do just exactly this, but.",
                    "label": 0
                },
                {
                    "sent": "At the clause level, dependency parsing out the clause level rather than at the word level.",
                    "label": 0
                },
                {
                    "sent": "That's our current approach and to annotate massive data.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're crowdsourcing out.",
                    "label": 0
                },
                {
                    "sent": "To generate an annotated training corpus of of 10s of 10s of thousands of discourse relationships within blog data to in order to identify what are the what are the statistical features that are most predictive of causal links?",
                    "label": 0
                },
                {
                    "sent": "So I should take another question maybe.",
                    "label": 0
                },
                {
                    "sent": "Are you?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so.",
                    "label": 0
                },
                {
                    "sent": "So is so so you had those examples where you have this discriminating path where you had one antecedent into possible consequences.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Ann is that kind of if you were to use that for crowdsourcing, how do you generate those candidates?",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea of using crowdsourcing here is in is to generate a annotated training corpus in order to do the discourse parsing correctly.",
                    "label": 0
                },
                {
                    "sent": "OK once, once you have entered and a corpus of social you learn, then a discourse parser that you apply to millions and millions of stories.",
                    "label": 0
                },
                {
                    "sent": "To do that I have by asking to anybody I have to ask something like that, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, and in fact I did generate those things to start with to give to the to my annotators to my.",
                    "label": 0
                },
                {
                    "sent": "Oh right now so the annotators are actually they're not answering questions like this.",
                    "label": 0
                },
                {
                    "sent": "They are marking up random web log stories.",
                    "label": 0
                },
                {
                    "sent": "OK, personal stories that people right in their web logs, and there are identifying the discourse units or clauses in this case.",
                    "label": 0
                },
                {
                    "sent": "And they're actually ordering them temporally and then doing the causal relationships between them.",
                    "label": 0
                },
                {
                    "sent": "So that's the task for the web log Turkers that has to be broken down and in many many iterations in order to get high reliability on the on the Turkers, but.",
                    "label": 0
                },
                {
                    "sent": "You know that's the challenge, that is how to decompose the task so that you get high accuracy, that's right.",
                    "label": 0
                }
            ]
        }
    }
}