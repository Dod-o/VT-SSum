{
    "id": "awpqmz5d6z74sqk36f4lg4r5gkhicugl",
    "title": "Online Learning, Regret Minimization, and Game Theory",
    "info": {
        "author": [
            "Avrim Blum, Carnegie Mellon University"
        ],
        "published": "May 7, 2008",
        "recorded": "March 2008",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Mathematics->Game Theory"
        ]
    },
    "url": "http://videolectures.net/mlss08au_blum_org/",
    "segmentation": [
        [
            "Let me just start by.",
            "I really like to thank the organizers for inviting me.",
            "I thank Marcus and all the all the organizers.",
            "Really been.",
            "Fantastic week and.",
            "Sorry could have been here for the for the first week.",
            "Part time it's really intense.",
            "An amazing place and getting the weather all set, you know I mean.",
            "Philia great so.",
            "Let's see, actually, first of all, so so I guess we need those lights for the camera, is that right so?",
            "Perfect.",
            "They won't work.",
            "So then, then the camera won't.",
            "You don't think so.",
            "Can you can you sleep well?",
            "I mean, I can see it's good question of people.",
            "OK. OK. Light on it.",
            "OK OK OK. Alright.",
            "At CMU, nothing starts until five after but.",
            "So I have to.",
            "Use up time in the beginning, so.",
            "I may be talking about.",
            "Two different things, so it most of this tutorial will be an on line learning and game theory and connections between the two and then the.",
            "The second half of tomorrow.",
            "So maybe the last two hours, hour and a half or so will be on a different topic about a kind of a different way of thinking about kernel functions and more general similarity functions and.",
            "And learning with us so just.",
            "Has nothing really to do with it.",
            "With the first topic, but I like them both, so I want to talk about them.",
            "OK. OK, and it says so, so we'll have.",
            "We talked about several things and I'll be your guide today."
        ],
        [
            "OK, and and the plan for it for the tour is our first stop.",
            "We will be looking at online learning online learning algorithms.",
            "I'll talk about something called the notion of regret and trying to minimize regret in online learning.",
            "And there's a very nice problem.",
            "Abstract certain aspects of this, called the problem of combining expert advice and we'll talk about that.",
            "Then we'll kind of switch a little bit and talk about game theory.",
            "We already see in the first part some game theory connections talk about game theory, minimax optimality, Nash equilibrium, and connections to the first part.",
            "How many people have seen the notion of minimax optimality?",
            "How many people have seen the notion of Nash equilibrium?",
            "OK. How many people have seen the proof of Minimax theorem?",
            "Cat.",
            "So actually we'll get that for free from some of the analysis of online learning.",
            "Then then we'll go to some little bit more.",
            "Concepts in game theory, notion of correlated equilibrium, and this particularly brings out some connections.",
            "Even more connections between these first 2 topics.",
            "Talk about some some recent research as well.",
            "And then at the end we'll do something completely different.",
            "Talking about learning and clustering with similarity functions and kernel functions and more general measures of similarity.",
            "OK, so that's sort of the outline for today and tomorrow.",
            "So today will kind of the first 2 are the bigger ones will kind of go through about here today and then somewhere here and then this rest tomorrow.",
            "Let's see I guess one of the things I should say at the beginning.",
            "So the usual way to do you know three or six hour tutorials.",
            "You start slow and get people into it, and then you kind of towards the end.",
            "You hit people over the head with the difficult stuff.",
            "So it's going to go a little backwards this time.",
            "OK, so the hardest part is going to be the first hour or so, so if you can survive that first hour, you're home free after that, OK?",
            "So just wanna let you know up front.",
            "OK, so let's go."
        ],
        [
            "Oh so let me just mention just some some books references.",
            "So they aren't in the printed notes, so there are various changes that have been made to the slides since the time Marcus asked for them.",
            "And now since lot of us do things at the last minute.",
            "So just to mention some books and references, so there's a very nice new book just came out on algorithmic game theory, and it's it's really the collection of chapters written by different researchers in different aspects of algorithms and game theory.",
            "So the whole area.",
            "So game theory is a very old area coming from economics in computer science.",
            "There's been a lot of recent interest in.",
            "Connections between algorithms and game theory because many of the big systems that that.",
            "Many of you know the Internet, web, various auctions and so forth.",
            "They involve multiple agents interacting who each have their own interests, and so when you design the system, you can't just design it as if you have some central controller that's going to decide how everyone is going to talk to each other.",
            "They're all they're going to do things based on what's their own motivations, and so when you keep when you have a problem where you have to keep in mind people's motivations that brings it into the context of game theory.",
            "So there's been a lot of interest, and I would say the last five to.",
            "6 seven years, maybe a little older, but really the mostly most of the work has been the last five or six years in connection between computer science and game theory.",
            "And so this book.",
            "Each chapter is by an expert in a different area.",
            "Chapter 4 is a chapter that I wrote together with another researcher.",
            "Yishai Mansour on learning, regret minimization and equilibrium, so some of the things I'll be talking about.",
            "It's on my web page, but you can also buy the book.",
            "I don't get any money from it.",
            "There's a nice book prediction learning in games by a Nicolo, Shesa, Bianchi, and Gabor Lugosi.",
            "It's also on similar topics to what I'll be talking about.",
            "It's it's pretty mathematical and, well, I'm pretty mathematical and it's pretty mathematical for me, but.",
            "But nonetheless, you know it depends where you're coming from.",
            "It's quite nice book.",
            "Also resend, I also have course notes that are on the web, so I happened to buy the domain name machine learning.com while it was available and not they have a company but I can hook up my course notes to it.",
            "Easy to remember them.",
            "I should have bought McDonald's or something.",
            "But anyway, OK."
        ],
        [
            "OK, so our first stop so online learning, minimizing regret and combining expert advice.",
            "So let me tell you what these mean.",
            "OK, so let me just."
        ],
        [
            "Actually with with kind of a motivating setting.",
            "So imagine that every morning you need to pick one of N possible routes to drive to work.",
            "OK, so for me I live in the suburbs and I have to drive to work each day and there are various ways I could go.",
            "OK. Fine, you know from a computer science perspective.",
            "That's fine, you tell me the length of the edges and I can solve for the shortest path problem.",
            "We've got plenty of algorithms for solving for shortest paths great.",
            "The problem is the traffic is different each day, and it's not clear apriori which is going to be best, because these costs of the edge is the amount of time it takes to go from one place.",
            "So it depends on the traffic, and I don't know in advance what that's going to be, so I have to make a decision before I get to see what the edge lengths are.",
            "OK, so I pick a path, so imagine that here you are.",
            "You gotta pick a way to go to work.",
            "We choose a path and you find out how long it took you, and we took me 32 minutes to get from home to work.",
            "And perhaps I also find out by asking my friends how long other routes would have taken.",
            "Or maybe not.",
            "Maybe I just get the information.",
            "I just get the feedback from the route I took.",
            "We can consider these two scenarios.",
            "Good, so that was day one and then, then the next day I pick a different path.",
            "Maybe I pick a different path and it takes me 27 minutes now to take 27 minutes because it was because of the past being different.",
            "Or maybe just because there was less traffic that day.",
            "Who knows when the next day I pick another route, next day pick another route next to pick another everyday.",
            "I gotta pick a way to go to work.",
            "And a natural question to ask here is, is there a strategy that I can use for choosing routes with the following property, but in the long run as I everyday go to work, whatever the sequence of traffic patterns happens to be, so I don't want to make an assumption that you know days or drawn from some distribution or something.",
            "It's just every day there's some traffic pattern and whatever that sequence happens to be.",
            "I'd like to have the property that I've done nearly as well as the best fixed route in hindsight, so nobody can tell me.",
            "Oh, I always go this way and.",
            "I did much better than you do.",
            "I would like to have the property that I'm doing at least nearly as well as the best fixed route that I could have chosen in hindsight and use that route every day.",
            "OK, so so one natural question.",
            "So can we achieve a guarantee like this?",
            "It turns out the answer is yes.",
            "Otherwise, I wouldn't talk about it.",
            "And so we're going to look at how we can do things like this.",
            "And actually results of this sort have been known for quite a while, but we'll get to that.",
            "So good, so let's just before we get into how to do this, let's just think a little bit about this kind of."
        ],
        [
            "Problem.",
            "OK, so a little bit more generally the setting is.",
            "Is how a game theorist will look at this, so we have our algorithm.",
            "We have some options which route to take and we can conceptually think of these as rows of a matrix.",
            "This is route #1 #2 number, just conceptually writing each choice we could make as a row of some matrix.",
            "And.",
            "There's also different possible traffic patterns that might exist in the world.",
            "We can conceptually think of them as different columns.",
            "Maybe it's infinitely many.",
            "You know who knows.",
            "And So what?",
            "And the way this game is working, is it every day our algorithm picks a row, our algorithm picks a route.",
            "The world picks a column.",
            "Now with, these columns are going to represent is how much it costs to take each route.",
            "So this column we do have some numbers that says if we take this route it takes us 32 minutes.",
            "This other route would have taken 27 minutes.",
            "This other route would have taken 45 minutes, so I think this filled with numbers and so we pick the route the world picks the.",
            "The cost vector, how much we would have paid had we chosen each of the different possibilities, so we pick the role.",
            "I fix the column and then we pay the cost.",
            "Here we pay the cost for the action we chose.",
            "And then we get his feedback.",
            "Either the whole column if we can ask our friends say, Oh well, you went this way.",
            "How long did that way take?",
            "How long that way take?",
            "We get the whole column?",
            "Or perhaps we only get our own cost only this entry here in what's called the bandit model.",
            "It's called the bandit.",
            "My listening to slot machines being one armed bandits, but.",
            "Like picking slot machines but.",
            "This is called the bandit model.",
            "We talked about how you did oh, I missed the first week, OK?",
            "Alright, and in order for this game to make sense, you need to assume that there is some bound on the maximum cost that you could incur.",
            "So if it's possible that you know you're driving to work one day and a piano falls in your head and you die, there's no more learning to do so.",
            "If there's like some, you know infinite cost here, that's it.",
            "I mean, you're OK, so you have to assume some kind of bound.",
            "Otherwise this.",
            "Hope and So what we're going to assume is all the costs are between zero and one, let's say zero in one hour.",
            "Yep."
        ],
        [
            "OK, so given this setup, there's a notion of regret and let me just define that that the average regret in T time steps is the gap between the average amount our average cost we pay per day and the average per day cost of the best best fixed choice in hindsight.",
            "So imagine that you know so far on average we've been taking us 2032 minutes to get to work per day, and the best fixed path in hindsight was 29 minutes.",
            "That gap, 3 minutes.",
            "That would be our regret.",
            "That's how much we regret not having chosen that nice other fixed path.",
            "We regret it by 3 minutes per day.",
            "And what we'd like is to have an algorithm where this quantity goes to 0 or maybe even less than 0.",
            "Maybe we can even do better than the best fixed path if we're lucky.",
            "OK, but we'd like this to go to zero or better as he gets large.",
            "And if you can do that, that's called a no regret algorithm.",
            "OK, so this is just this is kind of name definition will need, so no regret algorithm is one that has the ability to make the drive this to zero, and so then the regret is the difference in your average cost per trial per day, and the best fixed choice in hindsight.",
            "And it's a little misleading to call them no regret 'cause these aren't necessarily just.",
            "Going to 0 but not exactly at 0.",
            "Alright so there."
        ],
        [
            "So let's just think about this a little bit.",
            "You can read that so intuition and properties of no great algorithms.",
            "OK, the title is done.",
            "OK, so let's look at a really small example.",
            "Let's imagine that there are two ways that we can get to work.",
            "OK, so.",
            "That's good, OK, so let's imagine that there are two ways we can get to work OK, the top way in the bottom line.",
            "And let's imagine that there's two kinds of traffic pattern in traffic pattern number one.",
            "The top way is very expensive, takes one hour and the bottom line is cheap.",
            "Teleport for free and the other.",
            "The other kind of traffic pattern is the other way around, like maybe they're doing construction.",
            "You know it's not that far, it's just that they are kind of holding these signs up.",
            "Do they do that here?",
            "If the signs and they yeah, right everywhere, right?",
            "OK, so let's imagine that's the way the world is.",
            "So the first thing is one thing I should mention.",
            "We're not trying to compete with the best adaptive strategy, so it may be that in day one we should have done this.",
            "I mean like management day one was this kind day two is this kind day.",
            "Three was this kindly forward that kind?",
            "Well, then hindsight, we should have done this.",
            "The first thing that the second thing that the third day in the 4th day there's no way we can compete with that.",
            "You know with had we known in advance exactly which day was going to be?",
            "I mean what can we do?",
            "We're just trying to compete with the best fixed path in hindsight.",
            "So for example, if it turns out that over the course of the year that 70% of the days were of this type, and 30% of the days were of this type.",
            "OK, then the best fixed choice in hindsight was to take this route because I only had to pay a cost 30% of the time, right?",
            "So then then we should also have the property that we only had to pay 30% of the time to approximately or on the other hand, if it turned out that actually 70% with this kind 30% discount, then we'd like to, you know, then it's the other route that only had to pay 30% of time, and so we also would like to.",
            "Have done well.",
            "Only had to pay about 30% of the time.",
            "OK. And so no record algorithms can potentially do actually, if you view this as a game and will talk about viewing this as a 0 sum game, no regret algorithms can potentially do much better than playing the mini Max optimal strategy, which in this case would be 5050 depending on how the world is OK, but they'll never do much worse, and so I'll define minimax optimal.",
            "I mean, the next optimality later when we get to it.",
            "In fact, the existence of these algorithms gives an immediate proof of the Mini Max theorem, and we'll see why and I'll talk about that later to.",
            "OK."
        ],
        [
            "Good and I guess one of the things mentioned is that our view of the world of Life, of fate, whatever you want to call it, is it's some unknown sequence.",
            "It's in the future, you know there is going to be some sort of traffic patterns in future.",
            "We just don't know what it is.",
            "And our goal is to do well no matter what that sequence is.",
            "In expectation over.",
            "Internal randomization our algorithm can use.",
            "So in particular, in this view, our algorithms are going to have to be randomized.",
            "So what do I mean by that?",
            "What I mean is, if you had a terministic strategy.",
            "Then there always would exist the sequence in which every day you were paying the bad costs.",
            "Can you see that?",
            "But it's true.",
            "So if you were determined, strategies like OK, what are you going to do?",
            "First day I'll go down.",
            "Well, what about the world that starts this way?",
            "And then, if given that the next thing you would have done is go up, well, then whether the world the next day is like this.",
            "If you give me a deterministic strategy, I can always reverse engineer a world in which it's paying one every single time.",
            "And yet, in hindsight, one of the two choices must have been.",
            "You know, paid only half the time or so, or even better, so the only hope we have against fate kind of an adverse aerial view of life is we're going to be randomized.",
            "OK, so we're kind of doing the game.",
            "Is there algorithm against the world?",
            "Now, in practice, you know if the world is not really out to get you.",
            "You might not need that, but for analysis, you right."
        ],
        [
            "OK, so let me just say a little bit, an abridged history and development.",
            "So this kind of stuff goes back quite a ways.",
            "So hang on in 1957, building on work of Blackwell the year earlier in 1956 gave an algorithm with actually that actually solves this problem, and it has regret.",
            "So an was the number of choices we had.",
            "T was the amount of time it square root N / T so as time goes goes up.",
            "This is dropping down to 0.",
            "Exactly what we wanted and if we just kind of reorder this in a way which I like.",
            "If you ask, you know how long do we have to play to get our average regret down to some epsilon.",
            "So down to you know.",
            ".1 hours so then just setting that to epsilon and solving for T you get an over epsilon squared.",
            "So that's how many times you have to play in order to get your average regret down to epsilon.",
            "At the time, number of time steps it takes you, it's how how quickly you're converging.",
            "And this actually is optimal in terms of its dependence on T or epsilon.",
            "You can't beat square root of the 1 / sqrt T and game theorists view the number of rows.",
            "The number of choices are constant and not so important.",
            "This number of time steps.",
            "So pretty much done.",
            "OK so 57 people couldn't stop thinking about it.",
            "Um?",
            "Before I get to that, there's been work since then, so let me I'll get to why, how it came back in machine learning.",
            "Let me just kind of say a little bit about why this is the optimal dependence you could hope to get in terms of in terms of penance, Auntie.",
            "So think about the following setting.",
            "Let's go back to this picture we had of this two routes and two possible worlds.",
            "And imagine that the way life is is life flips a coin.",
            "Man, every day 5050 chance this is the expensive one.",
            "This is the cheap one or 5050 chance this is the cheap one.",
            "This is the expensive one.",
            "So life lips a coin then it doesn't matter what your algorithm does, so see the world flips a fair coin each day to determine which type of day it's going to be.",
            "Well, whatever your algorithm does, there's a 5050 chance you're going to pay one.",
            "What can you do?",
            "You choose something, world flips a coin.",
            "So whatever algorithm in expectation pays 1/2 every day, whatever you choose to do so in two days, you're expected cost is 2 / 2 nothing you can do about it.",
            "On the other hand, in hindsight, the best route in hindsight, is if I were to flip a coin T times, think of heads is this entails is that it's the expected value of the minimum of a number of heads and number of tails.",
            "OK, so I put the coin tee times, you know I expect you over 2 heads.",
            "I expect over 2 tails but the expected minimum 'cause it's going to be this variation.",
            "The expected minimum ends up being T / 2 minus about order square root of T. Right?",
            "'cause if you flip a coin three times, you know it's going to be this binomial like this and the binomial kind of 1st order approximation.",
            "It's like T / 2 plus or minus square root is kind of sort of like kind of like a box, right?",
            "So you're going to be if you flip a coin tee time you get over 2 heads plus or minus square root of T and the expected value.",
            "The minimum is going to be about square root of T. Off from the other two, and so when you divide by T, the per day gap, if the world acts this way, there's no way you can help.",
            "The per day gap is going to be one over Rooty.",
            "You can't.",
            "You can't beat that.",
            "So OK, so that's just explaining why you know the game figures said great, you know we're done.",
            "OK."
        ],
        [
            "Good, so learning theory, though in the 80s and 90s this problem came back and it came back by a problem called the problem of combining expert advice where we're really going to now think of N as a very large number.",
            "We're going to manage.",
            "We have a lot of prediction rules, so these choices are now a bunch of different prediction rules we might be using.",
            "It might be a whole space as a whole class of different possible classifiers, prediction rules we might be using, and we'd like to do nearly as well as the best of these prediction rules, so our jobs do nearly as well as the best function in that class.",
            "And it's very nice algorithm of little stone wormuth in the late 80s showed how you could get a cost which does nearly as well as the best function, only off by 1 plus epsilon factor plus an additive term that was of this type is log of a number of rules over epsilon.",
            "If you set that epsilon to balance the two things out, it gives you regret like the previous, but login over square root login over T instead of square root N / T. So the number of time steps to get you regret down to epsilon instead of being in over epsilon squared is like log in over epsilon squared.",
            "So that's great if the number of choices you have is a large number, OK, a lot of different possibilities to pick from because.",
            "You're getting good at lower at much faster, so it means that it doesn't hurt you too much to throw a lot more options into the mix.",
            "So I mean, you know 'cause.",
            "If an as you know to the 30th login is much better.",
            "The same result.",
            "Maybe not happy family something wrong.",
            "Oh yeah, that's right, that's right.",
            "Yeah, I'm not sure which is which came.",
            "I thought this was 90 anyway.",
            "OK, yeah.",
            "Yes, you also vote yes.",
            "That's right.",
            "I went to this conference and up there.",
            "OK, yeah, that's true.",
            "That's true so.",
            "We vote also had this kind of result, and since then there's been a lot of other work, so this is now optimal as a function of N have been a lot of work on getting the exact constants, getting the 2nd order terms, and various other things.",
            "But we're not going to worry about that.",
            "Now this is all in the non bandit model.",
            "This is all in the model where you get full feedback.",
            "So when you drive, when you're done, you find out from your friends how long each other option, and that's actually very natural in this context, because when you're trying to make a prediction, if you find out the right answer, you can figure out how well you would have done had you chosen one of the other prediction rules in the bandit model where you have any choices and you only get feedback from the choice you took, you have to at least try everything.",
            "I mean, you know maybe there's some option out there, you just haven't tried it.",
            "You gotta try everything.",
            "So you gotta at least.",
            "Pay the rent and it ends up costing this extra factor of it.",
            "OK, so this is kind of a quick."
        ],
        [
            "History and what I want to do now.",
            "Is a look at a particular problem in this area called the problem of combining expert advice?"
        ],
        [
            "OK.",
            "So imagine that for whatever reason, we'd like to predict the stock market.",
            "Be nice to do.",
            "Anne.",
            "Alright, make some money.",
            "So we're going to do is we're going to ask N experts for their opinions.",
            "Do they think the market is going to go up or down today?",
            "So these are maybe different prediction rules that could be actual people that could be prediction rules that could be different statistics that we're looking at different.",
            "You know, economic indicators, whatever, and each one is suggesting the market will go up, down, up, whatever we want to somehow combine them.",
            "OK, and then we'd like to use the advice of all these different indicators or people, or prediction rules, to somehow make our own prediction.",
            "OK, so for instance, you know maybe in day one these various experts say the markets go down, up, up.",
            "We see our neighbors dog is wagging his tail and then and then we make a prediction and then we find out what happened to the market.",
            "And then next day down, up, up, down and break it.",
            "You think it's funny the neighbors dog is always right?",
            "Well I got to make it.",
            "2 examples but.",
            "I think it's probably a good indicator of anything.",
            "OK, but this is how the game is going to go, so we're going to get this information.",
            "We're going to make our own prediction and then we either get it right or get it wrong and we get, you know.",
            "We we get a mistake if we get it wrong.",
            "Not a mistake.",
            "We got it right.",
            "OK, our job is to not make too many mistakes and the question is what can we do nearly as well as the best of these in hindsight?",
            "OK, so if one of them so if none of them are any good, we don't have to do very well.",
            "But if one of them ends up actually being quite good, then we want to do nearly as well as it did.",
            "OK, and then so I should just emphasize that an expert here doesn't mean someone who knows anything.",
            "It just means some with an opinion.",
            "OK, it's just so that's why I like financial, right?",
            "So.",
            "Just some with opinion and we don't know in advance if any of them are any good, but if one of them turns out to be good, we want to do nearly as well as it did.",
            "OK, so that's the problem.",
            "We're going to look at.",
            "Right?"
        ],
        [
            "Let's look at the easier version of this first just to get their feet wet.",
            "So here's a similar question.",
            "We have experts and suppose we were told that one of them is perfect.",
            "It never makes a mistake, we just don't know which one.",
            "Can anyone think of a strategy that would make it most logann mistakes?",
            "OK, you got the experts.",
            "Everyday, they're making predictions and then you get to see their predictions and make your own and your promised that one of them is perfect.",
            "Just don't know which one.",
            "Anyone think of a strategy for combining their opinions?",
            "Yeah.",
            "Everybody involved and working perfectly, making sense here exactly.",
            "Just take majority vote and.",
            "Then throw away the ones that made a mistake and that guarantees that every time you made a mistake.",
            "That you at least that you cut down the set of available experts by at least a factor of two.",
            "If you're lucky, you might even cut down the set even without making a mistake, but for sure, every time you make a mistake, that means at least half of the experts that were left predicted incorrectly, and so you get to cut down by at least a factor 2.",
            "Everybody successful trip it takes, but it's more or less rented, no, no, they can be anything at all.",
            "So you've got, you've got your experts in there making predictions and you're just taking majority votes.",
            "We gotta 100 experts, 57.",
            "Stay up and 43 say down so you take up if you are wrong, that means the majority of the experts were wrong.",
            "'cause you went with the majority and then you in every step you throw out the ones that.",
            "That made a mistake.",
            "'cause member promised that one of them is perfect so we can throw out the ones making mistakes so that guarantees every time we get it wrong.",
            "At least half of the ones left.",
            "So every mistake we chop down by Lisa factor too.",
            "So we make it most log base.",
            "Two of any mistakes.",
            "Schedule, we are assuming that raise one who never makes it so for the simpler version, just for the simpler version, let's assume that one of them is.",
            "Yeah, this is a simpler version before we get to the real problem, yeah.",
            "Tom.",
            "Good so.",
            "So we can do that.",
            "In fact, maybe just to make an aside.",
            "But we're not going to be going in the structure, but taking aside if we had a prior.",
            "Of which we thought was more likely to be their good one.",
            "We could, instead of taking a majority vote, we could wait them by the Priors.",
            "And then every time you make a mistake, at least half the probability mass goes away.",
            "And if the best expertise expert I so it's probability to say some PIP is the prior on expert I.",
            "If you're shopping at least half probability mass, you'll make at most log of 1 / \u03c0 mistakes, so this log in is kind of like.",
            "If your prior is the uniform.",
            "Log one over P6.",
            "OK, but we're not going to.",
            "We're not going to be going that way.",
            "OK, good, so sure we can solve this.",
            "We just take a majority vote.",
            "Overall, experts have been correct so far.",
            "Each mistake cuts number available by Factor 2.",
            "Which is great, 'cause it means it's OK for end to be very large 'cause we're only making log event mistakes.",
            "So this is called the halving algorithm you get cut by."
        ],
        [
            "Alright, so let's now go to the more general question.",
            "What if none of them is perfect?",
            "Can we still do nearly as well as the best of them in hindsight?",
            "Well, here's a strategy.",
            "One strategy we could just repeat that algorithm.",
            "So do the same thing as before, but once we've crossed off everybody, we say whoops, we crossed everyone off.",
            "That's too bad.",
            "Everyone made a mistake.",
            "Restart from the beginning.",
            "We could do that if we did that, then if the best expert make soft if opt is number makes the optimal expert then we make Logn mistakes for every time it makes one and then we restart, make another login mistakes every time it makes one so will make login times more mistakes in the best expert.",
            "So that's something we could do.",
            "It's not a great thing to do, though.",
            "This is kind of wasteful algorithm.",
            "It's constantly forgetting, So what it's learned and restarting from scratch, so be nice little bit better.",
            "And you can.",
            "Who makes music?",
            "Investors, so if he makes mistakes first time so it will cause it so still this will work with local.",
            "Yes, it's not doing that great because it just makes a mistake one time and we make login mistakes when we crossed everybody off.",
            "And then we restart from scratch and it makes one more mistake and we make another login mistakes that we crossed everybody off.",
            "So we're making log N times more mistakes than it did, so that's why this is not a great algorithm.",
            "For exactly that reason.",
            "OK, and then we're kind of every time we just restart from scratch.",
            "We forgot we learned the last time, so it's not the best thing to do.",
            "So here's a better thing to do."
        ],
        [
            "Very natural strategy.",
            "The point here is that you know when you make a mistake.",
            "That doesn't completely eliminate you from possibilities 'cause we even the best ones going to be making mistakes.",
            "So instead of crossing it off will just cut its weight down.",
            "So in particular, consider the following algorithm, so we'll start with all the experts having a weight of 1, and then we'll predict based on a weighted majority vote, just plain the beginning.",
            "Just plain majority vote and now will penalize mistakes, not by eliminating them, but just by cutting their weights in half.",
            "Seems like a reasonable thing to do so.",
            "So we've got weights.",
            "So they saw fit weights one maybe in the first time step.",
            "Three of them predict yes, one predicts no.",
            "We take majority vote.",
            "We predict yes.",
            "If yes, was the correct answer.",
            "These guys say the same weight one, but this guys wait gets cut in half.",
            "If now the predictions are yes, no, no, yes, there's a wait two on no.",
            "But wait 1/2 on yes we go with no.",
            "If the right answer was yes then we penalize these two and their weights get cut down by.",
            "So that's the algorithm.",
            "So we just make a mistake.",
            "We just cut their weights in half."
        ],
        [
            "So this turns out to actually give a pretty nice bound, and I'm going to go through the proof so so so I'm a mathira.",
            "I'm a theoretician.",
            "Come through here science and so.",
            "You know, I do like algorithms that actually work, and it's great.",
            "And I even like I like nice theorems.",
            "I really like her nice proofs.",
            "That's what really excites me, so this is a very nice proof so.",
            "OK, so we're going to show that this algorithm actually does nearly as well as the best expert in hindsight.",
            "And then will improve it and make it even better.",
            "So they analyze this so the analysis is a bad color.",
            "Alright.",
            "We're going to look at three things were going to look at Capital N, which is the number of mistakes we our algorithm is made so far.",
            "We're going to look at little M, the number of mistakes the best expert has made so far, and we're looking at one more thing, which is the total weight in the system.",
            "The total weight in the system.",
            "We start everybody with weight one, so the total way there's N experts.",
            "The total weight is capital is N. Is N total way so now?",
            "I claim that every time we make a mistake.",
            "Let's think what happens to the total weight come every time they can mistake the weight drops by at least 25%.",
            "Because you've got this total weight in the system, so here, let me just draw.",
            "You've got.",
            "This is the total weight in the system and we're predicting based on.",
            "Is there more weight predicting one way or more weight?",
            "Particularly other way.",
            "OK, so there's some this split some way like this.",
            "Some weight predicts want something in some way.",
            "The other way we go with the majority of the weight, and if we're wrong, remember we cut all of them down by a factor of two, so we cut them all down by a factor of two.",
            "So we remove at least 25% of the weight from the system.",
            "So every time we make a mistake, the total weight in the system drops by at least 25%.",
            "So what does that mean?",
            "It means that after.",
            "Am mistakes after capital and mistakes.",
            "The total weight is at most when it started with times 3/4 to them because every time we made a mistake it dropped by at least 25%.",
            "So you're multiplying it by at most 3/4.",
            "So after we've made capital mistakes, the total weight is at most that we have started with times 3/4, three quarters.",
            "Three course occurs every time we make a mistake.",
            "On the other hand, by definition the best expert has weight one over to the little M because.",
            "Just by definition of the algorithm, we start one and every time that thing makes a mistake, it gets cut in half half half, half.",
            "If it's made little mistakes, its weight is 1/2 little lamb.",
            "And clearly the total weight in the system is at least the weight of this one guy, because the weights are never negative.",
            "So we just set.",
            "We just write that out.",
            "This quality here is at least that quantity there and then you just sort of move the two the little M over here in the Four Thirds, the Big M. Over here you take the log base 2.",
            "And flip that around and this is one over the log base.",
            "Two 4 thirds and you get that our number of mistakes is at most 2.4 times the number of mistakes of the best expert plus additive login.",
            "So what that's saying is that you know if the best expert makes a mistake 1% of the time, they will make a mistake 2.4% of the time.",
            "Plus this ad of log in term kind of our learning rate.",
            "So how long it takes to so figure out something interesting is going on.",
            "So that's a nice a nice kind of statement.",
            "So in a certain sense, we're doing nearly as well as the best expert.",
            "How in the earliest, well, 2.4 times.",
            "That's how nearly as well.",
            "Now 2.4 times is fine if the best experts making mistake 1% we do 2.4% big deal.",
            "If the best extra making mistake 20% of the time, then we're making 48%.",
            "I must not so great.",
            "So you might ask at least the constant ratio, but could we do better?"
        ],
        [
            "Yeah, so the answer is yes, we can do better and here are two ideas that when you put them together really help.",
            "OK, so this is just setting up the question.",
            "I gotta change my colors.",
            "OK so you know this is fine if if if the best exercise making mistake only say 1% of the time but not so great.",
            "If it's like 20%.",
            "So.",
            "Here's an idea, so here's two ideas, 1.",
            "Instead of taking a strict majority vote.",
            "Let's use the weights as probabilities.",
            "So.",
            "If 70% of the experts say the markets go up in 30%, say it's going to go down instead of just saying up, let's flip a coin and with 70% probability, say up to 30% probability, say down.",
            "The reason this?",
            "It can help in our analysis is remember, we're kind of viewing the world.",
            "Is this sort of adverse aerial thing and so the world for the mean?",
            "What's the world going to do us if we are doing active Terministic Lee, the world is going to put 51% of the experts doing the wrong thing and 49% doing the right thing.",
            "And we're going to do the wrong thing.",
            "And it's going to say haha, OK, but this way, 'cause it's against us.",
            "But this way you see if it puts 51% on the wrong thing in 49% of the right thing where we at least have a 49% chance of doing the right thing.",
            "In fact, if you think about it, the amount of weight in the system that we remove from the system is now linearly related to our probability of making a mistake.",
            "So if it's 5050, we remove a quarter of the way the system if 100% get it wrong, we remove half the way the system and we have 100% chance of making mistake.",
            "If 10% go on week.",
            "So the probability we're going to make a mistake is going to be nicely linearly related to the amount of weight removed from the system, so this will smooth out the worst case there won't be this of single worst case anymore.",
            "All the cases will be equally good.",
            "Second idea is you know why we hit by why we multiply by half just seemed like a nice number.",
            "You could see what happens if you multiply it by something else, like .9.",
            "And if you do that, you get the following bound.",
            "You get up the mess this mess, but this this complicated thing which I don't even want you to look at is approximately.",
            "Something is approximately 1 plus epsilon over 2 times number mistakes, the best expert plus one over epsilon log in.",
            "So this.",
            "Epsilon this kind of learning rate.",
            "If you make it small, it takes a little bit longer to learn, so you get a little worse additive term.",
            "We end up doing much better compared to the best experts, so we just plug in some numbers just to get a feel of these things.",
            "If you plug in 1/2 you get 1.39 times.",
            "The best expert +2 natural log in.",
            "If you plug in 183 epsilon, so you multiply by 1 by 7/8, then you get 1.07 times the mistakes.",
            "The best expert plus eight login, so these are now pretty nice numbers.",
            "So before you know 2.4 is a little bit large, but you know 1.07, it's starting to get.",
            "Pretty good, so even if the game is going to go on for awhile, you might want to use a smaller epsilon because this term may mean more to you than after.",
            "OK, so so these numbers.",
            "These are worst case bounds on our expected number of mistakes.",
            "Now 'cause we're a randomized algorithm and the numbers are pretty good.",
            "So OK, so I told you that the first part is going to be the hardest part, and here it comes.",
            "So we're going to do this analysis.",
            "It's going to be 1 slide long.",
            "But we'll get several things for free out of it, so so it's I think it's a very nice.",
            "It's a nice argument."
        ],
        [
            "I like proof.",
            "OK, so here's the thing.",
            "It's a very nice argument, will get several things for free once we have it, including the mini Max here so.",
            "And actually, if you've seen the.",
            "People are people seen Ada boost boosting, so the argument that Adaboost works actually is.",
            "Incredibly similar to this, it's the same analysis, really.",
            "OK, so let's analyze like this.",
            "Let's say that at time T we have a fraction.",
            "Redraw this Pi here.",
            "Let's say that we have a fraction FT of our weight on the experts that made a mistake.",
            "So let's say these guys made a mistake.",
            "And these guys got it right.",
            "Then this is exactly our probability that we make a mistake because after all that's our algorithm algorithm says how much weight we split.",
            "You know how much weight set up, how much weight sat down, and we flip a coin.",
            "So our probability that we make a mistake is exactly the fraction of the total weight on the experts that made a mistake.",
            "OK, so so we have probability FT of making a mistake, and in addition we remove an epsilon times FT fraction of the weight from the system because we're multiplying these guys.",
            "By 1 minus epsilon and so if we do that, the amount of weight that gets removed from the system is an epsilon FT fraction of the total weight.",
            "Just like before, we were multiplying by 1/2, so we removed.",
            "Half of that now we're moving epsilon fraction of that.",
            "So if you look at the total weight in the system, it starts at this end and then after time one you remove an epsilon times F1 fraction of the weight from the system and after that I'm two.",
            "We remove an epsilon F2 fraction away from the system every time I move epsilon times FT fractional weight from the system.",
            "So the final weight is going to look like that.",
            "That's a product and it's messy.",
            "Let's take the log to turn into a sum so it will be a little nicer.",
            "Take the log, make it asam.",
            "We get the log in.",
            "We get the the log of this product, which is the sum of logs.",
            "This is messy.",
            "Yuck.",
            "But let's approximate this log of 1 -- X is less than negative X. OK, so that's this picture here.",
            "This is the function negative X.",
            "And this is the function log of 1 -- X log of 1 -- X is.",
            "What?",
            "To make it better.",
            "We got epsilon times the sum of the S&P's and the nice thing here is this is exactly our expected total number.",
            "Mistakes is the probability making mistake of time one plus time two times.",
            "This is how many mistakes we make an expectation, so it's great.",
            "We've directly related the weight in the system to our probability.",
            "Our expected number of mistakes.",
            "This is exactly login minus epsilon times our expected number of mistakes, and now the rest of the argument.",
            "That wasn't too bad, was it so so if the best expert makes little mistakes.",
            "Well, the final weight is at least his way.",
            "It's wait, it's 1 minus epsilon to the limit starts at one and we hit it with one minus epsilon every time it made a mistake.",
            "So it's wait.",
            "Is this thing here?",
            "We're just taking the log 'cause we're looking at the log, so its weight is 1 minus epsilon VM the final way it's gotta be at least that guys wait for the log of the final way.",
            "It's gotta be at least log of that guys.",
            "Wait, so it's gotta be at least this and so this quantity is at least that quantity.",
            "So that's it.",
            "Actually, you just solve this quandary here is at least that quantity there.",
            "And when you solve, you just gotta move the big M on the right side and you got the little lamb on the left side and divide by the epsilon.",
            "Get exactly this funky formula here.",
            "That's all came from just that.",
            "And then you just use Taylor series to approximate.",
            "Too bad.",
            "Good, yes, so so all the weights are positive.",
            "So the final weight is the sum of every, so it's a sum of.",
            "Anyway, I'm saying this one guy has this way.",
            "And the other ones have some non negative way.",
            "So when you Add all the weights together you get something bigger than this.",
            "Never to be quite there.",
            "Apparently on the right path.",
            "Well there then learning W final is is probably strictly larger, yeah?",
            "Well, it can't be large.",
            "It can't be equal 'cause none of the weights go to 0, so this is the sum of everybody's weights.",
            "So it's a sum of numbers that are all positive and one of them is this and the others are bigger than 0.",
            "It doesn't really matter, I think.",
            "I wrote less than or equal to hear just as good, so we've got.",
            "We've got this bound and then."
        ],
        [
            "Um?",
            "So that's that's the kind of thing we have we do nearly as well as the best expert plus this additive term.",
            "And now the connected to the first part.",
            "This regret business if we set our epsilon.",
            "If we knew in advance how many mistakes the best expert was going to make.",
            "If we knew that number in advance, we could set our epsilon to equalize the two losses.",
            "So the two losses are is the epsilon.",
            "So we're making the M plus an epsilon M plus this.",
            "If we set that epsilon M to equal this thing, we can do that by setting up onto this one here.",
            "If we knew in advance how many mistakes the best expert would make if we didn't know in advance one thing you could do is make a guess.",
            "As you run your algorithm, if once the best expert goes above that guess you restart with the smaller epsilon, the smaller one smaller one the telescoping series.",
            "In any case, if you do that, you'll get the expected mistakes.",
            "At most the number mistakes.",
            "The best expert plus this additive.",
            "Think it looks roughly like so two times number makes the best expert times log in so that in square root.",
            "So all I'm doing here if you just plug that value into there and you.",
            "M Plus the epsilon M plus the one over epsilon login plugin that five epsilon.",
            "You get that quality there and then just using the fact that you know worst case number of experts.",
            "The best expert at worst makes a mistake every single time.",
            "This would not be a very good expert but it works every single time.",
            "So we just plug that in here.",
            "It's exactly the bound we had on that first slide.",
            "#6 the best experts plus square root T login.",
            "So per time step is it's going square root of T. Been logging in, so this is exactly that.",
            "That that bound we had in the earlier slide.",
            "Good.",
            "There was a regret is going to zero as per time step.",
            "Our per average regret.",
            "So if you divide this by T it's dropping like square root of T and we have the extra login.",
            "So great, so we.",
            "So we did it.",
            "OK, let me see."
        ],
        [
            "Anywhere OK?",
            "So let me just go to.",
            "There's one part that I want to connect it back to the first problem, and so there's one issue here.",
            "We're talking here about having an predictors like of the stock market.",
            "It's not.",
            "It's a little bit different from having an routes to choose from, like we're taking majority vote.",
            "So that means, like one experts consistently wrong.",
            "Presumably he could generalize the address in sun.",
            "Rachel not make the Quakes right.",
            "You're not.",
            "Actually.",
            "You can actually improve doing.",
            "You can do much better in that situation, right?",
            "OK, so it's true that maybe you want to instantiate an expert, which is the opposite of whatever someone else says, and then and then there's always guaranteed the best expert makes a mistake that most half the time, right?",
            "This is totally accrued kind of thing here, right?",
            "So you might want to do that if you thought that your experts, for instance, might be adverse aerial.",
            "So you could always do that.",
            "You could you could.",
            "Instantiating be better than setting negative.",
            "Just have a shadow expert, just as the opposite of whatever.",
            "It's probably good for financial.",
            "OK. Good, So what if you have an options to choose from, not end predictors, so you can't really take the majority vote.",
            "You can't say I want to take the majority vote of this path.",
            "This path in this path or not voting for not predicting things.",
            "We're not combining and experts, we're just choosing one.",
            "But here's a nice thing.",
            "This randomized procedure still makes sense, because another way to think about the algorithm we just just analyzed.",
            "Remember, we said we ask all the experts, do you think the markets go up or down?",
            "You look at how much probability mass set up and how much probability mass sat down, and you pick random.",
            "That's the same thing.",
            "As.",
            "Choosing an expert at random with probability proportional to its weight and going with what it said.",
            "It's the same thing if if you do this, if you just choose an expert at random with probably proportional sway and then just go with what it said, then if 70% of the weight is saying up, there's exactly a 70% probability that you're going to say up also, so it's just another way of thinking of the same algorithm.",
            "OK, and so this you can apply.",
            "So what this is saying is what the way that you should do your route choosing is.",
            "You give a weight to each route and now you're going to pick her out at random with probability proportional to the weight of that route.",
            "OK, and then still the same algorithm.",
            "You get the same analysis.",
            "The only technical thing that just to do one last little bit to fix up 1 technical thing.",
            "I've been assuming that the costs are either zero or one the experts.",
            "They make a mistake.",
            "They don't make a mistake, but if you have routes you know it's going to be like 0 hours in one hour.",
            "Weather Route takes 25 minutes.",
            "What do you do then?",
            "Well, it turns out you can do the simplest thing so."
        ],
        [
            "It's in between.",
            "So before we were saying if somebody makes a mistake, if they have cost one, you multiply by 1 minus epsilon and if they have cost 0 you don't do anything.",
            "You keep them at one, so just do a linear interpolation.",
            "If they have cost CI between zero 1 * 1 minus CI times epsilon.",
            "So this one you got before zero we have we had before the half human minus epsilon for two.",
            "And if you do that, the analysis everything goes through.",
            "It's our expected cost.",
            "Is exactly.",
            "This is the probability we choose expert.",
            "I times the cost of expert eye.",
            "That's exactly our expected cost.",
            "The amount of weight removed is exactly.",
            "We remove this much weight.",
            "It's exactly epsilon times hour.",
            "Expected cost in terms of the fraction, the fraction of weight removed is exactly epsilon times our cost.",
            "That's the only thing we used.",
            "Everything else is exactly the same, so all I'm saying is that the analysis we did, it's just plug in these things in the exact same rest of the analysis.",
            "We get the exact same bound, so so I'm not cheating you that we can handle the full problem with the costs in between zero and one.",
            "So the rest of the proof continues before and so now we can drive to work.",
            "Assuming full feedback, we haven't gotten to the bandit problem yet, so maybe this is a good time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me just start by.",
                    "label": 0
                },
                {
                    "sent": "I really like to thank the organizers for inviting me.",
                    "label": 0
                },
                {
                    "sent": "I thank Marcus and all the all the organizers.",
                    "label": 0
                },
                {
                    "sent": "Really been.",
                    "label": 0
                },
                {
                    "sent": "Fantastic week and.",
                    "label": 0
                },
                {
                    "sent": "Sorry could have been here for the for the first week.",
                    "label": 0
                },
                {
                    "sent": "Part time it's really intense.",
                    "label": 0
                },
                {
                    "sent": "An amazing place and getting the weather all set, you know I mean.",
                    "label": 0
                },
                {
                    "sent": "Philia great so.",
                    "label": 0
                },
                {
                    "sent": "Let's see, actually, first of all, so so I guess we need those lights for the camera, is that right so?",
                    "label": 0
                },
                {
                    "sent": "Perfect.",
                    "label": 0
                },
                {
                    "sent": "They won't work.",
                    "label": 0
                },
                {
                    "sent": "So then, then the camera won't.",
                    "label": 0
                },
                {
                    "sent": "You don't think so.",
                    "label": 0
                },
                {
                    "sent": "Can you can you sleep well?",
                    "label": 0
                },
                {
                    "sent": "I mean, I can see it's good question of people.",
                    "label": 0
                },
                {
                    "sent": "OK. OK. Light on it.",
                    "label": 0
                },
                {
                    "sent": "OK OK OK. Alright.",
                    "label": 0
                },
                {
                    "sent": "At CMU, nothing starts until five after but.",
                    "label": 0
                },
                {
                    "sent": "So I have to.",
                    "label": 0
                },
                {
                    "sent": "Use up time in the beginning, so.",
                    "label": 0
                },
                {
                    "sent": "I may be talking about.",
                    "label": 0
                },
                {
                    "sent": "Two different things, so it most of this tutorial will be an on line learning and game theory and connections between the two and then the.",
                    "label": 1
                },
                {
                    "sent": "The second half of tomorrow.",
                    "label": 0
                },
                {
                    "sent": "So maybe the last two hours, hour and a half or so will be on a different topic about a kind of a different way of thinking about kernel functions and more general similarity functions and.",
                    "label": 1
                },
                {
                    "sent": "And learning with us so just.",
                    "label": 0
                },
                {
                    "sent": "Has nothing really to do with it.",
                    "label": 0
                },
                {
                    "sent": "With the first topic, but I like them both, so I want to talk about them.",
                    "label": 1
                },
                {
                    "sent": "OK. OK, and it says so, so we'll have.",
                    "label": 0
                },
                {
                    "sent": "We talked about several things and I'll be your guide today.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and and the plan for it for the tour is our first stop.",
                    "label": 1
                },
                {
                    "sent": "We will be looking at online learning online learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about something called the notion of regret and trying to minimize regret in online learning.",
                    "label": 0
                },
                {
                    "sent": "And there's a very nice problem.",
                    "label": 0
                },
                {
                    "sent": "Abstract certain aspects of this, called the problem of combining expert advice and we'll talk about that.",
                    "label": 0
                },
                {
                    "sent": "Then we'll kind of switch a little bit and talk about game theory.",
                    "label": 1
                },
                {
                    "sent": "We already see in the first part some game theory connections talk about game theory, minimax optimality, Nash equilibrium, and connections to the first part.",
                    "label": 0
                },
                {
                    "sent": "How many people have seen the notion of minimax optimality?",
                    "label": 0
                },
                {
                    "sent": "How many people have seen the notion of Nash equilibrium?",
                    "label": 0
                },
                {
                    "sent": "OK. How many people have seen the proof of Minimax theorem?",
                    "label": 0
                },
                {
                    "sent": "Cat.",
                    "label": 0
                },
                {
                    "sent": "So actually we'll get that for free from some of the analysis of online learning.",
                    "label": 0
                },
                {
                    "sent": "Then then we'll go to some little bit more.",
                    "label": 1
                },
                {
                    "sent": "Concepts in game theory, notion of correlated equilibrium, and this particularly brings out some connections.",
                    "label": 0
                },
                {
                    "sent": "Even more connections between these first 2 topics.",
                    "label": 1
                },
                {
                    "sent": "Talk about some some recent research as well.",
                    "label": 0
                },
                {
                    "sent": "And then at the end we'll do something completely different.",
                    "label": 0
                },
                {
                    "sent": "Talking about learning and clustering with similarity functions and kernel functions and more general measures of similarity.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's sort of the outline for today and tomorrow.",
                    "label": 0
                },
                {
                    "sent": "So today will kind of the first 2 are the bigger ones will kind of go through about here today and then somewhere here and then this rest tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Let's see I guess one of the things I should say at the beginning.",
                    "label": 0
                },
                {
                    "sent": "So the usual way to do you know three or six hour tutorials.",
                    "label": 0
                },
                {
                    "sent": "You start slow and get people into it, and then you kind of towards the end.",
                    "label": 0
                },
                {
                    "sent": "You hit people over the head with the difficult stuff.",
                    "label": 0
                },
                {
                    "sent": "So it's going to go a little backwards this time.",
                    "label": 0
                },
                {
                    "sent": "OK, so the hardest part is going to be the first hour or so, so if you can survive that first hour, you're home free after that, OK?",
                    "label": 0
                },
                {
                    "sent": "So just wanna let you know up front.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's go.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh so let me just mention just some some books references.",
                    "label": 0
                },
                {
                    "sent": "So they aren't in the printed notes, so there are various changes that have been made to the slides since the time Marcus asked for them.",
                    "label": 0
                },
                {
                    "sent": "And now since lot of us do things at the last minute.",
                    "label": 0
                },
                {
                    "sent": "So just to mention some books and references, so there's a very nice new book just came out on algorithmic game theory, and it's it's really the collection of chapters written by different researchers in different aspects of algorithms and game theory.",
                    "label": 1
                },
                {
                    "sent": "So the whole area.",
                    "label": 0
                },
                {
                    "sent": "So game theory is a very old area coming from economics in computer science.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of recent interest in.",
                    "label": 0
                },
                {
                    "sent": "Connections between algorithms and game theory because many of the big systems that that.",
                    "label": 0
                },
                {
                    "sent": "Many of you know the Internet, web, various auctions and so forth.",
                    "label": 0
                },
                {
                    "sent": "They involve multiple agents interacting who each have their own interests, and so when you design the system, you can't just design it as if you have some central controller that's going to decide how everyone is going to talk to each other.",
                    "label": 0
                },
                {
                    "sent": "They're all they're going to do things based on what's their own motivations, and so when you keep when you have a problem where you have to keep in mind people's motivations that brings it into the context of game theory.",
                    "label": 0
                },
                {
                    "sent": "So there's been a lot of interest, and I would say the last five to.",
                    "label": 0
                },
                {
                    "sent": "6 seven years, maybe a little older, but really the mostly most of the work has been the last five or six years in connection between computer science and game theory.",
                    "label": 0
                },
                {
                    "sent": "And so this book.",
                    "label": 0
                },
                {
                    "sent": "Each chapter is by an expert in a different area.",
                    "label": 1
                },
                {
                    "sent": "Chapter 4 is a chapter that I wrote together with another researcher.",
                    "label": 1
                },
                {
                    "sent": "Yishai Mansour on learning, regret minimization and equilibrium, so some of the things I'll be talking about.",
                    "label": 0
                },
                {
                    "sent": "It's on my web page, but you can also buy the book.",
                    "label": 0
                },
                {
                    "sent": "I don't get any money from it.",
                    "label": 0
                },
                {
                    "sent": "There's a nice book prediction learning in games by a Nicolo, Shesa, Bianchi, and Gabor Lugosi.",
                    "label": 0
                },
                {
                    "sent": "It's also on similar topics to what I'll be talking about.",
                    "label": 0
                },
                {
                    "sent": "It's it's pretty mathematical and, well, I'm pretty mathematical and it's pretty mathematical for me, but.",
                    "label": 0
                },
                {
                    "sent": "But nonetheless, you know it depends where you're coming from.",
                    "label": 0
                },
                {
                    "sent": "It's quite nice book.",
                    "label": 0
                },
                {
                    "sent": "Also resend, I also have course notes that are on the web, so I happened to buy the domain name machine learning.com while it was available and not they have a company but I can hook up my course notes to it.",
                    "label": 0
                },
                {
                    "sent": "Easy to remember them.",
                    "label": 0
                },
                {
                    "sent": "I should have bought McDonald's or something.",
                    "label": 0
                },
                {
                    "sent": "But anyway, OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so our first stop so online learning, minimizing regret and combining expert advice.",
                    "label": 1
                },
                {
                    "sent": "So let me tell you what these mean.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me just.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually with with kind of a motivating setting.",
                    "label": 0
                },
                {
                    "sent": "So imagine that every morning you need to pick one of N possible routes to drive to work.",
                    "label": 1
                },
                {
                    "sent": "OK, so for me I live in the suburbs and I have to drive to work each day and there are various ways I could go.",
                    "label": 0
                },
                {
                    "sent": "OK. Fine, you know from a computer science perspective.",
                    "label": 0
                },
                {
                    "sent": "That's fine, you tell me the length of the edges and I can solve for the shortest path problem.",
                    "label": 0
                },
                {
                    "sent": "We've got plenty of algorithms for solving for shortest paths great.",
                    "label": 1
                },
                {
                    "sent": "The problem is the traffic is different each day, and it's not clear apriori which is going to be best, because these costs of the edge is the amount of time it takes to go from one place.",
                    "label": 0
                },
                {
                    "sent": "So it depends on the traffic, and I don't know in advance what that's going to be, so I have to make a decision before I get to see what the edge lengths are.",
                    "label": 0
                },
                {
                    "sent": "OK, so I pick a path, so imagine that here you are.",
                    "label": 1
                },
                {
                    "sent": "You gotta pick a way to go to work.",
                    "label": 0
                },
                {
                    "sent": "We choose a path and you find out how long it took you, and we took me 32 minutes to get from home to work.",
                    "label": 0
                },
                {
                    "sent": "And perhaps I also find out by asking my friends how long other routes would have taken.",
                    "label": 0
                },
                {
                    "sent": "Or maybe not.",
                    "label": 0
                },
                {
                    "sent": "Maybe I just get the information.",
                    "label": 0
                },
                {
                    "sent": "I just get the feedback from the route I took.",
                    "label": 0
                },
                {
                    "sent": "We can consider these two scenarios.",
                    "label": 0
                },
                {
                    "sent": "Good, so that was day one and then, then the next day I pick a different path.",
                    "label": 0
                },
                {
                    "sent": "Maybe I pick a different path and it takes me 27 minutes now to take 27 minutes because it was because of the past being different.",
                    "label": 0
                },
                {
                    "sent": "Or maybe just because there was less traffic that day.",
                    "label": 0
                },
                {
                    "sent": "Who knows when the next day I pick another route, next day pick another route next to pick another everyday.",
                    "label": 0
                },
                {
                    "sent": "I gotta pick a way to go to work.",
                    "label": 1
                },
                {
                    "sent": "And a natural question to ask here is, is there a strategy that I can use for choosing routes with the following property, but in the long run as I everyday go to work, whatever the sequence of traffic patterns happens to be, so I don't want to make an assumption that you know days or drawn from some distribution or something.",
                    "label": 0
                },
                {
                    "sent": "It's just every day there's some traffic pattern and whatever that sequence happens to be.",
                    "label": 1
                },
                {
                    "sent": "I'd like to have the property that I've done nearly as well as the best fixed route in hindsight, so nobody can tell me.",
                    "label": 1
                },
                {
                    "sent": "Oh, I always go this way and.",
                    "label": 0
                },
                {
                    "sent": "I did much better than you do.",
                    "label": 0
                },
                {
                    "sent": "I would like to have the property that I'm doing at least nearly as well as the best fixed route that I could have chosen in hindsight and use that route every day.",
                    "label": 0
                },
                {
                    "sent": "OK, so so one natural question.",
                    "label": 0
                },
                {
                    "sent": "So can we achieve a guarantee like this?",
                    "label": 0
                },
                {
                    "sent": "It turns out the answer is yes.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, I wouldn't talk about it.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to look at how we can do things like this.",
                    "label": 0
                },
                {
                    "sent": "And actually results of this sort have been known for quite a while, but we'll get to that.",
                    "label": 0
                },
                {
                    "sent": "So good, so let's just before we get into how to do this, let's just think a little bit about this kind of.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so a little bit more generally the setting is.",
                    "label": 1
                },
                {
                    "sent": "Is how a game theorist will look at this, so we have our algorithm.",
                    "label": 0
                },
                {
                    "sent": "We have some options which route to take and we can conceptually think of these as rows of a matrix.",
                    "label": 0
                },
                {
                    "sent": "This is route #1 #2 number, just conceptually writing each choice we could make as a row of some matrix.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "There's also different possible traffic patterns that might exist in the world.",
                    "label": 0
                },
                {
                    "sent": "We can conceptually think of them as different columns.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's infinitely many.",
                    "label": 0
                },
                {
                    "sent": "You know who knows.",
                    "label": 0
                },
                {
                    "sent": "And So what?",
                    "label": 0
                },
                {
                    "sent": "And the way this game is working, is it every day our algorithm picks a row, our algorithm picks a route.",
                    "label": 0
                },
                {
                    "sent": "The world picks a column.",
                    "label": 0
                },
                {
                    "sent": "Now with, these columns are going to represent is how much it costs to take each route.",
                    "label": 0
                },
                {
                    "sent": "So this column we do have some numbers that says if we take this route it takes us 32 minutes.",
                    "label": 0
                },
                {
                    "sent": "This other route would have taken 27 minutes.",
                    "label": 0
                },
                {
                    "sent": "This other route would have taken 45 minutes, so I think this filled with numbers and so we pick the route the world picks the.",
                    "label": 0
                },
                {
                    "sent": "The cost vector, how much we would have paid had we chosen each of the different possibilities, so we pick the role.",
                    "label": 0
                },
                {
                    "sent": "I fix the column and then we pay the cost.",
                    "label": 1
                },
                {
                    "sent": "Here we pay the cost for the action we chose.",
                    "label": 0
                },
                {
                    "sent": "And then we get his feedback.",
                    "label": 0
                },
                {
                    "sent": "Either the whole column if we can ask our friends say, Oh well, you went this way.",
                    "label": 0
                },
                {
                    "sent": "How long did that way take?",
                    "label": 0
                },
                {
                    "sent": "How long that way take?",
                    "label": 0
                },
                {
                    "sent": "We get the whole column?",
                    "label": 0
                },
                {
                    "sent": "Or perhaps we only get our own cost only this entry here in what's called the bandit model.",
                    "label": 1
                },
                {
                    "sent": "It's called the bandit.",
                    "label": 0
                },
                {
                    "sent": "My listening to slot machines being one armed bandits, but.",
                    "label": 0
                },
                {
                    "sent": "Like picking slot machines but.",
                    "label": 0
                },
                {
                    "sent": "This is called the bandit model.",
                    "label": 1
                },
                {
                    "sent": "We talked about how you did oh, I missed the first week, OK?",
                    "label": 0
                },
                {
                    "sent": "Alright, and in order for this game to make sense, you need to assume that there is some bound on the maximum cost that you could incur.",
                    "label": 1
                },
                {
                    "sent": "So if it's possible that you know you're driving to work one day and a piano falls in your head and you die, there's no more learning to do so.",
                    "label": 0
                },
                {
                    "sent": "If there's like some, you know infinite cost here, that's it.",
                    "label": 0
                },
                {
                    "sent": "I mean, you're OK, so you have to assume some kind of bound.",
                    "label": 0
                },
                {
                    "sent": "Otherwise this.",
                    "label": 0
                },
                {
                    "sent": "Hope and So what we're going to assume is all the costs are between zero and one, let's say zero in one hour.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so given this setup, there's a notion of regret and let me just define that that the average regret in T time steps is the gap between the average amount our average cost we pay per day and the average per day cost of the best best fixed choice in hindsight.",
                    "label": 1
                },
                {
                    "sent": "So imagine that you know so far on average we've been taking us 2032 minutes to get to work per day, and the best fixed path in hindsight was 29 minutes.",
                    "label": 0
                },
                {
                    "sent": "That gap, 3 minutes.",
                    "label": 0
                },
                {
                    "sent": "That would be our regret.",
                    "label": 0
                },
                {
                    "sent": "That's how much we regret not having chosen that nice other fixed path.",
                    "label": 0
                },
                {
                    "sent": "We regret it by 3 minutes per day.",
                    "label": 0
                },
                {
                    "sent": "And what we'd like is to have an algorithm where this quantity goes to 0 or maybe even less than 0.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can even do better than the best fixed path if we're lucky.",
                    "label": 0
                },
                {
                    "sent": "OK, but we'd like this to go to zero or better as he gets large.",
                    "label": 1
                },
                {
                    "sent": "And if you can do that, that's called a no regret algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just this is kind of name definition will need, so no regret algorithm is one that has the ability to make the drive this to zero, and so then the regret is the difference in your average cost per trial per day, and the best fixed choice in hindsight.",
                    "label": 0
                },
                {
                    "sent": "And it's a little misleading to call them no regret 'cause these aren't necessarily just.",
                    "label": 0
                },
                {
                    "sent": "Going to 0 but not exactly at 0.",
                    "label": 0
                },
                {
                    "sent": "Alright so there.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's just think about this a little bit.",
                    "label": 0
                },
                {
                    "sent": "You can read that so intuition and properties of no great algorithms.",
                    "label": 1
                },
                {
                    "sent": "OK, the title is done.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look at a really small example.",
                    "label": 1
                },
                {
                    "sent": "Let's imagine that there are two ways that we can get to work.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "That's good, OK, so let's imagine that there are two ways we can get to work OK, the top way in the bottom line.",
                    "label": 0
                },
                {
                    "sent": "And let's imagine that there's two kinds of traffic pattern in traffic pattern number one.",
                    "label": 0
                },
                {
                    "sent": "The top way is very expensive, takes one hour and the bottom line is cheap.",
                    "label": 0
                },
                {
                    "sent": "Teleport for free and the other.",
                    "label": 0
                },
                {
                    "sent": "The other kind of traffic pattern is the other way around, like maybe they're doing construction.",
                    "label": 0
                },
                {
                    "sent": "You know it's not that far, it's just that they are kind of holding these signs up.",
                    "label": 0
                },
                {
                    "sent": "Do they do that here?",
                    "label": 0
                },
                {
                    "sent": "If the signs and they yeah, right everywhere, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's imagine that's the way the world is.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is one thing I should mention.",
                    "label": 0
                },
                {
                    "sent": "We're not trying to compete with the best adaptive strategy, so it may be that in day one we should have done this.",
                    "label": 1
                },
                {
                    "sent": "I mean like management day one was this kind day two is this kind day.",
                    "label": 0
                },
                {
                    "sent": "Three was this kindly forward that kind?",
                    "label": 0
                },
                {
                    "sent": "Well, then hindsight, we should have done this.",
                    "label": 0
                },
                {
                    "sent": "The first thing that the second thing that the third day in the 4th day there's no way we can compete with that.",
                    "label": 0
                },
                {
                    "sent": "You know with had we known in advance exactly which day was going to be?",
                    "label": 0
                },
                {
                    "sent": "I mean what can we do?",
                    "label": 0
                },
                {
                    "sent": "We're just trying to compete with the best fixed path in hindsight.",
                    "label": 1
                },
                {
                    "sent": "So for example, if it turns out that over the course of the year that 70% of the days were of this type, and 30% of the days were of this type.",
                    "label": 0
                },
                {
                    "sent": "OK, then the best fixed choice in hindsight was to take this route because I only had to pay a cost 30% of the time, right?",
                    "label": 0
                },
                {
                    "sent": "So then then we should also have the property that we only had to pay 30% of the time to approximately or on the other hand, if it turned out that actually 70% with this kind 30% discount, then we'd like to, you know, then it's the other route that only had to pay 30% of time, and so we also would like to.",
                    "label": 0
                },
                {
                    "sent": "Have done well.",
                    "label": 0
                },
                {
                    "sent": "Only had to pay about 30% of the time.",
                    "label": 0
                },
                {
                    "sent": "OK. And so no record algorithms can potentially do actually, if you view this as a game and will talk about viewing this as a 0 sum game, no regret algorithms can potentially do much better than playing the mini Max optimal strategy, which in this case would be 5050 depending on how the world is OK, but they'll never do much worse, and so I'll define minimax optimal.",
                    "label": 1
                },
                {
                    "sent": "I mean, the next optimality later when we get to it.",
                    "label": 0
                },
                {
                    "sent": "In fact, the existence of these algorithms gives an immediate proof of the Mini Max theorem, and we'll see why and I'll talk about that later to.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good and I guess one of the things mentioned is that our view of the world of Life, of fate, whatever you want to call it, is it's some unknown sequence.",
                    "label": 1
                },
                {
                    "sent": "It's in the future, you know there is going to be some sort of traffic patterns in future.",
                    "label": 0
                },
                {
                    "sent": "We just don't know what it is.",
                    "label": 0
                },
                {
                    "sent": "And our goal is to do well no matter what that sequence is.",
                    "label": 1
                },
                {
                    "sent": "In expectation over.",
                    "label": 0
                },
                {
                    "sent": "Internal randomization our algorithm can use.",
                    "label": 0
                },
                {
                    "sent": "So in particular, in this view, our algorithms are going to have to be randomized.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "What I mean is, if you had a terministic strategy.",
                    "label": 0
                },
                {
                    "sent": "Then there always would exist the sequence in which every day you were paying the bad costs.",
                    "label": 0
                },
                {
                    "sent": "Can you see that?",
                    "label": 0
                },
                {
                    "sent": "But it's true.",
                    "label": 0
                },
                {
                    "sent": "So if you were determined, strategies like OK, what are you going to do?",
                    "label": 0
                },
                {
                    "sent": "First day I'll go down.",
                    "label": 0
                },
                {
                    "sent": "Well, what about the world that starts this way?",
                    "label": 0
                },
                {
                    "sent": "And then, if given that the next thing you would have done is go up, well, then whether the world the next day is like this.",
                    "label": 0
                },
                {
                    "sent": "If you give me a deterministic strategy, I can always reverse engineer a world in which it's paying one every single time.",
                    "label": 0
                },
                {
                    "sent": "And yet, in hindsight, one of the two choices must have been.",
                    "label": 0
                },
                {
                    "sent": "You know, paid only half the time or so, or even better, so the only hope we have against fate kind of an adverse aerial view of life is we're going to be randomized.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're kind of doing the game.",
                    "label": 1
                },
                {
                    "sent": "Is there algorithm against the world?",
                    "label": 0
                },
                {
                    "sent": "Now, in practice, you know if the world is not really out to get you.",
                    "label": 0
                },
                {
                    "sent": "You might not need that, but for analysis, you right.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me just say a little bit, an abridged history and development.",
                    "label": 1
                },
                {
                    "sent": "So this kind of stuff goes back quite a ways.",
                    "label": 0
                },
                {
                    "sent": "So hang on in 1957, building on work of Blackwell the year earlier in 1956 gave an algorithm with actually that actually solves this problem, and it has regret.",
                    "label": 0
                },
                {
                    "sent": "So an was the number of choices we had.",
                    "label": 0
                },
                {
                    "sent": "T was the amount of time it square root N / T so as time goes goes up.",
                    "label": 0
                },
                {
                    "sent": "This is dropping down to 0.",
                    "label": 0
                },
                {
                    "sent": "Exactly what we wanted and if we just kind of reorder this in a way which I like.",
                    "label": 0
                },
                {
                    "sent": "If you ask, you know how long do we have to play to get our average regret down to some epsilon.",
                    "label": 1
                },
                {
                    "sent": "So down to you know.",
                    "label": 0
                },
                {
                    "sent": ".1 hours so then just setting that to epsilon and solving for T you get an over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "So that's how many times you have to play in order to get your average regret down to epsilon.",
                    "label": 1
                },
                {
                    "sent": "At the time, number of time steps it takes you, it's how how quickly you're converging.",
                    "label": 1
                },
                {
                    "sent": "And this actually is optimal in terms of its dependence on T or epsilon.",
                    "label": 1
                },
                {
                    "sent": "You can't beat square root of the 1 / sqrt T and game theorists view the number of rows.",
                    "label": 0
                },
                {
                    "sent": "The number of choices are constant and not so important.",
                    "label": 0
                },
                {
                    "sent": "This number of time steps.",
                    "label": 0
                },
                {
                    "sent": "So pretty much done.",
                    "label": 0
                },
                {
                    "sent": "OK so 57 people couldn't stop thinking about it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Before I get to that, there's been work since then, so let me I'll get to why, how it came back in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Let me just kind of say a little bit about why this is the optimal dependence you could hope to get in terms of in terms of penance, Auntie.",
                    "label": 0
                },
                {
                    "sent": "So think about the following setting.",
                    "label": 0
                },
                {
                    "sent": "Let's go back to this picture we had of this two routes and two possible worlds.",
                    "label": 0
                },
                {
                    "sent": "And imagine that the way life is is life flips a coin.",
                    "label": 0
                },
                {
                    "sent": "Man, every day 5050 chance this is the expensive one.",
                    "label": 1
                },
                {
                    "sent": "This is the cheap one or 5050 chance this is the cheap one.",
                    "label": 0
                },
                {
                    "sent": "This is the expensive one.",
                    "label": 0
                },
                {
                    "sent": "So life lips a coin then it doesn't matter what your algorithm does, so see the world flips a fair coin each day to determine which type of day it's going to be.",
                    "label": 0
                },
                {
                    "sent": "Well, whatever your algorithm does, there's a 5050 chance you're going to pay one.",
                    "label": 0
                },
                {
                    "sent": "What can you do?",
                    "label": 0
                },
                {
                    "sent": "You choose something, world flips a coin.",
                    "label": 0
                },
                {
                    "sent": "So whatever algorithm in expectation pays 1/2 every day, whatever you choose to do so in two days, you're expected cost is 2 / 2 nothing you can do about it.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, in hindsight, the best route in hindsight, is if I were to flip a coin T times, think of heads is this entails is that it's the expected value of the minimum of a number of heads and number of tails.",
                    "label": 0
                },
                {
                    "sent": "OK, so I put the coin tee times, you know I expect you over 2 heads.",
                    "label": 1
                },
                {
                    "sent": "I expect over 2 tails but the expected minimum 'cause it's going to be this variation.",
                    "label": 0
                },
                {
                    "sent": "The expected minimum ends up being T / 2 minus about order square root of T. Right?",
                    "label": 0
                },
                {
                    "sent": "'cause if you flip a coin three times, you know it's going to be this binomial like this and the binomial kind of 1st order approximation.",
                    "label": 0
                },
                {
                    "sent": "It's like T / 2 plus or minus square root is kind of sort of like kind of like a box, right?",
                    "label": 0
                },
                {
                    "sent": "So you're going to be if you flip a coin tee time you get over 2 heads plus or minus square root of T and the expected value.",
                    "label": 0
                },
                {
                    "sent": "The minimum is going to be about square root of T. Off from the other two, and so when you divide by T, the per day gap, if the world acts this way, there's no way you can help.",
                    "label": 0
                },
                {
                    "sent": "The per day gap is going to be one over Rooty.",
                    "label": 0
                },
                {
                    "sent": "You can't.",
                    "label": 0
                },
                {
                    "sent": "You can't beat that.",
                    "label": 0
                },
                {
                    "sent": "So OK, so that's just explaining why you know the game figures said great, you know we're done.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good, so learning theory, though in the 80s and 90s this problem came back and it came back by a problem called the problem of combining expert advice where we're really going to now think of N as a very large number.",
                    "label": 1
                },
                {
                    "sent": "We're going to manage.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of prediction rules, so these choices are now a bunch of different prediction rules we might be using.",
                    "label": 0
                },
                {
                    "sent": "It might be a whole space as a whole class of different possible classifiers, prediction rules we might be using, and we'd like to do nearly as well as the best of these prediction rules, so our jobs do nearly as well as the best function in that class.",
                    "label": 1
                },
                {
                    "sent": "And it's very nice algorithm of little stone wormuth in the late 80s showed how you could get a cost which does nearly as well as the best function, only off by 1 plus epsilon factor plus an additive term that was of this type is log of a number of rules over epsilon.",
                    "label": 1
                },
                {
                    "sent": "If you set that epsilon to balance the two things out, it gives you regret like the previous, but login over square root login over T instead of square root N / T. So the number of time steps to get you regret down to epsilon instead of being in over epsilon squared is like log in over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "So that's great if the number of choices you have is a large number, OK, a lot of different possibilities to pick from because.",
                    "label": 0
                },
                {
                    "sent": "You're getting good at lower at much faster, so it means that it doesn't hurt you too much to throw a lot more options into the mix.",
                    "label": 0
                },
                {
                    "sent": "So I mean, you know 'cause.",
                    "label": 0
                },
                {
                    "sent": "If an as you know to the 30th login is much better.",
                    "label": 0
                },
                {
                    "sent": "The same result.",
                    "label": 0
                },
                {
                    "sent": "Maybe not happy family something wrong.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, that's right, that's right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm not sure which is which came.",
                    "label": 0
                },
                {
                    "sent": "I thought this was 90 anyway.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, you also vote yes.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "I went to this conference and up there.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "That's true so.",
                    "label": 1
                },
                {
                    "sent": "We vote also had this kind of result, and since then there's been a lot of other work, so this is now optimal as a function of N have been a lot of work on getting the exact constants, getting the 2nd order terms, and various other things.",
                    "label": 0
                },
                {
                    "sent": "But we're not going to worry about that.",
                    "label": 0
                },
                {
                    "sent": "Now this is all in the non bandit model.",
                    "label": 0
                },
                {
                    "sent": "This is all in the model where you get full feedback.",
                    "label": 0
                },
                {
                    "sent": "So when you drive, when you're done, you find out from your friends how long each other option, and that's actually very natural in this context, because when you're trying to make a prediction, if you find out the right answer, you can figure out how well you would have done had you chosen one of the other prediction rules in the bandit model where you have any choices and you only get feedback from the choice you took, you have to at least try everything.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know maybe there's some option out there, you just haven't tried it.",
                    "label": 0
                },
                {
                    "sent": "You gotta try everything.",
                    "label": 0
                },
                {
                    "sent": "So you gotta at least.",
                    "label": 0
                },
                {
                    "sent": "Pay the rent and it ends up costing this extra factor of it.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is kind of a quick.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "History and what I want to do now.",
                    "label": 0
                },
                {
                    "sent": "Is a look at a particular problem in this area called the problem of combining expert advice?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So imagine that for whatever reason, we'd like to predict the stock market.",
                    "label": 0
                },
                {
                    "sent": "Be nice to do.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Alright, make some money.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do is we're going to ask N experts for their opinions.",
                    "label": 0
                },
                {
                    "sent": "Do they think the market is going to go up or down today?",
                    "label": 0
                },
                {
                    "sent": "So these are maybe different prediction rules that could be actual people that could be prediction rules that could be different statistics that we're looking at different.",
                    "label": 0
                },
                {
                    "sent": "You know, economic indicators, whatever, and each one is suggesting the market will go up, down, up, whatever we want to somehow combine them.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we'd like to use the advice of all these different indicators or people, or prediction rules, to somehow make our own prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, so for instance, you know maybe in day one these various experts say the markets go down, up, up.",
                    "label": 0
                },
                {
                    "sent": "We see our neighbors dog is wagging his tail and then and then we make a prediction and then we find out what happened to the market.",
                    "label": 0
                },
                {
                    "sent": "And then next day down, up, up, down and break it.",
                    "label": 0
                },
                {
                    "sent": "You think it's funny the neighbors dog is always right?",
                    "label": 0
                },
                {
                    "sent": "Well I got to make it.",
                    "label": 0
                },
                {
                    "sent": "2 examples but.",
                    "label": 0
                },
                {
                    "sent": "I think it's probably a good indicator of anything.",
                    "label": 0
                },
                {
                    "sent": "OK, but this is how the game is going to go, so we're going to get this information.",
                    "label": 0
                },
                {
                    "sent": "We're going to make our own prediction and then we either get it right or get it wrong and we get, you know.",
                    "label": 0
                },
                {
                    "sent": "We we get a mistake if we get it wrong.",
                    "label": 0
                },
                {
                    "sent": "Not a mistake.",
                    "label": 0
                },
                {
                    "sent": "We got it right.",
                    "label": 0
                },
                {
                    "sent": "OK, our job is to not make too many mistakes and the question is what can we do nearly as well as the best of these in hindsight?",
                    "label": 1
                },
                {
                    "sent": "OK, so if one of them so if none of them are any good, we don't have to do very well.",
                    "label": 0
                },
                {
                    "sent": "But if one of them ends up actually being quite good, then we want to do nearly as well as it did.",
                    "label": 0
                },
                {
                    "sent": "OK, and then so I should just emphasize that an expert here doesn't mean someone who knows anything.",
                    "label": 0
                },
                {
                    "sent": "It just means some with an opinion.",
                    "label": 0
                },
                {
                    "sent": "OK, it's just so that's why I like financial, right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just some with opinion and we don't know in advance if any of them are any good, but if one of them turns out to be good, we want to do nearly as well as it did.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the problem.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look at the easier version of this first just to get their feet wet.",
                    "label": 0
                },
                {
                    "sent": "So here's a similar question.",
                    "label": 0
                },
                {
                    "sent": "We have experts and suppose we were told that one of them is perfect.",
                    "label": 0
                },
                {
                    "sent": "It never makes a mistake, we just don't know which one.",
                    "label": 1
                },
                {
                    "sent": "Can anyone think of a strategy that would make it most logann mistakes?",
                    "label": 0
                },
                {
                    "sent": "OK, you got the experts.",
                    "label": 0
                },
                {
                    "sent": "Everyday, they're making predictions and then you get to see their predictions and make your own and your promised that one of them is perfect.",
                    "label": 0
                },
                {
                    "sent": "Just don't know which one.",
                    "label": 0
                },
                {
                    "sent": "Anyone think of a strategy for combining their opinions?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Everybody involved and working perfectly, making sense here exactly.",
                    "label": 0
                },
                {
                    "sent": "Just take majority vote and.",
                    "label": 0
                },
                {
                    "sent": "Then throw away the ones that made a mistake and that guarantees that every time you made a mistake.",
                    "label": 0
                },
                {
                    "sent": "That you at least that you cut down the set of available experts by at least a factor of two.",
                    "label": 0
                },
                {
                    "sent": "If you're lucky, you might even cut down the set even without making a mistake, but for sure, every time you make a mistake, that means at least half of the experts that were left predicted incorrectly, and so you get to cut down by at least a factor 2.",
                    "label": 0
                },
                {
                    "sent": "Everybody successful trip it takes, but it's more or less rented, no, no, they can be anything at all.",
                    "label": 0
                },
                {
                    "sent": "So you've got, you've got your experts in there making predictions and you're just taking majority votes.",
                    "label": 0
                },
                {
                    "sent": "We gotta 100 experts, 57.",
                    "label": 0
                },
                {
                    "sent": "Stay up and 43 say down so you take up if you are wrong, that means the majority of the experts were wrong.",
                    "label": 0
                },
                {
                    "sent": "'cause you went with the majority and then you in every step you throw out the ones that.",
                    "label": 0
                },
                {
                    "sent": "That made a mistake.",
                    "label": 0
                },
                {
                    "sent": "'cause member promised that one of them is perfect so we can throw out the ones making mistakes so that guarantees every time we get it wrong.",
                    "label": 0
                },
                {
                    "sent": "At least half of the ones left.",
                    "label": 0
                },
                {
                    "sent": "So every mistake we chop down by Lisa factor too.",
                    "label": 0
                },
                {
                    "sent": "So we make it most log base.",
                    "label": 0
                },
                {
                    "sent": "Two of any mistakes.",
                    "label": 0
                },
                {
                    "sent": "Schedule, we are assuming that raise one who never makes it so for the simpler version, just for the simpler version, let's assume that one of them is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a simpler version before we get to the real problem, yeah.",
                    "label": 0
                },
                {
                    "sent": "Tom.",
                    "label": 0
                },
                {
                    "sent": "Good so.",
                    "label": 0
                },
                {
                    "sent": "So we can do that.",
                    "label": 0
                },
                {
                    "sent": "In fact, maybe just to make an aside.",
                    "label": 0
                },
                {
                    "sent": "But we're not going to be going in the structure, but taking aside if we had a prior.",
                    "label": 0
                },
                {
                    "sent": "Of which we thought was more likely to be their good one.",
                    "label": 0
                },
                {
                    "sent": "We could, instead of taking a majority vote, we could wait them by the Priors.",
                    "label": 0
                },
                {
                    "sent": "And then every time you make a mistake, at least half the probability mass goes away.",
                    "label": 0
                },
                {
                    "sent": "And if the best expertise expert I so it's probability to say some PIP is the prior on expert I.",
                    "label": 0
                },
                {
                    "sent": "If you're shopping at least half probability mass, you'll make at most log of 1 / \u03c0 mistakes, so this log in is kind of like.",
                    "label": 0
                },
                {
                    "sent": "If your prior is the uniform.",
                    "label": 0
                },
                {
                    "sent": "Log one over P6.",
                    "label": 0
                },
                {
                    "sent": "OK, but we're not going to.",
                    "label": 0
                },
                {
                    "sent": "We're not going to be going that way.",
                    "label": 0
                },
                {
                    "sent": "OK, good, so sure we can solve this.",
                    "label": 0
                },
                {
                    "sent": "We just take a majority vote.",
                    "label": 1
                },
                {
                    "sent": "Overall, experts have been correct so far.",
                    "label": 0
                },
                {
                    "sent": "Each mistake cuts number available by Factor 2.",
                    "label": 0
                },
                {
                    "sent": "Which is great, 'cause it means it's OK for end to be very large 'cause we're only making log event mistakes.",
                    "label": 0
                },
                {
                    "sent": "So this is called the halving algorithm you get cut by.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let's now go to the more general question.",
                    "label": 0
                },
                {
                    "sent": "What if none of them is perfect?",
                    "label": 1
                },
                {
                    "sent": "Can we still do nearly as well as the best of them in hindsight?",
                    "label": 1
                },
                {
                    "sent": "Well, here's a strategy.",
                    "label": 0
                },
                {
                    "sent": "One strategy we could just repeat that algorithm.",
                    "label": 1
                },
                {
                    "sent": "So do the same thing as before, but once we've crossed off everybody, we say whoops, we crossed everyone off.",
                    "label": 0
                },
                {
                    "sent": "That's too bad.",
                    "label": 0
                },
                {
                    "sent": "Everyone made a mistake.",
                    "label": 0
                },
                {
                    "sent": "Restart from the beginning.",
                    "label": 0
                },
                {
                    "sent": "We could do that if we did that, then if the best expert make soft if opt is number makes the optimal expert then we make Logn mistakes for every time it makes one and then we restart, make another login mistakes every time it makes one so will make login times more mistakes in the best expert.",
                    "label": 0
                },
                {
                    "sent": "So that's something we could do.",
                    "label": 0
                },
                {
                    "sent": "It's not a great thing to do, though.",
                    "label": 0
                },
                {
                    "sent": "This is kind of wasteful algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's constantly forgetting, So what it's learned and restarting from scratch, so be nice little bit better.",
                    "label": 0
                },
                {
                    "sent": "And you can.",
                    "label": 0
                },
                {
                    "sent": "Who makes music?",
                    "label": 0
                },
                {
                    "sent": "Investors, so if he makes mistakes first time so it will cause it so still this will work with local.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's not doing that great because it just makes a mistake one time and we make login mistakes when we crossed everybody off.",
                    "label": 0
                },
                {
                    "sent": "And then we restart from scratch and it makes one more mistake and we make another login mistakes that we crossed everybody off.",
                    "label": 0
                },
                {
                    "sent": "So we're making log N times more mistakes than it did, so that's why this is not a great algorithm.",
                    "label": 0
                },
                {
                    "sent": "For exactly that reason.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we're kind of every time we just restart from scratch.",
                    "label": 0
                },
                {
                    "sent": "We forgot we learned the last time, so it's not the best thing to do.",
                    "label": 0
                },
                {
                    "sent": "So here's a better thing to do.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very natural strategy.",
                    "label": 0
                },
                {
                    "sent": "The point here is that you know when you make a mistake.",
                    "label": 0
                },
                {
                    "sent": "That doesn't completely eliminate you from possibilities 'cause we even the best ones going to be making mistakes.",
                    "label": 0
                },
                {
                    "sent": "So instead of crossing it off will just cut its weight down.",
                    "label": 1
                },
                {
                    "sent": "So in particular, consider the following algorithm, so we'll start with all the experts having a weight of 1, and then we'll predict based on a weighted majority vote, just plain the beginning.",
                    "label": 1
                },
                {
                    "sent": "Just plain majority vote and now will penalize mistakes, not by eliminating them, but just by cutting their weights in half.",
                    "label": 0
                },
                {
                    "sent": "Seems like a reasonable thing to do so.",
                    "label": 0
                },
                {
                    "sent": "So we've got weights.",
                    "label": 0
                },
                {
                    "sent": "So they saw fit weights one maybe in the first time step.",
                    "label": 0
                },
                {
                    "sent": "Three of them predict yes, one predicts no.",
                    "label": 0
                },
                {
                    "sent": "We take majority vote.",
                    "label": 0
                },
                {
                    "sent": "We predict yes.",
                    "label": 0
                },
                {
                    "sent": "If yes, was the correct answer.",
                    "label": 0
                },
                {
                    "sent": "These guys say the same weight one, but this guys wait gets cut in half.",
                    "label": 0
                },
                {
                    "sent": "If now the predictions are yes, no, no, yes, there's a wait two on no.",
                    "label": 0
                },
                {
                    "sent": "But wait 1/2 on yes we go with no.",
                    "label": 0
                },
                {
                    "sent": "If the right answer was yes then we penalize these two and their weights get cut down by.",
                    "label": 0
                },
                {
                    "sent": "So that's the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we just make a mistake.",
                    "label": 0
                },
                {
                    "sent": "We just cut their weights in half.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this turns out to actually give a pretty nice bound, and I'm going to go through the proof so so so I'm a mathira.",
                    "label": 0
                },
                {
                    "sent": "I'm a theoretician.",
                    "label": 0
                },
                {
                    "sent": "Come through here science and so.",
                    "label": 0
                },
                {
                    "sent": "You know, I do like algorithms that actually work, and it's great.",
                    "label": 0
                },
                {
                    "sent": "And I even like I like nice theorems.",
                    "label": 0
                },
                {
                    "sent": "I really like her nice proofs.",
                    "label": 0
                },
                {
                    "sent": "That's what really excites me, so this is a very nice proof so.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to show that this algorithm actually does nearly as well as the best expert in hindsight.",
                    "label": 1
                },
                {
                    "sent": "And then will improve it and make it even better.",
                    "label": 0
                },
                {
                    "sent": "So they analyze this so the analysis is a bad color.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at three things were going to look at Capital N, which is the number of mistakes we our algorithm is made so far.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at little M, the number of mistakes the best expert has made so far, and we're looking at one more thing, which is the total weight in the system.",
                    "label": 0
                },
                {
                    "sent": "The total weight in the system.",
                    "label": 0
                },
                {
                    "sent": "We start everybody with weight one, so the total way there's N experts.",
                    "label": 0
                },
                {
                    "sent": "The total weight is capital is N. Is N total way so now?",
                    "label": 0
                },
                {
                    "sent": "I claim that every time we make a mistake.",
                    "label": 0
                },
                {
                    "sent": "Let's think what happens to the total weight come every time they can mistake the weight drops by at least 25%.",
                    "label": 0
                },
                {
                    "sent": "Because you've got this total weight in the system, so here, let me just draw.",
                    "label": 0
                },
                {
                    "sent": "You've got.",
                    "label": 0
                },
                {
                    "sent": "This is the total weight in the system and we're predicting based on.",
                    "label": 0
                },
                {
                    "sent": "Is there more weight predicting one way or more weight?",
                    "label": 0
                },
                {
                    "sent": "Particularly other way.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's some this split some way like this.",
                    "label": 0
                },
                {
                    "sent": "Some weight predicts want something in some way.",
                    "label": 0
                },
                {
                    "sent": "The other way we go with the majority of the weight, and if we're wrong, remember we cut all of them down by a factor of two, so we cut them all down by a factor of two.",
                    "label": 0
                },
                {
                    "sent": "So we remove at least 25% of the weight from the system.",
                    "label": 0
                },
                {
                    "sent": "So every time we make a mistake, the total weight in the system drops by at least 25%.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "It means that after.",
                    "label": 0
                },
                {
                    "sent": "Am mistakes after capital and mistakes.",
                    "label": 1
                },
                {
                    "sent": "The total weight is at most when it started with times 3/4 to them because every time we made a mistake it dropped by at least 25%.",
                    "label": 0
                },
                {
                    "sent": "So you're multiplying it by at most 3/4.",
                    "label": 0
                },
                {
                    "sent": "So after we've made capital mistakes, the total weight is at most that we have started with times 3/4, three quarters.",
                    "label": 0
                },
                {
                    "sent": "Three course occurs every time we make a mistake.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, by definition the best expert has weight one over to the little M because.",
                    "label": 0
                },
                {
                    "sent": "Just by definition of the algorithm, we start one and every time that thing makes a mistake, it gets cut in half half half, half.",
                    "label": 0
                },
                {
                    "sent": "If it's made little mistakes, its weight is 1/2 little lamb.",
                    "label": 0
                },
                {
                    "sent": "And clearly the total weight in the system is at least the weight of this one guy, because the weights are never negative.",
                    "label": 0
                },
                {
                    "sent": "So we just set.",
                    "label": 0
                },
                {
                    "sent": "We just write that out.",
                    "label": 0
                },
                {
                    "sent": "This quality here is at least that quantity there and then you just sort of move the two the little M over here in the Four Thirds, the Big M. Over here you take the log base 2.",
                    "label": 0
                },
                {
                    "sent": "And flip that around and this is one over the log base.",
                    "label": 0
                },
                {
                    "sent": "Two 4 thirds and you get that our number of mistakes is at most 2.4 times the number of mistakes of the best expert plus additive login.",
                    "label": 0
                },
                {
                    "sent": "So what that's saying is that you know if the best expert makes a mistake 1% of the time, they will make a mistake 2.4% of the time.",
                    "label": 0
                },
                {
                    "sent": "Plus this ad of log in term kind of our learning rate.",
                    "label": 0
                },
                {
                    "sent": "So how long it takes to so figure out something interesting is going on.",
                    "label": 1
                },
                {
                    "sent": "So that's a nice a nice kind of statement.",
                    "label": 0
                },
                {
                    "sent": "So in a certain sense, we're doing nearly as well as the best expert.",
                    "label": 0
                },
                {
                    "sent": "How in the earliest, well, 2.4 times.",
                    "label": 0
                },
                {
                    "sent": "That's how nearly as well.",
                    "label": 0
                },
                {
                    "sent": "Now 2.4 times is fine if the best experts making mistake 1% we do 2.4% big deal.",
                    "label": 0
                },
                {
                    "sent": "If the best extra making mistake 20% of the time, then we're making 48%.",
                    "label": 0
                },
                {
                    "sent": "I must not so great.",
                    "label": 0
                },
                {
                    "sent": "So you might ask at least the constant ratio, but could we do better?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so the answer is yes, we can do better and here are two ideas that when you put them together really help.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just setting up the question.",
                    "label": 0
                },
                {
                    "sent": "I gotta change my colors.",
                    "label": 0
                },
                {
                    "sent": "OK so you know this is fine if if if the best exercise making mistake only say 1% of the time but not so great.",
                    "label": 1
                },
                {
                    "sent": "If it's like 20%.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here's an idea, so here's two ideas, 1.",
                    "label": 1
                },
                {
                    "sent": "Instead of taking a strict majority vote.",
                    "label": 0
                },
                {
                    "sent": "Let's use the weights as probabilities.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If 70% of the experts say the markets go up in 30%, say it's going to go down instead of just saying up, let's flip a coin and with 70% probability, say up to 30% probability, say down.",
                    "label": 0
                },
                {
                    "sent": "The reason this?",
                    "label": 0
                },
                {
                    "sent": "It can help in our analysis is remember, we're kind of viewing the world.",
                    "label": 0
                },
                {
                    "sent": "Is this sort of adverse aerial thing and so the world for the mean?",
                    "label": 0
                },
                {
                    "sent": "What's the world going to do us if we are doing active Terministic Lee, the world is going to put 51% of the experts doing the wrong thing and 49% doing the right thing.",
                    "label": 0
                },
                {
                    "sent": "And we're going to do the wrong thing.",
                    "label": 0
                },
                {
                    "sent": "And it's going to say haha, OK, but this way, 'cause it's against us.",
                    "label": 0
                },
                {
                    "sent": "But this way you see if it puts 51% on the wrong thing in 49% of the right thing where we at least have a 49% chance of doing the right thing.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you think about it, the amount of weight in the system that we remove from the system is now linearly related to our probability of making a mistake.",
                    "label": 1
                },
                {
                    "sent": "So if it's 5050, we remove a quarter of the way the system if 100% get it wrong, we remove half the way the system and we have 100% chance of making mistake.",
                    "label": 0
                },
                {
                    "sent": "If 10% go on week.",
                    "label": 0
                },
                {
                    "sent": "So the probability we're going to make a mistake is going to be nicely linearly related to the amount of weight removed from the system, so this will smooth out the worst case there won't be this of single worst case anymore.",
                    "label": 0
                },
                {
                    "sent": "All the cases will be equally good.",
                    "label": 0
                },
                {
                    "sent": "Second idea is you know why we hit by why we multiply by half just seemed like a nice number.",
                    "label": 0
                },
                {
                    "sent": "You could see what happens if you multiply it by something else, like .9.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, you get the following bound.",
                    "label": 0
                },
                {
                    "sent": "You get up the mess this mess, but this this complicated thing which I don't even want you to look at is approximately.",
                    "label": 0
                },
                {
                    "sent": "Something is approximately 1 plus epsilon over 2 times number mistakes, the best expert plus one over epsilon log in.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Epsilon this kind of learning rate.",
                    "label": 1
                },
                {
                    "sent": "If you make it small, it takes a little bit longer to learn, so you get a little worse additive term.",
                    "label": 0
                },
                {
                    "sent": "We end up doing much better compared to the best experts, so we just plug in some numbers just to get a feel of these things.",
                    "label": 0
                },
                {
                    "sent": "If you plug in 1/2 you get 1.39 times.",
                    "label": 0
                },
                {
                    "sent": "The best expert +2 natural log in.",
                    "label": 0
                },
                {
                    "sent": "If you plug in 183 epsilon, so you multiply by 1 by 7/8, then you get 1.07 times the mistakes.",
                    "label": 0
                },
                {
                    "sent": "The best expert plus eight login, so these are now pretty nice numbers.",
                    "label": 0
                },
                {
                    "sent": "So before you know 2.4 is a little bit large, but you know 1.07, it's starting to get.",
                    "label": 0
                },
                {
                    "sent": "Pretty good, so even if the game is going to go on for awhile, you might want to use a smaller epsilon because this term may mean more to you than after.",
                    "label": 0
                },
                {
                    "sent": "OK, so so these numbers.",
                    "label": 1
                },
                {
                    "sent": "These are worst case bounds on our expected number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "Now 'cause we're a randomized algorithm and the numbers are pretty good.",
                    "label": 0
                },
                {
                    "sent": "So OK, so I told you that the first part is going to be the hardest part, and here it comes.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do this analysis.",
                    "label": 0
                },
                {
                    "sent": "It's going to be 1 slide long.",
                    "label": 0
                },
                {
                    "sent": "But we'll get several things for free out of it, so so it's I think it's a very nice.",
                    "label": 0
                },
                {
                    "sent": "It's a nice argument.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I like proof.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the thing.",
                    "label": 0
                },
                {
                    "sent": "It's a very nice argument, will get several things for free once we have it, including the mini Max here so.",
                    "label": 0
                },
                {
                    "sent": "And actually, if you've seen the.",
                    "label": 0
                },
                {
                    "sent": "People are people seen Ada boost boosting, so the argument that Adaboost works actually is.",
                    "label": 0
                },
                {
                    "sent": "Incredibly similar to this, it's the same analysis, really.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's analyze like this.",
                    "label": 0
                },
                {
                    "sent": "Let's say that at time T we have a fraction.",
                    "label": 0
                },
                {
                    "sent": "Redraw this Pi here.",
                    "label": 0
                },
                {
                    "sent": "Let's say that we have a fraction FT of our weight on the experts that made a mistake.",
                    "label": 1
                },
                {
                    "sent": "So let's say these guys made a mistake.",
                    "label": 0
                },
                {
                    "sent": "And these guys got it right.",
                    "label": 0
                },
                {
                    "sent": "Then this is exactly our probability that we make a mistake because after all that's our algorithm algorithm says how much weight we split.",
                    "label": 0
                },
                {
                    "sent": "You know how much weight set up, how much weight sat down, and we flip a coin.",
                    "label": 0
                },
                {
                    "sent": "So our probability that we make a mistake is exactly the fraction of the total weight on the experts that made a mistake.",
                    "label": 0
                },
                {
                    "sent": "OK, so so we have probability FT of making a mistake, and in addition we remove an epsilon times FT fraction of the weight from the system because we're multiplying these guys.",
                    "label": 1
                },
                {
                    "sent": "By 1 minus epsilon and so if we do that, the amount of weight that gets removed from the system is an epsilon FT fraction of the total weight.",
                    "label": 0
                },
                {
                    "sent": "Just like before, we were multiplying by 1/2, so we removed.",
                    "label": 0
                },
                {
                    "sent": "Half of that now we're moving epsilon fraction of that.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the total weight in the system, it starts at this end and then after time one you remove an epsilon times F1 fraction of the weight from the system and after that I'm two.",
                    "label": 0
                },
                {
                    "sent": "We remove an epsilon F2 fraction away from the system every time I move epsilon times FT fractional weight from the system.",
                    "label": 0
                },
                {
                    "sent": "So the final weight is going to look like that.",
                    "label": 0
                },
                {
                    "sent": "That's a product and it's messy.",
                    "label": 0
                },
                {
                    "sent": "Let's take the log to turn into a sum so it will be a little nicer.",
                    "label": 0
                },
                {
                    "sent": "Take the log, make it asam.",
                    "label": 0
                },
                {
                    "sent": "We get the log in.",
                    "label": 0
                },
                {
                    "sent": "We get the the log of this product, which is the sum of logs.",
                    "label": 0
                },
                {
                    "sent": "This is messy.",
                    "label": 0
                },
                {
                    "sent": "Yuck.",
                    "label": 0
                },
                {
                    "sent": "But let's approximate this log of 1 -- X is less than negative X. OK, so that's this picture here.",
                    "label": 0
                },
                {
                    "sent": "This is the function negative X.",
                    "label": 0
                },
                {
                    "sent": "And this is the function log of 1 -- X log of 1 -- X is.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "To make it better.",
                    "label": 0
                },
                {
                    "sent": "We got epsilon times the sum of the S&P's and the nice thing here is this is exactly our expected total number.",
                    "label": 0
                },
                {
                    "sent": "Mistakes is the probability making mistake of time one plus time two times.",
                    "label": 0
                },
                {
                    "sent": "This is how many mistakes we make an expectation, so it's great.",
                    "label": 0
                },
                {
                    "sent": "We've directly related the weight in the system to our probability.",
                    "label": 0
                },
                {
                    "sent": "Our expected number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "This is exactly login minus epsilon times our expected number of mistakes, and now the rest of the argument.",
                    "label": 0
                },
                {
                    "sent": "That wasn't too bad, was it so so if the best expert makes little mistakes.",
                    "label": 0
                },
                {
                    "sent": "Well, the final weight is at least his way.",
                    "label": 0
                },
                {
                    "sent": "It's wait, it's 1 minus epsilon to the limit starts at one and we hit it with one minus epsilon every time it made a mistake.",
                    "label": 0
                },
                {
                    "sent": "So it's wait.",
                    "label": 0
                },
                {
                    "sent": "Is this thing here?",
                    "label": 0
                },
                {
                    "sent": "We're just taking the log 'cause we're looking at the log, so its weight is 1 minus epsilon VM the final way it's gotta be at least that guys wait for the log of the final way.",
                    "label": 0
                },
                {
                    "sent": "It's gotta be at least log of that guys.",
                    "label": 0
                },
                {
                    "sent": "Wait, so it's gotta be at least this and so this quantity is at least that quantity.",
                    "label": 0
                },
                {
                    "sent": "So that's it.",
                    "label": 0
                },
                {
                    "sent": "Actually, you just solve this quandary here is at least that quantity there.",
                    "label": 0
                },
                {
                    "sent": "And when you solve, you just gotta move the big M on the right side and you got the little lamb on the left side and divide by the epsilon.",
                    "label": 0
                },
                {
                    "sent": "Get exactly this funky formula here.",
                    "label": 0
                },
                {
                    "sent": "That's all came from just that.",
                    "label": 0
                },
                {
                    "sent": "And then you just use Taylor series to approximate.",
                    "label": 0
                },
                {
                    "sent": "Too bad.",
                    "label": 0
                },
                {
                    "sent": "Good, yes, so so all the weights are positive.",
                    "label": 0
                },
                {
                    "sent": "So the final weight is the sum of every, so it's a sum of.",
                    "label": 0
                },
                {
                    "sent": "Anyway, I'm saying this one guy has this way.",
                    "label": 0
                },
                {
                    "sent": "And the other ones have some non negative way.",
                    "label": 0
                },
                {
                    "sent": "So when you Add all the weights together you get something bigger than this.",
                    "label": 0
                },
                {
                    "sent": "Never to be quite there.",
                    "label": 0
                },
                {
                    "sent": "Apparently on the right path.",
                    "label": 0
                },
                {
                    "sent": "Well there then learning W final is is probably strictly larger, yeah?",
                    "label": 0
                },
                {
                    "sent": "Well, it can't be large.",
                    "label": 0
                },
                {
                    "sent": "It can't be equal 'cause none of the weights go to 0, so this is the sum of everybody's weights.",
                    "label": 0
                },
                {
                    "sent": "So it's a sum of numbers that are all positive and one of them is this and the others are bigger than 0.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter, I think.",
                    "label": 0
                },
                {
                    "sent": "I wrote less than or equal to hear just as good, so we've got.",
                    "label": 0
                },
                {
                    "sent": "We've got this bound and then.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So that's that's the kind of thing we have we do nearly as well as the best expert plus this additive term.",
                    "label": 0
                },
                {
                    "sent": "And now the connected to the first part.",
                    "label": 0
                },
                {
                    "sent": "This regret business if we set our epsilon.",
                    "label": 0
                },
                {
                    "sent": "If we knew in advance how many mistakes the best expert was going to make.",
                    "label": 0
                },
                {
                    "sent": "If we knew that number in advance, we could set our epsilon to equalize the two losses.",
                    "label": 0
                },
                {
                    "sent": "So the two losses are is the epsilon.",
                    "label": 0
                },
                {
                    "sent": "So we're making the M plus an epsilon M plus this.",
                    "label": 0
                },
                {
                    "sent": "If we set that epsilon M to equal this thing, we can do that by setting up onto this one here.",
                    "label": 0
                },
                {
                    "sent": "If we knew in advance how many mistakes the best expert would make if we didn't know in advance one thing you could do is make a guess.",
                    "label": 0
                },
                {
                    "sent": "As you run your algorithm, if once the best expert goes above that guess you restart with the smaller epsilon, the smaller one smaller one the telescoping series.",
                    "label": 0
                },
                {
                    "sent": "In any case, if you do that, you'll get the expected mistakes.",
                    "label": 0
                },
                {
                    "sent": "At most the number mistakes.",
                    "label": 1
                },
                {
                    "sent": "The best expert plus this additive.",
                    "label": 0
                },
                {
                    "sent": "Think it looks roughly like so two times number makes the best expert times log in so that in square root.",
                    "label": 0
                },
                {
                    "sent": "So all I'm doing here if you just plug that value into there and you.",
                    "label": 0
                },
                {
                    "sent": "M Plus the epsilon M plus the one over epsilon login plugin that five epsilon.",
                    "label": 0
                },
                {
                    "sent": "You get that quality there and then just using the fact that you know worst case number of experts.",
                    "label": 0
                },
                {
                    "sent": "The best expert at worst makes a mistake every single time.",
                    "label": 0
                },
                {
                    "sent": "This would not be a very good expert but it works every single time.",
                    "label": 0
                },
                {
                    "sent": "So we just plug that in here.",
                    "label": 0
                },
                {
                    "sent": "It's exactly the bound we had on that first slide.",
                    "label": 0
                },
                {
                    "sent": "#6 the best experts plus square root T login.",
                    "label": 0
                },
                {
                    "sent": "So per time step is it's going square root of T. Been logging in, so this is exactly that.",
                    "label": 0
                },
                {
                    "sent": "That that bound we had in the earlier slide.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "There was a regret is going to zero as per time step.",
                    "label": 0
                },
                {
                    "sent": "Our per average regret.",
                    "label": 0
                },
                {
                    "sent": "So if you divide this by T it's dropping like square root of T and we have the extra login.",
                    "label": 0
                },
                {
                    "sent": "So great, so we.",
                    "label": 0
                },
                {
                    "sent": "So we did it.",
                    "label": 0
                },
                {
                    "sent": "OK, let me see.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anywhere OK?",
                    "label": 0
                },
                {
                    "sent": "So let me just go to.",
                    "label": 0
                },
                {
                    "sent": "There's one part that I want to connect it back to the first problem, and so there's one issue here.",
                    "label": 0
                },
                {
                    "sent": "We're talking here about having an predictors like of the stock market.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit different from having an routes to choose from, like we're taking majority vote.",
                    "label": 0
                },
                {
                    "sent": "So that means, like one experts consistently wrong.",
                    "label": 0
                },
                {
                    "sent": "Presumably he could generalize the address in sun.",
                    "label": 0
                },
                {
                    "sent": "Rachel not make the Quakes right.",
                    "label": 0
                },
                {
                    "sent": "You're not.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "You can actually improve doing.",
                    "label": 0
                },
                {
                    "sent": "You can do much better in that situation, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so it's true that maybe you want to instantiate an expert, which is the opposite of whatever someone else says, and then and then there's always guaranteed the best expert makes a mistake that most half the time, right?",
                    "label": 0
                },
                {
                    "sent": "This is totally accrued kind of thing here, right?",
                    "label": 0
                },
                {
                    "sent": "So you might want to do that if you thought that your experts, for instance, might be adverse aerial.",
                    "label": 0
                },
                {
                    "sent": "So you could always do that.",
                    "label": 0
                },
                {
                    "sent": "You could you could.",
                    "label": 0
                },
                {
                    "sent": "Instantiating be better than setting negative.",
                    "label": 0
                },
                {
                    "sent": "Just have a shadow expert, just as the opposite of whatever.",
                    "label": 0
                },
                {
                    "sent": "It's probably good for financial.",
                    "label": 0
                },
                {
                    "sent": "OK. Good, So what if you have an options to choose from, not end predictors, so you can't really take the majority vote.",
                    "label": 0
                },
                {
                    "sent": "You can't say I want to take the majority vote of this path.",
                    "label": 0
                },
                {
                    "sent": "This path in this path or not voting for not predicting things.",
                    "label": 0
                },
                {
                    "sent": "We're not combining and experts, we're just choosing one.",
                    "label": 1
                },
                {
                    "sent": "But here's a nice thing.",
                    "label": 0
                },
                {
                    "sent": "This randomized procedure still makes sense, because another way to think about the algorithm we just just analyzed.",
                    "label": 0
                },
                {
                    "sent": "Remember, we said we ask all the experts, do you think the markets go up or down?",
                    "label": 0
                },
                {
                    "sent": "You look at how much probability mass set up and how much probability mass sat down, and you pick random.",
                    "label": 0
                },
                {
                    "sent": "That's the same thing.",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                },
                {
                    "sent": "Choosing an expert at random with probability proportional to its weight and going with what it said.",
                    "label": 0
                },
                {
                    "sent": "It's the same thing if if you do this, if you just choose an expert at random with probably proportional sway and then just go with what it said, then if 70% of the weight is saying up, there's exactly a 70% probability that you're going to say up also, so it's just another way of thinking of the same algorithm.",
                    "label": 1
                },
                {
                    "sent": "OK, and so this you can apply.",
                    "label": 0
                },
                {
                    "sent": "So what this is saying is what the way that you should do your route choosing is.",
                    "label": 0
                },
                {
                    "sent": "You give a weight to each route and now you're going to pick her out at random with probability proportional to the weight of that route.",
                    "label": 1
                },
                {
                    "sent": "OK, and then still the same algorithm.",
                    "label": 0
                },
                {
                    "sent": "You get the same analysis.",
                    "label": 0
                },
                {
                    "sent": "The only technical thing that just to do one last little bit to fix up 1 technical thing.",
                    "label": 0
                },
                {
                    "sent": "I've been assuming that the costs are either zero or one the experts.",
                    "label": 0
                },
                {
                    "sent": "They make a mistake.",
                    "label": 0
                },
                {
                    "sent": "They don't make a mistake, but if you have routes you know it's going to be like 0 hours in one hour.",
                    "label": 0
                },
                {
                    "sent": "Weather Route takes 25 minutes.",
                    "label": 0
                },
                {
                    "sent": "What do you do then?",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out you can do the simplest thing so.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's in between.",
                    "label": 0
                },
                {
                    "sent": "So before we were saying if somebody makes a mistake, if they have cost one, you multiply by 1 minus epsilon and if they have cost 0 you don't do anything.",
                    "label": 0
                },
                {
                    "sent": "You keep them at one, so just do a linear interpolation.",
                    "label": 0
                },
                {
                    "sent": "If they have cost CI between zero 1 * 1 minus CI times epsilon.",
                    "label": 0
                },
                {
                    "sent": "So this one you got before zero we have we had before the half human minus epsilon for two.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, the analysis everything goes through.",
                    "label": 0
                },
                {
                    "sent": "It's our expected cost.",
                    "label": 0
                },
                {
                    "sent": "Is exactly.",
                    "label": 0
                },
                {
                    "sent": "This is the probability we choose expert.",
                    "label": 0
                },
                {
                    "sent": "I times the cost of expert eye.",
                    "label": 1
                },
                {
                    "sent": "That's exactly our expected cost.",
                    "label": 1
                },
                {
                    "sent": "The amount of weight removed is exactly.",
                    "label": 1
                },
                {
                    "sent": "We remove this much weight.",
                    "label": 0
                },
                {
                    "sent": "It's exactly epsilon times hour.",
                    "label": 0
                },
                {
                    "sent": "Expected cost in terms of the fraction, the fraction of weight removed is exactly epsilon times our cost.",
                    "label": 0
                },
                {
                    "sent": "That's the only thing we used.",
                    "label": 0
                },
                {
                    "sent": "Everything else is exactly the same, so all I'm saying is that the analysis we did, it's just plug in these things in the exact same rest of the analysis.",
                    "label": 0
                },
                {
                    "sent": "We get the exact same bound, so so I'm not cheating you that we can handle the full problem with the costs in between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So the rest of the proof continues before and so now we can drive to work.",
                    "label": 1
                },
                {
                    "sent": "Assuming full feedback, we haven't gotten to the bandit problem yet, so maybe this is a good time.",
                    "label": 0
                }
            ]
        }
    }
}