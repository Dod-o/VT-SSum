{
    "id": "zys4xzfqrwb7li2zod4xsevoyj3sr7ua",
    "title": "The Next Big Thing In Science",
    "info": {
        "author": [
            "Adrian Mladenic Grobelnik, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Nov. 14, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/sikdd2019_mladenic_grobelnik_next_big_thing/",
    "segmentation": [
        [
            "Hello everyone, so if you don't know me my name is Adrian and today I will be presenting our work on predicting the next big thing in science."
        ],
        [
            "So here is a brief outline of my talk.",
            "I'll begin by stating our hypothesis followed by introduction.",
            "Then I will give you some insight into our data description followed by our topic representation.",
            "Then I'll describe our problem and the algorithm, and I'll conclude with some experimental results and discussion."
        ],
        [
            "OK, So what is our hypothesis?",
            "It is that scientific topics that will be important in the future already exist in today's scientific articles.",
            "So knowing this we can apply machine learning methods on large databases of such articles to predict what these, or to find what these topics are.",
            "Namely in our case we use the Microsoft academic graph.",
            "So more specifically, we defined the machine learning problems such that the model predicts early indicators, which would suggest which of these scientific topics in today's literature can become the emerging or the next big things in science."
        ],
        [
            "So here's a brief introduction.",
            "So as you might know, is becoming increasingly more difficult to predict.",
            "To keep track of all of today's current and relevant scientific topics, let alone predict what the emerging ones will be.",
            "But Luckily for us, the quality and quantity of digitized scientific publications is also increasing, making it easier to analyze such data, especially in the forms of large datasets of such publications such as the Microsoft Academic Graph or a minor, and so on.",
            "So there's already some existing research that takes advantage of this which is related to ours.",
            "Some papers do.",
            "One paper predicts the rising authors in academia using their personal and social features.",
            "Then other work also predicts emerging topics based only on the Citation Co citation structures from the datasets, and then finally, another paper predicts uses the appearance rate of mesh keywords in the Pub Med database to predict which of these keywords will become the next big things in science or will become popular in our work, we generate the engrams themselves and the engrams trends directly from the paper titles.",
            "To predict which of the topics or engrams will become the which ones are currently emerging and which of them will become popular in the future."
        ],
        [
            "So a bit more about our data, so one of the things that we have, we rely on our work in our work is the consistency.",
            "The naming, consistency amongst the scientific community.",
            "When it comes to scientific topics, and this allows us to effectively track the evolution of such topics very effectively using only the titles of the papers.",
            "So the data that we use is from the Microsoft academic graph.",
            "It's about 100 gigabytes in size, has 125 million articles.",
            "The articles are from the 1800s till 2015 and we're using the 2016 version so each article is essentially described by around 4 five things, so the title it's authors, the institutions the authors are affiliated to the Journal or conference that it was published in, and the Europe publication.",
            "There is the link to the Microsoft Academic Graph.",
            "So from this data we extracted around 2.5 million candidate topics.",
            "Each of them is represented by an engram.",
            "One to five words of length.",
            "And for an engram to be considered a topic, it has to appear at least 100 times in the entire datasets.",
            "So here you can see the distribution of these N grams that we can consider as topics.",
            "As you can see there is there mostly two and three grams, also some 1, four and five.",
            "And there is around 2.5 million of them."
        ],
        [
            "OK, now onto our architecture.",
            "So as mentioned before we use the paper titles to extract the engrams which act as a proxy for our scientific topics.",
            "Now that we have 2.5 million of these engrams, we only use the engrams that appear consecutively for at least 15 years.",
            "So every year at least it is mentioned in at least one paper title.",
            "Now this reduces the number down from 2.5 to 1 million of them.",
            "Now each of these 1 million engrams are represented by affiliations, conferences and journals.",
            "Of the of the papers that mention them more specifically, we give two figures for each of these.",
            "So the first is the total frequency of appearance of the ngram, and then the second is the slope of the linear regression done through the points through the frequency points on a time frequency graph over the time span of 10 years.",
            "More than 10 years later.",
            "So now that we have our representation, we can do define our machine learning problem.",
            "So we train our models on the 1st 10 years of an engrams performance and we this model is meant to predict the engrams performance in the next five years in terms of its popularity.",
            "Um?"
        ],
        [
            "So now a bit more about topic representation for our topic representation.",
            "So each engram is represented by roughly 55,000 features.",
            "Of these there are 23,000 journals, 1300 conferences and 30,000 affiliations and our model classifieds.",
            "These engrams either positive or negative, so an engram is classified as positive if it became popular within a span of 15 years where it appeared consecutively each year.",
            "But how do we define popular?",
            "We define popular.",
            "By the largest slope difference of ngram frequency between the first 10 years before its popularity in the next five years, when it became popular from all of the years where the engram appears consecutively for at least 15 years.",
            "So, for example, if no ones talking about the engram in the first 10 years, maybe the slope is pretty low, maybe 0.5 because its frequency is low and then next five years it becomes very popular.",
            "Lots of people become taught and start talking about it, so the slope goes very very very high, maybe 10 or 15.",
            "And as you can see, that's a large difference in slope, and this large difference in slope is how we define popularity.",
            "If the difference is no, is not large enough, then that is the engram is classified as negative.",
            "Yes.",
            "We varied the threshold into data so it's from one to five, but more on that later, and so the observed slow difference in our data.",
            "If we look at our data around 34% of our examples, 34% of the engrams have a slope difference of higher than one at least at least one point along their lifespan, and 20% of them have slope at least higher than five at any point.",
            "So now here's an example feature vector taken directly from our data, more or less so here.",
            "The ngram is SVM.",
            "It's classified as positive an if we look at the first 2 features.",
            "The first one says that in the time span of the 10 years before its popularity, it appeared four times in papers that are affiliated to the Max Planck Society and these appearances, or distributed across the 10 years so that the slope is free, meaning that it has been becoming increasingly more popular.",
            "And then of course we have other.",
            "Features as well as such as mentions and Nips Conference, Journal of Machine Learning and Research, and many more."
        ],
        [
            "Now onto the problem description, an algorithm.",
            "So our statistical model does two things.",
            "Essentially, the first is the text, early indicators of promising scientific topics, and the 2nd is classified scientific topics as whether or not they will become popular in the next five years.",
            "For our statistical model, we use the perceptual Max margin algorithm, which is essentially just an improved version of the perception traditional Perceptron algorithm.",
            "It uses to margins, one for each class, which are calculated based on the number of misclassified examples in each previous epoch.",
            "The learning rate is 0.0.",
            "Two of their enormous classifications.",
            "If there are misclassifications.",
            "It's also calculated based on the number of misclassifications in the previous iteration."
        ],
        [
            "Now onto the experimental results.",
            "So we split the topics into a training and test set.",
            "70% of them were training, 30% would test, and then we ran the model through 3000 training impulse on the training set.",
            "And here are the results.",
            "So this is the slope difference.",
            "This is what you asked before.",
            "This is the minimum threshold, so if the slope difference is larger than one here for example, then the example is classified as positive or as having become popular in the five years after the 10.",
            "So as you can see, as the threshold increases, as the criteria become more difficult for an engram to become popular, the problem difficulty also increases, as this is a performance decrease.",
            "As you can see, and this is very likely due to the fact that there is simply less positive examples as the slope difference increases, making it harder classification problem.",
            "So our informal model interpretation here is that more promising topics get more attention from important institutions, journals and conferences, which likely leads to the rest of the scientific community.",
            "Noticing this, and consequently publishing their own work on it, if we look at the first row in the table, we can see the precision is 74%.",
            "Recall a 72 F 173, so these are some pretty pretty OK results, especially since the majority class here is 66%, which is the negative examples.",
            "But of course the performance decreases as the threshold increases."
        ],
        [
            "Now that we have a model, we can also use it to predict what the actual future popular topics will be.",
            "So we took all the topics that appear consecutively each year from 2006 to 2015 to predict which of them will become the next big thing in science in five years from 2015, which is almost now actually so.",
            "The topics we the topic, some of the topics our model predicted is promising.",
            "Are proton proton collisions, Higgs bozon search for dark matter, molecular dynamics simulations?",
            "Now if we look into the features of these topics, for example for search for dark matter, we can see one of the features is that there were 65 papers affiliated with Purdue University in the 10 years before its popularity with a slope of 4.1, four.",
            "So in the publications kept increasing.",
            "Another more notable feature would be for proton proton collisions, where we have 8674 papers published by CERN with a slope of 295.",
            "So this means there was a seriously large increase in publications on the topic of program program collisions in the short time span of 10 years from scientists affiliated with CERN.",
            "Our model.",
            "Also, we also observed some of the most important overall features from our model, so some of them include affiliations such as CERN.",
            "Columbia College Princeton University an journals such as Journal Proteomics and Bioinformatics.",
            "Uh."
        ],
        [
            "I'll just like to finish off with some discussion, so the main contributions of our work are the proposed problem definition, along with the data representation that data representation which was proposed and the topics we identified as promising to be the next big thing in science.",
            "Some of our future work might include applying our approach on updated data set so Microsoft Academic Graph 2019, which has almost twice as much articles now.",
            "We could also consider paper abstracts and or citation graph structures to add to our representation.",
            "We could also validate our approach on other datasets and compare different machine machine learning algorithms and topic representations to see how our approach did compared to them.",
            "Lastly, we could also do a more in depth analysis of the topics we predicted as promising to see why it was these topics specifically.",
            "And Lastly, we could also we have also considered creating a public publicly accessible online version of this system.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everyone, so if you don't know me my name is Adrian and today I will be presenting our work on predicting the next big thing in science.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a brief outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "I'll begin by stating our hypothesis followed by introduction.",
                    "label": 0
                },
                {
                    "sent": "Then I will give you some insight into our data description followed by our topic representation.",
                    "label": 1
                },
                {
                    "sent": "Then I'll describe our problem and the algorithm, and I'll conclude with some experimental results and discussion.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is our hypothesis?",
                    "label": 0
                },
                {
                    "sent": "It is that scientific topics that will be important in the future already exist in today's scientific articles.",
                    "label": 1
                },
                {
                    "sent": "So knowing this we can apply machine learning methods on large databases of such articles to predict what these, or to find what these topics are.",
                    "label": 1
                },
                {
                    "sent": "Namely in our case we use the Microsoft academic graph.",
                    "label": 1
                },
                {
                    "sent": "So more specifically, we defined the machine learning problems such that the model predicts early indicators, which would suggest which of these scientific topics in today's literature can become the emerging or the next big things in science.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a brief introduction.",
                    "label": 0
                },
                {
                    "sent": "So as you might know, is becoming increasingly more difficult to predict.",
                    "label": 0
                },
                {
                    "sent": "To keep track of all of today's current and relevant scientific topics, let alone predict what the emerging ones will be.",
                    "label": 0
                },
                {
                    "sent": "But Luckily for us, the quality and quantity of digitized scientific publications is also increasing, making it easier to analyze such data, especially in the forms of large datasets of such publications such as the Microsoft Academic Graph or a minor, and so on.",
                    "label": 1
                },
                {
                    "sent": "So there's already some existing research that takes advantage of this which is related to ours.",
                    "label": 0
                },
                {
                    "sent": "Some papers do.",
                    "label": 0
                },
                {
                    "sent": "One paper predicts the rising authors in academia using their personal and social features.",
                    "label": 1
                },
                {
                    "sent": "Then other work also predicts emerging topics based only on the Citation Co citation structures from the datasets, and then finally, another paper predicts uses the appearance rate of mesh keywords in the Pub Med database to predict which of these keywords will become the next big things in science or will become popular in our work, we generate the engrams themselves and the engrams trends directly from the paper titles.",
                    "label": 1
                },
                {
                    "sent": "To predict which of the topics or engrams will become the which ones are currently emerging and which of them will become popular in the future.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a bit more about our data, so one of the things that we have, we rely on our work in our work is the consistency.",
                    "label": 0
                },
                {
                    "sent": "The naming, consistency amongst the scientific community.",
                    "label": 0
                },
                {
                    "sent": "When it comes to scientific topics, and this allows us to effectively track the evolution of such topics very effectively using only the titles of the papers.",
                    "label": 0
                },
                {
                    "sent": "So the data that we use is from the Microsoft academic graph.",
                    "label": 0
                },
                {
                    "sent": "It's about 100 gigabytes in size, has 125 million articles.",
                    "label": 0
                },
                {
                    "sent": "The articles are from the 1800s till 2015 and we're using the 2016 version so each article is essentially described by around 4 five things, so the title it's authors, the institutions the authors are affiliated to the Journal or conference that it was published in, and the Europe publication.",
                    "label": 0
                },
                {
                    "sent": "There is the link to the Microsoft Academic Graph.",
                    "label": 1
                },
                {
                    "sent": "So from this data we extracted around 2.5 million candidate topics.",
                    "label": 0
                },
                {
                    "sent": "Each of them is represented by an engram.",
                    "label": 0
                },
                {
                    "sent": "One to five words of length.",
                    "label": 0
                },
                {
                    "sent": "And for an engram to be considered a topic, it has to appear at least 100 times in the entire datasets.",
                    "label": 0
                },
                {
                    "sent": "So here you can see the distribution of these N grams that we can consider as topics.",
                    "label": 0
                },
                {
                    "sent": "As you can see there is there mostly two and three grams, also some 1, four and five.",
                    "label": 0
                },
                {
                    "sent": "And there is around 2.5 million of them.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now onto our architecture.",
                    "label": 0
                },
                {
                    "sent": "So as mentioned before we use the paper titles to extract the engrams which act as a proxy for our scientific topics.",
                    "label": 1
                },
                {
                    "sent": "Now that we have 2.5 million of these engrams, we only use the engrams that appear consecutively for at least 15 years.",
                    "label": 0
                },
                {
                    "sent": "So every year at least it is mentioned in at least one paper title.",
                    "label": 0
                },
                {
                    "sent": "Now this reduces the number down from 2.5 to 1 million of them.",
                    "label": 0
                },
                {
                    "sent": "Now each of these 1 million engrams are represented by affiliations, conferences and journals.",
                    "label": 1
                },
                {
                    "sent": "Of the of the papers that mention them more specifically, we give two figures for each of these.",
                    "label": 1
                },
                {
                    "sent": "So the first is the total frequency of appearance of the ngram, and then the second is the slope of the linear regression done through the points through the frequency points on a time frequency graph over the time span of 10 years.",
                    "label": 1
                },
                {
                    "sent": "More than 10 years later.",
                    "label": 0
                },
                {
                    "sent": "So now that we have our representation, we can do define our machine learning problem.",
                    "label": 0
                },
                {
                    "sent": "So we train our models on the 1st 10 years of an engrams performance and we this model is meant to predict the engrams performance in the next five years in terms of its popularity.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now a bit more about topic representation for our topic representation.",
                    "label": 0
                },
                {
                    "sent": "So each engram is represented by roughly 55,000 features.",
                    "label": 1
                },
                {
                    "sent": "Of these there are 23,000 journals, 1300 conferences and 30,000 affiliations and our model classifieds.",
                    "label": 1
                },
                {
                    "sent": "These engrams either positive or negative, so an engram is classified as positive if it became popular within a span of 15 years where it appeared consecutively each year.",
                    "label": 1
                },
                {
                    "sent": "But how do we define popular?",
                    "label": 0
                },
                {
                    "sent": "We define popular.",
                    "label": 0
                },
                {
                    "sent": "By the largest slope difference of ngram frequency between the first 10 years before its popularity in the next five years, when it became popular from all of the years where the engram appears consecutively for at least 15 years.",
                    "label": 1
                },
                {
                    "sent": "So, for example, if no ones talking about the engram in the first 10 years, maybe the slope is pretty low, maybe 0.5 because its frequency is low and then next five years it becomes very popular.",
                    "label": 0
                },
                {
                    "sent": "Lots of people become taught and start talking about it, so the slope goes very very very high, maybe 10 or 15.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, that's a large difference in slope, and this large difference in slope is how we define popularity.",
                    "label": 0
                },
                {
                    "sent": "If the difference is no, is not large enough, then that is the engram is classified as negative.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "We varied the threshold into data so it's from one to five, but more on that later, and so the observed slow difference in our data.",
                    "label": 0
                },
                {
                    "sent": "If we look at our data around 34% of our examples, 34% of the engrams have a slope difference of higher than one at least at least one point along their lifespan, and 20% of them have slope at least higher than five at any point.",
                    "label": 0
                },
                {
                    "sent": "So now here's an example feature vector taken directly from our data, more or less so here.",
                    "label": 0
                },
                {
                    "sent": "The ngram is SVM.",
                    "label": 0
                },
                {
                    "sent": "It's classified as positive an if we look at the first 2 features.",
                    "label": 0
                },
                {
                    "sent": "The first one says that in the time span of the 10 years before its popularity, it appeared four times in papers that are affiliated to the Max Planck Society and these appearances, or distributed across the 10 years so that the slope is free, meaning that it has been becoming increasingly more popular.",
                    "label": 0
                },
                {
                    "sent": "And then of course we have other.",
                    "label": 0
                },
                {
                    "sent": "Features as well as such as mentions and Nips Conference, Journal of Machine Learning and Research, and many more.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now onto the problem description, an algorithm.",
                    "label": 0
                },
                {
                    "sent": "So our statistical model does two things.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the first is the text, early indicators of promising scientific topics, and the 2nd is classified scientific topics as whether or not they will become popular in the next five years.",
                    "label": 1
                },
                {
                    "sent": "For our statistical model, we use the perceptual Max margin algorithm, which is essentially just an improved version of the perception traditional Perceptron algorithm.",
                    "label": 1
                },
                {
                    "sent": "It uses to margins, one for each class, which are calculated based on the number of misclassified examples in each previous epoch.",
                    "label": 0
                },
                {
                    "sent": "The learning rate is 0.0.",
                    "label": 0
                },
                {
                    "sent": "Two of their enormous classifications.",
                    "label": 0
                },
                {
                    "sent": "If there are misclassifications.",
                    "label": 0
                },
                {
                    "sent": "It's also calculated based on the number of misclassifications in the previous iteration.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now onto the experimental results.",
                    "label": 1
                },
                {
                    "sent": "So we split the topics into a training and test set.",
                    "label": 0
                },
                {
                    "sent": "70% of them were training, 30% would test, and then we ran the model through 3000 training impulse on the training set.",
                    "label": 0
                },
                {
                    "sent": "And here are the results.",
                    "label": 1
                },
                {
                    "sent": "So this is the slope difference.",
                    "label": 0
                },
                {
                    "sent": "This is what you asked before.",
                    "label": 0
                },
                {
                    "sent": "This is the minimum threshold, so if the slope difference is larger than one here for example, then the example is classified as positive or as having become popular in the five years after the 10.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, as the threshold increases, as the criteria become more difficult for an engram to become popular, the problem difficulty also increases, as this is a performance decrease.",
                    "label": 0
                },
                {
                    "sent": "As you can see, and this is very likely due to the fact that there is simply less positive examples as the slope difference increases, making it harder classification problem.",
                    "label": 0
                },
                {
                    "sent": "So our informal model interpretation here is that more promising topics get more attention from important institutions, journals and conferences, which likely leads to the rest of the scientific community.",
                    "label": 1
                },
                {
                    "sent": "Noticing this, and consequently publishing their own work on it, if we look at the first row in the table, we can see the precision is 74%.",
                    "label": 0
                },
                {
                    "sent": "Recall a 72 F 173, so these are some pretty pretty OK results, especially since the majority class here is 66%, which is the negative examples.",
                    "label": 0
                },
                {
                    "sent": "But of course the performance decreases as the threshold increases.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now that we have a model, we can also use it to predict what the actual future popular topics will be.",
                    "label": 0
                },
                {
                    "sent": "So we took all the topics that appear consecutively each year from 2006 to 2015 to predict which of them will become the next big thing in science in five years from 2015, which is almost now actually so.",
                    "label": 1
                },
                {
                    "sent": "The topics we the topic, some of the topics our model predicted is promising.",
                    "label": 0
                },
                {
                    "sent": "Are proton proton collisions, Higgs bozon search for dark matter, molecular dynamics simulations?",
                    "label": 1
                },
                {
                    "sent": "Now if we look into the features of these topics, for example for search for dark matter, we can see one of the features is that there were 65 papers affiliated with Purdue University in the 10 years before its popularity with a slope of 4.1, four.",
                    "label": 0
                },
                {
                    "sent": "So in the publications kept increasing.",
                    "label": 0
                },
                {
                    "sent": "Another more notable feature would be for proton proton collisions, where we have 8674 papers published by CERN with a slope of 295.",
                    "label": 0
                },
                {
                    "sent": "So this means there was a seriously large increase in publications on the topic of program program collisions in the short time span of 10 years from scientists affiliated with CERN.",
                    "label": 0
                },
                {
                    "sent": "Our model.",
                    "label": 1
                },
                {
                    "sent": "Also, we also observed some of the most important overall features from our model, so some of them include affiliations such as CERN.",
                    "label": 0
                },
                {
                    "sent": "Columbia College Princeton University an journals such as Journal Proteomics and Bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll just like to finish off with some discussion, so the main contributions of our work are the proposed problem definition, along with the data representation that data representation which was proposed and the topics we identified as promising to be the next big thing in science.",
                    "label": 1
                },
                {
                    "sent": "Some of our future work might include applying our approach on updated data set so Microsoft Academic Graph 2019, which has almost twice as much articles now.",
                    "label": 1
                },
                {
                    "sent": "We could also consider paper abstracts and or citation graph structures to add to our representation.",
                    "label": 1
                },
                {
                    "sent": "We could also validate our approach on other datasets and compare different machine machine learning algorithms and topic representations to see how our approach did compared to them.",
                    "label": 1
                },
                {
                    "sent": "Lastly, we could also do a more in depth analysis of the topics we predicted as promising to see why it was these topics specifically.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, we could also we have also considered creating a public publicly accessible online version of this system.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}