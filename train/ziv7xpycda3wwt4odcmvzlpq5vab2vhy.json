{
    "id": "ziv7xpycda3wwt4odcmvzlpq5vab2vhy",
    "title": "CoCo: Coding Cost for Parameter-free Outlier Detection",
    "info": {
        "author": [
            "Nikola M\u00fcller, Max Planck Institute for Biochemistry, Max Planck Institute"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "July 2009",
        "category": [
            "Top->Computer Science->Data Mining->Anomaly & Streams"
        ]
    },
    "url": "http://videolectures.net/kdd09_muller_cococcpfod/",
    "segmentation": [
        [
            "Where is trying to print one?",
            "Tell the pro."
        ],
        [
            "Time to print 1000 times A then 1000 * B and also transfer C as a single."
        ],
        [
            "And by that we are able to reduce the communication costs to about 2%.",
            "But just exploring the regularity's in the data."
        ],
        [
            "So combining everything like the MDL with our EPD, we have just imagined that small data set."
        ],
        [
            "Your project one dimension fit agoston like with the PD and then how do we compress?",
            "How do we say which point is good compressed or not?"
        ],
        [
            "It's pretty simple.",
            "Decoding causes just like the negative logarithm of the IPD.",
            "So if we want to code a point like from inside of the data so it's a regular data point, we can highly compress it based on the knowledge of the PDF and we end up with some coding costs."
        ],
        [
            "Close to 0.",
            "If we want to transfer a point to the receiver just from the side of the distribution.",
            "We can hardly compress it because there not any regular data around, so it has a very high compression, which is not not not."
        ],
        [
            "Here to 0.",
            "So what Coco is doing?",
            "We scan in a growing neighborhood for each data point, we approximate the data with ICA, PD and.",
            "Then in the very end, after we did for every arbitrary neighborhood, we determine the minimal coding costs in order to be independent of the neighborhood size that regular data are."
        ],
        [
            "Faster can be.",
            "So the performance on that synthetic data set."
        ],
        [
            "Told you in the beginning.",
            "We have the I've just told you how to determine the coding costs with cocoa and we plotted the coding costs in that direction and you can I see that all the outliers exhibit very high values, even up to 22, whereby all the outliers from the other cluster points.",
            "They like flat to see."
        ],
        [
            "But then when we do have the coding costs available for each point, so how do we detect the outliers?",
            "So what we thought is a good idea to just group them by X means we know the regular data should be close."
        ],
        [
            "Zero, we just simply apply parameter free and we get directly all the outliers in the data set."
        ],
        [
            "Compared to like one of the other outlier detection algorithms LOL."
        ],
        [
            "We are we can identify all the outliers by applying love.",
            "We what I have to say.",
            "We have to set a parameter.",
            "We do know it's 26 outliers so we have to detect 26 top outlier.",
            "But even then if we know how many we have so that's a problem where parameter we do falsely identified to cluster points and and also.",
            "Leave two outliers undetected."
        ],
        [
            "To summarize my talk yet lie detection with Koko we can.",
            "I first showed you clearly that we identify all outlier, at least in that synthetic data set.",
            "For more, you have to go into the paper.",
            "We second adapted to various data density in various data shops shapes and we are entirely problem free."
        ],
        [
            "Because I want to thank my professor Custom Boom cartoon Claudia from the team and Roland from for the funding from the MPF Bank chemistry.",
            "Thank you and if you have any questions or would like to answer."
        ],
        [
            "OK, so we we started with that.",
            "I think with an exponential increasing so we start I think with 20 because we need to get a good estimate of the PD and then we just go to 2025 and then 40 and then the bigger the neighborhood grows we go in bigger steps like 100 and 200 and 1000.",
            "Terminate.",
            "We just go go through the entire data set.",
            "It might be that it's just one cluster in the data set and we don't want to miss that.",
            "Yes.",
            "That's the thing we were just currently working on.",
            "I think it was the IPD estimate actually.",
            "Because you have to use the bisecting search to take all the parameters.",
            "No."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where is trying to print one?",
                    "label": 0
                },
                {
                    "sent": "Tell the pro.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time to print 1000 times A then 1000 * B and also transfer C as a single.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And by that we are able to reduce the communication costs to about 2%.",
                    "label": 0
                },
                {
                    "sent": "But just exploring the regularity's in the data.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So combining everything like the MDL with our EPD, we have just imagined that small data set.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your project one dimension fit agoston like with the PD and then how do we compress?",
                    "label": 0
                },
                {
                    "sent": "How do we say which point is good compressed or not?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's pretty simple.",
                    "label": 0
                },
                {
                    "sent": "Decoding causes just like the negative logarithm of the IPD.",
                    "label": 0
                },
                {
                    "sent": "So if we want to code a point like from inside of the data so it's a regular data point, we can highly compress it based on the knowledge of the PDF and we end up with some coding costs.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Close to 0.",
                    "label": 0
                },
                {
                    "sent": "If we want to transfer a point to the receiver just from the side of the distribution.",
                    "label": 0
                },
                {
                    "sent": "We can hardly compress it because there not any regular data around, so it has a very high compression, which is not not not.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here to 0.",
                    "label": 0
                },
                {
                    "sent": "So what Coco is doing?",
                    "label": 0
                },
                {
                    "sent": "We scan in a growing neighborhood for each data point, we approximate the data with ICA, PD and.",
                    "label": 1
                },
                {
                    "sent": "Then in the very end, after we did for every arbitrary neighborhood, we determine the minimal coding costs in order to be independent of the neighborhood size that regular data are.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Faster can be.",
                    "label": 0
                },
                {
                    "sent": "So the performance on that synthetic data set.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Told you in the beginning.",
                    "label": 0
                },
                {
                    "sent": "We have the I've just told you how to determine the coding costs with cocoa and we plotted the coding costs in that direction and you can I see that all the outliers exhibit very high values, even up to 22, whereby all the outliers from the other cluster points.",
                    "label": 0
                },
                {
                    "sent": "They like flat to see.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But then when we do have the coding costs available for each point, so how do we detect the outliers?",
                    "label": 0
                },
                {
                    "sent": "So what we thought is a good idea to just group them by X means we know the regular data should be close.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zero, we just simply apply parameter free and we get directly all the outliers in the data set.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compared to like one of the other outlier detection algorithms LOL.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are we can identify all the outliers by applying love.",
                    "label": 0
                },
                {
                    "sent": "We what I have to say.",
                    "label": 0
                },
                {
                    "sent": "We have to set a parameter.",
                    "label": 0
                },
                {
                    "sent": "We do know it's 26 outliers so we have to detect 26 top outlier.",
                    "label": 0
                },
                {
                    "sent": "But even then if we know how many we have so that's a problem where parameter we do falsely identified to cluster points and and also.",
                    "label": 0
                },
                {
                    "sent": "Leave two outliers undetected.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To summarize my talk yet lie detection with Koko we can.",
                    "label": 1
                },
                {
                    "sent": "I first showed you clearly that we identify all outlier, at least in that synthetic data set.",
                    "label": 0
                },
                {
                    "sent": "For more, you have to go into the paper.",
                    "label": 0
                },
                {
                    "sent": "We second adapted to various data density in various data shops shapes and we are entirely problem free.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because I want to thank my professor Custom Boom cartoon Claudia from the team and Roland from for the funding from the MPF Bank chemistry.",
                    "label": 0
                },
                {
                    "sent": "Thank you and if you have any questions or would like to answer.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we we started with that.",
                    "label": 0
                },
                {
                    "sent": "I think with an exponential increasing so we start I think with 20 because we need to get a good estimate of the PD and then we just go to 2025 and then 40 and then the bigger the neighborhood grows we go in bigger steps like 100 and 200 and 1000.",
                    "label": 0
                },
                {
                    "sent": "Terminate.",
                    "label": 0
                },
                {
                    "sent": "We just go go through the entire data set.",
                    "label": 0
                },
                {
                    "sent": "It might be that it's just one cluster in the data set and we don't want to miss that.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "That's the thing we were just currently working on.",
                    "label": 0
                },
                {
                    "sent": "I think it was the IPD estimate actually.",
                    "label": 0
                },
                {
                    "sent": "Because you have to use the bisecting search to take all the parameters.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        }
    }
}