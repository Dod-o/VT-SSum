{
    "id": "ypi6ptpcgpxq5yrhliacdizqq6fg5stf",
    "title": "Scalable Hands Free Transfer Learning for Online Advertising",
    "info": {
        "author": [
            "Brian d'Alessandro, Dstillery"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_dalessandro_transfer_learning/",
    "segmentation": [
        [
            "So this is I'm presenting this work, but this is really work that one of the credit goes to my colleagues who are all listed here at Distillery.",
            "We have A at least in my opinion, a great data science team."
        ],
        [
            "So what is the goal?",
            "Motivation?",
            "This is kind of the core problem that still re faces everyday and as a primary driver of all of our research we want to predict whether a person will convert after seeing an ad.",
            "Very standard optimization goal for online advertising and I want to state that when we say conversion, it's this concept of a post view conversion.",
            "So you see an ad an you don't necessarily click on it, but in a period of let's say seven days 730 days will that person converts?",
            "That's the context of what we're dealing with."
        ],
        [
            "So how do you do this right?",
            "The standard approach for this, as well as any sort of modeling is you created.",
            "You selected distribution.",
            "In this case it would be showing ads randomly and there's a very important reason why you have to show the ads randomly.",
            "You wait some time, you observe a conversion, build yourself a data metrics train, a model very standard kind of data science 101 approach."
        ],
        [
            "So what's the problem?",
            "Why do we need to have a paper at KDD to talk about that problem?",
            "Well, there's a lot of kind of statistical as well as operational issues that we face, and this is actually one of the most common Canonical problems in online advertising.",
            "For one thing, our feature matrix, our feature space, is very large K. So we're dealing with a huge set of sparse features an in our case feature is generally defined as it's a binary data point on whether or not.",
            "User has visited a specific URL.",
            "Another problem is conversion events are extremely sparse.",
            "It is fairly common to get conversion rates in the order of magnitude of one and 10,000 or oftentimes even less, and there's a lot of cases where you don't even observe conversions.",
            "People don't buy orange juice or cars online, so that's but you still have to.",
            "You still want to take those client budgets.",
            "Another thing is because we're serving ads randomly.",
            "Getting a sufficiently large data set so that you could do any sort of reasonable modeling is prohibitively expensive, so these are the."
        ],
        [
            "Challenges we faced it and there's a few more additional challenges.",
            "One is the cold start at the beginning of the campaign.",
            "You have no data.",
            "We need to support many models in production, and I'm talking building thousands of models a week with minimal human intervention.",
            "And this is another constraint that makes our problem different than what you see in a lot of the Katie adds papers, despite the fact that we have hundreds of clients running at the same time, and many of them are even in the same industry.",
            "In fact, their competitors because of their competitors.",
            "Often we cannot pull data across advertisers, so I can't.",
            "Learn from a Nike campaign and apply that to a Reebok campaign.",
            "And that's just a business constraint that we've agreed upon with our clients."
        ],
        [
            "So how do we approach this?",
            "This is what you might think of as the latest algorithm in a suite of algorithms that we generally AB test when you develop the system called we call it Beijing transfer learning with adaptive stochastic gradient descent.",
            "So it's kind of two components that we've two and one of them has two subcomponents, but several different solutions that we've put together to build a comprehensive and what we called the hands free learning system.",
            "So the Bayesian transfer learning it's a.",
            "Just a modification of standard L1 or L2 regularization, where instead of using a zero prior, we're actually learning and informative prior in shrinking our model towards that, the adaptive SGD.",
            "This is work that we've integrated two different works that are not our own, but we have made small modifications to get the state of the art in adaptive learning rates as well as adaptive regularization.",
            "So I'll get into those individually so."
        ],
        [
            "The Bayesian transfer learning."
        ],
        [
            "Again, so in order to have transfer learning, you need to have different datasets.",
            "So the in the language of transfer learning the what you might call the problem at hand, that you're trying to solve.",
            "They'll call that the target data or the target task.",
            "So again our target data is the ad serving data.",
            "And like I mentioned before, it's generally prohibitively expensive to get enough data to actually build a model that has any reasonable variance properties."
        ],
        [
            "So at the same time, we're persistently collecting this auxiliary data, which is which for us.",
            "One of the things that makes it useful is that it generally has the same feature space, so it's data on browsers or cookies that includes a lot of their web browsing interactions.",
            "From that data we can essentially.",
            "Artificially create labels that are similar to the active conversion but not the exact type of process that we're really trying to model and that data.",
            "Although we pay for it, we get it in mass abundance and we can use it almost as freely as we like.",
            "So."
        ],
        [
            "There just a high level overview about transfer learning is if you don't know it is.",
            "So we've got two functions.",
            "I've got a function, learn F sub SX, an FT sybex.",
            "Those are two different models that you learn off of the two different datasets.",
            "Your source data which is you might call your auxiliary data and your target data.",
            "That's the target.",
            "The data again, that you're trying to ultimately solve for, so we assume that the two relations are similar or related and that's why you can justify.",
            "This knowledge transfer, as they say, if they were completely different, if those two models were, you know if it's a linear vector and they're completely orthogonal.",
            "This methodology wouldn't work.",
            "Another thing that you kind of need for to justify transfer learning as well is that the model that you're really trying to learn is going to have a high expectation of variance, and that's because again we have very sparse data, particularly outcome and very few are too few data points.",
            "So what we end up doing is we learn a model on the source data and use that as a regularization prior for the target data.",
            "So little schematic here."
        ],
        [
            "So here is this two stage model kind of more on the formulas.",
            "So the top step is just learn a model on the auxiliary data and for this for our system using logistic regression again with the feature space is bit 01 bites on whether someone has visited a particular website and then essentially the parameter on that website correlate's visiting that website with the conversion action and then you could see on the bottom step that we've modified.",
            "In this case, L2 regularization with this prior and usually you just kind of shrink the magnitude of the beta weight, but in this case we're shrinking it towards the model that we learned in the first step.",
            "So you can think of this as you know in the first step we kind of center where the best guess is of the model, and then the.",
            "When we're learning on the actual target data set, we start at that point.",
            "We usually initialize at mu and then if there's sufficient data to prove that the target model sufficiently different, the right choice of regularization weight you could actually data adaptively move away from that center point.",
            "So it's think of it as kind of anchoring towards a different distribution.",
            "That's very related to your target."
        ],
        [
            "Ask so.",
            "As a production system, we have to build thousands of models simultaneously, so anything we do will often sacrifice maybe a little bit of accuracy for scale and so kind of.",
            "At this point the norm for getting scale with linear modeling, particularly linear convex modeling, is to use to catch a gradient descent."
        ],
        [
            "So we've.",
            "Implemented we call nearly hyperparameter free and I will explain why I say nearly in a second, it's essentially integration of these two different methods and here are the papers here and highly recommend taking a look at them and I have to give a shout out to Azure.",
            "We call Henry Chen in our who's on our team.",
            "He spent last summer building the software that we can so we could actually run the models on this, which includes these methodologies."
        ],
        [
            "So here is just an overview of them.",
            "I'm not going to get into too much detail because it's not necessarily art and it's in the references, so the adaptive learning rate it's an optimization step where at each learning step you solve an optimization problem which is.",
            "It is the learning rate that optimizes some loss function on the next step, and similarly the adaptive regularization instead of doing cross cross validation to get your appropriate regularization.",
            "Wait, you're doing an SGD essentially on the regularization weight and saying, OK, what is the regular regularization weight that optimizes the loss function on my test datasets?",
            "So when I said it was nearly hyperparameter free, what I mean is so.",
            "There is a in the first in the learning rate there is a initialization window that you have to run a certain number of iterations before the main algorithm kicks in and in the adaptive regularization there is a learning rate on that SGD step so.",
            "We found empirically that the those two parameters generally have low variance across different tasks within a class of problem.",
            "So if you did a little exploration at the beginning, you could essentially freeze those and set them.",
            "And As for every different client we have, we can almost use the same one.",
            "So there's again.",
            "There's more details of that in."
        ],
        [
            "Paper, so to get some."
        ],
        [
            "Miracle results the first experiment I'm going to show.",
            "Is set up like this.",
            "We have a source distribution in a target distribution sample.",
            "Two weeks of training data and then the test data set is on the target distribution and again the target distribution is the result of the ad serving logs.",
            "Serve ads, observed conversions, etc.",
            "We conducted a set of experiments to test the different variants or aspects over methodology transfer versus no transfer using kind of the traditional grid search methods where we used at a grad for SGD and justice kind of grid search over.",
            "A set of regularization values.",
            "We also did Grid search versus adaptive learning, just to see without transfer within the context of transfer learning, do you lose anything by these kind of new experimental approaches?",
            "So."
        ],
        [
            "Here's the grid search versus adaptive learning, so again, this is the grid searches traditional search over a space of learning rate parameters as well as regularization parameters, and you can see everything here.",
            "Hugs the identity line.",
            "The point here wasn't to prove that adaptive learning is better, it's more to prove that it's not worse and our results statistically across 100 different campaigns showed that it wasn't worse.",
            "The benefit for us is that instead of doing a grid search over potential hundred values, you could intelligently maybe refine that down to 10 to 20 instead of doing again a grid search over all those values.",
            "You just make one pass through the data, so that's clearly a huge efficiency gain in terms of learning time."
        ],
        [
            "So then putting it all together.",
            "The impact of doing transfer learning over no transfer learning generally increases the lift on 93% of the campaigns increased the lift by nearly 23% on average across all the campaigns, and because of the adaptive learning approach, is an order of magnitude faster.",
            "So for us that's pretty significant.",
            "Mainly if you increase lift by 23%, that's literally 23% more conversions are getting for spending the same amount of money, so that's great."
        ],
        [
            "Here is another experiment we did which more mirrors what we're actually doing in our system.",
            "So we're not just building models in a static framework, which is what the previous experiment showed.",
            "This is actually more closely related to how we build models in production, which is we every day we take in new training data and we start with the old model and we just update the model in a sort of mini batch incremental fashion.",
            "So this shows what the results look like over a 60 day training period, and you can see the.",
            "In the black dots, which is the difference in AUC average difference in AC when you first start off you get a much better.",
            "You get better performance by using transfer learning.",
            "But overtime that performance decays generally after 30 days.",
            "You're almost as good, so that's you know this is kind of a good representation of how the cold start.",
            "How you can solve a cold start problem, but overtime you've actually collected sufficient data that just doing the regular learning can kick in.",
            "The good thing about this approach is its data adaptive you can.",
            "Learn when and when not to use the transfer."
        ],
        [
            "Learning and my last slide and I'm out of time is it works live.",
            "This is kind of a complicated measurement that is better explained in the paper, but it this is from our actual production system.",
            "Performance, it's not necessarily clean experiment, so there are other parameters that could affect performance in there, but again, it generally works in thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is I'm presenting this work, but this is really work that one of the credit goes to my colleagues who are all listed here at Distillery.",
                    "label": 0
                },
                {
                    "sent": "We have A at least in my opinion, a great data science team.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the goal?",
                    "label": 0
                },
                {
                    "sent": "Motivation?",
                    "label": 0
                },
                {
                    "sent": "This is kind of the core problem that still re faces everyday and as a primary driver of all of our research we want to predict whether a person will convert after seeing an ad.",
                    "label": 1
                },
                {
                    "sent": "Very standard optimization goal for online advertising and I want to state that when we say conversion, it's this concept of a post view conversion.",
                    "label": 0
                },
                {
                    "sent": "So you see an ad an you don't necessarily click on it, but in a period of let's say seven days 730 days will that person converts?",
                    "label": 0
                },
                {
                    "sent": "That's the context of what we're dealing with.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do you do this right?",
                    "label": 1
                },
                {
                    "sent": "The standard approach for this, as well as any sort of modeling is you created.",
                    "label": 0
                },
                {
                    "sent": "You selected distribution.",
                    "label": 0
                },
                {
                    "sent": "In this case it would be showing ads randomly and there's a very important reason why you have to show the ads randomly.",
                    "label": 1
                },
                {
                    "sent": "You wait some time, you observe a conversion, build yourself a data metrics train, a model very standard kind of data science 101 approach.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's the problem?",
                    "label": 0
                },
                {
                    "sent": "Why do we need to have a paper at KDD to talk about that problem?",
                    "label": 0
                },
                {
                    "sent": "Well, there's a lot of kind of statistical as well as operational issues that we face, and this is actually one of the most common Canonical problems in online advertising.",
                    "label": 0
                },
                {
                    "sent": "For one thing, our feature matrix, our feature space, is very large K. So we're dealing with a huge set of sparse features an in our case feature is generally defined as it's a binary data point on whether or not.",
                    "label": 0
                },
                {
                    "sent": "User has visited a specific URL.",
                    "label": 0
                },
                {
                    "sent": "Another problem is conversion events are extremely sparse.",
                    "label": 0
                },
                {
                    "sent": "It is fairly common to get conversion rates in the order of magnitude of one and 10,000 or oftentimes even less, and there's a lot of cases where you don't even observe conversions.",
                    "label": 0
                },
                {
                    "sent": "People don't buy orange juice or cars online, so that's but you still have to.",
                    "label": 0
                },
                {
                    "sent": "You still want to take those client budgets.",
                    "label": 0
                },
                {
                    "sent": "Another thing is because we're serving ads randomly.",
                    "label": 0
                },
                {
                    "sent": "Getting a sufficiently large data set so that you could do any sort of reasonable modeling is prohibitively expensive, so these are the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Challenges we faced it and there's a few more additional challenges.",
                    "label": 0
                },
                {
                    "sent": "One is the cold start at the beginning of the campaign.",
                    "label": 0
                },
                {
                    "sent": "You have no data.",
                    "label": 0
                },
                {
                    "sent": "We need to support many models in production, and I'm talking building thousands of models a week with minimal human intervention.",
                    "label": 1
                },
                {
                    "sent": "And this is another constraint that makes our problem different than what you see in a lot of the Katie adds papers, despite the fact that we have hundreds of clients running at the same time, and many of them are even in the same industry.",
                    "label": 0
                },
                {
                    "sent": "In fact, their competitors because of their competitors.",
                    "label": 1
                },
                {
                    "sent": "Often we cannot pull data across advertisers, so I can't.",
                    "label": 0
                },
                {
                    "sent": "Learn from a Nike campaign and apply that to a Reebok campaign.",
                    "label": 0
                },
                {
                    "sent": "And that's just a business constraint that we've agreed upon with our clients.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we approach this?",
                    "label": 0
                },
                {
                    "sent": "This is what you might think of as the latest algorithm in a suite of algorithms that we generally AB test when you develop the system called we call it Beijing transfer learning with adaptive stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So it's kind of two components that we've two and one of them has two subcomponents, but several different solutions that we've put together to build a comprehensive and what we called the hands free learning system.",
                    "label": 1
                },
                {
                    "sent": "So the Bayesian transfer learning it's a.",
                    "label": 0
                },
                {
                    "sent": "Just a modification of standard L1 or L2 regularization, where instead of using a zero prior, we're actually learning and informative prior in shrinking our model towards that, the adaptive SGD.",
                    "label": 0
                },
                {
                    "sent": "This is work that we've integrated two different works that are not our own, but we have made small modifications to get the state of the art in adaptive learning rates as well as adaptive regularization.",
                    "label": 0
                },
                {
                    "sent": "So I'll get into those individually so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Bayesian transfer learning.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, so in order to have transfer learning, you need to have different datasets.",
                    "label": 0
                },
                {
                    "sent": "So the in the language of transfer learning the what you might call the problem at hand, that you're trying to solve.",
                    "label": 0
                },
                {
                    "sent": "They'll call that the target data or the target task.",
                    "label": 0
                },
                {
                    "sent": "So again our target data is the ad serving data.",
                    "label": 1
                },
                {
                    "sent": "And like I mentioned before, it's generally prohibitively expensive to get enough data to actually build a model that has any reasonable variance properties.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at the same time, we're persistently collecting this auxiliary data, which is which for us.",
                    "label": 0
                },
                {
                    "sent": "One of the things that makes it useful is that it generally has the same feature space, so it's data on browsers or cookies that includes a lot of their web browsing interactions.",
                    "label": 0
                },
                {
                    "sent": "From that data we can essentially.",
                    "label": 0
                },
                {
                    "sent": "Artificially create labels that are similar to the active conversion but not the exact type of process that we're really trying to model and that data.",
                    "label": 0
                },
                {
                    "sent": "Although we pay for it, we get it in mass abundance and we can use it almost as freely as we like.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There just a high level overview about transfer learning is if you don't know it is.",
                    "label": 0
                },
                {
                    "sent": "So we've got two functions.",
                    "label": 0
                },
                {
                    "sent": "I've got a function, learn F sub SX, an FT sybex.",
                    "label": 0
                },
                {
                    "sent": "Those are two different models that you learn off of the two different datasets.",
                    "label": 0
                },
                {
                    "sent": "Your source data which is you might call your auxiliary data and your target data.",
                    "label": 0
                },
                {
                    "sent": "That's the target.",
                    "label": 0
                },
                {
                    "sent": "The data again, that you're trying to ultimately solve for, so we assume that the two relations are similar or related and that's why you can justify.",
                    "label": 0
                },
                {
                    "sent": "This knowledge transfer, as they say, if they were completely different, if those two models were, you know if it's a linear vector and they're completely orthogonal.",
                    "label": 0
                },
                {
                    "sent": "This methodology wouldn't work.",
                    "label": 0
                },
                {
                    "sent": "Another thing that you kind of need for to justify transfer learning as well is that the model that you're really trying to learn is going to have a high expectation of variance, and that's because again we have very sparse data, particularly outcome and very few are too few data points.",
                    "label": 0
                },
                {
                    "sent": "So what we end up doing is we learn a model on the source data and use that as a regularization prior for the target data.",
                    "label": 1
                },
                {
                    "sent": "So little schematic here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is this two stage model kind of more on the formulas.",
                    "label": 1
                },
                {
                    "sent": "So the top step is just learn a model on the auxiliary data and for this for our system using logistic regression again with the feature space is bit 01 bites on whether someone has visited a particular website and then essentially the parameter on that website correlate's visiting that website with the conversion action and then you could see on the bottom step that we've modified.",
                    "label": 0
                },
                {
                    "sent": "In this case, L2 regularization with this prior and usually you just kind of shrink the magnitude of the beta weight, but in this case we're shrinking it towards the model that we learned in the first step.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this as you know in the first step we kind of center where the best guess is of the model, and then the.",
                    "label": 0
                },
                {
                    "sent": "When we're learning on the actual target data set, we start at that point.",
                    "label": 1
                },
                {
                    "sent": "We usually initialize at mu and then if there's sufficient data to prove that the target model sufficiently different, the right choice of regularization weight you could actually data adaptively move away from that center point.",
                    "label": 0
                },
                {
                    "sent": "So it's think of it as kind of anchoring towards a different distribution.",
                    "label": 0
                },
                {
                    "sent": "That's very related to your target.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ask so.",
                    "label": 0
                },
                {
                    "sent": "As a production system, we have to build thousands of models simultaneously, so anything we do will often sacrifice maybe a little bit of accuracy for scale and so kind of.",
                    "label": 0
                },
                {
                    "sent": "At this point the norm for getting scale with linear modeling, particularly linear convex modeling, is to use to catch a gradient descent.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we've.",
                    "label": 0
                },
                {
                    "sent": "Implemented we call nearly hyperparameter free and I will explain why I say nearly in a second, it's essentially integration of these two different methods and here are the papers here and highly recommend taking a look at them and I have to give a shout out to Azure.",
                    "label": 0
                },
                {
                    "sent": "We call Henry Chen in our who's on our team.",
                    "label": 0
                },
                {
                    "sent": "He spent last summer building the software that we can so we could actually run the models on this, which includes these methodologies.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is just an overview of them.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to get into too much detail because it's not necessarily art and it's in the references, so the adaptive learning rate it's an optimization step where at each learning step you solve an optimization problem which is.",
                    "label": 1
                },
                {
                    "sent": "It is the learning rate that optimizes some loss function on the next step, and similarly the adaptive regularization instead of doing cross cross validation to get your appropriate regularization.",
                    "label": 1
                },
                {
                    "sent": "Wait, you're doing an SGD essentially on the regularization weight and saying, OK, what is the regular regularization weight that optimizes the loss function on my test datasets?",
                    "label": 1
                },
                {
                    "sent": "So when I said it was nearly hyperparameter free, what I mean is so.",
                    "label": 1
                },
                {
                    "sent": "There is a in the first in the learning rate there is a initialization window that you have to run a certain number of iterations before the main algorithm kicks in and in the adaptive regularization there is a learning rate on that SGD step so.",
                    "label": 1
                },
                {
                    "sent": "We found empirically that the those two parameters generally have low variance across different tasks within a class of problem.",
                    "label": 0
                },
                {
                    "sent": "So if you did a little exploration at the beginning, you could essentially freeze those and set them.",
                    "label": 0
                },
                {
                    "sent": "And As for every different client we have, we can almost use the same one.",
                    "label": 0
                },
                {
                    "sent": "So there's again.",
                    "label": 0
                },
                {
                    "sent": "There's more details of that in.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paper, so to get some.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Miracle results the first experiment I'm going to show.",
                    "label": 0
                },
                {
                    "sent": "Is set up like this.",
                    "label": 1
                },
                {
                    "sent": "We have a source distribution in a target distribution sample.",
                    "label": 0
                },
                {
                    "sent": "Two weeks of training data and then the test data set is on the target distribution and again the target distribution is the result of the ad serving logs.",
                    "label": 0
                },
                {
                    "sent": "Serve ads, observed conversions, etc.",
                    "label": 0
                },
                {
                    "sent": "We conducted a set of experiments to test the different variants or aspects over methodology transfer versus no transfer using kind of the traditional grid search methods where we used at a grad for SGD and justice kind of grid search over.",
                    "label": 1
                },
                {
                    "sent": "A set of regularization values.",
                    "label": 0
                },
                {
                    "sent": "We also did Grid search versus adaptive learning, just to see without transfer within the context of transfer learning, do you lose anything by these kind of new experimental approaches?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the grid search versus adaptive learning, so again, this is the grid searches traditional search over a space of learning rate parameters as well as regularization parameters, and you can see everything here.",
                    "label": 1
                },
                {
                    "sent": "Hugs the identity line.",
                    "label": 0
                },
                {
                    "sent": "The point here wasn't to prove that adaptive learning is better, it's more to prove that it's not worse and our results statistically across 100 different campaigns showed that it wasn't worse.",
                    "label": 0
                },
                {
                    "sent": "The benefit for us is that instead of doing a grid search over potential hundred values, you could intelligently maybe refine that down to 10 to 20 instead of doing again a grid search over all those values.",
                    "label": 0
                },
                {
                    "sent": "You just make one pass through the data, so that's clearly a huge efficiency gain in terms of learning time.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then putting it all together.",
                    "label": 0
                },
                {
                    "sent": "The impact of doing transfer learning over no transfer learning generally increases the lift on 93% of the campaigns increased the lift by nearly 23% on average across all the campaigns, and because of the adaptive learning approach, is an order of magnitude faster.",
                    "label": 1
                },
                {
                    "sent": "So for us that's pretty significant.",
                    "label": 0
                },
                {
                    "sent": "Mainly if you increase lift by 23%, that's literally 23% more conversions are getting for spending the same amount of money, so that's great.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is another experiment we did which more mirrors what we're actually doing in our system.",
                    "label": 0
                },
                {
                    "sent": "So we're not just building models in a static framework, which is what the previous experiment showed.",
                    "label": 0
                },
                {
                    "sent": "This is actually more closely related to how we build models in production, which is we every day we take in new training data and we start with the old model and we just update the model in a sort of mini batch incremental fashion.",
                    "label": 0
                },
                {
                    "sent": "So this shows what the results look like over a 60 day training period, and you can see the.",
                    "label": 0
                },
                {
                    "sent": "In the black dots, which is the difference in AUC average difference in AC when you first start off you get a much better.",
                    "label": 0
                },
                {
                    "sent": "You get better performance by using transfer learning.",
                    "label": 0
                },
                {
                    "sent": "But overtime that performance decays generally after 30 days.",
                    "label": 0
                },
                {
                    "sent": "You're almost as good, so that's you know this is kind of a good representation of how the cold start.",
                    "label": 0
                },
                {
                    "sent": "How you can solve a cold start problem, but overtime you've actually collected sufficient data that just doing the regular learning can kick in.",
                    "label": 0
                },
                {
                    "sent": "The good thing about this approach is its data adaptive you can.",
                    "label": 0
                },
                {
                    "sent": "Learn when and when not to use the transfer.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning and my last slide and I'm out of time is it works live.",
                    "label": 1
                },
                {
                    "sent": "This is kind of a complicated measurement that is better explained in the paper, but it this is from our actual production system.",
                    "label": 0
                },
                {
                    "sent": "Performance, it's not necessarily clean experiment, so there are other parameters that could affect performance in there, but again, it generally works in thank you.",
                    "label": 0
                }
            ]
        }
    }
}