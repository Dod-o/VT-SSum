{
    "id": "flg7mfek7vgjudza6lpowzpu6ljreazy",
    "title": "k-NN Regression Adapts to Local Intrinsic Dimension",
    "info": {
        "author": [
            "Samory Kpotufe, Department of Operations Research and Financial Engineering, Princeton University"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Regression"
        ]
    },
    "url": "http://videolectures.net/nips2011_kpotufe_intrinsic/",
    "segmentation": [
        [
            "So high, so as you can see from the title, I'll be talking about nearest neighbor regression, an dimensional notions of that."
        ],
        [
            "Mission.",
            "And so let's start with nearest neighbour regression.",
            "So the setup is quite simple to the usual setup.",
            "We have NID point ID, pairs of points, exciting.",
            "Why I where we assume that Y is a function of X plus some noise.",
            "Here we're going to assume that X belongs to some matrix space with."
        ],
        [
            "Magic Roll An Why is a real variable, and we'll also assume that F is Lipschitz with some unknown Lipschitz parameter Lambda, and all.",
            "We mean here by Lipschitz is that if X&X prime, two points that are close, then F of X&F of X primer also close.",
            "So nearest neighbor regression is very simple.",
            "When we get a point query point X, we just simply take the K nearest neighbors of X and average the Y values of these neighbors and that's our estimate that."
        ],
        [
            "Regression estimates we might use just a normal average, or we might also use a weighted average, using maybe a kernel.",
            "And because it's so basic, it's so simple.",
            "It's very common in practice, and hence it's important."
        ],
        [
            "Unfortunately, Canon regression is subject to occur to the curse of dimension pretty bad, and what that means is that.",
            "There always exists a distributor existed there is this distributions on XY such that the excess risk essentially FN of FNC converges to F at a rate of the form and minus 2 / 2 + D. Where big deal here is the dimension of the input space."
        ],
        [
            "And this turns out to be true for most non parametric regressors but"
        ],
        [
            "But there is hope in the fact that high dimensional data often has low interesting complexity and some examples are data.",
            "High dimensional bad like, close to a low dimensional hyperplane."
        ],
        [
            "Another example is that lies close to a manifold."
        ],
        [
            "An another example that we often use in machine learning is data that is sparse, for instance images."
        ],
        [
            "So the common approach when we believe that that are that is high dimensional has low interesting complexity.",
            "The common approach is to use dimension reduction, an alot of different techniques have been developed overtime and with some."
        ],
        [
            "Success the main result is the following.",
            "What I would like to talk to you about is that it turns out that nearest neighbor regression performs well without dimension reduction.",
            "And what we mean by that is that FNK converges at a rate that is adaptive to the unknown interesting dimension, and this without doing any dimension reduction."
        ],
        [
            "And So what that means in practice is that you might rather spend your time tuning K then trying to tune the parameters of your favorite dimension reduction procedure."
        ],
        [
            "Adaptivity.",
            "Actually, is a recent direction and has been shown for other other learners.",
            "So for instance kernel.",
            "Regressor is there.",
            "Local polynomial regressors were shown by big only in 2006.",
            "To be adaptive to manifold dimension.",
            "And there was a follow up paper by Lafferty and Wasserman concerning semi supervised learning with these regressors.",
            "Scott in the work showed in 2006 that Dyadic tree classification is also adaptive to a certain notion of interesting dimension.",
            "Cool Carney imposed and then in 95 showed that one nearest neighbor regression is adaptive to interesting dimension in the sense that it converges at rates that depend just only transit dimension.",
            "But here one and then regression is not consistent because you just use a single nearest neighbor for any sample size.",
            "Also RP 3 random projection tree and I'd eat regression is adaptive.",
            "Our adaptive to interesting dimension or shown by me and my advisors on my old advisor Sandra Dot scooped up.",
            "And then there is this form of regressors also called that I call tree kernel hybrid regressors that combine ideas of tree based regression and kernel regression that also adapted to interesting dimension."
        ],
        [
            "All the above results, though, however, are under global notions of interesting dimension and hear what we're going to care about.",
            "Local notions of interesting dimensional explain why locality, isn't it?"
        ],
        [
            "Is important.",
            "So the outline of the talk is as follows.",
            "Will first try to give some intuitive notion of interesting dimension that captures all these natural.",
            "Jewel's situation is that we just talked about and then I'll talk about the result on adaptivity for a PSK.",
            "For any gain the range log into N and why we picked these branches is because these are where the best case.",
            "The various values of case can be.",
            "This is the range where the best values of K can be found, and then after that I'll talk about how to choose a good care in this range without knowing the interesting dimension of the day of the data at X at the point X loca."
        ],
        [
            "Lee so.",
            "Intrinsic dimension, so the notion that the notion of interest dimension that will use will be based on the following intuition, and so I'll describe the intuition in a very simple way.",
            "So suppose we consider these two balls.",
            "We have a ball centered at X of radius epsilon R, and a bigger ball centered at X of radius R, and then we'll see we'll look at the way the volumes of these balls differ."
        ],
        [
            "And these are D dimensional balls, OK?",
            "So the volume of the bigger ball is about C times are to the D and the volume of the smaller bowl will also be able see times epsilon to the D artadi.",
            "Therefore the two volumes differ by a factor of epsilon minus T. And notice that this factor is indicative of the dimension of these objects in the natural dimension of this object.",
            "So now let's look at a probability measure.",
            "Suppose that we have this measure mu.",
            "That is a uniform."
        ],
        [
            "Then the mass that is measure assigned to these balls will behave, as will behave just as the values behave, and so will have again that the mass of the bigger ball and the mass of the smaller ball differ by factor of about epsilon manisty.",
            "Anne."
        ],
        [
            "This leads to our notion the notion will be using of intrinsic dimension.",
            "We say will say that Mula measure music CD homogeneous on BXR in neighborhood of X.",
            "If for all our prime less than R and all epsilon, the measure of all the mass of a ball BXR prime and a massive above BX epsilon R prime differ by a factor of about epsilon minus T, and it will turn out that.",
            "On the various various conditions on the.",
            "On the measure, this will hold a for instance the measure concentrated near a low dimensional space, say manifold or sparse or collection of manifolds such as a sparse."
        ],
        [
            "I said so.",
            "We don't have to remember this equation.",
            "The main thing to remember is that given the query X, the behavior of Milan meal here will, I'll always use it as the marginal measure on the input SpaceX so that the behavior of meal in a neighborhood B of X can capture the interesting dimension in this neighborhood.",
            "Be here, let's suppose that space we considering is this square and this line.",
            "So then X is here, and I'm assuming that B is this.",
            "Small neighborhood."
        ],
        [
            "As you can see, the location of X matters in defining what the intrinsic dimension is.",
            "If X happens to be in the square, the intrinsic dimension will be higher.",
            "If X is on the line, the interesting dimension will be lower."
        ],
        [
            "The size of neighborhood of the neighborhood also is going to matter if we use a bigger neighborhood B, then we capture more of the square and the interesting dimension on this neighborhood will be higher."
        ],
        [
            "For nearest neighbor regression.",
            "The relevant size of the neighborhood be is going to depend on K and on N. So when we talk about the rate of convergence of nearest neighbor regression will have to say exactly what this neighborhood is under.",
            "To which neighborhood does it adapt to?"
        ],
        [
            "So now.",
            "I'll talk about activity for a face K in this range.",
            "Maybe I'll be going, maybe I'm going too fast, but if we have time in the end we can discuss more and ask questions so."
        ],
        [
            "Adaptivity for a fixed game.",
            "So pick Karen fix getting this range, and let X again.",
            "Be in some region B and suppose here that K is equal to three right?",
            "Then they neighborhood be.",
            "There will be considering we will have to be a neighborhood that contains at least K points and so here I drew this neighborhood that contains the Bell K points or a bit more than K points and RCL vex.",
            "Here is just defined as defined to be the distance from X to its kit nearest neighbor.",
            "So the intuition is as far."
        ],
        [
            "The rate of convergence of FNC will depend on the variance of FNC and on the bias of FNC."
        ],
        [
            "The variance would always be about 1 / K. Because there is always only care about K points that contributes to the estimate.",
            "And the bias will be will depend on the distance to the case nearest neighbor, because the farther the nearest neighbors are probably, the further the Y values they output.",
            "Values will be to the unknown output.",
            "The unknown why value of X."
        ],
        [
            "It turns out that our key of X will be of this form K over N 1 / D. Where D is the intrinsic dimension in this neighborhood B.",
            "And so here the only thing that will depend on the intensity dimension will be.",
            "These are key of X and the variance itself remains."
        ],
        [
            "Dicks also ourselves will depend on the mass of be, so intuitively you can imagine that nearest then the neighbors of X will be closer in denser regions be so that's the intuition behind this dependence."
        ],
        [
            "So we have the following result for any fixed key.",
            "The result is just says that the results actually holds with high probability simultaneously for all X in our space and for all K in this range.",
            "And he says that consider any Bobby centered at X such that the bulk contains about K points, so the ball has massive volcano van.",
            "And suppose that mu is CD homogeneous on B.",
            "Then we have that FNC is most this far from F of X FM.",
            "Kovacs is at most is far from F of X an.",
            "How far depend will depend on K and will depend on D which day is the interesting dimension in this neighborhood."
        ],
        [
            "Now notice that we have the mass of the in the rates and.",
            "What that means is that the rate is best if X is in a dense region B which has low dimension D."
        ],
        [
            "So now we can talk about how to choose K, how to best, how to choose K at X.",
            "And here again, we're not trying to choose K globally for every X we're trying to choose K at this particular X so that we can adapt to the interesting dimension in a neighborhood of X."
        ],
        [
            "So first, before we can talk about this, we have to see what is the best possible rate we can get for if the interesting dimension is the what is the best possible rate we can get for any?"
        ],
        [
            "Racer, so we have the following result.",
            "Consider any metric measure.",
            "SpaceX, Mu X Romeo.",
            "Such that the interesting dimension as we define it earlier is the small D. Then any regressor for any regrets at the best rate that we can expect.",
            "Is of the form N -- 2 / 2 + D. And one thing that is interesting in this result here is that.",
            "It doesn't depend on the choice of metric space on the choice of measure, but if you take any metric space and the measure behaves this way will have this rate."
        ],
        [
            "So now we can talk about how to choose K so that we get this right so first.",
            "If we were to choose K globally.",
            "Yeah, what we might do is cross validation or we might try to do dimension estimation.",
            "We might try to estimate the dimension.",
            "The interesting dimension D and plug this dimension into the setting of K, But this sort of thing would requires a lot of points, and here we want to choose care locali at X, meaning you'll have to.",
            "We'll need to have a lot of points locali at X and which is probably unlikely.",
            "So what we will do instead is the following.",
            "We will look at will let K very will let Kate grow and we look at the variance of the estimator which is about 1 / K and one other kid goes down and the bias.",
            "However the estimator goes up as K grows and we don't know the actual bias.",
            "But we know we can observe this nearest neighbor radio.",
            "We can observe how far they are.",
            "The nearest neighbors 2X R and so our Celldex grows.",
            "Ann will pick K essentially where these two.",
            "Curves cross and we just pick a somewhere around there and it'll turn out that this is.",
            "This will."
        ],
        [
            "Enough, and so we have the following results here.",
            "Which is that a suppose case chosen this way?",
            "Uh.",
            "And this result again also holds simultaneously at all X.",
            "And So what we get is we get nearly the optimal rate in terms of the optimal rate in terms of the interesting dimension D in the neighborhood be.",
            "But any neighborhood be, that is, that has mass at least N -- 1/3."
        ],
        [
            "And as you can see, also as N is allowed to grow to Infinity, the claim eventually applies to any neighborhood be that has non zero mass, so will eventually get into the almost nearly up to nearly optimal rate in any neighborhood in any neighborhood be there has not."
        ],
        [
            "Zero mass.",
            "So the result probably extend also to higher order polynomial regression over classification using KNN as opposed to just averaging the Y values.",
            "And the result probably also extend to the local choice of bandwidth in kernel regression, because yeah, under we can probably also do this sort of bias variance tradeoff in the in kernel regression to choose bandwidth locali at X so that we also adapt to intrinsic dimension at X."
        ],
        [
            "The take home message is.",
            "Simply that nearest neighbor regression performs well without dimension reduction.",
            "And what that means is that you can just focus on tuning K instead of turning a lot of other parameters of your dimension reduction reduction technique."
        ],
        [
            "General question that I'm quite interested in is whether there is a general principle for designing learners that are adapted to interesting dimension.",
            "An answering this question, I think we can have a lot of impact on how we approach the problem of curse of dimension.",
            "And so that's all I'll say about this, and I'll say a lot more and more."
        ],
        [
            "Hello Sir, and thank you for listening.",
            "Also, the question is if X is around the boundary then then the choice of of K becomes becomes more complicated and also.",
            "So, so the result essentially just hold hold for any neighborhood of X&Y maternal that OK, you have higher dimension around these boundaries and.",
            "And then you don't.",
            "You don't do well at this at this point, so the result holds for any X.",
            "Some excess.",
            "You'll do very well.",
            "Some X is you want to you want to die well, but if you make the choice is essentially this way this you'll do as well as you could.",
            "Have done so.",
            "The question is for the choice of K whether you need to know the Lipschitz parameter Lambda delicious hermit on Lambda is assumed unknown, the the result is nearly optimal only in the on.",
            "The nearly optimal relative to the relative to the interesting dimension.",
            "So optimality relative to the Lipschitz parameter, we don't get that.",
            "So instead we will get about Lambda squared instead of the optimal rate in terms of Lambda.",
            "However, when you look at the rate.",
            "The rate is more affected by the dimension, much more affected by dimension than by the smoothness of the regression function.",
            "And so, no, we don't need to know Lambda, but we don't do too bad in terms of Lambda.",
            "But we can talk more about it also later.",
            "OK, so let's thank the speaker again.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So high, so as you can see from the title, I'll be talking about nearest neighbor regression, an dimensional notions of that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mission.",
                    "label": 0
                },
                {
                    "sent": "And so let's start with nearest neighbour regression.",
                    "label": 0
                },
                {
                    "sent": "So the setup is quite simple to the usual setup.",
                    "label": 0
                },
                {
                    "sent": "We have NID point ID, pairs of points, exciting.",
                    "label": 0
                },
                {
                    "sent": "Why I where we assume that Y is a function of X plus some noise.",
                    "label": 0
                },
                {
                    "sent": "Here we're going to assume that X belongs to some matrix space with.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Magic Roll An Why is a real variable, and we'll also assume that F is Lipschitz with some unknown Lipschitz parameter Lambda, and all.",
                    "label": 0
                },
                {
                    "sent": "We mean here by Lipschitz is that if X&X prime, two points that are close, then F of X&F of X primer also close.",
                    "label": 0
                },
                {
                    "sent": "So nearest neighbor regression is very simple.",
                    "label": 0
                },
                {
                    "sent": "When we get a point query point X, we just simply take the K nearest neighbors of X and average the Y values of these neighbors and that's our estimate that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Regression estimates we might use just a normal average, or we might also use a weighted average, using maybe a kernel.",
                    "label": 0
                },
                {
                    "sent": "And because it's so basic, it's so simple.",
                    "label": 0
                },
                {
                    "sent": "It's very common in practice, and hence it's important.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unfortunately, Canon regression is subject to occur to the curse of dimension pretty bad, and what that means is that.",
                    "label": 0
                },
                {
                    "sent": "There always exists a distributor existed there is this distributions on XY such that the excess risk essentially FN of FNC converges to F at a rate of the form and minus 2 / 2 + D. Where big deal here is the dimension of the input space.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this turns out to be true for most non parametric regressors but",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there is hope in the fact that high dimensional data often has low interesting complexity and some examples are data.",
                    "label": 0
                },
                {
                    "sent": "High dimensional bad like, close to a low dimensional hyperplane.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another example is that lies close to a manifold.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An another example that we often use in machine learning is data that is sparse, for instance images.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the common approach when we believe that that are that is high dimensional has low interesting complexity.",
                    "label": 0
                },
                {
                    "sent": "The common approach is to use dimension reduction, an alot of different techniques have been developed overtime and with some.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Success the main result is the following.",
                    "label": 1
                },
                {
                    "sent": "What I would like to talk to you about is that it turns out that nearest neighbor regression performs well without dimension reduction.",
                    "label": 1
                },
                {
                    "sent": "And what we mean by that is that FNK converges at a rate that is adaptive to the unknown interesting dimension, and this without doing any dimension reduction.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what that means in practice is that you might rather spend your time tuning K then trying to tune the parameters of your favorite dimension reduction procedure.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Adaptivity.",
                    "label": 0
                },
                {
                    "sent": "Actually, is a recent direction and has been shown for other other learners.",
                    "label": 0
                },
                {
                    "sent": "So for instance kernel.",
                    "label": 0
                },
                {
                    "sent": "Regressor is there.",
                    "label": 0
                },
                {
                    "sent": "Local polynomial regressors were shown by big only in 2006.",
                    "label": 0
                },
                {
                    "sent": "To be adaptive to manifold dimension.",
                    "label": 0
                },
                {
                    "sent": "And there was a follow up paper by Lafferty and Wasserman concerning semi supervised learning with these regressors.",
                    "label": 1
                },
                {
                    "sent": "Scott in the work showed in 2006 that Dyadic tree classification is also adaptive to a certain notion of interesting dimension.",
                    "label": 0
                },
                {
                    "sent": "Cool Carney imposed and then in 95 showed that one nearest neighbor regression is adaptive to interesting dimension in the sense that it converges at rates that depend just only transit dimension.",
                    "label": 0
                },
                {
                    "sent": "But here one and then regression is not consistent because you just use a single nearest neighbor for any sample size.",
                    "label": 0
                },
                {
                    "sent": "Also RP 3 random projection tree and I'd eat regression is adaptive.",
                    "label": 0
                },
                {
                    "sent": "Our adaptive to interesting dimension or shown by me and my advisors on my old advisor Sandra Dot scooped up.",
                    "label": 0
                },
                {
                    "sent": "And then there is this form of regressors also called that I call tree kernel hybrid regressors that combine ideas of tree based regression and kernel regression that also adapted to interesting dimension.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the above results, though, however, are under global notions of interesting dimension and hear what we're going to care about.",
                    "label": 0
                },
                {
                    "sent": "Local notions of interesting dimensional explain why locality, isn't it?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is important.",
                    "label": 0
                },
                {
                    "sent": "So the outline of the talk is as follows.",
                    "label": 0
                },
                {
                    "sent": "Will first try to give some intuitive notion of interesting dimension that captures all these natural.",
                    "label": 0
                },
                {
                    "sent": "Jewel's situation is that we just talked about and then I'll talk about the result on adaptivity for a PSK.",
                    "label": 1
                },
                {
                    "sent": "For any gain the range log into N and why we picked these branches is because these are where the best case.",
                    "label": 0
                },
                {
                    "sent": "The various values of case can be.",
                    "label": 0
                },
                {
                    "sent": "This is the range where the best values of K can be found, and then after that I'll talk about how to choose a good care in this range without knowing the interesting dimension of the day of the data at X at the point X loca.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lee so.",
                    "label": 0
                },
                {
                    "sent": "Intrinsic dimension, so the notion that the notion of interest dimension that will use will be based on the following intuition, and so I'll describe the intuition in a very simple way.",
                    "label": 0
                },
                {
                    "sent": "So suppose we consider these two balls.",
                    "label": 0
                },
                {
                    "sent": "We have a ball centered at X of radius epsilon R, and a bigger ball centered at X of radius R, and then we'll see we'll look at the way the volumes of these balls differ.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are D dimensional balls, OK?",
                    "label": 0
                },
                {
                    "sent": "So the volume of the bigger ball is about C times are to the D and the volume of the smaller bowl will also be able see times epsilon to the D artadi.",
                    "label": 0
                },
                {
                    "sent": "Therefore the two volumes differ by a factor of epsilon minus T. And notice that this factor is indicative of the dimension of these objects in the natural dimension of this object.",
                    "label": 0
                },
                {
                    "sent": "So now let's look at a probability measure.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we have this measure mu.",
                    "label": 0
                },
                {
                    "sent": "That is a uniform.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the mass that is measure assigned to these balls will behave, as will behave just as the values behave, and so will have again that the mass of the bigger ball and the mass of the smaller ball differ by factor of about epsilon manisty.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This leads to our notion the notion will be using of intrinsic dimension.",
                    "label": 0
                },
                {
                    "sent": "We say will say that Mula measure music CD homogeneous on BXR in neighborhood of X.",
                    "label": 0
                },
                {
                    "sent": "If for all our prime less than R and all epsilon, the measure of all the mass of a ball BXR prime and a massive above BX epsilon R prime differ by a factor of about epsilon minus T, and it will turn out that.",
                    "label": 0
                },
                {
                    "sent": "On the various various conditions on the.",
                    "label": 0
                },
                {
                    "sent": "On the measure, this will hold a for instance the measure concentrated near a low dimensional space, say manifold or sparse or collection of manifolds such as a sparse.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I said so.",
                    "label": 0
                },
                {
                    "sent": "We don't have to remember this equation.",
                    "label": 0
                },
                {
                    "sent": "The main thing to remember is that given the query X, the behavior of Milan meal here will, I'll always use it as the marginal measure on the input SpaceX so that the behavior of meal in a neighborhood B of X can capture the interesting dimension in this neighborhood.",
                    "label": 1
                },
                {
                    "sent": "Be here, let's suppose that space we considering is this square and this line.",
                    "label": 0
                },
                {
                    "sent": "So then X is here, and I'm assuming that B is this.",
                    "label": 0
                },
                {
                    "sent": "Small neighborhood.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As you can see, the location of X matters in defining what the intrinsic dimension is.",
                    "label": 1
                },
                {
                    "sent": "If X happens to be in the square, the intrinsic dimension will be higher.",
                    "label": 0
                },
                {
                    "sent": "If X is on the line, the interesting dimension will be lower.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The size of neighborhood of the neighborhood also is going to matter if we use a bigger neighborhood B, then we capture more of the square and the interesting dimension on this neighborhood will be higher.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For nearest neighbor regression.",
                    "label": 0
                },
                {
                    "sent": "The relevant size of the neighborhood be is going to depend on K and on N. So when we talk about the rate of convergence of nearest neighbor regression will have to say exactly what this neighborhood is under.",
                    "label": 1
                },
                {
                    "sent": "To which neighborhood does it adapt to?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about activity for a face K in this range.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll be going, maybe I'm going too fast, but if we have time in the end we can discuss more and ask questions so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Adaptivity for a fixed game.",
                    "label": 0
                },
                {
                    "sent": "So pick Karen fix getting this range, and let X again.",
                    "label": 1
                },
                {
                    "sent": "Be in some region B and suppose here that K is equal to three right?",
                    "label": 0
                },
                {
                    "sent": "Then they neighborhood be.",
                    "label": 0
                },
                {
                    "sent": "There will be considering we will have to be a neighborhood that contains at least K points and so here I drew this neighborhood that contains the Bell K points or a bit more than K points and RCL vex.",
                    "label": 0
                },
                {
                    "sent": "Here is just defined as defined to be the distance from X to its kit nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "So the intuition is as far.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The rate of convergence of FNC will depend on the variance of FNC and on the bias of FNC.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The variance would always be about 1 / K. Because there is always only care about K points that contributes to the estimate.",
                    "label": 0
                },
                {
                    "sent": "And the bias will be will depend on the distance to the case nearest neighbor, because the farther the nearest neighbors are probably, the further the Y values they output.",
                    "label": 0
                },
                {
                    "sent": "Values will be to the unknown output.",
                    "label": 0
                },
                {
                    "sent": "The unknown why value of X.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It turns out that our key of X will be of this form K over N 1 / D. Where D is the intrinsic dimension in this neighborhood B.",
                    "label": 1
                },
                {
                    "sent": "And so here the only thing that will depend on the intensity dimension will be.",
                    "label": 0
                },
                {
                    "sent": "These are key of X and the variance itself remains.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dicks also ourselves will depend on the mass of be, so intuitively you can imagine that nearest then the neighbors of X will be closer in denser regions be so that's the intuition behind this dependence.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have the following result for any fixed key.",
                    "label": 1
                },
                {
                    "sent": "The result is just says that the results actually holds with high probability simultaneously for all X in our space and for all K in this range.",
                    "label": 1
                },
                {
                    "sent": "And he says that consider any Bobby centered at X such that the bulk contains about K points, so the ball has massive volcano van.",
                    "label": 1
                },
                {
                    "sent": "And suppose that mu is CD homogeneous on B.",
                    "label": 0
                },
                {
                    "sent": "Then we have that FNC is most this far from F of X FM.",
                    "label": 0
                },
                {
                    "sent": "Kovacs is at most is far from F of X an.",
                    "label": 0
                },
                {
                    "sent": "How far depend will depend on K and will depend on D which day is the interesting dimension in this neighborhood.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now notice that we have the mass of the in the rates and.",
                    "label": 0
                },
                {
                    "sent": "What that means is that the rate is best if X is in a dense region B which has low dimension D.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we can talk about how to choose K, how to best, how to choose K at X.",
                    "label": 0
                },
                {
                    "sent": "And here again, we're not trying to choose K globally for every X we're trying to choose K at this particular X so that we can adapt to the interesting dimension in a neighborhood of X.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first, before we can talk about this, we have to see what is the best possible rate we can get for if the interesting dimension is the what is the best possible rate we can get for any?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Racer, so we have the following result.",
                    "label": 0
                },
                {
                    "sent": "Consider any metric measure.",
                    "label": 0
                },
                {
                    "sent": "SpaceX, Mu X Romeo.",
                    "label": 0
                },
                {
                    "sent": "Such that the interesting dimension as we define it earlier is the small D. Then any regressor for any regrets at the best rate that we can expect.",
                    "label": 1
                },
                {
                    "sent": "Is of the form N -- 2 / 2 + D. And one thing that is interesting in this result here is that.",
                    "label": 0
                },
                {
                    "sent": "It doesn't depend on the choice of metric space on the choice of measure, but if you take any metric space and the measure behaves this way will have this rate.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we can talk about how to choose K so that we get this right so first.",
                    "label": 0
                },
                {
                    "sent": "If we were to choose K globally.",
                    "label": 0
                },
                {
                    "sent": "Yeah, what we might do is cross validation or we might try to do dimension estimation.",
                    "label": 0
                },
                {
                    "sent": "We might try to estimate the dimension.",
                    "label": 0
                },
                {
                    "sent": "The interesting dimension D and plug this dimension into the setting of K, But this sort of thing would requires a lot of points, and here we want to choose care locali at X, meaning you'll have to.",
                    "label": 0
                },
                {
                    "sent": "We'll need to have a lot of points locali at X and which is probably unlikely.",
                    "label": 1
                },
                {
                    "sent": "So what we will do instead is the following.",
                    "label": 0
                },
                {
                    "sent": "We will look at will let K very will let Kate grow and we look at the variance of the estimator which is about 1 / K and one other kid goes down and the bias.",
                    "label": 0
                },
                {
                    "sent": "However the estimator goes up as K grows and we don't know the actual bias.",
                    "label": 0
                },
                {
                    "sent": "But we know we can observe this nearest neighbor radio.",
                    "label": 0
                },
                {
                    "sent": "We can observe how far they are.",
                    "label": 0
                },
                {
                    "sent": "The nearest neighbors 2X R and so our Celldex grows.",
                    "label": 0
                },
                {
                    "sent": "Ann will pick K essentially where these two.",
                    "label": 0
                },
                {
                    "sent": "Curves cross and we just pick a somewhere around there and it'll turn out that this is.",
                    "label": 0
                },
                {
                    "sent": "This will.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Enough, and so we have the following results here.",
                    "label": 1
                },
                {
                    "sent": "Which is that a suppose case chosen this way?",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 1
                },
                {
                    "sent": "And this result again also holds simultaneously at all X.",
                    "label": 0
                },
                {
                    "sent": "And So what we get is we get nearly the optimal rate in terms of the optimal rate in terms of the interesting dimension D in the neighborhood be.",
                    "label": 0
                },
                {
                    "sent": "But any neighborhood be, that is, that has mass at least N -- 1/3.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as you can see, also as N is allowed to grow to Infinity, the claim eventually applies to any neighborhood be that has non zero mass, so will eventually get into the almost nearly up to nearly optimal rate in any neighborhood in any neighborhood be there has not.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Zero mass.",
                    "label": 0
                },
                {
                    "sent": "So the result probably extend also to higher order polynomial regression over classification using KNN as opposed to just averaging the Y values.",
                    "label": 1
                },
                {
                    "sent": "And the result probably also extend to the local choice of bandwidth in kernel regression, because yeah, under we can probably also do this sort of bias variance tradeoff in the in kernel regression to choose bandwidth locali at X so that we also adapt to intrinsic dimension at X.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The take home message is.",
                    "label": 1
                },
                {
                    "sent": "Simply that nearest neighbor regression performs well without dimension reduction.",
                    "label": 1
                },
                {
                    "sent": "And what that means is that you can just focus on tuning K instead of turning a lot of other parameters of your dimension reduction reduction technique.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "General question that I'm quite interested in is whether there is a general principle for designing learners that are adapted to interesting dimension.",
                    "label": 1
                },
                {
                    "sent": "An answering this question, I think we can have a lot of impact on how we approach the problem of curse of dimension.",
                    "label": 0
                },
                {
                    "sent": "And so that's all I'll say about this, and I'll say a lot more and more.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello Sir, and thank you for listening.",
                    "label": 1
                },
                {
                    "sent": "Also, the question is if X is around the boundary then then the choice of of K becomes becomes more complicated and also.",
                    "label": 0
                },
                {
                    "sent": "So, so the result essentially just hold hold for any neighborhood of X&Y maternal that OK, you have higher dimension around these boundaries and.",
                    "label": 0
                },
                {
                    "sent": "And then you don't.",
                    "label": 0
                },
                {
                    "sent": "You don't do well at this at this point, so the result holds for any X.",
                    "label": 0
                },
                {
                    "sent": "Some excess.",
                    "label": 0
                },
                {
                    "sent": "You'll do very well.",
                    "label": 0
                },
                {
                    "sent": "Some X is you want to you want to die well, but if you make the choice is essentially this way this you'll do as well as you could.",
                    "label": 0
                },
                {
                    "sent": "Have done so.",
                    "label": 0
                },
                {
                    "sent": "The question is for the choice of K whether you need to know the Lipschitz parameter Lambda delicious hermit on Lambda is assumed unknown, the the result is nearly optimal only in the on.",
                    "label": 0
                },
                {
                    "sent": "The nearly optimal relative to the relative to the interesting dimension.",
                    "label": 0
                },
                {
                    "sent": "So optimality relative to the Lipschitz parameter, we don't get that.",
                    "label": 0
                },
                {
                    "sent": "So instead we will get about Lambda squared instead of the optimal rate in terms of Lambda.",
                    "label": 0
                },
                {
                    "sent": "However, when you look at the rate.",
                    "label": 0
                },
                {
                    "sent": "The rate is more affected by the dimension, much more affected by dimension than by the smoothness of the regression function.",
                    "label": 0
                },
                {
                    "sent": "And so, no, we don't need to know Lambda, but we don't do too bad in terms of Lambda.",
                    "label": 0
                },
                {
                    "sent": "But we can talk more about it also later.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's thank the speaker again.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}