{
    "id": "j5b2n7usjqnq7lkw2plwmml4cmdxkvan",
    "title": "Prior Knowledge and Sparse Methods for Convolved Multiple Outputs Gaussian Processes",
    "info": {
        "author": [
            "Mauricio Alvarez, School of Computer Science, University of Manchester"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_alvarez_pksmcmogp/",
    "segmentation": [
        [
            "OK, so I'm going to talk about prior knowledge and sparse methods for Commvault multiple output Gaussian processes.",
            "This is joint work with Nail Lawrence David Lango who is the owner Carlos Tercero, Madrid Amicale states yes.",
            "Basically there was.",
            "This was done at the School of Computer Science at the University of."
        ],
        [
            "Chester so this talk is divided basically in two parts.",
            "In the first part I'm going to talk about model that we call Layton Force model which is just a hybrid model that combines a mechanistic model with a machine learning technique.",
            "And then I'm going to talk about some sparse approximations for those kind of models."
        ],
        [
            "Queso.",
            "Want to start with the Latent Force model idea?",
            "So traditionally the main focus in machine learning has been model generation through data driven approach.",
            "So usually you have data set.",
            "You have a flexible class of models and using regularization you may predictions over unseen data.",
            "So there are some problems with this approach.",
            "The first one is that if the data is cars relative to the complexity of the system, predictions are usually poor.",
            "Or if the model is used or is forced to extrapolate.",
            "This means to make predictions in regions of the input space for which it doesn't have any training data.",
            "Predictions are inaccurate.",
            "On the other hand."
        ],
        [
            "We have mechanistic models which are inspired by the underlying knowledge of a physical system.",
            "And are common in varying in several areas.",
            "Biology, for example, chemistry.",
            "And all of them basically make a description of a well characterized physical process using a set of differential equations.",
            "The problem with this approach is that usually it's difficult to identify and specify all the interactions in the model.",
            "However, a mechanistic model can enable accurate predictions in regions where they may be no available training data, so we can use this mechanistic idea to try to make extra."
        ],
        [
            "Relation, So what we propose here is to make a combination of both of those two ways of modeling data.",
            "What we call a hybrid approach in which we can hasten mechanistic model with machine learning techniques.",
            "And I'm going to present some examples in dynamical systems in which we use first order and 2nd order differential equations, and then I'm going to talk about how to deal with several inputs using partial differential equations.",
            "OK, so first."
        ],
        [
            "Let's review the basic concept of a latent variable model so our approach can be seen as a type of latent variable model in which we have a set of outputs, this matrix Y which is dimensions and data points, and the output dimensions.",
            "And then we expressed as a factor model with a set of latent variable models of a matrix of dimensions and times Q, and then we use this transformation matrix of dimensions kadio KD.",
            "Sorry.",
            "And then we allow a noise model which is just made this matrix E which is a matrix variate, Gaussian noise with columns following a Gaussian distribution with this covariance matrix.",
            "So this is the basic latent variable model.",
            "So in PCA principal component analysis on factor analysis, the usual approach is that you put a Gaussian process.",
            "Sorry you put a Gaussian prior over the latent variable.",
            "Then you integrate out and then what you do is optimize to find this weighting matrix W. So if you have data with the temporal nature."
        ],
        [
            "And assuming Gaussian prior for the Gaussian Markov prior for the rows of U and this leads to the common filter is smoother.",
            "Here we consider the joint distribution.",
            "Of the latent variables, given the time points.",
            "With the following form you see here, so it's a factor in this form where we each of the columns in this matrix U is basically a Gaussian process described by a particular covariance function.",
            "So each of the columns in this cover in this in this latent matrix U is Gaussian process with covariance KU KUQ.",
            "And with this form for the covariance, we can.",
            "Sorry with this form for the for the latent variables we can workout the covariance matrix of the covariance function for the outputs, and that is called a semiparametric time factor model that was introduced by UAT.",
            "Matthias, Michael Jordan.",
            "So."
        ],
        [
            "Our contribution is that we are going to include a further dynamical system with the mechanistic inspiration.",
            "So again, we can reinterpret our equation of the latent variable model as a force balance equation, where here we have included an additional matrix B.",
            "Where is?",
            "This?",
            "Is a diagonal matrix of something that we call the spring constants.",
            "So in this diagonal, each of the entries corresponds to a spring constant associated to each output.",
            "And then we have a game.",
            "This set of latent variables and those latent variables are weighted by this matrix that we call South, and it's a matrix of sensitivities on both models.",
            "The latent variable, the classic model and this force balance model can be related using this transformation for the weighting matrix.",
            "And here we have a cartoon example of their model."
        ],
        [
            "So each of the outputs has an associated spring constant and then we over each of those outputs we apply a force you T force.",
            "You take him for their key."
        ],
        [
            "We decompose instead of simply forces you want until you cube and the way in which those forces act over the output is through a set of levers and basically those levels are just giving a value of the sensitivity, which quantifies the particular influence influence of each of those latent forces.",
            "Latent function, sorry.",
            "So here we have the whole model, and here again this is the equation the force balance."
        ],
        [
            "Equation.",
            "Additionally, we can include dampers and masses so to the model that we had before we add some new factors or terms this."
        ],
        [
            "Term which is corresponds to the 1st order derivative with respect to time of the outputs times this matrix C which is a diagonal matrix that corresponds to the dumping where the entrance of this diagonal matrix are just dumping coefficients.",
            "And then we have this second order derivative with respect to time of the outputs times this matrix m.corresponds to the masses associated as diagonal matrix with entries.",
            "Representing the masses of each of the mass of each output."
        ],
        [
            "And then again we have this cartoon example here or diagram where we show that each output has an associated spring constant, Anna damper constant Annemasse and then."
        ],
        [
            "We can act over that using this set of forces through this set of levers, and this is our second order."
        ],
        [
            "Model."
        ],
        [
            "So the important point of these kind of models is that they allow you to include behaviors like inertia and resonance.",
            "Behaviors are very difficult to model with just the latent variable model approach the classic One.",
            "Since we are making this analogy with forces, we call these models latent force models, and when were you thinking of these models is to consider puppetry.",
            "So you have to pop it here then can manipulate this, pop it so the puppetry just moves make through a simple set of forces.",
            "You can generate a huge range of movements in the in the puppet."
        ],
        [
            "So here in this talk, I'm going to concentrate on this second order dynamical system, but we can also talk about first order systems.",
            "So again we have here our second order differential equation for each of the outputs with an associated mouse damper and spring constant.",
            "And then we have again a set of latent forces acting over this output."
        ],
        [
            "So we can workout the form of the output which is giving this in terms of a convolution process or a convolution operation.",
            "So where the convolution here is just this expectation.",
            "Sorry this exponential times sign times the latent force.",
            "And if we assume that this latent forces follow Gaussian process with particular covariances, for example, the RBF covariance, we can workout the covariance for the outputs analytically.",
            "Each output is described, but we score."
        ],
        [
            "The damping ratio, which is just the damping coefficient over the square root of the spring constant and damping ratio, is greater than one.",
            "You have an overdamped system if it's equal to 1 is a critical damn system is less than one, is an overdamped system.",
            "And what we show here is a plot of.",
            "System with three outputs on one latent force, and this is the covariance matrix that you obtain for this for this model.",
            "So the first output is.",
            "It corresponds to an underdamped system.",
            "The second output corresponds to an overdamped system.",
            "On the third one, so critical dumb system.",
            "So what we can see here in the blocks in the main diagonal or just the covariance, is associated to either the latent force or the outputs.",
            "So you see the covariance of the latent forces, just the classical RBF covariance matrix.",
            "And then we see that the covariance for the output number one, which is another underdamped system, has this alternate form we have.",
            "Low values of correlation, but then it has also big values of correlation, so you have this alternated form for the covariance and then when you compare these two covariance for the overdamped system for the critical damn system you see they look like one of them is like a bird.",
            "Blurred version of the other.",
            "So apparently this is taking is taking for the overtime system more time to express the correlation of the outputs because it takes a little more time to reach this equilibrium point and then the terms.",
            "Out of the diagonal is corresponds to the cross covariance terms between the latent force and the outputs that.",
            "Following some sense that particular some part of the auto covariance.",
            "We can draw samples."
        ],
        [
            "When that covariance matrix and this is what they look like inside and we have the latent force in red, we have the underdamped system.",
            "Green we have the overdamped system and blue.",
            "We have the critical dump system.",
            "This is for example, another."
        ],
        [
            "Al another sample from the model and that is where you can notice here.",
            "This bump of the latent force between 5:00 and 10:00 is in some sense driving the behavior of the critical dump on the Andover dump outputs, and you see that the critically damped output is following more faithfully.",
            "The behavior of the of the input force that the overdamped signal here in green, however they tried, they follow the same set of cycles here.",
            "So when this one is up the other is also up, and that's typical behavior for this over dump and this critical dump system.",
            "So you see that the drill one that corresponds to the underdamped system is more prone to follow the latent force.",
            "As you see there.",
            "This is another sample."
        ],
        [
            "The model."
        ],
        [
            "This is a further one, so here this is a particular very, very interesting one, because you see that it looks like the underdamped system is going to resonance with the latent Force One with the latent force function.",
            "OK."
        ],
        [
            "So we took this differential equations and we feed it some data from the same you database motion capture data.",
            "In particular, we took the motions 1819 and 20 from subject 14.",
            "Either corresponds to a balancing kind.",
            "I will show you next and.",
            "We feel the model with two latent forces.",
            "So it's going."
        ],
        [
            "Show you this simulation in my lab.",
            "So here you see.",
            "Subject 49 Motion 18 so it corresponds to a guy there is balancing.",
            "So first he has his hands up and then he brings his hands down and at the same time he's lifting his right leg.",
            "As you see there.",
            "This is a subject 14 emotion 19 which is basically doing the same movement.",
            "So we assume a differential equation for all the all the outputs in this model, and then we feed it using maximum likelihood with two forces and what we see in this in this panel.",
            "Um?",
            "Is a panel of four Swan against Force 2.",
            "And what you see in this cross is correspond to the posterior of this latent forces for this particular data set.",
            "So what I wanted to show you is how these models can make some kind of extrapolation.",
            "So if we follow these latent forces that you see here.",
            "So we see we see how we can generate some movement for this guy.",
            "And the guy is able to make some movements that were not in the training data.",
            "So is this movement which he saw in the training data.",
            "But moving the left leg back was not in the training data.",
            "So you see that there is the notion of using this mechanistic.",
            "Ideas to make extrapolation.",
            "Yeah.",
            "It was just a.",
            "An example.",
            "How?",
            "OK, so."
        ],
        [
            "Just to evaluate the performance of the model in terms of the in terms of the mean squared error, we took a smaller data set that corresponds to the subject left arm.",
            "So we train again using these two latent forces.",
            "Unfortunate thing we condition only on the observations of the shoulders.",
            "Orientation of the motion 20 and we make the prediction for the other part of the arm."
        ],
        [
            "So here I show the results of regression in terms of the room in square error for the latent force model that uses this idea of the differential equations and a simple regression model that assumes independence in all the outputs.",
            "So you see that for the radius and risk their performance is similar basically, But then for the other outputs the performance is much better in the in the latent Force model.",
            "OK this."
        ],
        [
            "Was an example using this.",
            "Ordinary differential equations in which the input is just the time.",
            "But we can use this model to allow for several input input dimensions.",
            "So.",
            "I'm going to my TV using this example, so this is an escape of the region of the Swiss Jura and some time ago there was huge mining activity.",
            "There is a classical problem, enjoy statistics and these methods were spread all over the place.",
            "So today we have access to concentrations of some of the metals in some particular locations.",
            "But we don't have access to the concentrations at all allowed sampling points.",
            "So what we want to do is to use the concentrations to which we have access to try to predict the concentrations for those metals for which we don't have access.",
            "So what we are going to assume is that the way in which these metals originally spread over the place where using the user model using a diffusion process.",
            "So we can assume that our model initially they were.",
            "Initially the metals were spread in a particular special configuration and there."
        ],
        [
            "Through time."
        ],
        [
            "The metals were just start."
        ],
        [
            "Into the fuse all over the place."
        ],
        [
            "And for that we use a simplified version of the diffusion equation that you see here.",
            "That includes the time and also include the input dimensions that in this case will correspond to the special input dimensions and yd correspond to the outputs.",
            "So if we make again the solution to this differential equation is given in the form of this convolution.",
            "Where this?",
            "Caramel here.",
            "This function here just correspond to this.",
            "This Greens function that you see here, so analogies that initially we assume that the latent force just give us some information how the metals were spread, and then through time they were just moving through this differential equation."
        ],
        [
            "So we have always this model using.",
            "In terms of the the mean absolute error.",
            "So the idea is that we, for this kind of system, so we have access to secondary variable, so secondary information.",
            "So the purpose is that you use that secondary information to predict the behavior of the primary information.",
            "So in this case what we want to do is we want to use information about the concentrations of nickel and seeing for example that we have to predict the concentration of cadmium and the same for these other metals.",
            "So we compare our approach against independent Gaussian processes.",
            "An ordinary cokriging, which is a method used in the in the geostatistics literature, and you see the three out of four of the metals in terms of the mean absolute error, we make very, very very performance.",
            "OK, so."
        ],
        [
            "These were examples of these latent force model idea.",
            "Now in the second part of the talk I'm going to give you an overview of some sparse approximations that are necessary when we want to work with several outputs and so input points or yeah.",
            "So I'm going to talk about the latent Force model again, this time in the context of convolution processes.",
            "So consider a gain of set of functions, so you have the functions any function can be expressed as this convolution.",
            "So we have this smoothing kernel functions times these.",
            "Latent functions you can allow the influence of more than one latent function.",
            "So Q functions and then you can also include an independent process and then this is the form of the of the output."
        ],
        [
            "So this is just a cartoon example.",
            "You have one latent force and then you can bold that force."
        ],
        [
            "Is 1 four's without a smoothing kernel for output one and one is moving kernel for output two and then you generate your model for the outputs and then you can add."
        ],
        [
            "And independent process to each output and then you get a noisy version."
        ],
        [
            "None of your of your original model."
        ],
        [
            "For the outputs, we can workout the covariance for this outputs YD and the covariance is just the sum of the covariance of the convolution operation and then the covariance of the independent process.",
            "And this covariance is just given this by this expression.",
            "This double sum and then this double integral and then you have a Green's function for each of the theory to each of the outputs.",
            "And then you can see that this covariance is just basically the most important term here is this.",
            "Once you choose the form for the covariance of the latent force an if you can solve analytically this integral, you can find the covariance for your outputs.",
            "Here we assume that the covariance for the outputs is just corresponds to independent Gaussian processes, so you have.",
            "One covariance for each of these Q, and in that way you will get rid of one of these sums."
        ],
        [
            "So traditionally what you do is that you find the marginal likelihood and you use the marginal likelihood to fit your parameters.",
            "So in the case in which we used the disco this convolution covariances, we have this expression here where Y corresponds to.",
            "If you have a set of input points.",
            "So you have some training data.",
            "So This is why corresponds to stacking all the corresponding outputs of the data of all your outputs.",
            "KFF corresponds to the covariance matrix with blocks given by this covariance expression of the covariance that we saw before this Sigma corresponds to the matrix of knowledge variances.",
            "In this case, we assume that the independent processes corresponds to noise.",
            "We have these five.",
            "There is the set of parameters of the covariance matrix, and you have a set of inputs X one to XN.",
            "So as I said before, when you want to use this in practice, what you do is you find your parameters maximizing this marginal likelihood, and then you also need that for the for the prediction step.",
            "So learning from the log likelihood involves the inverse, making the inverse of this covariance with growth with complexity and QBQ where N corresponds to the number of data points.",
            "Andy corresponds to the number of outputs."
        ],
        [
            "And this is the expression for the predictive distribution.",
            "So here you have again to make the inverse of covariance matrix and prediction is ND for them in an N squared.",
            "The square for the variance.",
            "So you have this cubic complexity when you have several inputs, several data, and several output."
        ],
        [
            "It can become problematic.",
            "So.",
            "To see the approximations that we make.",
            "Let's train to break this equation in terms of generative process.",
            "So what you do is that you have some uncertainty in the latent forces, which is described basically by stochastic process or uses.",
            "You draw a sample sample from that process and then you solve the integral and then the uncertainty that you have here is propagated through this linear operation to the outputs.",
            "So that's the way in which you would solve the whole problem."
        ],
        [
            "You can take.",
            "Another path to reduce the computational complexity and is to change this continuous operation by this discretized version of the convolution.",
            "So instead of working with the continuous function, you just work with a set of points and then you reduce your convolution operation to these discretized version.",
            "So what we do here is a different thing.",
            "In order to reduce computational."
        ],
        [
            "Flexity instead of working with the full process or with the discretized version of the process, we're going to work with the mean.",
            "With the main function of the conditional prior, so you draw some points of your full process, you take some points of your food process and condition on those points that I show here as red dots, you draw a sample using the conditional prior.",
            "So the sample that you see here is in blue.",
            "And with that blue you solve the integral operation."
        ],
        [
            "Let's say the leads to this form for the for the likelihood, so the likelihood of your outputs has this mean function that you see here as this covariance.",
            "So one might think OK if I condition on the on the latent force, the outputs would be independent.",
            "So ideally you wouldn't have any sort of uncertainty for this likelihood turn.",
            "The thing here is that since you have a discrete highest version, so you have this, you have a finite number of points.",
            "You still have to deal with some uncertainty in your outputs.",
            "So."
        ],
        [
            "Our first assumption to reduce computational complexity in this likelihood term is to assume that the cross terms in that likelihood are just zero.",
            "In other words, we assume that given this finite set of points or inducing points, the outputs are independent.",
            "So we change this form.",
            "We have three in this example.",
            "We just three outputs and we change that for this."
        ],
        [
            "Form of the of the covariance.",
            "So the intuition is that.",
            "If your Lenten forms is sufficiently smooth, this conditional mean will approximate.",
            "Better."
        ],
        [
            "And then you integrate out in that likelihood term, you integrate out the latent forces, or the latent functions, and then you get this new marginal likelihood which is given by this log low rank approximation.",
            "But sorry, plus this block diagonal matrix, which is just has this expression in the main blocks and zero outside the blocks.",
            "The main blocks of the diagonal."
        ],
        [
            "In other words, what you are doing is approximated your full covariance matrix by a matrix that is equal to the full covariance in the block diagonal terms and a low rank approximation out of the diagonal.",
            "Look, this is different."
        ],
        [
            "Use the discretized version in which you replace the full covariance matrix by this outer product of this design matrix related with the discrete convolution."
        ],
        [
            "So here you have the.",
            "Predictive distribution for the outputs where this matrix T without the Asterix is just a block diagonal terms that we had before.",
            "And the computational complexity here is."
        ],
        [
            "Associated to inverting, so once you apply the matrix inversion lemma to embarrassing this matrix D that grows as N qv.",
            "Times sorry, plus the computational complexity associated with a lower rank approximation, which is ND M ^2.",
            "So you see that if we have.",
            "If we assume that this M indicates the number of points that you choose of inducing points that you choose, if those numbers that number is equal to the number of data points that you have, then you have here N * N ^2 as NQ, and you match the complexity of using independent processes for each output.",
            "So the computational complexity for computing the mean grocers and DM for the approximation and DM square for the variance.",
            "And this has an analogous in the in the univariate case of a single output process, there is called in the literature is known as a partial independent training conditional approximation introduced by Kenyan Arrow Candelon cars, Rasmussen.",
            "Can we refer to this approximation also at speeds?"
        ],
        [
            "Brooks, spitze as the PC approximation.",
            "So in some applications the Ncube term in the computational complexity in the pits approximation.",
            "On the aim squared term, this storage is can be a steal.",
            "Spence, if so we can do an additional assumption and is to assume that the.",
            "Input points of the outputs are conditionally independent given the latent forces.",
            "So what I mean like before?"
        ],
        [
            "We have for the pits approximation that the outputs as a whole each output where independent, given the latent forces.",
            "Now we have an additional assumption.",
            "We assume that we assume that with initial output each point is."
        ],
        [
            "Given the latent force.",
            "And then we can again we can workout the marginal likelihood integrating out."
        ],
        [
            "The under a prior for this latent forces and the marginal likelihood has this form.",
            "Again, we have the low rank approximation and we have replaced the block diagonal matrix just for a diagonal matrix."
        ],
        [
            "So this is the figure that we had before for PC in fizi.",
            "We just replace this main blocks."
        ],
        [
            "Buy some approximation QFF."
        ],
        [
            "And that."
        ],
        [
            "Estimation.",
            "Is just again we keep the main so we keep the diagonal terms that correspond to the full Gaussian.",
            "Sorry photo full covariance matrix of the full process.",
            "An outside diagonal terms.",
            "We use the low rank approximation to compute the covariance."
        ],
        [
            "And now the computational complexity is NDM Square and storage is NDM, and again we have the same computation computational complexity to compute the mean and the variance.",
            "And this has an analogous again in the single output case, which is called a full independent training conditional introduced by Snelson Anger Ghahramani.",
            "And we call it the same.",
            "Here the fizzi approximation."
        ],
        [
            "We can have an additional approximation in which you assume that your marginal likelihood, sorry that your likelihood term just has zero covariance.",
            "So assume that given this set of finite points, you can totally determine determine the outputs, and then when you workout, the marginal likelihood is just this low rank approximation that you see here.",
            "This is a computational complexity, is the same as fizzie and also it has an analogous form to the two.",
            "Another approximation for the single output case, which is called the DTC deterministic training conditional approximation."
        ],
        [
            "OK, now I'm going to show some examples in which I will compare the performance of these approximations against the full Gaussian process.",
            "So for all, this payments are going to consider that the covariance of the latent forces latent forces follow this for exponential form with this matrix diagonal matrix L, which allows to have different length scales for each dimension, and then each of the greens functions or bear the smoothing kernel functions has has this form that you see here.",
            "Also an exponential.",
            "So then you can analytically workout the covariance for the output.",
            "So in the."
        ],
        [
            "This example with artificial data.",
            "What you do is that you draw.",
            "You have four.",
            "This in this setup you have four outputs and you use your your full Gaussian process and you draw a sample from that Gaussian process of these four outputs and what I'm showing here is the prediction that you make only over the output #4 the prediction using the full Gaussian process, again optimizing the marginal likelihood for the full Gaussian process.",
            "The prediction that you obtain using the DTC approximation.",
            "The feeds approximation the pizza approximation.",
            "For this 4th or 4th output, this crosses here.",
            "Represents the positions of inducing points.",
            "Once you optimize the marginal likelihood.",
            "So you see that the performance of the fits in the pizza approximation is quite.",
            "It's quite similar to the full GP.",
            "And the positions of the inducing points are great spread over the input interval.",
            "See that for the DC approximation there is a good feeling of the main function, but the variance is not so well fitted and the reason behind that is the fact that you are ignoring this additional uncertainty in the likelihood term."
        ],
        [
            "And here I'm just quantifying the performance in terms of the standardized mean squared error.",
            "For the full Gaussian process on for the three approximations and you see that the performance is basically the same in terms of the mean squared error.",
            "But then when you look at the mean standardized log loss that turns things into account, the uncertainty you see that debits debits approximation is the pits approximation.",
            "More negative values in this in this measure indicates better models.",
            "So what we see here is that PC obtains better results than, for example.",
            "DTC and fits is not so bad, I mean, but the best of the best of them is Pizzi.",
            "And here I have some training times per iteration just to give you an idea of the time it takes to train the full GP per iteration.",
            "One point, 97 seconds.",
            "For this example, the DTC just take .2 seconds .45 for 15.59 for PC."
        ],
        [
            "OK, so."
        ],
        [
            "Think I'm just going to keep this."
        ],
        [
            "No."
        ],
        [
            "Going to rain."
        ],
        [
            "You see the jury."
        ],
        [
            "Example that we saw."
        ],
        [
            "For.",
            "So here I'm going to make the prediction over commune using nickel and sink as we saw before, and I'm going to compare the mean absolute error using these approximations on the full Gaussian process.",
            "So here D stands for DTC F, stands for FIT, CP stands for pit CFGP for full Gaussian process.",
            "CK for the Kriging method and the independent Gaussian process and these numbers that you see here are the number of inducing points that you use.",
            "To make the approximation.",
            "So you see that there is a lot of uncertainty for the DTC.",
            "Well, you see that fits Ian Pizzi actually, let's concentrate PC for a moment.",
            "You see that pizza for 5450 inducing points is better than doing independent Gaussian processes.",
            "And then every time that you put more inducing points, you reach the performance of the full Gaussian process and with two 200 points you may bear the Coke region system.",
            "You see here and compare PC to fit.",
            "See you see that for the same amount of inducing points, the performance of PC's is better.",
            "Also, you can see the error bars are decent or normal for the both approximations compared to DTC."
        ],
        [
            "OK, so.",
            "We have presented a conclusions we have presented a hybrid approach in which we combine this simple mechanistic models with Gaussian processes.",
            "We have seen how convolution process.",
            "I wait till all men data trading models.",
            "To include characteristic characteristics of physical systems.",
            "So we can see how we can construct various structure covariance functions for the outputs under this.",
            "Differential equations models and we have presented a series of series of sparse approximations to deal with this, with the computational complexity in this combo Gaussian process."
        ],
        [
            "So Mark knowledge munts to Google for Google researcher Warranto EP is RC for this grant."
        ],
        [
            "Thank you."
        ],
        [
            "Do you have a technical report online that describes all these methods?",
            "Is all the software publicly available?",
            "Yeah.",
            "It is.",
            "Yes, you have to.",
            "I was unclear with the metal example.",
            "If you had, you wouldn't believe that they were basically diffusing at similar rates.",
            "And what if you have two different diffusion processes that are operating at different time scales?",
            "Well, I mean, this is a very simplified."
        ],
        [
            "Version of the diffusion equation because if you see we are not.",
            "I mean the way in which we introduced the latent Force.",
            "You see here, here in the in the form of the diffusion equation you will have an additional input input force, as in the case of the of the ordinary differential equation.",
            "So here the input force is basically the initial condition, so the uncertainties in the initial condition and this and this couple D that represents the.",
            "Diffusion coefficient in that in that input dimension is basically a parameter associated here that we learn hyperparameter that we learn.",
            "Is different for each.",
            "Yeah it is associated with the.",
            "No, yeah, in Europe.",
            "Calling office message so it's Ian.",
            "As I remember it, yeah he comes from Poland independent training on decimal or partial involuntary manslaughter.",
            "So how do you make your predictions is you use this conditional framework to make predictions?",
            "Or do you make it the full governance usually use the new equations that you have your party for your predictive distribution.",
            "So you can use this set of equations that include these terms associated."
        ],
        [
            "You're you're low rank approximations.",
            "But you can also try to do that.",
            "I mean if it's not too expensive you can train using your sparse approximations and then maybe you can do as a way to.",
            "No, it's not.",
            "If you if you did like that that you train with this person for tuition and make the principal GP, that would give you white patches on this, OK?",
            "This is.",
            "Yeah, yeah, this is a little bit like terminology so OK. Maybe custom?",
            "Just bring some shifting integrate over the space.",
            "So why couldn't?",
            "The space is very you really have two points already.",
            "Have someone the density in different areas are very different.",
            "You put can you take into account this in some way or something?",
            "You know situation wherever you wanna say equals are standing on some low dimensional structure or maybe just a few of them.",
            "While the situation well, I mean you can.",
            "You can amend this framework in several ways and we have done it so.",
            "There are solar extensions that you can do.",
            "For example, we can use this variational approximation in which we penalize regions of the input space that away from the data.",
            "So this inducing points start moving, but then with this variational approximation you make that those points are around your data, but you can also change a bit the framework.",
            "So for example, you could assume in this convolution idea.",
            "I think people have done before.",
            "In the past.",
            "Is that?",
            "So instead of working with the same, I mean instead of this, say having the same dimension that X, you can make a transformation.",
            "Let's say I don't know some kind of some matrix and then that matrix would appear in your covariance and you will have to make."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk about prior knowledge and sparse methods for Commvault multiple output Gaussian processes.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with Nail Lawrence David Lango who is the owner Carlos Tercero, Madrid Amicale states yes.",
                    "label": 0
                },
                {
                    "sent": "Basically there was.",
                    "label": 1
                },
                {
                    "sent": "This was done at the School of Computer Science at the University of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chester so this talk is divided basically in two parts.",
                    "label": 0
                },
                {
                    "sent": "In the first part I'm going to talk about model that we call Layton Force model which is just a hybrid model that combines a mechanistic model with a machine learning technique.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to talk about some sparse approximations for those kind of models.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Queso.",
                    "label": 0
                },
                {
                    "sent": "Want to start with the Latent Force model idea?",
                    "label": 0
                },
                {
                    "sent": "So traditionally the main focus in machine learning has been model generation through data driven approach.",
                    "label": 1
                },
                {
                    "sent": "So usually you have data set.",
                    "label": 1
                },
                {
                    "sent": "You have a flexible class of models and using regularization you may predictions over unseen data.",
                    "label": 1
                },
                {
                    "sent": "So there are some problems with this approach.",
                    "label": 1
                },
                {
                    "sent": "The first one is that if the data is cars relative to the complexity of the system, predictions are usually poor.",
                    "label": 0
                },
                {
                    "sent": "Or if the model is used or is forced to extrapolate.",
                    "label": 0
                },
                {
                    "sent": "This means to make predictions in regions of the input space for which it doesn't have any training data.",
                    "label": 0
                },
                {
                    "sent": "Predictions are inaccurate.",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have mechanistic models which are inspired by the underlying knowledge of a physical system.",
                    "label": 1
                },
                {
                    "sent": "And are common in varying in several areas.",
                    "label": 0
                },
                {
                    "sent": "Biology, for example, chemistry.",
                    "label": 0
                },
                {
                    "sent": "And all of them basically make a description of a well characterized physical process using a set of differential equations.",
                    "label": 1
                },
                {
                    "sent": "The problem with this approach is that usually it's difficult to identify and specify all the interactions in the model.",
                    "label": 0
                },
                {
                    "sent": "However, a mechanistic model can enable accurate predictions in regions where they may be no available training data, so we can use this mechanistic idea to try to make extra.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Relation, So what we propose here is to make a combination of both of those two ways of modeling data.",
                    "label": 0
                },
                {
                    "sent": "What we call a hybrid approach in which we can hasten mechanistic model with machine learning techniques.",
                    "label": 1
                },
                {
                    "sent": "And I'm going to present some examples in dynamical systems in which we use first order and 2nd order differential equations, and then I'm going to talk about how to deal with several inputs using partial differential equations.",
                    "label": 1
                },
                {
                    "sent": "OK, so first.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's review the basic concept of a latent variable model so our approach can be seen as a type of latent variable model in which we have a set of outputs, this matrix Y which is dimensions and data points, and the output dimensions.",
                    "label": 1
                },
                {
                    "sent": "And then we expressed as a factor model with a set of latent variable models of a matrix of dimensions and times Q, and then we use this transformation matrix of dimensions kadio KD.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 1
                },
                {
                    "sent": "And then we allow a noise model which is just made this matrix E which is a matrix variate, Gaussian noise with columns following a Gaussian distribution with this covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic latent variable model.",
                    "label": 0
                },
                {
                    "sent": "So in PCA principal component analysis on factor analysis, the usual approach is that you put a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Sorry you put a Gaussian prior over the latent variable.",
                    "label": 0
                },
                {
                    "sent": "Then you integrate out and then what you do is optimize to find this weighting matrix W. So if you have data with the temporal nature.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And assuming Gaussian prior for the Gaussian Markov prior for the rows of U and this leads to the common filter is smoother.",
                    "label": 1
                },
                {
                    "sent": "Here we consider the joint distribution.",
                    "label": 0
                },
                {
                    "sent": "Of the latent variables, given the time points.",
                    "label": 0
                },
                {
                    "sent": "With the following form you see here, so it's a factor in this form where we each of the columns in this matrix U is basically a Gaussian process described by a particular covariance function.",
                    "label": 0
                },
                {
                    "sent": "So each of the columns in this cover in this in this latent matrix U is Gaussian process with covariance KU KUQ.",
                    "label": 0
                },
                {
                    "sent": "And with this form for the covariance, we can.",
                    "label": 0
                },
                {
                    "sent": "Sorry with this form for the for the latent variables we can workout the covariance matrix of the covariance function for the outputs, and that is called a semiparametric time factor model that was introduced by UAT.",
                    "label": 0
                },
                {
                    "sent": "Matthias, Michael Jordan.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our contribution is that we are going to include a further dynamical system with the mechanistic inspiration.",
                    "label": 1
                },
                {
                    "sent": "So again, we can reinterpret our equation of the latent variable model as a force balance equation, where here we have included an additional matrix B.",
                    "label": 0
                },
                {
                    "sent": "Where is?",
                    "label": 0
                },
                {
                    "sent": "This?",
                    "label": 1
                },
                {
                    "sent": "Is a diagonal matrix of something that we call the spring constants.",
                    "label": 1
                },
                {
                    "sent": "So in this diagonal, each of the entries corresponds to a spring constant associated to each output.",
                    "label": 0
                },
                {
                    "sent": "And then we have a game.",
                    "label": 0
                },
                {
                    "sent": "This set of latent variables and those latent variables are weighted by this matrix that we call South, and it's a matrix of sensitivities on both models.",
                    "label": 0
                },
                {
                    "sent": "The latent variable, the classic model and this force balance model can be related using this transformation for the weighting matrix.",
                    "label": 0
                },
                {
                    "sent": "And here we have a cartoon example of their model.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So each of the outputs has an associated spring constant and then we over each of those outputs we apply a force you T force.",
                    "label": 0
                },
                {
                    "sent": "You take him for their key.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We decompose instead of simply forces you want until you cube and the way in which those forces act over the output is through a set of levers and basically those levels are just giving a value of the sensitivity, which quantifies the particular influence influence of each of those latent forces.",
                    "label": 0
                },
                {
                    "sent": "Latent function, sorry.",
                    "label": 0
                },
                {
                    "sent": "So here we have the whole model, and here again this is the equation the force balance.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equation.",
                    "label": 0
                },
                {
                    "sent": "Additionally, we can include dampers and masses so to the model that we had before we add some new factors or terms this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Term which is corresponds to the 1st order derivative with respect to time of the outputs times this matrix C which is a diagonal matrix that corresponds to the dumping where the entrance of this diagonal matrix are just dumping coefficients.",
                    "label": 1
                },
                {
                    "sent": "And then we have this second order derivative with respect to time of the outputs times this matrix m.corresponds to the masses associated as diagonal matrix with entries.",
                    "label": 0
                },
                {
                    "sent": "Representing the masses of each of the mass of each output.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then again we have this cartoon example here or diagram where we show that each output has an associated spring constant, Anna damper constant Annemasse and then.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can act over that using this set of forces through this set of levers, and this is our second order.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the important point of these kind of models is that they allow you to include behaviors like inertia and resonance.",
                    "label": 0
                },
                {
                    "sent": "Behaviors are very difficult to model with just the latent variable model approach the classic One.",
                    "label": 0
                },
                {
                    "sent": "Since we are making this analogy with forces, we call these models latent force models, and when were you thinking of these models is to consider puppetry.",
                    "label": 0
                },
                {
                    "sent": "So you have to pop it here then can manipulate this, pop it so the puppetry just moves make through a simple set of forces.",
                    "label": 0
                },
                {
                    "sent": "You can generate a huge range of movements in the in the puppet.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here in this talk, I'm going to concentrate on this second order dynamical system, but we can also talk about first order systems.",
                    "label": 1
                },
                {
                    "sent": "So again we have here our second order differential equation for each of the outputs with an associated mouse damper and spring constant.",
                    "label": 1
                },
                {
                    "sent": "And then we have again a set of latent forces acting over this output.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can workout the form of the output which is giving this in terms of a convolution process or a convolution operation.",
                    "label": 0
                },
                {
                    "sent": "So where the convolution here is just this expectation.",
                    "label": 0
                },
                {
                    "sent": "Sorry this exponential times sign times the latent force.",
                    "label": 0
                },
                {
                    "sent": "And if we assume that this latent forces follow Gaussian process with particular covariances, for example, the RBF covariance, we can workout the covariance for the outputs analytically.",
                    "label": 0
                },
                {
                    "sent": "Each output is described, but we score.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The damping ratio, which is just the damping coefficient over the square root of the spring constant and damping ratio, is greater than one.",
                    "label": 1
                },
                {
                    "sent": "You have an overdamped system if it's equal to 1 is a critical damn system is less than one, is an overdamped system.",
                    "label": 0
                },
                {
                    "sent": "And what we show here is a plot of.",
                    "label": 0
                },
                {
                    "sent": "System with three outputs on one latent force, and this is the covariance matrix that you obtain for this for this model.",
                    "label": 0
                },
                {
                    "sent": "So the first output is.",
                    "label": 1
                },
                {
                    "sent": "It corresponds to an underdamped system.",
                    "label": 0
                },
                {
                    "sent": "The second output corresponds to an overdamped system.",
                    "label": 1
                },
                {
                    "sent": "On the third one, so critical dumb system.",
                    "label": 0
                },
                {
                    "sent": "So what we can see here in the blocks in the main diagonal or just the covariance, is associated to either the latent force or the outputs.",
                    "label": 0
                },
                {
                    "sent": "So you see the covariance of the latent forces, just the classical RBF covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And then we see that the covariance for the output number one, which is another underdamped system, has this alternate form we have.",
                    "label": 0
                },
                {
                    "sent": "Low values of correlation, but then it has also big values of correlation, so you have this alternated form for the covariance and then when you compare these two covariance for the overdamped system for the critical damn system you see they look like one of them is like a bird.",
                    "label": 0
                },
                {
                    "sent": "Blurred version of the other.",
                    "label": 0
                },
                {
                    "sent": "So apparently this is taking is taking for the overtime system more time to express the correlation of the outputs because it takes a little more time to reach this equilibrium point and then the terms.",
                    "label": 0
                },
                {
                    "sent": "Out of the diagonal is corresponds to the cross covariance terms between the latent force and the outputs that.",
                    "label": 0
                },
                {
                    "sent": "Following some sense that particular some part of the auto covariance.",
                    "label": 0
                },
                {
                    "sent": "We can draw samples.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When that covariance matrix and this is what they look like inside and we have the latent force in red, we have the underdamped system.",
                    "label": 0
                },
                {
                    "sent": "Green we have the overdamped system and blue.",
                    "label": 0
                },
                {
                    "sent": "We have the critical dump system.",
                    "label": 0
                },
                {
                    "sent": "This is for example, another.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Al another sample from the model and that is where you can notice here.",
                    "label": 0
                },
                {
                    "sent": "This bump of the latent force between 5:00 and 10:00 is in some sense driving the behavior of the critical dump on the Andover dump outputs, and you see that the critically damped output is following more faithfully.",
                    "label": 0
                },
                {
                    "sent": "The behavior of the of the input force that the overdamped signal here in green, however they tried, they follow the same set of cycles here.",
                    "label": 0
                },
                {
                    "sent": "So when this one is up the other is also up, and that's typical behavior for this over dump and this critical dump system.",
                    "label": 0
                },
                {
                    "sent": "So you see that the drill one that corresponds to the underdamped system is more prone to follow the latent force.",
                    "label": 0
                },
                {
                    "sent": "As you see there.",
                    "label": 0
                },
                {
                    "sent": "This is another sample.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The model.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a further one, so here this is a particular very, very interesting one, because you see that it looks like the underdamped system is going to resonance with the latent Force One with the latent force function.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we took this differential equations and we feed it some data from the same you database motion capture data.",
                    "label": 1
                },
                {
                    "sent": "In particular, we took the motions 1819 and 20 from subject 14.",
                    "label": 1
                },
                {
                    "sent": "Either corresponds to a balancing kind.",
                    "label": 0
                },
                {
                    "sent": "I will show you next and.",
                    "label": 0
                },
                {
                    "sent": "We feel the model with two latent forces.",
                    "label": 0
                },
                {
                    "sent": "So it's going.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show you this simulation in my lab.",
                    "label": 0
                },
                {
                    "sent": "So here you see.",
                    "label": 0
                },
                {
                    "sent": "Subject 49 Motion 18 so it corresponds to a guy there is balancing.",
                    "label": 0
                },
                {
                    "sent": "So first he has his hands up and then he brings his hands down and at the same time he's lifting his right leg.",
                    "label": 0
                },
                {
                    "sent": "As you see there.",
                    "label": 0
                },
                {
                    "sent": "This is a subject 14 emotion 19 which is basically doing the same movement.",
                    "label": 0
                },
                {
                    "sent": "So we assume a differential equation for all the all the outputs in this model, and then we feed it using maximum likelihood with two forces and what we see in this in this panel.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Is a panel of four Swan against Force 2.",
                    "label": 0
                },
                {
                    "sent": "And what you see in this cross is correspond to the posterior of this latent forces for this particular data set.",
                    "label": 0
                },
                {
                    "sent": "So what I wanted to show you is how these models can make some kind of extrapolation.",
                    "label": 0
                },
                {
                    "sent": "So if we follow these latent forces that you see here.",
                    "label": 0
                },
                {
                    "sent": "So we see we see how we can generate some movement for this guy.",
                    "label": 0
                },
                {
                    "sent": "And the guy is able to make some movements that were not in the training data.",
                    "label": 0
                },
                {
                    "sent": "So is this movement which he saw in the training data.",
                    "label": 0
                },
                {
                    "sent": "But moving the left leg back was not in the training data.",
                    "label": 0
                },
                {
                    "sent": "So you see that there is the notion of using this mechanistic.",
                    "label": 0
                },
                {
                    "sent": "Ideas to make extrapolation.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It was just a.",
                    "label": 0
                },
                {
                    "sent": "An example.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to evaluate the performance of the model in terms of the in terms of the mean squared error, we took a smaller data set that corresponds to the subject left arm.",
                    "label": 0
                },
                {
                    "sent": "So we train again using these two latent forces.",
                    "label": 0
                },
                {
                    "sent": "Unfortunate thing we condition only on the observations of the shoulders.",
                    "label": 1
                },
                {
                    "sent": "Orientation of the motion 20 and we make the prediction for the other part of the arm.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here I show the results of regression in terms of the room in square error for the latent force model that uses this idea of the differential equations and a simple regression model that assumes independence in all the outputs.",
                    "label": 1
                },
                {
                    "sent": "So you see that for the radius and risk their performance is similar basically, But then for the other outputs the performance is much better in the in the latent Force model.",
                    "label": 0
                },
                {
                    "sent": "OK this.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Was an example using this.",
                    "label": 0
                },
                {
                    "sent": "Ordinary differential equations in which the input is just the time.",
                    "label": 0
                },
                {
                    "sent": "But we can use this model to allow for several input input dimensions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm going to my TV using this example, so this is an escape of the region of the Swiss Jura and some time ago there was huge mining activity.",
                    "label": 1
                },
                {
                    "sent": "There is a classical problem, enjoy statistics and these methods were spread all over the place.",
                    "label": 0
                },
                {
                    "sent": "So today we have access to concentrations of some of the metals in some particular locations.",
                    "label": 0
                },
                {
                    "sent": "But we don't have access to the concentrations at all allowed sampling points.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is to use the concentrations to which we have access to try to predict the concentrations for those metals for which we don't have access.",
                    "label": 0
                },
                {
                    "sent": "So what we are going to assume is that the way in which these metals originally spread over the place where using the user model using a diffusion process.",
                    "label": 0
                },
                {
                    "sent": "So we can assume that our model initially they were.",
                    "label": 0
                },
                {
                    "sent": "Initially the metals were spread in a particular special configuration and there.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through time.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The metals were just start.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into the fuse all over the place.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for that we use a simplified version of the diffusion equation that you see here.",
                    "label": 1
                },
                {
                    "sent": "That includes the time and also include the input dimensions that in this case will correspond to the special input dimensions and yd correspond to the outputs.",
                    "label": 1
                },
                {
                    "sent": "So if we make again the solution to this differential equation is given in the form of this convolution.",
                    "label": 0
                },
                {
                    "sent": "Where this?",
                    "label": 0
                },
                {
                    "sent": "Caramel here.",
                    "label": 0
                },
                {
                    "sent": "This function here just correspond to this.",
                    "label": 0
                },
                {
                    "sent": "This Greens function that you see here, so analogies that initially we assume that the latent force just give us some information how the metals were spread, and then through time they were just moving through this differential equation.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have always this model using.",
                    "label": 0
                },
                {
                    "sent": "In terms of the the mean absolute error.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we, for this kind of system, so we have access to secondary variable, so secondary information.",
                    "label": 0
                },
                {
                    "sent": "So the purpose is that you use that secondary information to predict the behavior of the primary information.",
                    "label": 0
                },
                {
                    "sent": "So in this case what we want to do is we want to use information about the concentrations of nickel and seeing for example that we have to predict the concentration of cadmium and the same for these other metals.",
                    "label": 0
                },
                {
                    "sent": "So we compare our approach against independent Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "An ordinary cokriging, which is a method used in the in the geostatistics literature, and you see the three out of four of the metals in terms of the mean absolute error, we make very, very very performance.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These were examples of these latent force model idea.",
                    "label": 0
                },
                {
                    "sent": "Now in the second part of the talk I'm going to give you an overview of some sparse approximations that are necessary when we want to work with several outputs and so input points or yeah.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about the latent Force model again, this time in the context of convolution processes.",
                    "label": 1
                },
                {
                    "sent": "So consider a gain of set of functions, so you have the functions any function can be expressed as this convolution.",
                    "label": 1
                },
                {
                    "sent": "So we have this smoothing kernel functions times these.",
                    "label": 1
                },
                {
                    "sent": "Latent functions you can allow the influence of more than one latent function.",
                    "label": 0
                },
                {
                    "sent": "So Q functions and then you can also include an independent process and then this is the form of the of the output.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just a cartoon example.",
                    "label": 0
                },
                {
                    "sent": "You have one latent force and then you can bold that force.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is 1 four's without a smoothing kernel for output one and one is moving kernel for output two and then you generate your model for the outputs and then you can add.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And independent process to each output and then you get a noisy version.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "None of your of your original model.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the outputs, we can workout the covariance for this outputs YD and the covariance is just the sum of the covariance of the convolution operation and then the covariance of the independent process.",
                    "label": 0
                },
                {
                    "sent": "And this covariance is just given this by this expression.",
                    "label": 0
                },
                {
                    "sent": "This double sum and then this double integral and then you have a Green's function for each of the theory to each of the outputs.",
                    "label": 0
                },
                {
                    "sent": "And then you can see that this covariance is just basically the most important term here is this.",
                    "label": 0
                },
                {
                    "sent": "Once you choose the form for the covariance of the latent force an if you can solve analytically this integral, you can find the covariance for your outputs.",
                    "label": 1
                },
                {
                    "sent": "Here we assume that the covariance for the outputs is just corresponds to independent Gaussian processes, so you have.",
                    "label": 0
                },
                {
                    "sent": "One covariance for each of these Q, and in that way you will get rid of one of these sums.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So traditionally what you do is that you find the marginal likelihood and you use the marginal likelihood to fit your parameters.",
                    "label": 0
                },
                {
                    "sent": "So in the case in which we used the disco this convolution covariances, we have this expression here where Y corresponds to.",
                    "label": 0
                },
                {
                    "sent": "If you have a set of input points.",
                    "label": 1
                },
                {
                    "sent": "So you have some training data.",
                    "label": 0
                },
                {
                    "sent": "So This is why corresponds to stacking all the corresponding outputs of the data of all your outputs.",
                    "label": 0
                },
                {
                    "sent": "KFF corresponds to the covariance matrix with blocks given by this covariance expression of the covariance that we saw before this Sigma corresponds to the matrix of knowledge variances.",
                    "label": 1
                },
                {
                    "sent": "In this case, we assume that the independent processes corresponds to noise.",
                    "label": 0
                },
                {
                    "sent": "We have these five.",
                    "label": 0
                },
                {
                    "sent": "There is the set of parameters of the covariance matrix, and you have a set of inputs X one to XN.",
                    "label": 1
                },
                {
                    "sent": "So as I said before, when you want to use this in practice, what you do is you find your parameters maximizing this marginal likelihood, and then you also need that for the for the prediction step.",
                    "label": 0
                },
                {
                    "sent": "So learning from the log likelihood involves the inverse, making the inverse of this covariance with growth with complexity and QBQ where N corresponds to the number of data points.",
                    "label": 0
                },
                {
                    "sent": "Andy corresponds to the number of outputs.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is the expression for the predictive distribution.",
                    "label": 1
                },
                {
                    "sent": "So here you have again to make the inverse of covariance matrix and prediction is ND for them in an N squared.",
                    "label": 0
                },
                {
                    "sent": "The square for the variance.",
                    "label": 1
                },
                {
                    "sent": "So you have this cubic complexity when you have several inputs, several data, and several output.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It can become problematic.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To see the approximations that we make.",
                    "label": 0
                },
                {
                    "sent": "Let's train to break this equation in terms of generative process.",
                    "label": 0
                },
                {
                    "sent": "So what you do is that you have some uncertainty in the latent forces, which is described basically by stochastic process or uses.",
                    "label": 0
                },
                {
                    "sent": "You draw a sample sample from that process and then you solve the integral and then the uncertainty that you have here is propagated through this linear operation to the outputs.",
                    "label": 0
                },
                {
                    "sent": "So that's the way in which you would solve the whole problem.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can take.",
                    "label": 0
                },
                {
                    "sent": "Another path to reduce the computational complexity and is to change this continuous operation by this discretized version of the convolution.",
                    "label": 0
                },
                {
                    "sent": "So instead of working with the continuous function, you just work with a set of points and then you reduce your convolution operation to these discretized version.",
                    "label": 0
                },
                {
                    "sent": "So what we do here is a different thing.",
                    "label": 0
                },
                {
                    "sent": "In order to reduce computational.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Flexity instead of working with the full process or with the discretized version of the process, we're going to work with the mean.",
                    "label": 0
                },
                {
                    "sent": "With the main function of the conditional prior, so you draw some points of your full process, you take some points of your food process and condition on those points that I show here as red dots, you draw a sample using the conditional prior.",
                    "label": 0
                },
                {
                    "sent": "So the sample that you see here is in blue.",
                    "label": 0
                },
                {
                    "sent": "And with that blue you solve the integral operation.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's say the leads to this form for the for the likelihood, so the likelihood of your outputs has this mean function that you see here as this covariance.",
                    "label": 1
                },
                {
                    "sent": "So one might think OK if I condition on the on the latent force, the outputs would be independent.",
                    "label": 0
                },
                {
                    "sent": "So ideally you wouldn't have any sort of uncertainty for this likelihood turn.",
                    "label": 0
                },
                {
                    "sent": "The thing here is that since you have a discrete highest version, so you have this, you have a finite number of points.",
                    "label": 1
                },
                {
                    "sent": "You still have to deal with some uncertainty in your outputs.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our first assumption to reduce computational complexity in this likelihood term is to assume that the cross terms in that likelihood are just zero.",
                    "label": 0
                },
                {
                    "sent": "In other words, we assume that given this finite set of points or inducing points, the outputs are independent.",
                    "label": 0
                },
                {
                    "sent": "So we change this form.",
                    "label": 0
                },
                {
                    "sent": "We have three in this example.",
                    "label": 0
                },
                {
                    "sent": "We just three outputs and we change that for this.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Form of the of the covariance.",
                    "label": 0
                },
                {
                    "sent": "So the intuition is that.",
                    "label": 0
                },
                {
                    "sent": "If your Lenten forms is sufficiently smooth, this conditional mean will approximate.",
                    "label": 0
                },
                {
                    "sent": "Better.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then you integrate out in that likelihood term, you integrate out the latent forces, or the latent functions, and then you get this new marginal likelihood which is given by this log low rank approximation.",
                    "label": 1
                },
                {
                    "sent": "But sorry, plus this block diagonal matrix, which is just has this expression in the main blocks and zero outside the blocks.",
                    "label": 0
                },
                {
                    "sent": "The main blocks of the diagonal.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In other words, what you are doing is approximated your full covariance matrix by a matrix that is equal to the full covariance in the block diagonal terms and a low rank approximation out of the diagonal.",
                    "label": 0
                },
                {
                    "sent": "Look, this is different.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use the discretized version in which you replace the full covariance matrix by this outer product of this design matrix related with the discrete convolution.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here you have the.",
                    "label": 0
                },
                {
                    "sent": "Predictive distribution for the outputs where this matrix T without the Asterix is just a block diagonal terms that we had before.",
                    "label": 1
                },
                {
                    "sent": "And the computational complexity here is.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Associated to inverting, so once you apply the matrix inversion lemma to embarrassing this matrix D that grows as N qv.",
                    "label": 0
                },
                {
                    "sent": "Times sorry, plus the computational complexity associated with a lower rank approximation, which is ND M ^2.",
                    "label": 0
                },
                {
                    "sent": "So you see that if we have.",
                    "label": 0
                },
                {
                    "sent": "If we assume that this M indicates the number of points that you choose of inducing points that you choose, if those numbers that number is equal to the number of data points that you have, then you have here N * N ^2 as NQ, and you match the complexity of using independent processes for each output.",
                    "label": 0
                },
                {
                    "sent": "So the computational complexity for computing the mean grocers and DM for the approximation and DM square for the variance.",
                    "label": 1
                },
                {
                    "sent": "And this has an analogous in the in the univariate case of a single output process, there is called in the literature is known as a partial independent training conditional approximation introduced by Kenyan Arrow Candelon cars, Rasmussen.",
                    "label": 0
                },
                {
                    "sent": "Can we refer to this approximation also at speeds?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Brooks, spitze as the PC approximation.",
                    "label": 0
                },
                {
                    "sent": "So in some applications the Ncube term in the computational complexity in the pits approximation.",
                    "label": 1
                },
                {
                    "sent": "On the aim squared term, this storage is can be a steal.",
                    "label": 1
                },
                {
                    "sent": "Spence, if so we can do an additional assumption and is to assume that the.",
                    "label": 0
                },
                {
                    "sent": "Input points of the outputs are conditionally independent given the latent forces.",
                    "label": 0
                },
                {
                    "sent": "So what I mean like before?",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have for the pits approximation that the outputs as a whole each output where independent, given the latent forces.",
                    "label": 0
                },
                {
                    "sent": "Now we have an additional assumption.",
                    "label": 1
                },
                {
                    "sent": "We assume that we assume that with initial output each point is.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Given the latent force.",
                    "label": 0
                },
                {
                    "sent": "And then we can again we can workout the marginal likelihood integrating out.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The under a prior for this latent forces and the marginal likelihood has this form.",
                    "label": 0
                },
                {
                    "sent": "Again, we have the low rank approximation and we have replaced the block diagonal matrix just for a diagonal matrix.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the figure that we had before for PC in fizi.",
                    "label": 0
                },
                {
                    "sent": "We just replace this main blocks.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Buy some approximation QFF.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimation.",
                    "label": 0
                },
                {
                    "sent": "Is just again we keep the main so we keep the diagonal terms that correspond to the full Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Sorry photo full covariance matrix of the full process.",
                    "label": 0
                },
                {
                    "sent": "An outside diagonal terms.",
                    "label": 0
                },
                {
                    "sent": "We use the low rank approximation to compute the covariance.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now the computational complexity is NDM Square and storage is NDM, and again we have the same computation computational complexity to compute the mean and the variance.",
                    "label": 1
                },
                {
                    "sent": "And this has an analogous again in the single output case, which is called a full independent training conditional introduced by Snelson Anger Ghahramani.",
                    "label": 0
                },
                {
                    "sent": "And we call it the same.",
                    "label": 0
                },
                {
                    "sent": "Here the fizzi approximation.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can have an additional approximation in which you assume that your marginal likelihood, sorry that your likelihood term just has zero covariance.",
                    "label": 0
                },
                {
                    "sent": "So assume that given this set of finite points, you can totally determine determine the outputs, and then when you workout, the marginal likelihood is just this low rank approximation that you see here.",
                    "label": 1
                },
                {
                    "sent": "This is a computational complexity, is the same as fizzie and also it has an analogous form to the two.",
                    "label": 0
                },
                {
                    "sent": "Another approximation for the single output case, which is called the DTC deterministic training conditional approximation.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now I'm going to show some examples in which I will compare the performance of these approximations against the full Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So for all, this payments are going to consider that the covariance of the latent forces latent forces follow this for exponential form with this matrix diagonal matrix L, which allows to have different length scales for each dimension, and then each of the greens functions or bear the smoothing kernel functions has has this form that you see here.",
                    "label": 1
                },
                {
                    "sent": "Also an exponential.",
                    "label": 1
                },
                {
                    "sent": "So then you can analytically workout the covariance for the output.",
                    "label": 0
                },
                {
                    "sent": "So in the.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This example with artificial data.",
                    "label": 1
                },
                {
                    "sent": "What you do is that you draw.",
                    "label": 0
                },
                {
                    "sent": "You have four.",
                    "label": 0
                },
                {
                    "sent": "This in this setup you have four outputs and you use your your full Gaussian process and you draw a sample from that Gaussian process of these four outputs and what I'm showing here is the prediction that you make only over the output #4 the prediction using the full Gaussian process, again optimizing the marginal likelihood for the full Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "The prediction that you obtain using the DTC approximation.",
                    "label": 1
                },
                {
                    "sent": "The feeds approximation the pizza approximation.",
                    "label": 0
                },
                {
                    "sent": "For this 4th or 4th output, this crosses here.",
                    "label": 0
                },
                {
                    "sent": "Represents the positions of inducing points.",
                    "label": 1
                },
                {
                    "sent": "Once you optimize the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "So you see that the performance of the fits in the pizza approximation is quite.",
                    "label": 0
                },
                {
                    "sent": "It's quite similar to the full GP.",
                    "label": 0
                },
                {
                    "sent": "And the positions of the inducing points are great spread over the input interval.",
                    "label": 0
                },
                {
                    "sent": "See that for the DC approximation there is a good feeling of the main function, but the variance is not so well fitted and the reason behind that is the fact that you are ignoring this additional uncertainty in the likelihood term.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here I'm just quantifying the performance in terms of the standardized mean squared error.",
                    "label": 0
                },
                {
                    "sent": "For the full Gaussian process on for the three approximations and you see that the performance is basically the same in terms of the mean squared error.",
                    "label": 1
                },
                {
                    "sent": "But then when you look at the mean standardized log loss that turns things into account, the uncertainty you see that debits debits approximation is the pits approximation.",
                    "label": 0
                },
                {
                    "sent": "More negative values in this in this measure indicates better models.",
                    "label": 1
                },
                {
                    "sent": "So what we see here is that PC obtains better results than, for example.",
                    "label": 0
                },
                {
                    "sent": "DTC and fits is not so bad, I mean, but the best of the best of them is Pizzi.",
                    "label": 0
                },
                {
                    "sent": "And here I have some training times per iteration just to give you an idea of the time it takes to train the full GP per iteration.",
                    "label": 0
                },
                {
                    "sent": "One point, 97 seconds.",
                    "label": 0
                },
                {
                    "sent": "For this example, the DTC just take .2 seconds .45 for 15.59 for PC.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think I'm just going to keep this.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to rain.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You see the jury.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example that we saw.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "So here I'm going to make the prediction over commune using nickel and sink as we saw before, and I'm going to compare the mean absolute error using these approximations on the full Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So here D stands for DTC F, stands for FIT, CP stands for pit CFGP for full Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "CK for the Kriging method and the independent Gaussian process and these numbers that you see here are the number of inducing points that you use.",
                    "label": 0
                },
                {
                    "sent": "To make the approximation.",
                    "label": 0
                },
                {
                    "sent": "So you see that there is a lot of uncertainty for the DTC.",
                    "label": 0
                },
                {
                    "sent": "Well, you see that fits Ian Pizzi actually, let's concentrate PC for a moment.",
                    "label": 0
                },
                {
                    "sent": "You see that pizza for 5450 inducing points is better than doing independent Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "And then every time that you put more inducing points, you reach the performance of the full Gaussian process and with two 200 points you may bear the Coke region system.",
                    "label": 0
                },
                {
                    "sent": "You see here and compare PC to fit.",
                    "label": 0
                },
                {
                    "sent": "See you see that for the same amount of inducing points, the performance of PC's is better.",
                    "label": 0
                },
                {
                    "sent": "Also, you can see the error bars are decent or normal for the both approximations compared to DTC.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We have presented a conclusions we have presented a hybrid approach in which we combine this simple mechanistic models with Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "We have seen how convolution process.",
                    "label": 0
                },
                {
                    "sent": "I wait till all men data trading models.",
                    "label": 0
                },
                {
                    "sent": "To include characteristic characteristics of physical systems.",
                    "label": 0
                },
                {
                    "sent": "So we can see how we can construct various structure covariance functions for the outputs under this.",
                    "label": 0
                },
                {
                    "sent": "Differential equations models and we have presented a series of series of sparse approximations to deal with this, with the computational complexity in this combo Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Mark knowledge munts to Google for Google researcher Warranto EP is RC for this grant.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do you have a technical report online that describes all these methods?",
                    "label": 0
                },
                {
                    "sent": "Is all the software publicly available?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "Yes, you have to.",
                    "label": 0
                },
                {
                    "sent": "I was unclear with the metal example.",
                    "label": 0
                },
                {
                    "sent": "If you had, you wouldn't believe that they were basically diffusing at similar rates.",
                    "label": 0
                },
                {
                    "sent": "And what if you have two different diffusion processes that are operating at different time scales?",
                    "label": 0
                },
                {
                    "sent": "Well, I mean, this is a very simplified.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Version of the diffusion equation because if you see we are not.",
                    "label": 0
                },
                {
                    "sent": "I mean the way in which we introduced the latent Force.",
                    "label": 0
                },
                {
                    "sent": "You see here, here in the in the form of the diffusion equation you will have an additional input input force, as in the case of the of the ordinary differential equation.",
                    "label": 0
                },
                {
                    "sent": "So here the input force is basically the initial condition, so the uncertainties in the initial condition and this and this couple D that represents the.",
                    "label": 0
                },
                {
                    "sent": "Diffusion coefficient in that in that input dimension is basically a parameter associated here that we learn hyperparameter that we learn.",
                    "label": 0
                },
                {
                    "sent": "Is different for each.",
                    "label": 0
                },
                {
                    "sent": "Yeah it is associated with the.",
                    "label": 0
                },
                {
                    "sent": "No, yeah, in Europe.",
                    "label": 0
                },
                {
                    "sent": "Calling office message so it's Ian.",
                    "label": 0
                },
                {
                    "sent": "As I remember it, yeah he comes from Poland independent training on decimal or partial involuntary manslaughter.",
                    "label": 0
                },
                {
                    "sent": "So how do you make your predictions is you use this conditional framework to make predictions?",
                    "label": 0
                },
                {
                    "sent": "Or do you make it the full governance usually use the new equations that you have your party for your predictive distribution.",
                    "label": 0
                },
                {
                    "sent": "So you can use this set of equations that include these terms associated.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're you're low rank approximations.",
                    "label": 0
                },
                {
                    "sent": "But you can also try to do that.",
                    "label": 0
                },
                {
                    "sent": "I mean if it's not too expensive you can train using your sparse approximations and then maybe you can do as a way to.",
                    "label": 0
                },
                {
                    "sent": "No, it's not.",
                    "label": 0
                },
                {
                    "sent": "If you if you did like that that you train with this person for tuition and make the principal GP, that would give you white patches on this, OK?",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, this is a little bit like terminology so OK. Maybe custom?",
                    "label": 0
                },
                {
                    "sent": "Just bring some shifting integrate over the space.",
                    "label": 0
                },
                {
                    "sent": "So why couldn't?",
                    "label": 0
                },
                {
                    "sent": "The space is very you really have two points already.",
                    "label": 0
                },
                {
                    "sent": "Have someone the density in different areas are very different.",
                    "label": 0
                },
                {
                    "sent": "You put can you take into account this in some way or something?",
                    "label": 0
                },
                {
                    "sent": "You know situation wherever you wanna say equals are standing on some low dimensional structure or maybe just a few of them.",
                    "label": 0
                },
                {
                    "sent": "While the situation well, I mean you can.",
                    "label": 0
                },
                {
                    "sent": "You can amend this framework in several ways and we have done it so.",
                    "label": 0
                },
                {
                    "sent": "There are solar extensions that you can do.",
                    "label": 0
                },
                {
                    "sent": "For example, we can use this variational approximation in which we penalize regions of the input space that away from the data.",
                    "label": 0
                },
                {
                    "sent": "So this inducing points start moving, but then with this variational approximation you make that those points are around your data, but you can also change a bit the framework.",
                    "label": 0
                },
                {
                    "sent": "So for example, you could assume in this convolution idea.",
                    "label": 0
                },
                {
                    "sent": "I think people have done before.",
                    "label": 0
                },
                {
                    "sent": "In the past.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "So instead of working with the same, I mean instead of this, say having the same dimension that X, you can make a transformation.",
                    "label": 0
                },
                {
                    "sent": "Let's say I don't know some kind of some matrix and then that matrix would appear in your covariance and you will have to make.",
                    "label": 0
                }
            ]
        }
    }
}