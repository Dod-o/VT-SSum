{
    "id": "3gh4umc6vwth7n74cf3uwjs7juq4xcaq",
    "title": "SchemEX -- Web-Scale Indexed Schema Extraction of Linked Open Data",
    "info": {
        "author": [
            "Ansgar Scherp, University of Koblenz-Landau"
        ],
        "published": "Nov. 25, 2011",
        "recorded": "October 2011",
        "category": [
            "Top->Computer Science->Information Extraction",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2011_scherp_data/",
    "segmentation": [
        [
            "OK, so welcome everybody to our presentation on schematics creating Jello.",
            "The Yellow Pages for the linked open data cloud.",
            "So my name is Oscar from the Institute of Websites and Copeland's here and will lead you to how we will create the next look up for the link."
        ],
        [
            "Data we start with this scenario which we have already heard of.",
            "So let's think about people.",
            "So we all people and we think about other people who are probably not in this room.",
            "So we think about politicians and an actress and give me names.",
            "So who are politicians who are actors at the same time?",
            "Already we have heard on you already.",
            "There was example anything else.",
            "Reagan great another one.",
            "OK, so these are exactly those which we also picked out and the question now is OK. Who else?",
            "So there will be a lot of other politicians who would also have also been maybe work this actor or somewhere or later to let Matt also be from from bond or some other city which we don't know.",
            "I would like to know OK where Windows Live, where do they come from home visiting Rowan?"
        ],
        [
            "And so on, and so we would like to execute those queries on the little data cloud.",
            "But the problem is at the moment we do not have such a query interface.",
            "We do not have a single Federated interface which allows us to execute queries here on the left hand side, politicians and actors on the data on the linked data cloud show on the."
        ],
        [
            "On the right hand side and the principle well we don't know which which of the data sources which are which."
        ],
        [
            "We should be asked and so principle solution to find source relevant data sources is to have some kind of index.",
            "So we would like to have a suitable index structure for looking up sources.",
            "So the query should somehow communicate with an index and the index should tell me go to this data source and go to that data source to find politicians and actors."
        ],
        [
            "And this could be done very easily and we have seen quite close approaches in the past, so the native approach would be to 1st download the entire lot.",
            "Cloud takes her 31 billion triples which we have put them on to put them onto a local hard disk and use a large really really large triple store to store and then to process all the data and having that.",
            "Then process and extracting the schema we might provide to look up to say OK for this kind of information.",
            "You ask this data sources, and when we look at that, of course you need for this a really, really big machinery.",
            "That's quite obvious and but there's also another very important drawback is here or very late and processing the data.",
            "So first of all, you have to download and then put everything in the entire lot cloud into your local store and then you can actually only start processing it and providing this schema.",
            "And in total of course Link Cloud is growing exponentially.",
            "You will have a lot of high effort to scale.",
            "Your infrastructure to compensates increase."
        ],
        [
            "So lot cloud.",
            "So the question here is can we do smarter?",
            "When somebody on the stage Oscar came into smarter, what would be the obvious answer?",
            "No."
        ],
        [
            "Yes, weekend of course, and this is what we're doing."
        ],
        [
            "Was whiskey mix so whiskey mix we throw away is storing all the data locali approach and replace it by a stream based schema extraction which we do while crawling the data.",
            "So how does it work?",
            "We start with or assume we have from End Quote stream so we have some kind of linked open data crawler.",
            "Might also have an RDF number.",
            "Triple Store doesn't matter, we just assume we have some kind of triple stream which we parse using.",
            "For example Alex Parcel then putting it into some schematic structure.",
            "So some kind of clever algorithm and tool.",
            "Which extracts our schema information and what we do is.",
            "Here we use a very very efficient fee.",
            "For instance cache which creates actually are helps us in creating our index.",
            "And finally we can then stores the schema.",
            "For example in Triple Store but also well any kind of other infrastructure which serves our needs for having such a look up could be also for example RDBMS."
        ],
        [
            "So let's have a look at the tool at the two key components of our infrastructure.",
            "Let's start with."
        ],
        [
            "Kiss first, how does it work?",
            "So I said we observe quadruple stream from the linked data Spider.",
            "So we have here some kind of stream and we look at a specific window size of the stream only and we do that by using data structure as over users data structuring Q backed up by a hashmap.",
            "This allows us to organize the incoming triples and into groups and arranging them along the same subject areas, so each.",
            "Triple which we get into our stream is grouped or it's looked at the subject and we create groups according to same subjects and we fill our cash until it's full and when it's full we dismissed the oldest one so it works like a fearful and the nice thing about this is this very efficient, so it has all operations work in 01."
        ],
        [
            "And this cash is used to actually build the schema and our index, and this looks like follow.",
            "So the bottom you see the different data sources link data sources.",
            "We have like 12X data sources and 1st we do create or extract all RDF classes or classes which we find pointers C1 to CK and 2nd we build so-called type clusters.",
            "So we create type clusters TC which can be considered as just as sets.",
            "Of RDF classes, and these so S as a set of RDS, a type class is a set of RDF classes.",
            "We can say, for example, TC One consists of C1 and C2 T C2 type cluster two consists of C2 and C3 and finally what we do is to answer those queries as we have stated in the beginning.",
            "We create so called equivalence classes which you can consider as connecting two type clusters by a set of properties.",
            "So basically what it does for example or equivalence Class 1.",
            "Connect Skype plus the one was type cluster 2 using property one and so on.",
            "This case it's one property could be, but it said it's a set.",
            "Could be any kind of any number of properties.",
            "Finally we do this.",
            "Three layers is the schema and finally then we connect this schema information with the data sources.",
            "So we connect the equivalence classes and by this also the type and type clusters and RDF classes with the data sources with.",
            "This allows us later to look up in which data source to go.",
            "When we are interested in specific RDF classes, type clusters.",
            "Equivalence classes."
        ],
        [
            "Are we applied this approach on different datasets?",
            "One is that what we call the timber datasets?",
            "It's horrible small data set of 11 million triples crawled starting with symbols full profile using the Rd Spider Frontiers Heart and at that point thanks to Andreas, who has actually provided us this, this data set and what's interesting to notice, it's a link data spiders Ald Spider can't provide stream of triples at a speed of 2K per second, so we can have 2000.",
            "Turrets per second.",
            "That's what we have to expect.",
            "And when we looked into the status that we apply different cache sizes, we just experimented getting too well getting some feeling how this houses approach works.",
            "So we applied with we work with the cache size of 100 instances, one K10 K 50K100K and so on.",
            "We compared our scheme X with the reference chemo gold standard.",
            "Bike running different index queries on our on our expected schema and these index creases we want.",
            "We conducted tears.",
            "We ran crease on all types which we found all type classes on all equivalence classes and computed precision record.",
            "So details of that are in the paper or if you like later some more and we by doing that we figured out that the go to ratio of precision recall starts at about 50K.",
            "Cash price of 50K."
        ],
        [
            "And this size is what we took then.",
            "As for processing the billion temperature range data and we started with the first Berlin and the second billion just separately to sail to see OK, how does this the approach work?",
            "How does it perform?",
            "And then finally on the full data set were so we were streaming 2.17 billion triples roll approach."
        ],
        [
            "And while doing that, we were counting the instances for it's about 450 million instances which we found or which which are extracted from.",
            "From these from this data set provided from about 24 different 24 million different data sources, different.",
            "Uh, context information and this this this.",
            "This triple contained apart roughly 4 not 50K type clusters, so quite a lot and two point about 2.1 million million equivalence classes.",
            "So this these are just some numbers showing OK, what is the amount of data which we which we created or extracted from the data set?",
            "What is more interesting is so we have for representing our schema index.",
            "We use a lightweight ontology.",
            "And we can, of course, when we have represented the schema using our lightweight ontology, we can count the triples and we see that this schema only needs about 55 million triples at the moment, so it easily fits in any kind of any.",
            "Any triple store could just.",
            "For example, you sesame and put it into their and compared to the full data set with the triples.",
            "The two .17 billion triples.",
            "We have a compression ratio of about 2.5%, so it's really really an efficient index.",
            "Plant processing the data took about 15 hours and the most interesting aspect here on the on the performance measures is that we can do and on average with, measured by ten runs, 40K triples per second, so we can process in real time 40,000 triples per second, which makes us 20 * 20 times faster is the link data Spider provides us today."
        ],
        [
            "This already concludes my talk, So what have we presented to you?",
            "Is scheme access a stream based approach to schema extraction on the linked open Data cloud.",
            "It's scalable to an arbitrary amount of linked data because we do not store any of the triplets which we analyze on our heart does not even temporarily, only the schema.",
            "Everything else is thrown away and it's applicable on commodity hardware.",
            "We used really a very, very simple machine.",
            "Forgive it, GB of RAM stand with the standard single CPU and can tell you that with the cache size of 100K instances we only need one.",
            "Gigabyte of ram.",
            "So it could be even lower, so it's really, really easy to run on local machine.",
            "So what we then have here with this key Max is look up index for finding relevant data sources and link data.",
            "Cloud and supporting Federated queries on this cloud in the future.",
            "Thank you.",
            "So my question was, what are the kinds of queries that are supported that you can look up other queries, yes."
        ],
        [
            "Let's have a look into this and and which ones are the most useful ones.",
            "Using all principle can have with this schema.",
            "Here you can have three kinds, three types of queries, so can just look for specific for instances of a specific type.",
            "So give me all actors, give me all persons, then could use these type clusters and say I want to know all instances which are actors and politicians at the same time.",
            "There's also a query type also written in the paper.",
            "As a super tight clusters where you look also into a subsets of the type cluster.",
            "So you might want to have all politicians and actors, but if you don't find any, you might also be satisfied with having instances which are only actors as well and then the equivalence classes you look into well give me give me all instances of our politicians actors who live in a certain country or who know specific people or so and serving.",
            "Well, having different this different levels of the schema serve different information needs basically.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so welcome everybody to our presentation on schematics creating Jello.",
                    "label": 0
                },
                {
                    "sent": "The Yellow Pages for the linked open data cloud.",
                    "label": 1
                },
                {
                    "sent": "So my name is Oscar from the Institute of Websites and Copeland's here and will lead you to how we will create the next look up for the link.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data we start with this scenario which we have already heard of.",
                    "label": 0
                },
                {
                    "sent": "So let's think about people.",
                    "label": 0
                },
                {
                    "sent": "So we all people and we think about other people who are probably not in this room.",
                    "label": 0
                },
                {
                    "sent": "So we think about politicians and an actress and give me names.",
                    "label": 1
                },
                {
                    "sent": "So who are politicians who are actors at the same time?",
                    "label": 1
                },
                {
                    "sent": "Already we have heard on you already.",
                    "label": 0
                },
                {
                    "sent": "There was example anything else.",
                    "label": 0
                },
                {
                    "sent": "Reagan great another one.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are exactly those which we also picked out and the question now is OK. Who else?",
                    "label": 0
                },
                {
                    "sent": "So there will be a lot of other politicians who would also have also been maybe work this actor or somewhere or later to let Matt also be from from bond or some other city which we don't know.",
                    "label": 1
                },
                {
                    "sent": "I would like to know OK where Windows Live, where do they come from home visiting Rowan?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so on, and so we would like to execute those queries on the little data cloud.",
                    "label": 1
                },
                {
                    "sent": "But the problem is at the moment we do not have such a query interface.",
                    "label": 0
                },
                {
                    "sent": "We do not have a single Federated interface which allows us to execute queries here on the left hand side, politicians and actors on the data on the linked data cloud show on the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the right hand side and the principle well we don't know which which of the data sources which are which.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We should be asked and so principle solution to find source relevant data sources is to have some kind of index.",
                    "label": 0
                },
                {
                    "sent": "So we would like to have a suitable index structure for looking up sources.",
                    "label": 1
                },
                {
                    "sent": "So the query should somehow communicate with an index and the index should tell me go to this data source and go to that data source to find politicians and actors.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this could be done very easily and we have seen quite close approaches in the past, so the native approach would be to 1st download the entire lot.",
                    "label": 0
                },
                {
                    "sent": "Cloud takes her 31 billion triples which we have put them on to put them onto a local hard disk and use a large really really large triple store to store and then to process all the data and having that.",
                    "label": 1
                },
                {
                    "sent": "Then process and extracting the schema we might provide to look up to say OK for this kind of information.",
                    "label": 1
                },
                {
                    "sent": "You ask this data sources, and when we look at that, of course you need for this a really, really big machinery.",
                    "label": 1
                },
                {
                    "sent": "That's quite obvious and but there's also another very important drawback is here or very late and processing the data.",
                    "label": 0
                },
                {
                    "sent": "So first of all, you have to download and then put everything in the entire lot cloud into your local store and then you can actually only start processing it and providing this schema.",
                    "label": 0
                },
                {
                    "sent": "And in total of course Link Cloud is growing exponentially.",
                    "label": 1
                },
                {
                    "sent": "You will have a lot of high effort to scale.",
                    "label": 0
                },
                {
                    "sent": "Your infrastructure to compensates increase.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So lot cloud.",
                    "label": 0
                },
                {
                    "sent": "So the question here is can we do smarter?",
                    "label": 0
                },
                {
                    "sent": "When somebody on the stage Oscar came into smarter, what would be the obvious answer?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, weekend of course, and this is what we're doing.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Was whiskey mix so whiskey mix we throw away is storing all the data locali approach and replace it by a stream based schema extraction which we do while crawling the data.",
                    "label": 1
                },
                {
                    "sent": "So how does it work?",
                    "label": 0
                },
                {
                    "sent": "We start with or assume we have from End Quote stream so we have some kind of linked open data crawler.",
                    "label": 0
                },
                {
                    "sent": "Might also have an RDF number.",
                    "label": 0
                },
                {
                    "sent": "Triple Store doesn't matter, we just assume we have some kind of triple stream which we parse using.",
                    "label": 0
                },
                {
                    "sent": "For example Alex Parcel then putting it into some schematic structure.",
                    "label": 0
                },
                {
                    "sent": "So some kind of clever algorithm and tool.",
                    "label": 0
                },
                {
                    "sent": "Which extracts our schema information and what we do is.",
                    "label": 0
                },
                {
                    "sent": "Here we use a very very efficient fee.",
                    "label": 1
                },
                {
                    "sent": "For instance cache which creates actually are helps us in creating our index.",
                    "label": 0
                },
                {
                    "sent": "And finally we can then stores the schema.",
                    "label": 0
                },
                {
                    "sent": "For example in Triple Store but also well any kind of other infrastructure which serves our needs for having such a look up could be also for example RDBMS.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's have a look at the tool at the two key components of our infrastructure.",
                    "label": 0
                },
                {
                    "sent": "Let's start with.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kiss first, how does it work?",
                    "label": 0
                },
                {
                    "sent": "So I said we observe quadruple stream from the linked data Spider.",
                    "label": 1
                },
                {
                    "sent": "So we have here some kind of stream and we look at a specific window size of the stream only and we do that by using data structure as over users data structuring Q backed up by a hashmap.",
                    "label": 0
                },
                {
                    "sent": "This allows us to organize the incoming triples and into groups and arranging them along the same subject areas, so each.",
                    "label": 0
                },
                {
                    "sent": "Triple which we get into our stream is grouped or it's looked at the subject and we create groups according to same subjects and we fill our cash until it's full and when it's full we dismissed the oldest one so it works like a fearful and the nice thing about this is this very efficient, so it has all operations work in 01.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this cash is used to actually build the schema and our index, and this looks like follow.",
                    "label": 0
                },
                {
                    "sent": "So the bottom you see the different data sources link data sources.",
                    "label": 0
                },
                {
                    "sent": "We have like 12X data sources and 1st we do create or extract all RDF classes or classes which we find pointers C1 to CK and 2nd we build so-called type clusters.",
                    "label": 0
                },
                {
                    "sent": "So we create type clusters TC which can be considered as just as sets.",
                    "label": 0
                },
                {
                    "sent": "Of RDF classes, and these so S as a set of RDS, a type class is a set of RDF classes.",
                    "label": 0
                },
                {
                    "sent": "We can say, for example, TC One consists of C1 and C2 T C2 type cluster two consists of C2 and C3 and finally what we do is to answer those queries as we have stated in the beginning.",
                    "label": 0
                },
                {
                    "sent": "We create so called equivalence classes which you can consider as connecting two type clusters by a set of properties.",
                    "label": 0
                },
                {
                    "sent": "So basically what it does for example or equivalence Class 1.",
                    "label": 0
                },
                {
                    "sent": "Connect Skype plus the one was type cluster 2 using property one and so on.",
                    "label": 0
                },
                {
                    "sent": "This case it's one property could be, but it said it's a set.",
                    "label": 0
                },
                {
                    "sent": "Could be any kind of any number of properties.",
                    "label": 0
                },
                {
                    "sent": "Finally we do this.",
                    "label": 0
                },
                {
                    "sent": "Three layers is the schema and finally then we connect this schema information with the data sources.",
                    "label": 0
                },
                {
                    "sent": "So we connect the equivalence classes and by this also the type and type clusters and RDF classes with the data sources with.",
                    "label": 1
                },
                {
                    "sent": "This allows us later to look up in which data source to go.",
                    "label": 0
                },
                {
                    "sent": "When we are interested in specific RDF classes, type clusters.",
                    "label": 0
                },
                {
                    "sent": "Equivalence classes.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are we applied this approach on different datasets?",
                    "label": 0
                },
                {
                    "sent": "One is that what we call the timber datasets?",
                    "label": 0
                },
                {
                    "sent": "It's horrible small data set of 11 million triples crawled starting with symbols full profile using the Rd Spider Frontiers Heart and at that point thanks to Andreas, who has actually provided us this, this data set and what's interesting to notice, it's a link data spiders Ald Spider can't provide stream of triples at a speed of 2K per second, so we can have 2000.",
                    "label": 0
                },
                {
                    "sent": "Turrets per second.",
                    "label": 0
                },
                {
                    "sent": "That's what we have to expect.",
                    "label": 0
                },
                {
                    "sent": "And when we looked into the status that we apply different cache sizes, we just experimented getting too well getting some feeling how this houses approach works.",
                    "label": 1
                },
                {
                    "sent": "So we applied with we work with the cache size of 100 instances, one K10 K 50K100K and so on.",
                    "label": 0
                },
                {
                    "sent": "We compared our scheme X with the reference chemo gold standard.",
                    "label": 0
                },
                {
                    "sent": "Bike running different index queries on our on our expected schema and these index creases we want.",
                    "label": 1
                },
                {
                    "sent": "We conducted tears.",
                    "label": 1
                },
                {
                    "sent": "We ran crease on all types which we found all type classes on all equivalence classes and computed precision record.",
                    "label": 0
                },
                {
                    "sent": "So details of that are in the paper or if you like later some more and we by doing that we figured out that the go to ratio of precision recall starts at about 50K.",
                    "label": 0
                },
                {
                    "sent": "Cash price of 50K.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this size is what we took then.",
                    "label": 0
                },
                {
                    "sent": "As for processing the billion temperature range data and we started with the first Berlin and the second billion just separately to sail to see OK, how does this the approach work?",
                    "label": 0
                },
                {
                    "sent": "How does it perform?",
                    "label": 0
                },
                {
                    "sent": "And then finally on the full data set were so we were streaming 2.17 billion triples roll approach.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And while doing that, we were counting the instances for it's about 450 million instances which we found or which which are extracted from.",
                    "label": 0
                },
                {
                    "sent": "From these from this data set provided from about 24 different 24 million different data sources, different.",
                    "label": 0
                },
                {
                    "sent": "Uh, context information and this this this.",
                    "label": 0
                },
                {
                    "sent": "This triple contained apart roughly 4 not 50K type clusters, so quite a lot and two point about 2.1 million million equivalence classes.",
                    "label": 0
                },
                {
                    "sent": "So this these are just some numbers showing OK, what is the amount of data which we which we created or extracted from the data set?",
                    "label": 0
                },
                {
                    "sent": "What is more interesting is so we have for representing our schema index.",
                    "label": 0
                },
                {
                    "sent": "We use a lightweight ontology.",
                    "label": 0
                },
                {
                    "sent": "And we can, of course, when we have represented the schema using our lightweight ontology, we can count the triples and we see that this schema only needs about 55 million triples at the moment, so it easily fits in any kind of any.",
                    "label": 0
                },
                {
                    "sent": "Any triple store could just.",
                    "label": 0
                },
                {
                    "sent": "For example, you sesame and put it into their and compared to the full data set with the triples.",
                    "label": 0
                },
                {
                    "sent": "The two .17 billion triples.",
                    "label": 0
                },
                {
                    "sent": "We have a compression ratio of about 2.5%, so it's really really an efficient index.",
                    "label": 0
                },
                {
                    "sent": "Plant processing the data took about 15 hours and the most interesting aspect here on the on the performance measures is that we can do and on average with, measured by ten runs, 40K triples per second, so we can process in real time 40,000 triples per second, which makes us 20 * 20 times faster is the link data Spider provides us today.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This already concludes my talk, So what have we presented to you?",
                    "label": 0
                },
                {
                    "sent": "Is scheme access a stream based approach to schema extraction on the linked open Data cloud.",
                    "label": 1
                },
                {
                    "sent": "It's scalable to an arbitrary amount of linked data because we do not store any of the triplets which we analyze on our heart does not even temporarily, only the schema.",
                    "label": 0
                },
                {
                    "sent": "Everything else is thrown away and it's applicable on commodity hardware.",
                    "label": 0
                },
                {
                    "sent": "We used really a very, very simple machine.",
                    "label": 0
                },
                {
                    "sent": "Forgive it, GB of RAM stand with the standard single CPU and can tell you that with the cache size of 100K instances we only need one.",
                    "label": 0
                },
                {
                    "sent": "Gigabyte of ram.",
                    "label": 1
                },
                {
                    "sent": "So it could be even lower, so it's really, really easy to run on local machine.",
                    "label": 0
                },
                {
                    "sent": "So what we then have here with this key Max is look up index for finding relevant data sources and link data.",
                    "label": 0
                },
                {
                    "sent": "Cloud and supporting Federated queries on this cloud in the future.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So my question was, what are the kinds of queries that are supported that you can look up other queries, yes.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's have a look into this and and which ones are the most useful ones.",
                    "label": 0
                },
                {
                    "sent": "Using all principle can have with this schema.",
                    "label": 0
                },
                {
                    "sent": "Here you can have three kinds, three types of queries, so can just look for specific for instances of a specific type.",
                    "label": 0
                },
                {
                    "sent": "So give me all actors, give me all persons, then could use these type clusters and say I want to know all instances which are actors and politicians at the same time.",
                    "label": 0
                },
                {
                    "sent": "There's also a query type also written in the paper.",
                    "label": 0
                },
                {
                    "sent": "As a super tight clusters where you look also into a subsets of the type cluster.",
                    "label": 0
                },
                {
                    "sent": "So you might want to have all politicians and actors, but if you don't find any, you might also be satisfied with having instances which are only actors as well and then the equivalence classes you look into well give me give me all instances of our politicians actors who live in a certain country or who know specific people or so and serving.",
                    "label": 0
                },
                {
                    "sent": "Well, having different this different levels of the schema serve different information needs basically.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}