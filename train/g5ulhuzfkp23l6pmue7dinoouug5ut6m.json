{
    "id": "g5ulhuzfkp23l6pmue7dinoouug5ut6m",
    "title": "Boosting Performance of Web Search Engines Using Query Logs",
    "info": {
        "author": [
            "Fabrizio Silvestri, Facebook"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "May 2006",
        "category": [
            "Top->Computer Science->Information Retrieval",
            "Top->Computer Science->Web Mining"
        ]
    },
    "url": "http://videolectures.net/fws06_silvestri_bpwse/",
    "segmentation": [
        [
            "With my information severity, I come from the CNN Pizza and I'm here to show some works, some ongoing, some passwords and some ongoing worse, which we are carrying out.",
            "Actually in this moment in instance in our lab in pizza.",
            "So the talk is about boosting performance of web search engines, algorithms using query log information.",
            "So what we are going to talk about?"
        ],
        [
            "Is about using information coming from catalogs in order to boost enhance the performance of.",
            "Information retrieval algorithms or for instance, other operations of search engines.",
            "So we all know that query logs are valuable valuable source of information for multiple purposes, not only caching, for instance, like it is being traditionally used, but also for optimize other web search engines operations and also for carrying out marketing strategies we just saw the talk by Ricardo that show a lot of possible users uses in this kind of direction so.",
            "We'll know what exactly log."
        ],
        [
            "Now we're in our works.",
            "We consider query log as a query topic very casted page of results and the user ID which issued the request and also the timestamp.",
            "We didn't consider the click data, they click through data because simply because we don't, we didn't have them.",
            "So in our in our experiments and in our in our study.",
            "We didn't consider that, but OK, we can even take advantage in using an obviously."
        ],
        [
            "So these are the logs we used and the logs, the characteristics of the locks we use, then the characteristics of the logs we using in the experiment.",
            "These are three query logs.",
            "The excite query log, which is quite old, but it's also popular in the field of study.",
            "It gains from 99 seven and it has been used quite a lot in query log studies.",
            "We have a query out from the typical in Italian, so changing the tickets are changing which came from April 2002 and please compose by almost almost 3.2 million queries where one point 5.5 million queries where distinct in Destin that query log.",
            "We also have the Alta Vista catalog, which came from summer of 2001 and it is composed by 7.1 millions of queries, OK?",
            "And two point 7 two point 6.",
            "OK where distinct?",
            "So."
        ],
        [
            "You will know this kind of distribution of queries and within the query log OK if you rank the query by their popularity the queries by their popularity you will get this kind of power law plot.",
            "I mean the first query is the most popular and of course a lot of time whereas the last query which is the less popular of course just once or twice and the majority of since disease are log log plot.",
            "The majority of the queries appears just once or twice.",
            "Why 20% or even less of the queries appears a lot of time.",
            "At this."
        ],
        [
            "Is another blood, which is quite strange and this is not the percentage of request of the pages following the first.",
            "I mean on the X axis that you can you can see the page idea of the requested pages, whereas in on the Y axis you can.",
            "You can see the probability of requesting a page I TH or page I given that the previous page has been requested and as you can see if a user submit sorry.",
            "Go through the first page and stop them.",
            "Then OK, that's it.",
            "But if a user go through the second page then with high probability more than 50% will go through the other pages of results from the result set, not pages result set.",
            "Yeah, result.",
            "So."
        ],
        [
            "This is just to introduce the content of query logs from our point of view.",
            "How we exploited query logs?",
            "OK, we designed a new caching policy.",
            "We designed and tested also a new document partitioning, inquiry routing strategy and then this is an ongoing work.",
            "The last one is an ongoing work.",
            "We designed a new time partitioning schema.",
            "I mean using the knowledge coming from the query, we design a new term partitioning schema to use in a 10 partition.",
            "It invented so information retrieval system.",
            "OK, caching policies."
        ],
        [
            "The traditional caching policies does not do not take advantage of usage information.",
            "For instance are you is is purely in line and does does not consider usage information.",
            "So now statistics are used.",
            "Anne.",
            "In the www."
        ],
        [
            "And three Ronnie Lampel Islam or am presented new kind of caching strategy that actually exploited usage information.",
            "In that paper, statistics were drawn from from the query log an they use these statistics in order to get some priorities to score.",
            "You know to score queries and to get some priority in order to nowhere, nowhere, whether a query is important or not.",
            "Well, once a query is presented to the search engine to know whether to cache or not, this is actually the one of maybe the first caching policy that might not cash just inquiry.",
            "It depends on the on the on the.",
            "Priority option.",
            "It they achieved a really good eat ratio, but.",
            "They have.",
            "Sorry.",
            "But Dematteis the complexity of their their solution was a little bit.",
            "I log K in as you know, as order of Lucky."
        ],
        [
            "We proposed.",
            "New caching policy called SDC, which is slightly more or less the same ideas use the zip flow within the catalog to draw statistics that we used to divide the our cash."
        ],
        [
            "Intersection a static section which was filled up with which is fed up with the most frequent queries.",
            "We just filled fill it up with the most frequent queries.",
            "It's read only because it's never change it online, it's static and it's the first section that is queried when a new search is issued.",
            "Then there's a dynamic section which is a traditional cash and you can choose whatever caching policy you want.",
            "Obviously is the 2nd is the second choice.",
            "I mean if the first section.",
            "Cannot answer the query.",
            "Then you pass to the second.",
            "We also"
        ],
        [
            "Adopted pre fetching like in the work of Ronnie.",
            "But OK, it's a new kind of prefetching.",
            "I mean an adaptive prefetching in the sense that we exploited the knowledge of the fact that if user submit sorry stop at the first page results, then so if a user goes through the second page is the result, then with high probability we go through the other pages.",
            "So our protection strategies works as follow.",
            "If the user requests the first page results, we don't apply.",
            "Pre fetching, whereas if the users go through the other pages we apply prefetching.",
            "This is more or less the main idea.",
            "So."
        ],
        [
            "These plots show date ratio when the static factor, I mean the the percentage of the cache devoted to the static section.",
            "Verizon.",
            "So as you can see, the iteration depends on the on the aesthetic factors.",
            "So it depends on the sides of the static section an.",
            "It increase as the static static section size increase up to 70%, for instance and for instance using prefetching an A simpler LRU policy you can achieve.",
            "More or less 55% of it ratio, whereas the other the other policy that doesn't use that don't use the static section.",
            "Stop at around 5051%.",
            "But the very good news is the following."
        ],
        [
            "Mean.",
            "The throughput of our cash is doubled.",
            "This because obviously we use the static section with which don't trick doesn't require a lock mutex lock, so.",
            "Concurrent accesses to the to this part of the of the cache does not require a lock and so.",
            "More than one thread in parallel came access to the cash these experiments showed.",
            "The throughput, I mean the queries per second answered by a simulator by varying the number of threads they they should line show PDC.",
            "I mean the same policy.",
            "So PC using a salary oh and a static section.",
            "But in the static section we applied lock so we lock every entry of the cache independent of of being in the static static section or dynamic one, whereas in the in the.",
            "In all say.",
            "Whereas in the higher floor in the airline, I mean in the in the plot which is double with respect to the other.",
            "We adopted the real STC policy so we just lock the dynamic section and as you can see the throughput is doubled.",
            "The other."
        ],
        [
            "The work which is jointly done with Diego Pena Nicola Firenze is at work, which is going to present it to the next thing for scale 2006 conference and the main idea here is to use the query log to drive assignment of documents in a document partitioning environment.",
            "How we did that we apply a clustering or by clustering technique to queries to the metrics of queries and document.",
            "So we enforce the clustering of queries with the clustering documents and vice versa.",
            "And obviously we cluster only the documents that answered the queries we have in the query log, so."
        ],
        [
            "The main innovations here is a kind of new model which we call query vector where documents are represented.",
            "By the queries that the answer instead of the terms they contain.",
            "Collections are represented by the Co clustering results.",
            "I mean, we represent each collection by the representant of the clustering that we apply instead of representing.",
            "As you know, vocabulary or vocabulary of terms with frequency.",
            "Stuff like that.",
            "We design a new selection strategy based on the results of clustering.",
            "We we also carried identify rarely, rarely asking documents on the basis of the information of the query log, and we discovered, but it's not a very you know, fresh news that more than 50% of documents.",
            "Actually, we are never hit by any query on our logs, so more than half of the documents are let's say useless.",
            "You know what you know about vision?",
            "OK, this is this is just how we are."
        ],
        [
            "Michael Classroom we built a cookie or contingency matrix where we had our queries per documents.",
            "You know, metrics where OK documents where entries contained sort of normalized rank of death documents for the query.",
            "We obtained the rank by using that as a search engine just to do tests.",
            "OK."
        ],
        [
            "These these image he has shown more or less in a in a visual way.",
            "The results of our Co clustering.",
            "I mean if you start by a query Dinosaur.",
            "Except I'm document magics, if you start by a term document matrix like that one, you will finally have this kind of block it matrix here and Deco clustering results are the descriptor of these blocks.",
            "So we have reduced matrix composed by.",
            "Those entries over there.",
            "And we also use those entry to rank queries and then to rank document collection.",
            "We carried out experiment."
        ],
        [
            "Using another collection of documents where the WBR 99 collection which come from Brazil from Brazilian websites, it contains almost 6 million documents.",
            "It's quite small.",
            "We had a query log from that search engine which goes through January 23 to October 2003.",
            "We divided the query log into two parts OK, the training set and test set an in the training set for each query we retrieved the top 50 results and we used that as a as a baseline for comparing the most relevant results.",
            "OK, it's you know it's not.",
            "I don't know if it's correct or not.",
            "I mean this kind of doing experiment.",
            "I mean considering just the top or matter considering this top 55 results returned by search engines.",
            "But since we don't have.",
            "Any judgment results on.",
            "On these queries.",
            "So that's the best we could do at that.",
            "At the time we don't have the click data, so we cannot.",
            "You know, compare what the user actually click, click it with what we have so we don't have relevance judgment.",
            "Different search engine, not another now OK, we also using a different set yes but we issue the query to the sensor changing."
        ],
        [
            "So, so the results are more or less consistent, let's say so.",
            "OK, first of all we just to have a baseline.",
            "We miss me, measured the precision of God, which is a popular document select collection selection metrics on a random allocation.",
            "And as you can see, the precision at 5:10 or 20 by varying the number of collection selection selected is very low.",
            "It almost is a random selection.",
            "I mean, if you divide 1 by 16, you can obtain more or less the same number of.",
            "The same precision letter.",
            "The number of clusters are actually 17 cause in the first sixteen there at the cluster which can be built by looking just at the queries and documents.",
            "The last one is those documents which are not present in the in the query log in the query results of the query log anyway, so this is a baseline.",
            "As you can see by app."
        ],
        [
            "In clustering the precision, even for Corey and for our magic, improves a lot.",
            "I mean, we pass from, for instance, 0.3 of precision at five.",
            "When considering just one collection, two 1.66 or 1.7.",
            "Of Korea Pick app, which is our collection selection strategy considering the same precision level and the same number of collection selected.",
            "OK, So what we learn from from this study?",
            "OK, we learned that we can do something by clustering queries.",
            "Sorry, documents by considering queries, and that may be considering just the document contents is not enough for answering queries in a large environment like the web for instance.",
            "Um?"
        ],
        [
            "Another work that is really ongoing is this one.",
            "I mean the main idea here is organizer partitioning vocabulary in a 10 partitioning information Inter partition and information retrieval system in order to allow better scalability, better load balancing because OK, the story would be quite long, but in 10 partitioning information retrieval system usually will there problem with load balancing and stuff like that.",
            "And.",
            "What we are what we like to do here is try to OK load balances is the main problem with this kind of system and that's the main reason why usually document partitioning is chosen among the two.",
            "The two organizations the idea is again exploit the Co occurrence of times in queries from query log in order to drive a better term assignment strategy.",
            "Oh OK well."
        ],
        [
            "Then partitioning there's a recent work by also from Ricardo, which proposed a new kind of system for answering, querying attend partition at the environment is different from the previous one, because this kind of system is a pipeline one.",
            "I mean, queries are made circulating through all the server that servers that are responsible for the terms contained within the query.",
            "They had.",
            "Some encouraging."
        ],
        [
            "Figures, I mean they measured that the number of sector reads where a third of the document partitioning one.",
            "The number of distinct threats.",
            "The total number of distinct threats.",
            "Where are 50?",
            "And so on and so forth.",
            "But"
        ],
        [
            "Fortunately, they also had worst worse throughput figures an after measuring after investigating the possible reasons, they discovered that maybe one of the reasons is due by due to the high load unbalance of this kind of system.",
            "But as you can see, the average load is below the average load of the document distributed one, so there's a kind of.",
            "You know results.",
            "These are results that encourage you to study this kind of system because the load is lower, is lowest OK, is lower than the document distributed one so.",
            "We"
        ],
        [
            "Starting from this point.",
            "We have designed a new method to assign terms to server.",
            "That by exploiting this knowledge about the frequencies of currency and they encode currency of term, we can try to.",
            "Back together terms that are very likely to be requested together and try also to balance the load by carefully assigning the terms to the different partitions.",
            "We could only see."
        ],
        [
            "Relate our our assignment strategy using the query log we have here.",
            "We also have OK. That's the same query log we I showed you before.",
            "And we optimized this kind of measure here, which we call we Omega Omega Lambda measure by using kind of clustering techniques based on Association rule mining or by the frequent set mining, which is a popular data mining techniques for finding subset that are very frequent.",
            "OK one."
        ],
        [
            "One is out.",
            "We measured the number of servers queried for each query OK. By considering a random assignment or even a bin packing, so an assignment based only on the frequency of appearance of term, we saw that almost 28% of query my may be answered just using one server in the case for instance, of the total be our query log by using a term assignment strategy, OK, Alpha is a perimeter perimeter and won't get into this because I don't have too much time anyway by assigning the term in a careful way we.",
            "Past 2 from 28% of queries answered by just looking at 1 seven 250% of queries answered by looking at just one service.",
            "So almost half of the queries were answered by a single server.",
            "Oh, OK."
        ],
        [
            "We also consider consider kind of list replication.",
            "I mean some terms are replicated through all the lists and we also have nice results."
        ],
        [
            "OK, for instance by replicating one.",
            "Point 1% of the term of the most frequent terms among all the service we pass from 50% of the previous light to 7067% of queries answered by just one service.",
            "So it's more than 2/3 of queries answered by a single server."
        ],
        [
            "Also the load balancing."
        ],
        [
            "OK, I will go fast here on the accesses.",
            "OK, there's a parameter tuning that has to be tuned in order to optimize our our assignment, but as you can see in this two plots, yeah, these two lines over here.",
            "This is the maximum load that we have.",
            "By considering you know what kind of or better what service are involved in the resolution of a query, and this is the average and as you can see the distance is very small, so we also succeed in balancing the load.",
            "Obviously there's a tradeoff because varying Alpha.",
            "They load unbalance Verizon too and also the number of servers that are needed to answer a query.",
            "Obviously if you try to balance the load you will lost power in OK in reducing the number of queries or service queries per query per answer.",
            "OK. Anne, OK."
        ],
        [
            "Hey.",
            "The last thing I would like to say is that.",
            "Up to now so far I. I've never heard about, you know, evaluating what we call the topic shift.",
            "I mean they the training data are not cannot last forever.",
            "I mean they get old.",
            "We also did some some some experiment and you can read the experiment on the toilet paper that I mentioned before and we saw actually that the OK.",
            "There are some data which get old, but the majority of them remain valid, valid even for more than a week.",
            "OK, this is just to to mention this.",
            "And."
        ],
        [
            "OK. That's it, thank you."
        ],
        [
            "So.",
            "In the simulations you were using the normal term conditioning or.",
            "No, in this mission we just evaluated the number of servers and load, so we didn't actually have a real real.",
            "Yeah, it it will be no no, no.",
            "It's it's independent.",
            "I mean this.",
            "I'd like to do OK, yeah, yeah, yeah yeah.",
            "For the load balancing yes.",
            "Yeah, it's independent from the number of service queries query, yeah?",
            "Do you do your measurements?",
            "Do you split the fare lock in a training and test?",
            "Yeah yeah.",
            "And we also I didn't show here, but we also ride the sides of the test.",
            "So to evaluate how long the model lost and we saw that it can be built by, you know, by a simple computation in half an hour and it lasts for a week so.",
            "Sorry for jumping in.",
            "No well, then I suggest we go home through the final talk of this session and we should take the speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With my information severity, I come from the CNN Pizza and I'm here to show some works, some ongoing, some passwords and some ongoing worse, which we are carrying out.",
                    "label": 1
                },
                {
                    "sent": "Actually in this moment in instance in our lab in pizza.",
                    "label": 1
                },
                {
                    "sent": "So the talk is about boosting performance of web search engines, algorithms using query log information.",
                    "label": 0
                },
                {
                    "sent": "So what we are going to talk about?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is about using information coming from catalogs in order to boost enhance the performance of.",
                    "label": 0
                },
                {
                    "sent": "Information retrieval algorithms or for instance, other operations of search engines.",
                    "label": 0
                },
                {
                    "sent": "So we all know that query logs are valuable valuable source of information for multiple purposes, not only caching, for instance, like it is being traditionally used, but also for optimize other web search engines operations and also for carrying out marketing strategies we just saw the talk by Ricardo that show a lot of possible users uses in this kind of direction so.",
                    "label": 0
                },
                {
                    "sent": "We'll know what exactly log.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we're in our works.",
                    "label": 0
                },
                {
                    "sent": "We consider query log as a query topic very casted page of results and the user ID which issued the request and also the timestamp.",
                    "label": 0
                },
                {
                    "sent": "We didn't consider the click data, they click through data because simply because we don't, we didn't have them.",
                    "label": 0
                },
                {
                    "sent": "So in our in our experiments and in our in our study.",
                    "label": 0
                },
                {
                    "sent": "We didn't consider that, but OK, we can even take advantage in using an obviously.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the logs we used and the logs, the characteristics of the locks we use, then the characteristics of the logs we using in the experiment.",
                    "label": 0
                },
                {
                    "sent": "These are three query logs.",
                    "label": 0
                },
                {
                    "sent": "The excite query log, which is quite old, but it's also popular in the field of study.",
                    "label": 0
                },
                {
                    "sent": "It gains from 99 seven and it has been used quite a lot in query log studies.",
                    "label": 0
                },
                {
                    "sent": "We have a query out from the typical in Italian, so changing the tickets are changing which came from April 2002 and please compose by almost almost 3.2 million queries where one point 5.5 million queries where distinct in Destin that query log.",
                    "label": 0
                },
                {
                    "sent": "We also have the Alta Vista catalog, which came from summer of 2001 and it is composed by 7.1 millions of queries, OK?",
                    "label": 0
                },
                {
                    "sent": "And two point 7 two point 6.",
                    "label": 0
                },
                {
                    "sent": "OK where distinct?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You will know this kind of distribution of queries and within the query log OK if you rank the query by their popularity the queries by their popularity you will get this kind of power law plot.",
                    "label": 0
                },
                {
                    "sent": "I mean the first query is the most popular and of course a lot of time whereas the last query which is the less popular of course just once or twice and the majority of since disease are log log plot.",
                    "label": 0
                },
                {
                    "sent": "The majority of the queries appears just once or twice.",
                    "label": 0
                },
                {
                    "sent": "Why 20% or even less of the queries appears a lot of time.",
                    "label": 0
                },
                {
                    "sent": "At this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is another blood, which is quite strange and this is not the percentage of request of the pages following the first.",
                    "label": 0
                },
                {
                    "sent": "I mean on the X axis that you can you can see the page idea of the requested pages, whereas in on the Y axis you can.",
                    "label": 0
                },
                {
                    "sent": "You can see the probability of requesting a page I TH or page I given that the previous page has been requested and as you can see if a user submit sorry.",
                    "label": 0
                },
                {
                    "sent": "Go through the first page and stop them.",
                    "label": 0
                },
                {
                    "sent": "Then OK, that's it.",
                    "label": 0
                },
                {
                    "sent": "But if a user go through the second page then with high probability more than 50% will go through the other pages of results from the result set, not pages result set.",
                    "label": 0
                },
                {
                    "sent": "Yeah, result.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just to introduce the content of query logs from our point of view.",
                    "label": 0
                },
                {
                    "sent": "How we exploited query logs?",
                    "label": 0
                },
                {
                    "sent": "OK, we designed a new caching policy.",
                    "label": 0
                },
                {
                    "sent": "We designed and tested also a new document partitioning, inquiry routing strategy and then this is an ongoing work.",
                    "label": 0
                },
                {
                    "sent": "The last one is an ongoing work.",
                    "label": 0
                },
                {
                    "sent": "We designed a new time partitioning schema.",
                    "label": 0
                },
                {
                    "sent": "I mean using the knowledge coming from the query, we design a new term partitioning schema to use in a 10 partition.",
                    "label": 0
                },
                {
                    "sent": "It invented so information retrieval system.",
                    "label": 0
                },
                {
                    "sent": "OK, caching policies.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The traditional caching policies does not do not take advantage of usage information.",
                    "label": 0
                },
                {
                    "sent": "For instance are you is is purely in line and does does not consider usage information.",
                    "label": 0
                },
                {
                    "sent": "So now statistics are used.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "In the www.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And three Ronnie Lampel Islam or am presented new kind of caching strategy that actually exploited usage information.",
                    "label": 0
                },
                {
                    "sent": "In that paper, statistics were drawn from from the query log an they use these statistics in order to get some priorities to score.",
                    "label": 0
                },
                {
                    "sent": "You know to score queries and to get some priority in order to nowhere, nowhere, whether a query is important or not.",
                    "label": 0
                },
                {
                    "sent": "Well, once a query is presented to the search engine to know whether to cache or not, this is actually the one of maybe the first caching policy that might not cash just inquiry.",
                    "label": 0
                },
                {
                    "sent": "It depends on the on the on the.",
                    "label": 0
                },
                {
                    "sent": "Priority option.",
                    "label": 0
                },
                {
                    "sent": "It they achieved a really good eat ratio, but.",
                    "label": 0
                },
                {
                    "sent": "They have.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "But Dematteis the complexity of their their solution was a little bit.",
                    "label": 0
                },
                {
                    "sent": "I log K in as you know, as order of Lucky.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We proposed.",
                    "label": 0
                },
                {
                    "sent": "New caching policy called SDC, which is slightly more or less the same ideas use the zip flow within the catalog to draw statistics that we used to divide the our cash.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Intersection a static section which was filled up with which is fed up with the most frequent queries.",
                    "label": 0
                },
                {
                    "sent": "We just filled fill it up with the most frequent queries.",
                    "label": 0
                },
                {
                    "sent": "It's read only because it's never change it online, it's static and it's the first section that is queried when a new search is issued.",
                    "label": 0
                },
                {
                    "sent": "Then there's a dynamic section which is a traditional cash and you can choose whatever caching policy you want.",
                    "label": 0
                },
                {
                    "sent": "Obviously is the 2nd is the second choice.",
                    "label": 0
                },
                {
                    "sent": "I mean if the first section.",
                    "label": 0
                },
                {
                    "sent": "Cannot answer the query.",
                    "label": 0
                },
                {
                    "sent": "Then you pass to the second.",
                    "label": 0
                },
                {
                    "sent": "We also",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adopted pre fetching like in the work of Ronnie.",
                    "label": 0
                },
                {
                    "sent": "But OK, it's a new kind of prefetching.",
                    "label": 0
                },
                {
                    "sent": "I mean an adaptive prefetching in the sense that we exploited the knowledge of the fact that if user submit sorry stop at the first page results, then so if a user goes through the second page is the result, then with high probability we go through the other pages.",
                    "label": 0
                },
                {
                    "sent": "So our protection strategies works as follow.",
                    "label": 0
                },
                {
                    "sent": "If the user requests the first page results, we don't apply.",
                    "label": 0
                },
                {
                    "sent": "Pre fetching, whereas if the users go through the other pages we apply prefetching.",
                    "label": 0
                },
                {
                    "sent": "This is more or less the main idea.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These plots show date ratio when the static factor, I mean the the percentage of the cache devoted to the static section.",
                    "label": 0
                },
                {
                    "sent": "Verizon.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, the iteration depends on the on the aesthetic factors.",
                    "label": 0
                },
                {
                    "sent": "So it depends on the sides of the static section an.",
                    "label": 0
                },
                {
                    "sent": "It increase as the static static section size increase up to 70%, for instance and for instance using prefetching an A simpler LRU policy you can achieve.",
                    "label": 0
                },
                {
                    "sent": "More or less 55% of it ratio, whereas the other the other policy that doesn't use that don't use the static section.",
                    "label": 0
                },
                {
                    "sent": "Stop at around 5051%.",
                    "label": 0
                },
                {
                    "sent": "But the very good news is the following.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mean.",
                    "label": 0
                },
                {
                    "sent": "The throughput of our cash is doubled.",
                    "label": 0
                },
                {
                    "sent": "This because obviously we use the static section with which don't trick doesn't require a lock mutex lock, so.",
                    "label": 0
                },
                {
                    "sent": "Concurrent accesses to the to this part of the of the cache does not require a lock and so.",
                    "label": 0
                },
                {
                    "sent": "More than one thread in parallel came access to the cash these experiments showed.",
                    "label": 0
                },
                {
                    "sent": "The throughput, I mean the queries per second answered by a simulator by varying the number of threads they they should line show PDC.",
                    "label": 0
                },
                {
                    "sent": "I mean the same policy.",
                    "label": 0
                },
                {
                    "sent": "So PC using a salary oh and a static section.",
                    "label": 0
                },
                {
                    "sent": "But in the static section we applied lock so we lock every entry of the cache independent of of being in the static static section or dynamic one, whereas in the in the.",
                    "label": 0
                },
                {
                    "sent": "In all say.",
                    "label": 0
                },
                {
                    "sent": "Whereas in the higher floor in the airline, I mean in the in the plot which is double with respect to the other.",
                    "label": 0
                },
                {
                    "sent": "We adopted the real STC policy so we just lock the dynamic section and as you can see the throughput is doubled.",
                    "label": 0
                },
                {
                    "sent": "The other.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The work which is jointly done with Diego Pena Nicola Firenze is at work, which is going to present it to the next thing for scale 2006 conference and the main idea here is to use the query log to drive assignment of documents in a document partitioning environment.",
                    "label": 0
                },
                {
                    "sent": "How we did that we apply a clustering or by clustering technique to queries to the metrics of queries and document.",
                    "label": 0
                },
                {
                    "sent": "So we enforce the clustering of queries with the clustering documents and vice versa.",
                    "label": 0
                },
                {
                    "sent": "And obviously we cluster only the documents that answered the queries we have in the query log, so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main innovations here is a kind of new model which we call query vector where documents are represented.",
                    "label": 0
                },
                {
                    "sent": "By the queries that the answer instead of the terms they contain.",
                    "label": 0
                },
                {
                    "sent": "Collections are represented by the Co clustering results.",
                    "label": 0
                },
                {
                    "sent": "I mean, we represent each collection by the representant of the clustering that we apply instead of representing.",
                    "label": 0
                },
                {
                    "sent": "As you know, vocabulary or vocabulary of terms with frequency.",
                    "label": 0
                },
                {
                    "sent": "Stuff like that.",
                    "label": 0
                },
                {
                    "sent": "We design a new selection strategy based on the results of clustering.",
                    "label": 0
                },
                {
                    "sent": "We we also carried identify rarely, rarely asking documents on the basis of the information of the query log, and we discovered, but it's not a very you know, fresh news that more than 50% of documents.",
                    "label": 0
                },
                {
                    "sent": "Actually, we are never hit by any query on our logs, so more than half of the documents are let's say useless.",
                    "label": 0
                },
                {
                    "sent": "You know what you know about vision?",
                    "label": 0
                },
                {
                    "sent": "OK, this is this is just how we are.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Michael Classroom we built a cookie or contingency matrix where we had our queries per documents.",
                    "label": 1
                },
                {
                    "sent": "You know, metrics where OK documents where entries contained sort of normalized rank of death documents for the query.",
                    "label": 0
                },
                {
                    "sent": "We obtained the rank by using that as a search engine just to do tests.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These these image he has shown more or less in a in a visual way.",
                    "label": 0
                },
                {
                    "sent": "The results of our Co clustering.",
                    "label": 0
                },
                {
                    "sent": "I mean if you start by a query Dinosaur.",
                    "label": 0
                },
                {
                    "sent": "Except I'm document magics, if you start by a term document matrix like that one, you will finally have this kind of block it matrix here and Deco clustering results are the descriptor of these blocks.",
                    "label": 0
                },
                {
                    "sent": "So we have reduced matrix composed by.",
                    "label": 0
                },
                {
                    "sent": "Those entries over there.",
                    "label": 0
                },
                {
                    "sent": "And we also use those entry to rank queries and then to rank document collection.",
                    "label": 0
                },
                {
                    "sent": "We carried out experiment.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using another collection of documents where the WBR 99 collection which come from Brazil from Brazilian websites, it contains almost 6 million documents.",
                    "label": 0
                },
                {
                    "sent": "It's quite small.",
                    "label": 0
                },
                {
                    "sent": "We had a query log from that search engine which goes through January 23 to October 2003.",
                    "label": 0
                },
                {
                    "sent": "We divided the query log into two parts OK, the training set and test set an in the training set for each query we retrieved the top 50 results and we used that as a as a baseline for comparing the most relevant results.",
                    "label": 0
                },
                {
                    "sent": "OK, it's you know it's not.",
                    "label": 0
                },
                {
                    "sent": "I don't know if it's correct or not.",
                    "label": 0
                },
                {
                    "sent": "I mean this kind of doing experiment.",
                    "label": 0
                },
                {
                    "sent": "I mean considering just the top or matter considering this top 55 results returned by search engines.",
                    "label": 0
                },
                {
                    "sent": "But since we don't have.",
                    "label": 0
                },
                {
                    "sent": "Any judgment results on.",
                    "label": 0
                },
                {
                    "sent": "On these queries.",
                    "label": 0
                },
                {
                    "sent": "So that's the best we could do at that.",
                    "label": 0
                },
                {
                    "sent": "At the time we don't have the click data, so we cannot.",
                    "label": 0
                },
                {
                    "sent": "You know, compare what the user actually click, click it with what we have so we don't have relevance judgment.",
                    "label": 0
                },
                {
                    "sent": "Different search engine, not another now OK, we also using a different set yes but we issue the query to the sensor changing.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so the results are more or less consistent, let's say so.",
                    "label": 0
                },
                {
                    "sent": "OK, first of all we just to have a baseline.",
                    "label": 0
                },
                {
                    "sent": "We miss me, measured the precision of God, which is a popular document select collection selection metrics on a random allocation.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, the precision at 5:10 or 20 by varying the number of collection selection selected is very low.",
                    "label": 0
                },
                {
                    "sent": "It almost is a random selection.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you divide 1 by 16, you can obtain more or less the same number of.",
                    "label": 0
                },
                {
                    "sent": "The same precision letter.",
                    "label": 0
                },
                {
                    "sent": "The number of clusters are actually 17 cause in the first sixteen there at the cluster which can be built by looking just at the queries and documents.",
                    "label": 0
                },
                {
                    "sent": "The last one is those documents which are not present in the in the query log in the query results of the query log anyway, so this is a baseline.",
                    "label": 0
                },
                {
                    "sent": "As you can see by app.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In clustering the precision, even for Corey and for our magic, improves a lot.",
                    "label": 0
                },
                {
                    "sent": "I mean, we pass from, for instance, 0.3 of precision at five.",
                    "label": 0
                },
                {
                    "sent": "When considering just one collection, two 1.66 or 1.7.",
                    "label": 0
                },
                {
                    "sent": "Of Korea Pick app, which is our collection selection strategy considering the same precision level and the same number of collection selected.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we learn from from this study?",
                    "label": 0
                },
                {
                    "sent": "OK, we learned that we can do something by clustering queries.",
                    "label": 0
                },
                {
                    "sent": "Sorry, documents by considering queries, and that may be considering just the document contents is not enough for answering queries in a large environment like the web for instance.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another work that is really ongoing is this one.",
                    "label": 0
                },
                {
                    "sent": "I mean the main idea here is organizer partitioning vocabulary in a 10 partitioning information Inter partition and information retrieval system in order to allow better scalability, better load balancing because OK, the story would be quite long, but in 10 partitioning information retrieval system usually will there problem with load balancing and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What we are what we like to do here is try to OK load balances is the main problem with this kind of system and that's the main reason why usually document partitioning is chosen among the two.",
                    "label": 0
                },
                {
                    "sent": "The two organizations the idea is again exploit the Co occurrence of times in queries from query log in order to drive a better term assignment strategy.",
                    "label": 1
                },
                {
                    "sent": "Oh OK well.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then partitioning there's a recent work by also from Ricardo, which proposed a new kind of system for answering, querying attend partition at the environment is different from the previous one, because this kind of system is a pipeline one.",
                    "label": 0
                },
                {
                    "sent": "I mean, queries are made circulating through all the server that servers that are responsible for the terms contained within the query.",
                    "label": 0
                },
                {
                    "sent": "They had.",
                    "label": 0
                },
                {
                    "sent": "Some encouraging.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Figures, I mean they measured that the number of sector reads where a third of the document partitioning one.",
                    "label": 0
                },
                {
                    "sent": "The number of distinct threats.",
                    "label": 0
                },
                {
                    "sent": "The total number of distinct threats.",
                    "label": 0
                },
                {
                    "sent": "Where are 50?",
                    "label": 0
                },
                {
                    "sent": "And so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fortunately, they also had worst worse throughput figures an after measuring after investigating the possible reasons, they discovered that maybe one of the reasons is due by due to the high load unbalance of this kind of system.",
                    "label": 0
                },
                {
                    "sent": "But as you can see, the average load is below the average load of the document distributed one, so there's a kind of.",
                    "label": 0
                },
                {
                    "sent": "You know results.",
                    "label": 0
                },
                {
                    "sent": "These are results that encourage you to study this kind of system because the load is lower, is lowest OK, is lower than the document distributed one so.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Starting from this point.",
                    "label": 0
                },
                {
                    "sent": "We have designed a new method to assign terms to server.",
                    "label": 0
                },
                {
                    "sent": "That by exploiting this knowledge about the frequencies of currency and they encode currency of term, we can try to.",
                    "label": 0
                },
                {
                    "sent": "Back together terms that are very likely to be requested together and try also to balance the load by carefully assigning the terms to the different partitions.",
                    "label": 0
                },
                {
                    "sent": "We could only see.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relate our our assignment strategy using the query log we have here.",
                    "label": 0
                },
                {
                    "sent": "We also have OK. That's the same query log we I showed you before.",
                    "label": 0
                },
                {
                    "sent": "And we optimized this kind of measure here, which we call we Omega Omega Lambda measure by using kind of clustering techniques based on Association rule mining or by the frequent set mining, which is a popular data mining techniques for finding subset that are very frequent.",
                    "label": 0
                },
                {
                    "sent": "OK one.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One is out.",
                    "label": 0
                },
                {
                    "sent": "We measured the number of servers queried for each query OK. By considering a random assignment or even a bin packing, so an assignment based only on the frequency of appearance of term, we saw that almost 28% of query my may be answered just using one server in the case for instance, of the total be our query log by using a term assignment strategy, OK, Alpha is a perimeter perimeter and won't get into this because I don't have too much time anyway by assigning the term in a careful way we.",
                    "label": 0
                },
                {
                    "sent": "Past 2 from 28% of queries answered by just looking at 1 seven 250% of queries answered by looking at just one service.",
                    "label": 0
                },
                {
                    "sent": "So almost half of the queries were answered by a single server.",
                    "label": 0
                },
                {
                    "sent": "Oh, OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also consider consider kind of list replication.",
                    "label": 0
                },
                {
                    "sent": "I mean some terms are replicated through all the lists and we also have nice results.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, for instance by replicating one.",
                    "label": 0
                },
                {
                    "sent": "Point 1% of the term of the most frequent terms among all the service we pass from 50% of the previous light to 7067% of queries answered by just one service.",
                    "label": 0
                },
                {
                    "sent": "So it's more than 2/3 of queries answered by a single server.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also the load balancing.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I will go fast here on the accesses.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a parameter tuning that has to be tuned in order to optimize our our assignment, but as you can see in this two plots, yeah, these two lines over here.",
                    "label": 0
                },
                {
                    "sent": "This is the maximum load that we have.",
                    "label": 0
                },
                {
                    "sent": "By considering you know what kind of or better what service are involved in the resolution of a query, and this is the average and as you can see the distance is very small, so we also succeed in balancing the load.",
                    "label": 0
                },
                {
                    "sent": "Obviously there's a tradeoff because varying Alpha.",
                    "label": 0
                },
                {
                    "sent": "They load unbalance Verizon too and also the number of servers that are needed to answer a query.",
                    "label": 0
                },
                {
                    "sent": "Obviously if you try to balance the load you will lost power in OK in reducing the number of queries or service queries per query per answer.",
                    "label": 0
                },
                {
                    "sent": "OK. Anne, OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey.",
                    "label": 0
                },
                {
                    "sent": "The last thing I would like to say is that.",
                    "label": 0
                },
                {
                    "sent": "Up to now so far I. I've never heard about, you know, evaluating what we call the topic shift.",
                    "label": 0
                },
                {
                    "sent": "I mean they the training data are not cannot last forever.",
                    "label": 0
                },
                {
                    "sent": "I mean they get old.",
                    "label": 0
                },
                {
                    "sent": "We also did some some some experiment and you can read the experiment on the toilet paper that I mentioned before and we saw actually that the OK.",
                    "label": 0
                },
                {
                    "sent": "There are some data which get old, but the majority of them remain valid, valid even for more than a week.",
                    "label": 0
                },
                {
                    "sent": "OK, this is just to to mention this.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. That's it, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In the simulations you were using the normal term conditioning or.",
                    "label": 0
                },
                {
                    "sent": "No, in this mission we just evaluated the number of servers and load, so we didn't actually have a real real.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it it will be no no, no.",
                    "label": 0
                },
                {
                    "sent": "It's it's independent.",
                    "label": 0
                },
                {
                    "sent": "I mean this.",
                    "label": 0
                },
                {
                    "sent": "I'd like to do OK, yeah, yeah, yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "For the load balancing yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's independent from the number of service queries query, yeah?",
                    "label": 0
                },
                {
                    "sent": "Do you do your measurements?",
                    "label": 0
                },
                {
                    "sent": "Do you split the fare lock in a training and test?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "And we also I didn't show here, but we also ride the sides of the test.",
                    "label": 0
                },
                {
                    "sent": "So to evaluate how long the model lost and we saw that it can be built by, you know, by a simple computation in half an hour and it lasts for a week so.",
                    "label": 0
                },
                {
                    "sent": "Sorry for jumping in.",
                    "label": 0
                },
                {
                    "sent": "No well, then I suggest we go home through the final talk of this session and we should take the speaker.",
                    "label": 0
                }
            ]
        }
    }
}