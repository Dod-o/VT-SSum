{
    "id": "cqwgl7vamfjwiligxf33hxafjhzdetxo",
    "title": "Machine Learning and Cognitive Science",
    "info": {
        "author": [
            "Joshua B. Tenenbaum, Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Nov. 2, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Science->Cognitive Science"
        ]
    },
    "url": "http://videolectures.net/mlss09uk_tenenbaum_mlcs/",
    "segmentation": [
        [
            "OK, I'm going to get started.",
            "Can people hear me an are the relevant mikes all on?",
            "Yeah OK, good thanks.",
            "So I'm Josh Tenenbaum here to talk to you about machine learning and cognitive science.",
            "Thanks very much to the organizers for having me here.",
            "And most of the time I'm over in the other Cambridge, MA, USA at MIT.",
            "So probably most of you have heard the term cognitive science.",
            "You may not know what this field is about it you can define it in different ways.",
            "Basically it's the scientific study of the mind, particularly from the point of view of computation.",
            "Thinking of the mind is a certain kind of computer, and not that that's all the mind is.",
            "But let's say that's a lot of interesting things about the mind can be interesting be illuminated from that perspective and trying to understand what kind of computations are going on in the mind.",
            "Now you know, in machine learning you build computer systems for learning, and we're one of the not.",
            "The only thing that we do in cognitive science, but it's one of the main things we do is study learning and inference.",
            "I think the reason I'm here is because in the last few years there's been a very exciting convergence of ideas, bidirectional idea flow between these two fields, studying people who are trying to build smarter, more human like machine learning systems, and people who are trying to have a better computational understanding of human learning.",
            "And I'm going to tell you about that set of ideas, but I think I wanted to start off by putting a little bit of a historical perspective on it that the relationship between these two fields.",
            "Studying human learning from any kind of a. Computational or mathematical or formal perspective and machine learning goes back a long time, basically to the beginning of these fields.",
            "They sort of grew up around the same time machine learning, I mean, but both of them were only made possible.",
            "An even intellectually made sense in light of computers.",
            "So they kind of grew up around the same time as the whole computer revolution, and they've grown up together, although sometimes particularly say as machine learning got very mathematical in the 90s, they grew apart for awhile.",
            "And only now are they starting to come back together as enough of the math becomes common currency on the cognitive science side and the people who are more mathematical.",
            "Machine learning people realize, hey, there's actually some interesting things we can do with that math.",
            "That's about actual human learning and not just, you know, convex optimization or something like that.",
            "That was actually Zubrin's line.",
            "I don't know if you want credit or avoid the blame or whatever, but.",
            "Right so."
        ],
        [
            "This is just a list I made on the train coming up here this morning from London of of, you know, approaches to various branches of machine learning, unsupervised supervised learning, reinforcement learning, in which there's been serious idea flow from the very beginning between these two fields, and arguably all of these things here, which I put machine learning names on these on these topics.",
            "All of them were actually invented by cognitive scientists.",
            "You may not believe that we can argue about that if you want.",
            "But what I mean by that is that in some cases they actually were invented by people who called themselves psychologists or had degrees in psychology or in Department of Psychology.",
            "That's where most cognitive scientists live, or people did things that were effectively equivalent to this well before machine learning was doing it.",
            "And somewhat somewhat independently, machine learning people, maybe kind of came to it later, but although there are also cases where you know from the very beginning the machine learning was developed first in the psychology literature and then spun out like perceptrons, the pretty much the basis of of any kind of statistical example based approach to machine learning.",
            "It was first published in the Journal Psychological Review that which is the bleeding Journal of Theoretical Psychology.",
            "And there's many other examples of this sort of thing.",
            "If you're interested, we can talk over lunch about this this history, but I won't really go into that here.",
            "What I'm going to focus on here is a certain research program which admittedly is somewhat a personal perspective.",
            "It's I'm going to talk a lot about work that I've done in some of my collaborators and students I wanted to."
        ],
        [
            "Just put up some of their faces, in particular the work I'm going to talk about today, and probably some of the next time is a lot of it is work that Tom Griffiths, Charles Camp, Amy, Perforce, Pat Shaft, Overcash, Mensing, Gandin Roy did, and then next time time permitting I'll talk about some work that some of these guys did, some of whom are your fellow students like Touma and Steve are over here, and also Chris Baker and Noah Goodman, and not only going to talk about their work, but it's what I know best and it makes a coherent story, so I hope that's valuable to.",
            "Now, the problems that we're interested in are in some ways central problems in machine learning, but there's a slight different twist when you often when you think about what makes machine learning challenging, or what are the challenges we're trying to solve.",
            "We talk about really large datasets and high dimensional spaces and all that sort of thing.",
            "But a lot of the problems for human learning have a slightly different flavor.",
            "They often look like we have a very small data set."
        ],
        [
            "And often what's most remarkable about human learning is our ability to get so much from so little right.",
            "So across cognition, you see the mind making building models, making generalizations, abstractions from often very little data.",
            "Sometimes, for example, learning a concept from just one or a few examples, it looks rather different from the case of machine learning, where you're interested in large datasets, and you're often interested in theoretical guarantees of how is my algorithm to do as N goes to Infinity.",
            "In human learning and doesn't go to Infinity.",
            "So it's a different perspective, but as we'll see, there's a lot we can learn from machine learning, and I think some things we can offer back."
        ],
        [
            "So here's an example of a kind of experiment that we do in our lab which illustrates this kind of learning from a very small number of examples.",
            "It's meant to capture in a form that you can all appreciate as adults of problem.",
            "That's really more commonly encountered in the natural world by children, say children who are learning words.",
            "What does it mean to learn a word?",
            "I don't mean like just learn the sound, but learn what the word means and how to use it so in a child learns a word like dog or horse or chair, they're learning a way to refer to a category of objects that not all words.",
            "In fact, most words don't label categories of objects, but those are actually some of the simplest cases, and where.",
            "Most of the attention is, so you're learning to refer to a category of objects.",
            "What is that?",
            "Well, as a simple kind of mathematical model, you might say there's this set of effectively infinite set of all objects, all possible objects, and a subset of those.",
            "It's constrained, but it's also kind of infinite set in a way of horses.",
            "Let's say when a child learns the workhorse, there's generative processes in the world that create more horses, and they could go on forever.",
            "But so there's an infinite set of horses, and somehow what child does when they learn that word is, they learn how to grasp the boundaries of that set.",
            "So they learn how to say which things are in.",
            "At set, which things are horses and which ones aren't?",
            "And they learn this from just one or a few examples like just one or a few points.",
            "Random points within that set in this infinite space.",
            "And yet from just those few examples, they're able to graph the boundaries approximately.",
            "So how does that work?"
        ],
        [
            "Well, here's an example of doing this in our in our lab experiment, where we take our subjects to the planet kazoo, we show you these gouvion objects.",
            "And teach you some words in the Gazoo be in language, so here's a word tofa, and from these examples you know you didn't have this concept before, but now you have it.",
            "I would say we can check this by testing your generalization performance in machine learning lingo you would say here's the training set I gave you these three examples and now the rest of this is the test set.",
            "It's kind of transduction problem if you know that that term.",
            "So let's go through some test examples.",
            "Yes, yes or no too far now.",
            "Yes.",
            "Yes.",
            "Yeah.",
            "A little more uncertainty there, which is fine that you know.",
            "We're Bayesian, our model predicts that it's just fine as you'll see, OK, so that's the idea, right?",
            "Somehow you got this.",
            "I mean, I guess we don't know what the ground truth is, but trust me, you're right."
        ],
        [
            "Alright, so this is one kind of problem that we study, but Cognizant reciting this sort of problem all over the place.",
            "I mean just in perception when we perceive the 3D World from a 2D image, that's a certain kind of leap beyond the data.",
            "A lot of the interest in our group and cognitive science more generally is in larger scale systems of knowledge.",
            "What are sometimes called intuitive theories.",
            "So machine learning is usually focused on learning little bits of knowledge, like learning a single concept or a single function, but our knowledge of the world you could think of it as.",
            "Having many of these bits, but they are integrated into some larger system that's larger than more than the sum of the sum of the parts.",
            "So we have intuitive theories of physics or intuitive theories of psychology that underlie our common sense in the world.",
            "Right here I am standing up here.",
            "My ability to not fall off the stage, my ability to make sure my laptop doesn't fall and suffer a catastrophic failure, hitting the ground.",
            "All these sorts of things is guided by an implicit intuitive theory of physical objects in dynamics, and you can see evidence that even very young children like infants.",
            "Who can't even speak or walk have these kinds of intuitive theories about how objects work or intuitive psychology we?",
            "You've heard of theory of mind.",
            "That's a kind of intuitive psychology where we see people acting in the world.",
            "Even just moving around.",
            "And when you see somebody moving, you don't just see them as an object in motion, but you see them as an entity governed by these very interesting latent variables.",
            "Mental states, like beliefs and desires, you see somebody head out that if somebody, if one of you just got up in the middle of this lecture and ran out that door, I wouldn't be wouldn't just be empty.",
            "My motion area processing that.",
            "I'd be thinking what's wrong with that guy?",
            "Did you forget something or did what did I say?",
            "It's intuitive psychology.",
            "We interpret an and make predictions about other peoples behavior by inferring these latent variables and time permitting at the end of next lecture.",
            "I'll talk a little bit how we can take ideas from probabilistic inference and machine learning all the way up to those kinds of systems of knowledge.",
            "So our goal is to try to reverse engineer these.",
            "These pieces of cognition and how they get there, how the learning works, and then also to try to build more human like machine learning algorithms.",
            "And I'll go back and forth between those two.",
            "Problems this isn't just to say that any ideal computational model we come up with necessarily describes how how the mind works, but I think it's pretty clear that human learning is the gold standard that machine learning should be aspiring to match."
        ],
        [
            "At least for these kinds of problems.",
            "Now this these problems are not actually original to psychologists or machine learning people, but they have a history in the Western philosophical tradition under the name of the problem of induction.",
            "So as long as people, at least in the Western tradition, have been thinking about thinking they've been thinking about these problems, I'm talking about Plato, Aristotle, Hume.",
            "You know the whole modern European philosophical tradition, and as long as people have been thinking about, how do you generalize beyond the data that's given?",
            "They've had the same kind of answer.",
            "Just basically they're just different variants on it.",
            "If you're making leaps that go beyond a small number of data points from going beyond the data that's in there, there's gotta be some other bits that make up the gap.",
            "Some other kind of knowledge.",
            "It's usually something more abstract that you can bring to bear on this particular situation, and you know Bayesians will call these priors people in.",
            "You know, in general, machine learning people talk about inductive bias.",
            "You're familiar with the bias variance Dilemma in statistics.",
            "It's a version of the same problem.",
            "Cognitive scientist sometimes talk about constraints.",
            "Learner has some hypothesis space, and there are some constraints that tell you out of all logically possible hypothesis about what a word could mean.",
            "Maybe some are, some are more."
        ],
        [
            "Right than others.",
            "So we know that in some sense the form of the answer, but the questions that we want to know are go beyond just saying abstract knowledge.",
            "We want to know how does some kind of prior knowledge abstract knowledge, guide learning and inference from sparse data?",
            "What form does it take across different domains and tasks and how might that abstract knowledge itself be acquired?",
            "Particularly in machine learning?",
            "There's you know there's an aesthetic value and sometimes practical value and wanting to have to wire in as little by hand as possible.",
            "Now it's an open question how much of the brain is kind of wired in by hand.",
            "Maybe the invisible hand of evolution, but it's quite likely that some significant parts of cognition are wired in or supported by things that are wired in, so we don't necessarily want to say everything is learned, but a lot of the deepest questions in the field.",
            "These questions of nature versus nurture empiricism versus nativism, are debates about what's wired in.",
            "What isn't and having computational tools that can tell us how the most fundamental aspects of knowledge might in principle be acquired will give us useful ways to understand in reality whether and where these aspects of knowledge are in fact learned from data and the kind of things I'll talk about here in these two lectures are using some ideas.",
            "Again, most of these are somewhat familiar in machine learning, and I think you've seen, oops, I think you've seen versions of of some of these already here, but I'm going to show you how to apply them to these kinds of problems motivated by human learning.",
            "And maybe mix them up in some interesting ways that are not standard in machine learning, but I think will give us the potential for more interesting human like machine learning approaches.",
            "So roughly these ideas that will be talking about the technical ideas, I've kind of organized an answer to these three questions, but really they all go together in interesting ways, and that's just for convenience.",
            "So the basic form.",
            "Of understanding how abstract knowledge guides learning from sparse data will be formalizing from a Bayesian POV or sort of more generally a probabilistic inference POV, and thinking very strongly in terms of generative models.",
            "So your prior knowledge is not just a bunch of numbers, but it takes that takes the form of a almost a causal description of what's out there in the world that generates the data that you're seeing.",
            "The kind of generative models will talk about.",
            "Some of them are going to be the sort of things you're familiar with for machine learning, and most of what I'll do today, I was going to stick to that.",
            "So things like representations like multi dimensional vector spaces or graphical models.",
            "But to capture these more cognitive parts of cognition, the higher level intuitive theories we're going to need more structured kinds of representations.",
            "So things like grammars, logic schemas, even even programs.",
            "So we're going to be writing down probabilities over.",
            "Logic or or programs where those things will describe much more powerfully structured generative models and from a again from a sort of historical POV, these more structured representations are often not certainly not associated with machine learning or even the whole modern statistical approach to AI, but more of the kind of classical good old fashioned AI.",
            "You know the early days of AI, where everything was about symbols and logic and all of that, and what many people have come to realize.",
            "Is that if you like, there's there's two classic eras of artificial intelligence, and they had had corresponding errors in cognitive science, the sort of symbolic.",
            "Era and the statistical era and people have come to realize that those aren't and don't have to be defined as competing or conflicting worldviews, but actually a lot of the interesting meat is to be found in coming up with ways to combine those ideas and to be able to do probabilistic inference over structured representations and even very sophisticated kinds of symbolic objects like computer programs.",
            "So there was for example at NIPS last year, some of us in a bunch of others organized a workshop on probabilistic programming, which means many different things, but.",
            "It's giving tools to try to do this sort of thing, and it's increasingly a very active area of research on both the human and machine sides, and then as far as trying to understand how these this abstract knowledge might be acquired.",
            "Well, if you like, we're asking where the priors come from and within Bayesian statistics in certain very simple forms, base inside assistants have long had an answer to that.",
            "They call it hierarchical models or hierarchical Bayes, and over the last few years and in part through work with people like UI and Zoom in and others.",
            "And Carly alot of people here organizing this summer school.",
            "These ideas have come into machine learning and in very powerful ways and I think you've already seen some of them.",
            "But you know the.",
            "The Cognitive science version of Hierarchical Bayes is sometimes called learning to learn where you take the priors that guide learning themselves are the objects of learning, so we'll see some of those ideas and will also be seeing a little bit of nonparametric Bayes, which I think you I was just starting to tell you about this.",
            "This is very important because if you think about human learning where you're learning over an entire lifetime, again, you're not just learning a little bit of knowledge here.",
            "The problems which motivate the nonparametric approach are central here, right?",
            "You can't just say there's a model of some fixed finite complexity.",
            "And you know what that is in advance, but you're going to have to have ways to allow the effective complexity of your model to grow over a whole lifetime.",
            "Trading off in Occam's razor sorts of ways.",
            "The complexity and fit just the sorts of things that are very elegantly approached in a nonparametric perspective.",
            "So we'll see some of these ideas over today in the next time.",
            "OK."
        ],
        [
            "So we finished the introduction.",
            "They'll be kind of three.",
            "Case study areas for probably the next 20 minutes or so.",
            "I'll talk about cognition is probabilistic inference.",
            "This is just laying the groundwork, basically showing how the basic idea of Bayesian inference and some very very simple kinds of generative models really, really simple.",
            "You might even think trivial almost textbook statistics things can be brought to bear on actual behavioral data from human subjects and to show yes, this really is a way to think about basic learning and inference behaviors in cognition.",
            "And then will.",
            "Go to more.",
            "Some more interesting problems like the problem I started with learning concepts from examples, which is an area where we're going beyond just textbook Bayesian statistics, basically to the frontiers of machine learning.",
            "It's an area where machine learning has made a lot of progress, but where there's still a lot of progress to be made in coming up with more human like systems and then probably some next time will go to these larger scale systems of knowledge like intuitive theories.",
            "OK, and I understand that people sometimes like to ask questions, so feel free to jump in as you like these lectures compared to the other lectures, will probably be a little bit less full of technical specifics, partly because I'm trying to build on what I think our technical things you've already learned and show you things you can do with them, but feel free to ask jumping if there's something that seems like I'm not being clear.",
            "You're not sure exactly what technical idea I'm referring to.",
            "All right now, this idea of viewing."
        ],
        [
            "Cognition as probabilistic inference is much bigger than any of the work that I have to tell you about here.",
            "Any of the work that I've been involved in.",
            "This is a slide that I prepared for NIPS tutorial couple of years ago on the same topic.",
            "I was just taking all the different areas of cognitive science where people have made progress recently by viewing them as probabilistic inference.",
            "I didn't have time to update this slide since 2007, so apologies to those of you who started working on these things since 2007, but I wouldn't have had room for your names on here anyway.",
            "Point is, this is an extremely active.",
            "Exciting research area and it's only getting more active, so it's a good time to get interested in it."
        ],
        [
            "What I'll tell you bout here just a couple of examples from work that I did mostly with Tom Griffiths, which was just again trying to look at basic cognitive capacities that can be analyzed from the point of view of almost textbook Bayesian statistics.",
            "Really, really simple, elementary things from machine learning POV, so that's trivial, but the place you have to start to show that these ideas actually apply to human cognition.",
            "I won't go into some of the history here, but there's a long history in psychology of going back and forth between.",
            "Viewing the mind as a kind of intuitive statistician and people being extremely skeptical of that idea.",
            "So within cognitive psychology, there's a lot of work to be done initially to just establish the groundwork that these ideas from probability theory were appropriate and useful for describing basic cognition.",
            "So I'll give a couple of examples.",
            "One.",
            "Start off with an example from causal learning.",
            "This is again a basic thing.",
            "This isn't just about Bayesian."
        ],
        [
            "6 one of the basic things that statisticians want to do is establish reliable relationships between variables.",
            "You might just be interested into correlation, although increasingly, statisticians don't just want a correlation, they want to know is it actually a causal relation?",
            "Can I intervene and manipulate and make something happen?",
            "But you know, here's again textbook problem.",
            "You have data that comes in the form of a two by two set up like this.",
            "There's some variable C and some other variable E. They are binary, and you observe instances where C either happened or didn't happen, either happened or didn't happen, and you're interested in whether or not C causes E. So example might be whether injecting some chemical causes mice to express a certain gene.",
            "That's a semi scientific example, but the experiments that have been done with people.",
            "Are you give them data?",
            "Sort of as if they were a scientist doing an experiment and they see data coming in either in a table or online and you basically ask them to make a judgment about that question.",
            "Scale from zero to 100.",
            "Traditionally, people haven't been so clear.",
            "I mean, if you want to know if you might be thinking if you're thinking you look at the question to see Cause E rate, judge that on a scale from zero to 100.",
            "And if your first thought is well, I'm not sure exactly what you mean.",
            "That could mean different things.",
            "That is part of the point that I'm trying to make that psychologists weren't originally very clear about that, but now that we've tried out a bunch of different models as a field, it's become clear that there's at least two different things you could mean by that that correspond to two basic kinds of learning in graphical mode."
        ],
        [
            "What you can think of as parameter estimation and what you could think of a structure.",
            "Learning psychologists have talked about judgments of causal strength, which are like basically estimates of parameters in a graphical model that represents the strength of relationship and also judgments of structure.",
            "Whether a link exists if you like in a graphical model.",
            "So here's a very simple graphical model for this problem.",
            "We have our two variables C&E.",
            "We're going to assume that we know that isn't the cause of C, and also that there isn't, say, a hidden common cause, it's.",
            "It's just either see consciously or it doesn't.",
            "That's set up here and there's.",
            "There's also going to be some background variables, some background cause which is not observed, but without loss of too much generality we can assume it's just always present, and it has some strength W 0."
        ],
        [
            "And that's important because there's going to be some cases when the cause isn't present and the effects still happen.",
            "So something has to have cause it."
        ],
        [
            "So the question is either in a model where there's these two causes, what's the strength of the observable potential cost?",
            "C or or more of a structural question?",
            "Is there a link from C to E?",
            "And then you're asking to compare these two hypothesis?"
        ],
        [
            "So the.",
            "The state of the art say up till about 10 years ago in psychology consisted of various simple rules that people gave for combining the cells of this two by two contingency table.",
            "If you like or these different conditional probabilities, there was what one classic measure means basically correspond to statistics that you might compute in traditional statistics where you compute us, you compute a statistic on your sample and then check whether it's significantly different from what you get by chance and that sort of thing.",
            "One popular model was called Delta P, which is just the difference in the conditional probability of the effect occurring given that the cause occurs and the conditional probability effect occurring given the doesn't occur.",
            "A lot of progress was made when the psychologist Pat Chang proposed this other measure, called causal power and derived it from an interesting set of axioms which now we recognize as basically a noisy or causal model.",
            "I mean, again, I mean causal here in a pretty light sense.",
            "Guys talk about noisy doors at all.",
            "Or do people know what that preparation?",
            "OK, well it's not that important, but it's basically a model which says the effect happens.",
            "It's a probabilistic generalization of an oven or function.",
            "The effect happens if this variable is present or that variable present in each one of those areas has an independent chance to cause the effect.",
            "And if either one or both of them succeed in causing it, then it happens.",
            "So it's like you know the probability from here plus there minus the product.",
            "And it's not too hard to show that both of these measures of causal strength estimation correspond to maximum likelihood estimates of the W one parameter in this very simple graphical model, the only difference is what's the parameterisation?",
            "And there's a slightly strange linear parameterisation which gives you this Delta P, but the noisy or is probably the most natural parameter station that people in Bayesian networks have used, and that gives you the causal power estimate here."
        ],
        [
            "Now another approach which which Tom and I looked at was inspired by Bayesian model selection and we were thinking, well, actually in a lot of these cases the most fundamental judgment is not.",
            "But to start off with the strength of the causal relation, but does a causal relation exists at all?",
            "So we pose that as the following kind of model selection thing we said.",
            "Let's take the model that underlies this.",
            "This causal causal power metric this this model, in which there's this noisy or relation.",
            "And there's a.",
            "Strength parameter for the cause and then let's compare it to a model which just doesn't have that.",
            "So it's basically.",
            "This is basically just a simple coin flipping model and in the traditional Bayesian model selection way we're going to do two things.",
            "We're going to first of all, we're going to compute this Bayes factor.",
            "Essentially, the log likelihood ratio under these two models.",
            "And we're going to do this in a way which is just sensitive to the difference in structure.",
            "So that means integrating out the parameters.",
            "In this case, integrating out the strength of the background, causing here also integrating out the strength of the cost.",
            "So we're asking in a Bayesian sense, how much evidence is there for this structure over that structure, independent of the strength.",
            "Of course, the stronger the causes there's going to be interaction, right?",
            "The stronger the cause, then the easier it is to see evidence for its existence in a small sample.",
            "And all of these experiments are done with very small sample sizes.",
            "Yeah.",
            "Prior here that that means you're actually comparing model clothes dryer unit prior on the parameters, yes, so here we've, that's exactly the question.",
            "So here we are.",
            "So you can either see this as a problem or as a feature.",
            "In Bayesian model selection.",
            "Your your inferences about which model is correct are going to be sensitive to the functional form of the model as well as the priors you put on there.",
            "Even though you're integrating out the parameters.",
            "It's true that it's not.",
            "It's not purely a judgment about the existence of a dependency, but it does depend on your prior.",
            "And here for simplicity, we just chose a uniform prior, but as some other recent work has shown, like Hongjing, Lu, Ann, Allen, Newell and Keith Holyoak, and others having a non.",
            "An interesting kind of non uniform prior might might make this better.",
            "And with David, thanks Tom and I also showed something kind of like that.",
            "Is that the answer?",
            "But here, yes, we're assuming just uniform parameters uniform prior, so you know in the noisy or these parameters.",
            "I guess I really didn't tell you enough detail to know what those things are in the noisy or.",
            "These parameters vary between zero and one 'cause, they're just the probability that on its own, each of these variables would cause the effect, and so the prior is just uniform over the interval zero to 1."
        ],
        [
            "OK, so when we do that, here's the results of applying these three different models.",
            "The two parameter estimation models and this model selection model.",
            "The Bayes factor model for two data from.",
            "A classic behavioral experiment that Pat Chain did with Mark Buehner.",
            "Now when we talk about data here in a lot of the experiments going to talk about, we mean two different.",
            "We could mean two different things.",
            "There's the data that the human learner season.",
            "Then there's the data that we as the psychologist measure from the human learner.",
            "So the data the human learner seizes across the top here and then.",
            "These bar graphs are plotting the behavioral data from the human subjects.",
            "The design of the experiment was very simple.",
            "There were, I think, a bunch of different conditions, always 16 trials, like 16 different mice who were either.",
            "Injected with a chemical or not.",
            "And I think it was always 8 mice were injected, 8 mice were not injected.",
            "There were different chemicals in different genes and so on and they varied.",
            "Just very simply in steps of probability.",
            ".25 all the possible conditional probability effect given at the present and the probability of the effect given the cause was absent.",
            "So for example, this case here .75 point 25.",
            "That was a case where six of the eight mice that were injected expressed the gene and two of the eight mice who were not injected expressed the gene and subjects were asked to judge on a scale of zero to 100.",
            "To what extent or the various different versions of the question, but something like to what extent does this inject this chemical?",
            "Cause that gene to be expressed and on a scale of zero to 100 you get these sorts of ratings.",
            "These are standard errors here, so you can see.",
            "Well, first of all, you can see the kind of thing that animated debate literally for 10 years in psychology between proponents of these two models.",
            "Basically, each of these models here explains one trend in the data but not the other, so there's an overall effect of increasing Delta P that means.",
            "So these are organized into blocks here that was not transparent to subjects, but they're organized into blocks where there's of increasing difference in these conditional probabilities and the simple Delta P model predicts that that's really the only factor, and you can see that overall trend.",
            "But there's also this trend within each of these blocks.",
            "This decreasing downward thing here, although it's a little bit not decreasing there, but kind of a little more U shaped, But basically that that's decreasing with the base rate, so you can see as we go through here.",
            "The base rate that probably affect when the cost isn't present is decreasing in each of these cases, right?",
            "So that's what the console power model predicts that Delta P predicts.",
            "The other thing, but it's actually really combination of those.",
            "And then there's this other bizarre effect, which people often observed but didn't really have a good story about, which is this one here, where even when Delta P is 0.",
            "So that means even when there's no difference between the number of mice expressing the gene have been injected in those who haven't like 4 out of eight.",
            "Injected half the expressive gene, four out of eight not injected the gene people actually give you different answers.",
            "The main effect here is that as the base rate goes down to zero, there are numbers get much lower, but as the base rate approaches one, their numbers get towards the middle of the scale.",
            "And what we showed is that this Bayesian structure, learning account, or even Bayesian model selection predicts all of these effects, including both the Delta P effect and the effect of the base rate, and even the fact that the base rate has an effect when Delta P = 0.",
            "And the reason for that is essentially that the midpoint of this scale is interpreted as having a Bayes factor of 0 or or.",
            "The odds are one.",
            "So this is the point at which the evidence is completely neutral.",
            "And if you think about it, you know if I tell you OK, eight out of eight mice were injected, had the had the gene expressed eight out of eight mice were injected.",
            "Also, had the gene expressed you, you see that in a sense I have no information about whether the injection causes the gene to be expressed because.",
            "All of them were expressing it anyway, so if the cause is just an additive term on top of that, then you don't.",
            "There's no dynamic range there, but as the base rate goes down, you get more dynamic range for the cost to express itself and the Bayesian model selection is sensitive to that.",
            "That's part of.",
            "This is why I'd say it's sort of a good thing that you've got sensitivity in this analysis to the functional form of the model.",
            "It's sensitive to the fact that what this cause does.",
            "If it does anything, is to increase the probability effect."
        ],
        [
            "And one way to test this was a clever experiment that Tom came up with where we said, well, let's change the scenario a little bit, and in one case described the cause of something which which injection, which increases the probability effects.",
            "So here's here's a gene, which here's a chemical which causes the express and another one preventive.",
            "So we say.",
            ", some number of mice normally expresses gene, but this chemical might or might not prevent that gene expression.",
            "And then we could also just say we could.",
            "We could just say the chemical might make a difference and the question is, does it in this case, does it cause it?",
            "Does it prevent it?",
            "And in this case, does it make a difference and what you see is you get in general different patterns of results, but in particular on these cases when there is actually no difference between the two conditional probabilities of the effect with or without the cause, you get very different responses and those are completely predicted by.",
            "Again, this Bayesian model selection model where we changing is the parameterisation of the graphical model.",
            "So in this case it's just a generic multinomial or binomial conditional probability table.",
            "In this case it's in noisy or in this case it said noisy andnot, which is basically analogous sort of noisy logical function for a preventing cause.",
            "So again, this is showing that this idea that.",
            "Lots of ways in statistics to do model selection, but the Bayesian approach which is sensitive to the functional form of the model is actually doing."
        ],
        [
            "Out of work for you here.",
            "OK."
        ],
        [
            "Any questions on this yeah.",
            "And get the whole side of experiments and why always what does it mean that it goes up and down or down?"
        ],
        [
            "Previous picture, I think it's much more yes.",
            "Why do people predict the mean?",
            "Why do you have this effect?",
            "Here goes right, yeah?",
            "Victim so repeat that basically.",
            "Yeah, probably.",
            "Just I probably.",
            "What is the experience?",
            "Yeah, I mean, I guess there's a lot of so I didn't plan on telling you a whole lot about how to do behavioral experiments.",
            "Are you asking more about how the behavioral experiment works or.",
            "You did this experiment, yeah?",
            "There are some methods and they perform differently, but I mean the models perform differently.",
            "Predict worse when it goes to the right and this here in different clusters.",
            "Yeah well OK, so let's try this vision structure.",
            "Why does it predict these different effects?",
            "For example?",
            "Well, so in general, what's going on?",
            "If it's sort of just ignore this.",
            "Hopefully this makes sense.",
            "Basically, as you go this way you're getting more and more evidence for the existence of A cause, right?",
            "I mean, if if I say zero out of eight mice who were not injected, expressed the gene an 8 out of eight who were injected, expressed the gene, that should provide very strong evidence.",
            "Well, that injection really seems to have a causal effect right now.",
            "Try publishing that study in a medical Journal and it would get rejected right?",
            "But you only had eight mice in each condition.",
            "What's wrong with you?",
            "But people kind of leap to these conclusions, and it turns out that you know if if if medical statistics were more Bayesian, maybe that would be OK, I mean.",
            "That's somewhat glib.",
            "The actual size of these.",
            "This is again a measure of statistical evidence for the model.",
            "The size of these Bayes factors are not very big, but basically what we're showing here is that the same thing that base inside addition would use to assess the evidence for one model over another.",
            "People are sensitive to rather fine grained differences in that measure.",
            "Small differences that a statistician would want to see more evidence before they would make any conclusion.",
            "But we were sort of leading to these conclusions and we're doing it in ways that are basically exactly well calibrated to the amount of evidence in the data.",
            "Even in a case that might seem counter intuitive, right?",
            "Like why should people give you any?",
            "I mean what you might think is is something like this.",
            "If you.",
            "If you know again, let's say you're a. I'm doing a simple experiment and you have a reasonable sample like 1000 mice were injected and 500 of them that were injected have the gene expressed and 500 were not injected.",
            "Also have the gene expressed.",
            "Does the injection cause the gene no, no.",
            "No way you have great evidence against that, but if it was only if you only injected 8 and you had four out of eight injected, half the gene, four out of eight, not injected to gene, well, then you can't really be sure, right?",
            "So that's the difference between not being sure and being quite positive that it's not a cause.",
            "That's the difference in this case here.",
            "So basically, the reason why this Bayesian model predicts this this interesting effect here is the interaction of two things.",
            "First of all, it's being Bayesian.",
            "But it's the fact that we have a very small sample size and the fact that the middle of the scale this 50% line is what we're suggesting is that that's actually people are interpreting.",
            "That is kind of, you know, no evidence one way or the other, and they're taking the zero point as evidence against an.",
            "The 100 point is evidence for.",
            "As opposed to that, make some more sense.",
            "OK, yeah?",
            "The axe so so.",
            "So all all of so again, this is these are very good questions at any colleges to worry about, but I was hoping not to worry about here so.",
            "All of these models have slightly different units, and there are also sort of sort of transform to normalize the scales to have the same min and Max, so we don't necessarily think that this particular skill that people are using is directly meaningful.",
            "Basically there's the units of this are measured in.",
            "You know I mean so."
        ],
        [
            "This is the.",
            "This is the measure.",
            "It's this Bayes factor, but then we're putting it through sort of a nonlinear power law scaling which all the models go through.",
            "That kind of thing, which takes into account the fact that when people use array to judgment rating scale, they might not use the ratings equally, if that makes sense.",
            "So there's a weak non linearity for all of these models.",
            "That's just a parameter that's fit to.",
            "Two to the data.",
            "Yep.",
            "Although actually it turns out you don't need that.",
            "I mean, it turns out that if you if you just computed posterior probability, you would get basically the same thing.",
            "So if that's not what I'm showing here, but that is true.",
            "Yeah, like people are amazingly precise, exactly which many subjects together and their role impact really noisy.",
            "So I didn't collect these data, although we have collected.",
            "We've replicated this experiment and gotten basically the same data, and I actually don't remember if those are our data or butyrin chains, but.",
            "Because there's error bars, they probably are data.",
            "I mean, just not that we were better.",
            "Just I didn't have the error bars.",
            "What sorry, what did you ask?",
            "Yeah, people replaced individuals, so that's a good question.",
            "I think.",
            "So one thing that I don't, I don't know exactly exactly mean.",
            "Well, so in this experiment, yeah, the basic qualitative shapes are there in individual subjects, so we tested.",
            "For example, do you have this linear effect in individual Subs?"
        ],
        [
            "Another thing we tested is do you have?",
            "There's a slightly statistically significant kind of you shape here, which is interesting 'cause the Bayesian model predicts that and the other ones don't, so those are those are present in.",
            "Enough individual subjects more than you'd expect by chance, so I don't.",
            "I can't exactly tell you what the individual plots look like, but they have the same qualitative shapes.",
            "Most of them do.",
            "But yes, there certainly were certainly averaging here on the order of.",
            "50 subjects those are standard errors, so you know.",
            "Multiply by sqrt 50 to get the actual standard deviations and judgments and you know there's some significant variation.",
            "And that's standard thing and cognitive psychology.",
            "1.25 these ones.",
            "Listening to his letter.",
            "I think it's statistically significant mean intuitively, why should the real reason or.",
            "Well, in general I mean from the point of view of the Bayesian model.",
            "The reason why an there is a difference in the Bayesian model, but the non linearity kind of obscures it.",
            "But you know, in general the reason why does kind of well, it's a version of the same kind of thing that was going on here, but it's most dramatic in this case as the base rate decreases, the more dynamic range in a sense, for A cause of the same strength to express itself.",
            "So you get stronger evidence from the same.",
            "The same absolute difference in conditional probabilities for the presence of A cause.",
            "And if you can actually look at the form of the posteriors over the over the model parameters and see that happening there.",
            "OK, that I hope that helps.",
            "For this.",
            "I mean it's worth going through an experiment in some detail, which I guess I should have anticipated that that would be a good idea, but I didn't.",
            "But if possible I'd like to move on 'cause there's a much more that I want to cover all right.",
            "OK, so I'll tell you very quickly about two other experiments and hopefully many of the same issues are relevant.",
            "Like if you want to know what the scale is, it's the same.",
            "Again, I'm going to compare a Bayes factor, a log likelihood ratio to human judgments with some arbitrary nonlinear monotonic scaling."
        ],
        [
            "This is also a kind of a causal judgment, but it's one that's maybe a little bit more interesting, a little bit less text booky here we're trying to make sense of peoples.",
            "Intriguing abilities sometimes remarkable, sometimes frustrating to see hidden causes from coincidences, right?",
            "We're off were often sensitive to coincidences that in retrospect, turn out to be wrong, right?",
            "And so when we think of our sense of coincidence, we might think of it as this kind of mystical thing.",
            "And whenever I write papers on coincidences, I get a lot of attention from the popular press because they think, ooh, that's like real psychology.",
            "You know, we're mysterious, Paris, something or other.",
            "There's the most recognition I've ever had in any kind of media which is not saying very much is there's a show called Criminal Minds and.",
            "Something I said about coincidences was quoted by the by the suspect of the serial killer thing anyway.",
            "It was on CBS 10,000,000 people watched it.",
            "So I guess this is a good topic to work on, but but the reason we're interested in this is because so much of our actual causal knowledge of the world we think is driven by again noticing coincidences, which to a statistical point of view are probably not remarkable but do provide some evidence for a hidden cause that's worth exploring.",
            "So here's an example that may be motivated by something you might you know kind of thing that sometimes gets written up in the press.",
            "You've maybe heard of cancer clusters or disease clusters, like somebody notes that in a certain city there's an unusually high number of cases of some rare cancer.",
            "And maybe it's because some evil company was leaking something into the water and we should go and Sue them.",
            "It turns out most of the time those don't.",
            "Those claims don't turn out to hold up, but sometimes that kind of reasoning really leads you to a new disease like Lyme disease.",
            "I don't know if they have Lyme disease here.",
            "OK, well, this terrible thing.",
            "My daughter actually had it.",
            "She was treated for, but it's this thing you can get in the US from being bitten by a certain kind of tick, a deer tick, and if it's untreated it can cause serious neurological damage.",
            "It's kind of like syphilis, I mean, but.",
            "You get it from being bitten by a tick.",
            "Anyway, so so this was only rediscovered, you know, maybe 10 or 20 years ago in Lyme, CT. And that's why it's called Lyme disease.",
            "But anyway, so imagine that you know this is this is 1 square mile of a city and I'm plotting each incidence of a rare cancer in over a year.",
            "And I say, well, you see, like there's a little potential cluster over there in the upper left hand corner.",
            "How much evidence you think there is that there's actually some hidden localized cause some actual thing causing people.",
            "To get this disease more often in some part of the city than others.",
            "You know, here I don't know.",
            "You might say it's not might be strong or weak.",
            "This was kind of a middle in case."
        ],
        [
            "The way we modeled this sort of measure is, we said, OK. Again, we're looking at it's A kind of model selection thing, but one model is just the uniform distribution of these events over this space.",
            "But another model.",
            "The interesting one says there's some hidden common cause which is spatially localized, and it's giving rise to some of these events, but not necessarily all of them.",
            "And this idea of picking out a hidden common cause in a sea of noise.",
            "Again, a lot of what the brain is designed to do looks like that.",
            "So the space of possible hidden causes we used were just.",
            "Gaussians you know, sort of Gaussians of different scale, not rotation in different positions of this space.",
            "So so sort of a continuous parameter space here varying in mean and variance.",
            "These are all possible represent all possible causes for these disease clusters and in computing the evidence for this model over that one we have to integrate out those hidden variables so I.",
            "Won't describe the math for that, but you probably know most of the math you need to know to be able to."
        ],
        [
            "That and I'll just show you the results of an experiment that we did recently for this where we gave people.",
            "Different, these different patterns and they had to judge on a scale of zero to 10.",
            "So here are their judgments.",
            "Again, we don't think of the absolute values as being particularly meaningful, and we're going to take the model predictions and normalize it to have the same min and Max.",
            "So, but the key thing is that we first of all only gave people a pretty small number of events, so this is these are all weak signals to a traditional statistical analysis, but people give you meaningfully different answers that depend on a function of.",
            "For example, how many events we give them, or in this case we're varying is the ratio of the number of events that look like they're in a tight cluster to those which aren't.",
            "So here you've got five tightly clustered events here for three.",
            "Two, you can barely notice it an one or zero is doesn't really mean anything.",
            "If the cluster has one, but people across again across a large number of subjects, they show very systematic effect here, increasing the judgment.",
            "There's a hidden cause as you go this way, and similarly for some of these other things.",
            "Here's I mean here we wanted to see how much detail we can get, so this is a very very weak effect, but there is a statistically significant effect of just bringing one of those dots over here.",
            "And anyway, the model makes the same basic predictions.",
            "Again, just showing that these these intuitions people have, although there we can by any normal statistical standard you know, shouldn't count as evidence in court of law.",
            "But they're all actually very sensible intuitions."
        ],
        [
            "Under a sensible Bayesian model selection analysis, given a pretty sensible hypothesis base, if possible, hidden causes."
        ],
        [
            "OK, yeah.",
            "Thanks for showing these.",
            "Yeah they were showing the plots and they were showing them in different random orders.",
            "And then somehow in the visual system it doesn't interfere.",
            "I mean, I'm so so.",
            "Yeah, no, that's a very good question in that one of the so one of the things you might think or you might question you might ask is, is this really a cognitive judgment?",
            "It's more of a perceptual judgement, right?",
            "So for this kind of thing I don't.",
            "I don't think you can draw a principle line there.",
            "In other work we've actually been interested in what we call a much more perceptual judgments, like modeling things like perceptual organization, using Bayesian principles, and that seems to work well too, but.",
            "It's a good.",
            "It's a good question, I mean here.",
            "In other experiments that we did, and you can, you can see this paper.",
            "I think there's a reference to hear this cognition.",
            "2007 paper.",
            "We did versions of this kind of thing, but we also did a number of more cognitive things where, for example, you know the famous birthday problem, like how many people do you have to have in a room such that the probability is greater than 50%?",
            "That two of them have had the same birthday.",
            "It's not.",
            "It's much less than the number we have here.",
            "I think it's 23 or something.",
            "Is that right?",
            "So.",
            "I recently heard a version of this where it says how many do you have to have in the room so that the probability is greater than 50% that two of them have a birthday in the same week.",
            "Think it is?",
            "Anyone know what the answer to that is?",
            "Within seven days of each other.",
            "Guess.",
            "Yeah, I think it's six so you were out right so the the classic result is supposed to be very counter intuitive and it probably we all may be the first time we heard it thought it was but but our sense was that actually people are pretty well calibrated.",
            "How big you know the relative magnitude of these coincidences.",
            "So it's obviously a bigger coincidence to have a birthday on the same day than to be within seven days of each other.",
            "And in the experiment we did is this was before I heard about this other problem.",
            "But what we did is we actually gave people patterns of birthdays, so it was just like this.",
            "But we said, well, suppose you go to a party and you meet people with the following birthdays.",
            "How much of a coincidence is it?",
            "We just said how, how strong a coincidence is it and not all the birthdays had a pattern to them.",
            "But there was some of them that you know there could be a strong pattern like three people on the same day or a week pattern like four people kind of within a month of each other and you get the exactly the same kind of thing.",
            "But of course you have to have a different hypothesis space of hidden causes.",
            "There the hidden causes are basically proximity in space.",
            "Sorry, proximity in time and other kinds of things like you know.",
            "The same day of the month, but basically it's a space of alternative regularity's and the Bayes factor does just as well a good job predicting judgments there.",
            "So I think this is a general.",
            "I mean the ability to detect hidden causes via coincidences in continuous data, whether it's spatial or temporal is something that the brain does all over the place, and computational neuro scientists have proposed that is what learning in the brain is.",
            "It's coincidence detection.",
            "I think that goes way too far to say that's all there is to learning in the brain.",
            "That sort of the whole point of the rest of these lectures is.",
            "All the things that go beyond finding coincidences, but it is the case that patterns of coincidence in Co occurrence in data are, you know, are good cues to hidden causal structure and I think across cognition and perception we're good at picking those out."
        ],
        [
            "Here's one other example.",
            "Again, this is these may be a little less familiar, but this is.",
            "This is actually a textbook problem from Bayesian statistics.",
            "If you've heard of the taxi cab problem, how many people heard of that, anyone?",
            "I think that probably the classic version of this is written up in Sir Harold Jeffreys introduction to theory of probability or something, which is one of the great Bayesian books of the Bayesian.",
            "Dark ages when you know you weren't allowed to be a Bayesian unless you were a physicist at Cambridge or something.",
            "So anyway, but here here are some real world examples of these problems.",
            "We just gave these to people in the questionnaire.",
            "You read about a movie that's made $60,000,000 to date.",
            "How much money will it make?",
            "In total, you see that something's been baking in the oven for 34 minutes.",
            "How long until it's ready?",
            "You meet someone who's 78 years old?",
            "How long will they live?",
            "Your friend quotes to you from line 17 of his favorite poem.",
            "How long is the poem?",
            "And so on.",
            "So in each of these cases, the formal structure here is that there's some phenomenon or event with an unknown extent or duration teetotal, so that could be the age of the total lifetime of this person.",
            "But the total amount of money this movie will make.",
            "And you just observe one random value T that's less than T total.",
            "That's all you know.",
            "And the question is, what's the total?",
            "What's the total amount of money the movie will make?",
            "Or this if you've heard of the doomsday problem that some philosophers analyze, it's sort of the same problem.",
            "So how do we analyze this formally from a Bayesian point of view?"
        ],
        [
            "Well, we want to compute the posterior over the total extents given the one sample T, which basically all we know is that it's less than T, so our likelihood is very simple.",
            "We just assume that T is a uniform random sample between zero and T total, and so so this is 1 / T total, so that integrates up to one over the possible range of observations and at zero if for any value of the total that would be less than T, right?",
            "So if the total life movie grosses is 50 million, you couldn't couldn't have made 60,000,000 to date.",
            "Then we have to plug in an appropriate prior and that's going to be sort of interesting.",
            "The standard textbook prior that, say Jeffries uses what he has come to be called an uninformative prior, which for a variable like continuous variable between zero and Infinity.",
            "The typical uninformative prior is also has this form of 1 / T total, but for different real world events.",
            "Part of why we wanted to study this is we can actually measure the actual prior distribution.",
            "Sorry, the actual prior distribution of these different classes events and see.",
            "Whether people are sensitive to using informative priors and using the right ones."
        ],
        [
            "Just just I'll try to be a little clearer here.",
            "'cause it's a little subtle on the questions you guys were asking about.",
            "What's the what's the actual model that we're we're evaluating?",
            "What's the actual quantity?",
            "So we compute.",
            "We take the posterior, which will have this form.",
            "It's going to be 0 for T total up until T, then it's it's typically going to be have its highest value around T and fall off like this, and the estimator of thetotal that were."
        ],
        [
            "Using is the posterior median, so we're taking this point T star where half of the posterior mass is less than T star, and half of it is greater than the star, and we're going to compare that posterior meeting to peoples judgments.",
            "Extrapolating the total duration from 1 sample T. Is that clear, hopefully.",
            "OK, we're actually going to compare the posterior median to the median of a large group of subjects.",
            "It's interesting, actually, that if you look at the variance in subjects, it actually looks a lot like the variance in the posterior, but that's a topic for another time.",
            "Here we're just looking at the posterior median to the peoples."
        ],
        [
            "Indian guesses.",
            "So these are these are two different kinds of data, right down here.",
            "Down here are the behavioral judgments of people.",
            "And up here are the relevant priors as measured empirically in the world.",
            "So let's start up here.",
            "What we chose, problems where we could actually go out and collect publicly available data on for a whole class of events.",
            "How long they tend to last.",
            "So you can look on IMDb and see how much do movies have movies made an you see?",
            "Most movies actually don't make very much money.",
            "There's kind of this power law distribution.",
            "Most most movies make significantly less than 100 million dollars, but every so often you have something like Star Wars or out here that makes many hundreds of millions.",
            "Similar kind of distribution governs the length of poems, so I think we looked at the Oxford Anthology of English Poetry or something like that.",
            "And you know most poems are pretty short, like 14 lines or something like that, but every so often you get some epic.",
            "You know thousand line poem, but yeah.",
            "Of course, other kinds, other classes, events have very different distributions, so lifespans we're pretty familiar with the distribution of lifespans.",
            "It's approximately normal, but not exactly.",
            "And there's a little infant mortality spike over here.",
            "Movie runtime similarly is kind of unimodal.",
            "Sort of normal like what I'm doing here.",
            "The red curves are fits within a parametric model of one of a few different types, so these are power law distributions.",
            "These are gaussians.",
            "This is a distribution of how long people serve in the US House of Representatives, and it has more of this gamma shape.",
            "So this is an air long.",
            "It's a special case of a gamma distribution fit to that thing.",
            "Here is the baking times of cakes that we collected from a cookbook.",
            "You can see it doesn't have a simple parametric form.",
            "But maybe again intuitively familiar, most cakes or the the modal cake takes 60 minutes to bake.",
            "Most cakes are this sort of, you know, in this big broad 3045 minutes peak.",
            "And then there's occasional sort of epic 90 minute cakes over there.",
            "Alright, so now what?",
            "We can use these priors plugging them in."
        ],
        [
            "You know right here."
        ],
        [
            "To then compute."
        ],
        [
            "The posterior and."
        ],
        [
            "Looking at the posterior median, and that's what we're plotting down here where we're doing that we're doing that for the models both for the empirical histogram priors and also the best parametric fits.",
            "These red plots.",
            "Those are the curves here, and we're comparing that to the black data points, which are the predictions.",
            "Again, the median production predictions of about 100 subjects who were given different groups of subjects were given a different T value, so some people were told about, you know.",
            "A poem that was 17 lines, others were told about opponent with seven lines or a poem that was, you know, 65 lines or whatever that is, and they made their judgment and what you can see is that basically across all these different datasets, people people are pretty much in line with the Bayesian predictions, including even for something you know, kind of a little bit weird.",
            "Like these cake baking time things.",
            "So that tells us two things.",
            "One is that in some sense people's intuitions about these everyday predictions are approximately Bayesian.",
            "But maybe the more interesting thing is that.",
            "The that they're at least implicitly sensitive to the right priors they know somehow something about the distributions of these different classes of events that enable them to make the right predictions, and you can't get this unless you plug the right priors in, particularly given that there are such different shapes across these different domains, and people seem to get exactly well, more or less the right form of the prediction function, not just.",
            "The actual values.",
            "OK. Alright, so that's.",
            "Well, that's pretty much the end of this.",
            "Of this first unit, and I can take questions on that if you want.",
            "Or I can.",
            "Yes.",
            "The hypothesis that a single person has a point prior and those cars that you will yes over here our distribution over the population, so appoint prior meaning like they just think that all problems are the same length or, well, well, OK, so that's a really good question and somebody published a paper arguing version version of that Mike Moser an how pachler published a paper saying, hey, you could get the same thing, probably not with a point prior, but if people just.",
            "If just people had in memory only a few poems or a few examples of movies they showed, you could fit this data pretty well just using like two or three examples in memory.",
            "And that's interesting.",
            "I think there's something interesting deepen right about that, and also something wrong.",
            "There's lots of other evidence we have, and Thomas published a response to this, showing that people really even within individual subjects.",
            "You can show evidence that people have these.",
            "These you have not not exactly, and it's not like everybody has the same distribution fully accurately, but people have very much like a full distribution and he used this very.",
            "Clever, remarkable method that Adam Sanborn and who I guess Adam isn't here, but some of you may know him.",
            "He's a postdoc at the Gatsby right now.",
            "Adam and Tom developed this so called human Markov chain Monte Carlo to show within individual subjects.",
            "It's a way of using MCMC with individual subsystem app out there prior, and they showed that individual subjects do have priors that look like this.",
            "But I think there's also something very much right about the point, which is that in this kind of study, and in a lot of other.",
            "Studies you know these Bayesian cognitive modeling things.",
            "The data are often consistent with, and sometimes when you look really closely at individual subjects which we can do here.",
            "They are consistent with the idea that people's inferences are based on taking a very small number of samples from the posterior.",
            "So I would say it's not the case that people have in mind, or that implicitly in our long-term memory, we only have two samples.",
            "But what we might bring to mind in anyone case might be a very small number of samples from the posterior, and that's the thing that's emerged across a number of different places and a bunch of us are interested in exploring this idea that.",
            "That the way people might implement Bayesian inference and learning first of all, might be a kind of Monte Carlo sampling based approximate inference, but also that they might use a very very small number of samples, which to any normal statistician would seem crazy.",
            "You know.",
            "Statisticians work really hard to get.",
            "To get very large numbers of independent samples.",
            "But empirically in practice.",
            "Machine learning people have found that that often very small samples were actually work quite well actually.",
            "The first time I remember seeing that was a talk that Zubin gave back in grad school.",
            "Remember, in your pretty sure it was in your factorial learning thing.",
            "You were doing this really really fast Gibbs sampling with like 10 gig samples or something like that.",
            "Well, I've made an impact on me.",
            "But anecdotally, many of you have done.",
            "MCMC based?",
            "Learn learning where there's an inner loop of MCMC.",
            "Inference might have noticed something like that.",
            "And again, I wasn't planning really to dwell on that, but I'm happy to talk about it and I think it's quite an interesting place where human human learning and machine learning can meet up.",
            "Sort of thinking about mechanisms for approximate inference and thinking about when you might actually be able to get away with, and maybe even when a cost benefit analysis where you take into account the cost of sampling might actually favor working with very small numbers of samples.",
            "Each subject was given only one question.",
            "For each of these topics, but across subjects, they got different subjects, got different numbers plugged in for these different topics, and it was just a one page questionnaire and they.",
            "You know they wrote a number next to each line.",
            "The same results when you present these these visually visually, you mean like in a graph or something, I'm not sure.",
            "Yeah, I mean in general sorry.",
            "Didn't mean to laugh at that cause I mean it was a really good question, and in general that's something that we're, you know.",
            "It's really something to worry about and to think about right, but it's sort of.",
            "It's both a worry and a good thing, like I think the brain that makes sense to think of the brain as I kind of intuitive statistician.",
            "And it also makes sense to separately think of the visual system as an intuitive decision.",
            "And sometimes the visual system might be a smarter statistician than the rest of the brain.",
            "The more linguistic, higher level part of the brain.",
            "That's often why it's very helpful to graph our data right.",
            "Like, I mean, good statisticians and learning people know this.",
            "Don't just compute numbers, don't just look at tables, but.",
            "Plot your data and look at it.",
            "Because you're able to use the very smart unsupervised learning exploratory data analysis stuff that's basically built into your visual system.",
            "Yeah.",
            "I remember seeing a paper where they studied the I don't remember by whom plus where they basically studied, how the brain combines combines information from visual cues.",
            "Yeah.",
            "And they showed they had some some evidence for the brain doing that in a Bayesian way.",
            "Yeah, only model like cervical models that explain.",
            "Yeah yeah.",
            "Model where you actually like and.",
            "Didn't even have like some sort of simple Gaussian model, yeah?",
            "Live a life within prior then yeah, yeah.",
            "There's a few papers like that.",
            "I wrote one of them, or rather, Conrad cording.",
            "Wrote one of them when he was a postdoc with me, and nicely put my name on it.",
            "Ian Murray also had it.",
            "Had a nice paper like that recently at NIPS with resemble and a few others, and people like David Nil at at Rochester have.",
            "There's a bunch of models like that.",
            "Alan Yuille has proposed models of basically multimodal sensor fusions.",
            "Sensory integration.",
            "I guess actually has been also worked on that in his thesis from a Bayesian standpoint and.",
            "I mean, yeah, so that's that's a rich, rich literature.",
            "I'm not sure.",
            "I mean, I think roughly it's roughly.",
            "I would say it's certainly similar to these things.",
            "Conrad, actually, recording described it actually as a causal inference.",
            "The way the way he set up his model was to say it was.",
            "It was very much."
        ],
        [
            "Like and kind of, I think, inspired by this sort of model or or this sort of model where he said so.",
            "It's actually sort of like this only the other way around.",
            "It's like if you you know when you see something like this.",
            "This is the kind of phenomenon, right?",
            "You see a visual event like this this contact thing, or imagine like hitting a baseball or cricket ball.",
            "Something you see a visual event and you also hear a sound and your mind conjoins those two similar kind of thing happens in ventriloquism, right when you see somebody's mouth moving or the failures of this when you see a badly dubbed.",
            "Moving in a foreign language right?",
            "So your visual system is actually very good at integrating or sort of deciding.",
            "Should I integrate this set of visual?",
            "Events with this set of auditory events, and you can or Conrad suggested modeling that as a kind of Bayesian model selection thing where you're actually comparing hypothesis that maybe there's one cause of both the visual trends.",
            "Or maybe there's two different causes.",
            "So it's sort of sort of like this, only slightly different graphical model.",
            "I think Ian's paper argued that that.",
            "The.",
            "There was a slightly different way, or I guess there was a debate about whether it's whether there's evidence that people are doing sort of map model selection or actual model averaging, and I think the data are still a little bit ambiguous, but that's one of the things people are arguing about, yeah?",
            "If you know anyone actually hypothesize what sort of structures in the brain might be used to, well, I don't know if represent problem with it.",
            "Yeah, so that's so.",
            "That's so there's a really active area of research on how our probability distributions represented in the brain and how are some of these very basic kinds of things, like just computing posterior in a Gaussian, or about the most sophisticated it's gotten is common filter.",
            "Like how does the brain do common filtering?",
            "People like Alex Pouget Raj Rao and Rich Demo has worked on this.",
            "Peter Diane.",
            "There's a lot of people interested in that, but for the most part, it's only focused on the very simplest.",
            "Bayesian statistics.",
            "Like how do you represent a distribution at all, say in the population code?",
            "So how might a population of neurons with different tunings represented distribution?",
            "And how might you just combine a prior and likelihood to get a posterior?",
            "People are starting to get to some slightly more Interestingly structured models, but nobody's really tried and nobody really has a clue how to represent you.",
            "Even these kinds of structure learning problems, I think that's.",
            "Pretty clearly to me at least, the single biggest challenge in computational neuroscience is that problem.",
            "It's of course a problem to Bayesian computation lessons, but it's more generally a problem of what cognitive science tells us is that human knowledge has all sorts of structure to it.",
            "Yet when we look at the brain, we don't know how any of that structure is represented, right?",
            "There's a very nice fit between the kind of conventional machine learning toolkit of learning functions in high dimensional spaces and looking at the brain.",
            "Where there's you know high dimensional spaces in patterns of neural activation or synaptic strengths, and so there's a nice fit between that.",
            "You know you could do a lot of computational neuroscience under the idea that it's all about you know regression in high dimensional spaces, but I think cognitive science tells us pretty clearly we need much more structured representations and where machine learning and statistical AI has gone recently is showing us tools for doing that, but bringing that into contact with the brain is complete mystery, and I think we're.",
            "We have some very speculative ideas, but it's you know, that's that's that's the most important problem to work on.",
            "I I wouldn't urge you too much to work on it, 'cause it's so hard, but I figure when I run out of sort of things I can actually make progress on.",
            "That's what I'll spend all my time thinking about.",
            "Any other questions about this stuff?",
            "Alright, so let me let me go into the concept learning session.",
            "Obviously I won't.",
            "I mean, we only have about 15 min."
        ],
        [
            "So I want to try to make as much of a start on this as I can, will finish it up next time and talk a little about intuitive theories.",
            "So this was the problem that I used to motivate this."
        ],
        [
            "You know, learning, learning a category from examples.",
            "It's interesting because.",
            "As posed here, it's not the traditional way that classification has been presented in statistics and machine learning, where again the usual you know the textbook picture you start off with is you get a bunch of positive examples in a bunch of negative examples, Anuar either learning a discriminating function, or you're doing something like a density based approach.",
            "You know Bayesian classifier, where you learn a density model for the positives and density model for the negatives, and then you compute for any new thing the posterior probability by comparing its relative densities under the positive model in the negative model.",
            "Whereas this doesn't really look like that here, I'm only giving you first of all three examples.",
            "It's hard to learn a discriminating function for just three examples, although you could learn something if it was very simple, like linear, but I'm not giving you any negative examples, so naive discriminative approaches or most discriminative approaches, even very not naive ones, are not naively applicable to this problem.",
            "And even generative approaches where you say, learn a density model for the positive examples, these are very complex objects.",
            "However, we're going to represent these.",
            "It's seems like it might be a high dimensional space and with three points, what density model are you going to learn?",
            "So it's kind of a puzzle.",
            "The other hand you know it's not insignificant that I also gave you all the other examples.",
            "We could call this the test set, or we could call this a semi supervised learning problem.",
            "Did you guys talk about that kind of semi supervised learning?",
            "OK, but so does people know that term.",
            "OK.",
            "So again, excuse me if I'm going over ground that you covered, but there's a lot of interest recently in problems where you have a lot of unlabeled data and a few labeled examples, so the unlabeled data make it like an unsupervised structure discovery problem.",
            "But the actual task you have to solve is defined by the small number of labeled examples.",
            "And the.",
            "The interesting computational challenge is to figure out how to extract something from all of these unlabeled examples that will help me generalize in a much more useful way.",
            "The concept I'm learning from these few label examples, and I think one way to approach this problem that takes us pretty quickly to the state of the art in machine learning, is thinking of this as an interesting kind of semi supervised learning problem.",
            "So we'll talk about talk about that, but the particularly the Bayesian approach to semi supervised learning that we're interested in really makes it an unsupervised learning problem.",
            "It's really about and in this sense, maybe my initial framing the problem was a bit of a cheat, but it's really about what can we learn about the structure of the world from these data and then.",
            "You know we'll use these labels in some clever way to go from that, but the real work is being done by coming up with the right hypothesis space in a sense, from the."
        ],
        [
            "Data now here's a very simple approach to solving this kind of problem.",
            "And I'll just illustrate it naively with a picture.",
            "So let's say we've got, you know our objects are points in a 2 dimensional space here.",
            "So each of these data points is one of these objects.",
            "And let's say you.",
            "So you get mostly unlabeled data and just one labeled example, so that one here and I say that sub licat or two file here it's applicate.",
            "OK, so which are the other buckets you tell me?",
            "OK, so is this a blanket?",
            "Is that a blanket?",
            "Is that a blanket?",
            "How to block it?",
            "OK."
        ],
        [
            "Great good, that was easy.",
            "Alright, so So what?"
        ],
        [
            "Easy.",
            "Well basically you look at this and you see these three clusters kind of pop out at you, right?",
            "And if you're able to identify the clusters from the unlabeled data, which in this case you were and you have some prior abstract knowledge that tells you that word labels pick out clusters, the clusters you can identify, then all I need is 1 example right to tell you and only one positive example that's basically just telling you which is the right cluster to label and versions of this approach have again been developed in machine learning.",
            "The first one that I know about was again work that Zubin did in grad school alot.",
            "I guess a lot of the work that we're doing is kind of footnotes on ghahremani.",
            "But sorry.",
            "OK, just kidding, but but you know Zubin I really like this technical report and I don't know if it was ever published outside of a tech number, but it's a great TR.",
            "Maybe there was an IP paper or something that in which zuben talked about.",
            "Yeah, I think it was the 1st paper I know to use.",
            "Gaussian mixture models or multinomial mixtures to think about unsupervised categorization and also supervising problems which were essentially these semi supervised problems and using EM back when that was like the.",
            "Sorry, I'm just making both of us killed them was the hot thing OK?",
            "And more recently there's been a lot of interest in these nonparametric mixture models, so I think you I started to tell you about Dirichlet process mixtures and so on, and I think he didn't quite finish telling you about them, but I'm going to assume he told you about them, but I think for the purposes of what I'm doing, you probably know most of what you need to know.",
            "But anyway, the basic idea is these are models which, unlike a finite mixture, have actually an infinite number of.",
            "Of components.",
            "But in a sense, when you do posterior inference conditioned on some finite sample, only a small finite number of those infinite components are effectively present.",
            "The effective number of degrees of freedom is is much smaller and kind of always sort of just the right size for the data you have.",
            "Assuming that the model predicted the model assumptions are appropriate.",
            "So Radford Neal introduced some of these things, but also Carl Rasmussen and.",
            "Starting about 10 years ago.",
            "Um?",
            "So that's so.",
            "This is one place where cognitive scientists again have been inspired by the same kinds of ideas that machine learning people have been developing with finite an infinite mixtures.",
            "Trysting thing is actually the psychologists thought of these things.",
            "First, in a certain sense."
        ],
        [
            "So there was this paper by Fried in Holyoke in the early 80s.",
            "You know, back when zoom in and I were like in high school or something in which they proposed, it was a little bit less elegant, but they proposed basically M like algorithm for learning in a Gaussian mixture that essentially was a model of unsupervised and semi supervised categorization.",
            "And then there's another psychologist, John Anderson proposed something which was mathematically equivalent to a Dirichlet process mixture, but he had no idea what that was, but.",
            "He just kind of derived it from first principles.",
            "It's pretty remarkable he's a smart guy.",
            "And he showed that that could be all."
        ],
        [
            "We used to describe human learning.",
            "I'll just illustrate a little bit about how this goes.",
            "This is a typical, extremely boring, trivial computationally trivial category learning problem that people have studied in cognitive psychology.",
            "Going back to the 50s.",
            "Again, this this looks more like a traditional machine learning problem, and there's a few positive examples, a few negative examples, so this is the category label.",
            "Here you have a training set and the test set the actual stimuli in these experiments.",
            "Are formally represented by a vector of a few binary features.",
            "They actually look like in the lot of the versions, including the first studies which were done by Jerome.",
            "Bruner and colleagues are these things which look like basically cards in the game of set.",
            "People know that card game set right now.",
            "Raise your hand if you do.",
            "I'm just curious, OK, I guess it's more popular in the states.",
            "It's it's a game with.",
            "Basically you have these cards and they have one or two or three shapes, and they're either circles or triangles.",
            "That kind of thing that actually ovals whatever they have different colors.",
            "They could have different shading, and there's some concept that's defined in this low dimensional discrete vector space.",
            "Alright, so how might you apply a one of these nonparametric mixture model?"
        ],
        [
            "Here, well, this is.",
            "This is the way Anderson did it, and what's interesting is he he did it in the context of a a certain kind of approach to inference, which you could think of as a greedy sequential search, or as Sanborn and colleagues showed, you can think of it as a particle filter with one particle.",
            "You probably don't.",
            "I think you haven't yet learned what particle filters are.",
            "There are kind of sequential Monte Carlo approach, but I think you will be learning about them very soon, right hope?",
            "And they're particularly elegant approaches to approximate inference to apply in a setting like this where you get examples coming in sequentially one at a time, and you want to grow out your hypothesis as the data come in in ways where the complexity of your hypothesis grow sort of just when you need it.",
            "So I'll just show this intuitively here, but hopefully this will be a useful intuition for when you.",
            "See this more formally, so what's going on here?",
            "This is what this tree is showing.",
            "Is the process that Anderson says is going on in the human learners mind, but it's also an actual legitimate approximate inference scheme for growing your mixture model as as examples come in.",
            "So these rows of these these tables are Boolean vectors specifying the features of the objects.",
            "So here are the first object comes in and it has its features are 11111 and.",
            "And what we're seeing is as new objects come in.",
            "The learner has to decide.",
            "Am I going to put this object in one of the clusters I already have, or am I going to make a new cluster?",
            "And of course, you're not literally making a new cluster, because in one of these nonparametric models, there's actually an infinite number of classes.",
            "You're just discovering one that you hadn't seen before, and what you're deciding is in the posterior.",
            "What's more likely is this.",
            "Is this a cluster that I just haven't seen yet, or is it?",
            "Should I think of this as an instance of a cluster, or a mixture component that I've already seen?",
            "And so you're always evaluating when a new object comes in.",
            "You're always evaluating it with respect to the clusters.",
            "You have an the new one, and the the way the Dirichlet process is defined.",
            "Gives you a natural tradeoff between basically a prior on how objects should be partitioned, or you know which which are the draws from the base distribution.",
            "I'm not sure exactly which terminology you've seen yet.",
            "Versus the kind of fit that's like that's one of the complexity terms, and then there's kind of the fit to the data where you want each cluster to be as clean as possible.",
            "In.",
            "The idea is that the probability is highest when all the things in a cluster are very similar, because then the cluster can very tightly concentrate its probability just on what those things look like, rather than spreading its probability mass over a very broad set of different things.",
            "So you can see this going on here.",
            "What the tree is showing is it's tracing out the map hypothesis, the.",
            "That the single highest posterior probability hypothesis as examples come in the first example is this just of course in its own cluster.",
            "Then you see 10101 and the question is, should I put that?",
            "Should I think that's that's in the same mixture component as the first one, or is it in a new one and?",
            "Here the best solution is to put it with the other one, basically because it has more features in common with that one.",
            "Then not right if this one.",
            "If the second, we're all zeros, it would probably be better to put it in a different cluster, because putting it in the same cluster as this one would require that cluster to have its probability mass spread out too much.",
            "Now you get 10110, and again it seems to be slightly better to put it in with this one then to make a new one.",
            "But here you get a case when now now you've seen basically you've seen the first 3 all have mostly ones and then you see one.",
            "That's all zeros.",
            "And now it's better to make a new cluster, because if you again, if you put that one in with the other with the previous examples, then that cluster would just have to get too broad, so it's better to make a very tightly focused sort of all zeros cluster.",
            "The.",
            "You know, basically we keep going here.",
            "You get the 5th example is has just one one so again it gets put in with the all zeros.",
            "So we sort of have a cluster with prototype is looking like it's sort of zeros and another one whose prototype is looking like it's once and so hopefully that's fairly intuitive and I think over the next couple of days you'll see the math necessary to understand why this is a greedy greedy meaning at each step.",
            "As an example comes in, you're sort of making the best.",
            "Choice the maximum posteriori choice, but it's also the kind of thing you would get if you were actually doing a sequential Monte Carlo or particle filter sample."
        ],
        [
            "And you'll see how that works.",
            "I guess I'm just about to to end here.",
            "I'll just show you an example of this that Griffiths at all showed elegantly explained some otherwise puzzling data in the psychology literature, and I think.",
            "Yeah, and then I'll save the rest for next time.",
            "So here's the example that they showed.",
            "It's let me first set the context for this, which is.",
            "Kind of like in the causal learning example that I showed at the beginning.",
            "Cognitive psychologist love to have these debates between two models, each of which two machine learning people often look kind of trivial.",
            "But the but Congress psychologists will debate about these for years or decades.",
            "Which is it?",
            "Is it this one, or that one and machine learning person might say, well, probably both of those are way too simple, and sometimes it's going to be one and sometimes going to the other, and usually in the real world you want something more complex that maybe generalizes both of them.",
            "So here's another example of that kind of dynamic.",
            "Two very popular models.",
            "In psychology are what are called prototypes and exemplars, and each of them also corresponds to a textbook statistical pattern recognition model.",
            "Prototypes are basically a naive Bayes model that says each category has a has a single prototype, which in this in the binary case you would just.",
            "Correspond to the.",
            "Well.",
            "If you think of it as a naive Bayes model, it's basically just specifying the probability of each of these features independently in the category, but then the prototype is just the thing that you know the the mean of that vector or the mode if you like.",
            "So here for a category category A, which are these examples here?",
            "It seems pretty clear the prototype should be this vector of all zeros and the prototype for B should be the vector of all ones, and if you imagine learning a naive Bayes classifier, that's.",
            "You know what you get is 1 category.",
            "A category would say for each feature independently, it's most likely to be 0 and for the one it's most likely be one.",
            "An exemplar model is like a.",
            "Well, there's different versions of it, but it's like a kernel density estimator for a Bayesian classifier.",
            "It basically says in psychology what well.",
            "I guess I should tell you what these things are to psychologist.",
            "Psychologist prototype model is an image you have in your mind of the category prototype, and you judge whether something is in the category by comparing its similarity to that one prototype.",
            "Psychologically and exemplar model is used or in memory instances of all the things you've ever seen tagged with which category they are?",
            "And then when the new thing comes in, you compute its similarity to the examples of Category A and the examples of Category B and you say it's most likely to be a if it's more similar on average to the AIDS, then to the bees.",
            "But of course if you think about it, that looks a lot like a kernel density estimate or taking a kernel density estimate of the density of category A in this feature space, and then also the density of category B and seeing.",
            "Which is a new out is a new object more likely under the category a distribution of the Category B distribution and it's for formerly equivalent, so that formal equivalence had been established in the mathematical psychology literature between prototype models and essentially naive Bayes or mixture models and exemplar models.",
            "And these kernel density classifiers.",
            "But there are all sorts of puzzling phenomena of sometimes human learning looks better described by 1.",
            "Sometimes it looks better described by the other, and sometimes it seems to.",
            "Change as learning goes on in ways that if you think about it, sort of seems like you have to be.",
            "You have to see versions of this if you're dealing with learning over a long time scale for Occam's razor, sorts of reasons, exactly the kind of things that nonparametric Bayes was introduced into machine learning to try to solve.",
            "So you know if you think about it, a prototype model is about the simplest possible model of what a category is.",
            "It just says essentially a category of objects is just one single thing plus random variation.",
            "So it's really really simple notion of the category, whereas exemplar models are quite complicated.",
            "They could.",
            "They could model, you know, various different.",
            "If you think about it as a kernel density classifier, the density functions could be just about anything, right?",
            "If you think about the discriminating boundaries that are induced by doing kernel density classification, there again can approximate any nonlinear discriminating function.",
            "If you think about it psychologically.",
            "A prototype model makes a very strong commitment to say what a dog is a dog is this thing.",
            "I imagine I have this platonic dog in mind, where as an example, our model says almost nothing about what a dog is.",
            "A dog is just all the dogs I've ever seen and a dog is just something like those, right?",
            "So this is so in some sense, the difference between prototype and exemplar models are just a spectrum of the simplest possible representation to the most complex possible representation, and what you see as a human learning goes on is in some sense.",
            "You often see things like this, which would show a shift from people starting off with the kind of a first simple approximation to a category, and then they overtime they learn a more complex, perhaps more accurate, hopefully more accurate, not overfit representation of the category, and that's illustrated in this in this sort of experiment where the stimuli were set up to have.",
            "To basically fit a nice prototype structure except for one exception.",
            "So notice this.",
            "This white one here.",
            "The exceptions are indicated by triangles.",
            "The category is indicated by the color, so this white triangle is a white one that looks more like a black one, and the black triangle is a black one that looks more like a white one.",
            "And basically you can see that it looks just as if these are all single one bit off from the category prototype, and then you just flopped sort of switched at birth.",
            "One of the one of the.",
            "White ones for the black ones and vice versa, right?",
            "So what happens when you give these examples to people and the here?",
            "What you do is you're just giving them examples to classify in an online setting and they have a number of different blocks of the experiment, so they're seeing the same stimuli over and over again, each curve going along the X axis corresponds to it.",
            "Another run through the stimuli.",
            "So if you like here at the beginning, they've only seen each exception once.",
            "And over the course of the experience, they start to see all the objects, but in particular the exceptions more often.",
            "So what happens is at the beginning of learning, the exceptions are just treated just like any other instance of the the prototype of the other category.",
            "So this one, even though it's labeled B or black subjects, are categorizing it as an A or a white one, and similarly this one is being categorized with the bees and they're getting feedback saying no, that's wrong, but it doesn't sink in immediately, right?",
            "So for the first few blocks of trials, there just subjects are just.",
            "Basically ignoring the feedback, telling them that they were wrong on these things, but then overtime they start to they start to both solidify the prototypes of A&B categories, but also realize gradually that actually Nope, this one really is a B and this one really isn't a an innocence.",
            "What or the way psychologists founded this before is that there were kind of two different things going on.",
            "There was early extraction of a prototype of each category and then later on more of an exemplar representation that was developing, which is.",
            "Basically, the exemplar model describes the final state and the prototype model looks most like the beginning state.",
            "But what Tom and colleagues showed is that this could be very naturally captured with one of these Dirichlet process mixture models, where basically what's going on is that at the beginning you're only finding two categories, and as it goes on, it's becoming more and more likely that you're going to introduce a separate category just for the exception.",
            "So at the end, you've actually got his four components in the mixture, two of which.",
            "Are associated with the a label and one with the B label, but it takes awhile for the exceptions to become their own category, 'cause you need to see you know the prior in the richly process.",
            "Initially favors just lumping them into a small number of categories.",
            "That's the concentration feature of the directly process, but once you get once you see enough instances of these exceptions, then it becomes.",
            "It becomes more and enough instances of the whole thing.",
            "Then the general shape of the distribution says OK, Now it's better.",
            "It's now.",
            "It's more likely that there's actually a separate small category that's just generating those exceptions, and by carving them off from the rest of the objects in the A and the B set, you're able to get an overall much better fit.",
            "It's essentially an automatic Occam's razor that's being illustrated over the course of this experiment, and it's just one of many places where this sort of idea that emerges very naturally from.",
            "Modern Bayesian machine learning seems to also be built into our heads.",
            "OK, I see we're pretty much overtime, so we should stop.",
            "OK, I'll pick up with categorization next time and I'll be happy to talk with you over the lunch break or anytime between now and then."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I'm going to get started.",
                    "label": 0
                },
                {
                    "sent": "Can people hear me an are the relevant mikes all on?",
                    "label": 0
                },
                {
                    "sent": "Yeah OK, good thanks.",
                    "label": 0
                },
                {
                    "sent": "So I'm Josh Tenenbaum here to talk to you about machine learning and cognitive science.",
                    "label": 1
                },
                {
                    "sent": "Thanks very much to the organizers for having me here.",
                    "label": 0
                },
                {
                    "sent": "And most of the time I'm over in the other Cambridge, MA, USA at MIT.",
                    "label": 0
                },
                {
                    "sent": "So probably most of you have heard the term cognitive science.",
                    "label": 0
                },
                {
                    "sent": "You may not know what this field is about it you can define it in different ways.",
                    "label": 0
                },
                {
                    "sent": "Basically it's the scientific study of the mind, particularly from the point of view of computation.",
                    "label": 0
                },
                {
                    "sent": "Thinking of the mind is a certain kind of computer, and not that that's all the mind is.",
                    "label": 0
                },
                {
                    "sent": "But let's say that's a lot of interesting things about the mind can be interesting be illuminated from that perspective and trying to understand what kind of computations are going on in the mind.",
                    "label": 0
                },
                {
                    "sent": "Now you know, in machine learning you build computer systems for learning, and we're one of the not.",
                    "label": 0
                },
                {
                    "sent": "The only thing that we do in cognitive science, but it's one of the main things we do is study learning and inference.",
                    "label": 0
                },
                {
                    "sent": "I think the reason I'm here is because in the last few years there's been a very exciting convergence of ideas, bidirectional idea flow between these two fields, studying people who are trying to build smarter, more human like machine learning systems, and people who are trying to have a better computational understanding of human learning.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to tell you about that set of ideas, but I think I wanted to start off by putting a little bit of a historical perspective on it that the relationship between these two fields.",
                    "label": 0
                },
                {
                    "sent": "Studying human learning from any kind of a. Computational or mathematical or formal perspective and machine learning goes back a long time, basically to the beginning of these fields.",
                    "label": 0
                },
                {
                    "sent": "They sort of grew up around the same time machine learning, I mean, but both of them were only made possible.",
                    "label": 0
                },
                {
                    "sent": "An even intellectually made sense in light of computers.",
                    "label": 0
                },
                {
                    "sent": "So they kind of grew up around the same time as the whole computer revolution, and they've grown up together, although sometimes particularly say as machine learning got very mathematical in the 90s, they grew apart for awhile.",
                    "label": 0
                },
                {
                    "sent": "And only now are they starting to come back together as enough of the math becomes common currency on the cognitive science side and the people who are more mathematical.",
                    "label": 0
                },
                {
                    "sent": "Machine learning people realize, hey, there's actually some interesting things we can do with that math.",
                    "label": 0
                },
                {
                    "sent": "That's about actual human learning and not just, you know, convex optimization or something like that.",
                    "label": 0
                },
                {
                    "sent": "That was actually Zubrin's line.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you want credit or avoid the blame or whatever, but.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is just a list I made on the train coming up here this morning from London of of, you know, approaches to various branches of machine learning, unsupervised supervised learning, reinforcement learning, in which there's been serious idea flow from the very beginning between these two fields, and arguably all of these things here, which I put machine learning names on these on these topics.",
                    "label": 1
                },
                {
                    "sent": "All of them were actually invented by cognitive scientists.",
                    "label": 0
                },
                {
                    "sent": "You may not believe that we can argue about that if you want.",
                    "label": 0
                },
                {
                    "sent": "But what I mean by that is that in some cases they actually were invented by people who called themselves psychologists or had degrees in psychology or in Department of Psychology.",
                    "label": 0
                },
                {
                    "sent": "That's where most cognitive scientists live, or people did things that were effectively equivalent to this well before machine learning was doing it.",
                    "label": 0
                },
                {
                    "sent": "And somewhat somewhat independently, machine learning people, maybe kind of came to it later, but although there are also cases where you know from the very beginning the machine learning was developed first in the psychology literature and then spun out like perceptrons, the pretty much the basis of of any kind of statistical example based approach to machine learning.",
                    "label": 0
                },
                {
                    "sent": "It was first published in the Journal Psychological Review that which is the bleeding Journal of Theoretical Psychology.",
                    "label": 0
                },
                {
                    "sent": "And there's many other examples of this sort of thing.",
                    "label": 0
                },
                {
                    "sent": "If you're interested, we can talk over lunch about this this history, but I won't really go into that here.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to focus on here is a certain research program which admittedly is somewhat a personal perspective.",
                    "label": 0
                },
                {
                    "sent": "It's I'm going to talk a lot about work that I've done in some of my collaborators and students I wanted to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just put up some of their faces, in particular the work I'm going to talk about today, and probably some of the next time is a lot of it is work that Tom Griffiths, Charles Camp, Amy, Perforce, Pat Shaft, Overcash, Mensing, Gandin Roy did, and then next time time permitting I'll talk about some work that some of these guys did, some of whom are your fellow students like Touma and Steve are over here, and also Chris Baker and Noah Goodman, and not only going to talk about their work, but it's what I know best and it makes a coherent story, so I hope that's valuable to.",
                    "label": 0
                },
                {
                    "sent": "Now, the problems that we're interested in are in some ways central problems in machine learning, but there's a slight different twist when you often when you think about what makes machine learning challenging, or what are the challenges we're trying to solve.",
                    "label": 0
                },
                {
                    "sent": "We talk about really large datasets and high dimensional spaces and all that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "But a lot of the problems for human learning have a slightly different flavor.",
                    "label": 0
                },
                {
                    "sent": "They often look like we have a very small data set.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And often what's most remarkable about human learning is our ability to get so much from so little right.",
                    "label": 1
                },
                {
                    "sent": "So across cognition, you see the mind making building models, making generalizations, abstractions from often very little data.",
                    "label": 0
                },
                {
                    "sent": "Sometimes, for example, learning a concept from just one or a few examples, it looks rather different from the case of machine learning, where you're interested in large datasets, and you're often interested in theoretical guarantees of how is my algorithm to do as N goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "In human learning and doesn't go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So it's a different perspective, but as we'll see, there's a lot we can learn from machine learning, and I think some things we can offer back.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example of a kind of experiment that we do in our lab which illustrates this kind of learning from a very small number of examples.",
                    "label": 0
                },
                {
                    "sent": "It's meant to capture in a form that you can all appreciate as adults of problem.",
                    "label": 0
                },
                {
                    "sent": "That's really more commonly encountered in the natural world by children, say children who are learning words.",
                    "label": 0
                },
                {
                    "sent": "What does it mean to learn a word?",
                    "label": 0
                },
                {
                    "sent": "I don't mean like just learn the sound, but learn what the word means and how to use it so in a child learns a word like dog or horse or chair, they're learning a way to refer to a category of objects that not all words.",
                    "label": 0
                },
                {
                    "sent": "In fact, most words don't label categories of objects, but those are actually some of the simplest cases, and where.",
                    "label": 0
                },
                {
                    "sent": "Most of the attention is, so you're learning to refer to a category of objects.",
                    "label": 0
                },
                {
                    "sent": "What is that?",
                    "label": 0
                },
                {
                    "sent": "Well, as a simple kind of mathematical model, you might say there's this set of effectively infinite set of all objects, all possible objects, and a subset of those.",
                    "label": 0
                },
                {
                    "sent": "It's constrained, but it's also kind of infinite set in a way of horses.",
                    "label": 0
                },
                {
                    "sent": "Let's say when a child learns the workhorse, there's generative processes in the world that create more horses, and they could go on forever.",
                    "label": 0
                },
                {
                    "sent": "But so there's an infinite set of horses, and somehow what child does when they learn that word is, they learn how to grasp the boundaries of that set.",
                    "label": 0
                },
                {
                    "sent": "So they learn how to say which things are in.",
                    "label": 0
                },
                {
                    "sent": "At set, which things are horses and which ones aren't?",
                    "label": 0
                },
                {
                    "sent": "And they learn this from just one or a few examples like just one or a few points.",
                    "label": 0
                },
                {
                    "sent": "Random points within that set in this infinite space.",
                    "label": 0
                },
                {
                    "sent": "And yet from just those few examples, they're able to graph the boundaries approximately.",
                    "label": 0
                },
                {
                    "sent": "So how does that work?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, here's an example of doing this in our in our lab experiment, where we take our subjects to the planet kazoo, we show you these gouvion objects.",
                    "label": 0
                },
                {
                    "sent": "And teach you some words in the Gazoo be in language, so here's a word tofa, and from these examples you know you didn't have this concept before, but now you have it.",
                    "label": 0
                },
                {
                    "sent": "I would say we can check this by testing your generalization performance in machine learning lingo you would say here's the training set I gave you these three examples and now the rest of this is the test set.",
                    "label": 0
                },
                {
                    "sent": "It's kind of transduction problem if you know that that term.",
                    "label": 0
                },
                {
                    "sent": "So let's go through some test examples.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes or no too far now.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "A little more uncertainty there, which is fine that you know.",
                    "label": 0
                },
                {
                    "sent": "We're Bayesian, our model predicts that it's just fine as you'll see, OK, so that's the idea, right?",
                    "label": 0
                },
                {
                    "sent": "Somehow you got this.",
                    "label": 0
                },
                {
                    "sent": "I mean, I guess we don't know what the ground truth is, but trust me, you're right.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so this is one kind of problem that we study, but Cognizant reciting this sort of problem all over the place.",
                    "label": 0
                },
                {
                    "sent": "I mean just in perception when we perceive the 3D World from a 2D image, that's a certain kind of leap beyond the data.",
                    "label": 0
                },
                {
                    "sent": "A lot of the interest in our group and cognitive science more generally is in larger scale systems of knowledge.",
                    "label": 0
                },
                {
                    "sent": "What are sometimes called intuitive theories.",
                    "label": 0
                },
                {
                    "sent": "So machine learning is usually focused on learning little bits of knowledge, like learning a single concept or a single function, but our knowledge of the world you could think of it as.",
                    "label": 0
                },
                {
                    "sent": "Having many of these bits, but they are integrated into some larger system that's larger than more than the sum of the sum of the parts.",
                    "label": 0
                },
                {
                    "sent": "So we have intuitive theories of physics or intuitive theories of psychology that underlie our common sense in the world.",
                    "label": 1
                },
                {
                    "sent": "Right here I am standing up here.",
                    "label": 0
                },
                {
                    "sent": "My ability to not fall off the stage, my ability to make sure my laptop doesn't fall and suffer a catastrophic failure, hitting the ground.",
                    "label": 0
                },
                {
                    "sent": "All these sorts of things is guided by an implicit intuitive theory of physical objects in dynamics, and you can see evidence that even very young children like infants.",
                    "label": 0
                },
                {
                    "sent": "Who can't even speak or walk have these kinds of intuitive theories about how objects work or intuitive psychology we?",
                    "label": 0
                },
                {
                    "sent": "You've heard of theory of mind.",
                    "label": 0
                },
                {
                    "sent": "That's a kind of intuitive psychology where we see people acting in the world.",
                    "label": 0
                },
                {
                    "sent": "Even just moving around.",
                    "label": 0
                },
                {
                    "sent": "And when you see somebody moving, you don't just see them as an object in motion, but you see them as an entity governed by these very interesting latent variables.",
                    "label": 0
                },
                {
                    "sent": "Mental states, like beliefs and desires, you see somebody head out that if somebody, if one of you just got up in the middle of this lecture and ran out that door, I wouldn't be wouldn't just be empty.",
                    "label": 0
                },
                {
                    "sent": "My motion area processing that.",
                    "label": 0
                },
                {
                    "sent": "I'd be thinking what's wrong with that guy?",
                    "label": 0
                },
                {
                    "sent": "Did you forget something or did what did I say?",
                    "label": 0
                },
                {
                    "sent": "It's intuitive psychology.",
                    "label": 0
                },
                {
                    "sent": "We interpret an and make predictions about other peoples behavior by inferring these latent variables and time permitting at the end of next lecture.",
                    "label": 0
                },
                {
                    "sent": "I'll talk a little bit how we can take ideas from probabilistic inference and machine learning all the way up to those kinds of systems of knowledge.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to try to reverse engineer these.",
                    "label": 1
                },
                {
                    "sent": "These pieces of cognition and how they get there, how the learning works, and then also to try to build more human like machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "And I'll go back and forth between those two.",
                    "label": 0
                },
                {
                    "sent": "Problems this isn't just to say that any ideal computational model we come up with necessarily describes how how the mind works, but I think it's pretty clear that human learning is the gold standard that machine learning should be aspiring to match.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At least for these kinds of problems.",
                    "label": 0
                },
                {
                    "sent": "Now this these problems are not actually original to psychologists or machine learning people, but they have a history in the Western philosophical tradition under the name of the problem of induction.",
                    "label": 1
                },
                {
                    "sent": "So as long as people, at least in the Western tradition, have been thinking about thinking they've been thinking about these problems, I'm talking about Plato, Aristotle, Hume.",
                    "label": 0
                },
                {
                    "sent": "You know the whole modern European philosophical tradition, and as long as people have been thinking about, how do you generalize beyond the data that's given?",
                    "label": 0
                },
                {
                    "sent": "They've had the same kind of answer.",
                    "label": 0
                },
                {
                    "sent": "Just basically they're just different variants on it.",
                    "label": 0
                },
                {
                    "sent": "If you're making leaps that go beyond a small number of data points from going beyond the data that's in there, there's gotta be some other bits that make up the gap.",
                    "label": 0
                },
                {
                    "sent": "Some other kind of knowledge.",
                    "label": 0
                },
                {
                    "sent": "It's usually something more abstract that you can bring to bear on this particular situation, and you know Bayesians will call these priors people in.",
                    "label": 1
                },
                {
                    "sent": "You know, in general, machine learning people talk about inductive bias.",
                    "label": 0
                },
                {
                    "sent": "You're familiar with the bias variance Dilemma in statistics.",
                    "label": 0
                },
                {
                    "sent": "It's a version of the same problem.",
                    "label": 0
                },
                {
                    "sent": "Cognitive scientist sometimes talk about constraints.",
                    "label": 0
                },
                {
                    "sent": "Learner has some hypothesis space, and there are some constraints that tell you out of all logically possible hypothesis about what a word could mean.",
                    "label": 0
                },
                {
                    "sent": "Maybe some are, some are more.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right than others.",
                    "label": 0
                },
                {
                    "sent": "So we know that in some sense the form of the answer, but the questions that we want to know are go beyond just saying abstract knowledge.",
                    "label": 0
                },
                {
                    "sent": "We want to know how does some kind of prior knowledge abstract knowledge, guide learning and inference from sparse data?",
                    "label": 1
                },
                {
                    "sent": "What form does it take across different domains and tasks and how might that abstract knowledge itself be acquired?",
                    "label": 1
                },
                {
                    "sent": "Particularly in machine learning?",
                    "label": 0
                },
                {
                    "sent": "There's you know there's an aesthetic value and sometimes practical value and wanting to have to wire in as little by hand as possible.",
                    "label": 0
                },
                {
                    "sent": "Now it's an open question how much of the brain is kind of wired in by hand.",
                    "label": 0
                },
                {
                    "sent": "Maybe the invisible hand of evolution, but it's quite likely that some significant parts of cognition are wired in or supported by things that are wired in, so we don't necessarily want to say everything is learned, but a lot of the deepest questions in the field.",
                    "label": 0
                },
                {
                    "sent": "These questions of nature versus nurture empiricism versus nativism, are debates about what's wired in.",
                    "label": 0
                },
                {
                    "sent": "What isn't and having computational tools that can tell us how the most fundamental aspects of knowledge might in principle be acquired will give us useful ways to understand in reality whether and where these aspects of knowledge are in fact learned from data and the kind of things I'll talk about here in these two lectures are using some ideas.",
                    "label": 0
                },
                {
                    "sent": "Again, most of these are somewhat familiar in machine learning, and I think you've seen, oops, I think you've seen versions of of some of these already here, but I'm going to show you how to apply them to these kinds of problems motivated by human learning.",
                    "label": 0
                },
                {
                    "sent": "And maybe mix them up in some interesting ways that are not standard in machine learning, but I think will give us the potential for more interesting human like machine learning approaches.",
                    "label": 0
                },
                {
                    "sent": "So roughly these ideas that will be talking about the technical ideas, I've kind of organized an answer to these three questions, but really they all go together in interesting ways, and that's just for convenience.",
                    "label": 0
                },
                {
                    "sent": "So the basic form.",
                    "label": 0
                },
                {
                    "sent": "Of understanding how abstract knowledge guides learning from sparse data will be formalizing from a Bayesian POV or sort of more generally a probabilistic inference POV, and thinking very strongly in terms of generative models.",
                    "label": 1
                },
                {
                    "sent": "So your prior knowledge is not just a bunch of numbers, but it takes that takes the form of a almost a causal description of what's out there in the world that generates the data that you're seeing.",
                    "label": 0
                },
                {
                    "sent": "The kind of generative models will talk about.",
                    "label": 0
                },
                {
                    "sent": "Some of them are going to be the sort of things you're familiar with for machine learning, and most of what I'll do today, I was going to stick to that.",
                    "label": 0
                },
                {
                    "sent": "So things like representations like multi dimensional vector spaces or graphical models.",
                    "label": 0
                },
                {
                    "sent": "But to capture these more cognitive parts of cognition, the higher level intuitive theories we're going to need more structured kinds of representations.",
                    "label": 0
                },
                {
                    "sent": "So things like grammars, logic schemas, even even programs.",
                    "label": 0
                },
                {
                    "sent": "So we're going to be writing down probabilities over.",
                    "label": 0
                },
                {
                    "sent": "Logic or or programs where those things will describe much more powerfully structured generative models and from a again from a sort of historical POV, these more structured representations are often not certainly not associated with machine learning or even the whole modern statistical approach to AI, but more of the kind of classical good old fashioned AI.",
                    "label": 0
                },
                {
                    "sent": "You know the early days of AI, where everything was about symbols and logic and all of that, and what many people have come to realize.",
                    "label": 0
                },
                {
                    "sent": "Is that if you like, there's there's two classic eras of artificial intelligence, and they had had corresponding errors in cognitive science, the sort of symbolic.",
                    "label": 0
                },
                {
                    "sent": "Era and the statistical era and people have come to realize that those aren't and don't have to be defined as competing or conflicting worldviews, but actually a lot of the interesting meat is to be found in coming up with ways to combine those ideas and to be able to do probabilistic inference over structured representations and even very sophisticated kinds of symbolic objects like computer programs.",
                    "label": 0
                },
                {
                    "sent": "So there was for example at NIPS last year, some of us in a bunch of others organized a workshop on probabilistic programming, which means many different things, but.",
                    "label": 0
                },
                {
                    "sent": "It's giving tools to try to do this sort of thing, and it's increasingly a very active area of research on both the human and machine sides, and then as far as trying to understand how these this abstract knowledge might be acquired.",
                    "label": 0
                },
                {
                    "sent": "Well, if you like, we're asking where the priors come from and within Bayesian statistics in certain very simple forms, base inside assistants have long had an answer to that.",
                    "label": 0
                },
                {
                    "sent": "They call it hierarchical models or hierarchical Bayes, and over the last few years and in part through work with people like UI and Zoom in and others.",
                    "label": 0
                },
                {
                    "sent": "And Carly alot of people here organizing this summer school.",
                    "label": 0
                },
                {
                    "sent": "These ideas have come into machine learning and in very powerful ways and I think you've already seen some of them.",
                    "label": 0
                },
                {
                    "sent": "But you know the.",
                    "label": 0
                },
                {
                    "sent": "The Cognitive science version of Hierarchical Bayes is sometimes called learning to learn where you take the priors that guide learning themselves are the objects of learning, so we'll see some of those ideas and will also be seeing a little bit of nonparametric Bayes, which I think you I was just starting to tell you about this.",
                    "label": 0
                },
                {
                    "sent": "This is very important because if you think about human learning where you're learning over an entire lifetime, again, you're not just learning a little bit of knowledge here.",
                    "label": 0
                },
                {
                    "sent": "The problems which motivate the nonparametric approach are central here, right?",
                    "label": 0
                },
                {
                    "sent": "You can't just say there's a model of some fixed finite complexity.",
                    "label": 0
                },
                {
                    "sent": "And you know what that is in advance, but you're going to have to have ways to allow the effective complexity of your model to grow over a whole lifetime.",
                    "label": 0
                },
                {
                    "sent": "Trading off in Occam's razor sorts of ways.",
                    "label": 0
                },
                {
                    "sent": "The complexity and fit just the sorts of things that are very elegantly approached in a nonparametric perspective.",
                    "label": 0
                },
                {
                    "sent": "So we'll see some of these ideas over today in the next time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we finished the introduction.",
                    "label": 0
                },
                {
                    "sent": "They'll be kind of three.",
                    "label": 0
                },
                {
                    "sent": "Case study areas for probably the next 20 minutes or so.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about cognition is probabilistic inference.",
                    "label": 1
                },
                {
                    "sent": "This is just laying the groundwork, basically showing how the basic idea of Bayesian inference and some very very simple kinds of generative models really, really simple.",
                    "label": 0
                },
                {
                    "sent": "You might even think trivial almost textbook statistics things can be brought to bear on actual behavioral data from human subjects and to show yes, this really is a way to think about basic learning and inference behaviors in cognition.",
                    "label": 0
                },
                {
                    "sent": "And then will.",
                    "label": 0
                },
                {
                    "sent": "Go to more.",
                    "label": 0
                },
                {
                    "sent": "Some more interesting problems like the problem I started with learning concepts from examples, which is an area where we're going beyond just textbook Bayesian statistics, basically to the frontiers of machine learning.",
                    "label": 1
                },
                {
                    "sent": "It's an area where machine learning has made a lot of progress, but where there's still a lot of progress to be made in coming up with more human like systems and then probably some next time will go to these larger scale systems of knowledge like intuitive theories.",
                    "label": 0
                },
                {
                    "sent": "OK, and I understand that people sometimes like to ask questions, so feel free to jump in as you like these lectures compared to the other lectures, will probably be a little bit less full of technical specifics, partly because I'm trying to build on what I think our technical things you've already learned and show you things you can do with them, but feel free to ask jumping if there's something that seems like I'm not being clear.",
                    "label": 0
                },
                {
                    "sent": "You're not sure exactly what technical idea I'm referring to.",
                    "label": 0
                },
                {
                    "sent": "All right now, this idea of viewing.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cognition as probabilistic inference is much bigger than any of the work that I have to tell you about here.",
                    "label": 0
                },
                {
                    "sent": "Any of the work that I've been involved in.",
                    "label": 0
                },
                {
                    "sent": "This is a slide that I prepared for NIPS tutorial couple of years ago on the same topic.",
                    "label": 0
                },
                {
                    "sent": "I was just taking all the different areas of cognitive science where people have made progress recently by viewing them as probabilistic inference.",
                    "label": 0
                },
                {
                    "sent": "I didn't have time to update this slide since 2007, so apologies to those of you who started working on these things since 2007, but I wouldn't have had room for your names on here anyway.",
                    "label": 0
                },
                {
                    "sent": "Point is, this is an extremely active.",
                    "label": 0
                },
                {
                    "sent": "Exciting research area and it's only getting more active, so it's a good time to get interested in it.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I'll tell you bout here just a couple of examples from work that I did mostly with Tom Griffiths, which was just again trying to look at basic cognitive capacities that can be analyzed from the point of view of almost textbook Bayesian statistics.",
                    "label": 1
                },
                {
                    "sent": "Really, really simple, elementary things from machine learning POV, so that's trivial, but the place you have to start to show that these ideas actually apply to human cognition.",
                    "label": 0
                },
                {
                    "sent": "I won't go into some of the history here, but there's a long history in psychology of going back and forth between.",
                    "label": 0
                },
                {
                    "sent": "Viewing the mind as a kind of intuitive statistician and people being extremely skeptical of that idea.",
                    "label": 0
                },
                {
                    "sent": "So within cognitive psychology, there's a lot of work to be done initially to just establish the groundwork that these ideas from probability theory were appropriate and useful for describing basic cognition.",
                    "label": 0
                },
                {
                    "sent": "So I'll give a couple of examples.",
                    "label": 0
                },
                {
                    "sent": "One.",
                    "label": 0
                },
                {
                    "sent": "Start off with an example from causal learning.",
                    "label": 0
                },
                {
                    "sent": "This is again a basic thing.",
                    "label": 0
                },
                {
                    "sent": "This isn't just about Bayesian.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "6 one of the basic things that statisticians want to do is establish reliable relationships between variables.",
                    "label": 0
                },
                {
                    "sent": "You might just be interested into correlation, although increasingly, statisticians don't just want a correlation, they want to know is it actually a causal relation?",
                    "label": 0
                },
                {
                    "sent": "Can I intervene and manipulate and make something happen?",
                    "label": 0
                },
                {
                    "sent": "But you know, here's again textbook problem.",
                    "label": 0
                },
                {
                    "sent": "You have data that comes in the form of a two by two set up like this.",
                    "label": 0
                },
                {
                    "sent": "There's some variable C and some other variable E. They are binary, and you observe instances where C either happened or didn't happen, either happened or didn't happen, and you're interested in whether or not C causes E. So example might be whether injecting some chemical causes mice to express a certain gene.",
                    "label": 0
                },
                {
                    "sent": "That's a semi scientific example, but the experiments that have been done with people.",
                    "label": 0
                },
                {
                    "sent": "Are you give them data?",
                    "label": 0
                },
                {
                    "sent": "Sort of as if they were a scientist doing an experiment and they see data coming in either in a table or online and you basically ask them to make a judgment about that question.",
                    "label": 0
                },
                {
                    "sent": "Scale from zero to 100.",
                    "label": 0
                },
                {
                    "sent": "Traditionally, people haven't been so clear.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you want to know if you might be thinking if you're thinking you look at the question to see Cause E rate, judge that on a scale from zero to 100.",
                    "label": 1
                },
                {
                    "sent": "And if your first thought is well, I'm not sure exactly what you mean.",
                    "label": 0
                },
                {
                    "sent": "That could mean different things.",
                    "label": 0
                },
                {
                    "sent": "That is part of the point that I'm trying to make that psychologists weren't originally very clear about that, but now that we've tried out a bunch of different models as a field, it's become clear that there's at least two different things you could mean by that that correspond to two basic kinds of learning in graphical mode.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you can think of as parameter estimation and what you could think of a structure.",
                    "label": 0
                },
                {
                    "sent": "Learning psychologists have talked about judgments of causal strength, which are like basically estimates of parameters in a graphical model that represents the strength of relationship and also judgments of structure.",
                    "label": 0
                },
                {
                    "sent": "Whether a link exists if you like in a graphical model.",
                    "label": 0
                },
                {
                    "sent": "So here's a very simple graphical model for this problem.",
                    "label": 0
                },
                {
                    "sent": "We have our two variables C&E.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that we know that isn't the cause of C, and also that there isn't, say, a hidden common cause, it's.",
                    "label": 0
                },
                {
                    "sent": "It's just either see consciously or it doesn't.",
                    "label": 0
                },
                {
                    "sent": "That's set up here and there's.",
                    "label": 0
                },
                {
                    "sent": "There's also going to be some background variables, some background cause which is not observed, but without loss of too much generality we can assume it's just always present, and it has some strength W 0.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's important because there's going to be some cases when the cause isn't present and the effects still happen.",
                    "label": 0
                },
                {
                    "sent": "So something has to have cause it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question is either in a model where there's these two causes, what's the strength of the observable potential cost?",
                    "label": 0
                },
                {
                    "sent": "C or or more of a structural question?",
                    "label": 0
                },
                {
                    "sent": "Is there a link from C to E?",
                    "label": 0
                },
                {
                    "sent": "And then you're asking to compare these two hypothesis?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The state of the art say up till about 10 years ago in psychology consisted of various simple rules that people gave for combining the cells of this two by two contingency table.",
                    "label": 0
                },
                {
                    "sent": "If you like or these different conditional probabilities, there was what one classic measure means basically correspond to statistics that you might compute in traditional statistics where you compute us, you compute a statistic on your sample and then check whether it's significantly different from what you get by chance and that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "One popular model was called Delta P, which is just the difference in the conditional probability of the effect occurring given that the cause occurs and the conditional probability effect occurring given the doesn't occur.",
                    "label": 0
                },
                {
                    "sent": "A lot of progress was made when the psychologist Pat Chang proposed this other measure, called causal power and derived it from an interesting set of axioms which now we recognize as basically a noisy or causal model.",
                    "label": 0
                },
                {
                    "sent": "I mean, again, I mean causal here in a pretty light sense.",
                    "label": 0
                },
                {
                    "sent": "Guys talk about noisy doors at all.",
                    "label": 0
                },
                {
                    "sent": "Or do people know what that preparation?",
                    "label": 0
                },
                {
                    "sent": "OK, well it's not that important, but it's basically a model which says the effect happens.",
                    "label": 0
                },
                {
                    "sent": "It's a probabilistic generalization of an oven or function.",
                    "label": 0
                },
                {
                    "sent": "The effect happens if this variable is present or that variable present in each one of those areas has an independent chance to cause the effect.",
                    "label": 0
                },
                {
                    "sent": "And if either one or both of them succeed in causing it, then it happens.",
                    "label": 0
                },
                {
                    "sent": "So it's like you know the probability from here plus there minus the product.",
                    "label": 0
                },
                {
                    "sent": "And it's not too hard to show that both of these measures of causal strength estimation correspond to maximum likelihood estimates of the W one parameter in this very simple graphical model, the only difference is what's the parameterisation?",
                    "label": 1
                },
                {
                    "sent": "And there's a slightly strange linear parameterisation which gives you this Delta P, but the noisy or is probably the most natural parameter station that people in Bayesian networks have used, and that gives you the causal power estimate here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now another approach which which Tom and I looked at was inspired by Bayesian model selection and we were thinking, well, actually in a lot of these cases the most fundamental judgment is not.",
                    "label": 0
                },
                {
                    "sent": "But to start off with the strength of the causal relation, but does a causal relation exists at all?",
                    "label": 0
                },
                {
                    "sent": "So we pose that as the following kind of model selection thing we said.",
                    "label": 0
                },
                {
                    "sent": "Let's take the model that underlies this.",
                    "label": 0
                },
                {
                    "sent": "This causal causal power metric this this model, in which there's this noisy or relation.",
                    "label": 0
                },
                {
                    "sent": "And there's a.",
                    "label": 0
                },
                {
                    "sent": "Strength parameter for the cause and then let's compare it to a model which just doesn't have that.",
                    "label": 0
                },
                {
                    "sent": "So it's basically.",
                    "label": 0
                },
                {
                    "sent": "This is basically just a simple coin flipping model and in the traditional Bayesian model selection way we're going to do two things.",
                    "label": 0
                },
                {
                    "sent": "We're going to first of all, we're going to compute this Bayes factor.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the log likelihood ratio under these two models.",
                    "label": 0
                },
                {
                    "sent": "And we're going to do this in a way which is just sensitive to the difference in structure.",
                    "label": 0
                },
                {
                    "sent": "So that means integrating out the parameters.",
                    "label": 0
                },
                {
                    "sent": "In this case, integrating out the strength of the background, causing here also integrating out the strength of the cost.",
                    "label": 0
                },
                {
                    "sent": "So we're asking in a Bayesian sense, how much evidence is there for this structure over that structure, independent of the strength.",
                    "label": 0
                },
                {
                    "sent": "Of course, the stronger the causes there's going to be interaction, right?",
                    "label": 0
                },
                {
                    "sent": "The stronger the cause, then the easier it is to see evidence for its existence in a small sample.",
                    "label": 0
                },
                {
                    "sent": "And all of these experiments are done with very small sample sizes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Prior here that that means you're actually comparing model clothes dryer unit prior on the parameters, yes, so here we've, that's exactly the question.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                },
                {
                    "sent": "So you can either see this as a problem or as a feature.",
                    "label": 0
                },
                {
                    "sent": "In Bayesian model selection.",
                    "label": 0
                },
                {
                    "sent": "Your your inferences about which model is correct are going to be sensitive to the functional form of the model as well as the priors you put on there.",
                    "label": 0
                },
                {
                    "sent": "Even though you're integrating out the parameters.",
                    "label": 0
                },
                {
                    "sent": "It's true that it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not purely a judgment about the existence of a dependency, but it does depend on your prior.",
                    "label": 0
                },
                {
                    "sent": "And here for simplicity, we just chose a uniform prior, but as some other recent work has shown, like Hongjing, Lu, Ann, Allen, Newell and Keith Holyoak, and others having a non.",
                    "label": 0
                },
                {
                    "sent": "An interesting kind of non uniform prior might might make this better.",
                    "label": 0
                },
                {
                    "sent": "And with David, thanks Tom and I also showed something kind of like that.",
                    "label": 0
                },
                {
                    "sent": "Is that the answer?",
                    "label": 0
                },
                {
                    "sent": "But here, yes, we're assuming just uniform parameters uniform prior, so you know in the noisy or these parameters.",
                    "label": 0
                },
                {
                    "sent": "I guess I really didn't tell you enough detail to know what those things are in the noisy or.",
                    "label": 0
                },
                {
                    "sent": "These parameters vary between zero and one 'cause, they're just the probability that on its own, each of these variables would cause the effect, and so the prior is just uniform over the interval zero to 1.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so when we do that, here's the results of applying these three different models.",
                    "label": 0
                },
                {
                    "sent": "The two parameter estimation models and this model selection model.",
                    "label": 0
                },
                {
                    "sent": "The Bayes factor model for two data from.",
                    "label": 0
                },
                {
                    "sent": "A classic behavioral experiment that Pat Chain did with Mark Buehner.",
                    "label": 0
                },
                {
                    "sent": "Now when we talk about data here in a lot of the experiments going to talk about, we mean two different.",
                    "label": 0
                },
                {
                    "sent": "We could mean two different things.",
                    "label": 0
                },
                {
                    "sent": "There's the data that the human learner season.",
                    "label": 0
                },
                {
                    "sent": "Then there's the data that we as the psychologist measure from the human learner.",
                    "label": 0
                },
                {
                    "sent": "So the data the human learner seizes across the top here and then.",
                    "label": 0
                },
                {
                    "sent": "These bar graphs are plotting the behavioral data from the human subjects.",
                    "label": 0
                },
                {
                    "sent": "The design of the experiment was very simple.",
                    "label": 0
                },
                {
                    "sent": "There were, I think, a bunch of different conditions, always 16 trials, like 16 different mice who were either.",
                    "label": 0
                },
                {
                    "sent": "Injected with a chemical or not.",
                    "label": 0
                },
                {
                    "sent": "And I think it was always 8 mice were injected, 8 mice were not injected.",
                    "label": 0
                },
                {
                    "sent": "There were different chemicals in different genes and so on and they varied.",
                    "label": 0
                },
                {
                    "sent": "Just very simply in steps of probability.",
                    "label": 0
                },
                {
                    "sent": ".25 all the possible conditional probability effect given at the present and the probability of the effect given the cause was absent.",
                    "label": 0
                },
                {
                    "sent": "So for example, this case here .75 point 25.",
                    "label": 0
                },
                {
                    "sent": "That was a case where six of the eight mice that were injected expressed the gene and two of the eight mice who were not injected expressed the gene and subjects were asked to judge on a scale of zero to 100.",
                    "label": 0
                },
                {
                    "sent": "To what extent or the various different versions of the question, but something like to what extent does this inject this chemical?",
                    "label": 0
                },
                {
                    "sent": "Cause that gene to be expressed and on a scale of zero to 100 you get these sorts of ratings.",
                    "label": 0
                },
                {
                    "sent": "These are standard errors here, so you can see.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, you can see the kind of thing that animated debate literally for 10 years in psychology between proponents of these two models.",
                    "label": 0
                },
                {
                    "sent": "Basically, each of these models here explains one trend in the data but not the other, so there's an overall effect of increasing Delta P that means.",
                    "label": 0
                },
                {
                    "sent": "So these are organized into blocks here that was not transparent to subjects, but they're organized into blocks where there's of increasing difference in these conditional probabilities and the simple Delta P model predicts that that's really the only factor, and you can see that overall trend.",
                    "label": 0
                },
                {
                    "sent": "But there's also this trend within each of these blocks.",
                    "label": 0
                },
                {
                    "sent": "This decreasing downward thing here, although it's a little bit not decreasing there, but kind of a little more U shaped, But basically that that's decreasing with the base rate, so you can see as we go through here.",
                    "label": 0
                },
                {
                    "sent": "The base rate that probably affect when the cost isn't present is decreasing in each of these cases, right?",
                    "label": 0
                },
                {
                    "sent": "So that's what the console power model predicts that Delta P predicts.",
                    "label": 0
                },
                {
                    "sent": "The other thing, but it's actually really combination of those.",
                    "label": 0
                },
                {
                    "sent": "And then there's this other bizarre effect, which people often observed but didn't really have a good story about, which is this one here, where even when Delta P is 0.",
                    "label": 0
                },
                {
                    "sent": "So that means even when there's no difference between the number of mice expressing the gene have been injected in those who haven't like 4 out of eight.",
                    "label": 0
                },
                {
                    "sent": "Injected half the expressive gene, four out of eight not injected the gene people actually give you different answers.",
                    "label": 0
                },
                {
                    "sent": "The main effect here is that as the base rate goes down to zero, there are numbers get much lower, but as the base rate approaches one, their numbers get towards the middle of the scale.",
                    "label": 0
                },
                {
                    "sent": "And what we showed is that this Bayesian structure, learning account, or even Bayesian model selection predicts all of these effects, including both the Delta P effect and the effect of the base rate, and even the fact that the base rate has an effect when Delta P = 0.",
                    "label": 0
                },
                {
                    "sent": "And the reason for that is essentially that the midpoint of this scale is interpreted as having a Bayes factor of 0 or or.",
                    "label": 0
                },
                {
                    "sent": "The odds are one.",
                    "label": 0
                },
                {
                    "sent": "So this is the point at which the evidence is completely neutral.",
                    "label": 0
                },
                {
                    "sent": "And if you think about it, you know if I tell you OK, eight out of eight mice were injected, had the had the gene expressed eight out of eight mice were injected.",
                    "label": 0
                },
                {
                    "sent": "Also, had the gene expressed you, you see that in a sense I have no information about whether the injection causes the gene to be expressed because.",
                    "label": 0
                },
                {
                    "sent": "All of them were expressing it anyway, so if the cause is just an additive term on top of that, then you don't.",
                    "label": 0
                },
                {
                    "sent": "There's no dynamic range there, but as the base rate goes down, you get more dynamic range for the cost to express itself and the Bayesian model selection is sensitive to that.",
                    "label": 0
                },
                {
                    "sent": "That's part of.",
                    "label": 0
                },
                {
                    "sent": "This is why I'd say it's sort of a good thing that you've got sensitivity in this analysis to the functional form of the model.",
                    "label": 0
                },
                {
                    "sent": "It's sensitive to the fact that what this cause does.",
                    "label": 0
                },
                {
                    "sent": "If it does anything, is to increase the probability effect.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one way to test this was a clever experiment that Tom came up with where we said, well, let's change the scenario a little bit, and in one case described the cause of something which which injection, which increases the probability effects.",
                    "label": 0
                },
                {
                    "sent": "So here's here's a gene, which here's a chemical which causes the express and another one preventive.",
                    "label": 0
                },
                {
                    "sent": "So we say.",
                    "label": 0
                },
                {
                    "sent": ", some number of mice normally expresses gene, but this chemical might or might not prevent that gene expression.",
                    "label": 0
                },
                {
                    "sent": "And then we could also just say we could.",
                    "label": 0
                },
                {
                    "sent": "We could just say the chemical might make a difference and the question is, does it in this case, does it cause it?",
                    "label": 0
                },
                {
                    "sent": "Does it prevent it?",
                    "label": 0
                },
                {
                    "sent": "And in this case, does it make a difference and what you see is you get in general different patterns of results, but in particular on these cases when there is actually no difference between the two conditional probabilities of the effect with or without the cause, you get very different responses and those are completely predicted by.",
                    "label": 0
                },
                {
                    "sent": "Again, this Bayesian model selection model where we changing is the parameterisation of the graphical model.",
                    "label": 0
                },
                {
                    "sent": "So in this case it's just a generic multinomial or binomial conditional probability table.",
                    "label": 0
                },
                {
                    "sent": "In this case it's in noisy or in this case it said noisy andnot, which is basically analogous sort of noisy logical function for a preventing cause.",
                    "label": 0
                },
                {
                    "sent": "So again, this is showing that this idea that.",
                    "label": 0
                },
                {
                    "sent": "Lots of ways in statistics to do model selection, but the Bayesian approach which is sensitive to the functional form of the model is actually doing.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out of work for you here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions on this yeah.",
                    "label": 0
                },
                {
                    "sent": "And get the whole side of experiments and why always what does it mean that it goes up and down or down?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Previous picture, I think it's much more yes.",
                    "label": 0
                },
                {
                    "sent": "Why do people predict the mean?",
                    "label": 0
                },
                {
                    "sent": "Why do you have this effect?",
                    "label": 0
                },
                {
                    "sent": "Here goes right, yeah?",
                    "label": 0
                },
                {
                    "sent": "Victim so repeat that basically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, probably.",
                    "label": 0
                },
                {
                    "sent": "Just I probably.",
                    "label": 0
                },
                {
                    "sent": "What is the experience?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, I guess there's a lot of so I didn't plan on telling you a whole lot about how to do behavioral experiments.",
                    "label": 0
                },
                {
                    "sent": "Are you asking more about how the behavioral experiment works or.",
                    "label": 0
                },
                {
                    "sent": "You did this experiment, yeah?",
                    "label": 0
                },
                {
                    "sent": "There are some methods and they perform differently, but I mean the models perform differently.",
                    "label": 0
                },
                {
                    "sent": "Predict worse when it goes to the right and this here in different clusters.",
                    "label": 0
                },
                {
                    "sent": "Yeah well OK, so let's try this vision structure.",
                    "label": 0
                },
                {
                    "sent": "Why does it predict these different effects?",
                    "label": 0
                },
                {
                    "sent": "For example?",
                    "label": 0
                },
                {
                    "sent": "Well, so in general, what's going on?",
                    "label": 0
                },
                {
                    "sent": "If it's sort of just ignore this.",
                    "label": 0
                },
                {
                    "sent": "Hopefully this makes sense.",
                    "label": 0
                },
                {
                    "sent": "Basically, as you go this way you're getting more and more evidence for the existence of A cause, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, if if I say zero out of eight mice who were not injected, expressed the gene an 8 out of eight who were injected, expressed the gene, that should provide very strong evidence.",
                    "label": 0
                },
                {
                    "sent": "Well, that injection really seems to have a causal effect right now.",
                    "label": 0
                },
                {
                    "sent": "Try publishing that study in a medical Journal and it would get rejected right?",
                    "label": 0
                },
                {
                    "sent": "But you only had eight mice in each condition.",
                    "label": 0
                },
                {
                    "sent": "What's wrong with you?",
                    "label": 0
                },
                {
                    "sent": "But people kind of leap to these conclusions, and it turns out that you know if if if medical statistics were more Bayesian, maybe that would be OK, I mean.",
                    "label": 0
                },
                {
                    "sent": "That's somewhat glib.",
                    "label": 0
                },
                {
                    "sent": "The actual size of these.",
                    "label": 0
                },
                {
                    "sent": "This is again a measure of statistical evidence for the model.",
                    "label": 0
                },
                {
                    "sent": "The size of these Bayes factors are not very big, but basically what we're showing here is that the same thing that base inside addition would use to assess the evidence for one model over another.",
                    "label": 0
                },
                {
                    "sent": "People are sensitive to rather fine grained differences in that measure.",
                    "label": 0
                },
                {
                    "sent": "Small differences that a statistician would want to see more evidence before they would make any conclusion.",
                    "label": 0
                },
                {
                    "sent": "But we were sort of leading to these conclusions and we're doing it in ways that are basically exactly well calibrated to the amount of evidence in the data.",
                    "label": 0
                },
                {
                    "sent": "Even in a case that might seem counter intuitive, right?",
                    "label": 0
                },
                {
                    "sent": "Like why should people give you any?",
                    "label": 0
                },
                {
                    "sent": "I mean what you might think is is something like this.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                },
                {
                    "sent": "If you know again, let's say you're a. I'm doing a simple experiment and you have a reasonable sample like 1000 mice were injected and 500 of them that were injected have the gene expressed and 500 were not injected.",
                    "label": 0
                },
                {
                    "sent": "Also have the gene expressed.",
                    "label": 0
                },
                {
                    "sent": "Does the injection cause the gene no, no.",
                    "label": 0
                },
                {
                    "sent": "No way you have great evidence against that, but if it was only if you only injected 8 and you had four out of eight injected, half the gene, four out of eight, not injected to gene, well, then you can't really be sure, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the difference between not being sure and being quite positive that it's not a cause.",
                    "label": 0
                },
                {
                    "sent": "That's the difference in this case here.",
                    "label": 0
                },
                {
                    "sent": "So basically, the reason why this Bayesian model predicts this this interesting effect here is the interaction of two things.",
                    "label": 0
                },
                {
                    "sent": "First of all, it's being Bayesian.",
                    "label": 0
                },
                {
                    "sent": "But it's the fact that we have a very small sample size and the fact that the middle of the scale this 50% line is what we're suggesting is that that's actually people are interpreting.",
                    "label": 0
                },
                {
                    "sent": "That is kind of, you know, no evidence one way or the other, and they're taking the zero point as evidence against an.",
                    "label": 0
                },
                {
                    "sent": "The 100 point is evidence for.",
                    "label": 0
                },
                {
                    "sent": "As opposed to that, make some more sense.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah?",
                    "label": 0
                },
                {
                    "sent": "The axe so so.",
                    "label": 0
                },
                {
                    "sent": "So all all of so again, this is these are very good questions at any colleges to worry about, but I was hoping not to worry about here so.",
                    "label": 0
                },
                {
                    "sent": "All of these models have slightly different units, and there are also sort of sort of transform to normalize the scales to have the same min and Max, so we don't necessarily think that this particular skill that people are using is directly meaningful.",
                    "label": 0
                },
                {
                    "sent": "Basically there's the units of this are measured in.",
                    "label": 0
                },
                {
                    "sent": "You know I mean so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is the measure.",
                    "label": 0
                },
                {
                    "sent": "It's this Bayes factor, but then we're putting it through sort of a nonlinear power law scaling which all the models go through.",
                    "label": 0
                },
                {
                    "sent": "That kind of thing, which takes into account the fact that when people use array to judgment rating scale, they might not use the ratings equally, if that makes sense.",
                    "label": 0
                },
                {
                    "sent": "So there's a weak non linearity for all of these models.",
                    "label": 0
                },
                {
                    "sent": "That's just a parameter that's fit to.",
                    "label": 0
                },
                {
                    "sent": "Two to the data.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Although actually it turns out you don't need that.",
                    "label": 0
                },
                {
                    "sent": "I mean, it turns out that if you if you just computed posterior probability, you would get basically the same thing.",
                    "label": 0
                },
                {
                    "sent": "So if that's not what I'm showing here, but that is true.",
                    "label": 0
                },
                {
                    "sent": "Yeah, like people are amazingly precise, exactly which many subjects together and their role impact really noisy.",
                    "label": 0
                },
                {
                    "sent": "So I didn't collect these data, although we have collected.",
                    "label": 0
                },
                {
                    "sent": "We've replicated this experiment and gotten basically the same data, and I actually don't remember if those are our data or butyrin chains, but.",
                    "label": 0
                },
                {
                    "sent": "Because there's error bars, they probably are data.",
                    "label": 0
                },
                {
                    "sent": "I mean, just not that we were better.",
                    "label": 0
                },
                {
                    "sent": "Just I didn't have the error bars.",
                    "label": 0
                },
                {
                    "sent": "What sorry, what did you ask?",
                    "label": 0
                },
                {
                    "sent": "Yeah, people replaced individuals, so that's a good question.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "So one thing that I don't, I don't know exactly exactly mean.",
                    "label": 0
                },
                {
                    "sent": "Well, so in this experiment, yeah, the basic qualitative shapes are there in individual subjects, so we tested.",
                    "label": 0
                },
                {
                    "sent": "For example, do you have this linear effect in individual Subs?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing we tested is do you have?",
                    "label": 0
                },
                {
                    "sent": "There's a slightly statistically significant kind of you shape here, which is interesting 'cause the Bayesian model predicts that and the other ones don't, so those are those are present in.",
                    "label": 0
                },
                {
                    "sent": "Enough individual subjects more than you'd expect by chance, so I don't.",
                    "label": 0
                },
                {
                    "sent": "I can't exactly tell you what the individual plots look like, but they have the same qualitative shapes.",
                    "label": 0
                },
                {
                    "sent": "Most of them do.",
                    "label": 0
                },
                {
                    "sent": "But yes, there certainly were certainly averaging here on the order of.",
                    "label": 0
                },
                {
                    "sent": "50 subjects those are standard errors, so you know.",
                    "label": 0
                },
                {
                    "sent": "Multiply by sqrt 50 to get the actual standard deviations and judgments and you know there's some significant variation.",
                    "label": 0
                },
                {
                    "sent": "And that's standard thing and cognitive psychology.",
                    "label": 0
                },
                {
                    "sent": "1.25 these ones.",
                    "label": 0
                },
                {
                    "sent": "Listening to his letter.",
                    "label": 0
                },
                {
                    "sent": "I think it's statistically significant mean intuitively, why should the real reason or.",
                    "label": 0
                },
                {
                    "sent": "Well, in general I mean from the point of view of the Bayesian model.",
                    "label": 0
                },
                {
                    "sent": "The reason why an there is a difference in the Bayesian model, but the non linearity kind of obscures it.",
                    "label": 0
                },
                {
                    "sent": "But you know, in general the reason why does kind of well, it's a version of the same kind of thing that was going on here, but it's most dramatic in this case as the base rate decreases, the more dynamic range in a sense, for A cause of the same strength to express itself.",
                    "label": 0
                },
                {
                    "sent": "So you get stronger evidence from the same.",
                    "label": 0
                },
                {
                    "sent": "The same absolute difference in conditional probabilities for the presence of A cause.",
                    "label": 0
                },
                {
                    "sent": "And if you can actually look at the form of the posteriors over the over the model parameters and see that happening there.",
                    "label": 0
                },
                {
                    "sent": "OK, that I hope that helps.",
                    "label": 0
                },
                {
                    "sent": "For this.",
                    "label": 0
                },
                {
                    "sent": "I mean it's worth going through an experiment in some detail, which I guess I should have anticipated that that would be a good idea, but I didn't.",
                    "label": 0
                },
                {
                    "sent": "But if possible I'd like to move on 'cause there's a much more that I want to cover all right.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll tell you very quickly about two other experiments and hopefully many of the same issues are relevant.",
                    "label": 0
                },
                {
                    "sent": "Like if you want to know what the scale is, it's the same.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm going to compare a Bayes factor, a log likelihood ratio to human judgments with some arbitrary nonlinear monotonic scaling.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is also a kind of a causal judgment, but it's one that's maybe a little bit more interesting, a little bit less text booky here we're trying to make sense of peoples.",
                    "label": 0
                },
                {
                    "sent": "Intriguing abilities sometimes remarkable, sometimes frustrating to see hidden causes from coincidences, right?",
                    "label": 0
                },
                {
                    "sent": "We're off were often sensitive to coincidences that in retrospect, turn out to be wrong, right?",
                    "label": 0
                },
                {
                    "sent": "And so when we think of our sense of coincidence, we might think of it as this kind of mystical thing.",
                    "label": 0
                },
                {
                    "sent": "And whenever I write papers on coincidences, I get a lot of attention from the popular press because they think, ooh, that's like real psychology.",
                    "label": 0
                },
                {
                    "sent": "You know, we're mysterious, Paris, something or other.",
                    "label": 0
                },
                {
                    "sent": "There's the most recognition I've ever had in any kind of media which is not saying very much is there's a show called Criminal Minds and.",
                    "label": 0
                },
                {
                    "sent": "Something I said about coincidences was quoted by the by the suspect of the serial killer thing anyway.",
                    "label": 0
                },
                {
                    "sent": "It was on CBS 10,000,000 people watched it.",
                    "label": 0
                },
                {
                    "sent": "So I guess this is a good topic to work on, but but the reason we're interested in this is because so much of our actual causal knowledge of the world we think is driven by again noticing coincidences, which to a statistical point of view are probably not remarkable but do provide some evidence for a hidden cause that's worth exploring.",
                    "label": 1
                },
                {
                    "sent": "So here's an example that may be motivated by something you might you know kind of thing that sometimes gets written up in the press.",
                    "label": 0
                },
                {
                    "sent": "You've maybe heard of cancer clusters or disease clusters, like somebody notes that in a certain city there's an unusually high number of cases of some rare cancer.",
                    "label": 0
                },
                {
                    "sent": "And maybe it's because some evil company was leaking something into the water and we should go and Sue them.",
                    "label": 0
                },
                {
                    "sent": "It turns out most of the time those don't.",
                    "label": 0
                },
                {
                    "sent": "Those claims don't turn out to hold up, but sometimes that kind of reasoning really leads you to a new disease like Lyme disease.",
                    "label": 0
                },
                {
                    "sent": "I don't know if they have Lyme disease here.",
                    "label": 0
                },
                {
                    "sent": "OK, well, this terrible thing.",
                    "label": 0
                },
                {
                    "sent": "My daughter actually had it.",
                    "label": 0
                },
                {
                    "sent": "She was treated for, but it's this thing you can get in the US from being bitten by a certain kind of tick, a deer tick, and if it's untreated it can cause serious neurological damage.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like syphilis, I mean, but.",
                    "label": 0
                },
                {
                    "sent": "You get it from being bitten by a tick.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so so this was only rediscovered, you know, maybe 10 or 20 years ago in Lyme, CT. And that's why it's called Lyme disease.",
                    "label": 0
                },
                {
                    "sent": "But anyway, so imagine that you know this is this is 1 square mile of a city and I'm plotting each incidence of a rare cancer in over a year.",
                    "label": 0
                },
                {
                    "sent": "And I say, well, you see, like there's a little potential cluster over there in the upper left hand corner.",
                    "label": 0
                },
                {
                    "sent": "How much evidence you think there is that there's actually some hidden localized cause some actual thing causing people.",
                    "label": 0
                },
                {
                    "sent": "To get this disease more often in some part of the city than others.",
                    "label": 0
                },
                {
                    "sent": "You know, here I don't know.",
                    "label": 0
                },
                {
                    "sent": "You might say it's not might be strong or weak.",
                    "label": 0
                },
                {
                    "sent": "This was kind of a middle in case.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way we modeled this sort of measure is, we said, OK. Again, we're looking at it's A kind of model selection thing, but one model is just the uniform distribution of these events over this space.",
                    "label": 0
                },
                {
                    "sent": "But another model.",
                    "label": 0
                },
                {
                    "sent": "The interesting one says there's some hidden common cause which is spatially localized, and it's giving rise to some of these events, but not necessarily all of them.",
                    "label": 0
                },
                {
                    "sent": "And this idea of picking out a hidden common cause in a sea of noise.",
                    "label": 0
                },
                {
                    "sent": "Again, a lot of what the brain is designed to do looks like that.",
                    "label": 0
                },
                {
                    "sent": "So the space of possible hidden causes we used were just.",
                    "label": 0
                },
                {
                    "sent": "Gaussians you know, sort of Gaussians of different scale, not rotation in different positions of this space.",
                    "label": 0
                },
                {
                    "sent": "So so sort of a continuous parameter space here varying in mean and variance.",
                    "label": 0
                },
                {
                    "sent": "These are all possible represent all possible causes for these disease clusters and in computing the evidence for this model over that one we have to integrate out those hidden variables so I.",
                    "label": 0
                },
                {
                    "sent": "Won't describe the math for that, but you probably know most of the math you need to know to be able to.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That and I'll just show you the results of an experiment that we did recently for this where we gave people.",
                    "label": 0
                },
                {
                    "sent": "Different, these different patterns and they had to judge on a scale of zero to 10.",
                    "label": 0
                },
                {
                    "sent": "So here are their judgments.",
                    "label": 0
                },
                {
                    "sent": "Again, we don't think of the absolute values as being particularly meaningful, and we're going to take the model predictions and normalize it to have the same min and Max.",
                    "label": 0
                },
                {
                    "sent": "So, but the key thing is that we first of all only gave people a pretty small number of events, so this is these are all weak signals to a traditional statistical analysis, but people give you meaningfully different answers that depend on a function of.",
                    "label": 0
                },
                {
                    "sent": "For example, how many events we give them, or in this case we're varying is the ratio of the number of events that look like they're in a tight cluster to those which aren't.",
                    "label": 0
                },
                {
                    "sent": "So here you've got five tightly clustered events here for three.",
                    "label": 0
                },
                {
                    "sent": "Two, you can barely notice it an one or zero is doesn't really mean anything.",
                    "label": 0
                },
                {
                    "sent": "If the cluster has one, but people across again across a large number of subjects, they show very systematic effect here, increasing the judgment.",
                    "label": 0
                },
                {
                    "sent": "There's a hidden cause as you go this way, and similarly for some of these other things.",
                    "label": 1
                },
                {
                    "sent": "Here's I mean here we wanted to see how much detail we can get, so this is a very very weak effect, but there is a statistically significant effect of just bringing one of those dots over here.",
                    "label": 0
                },
                {
                    "sent": "And anyway, the model makes the same basic predictions.",
                    "label": 0
                },
                {
                    "sent": "Again, just showing that these these intuitions people have, although there we can by any normal statistical standard you know, shouldn't count as evidence in court of law.",
                    "label": 0
                },
                {
                    "sent": "But they're all actually very sensible intuitions.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Under a sensible Bayesian model selection analysis, given a pretty sensible hypothesis base, if possible, hidden causes.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Thanks for showing these.",
                    "label": 0
                },
                {
                    "sent": "Yeah they were showing the plots and they were showing them in different random orders.",
                    "label": 0
                },
                {
                    "sent": "And then somehow in the visual system it doesn't interfere.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm so so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, no, that's a very good question in that one of the so one of the things you might think or you might question you might ask is, is this really a cognitive judgment?",
                    "label": 0
                },
                {
                    "sent": "It's more of a perceptual judgement, right?",
                    "label": 1
                },
                {
                    "sent": "So for this kind of thing I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't think you can draw a principle line there.",
                    "label": 0
                },
                {
                    "sent": "In other work we've actually been interested in what we call a much more perceptual judgments, like modeling things like perceptual organization, using Bayesian principles, and that seems to work well too, but.",
                    "label": 0
                },
                {
                    "sent": "It's a good.",
                    "label": 0
                },
                {
                    "sent": "It's a good question, I mean here.",
                    "label": 0
                },
                {
                    "sent": "In other experiments that we did, and you can, you can see this paper.",
                    "label": 0
                },
                {
                    "sent": "I think there's a reference to hear this cognition.",
                    "label": 0
                },
                {
                    "sent": "2007 paper.",
                    "label": 0
                },
                {
                    "sent": "We did versions of this kind of thing, but we also did a number of more cognitive things where, for example, you know the famous birthday problem, like how many people do you have to have in a room such that the probability is greater than 50%?",
                    "label": 0
                },
                {
                    "sent": "That two of them have had the same birthday.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's much less than the number we have here.",
                    "label": 0
                },
                {
                    "sent": "I think it's 23 or something.",
                    "label": 0
                },
                {
                    "sent": "Is that right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I recently heard a version of this where it says how many do you have to have in the room so that the probability is greater than 50% that two of them have a birthday in the same week.",
                    "label": 0
                },
                {
                    "sent": "Think it is?",
                    "label": 0
                },
                {
                    "sent": "Anyone know what the answer to that is?",
                    "label": 0
                },
                {
                    "sent": "Within seven days of each other.",
                    "label": 0
                },
                {
                    "sent": "Guess.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think it's six so you were out right so the the classic result is supposed to be very counter intuitive and it probably we all may be the first time we heard it thought it was but but our sense was that actually people are pretty well calibrated.",
                    "label": 0
                },
                {
                    "sent": "How big you know the relative magnitude of these coincidences.",
                    "label": 0
                },
                {
                    "sent": "So it's obviously a bigger coincidence to have a birthday on the same day than to be within seven days of each other.",
                    "label": 0
                },
                {
                    "sent": "And in the experiment we did is this was before I heard about this other problem.",
                    "label": 0
                },
                {
                    "sent": "But what we did is we actually gave people patterns of birthdays, so it was just like this.",
                    "label": 0
                },
                {
                    "sent": "But we said, well, suppose you go to a party and you meet people with the following birthdays.",
                    "label": 0
                },
                {
                    "sent": "How much of a coincidence is it?",
                    "label": 0
                },
                {
                    "sent": "We just said how, how strong a coincidence is it and not all the birthdays had a pattern to them.",
                    "label": 0
                },
                {
                    "sent": "But there was some of them that you know there could be a strong pattern like three people on the same day or a week pattern like four people kind of within a month of each other and you get the exactly the same kind of thing.",
                    "label": 0
                },
                {
                    "sent": "But of course you have to have a different hypothesis space of hidden causes.",
                    "label": 0
                },
                {
                    "sent": "There the hidden causes are basically proximity in space.",
                    "label": 0
                },
                {
                    "sent": "Sorry, proximity in time and other kinds of things like you know.",
                    "label": 0
                },
                {
                    "sent": "The same day of the month, but basically it's a space of alternative regularity's and the Bayes factor does just as well a good job predicting judgments there.",
                    "label": 0
                },
                {
                    "sent": "So I think this is a general.",
                    "label": 0
                },
                {
                    "sent": "I mean the ability to detect hidden causes via coincidences in continuous data, whether it's spatial or temporal is something that the brain does all over the place, and computational neuro scientists have proposed that is what learning in the brain is.",
                    "label": 0
                },
                {
                    "sent": "It's coincidence detection.",
                    "label": 0
                },
                {
                    "sent": "I think that goes way too far to say that's all there is to learning in the brain.",
                    "label": 0
                },
                {
                    "sent": "That sort of the whole point of the rest of these lectures is.",
                    "label": 0
                },
                {
                    "sent": "All the things that go beyond finding coincidences, but it is the case that patterns of coincidence in Co occurrence in data are, you know, are good cues to hidden causal structure and I think across cognition and perception we're good at picking those out.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's one other example.",
                    "label": 0
                },
                {
                    "sent": "Again, this is these may be a little less familiar, but this is.",
                    "label": 0
                },
                {
                    "sent": "This is actually a textbook problem from Bayesian statistics.",
                    "label": 0
                },
                {
                    "sent": "If you've heard of the taxi cab problem, how many people heard of that, anyone?",
                    "label": 0
                },
                {
                    "sent": "I think that probably the classic version of this is written up in Sir Harold Jeffreys introduction to theory of probability or something, which is one of the great Bayesian books of the Bayesian.",
                    "label": 0
                },
                {
                    "sent": "Dark ages when you know you weren't allowed to be a Bayesian unless you were a physicist at Cambridge or something.",
                    "label": 0
                },
                {
                    "sent": "So anyway, but here here are some real world examples of these problems.",
                    "label": 0
                },
                {
                    "sent": "We just gave these to people in the questionnaire.",
                    "label": 0
                },
                {
                    "sent": "You read about a movie that's made $60,000,000 to date.",
                    "label": 1
                },
                {
                    "sent": "How much money will it make?",
                    "label": 1
                },
                {
                    "sent": "In total, you see that something's been baking in the oven for 34 minutes.",
                    "label": 1
                },
                {
                    "sent": "How long until it's ready?",
                    "label": 0
                },
                {
                    "sent": "You meet someone who's 78 years old?",
                    "label": 0
                },
                {
                    "sent": "How long will they live?",
                    "label": 0
                },
                {
                    "sent": "Your friend quotes to you from line 17 of his favorite poem.",
                    "label": 1
                },
                {
                    "sent": "How long is the poem?",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So in each of these cases, the formal structure here is that there's some phenomenon or event with an unknown extent or duration teetotal, so that could be the age of the total lifetime of this person.",
                    "label": 0
                },
                {
                    "sent": "But the total amount of money this movie will make.",
                    "label": 0
                },
                {
                    "sent": "And you just observe one random value T that's less than T total.",
                    "label": 0
                },
                {
                    "sent": "That's all you know.",
                    "label": 0
                },
                {
                    "sent": "And the question is, what's the total?",
                    "label": 0
                },
                {
                    "sent": "What's the total amount of money the movie will make?",
                    "label": 0
                },
                {
                    "sent": "Or this if you've heard of the doomsday problem that some philosophers analyze, it's sort of the same problem.",
                    "label": 0
                },
                {
                    "sent": "So how do we analyze this formally from a Bayesian point of view?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, we want to compute the posterior over the total extents given the one sample T, which basically all we know is that it's less than T, so our likelihood is very simple.",
                    "label": 0
                },
                {
                    "sent": "We just assume that T is a uniform random sample between zero and T total, and so so this is 1 / T total, so that integrates up to one over the possible range of observations and at zero if for any value of the total that would be less than T, right?",
                    "label": 0
                },
                {
                    "sent": "So if the total life movie grosses is 50 million, you couldn't couldn't have made 60,000,000 to date.",
                    "label": 0
                },
                {
                    "sent": "Then we have to plug in an appropriate prior and that's going to be sort of interesting.",
                    "label": 0
                },
                {
                    "sent": "The standard textbook prior that, say Jeffries uses what he has come to be called an uninformative prior, which for a variable like continuous variable between zero and Infinity.",
                    "label": 0
                },
                {
                    "sent": "The typical uninformative prior is also has this form of 1 / T total, but for different real world events.",
                    "label": 0
                },
                {
                    "sent": "Part of why we wanted to study this is we can actually measure the actual prior distribution.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the actual prior distribution of these different classes events and see.",
                    "label": 0
                },
                {
                    "sent": "Whether people are sensitive to using informative priors and using the right ones.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just just I'll try to be a little clearer here.",
                    "label": 0
                },
                {
                    "sent": "'cause it's a little subtle on the questions you guys were asking about.",
                    "label": 0
                },
                {
                    "sent": "What's the what's the actual model that we're we're evaluating?",
                    "label": 0
                },
                {
                    "sent": "What's the actual quantity?",
                    "label": 0
                },
                {
                    "sent": "So we compute.",
                    "label": 0
                },
                {
                    "sent": "We take the posterior, which will have this form.",
                    "label": 0
                },
                {
                    "sent": "It's going to be 0 for T total up until T, then it's it's typically going to be have its highest value around T and fall off like this, and the estimator of thetotal that were.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using is the posterior median, so we're taking this point T star where half of the posterior mass is less than T star, and half of it is greater than the star, and we're going to compare that posterior meeting to peoples judgments.",
                    "label": 0
                },
                {
                    "sent": "Extrapolating the total duration from 1 sample T. Is that clear, hopefully.",
                    "label": 0
                },
                {
                    "sent": "OK, we're actually going to compare the posterior median to the median of a large group of subjects.",
                    "label": 0
                },
                {
                    "sent": "It's interesting, actually, that if you look at the variance in subjects, it actually looks a lot like the variance in the posterior, but that's a topic for another time.",
                    "label": 0
                },
                {
                    "sent": "Here we're just looking at the posterior median to the peoples.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Indian guesses.",
                    "label": 0
                },
                {
                    "sent": "So these are these are two different kinds of data, right down here.",
                    "label": 0
                },
                {
                    "sent": "Down here are the behavioral judgments of people.",
                    "label": 1
                },
                {
                    "sent": "And up here are the relevant priors as measured empirically in the world.",
                    "label": 0
                },
                {
                    "sent": "So let's start up here.",
                    "label": 1
                },
                {
                    "sent": "What we chose, problems where we could actually go out and collect publicly available data on for a whole class of events.",
                    "label": 0
                },
                {
                    "sent": "How long they tend to last.",
                    "label": 0
                },
                {
                    "sent": "So you can look on IMDb and see how much do movies have movies made an you see?",
                    "label": 0
                },
                {
                    "sent": "Most movies actually don't make very much money.",
                    "label": 0
                },
                {
                    "sent": "There's kind of this power law distribution.",
                    "label": 0
                },
                {
                    "sent": "Most most movies make significantly less than 100 million dollars, but every so often you have something like Star Wars or out here that makes many hundreds of millions.",
                    "label": 0
                },
                {
                    "sent": "Similar kind of distribution governs the length of poems, so I think we looked at the Oxford Anthology of English Poetry or something like that.",
                    "label": 0
                },
                {
                    "sent": "And you know most poems are pretty short, like 14 lines or something like that, but every so often you get some epic.",
                    "label": 0
                },
                {
                    "sent": "You know thousand line poem, but yeah.",
                    "label": 0
                },
                {
                    "sent": "Of course, other kinds, other classes, events have very different distributions, so lifespans we're pretty familiar with the distribution of lifespans.",
                    "label": 0
                },
                {
                    "sent": "It's approximately normal, but not exactly.",
                    "label": 0
                },
                {
                    "sent": "And there's a little infant mortality spike over here.",
                    "label": 0
                },
                {
                    "sent": "Movie runtime similarly is kind of unimodal.",
                    "label": 0
                },
                {
                    "sent": "Sort of normal like what I'm doing here.",
                    "label": 0
                },
                {
                    "sent": "The red curves are fits within a parametric model of one of a few different types, so these are power law distributions.",
                    "label": 0
                },
                {
                    "sent": "These are gaussians.",
                    "label": 0
                },
                {
                    "sent": "This is a distribution of how long people serve in the US House of Representatives, and it has more of this gamma shape.",
                    "label": 0
                },
                {
                    "sent": "So this is an air long.",
                    "label": 0
                },
                {
                    "sent": "It's a special case of a gamma distribution fit to that thing.",
                    "label": 0
                },
                {
                    "sent": "Here is the baking times of cakes that we collected from a cookbook.",
                    "label": 0
                },
                {
                    "sent": "You can see it doesn't have a simple parametric form.",
                    "label": 0
                },
                {
                    "sent": "But maybe again intuitively familiar, most cakes or the the modal cake takes 60 minutes to bake.",
                    "label": 0
                },
                {
                    "sent": "Most cakes are this sort of, you know, in this big broad 3045 minutes peak.",
                    "label": 0
                },
                {
                    "sent": "And then there's occasional sort of epic 90 minute cakes over there.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now what?",
                    "label": 0
                },
                {
                    "sent": "We can use these priors plugging them in.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know right here.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To then compute.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The posterior and.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looking at the posterior median, and that's what we're plotting down here where we're doing that we're doing that for the models both for the empirical histogram priors and also the best parametric fits.",
                    "label": 0
                },
                {
                    "sent": "These red plots.",
                    "label": 0
                },
                {
                    "sent": "Those are the curves here, and we're comparing that to the black data points, which are the predictions.",
                    "label": 0
                },
                {
                    "sent": "Again, the median production predictions of about 100 subjects who were given different groups of subjects were given a different T value, so some people were told about, you know.",
                    "label": 0
                },
                {
                    "sent": "A poem that was 17 lines, others were told about opponent with seven lines or a poem that was, you know, 65 lines or whatever that is, and they made their judgment and what you can see is that basically across all these different datasets, people people are pretty much in line with the Bayesian predictions, including even for something you know, kind of a little bit weird.",
                    "label": 0
                },
                {
                    "sent": "Like these cake baking time things.",
                    "label": 0
                },
                {
                    "sent": "So that tells us two things.",
                    "label": 0
                },
                {
                    "sent": "One is that in some sense people's intuitions about these everyday predictions are approximately Bayesian.",
                    "label": 0
                },
                {
                    "sent": "But maybe the more interesting thing is that.",
                    "label": 0
                },
                {
                    "sent": "The that they're at least implicitly sensitive to the right priors they know somehow something about the distributions of these different classes of events that enable them to make the right predictions, and you can't get this unless you plug the right priors in, particularly given that there are such different shapes across these different domains, and people seem to get exactly well, more or less the right form of the prediction function, not just.",
                    "label": 0
                },
                {
                    "sent": "The actual values.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, so that's.",
                    "label": 0
                },
                {
                    "sent": "Well, that's pretty much the end of this.",
                    "label": 0
                },
                {
                    "sent": "Of this first unit, and I can take questions on that if you want.",
                    "label": 0
                },
                {
                    "sent": "Or I can.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The hypothesis that a single person has a point prior and those cars that you will yes over here our distribution over the population, so appoint prior meaning like they just think that all problems are the same length or, well, well, OK, so that's a really good question and somebody published a paper arguing version version of that Mike Moser an how pachler published a paper saying, hey, you could get the same thing, probably not with a point prior, but if people just.",
                    "label": 0
                },
                {
                    "sent": "If just people had in memory only a few poems or a few examples of movies they showed, you could fit this data pretty well just using like two or three examples in memory.",
                    "label": 0
                },
                {
                    "sent": "And that's interesting.",
                    "label": 0
                },
                {
                    "sent": "I think there's something interesting deepen right about that, and also something wrong.",
                    "label": 0
                },
                {
                    "sent": "There's lots of other evidence we have, and Thomas published a response to this, showing that people really even within individual subjects.",
                    "label": 0
                },
                {
                    "sent": "You can show evidence that people have these.",
                    "label": 0
                },
                {
                    "sent": "These you have not not exactly, and it's not like everybody has the same distribution fully accurately, but people have very much like a full distribution and he used this very.",
                    "label": 0
                },
                {
                    "sent": "Clever, remarkable method that Adam Sanborn and who I guess Adam isn't here, but some of you may know him.",
                    "label": 0
                },
                {
                    "sent": "He's a postdoc at the Gatsby right now.",
                    "label": 0
                },
                {
                    "sent": "Adam and Tom developed this so called human Markov chain Monte Carlo to show within individual subjects.",
                    "label": 0
                },
                {
                    "sent": "It's a way of using MCMC with individual subsystem app out there prior, and they showed that individual subjects do have priors that look like this.",
                    "label": 0
                },
                {
                    "sent": "But I think there's also something very much right about the point, which is that in this kind of study, and in a lot of other.",
                    "label": 0
                },
                {
                    "sent": "Studies you know these Bayesian cognitive modeling things.",
                    "label": 0
                },
                {
                    "sent": "The data are often consistent with, and sometimes when you look really closely at individual subjects which we can do here.",
                    "label": 0
                },
                {
                    "sent": "They are consistent with the idea that people's inferences are based on taking a very small number of samples from the posterior.",
                    "label": 1
                },
                {
                    "sent": "So I would say it's not the case that people have in mind, or that implicitly in our long-term memory, we only have two samples.",
                    "label": 0
                },
                {
                    "sent": "But what we might bring to mind in anyone case might be a very small number of samples from the posterior, and that's the thing that's emerged across a number of different places and a bunch of us are interested in exploring this idea that.",
                    "label": 0
                },
                {
                    "sent": "That the way people might implement Bayesian inference and learning first of all, might be a kind of Monte Carlo sampling based approximate inference, but also that they might use a very very small number of samples, which to any normal statistician would seem crazy.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "Statisticians work really hard to get.",
                    "label": 0
                },
                {
                    "sent": "To get very large numbers of independent samples.",
                    "label": 0
                },
                {
                    "sent": "But empirically in practice.",
                    "label": 0
                },
                {
                    "sent": "Machine learning people have found that that often very small samples were actually work quite well actually.",
                    "label": 0
                },
                {
                    "sent": "The first time I remember seeing that was a talk that Zubin gave back in grad school.",
                    "label": 0
                },
                {
                    "sent": "Remember, in your pretty sure it was in your factorial learning thing.",
                    "label": 0
                },
                {
                    "sent": "You were doing this really really fast Gibbs sampling with like 10 gig samples or something like that.",
                    "label": 0
                },
                {
                    "sent": "Well, I've made an impact on me.",
                    "label": 0
                },
                {
                    "sent": "But anecdotally, many of you have done.",
                    "label": 0
                },
                {
                    "sent": "MCMC based?",
                    "label": 0
                },
                {
                    "sent": "Learn learning where there's an inner loop of MCMC.",
                    "label": 0
                },
                {
                    "sent": "Inference might have noticed something like that.",
                    "label": 0
                },
                {
                    "sent": "And again, I wasn't planning really to dwell on that, but I'm happy to talk about it and I think it's quite an interesting place where human human learning and machine learning can meet up.",
                    "label": 0
                },
                {
                    "sent": "Sort of thinking about mechanisms for approximate inference and thinking about when you might actually be able to get away with, and maybe even when a cost benefit analysis where you take into account the cost of sampling might actually favor working with very small numbers of samples.",
                    "label": 0
                },
                {
                    "sent": "Each subject was given only one question.",
                    "label": 0
                },
                {
                    "sent": "For each of these topics, but across subjects, they got different subjects, got different numbers plugged in for these different topics, and it was just a one page questionnaire and they.",
                    "label": 0
                },
                {
                    "sent": "You know they wrote a number next to each line.",
                    "label": 0
                },
                {
                    "sent": "The same results when you present these these visually visually, you mean like in a graph or something, I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean in general sorry.",
                    "label": 0
                },
                {
                    "sent": "Didn't mean to laugh at that cause I mean it was a really good question, and in general that's something that we're, you know.",
                    "label": 0
                },
                {
                    "sent": "It's really something to worry about and to think about right, but it's sort of.",
                    "label": 0
                },
                {
                    "sent": "It's both a worry and a good thing, like I think the brain that makes sense to think of the brain as I kind of intuitive statistician.",
                    "label": 0
                },
                {
                    "sent": "And it also makes sense to separately think of the visual system as an intuitive decision.",
                    "label": 0
                },
                {
                    "sent": "And sometimes the visual system might be a smarter statistician than the rest of the brain.",
                    "label": 0
                },
                {
                    "sent": "The more linguistic, higher level part of the brain.",
                    "label": 1
                },
                {
                    "sent": "That's often why it's very helpful to graph our data right.",
                    "label": 0
                },
                {
                    "sent": "Like, I mean, good statisticians and learning people know this.",
                    "label": 0
                },
                {
                    "sent": "Don't just compute numbers, don't just look at tables, but.",
                    "label": 0
                },
                {
                    "sent": "Plot your data and look at it.",
                    "label": 0
                },
                {
                    "sent": "Because you're able to use the very smart unsupervised learning exploratory data analysis stuff that's basically built into your visual system.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I remember seeing a paper where they studied the I don't remember by whom plus where they basically studied, how the brain combines combines information from visual cues.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And they showed they had some some evidence for the brain doing that in a Bayesian way.",
                    "label": 0
                },
                {
                    "sent": "Yeah, only model like cervical models that explain.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Model where you actually like and.",
                    "label": 0
                },
                {
                    "sent": "Didn't even have like some sort of simple Gaussian model, yeah?",
                    "label": 0
                },
                {
                    "sent": "Live a life within prior then yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "There's a few papers like that.",
                    "label": 0
                },
                {
                    "sent": "I wrote one of them, or rather, Conrad cording.",
                    "label": 0
                },
                {
                    "sent": "Wrote one of them when he was a postdoc with me, and nicely put my name on it.",
                    "label": 0
                },
                {
                    "sent": "Ian Murray also had it.",
                    "label": 0
                },
                {
                    "sent": "Had a nice paper like that recently at NIPS with resemble and a few others, and people like David Nil at at Rochester have.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of models like that.",
                    "label": 0
                },
                {
                    "sent": "Alan Yuille has proposed models of basically multimodal sensor fusions.",
                    "label": 0
                },
                {
                    "sent": "Sensory integration.",
                    "label": 0
                },
                {
                    "sent": "I guess actually has been also worked on that in his thesis from a Bayesian standpoint and.",
                    "label": 0
                },
                {
                    "sent": "I mean, yeah, so that's that's a rich, rich literature.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think roughly it's roughly.",
                    "label": 0
                },
                {
                    "sent": "I would say it's certainly similar to these things.",
                    "label": 0
                },
                {
                    "sent": "Conrad, actually, recording described it actually as a causal inference.",
                    "label": 0
                },
                {
                    "sent": "The way the way he set up his model was to say it was.",
                    "label": 0
                },
                {
                    "sent": "It was very much.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like and kind of, I think, inspired by this sort of model or or this sort of model where he said so.",
                    "label": 0
                },
                {
                    "sent": "It's actually sort of like this only the other way around.",
                    "label": 0
                },
                {
                    "sent": "It's like if you you know when you see something like this.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of phenomenon, right?",
                    "label": 0
                },
                {
                    "sent": "You see a visual event like this this contact thing, or imagine like hitting a baseball or cricket ball.",
                    "label": 0
                },
                {
                    "sent": "Something you see a visual event and you also hear a sound and your mind conjoins those two similar kind of thing happens in ventriloquism, right when you see somebody's mouth moving or the failures of this when you see a badly dubbed.",
                    "label": 0
                },
                {
                    "sent": "Moving in a foreign language right?",
                    "label": 0
                },
                {
                    "sent": "So your visual system is actually very good at integrating or sort of deciding.",
                    "label": 0
                },
                {
                    "sent": "Should I integrate this set of visual?",
                    "label": 0
                },
                {
                    "sent": "Events with this set of auditory events, and you can or Conrad suggested modeling that as a kind of Bayesian model selection thing where you're actually comparing hypothesis that maybe there's one cause of both the visual trends.",
                    "label": 0
                },
                {
                    "sent": "Or maybe there's two different causes.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of sort of like this, only slightly different graphical model.",
                    "label": 0
                },
                {
                    "sent": "I think Ian's paper argued that that.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "There was a slightly different way, or I guess there was a debate about whether it's whether there's evidence that people are doing sort of map model selection or actual model averaging, and I think the data are still a little bit ambiguous, but that's one of the things people are arguing about, yeah?",
                    "label": 0
                },
                {
                    "sent": "If you know anyone actually hypothesize what sort of structures in the brain might be used to, well, I don't know if represent problem with it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's so.",
                    "label": 0
                },
                {
                    "sent": "That's so there's a really active area of research on how our probability distributions represented in the brain and how are some of these very basic kinds of things, like just computing posterior in a Gaussian, or about the most sophisticated it's gotten is common filter.",
                    "label": 0
                },
                {
                    "sent": "Like how does the brain do common filtering?",
                    "label": 0
                },
                {
                    "sent": "People like Alex Pouget Raj Rao and Rich Demo has worked on this.",
                    "label": 0
                },
                {
                    "sent": "Peter Diane.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of people interested in that, but for the most part, it's only focused on the very simplest.",
                    "label": 0
                },
                {
                    "sent": "Bayesian statistics.",
                    "label": 0
                },
                {
                    "sent": "Like how do you represent a distribution at all, say in the population code?",
                    "label": 0
                },
                {
                    "sent": "So how might a population of neurons with different tunings represented distribution?",
                    "label": 0
                },
                {
                    "sent": "And how might you just combine a prior and likelihood to get a posterior?",
                    "label": 0
                },
                {
                    "sent": "People are starting to get to some slightly more Interestingly structured models, but nobody's really tried and nobody really has a clue how to represent you.",
                    "label": 0
                },
                {
                    "sent": "Even these kinds of structure learning problems, I think that's.",
                    "label": 0
                },
                {
                    "sent": "Pretty clearly to me at least, the single biggest challenge in computational neuroscience is that problem.",
                    "label": 0
                },
                {
                    "sent": "It's of course a problem to Bayesian computation lessons, but it's more generally a problem of what cognitive science tells us is that human knowledge has all sorts of structure to it.",
                    "label": 0
                },
                {
                    "sent": "Yet when we look at the brain, we don't know how any of that structure is represented, right?",
                    "label": 0
                },
                {
                    "sent": "There's a very nice fit between the kind of conventional machine learning toolkit of learning functions in high dimensional spaces and looking at the brain.",
                    "label": 0
                },
                {
                    "sent": "Where there's you know high dimensional spaces in patterns of neural activation or synaptic strengths, and so there's a nice fit between that.",
                    "label": 0
                },
                {
                    "sent": "You know you could do a lot of computational neuroscience under the idea that it's all about you know regression in high dimensional spaces, but I think cognitive science tells us pretty clearly we need much more structured representations and where machine learning and statistical AI has gone recently is showing us tools for doing that, but bringing that into contact with the brain is complete mystery, and I think we're.",
                    "label": 0
                },
                {
                    "sent": "We have some very speculative ideas, but it's you know, that's that's that's the most important problem to work on.",
                    "label": 0
                },
                {
                    "sent": "I I wouldn't urge you too much to work on it, 'cause it's so hard, but I figure when I run out of sort of things I can actually make progress on.",
                    "label": 0
                },
                {
                    "sent": "That's what I'll spend all my time thinking about.",
                    "label": 0
                },
                {
                    "sent": "Any other questions about this stuff?",
                    "label": 0
                },
                {
                    "sent": "Alright, so let me let me go into the concept learning session.",
                    "label": 0
                },
                {
                    "sent": "Obviously I won't.",
                    "label": 0
                },
                {
                    "sent": "I mean, we only have about 15 min.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to try to make as much of a start on this as I can, will finish it up next time and talk a little about intuitive theories.",
                    "label": 0
                },
                {
                    "sent": "So this was the problem that I used to motivate this.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, learning, learning a category from examples.",
                    "label": 0
                },
                {
                    "sent": "It's interesting because.",
                    "label": 0
                },
                {
                    "sent": "As posed here, it's not the traditional way that classification has been presented in statistics and machine learning, where again the usual you know the textbook picture you start off with is you get a bunch of positive examples in a bunch of negative examples, Anuar either learning a discriminating function, or you're doing something like a density based approach.",
                    "label": 0
                },
                {
                    "sent": "You know Bayesian classifier, where you learn a density model for the positives and density model for the negatives, and then you compute for any new thing the posterior probability by comparing its relative densities under the positive model in the negative model.",
                    "label": 0
                },
                {
                    "sent": "Whereas this doesn't really look like that here, I'm only giving you first of all three examples.",
                    "label": 0
                },
                {
                    "sent": "It's hard to learn a discriminating function for just three examples, although you could learn something if it was very simple, like linear, but I'm not giving you any negative examples, so naive discriminative approaches or most discriminative approaches, even very not naive ones, are not naively applicable to this problem.",
                    "label": 0
                },
                {
                    "sent": "And even generative approaches where you say, learn a density model for the positive examples, these are very complex objects.",
                    "label": 0
                },
                {
                    "sent": "However, we're going to represent these.",
                    "label": 0
                },
                {
                    "sent": "It's seems like it might be a high dimensional space and with three points, what density model are you going to learn?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a puzzle.",
                    "label": 0
                },
                {
                    "sent": "The other hand you know it's not insignificant that I also gave you all the other examples.",
                    "label": 0
                },
                {
                    "sent": "We could call this the test set, or we could call this a semi supervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "Did you guys talk about that kind of semi supervised learning?",
                    "label": 0
                },
                {
                    "sent": "OK, but so does people know that term.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So again, excuse me if I'm going over ground that you covered, but there's a lot of interest recently in problems where you have a lot of unlabeled data and a few labeled examples, so the unlabeled data make it like an unsupervised structure discovery problem.",
                    "label": 0
                },
                {
                    "sent": "But the actual task you have to solve is defined by the small number of labeled examples.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "The interesting computational challenge is to figure out how to extract something from all of these unlabeled examples that will help me generalize in a much more useful way.",
                    "label": 1
                },
                {
                    "sent": "The concept I'm learning from these few label examples, and I think one way to approach this problem that takes us pretty quickly to the state of the art in machine learning, is thinking of this as an interesting kind of semi supervised learning problem.",
                    "label": 1
                },
                {
                    "sent": "So we'll talk about talk about that, but the particularly the Bayesian approach to semi supervised learning that we're interested in really makes it an unsupervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "It's really about and in this sense, maybe my initial framing the problem was a bit of a cheat, but it's really about what can we learn about the structure of the world from these data and then.",
                    "label": 0
                },
                {
                    "sent": "You know we'll use these labels in some clever way to go from that, but the real work is being done by coming up with the right hypothesis space in a sense, from the.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data now here's a very simple approach to solving this kind of problem.",
                    "label": 0
                },
                {
                    "sent": "And I'll just illustrate it naively with a picture.",
                    "label": 0
                },
                {
                    "sent": "So let's say we've got, you know our objects are points in a 2 dimensional space here.",
                    "label": 0
                },
                {
                    "sent": "So each of these data points is one of these objects.",
                    "label": 0
                },
                {
                    "sent": "And let's say you.",
                    "label": 0
                },
                {
                    "sent": "So you get mostly unlabeled data and just one labeled example, so that one here and I say that sub licat or two file here it's applicate.",
                    "label": 0
                },
                {
                    "sent": "OK, so which are the other buckets you tell me?",
                    "label": 1
                },
                {
                    "sent": "OK, so is this a blanket?",
                    "label": 0
                },
                {
                    "sent": "Is that a blanket?",
                    "label": 0
                },
                {
                    "sent": "Is that a blanket?",
                    "label": 0
                },
                {
                    "sent": "How to block it?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great good, that was easy.",
                    "label": 0
                },
                {
                    "sent": "Alright, so So what?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Easy.",
                    "label": 0
                },
                {
                    "sent": "Well basically you look at this and you see these three clusters kind of pop out at you, right?",
                    "label": 0
                },
                {
                    "sent": "And if you're able to identify the clusters from the unlabeled data, which in this case you were and you have some prior abstract knowledge that tells you that word labels pick out clusters, the clusters you can identify, then all I need is 1 example right to tell you and only one positive example that's basically just telling you which is the right cluster to label and versions of this approach have again been developed in machine learning.",
                    "label": 1
                },
                {
                    "sent": "The first one that I know about was again work that Zubin did in grad school alot.",
                    "label": 0
                },
                {
                    "sent": "I guess a lot of the work that we're doing is kind of footnotes on ghahremani.",
                    "label": 0
                },
                {
                    "sent": "But sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, just kidding, but but you know Zubin I really like this technical report and I don't know if it was ever published outside of a tech number, but it's a great TR.",
                    "label": 0
                },
                {
                    "sent": "Maybe there was an IP paper or something that in which zuben talked about.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think it was the 1st paper I know to use.",
                    "label": 0
                },
                {
                    "sent": "Gaussian mixture models or multinomial mixtures to think about unsupervised categorization and also supervising problems which were essentially these semi supervised problems and using EM back when that was like the.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I'm just making both of us killed them was the hot thing OK?",
                    "label": 0
                },
                {
                    "sent": "And more recently there's been a lot of interest in these nonparametric mixture models, so I think you I started to tell you about Dirichlet process mixtures and so on, and I think he didn't quite finish telling you about them, but I'm going to assume he told you about them, but I think for the purposes of what I'm doing, you probably know most of what you need to know.",
                    "label": 0
                },
                {
                    "sent": "But anyway, the basic idea is these are models which, unlike a finite mixture, have actually an infinite number of.",
                    "label": 0
                },
                {
                    "sent": "Of components.",
                    "label": 0
                },
                {
                    "sent": "But in a sense, when you do posterior inference conditioned on some finite sample, only a small finite number of those infinite components are effectively present.",
                    "label": 0
                },
                {
                    "sent": "The effective number of degrees of freedom is is much smaller and kind of always sort of just the right size for the data you have.",
                    "label": 0
                },
                {
                    "sent": "Assuming that the model predicted the model assumptions are appropriate.",
                    "label": 0
                },
                {
                    "sent": "So Radford Neal introduced some of these things, but also Carl Rasmussen and.",
                    "label": 0
                },
                {
                    "sent": "Starting about 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So that's so.",
                    "label": 1
                },
                {
                    "sent": "This is one place where cognitive scientists again have been inspired by the same kinds of ideas that machine learning people have been developing with finite an infinite mixtures.",
                    "label": 0
                },
                {
                    "sent": "Trysting thing is actually the psychologists thought of these things.",
                    "label": 0
                },
                {
                    "sent": "First, in a certain sense.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there was this paper by Fried in Holyoke in the early 80s.",
                    "label": 0
                },
                {
                    "sent": "You know, back when zoom in and I were like in high school or something in which they proposed, it was a little bit less elegant, but they proposed basically M like algorithm for learning in a Gaussian mixture that essentially was a model of unsupervised and semi supervised categorization.",
                    "label": 1
                },
                {
                    "sent": "And then there's another psychologist, John Anderson proposed something which was mathematically equivalent to a Dirichlet process mixture, but he had no idea what that was, but.",
                    "label": 0
                },
                {
                    "sent": "He just kind of derived it from first principles.",
                    "label": 0
                },
                {
                    "sent": "It's pretty remarkable he's a smart guy.",
                    "label": 0
                },
                {
                    "sent": "And he showed that that could be all.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We used to describe human learning.",
                    "label": 0
                },
                {
                    "sent": "I'll just illustrate a little bit about how this goes.",
                    "label": 0
                },
                {
                    "sent": "This is a typical, extremely boring, trivial computationally trivial category learning problem that people have studied in cognitive psychology.",
                    "label": 0
                },
                {
                    "sent": "Going back to the 50s.",
                    "label": 0
                },
                {
                    "sent": "Again, this this looks more like a traditional machine learning problem, and there's a few positive examples, a few negative examples, so this is the category label.",
                    "label": 0
                },
                {
                    "sent": "Here you have a training set and the test set the actual stimuli in these experiments.",
                    "label": 0
                },
                {
                    "sent": "Are formally represented by a vector of a few binary features.",
                    "label": 0
                },
                {
                    "sent": "They actually look like in the lot of the versions, including the first studies which were done by Jerome.",
                    "label": 0
                },
                {
                    "sent": "Bruner and colleagues are these things which look like basically cards in the game of set.",
                    "label": 0
                },
                {
                    "sent": "People know that card game set right now.",
                    "label": 0
                },
                {
                    "sent": "Raise your hand if you do.",
                    "label": 0
                },
                {
                    "sent": "I'm just curious, OK, I guess it's more popular in the states.",
                    "label": 0
                },
                {
                    "sent": "It's it's a game with.",
                    "label": 0
                },
                {
                    "sent": "Basically you have these cards and they have one or two or three shapes, and they're either circles or triangles.",
                    "label": 0
                },
                {
                    "sent": "That kind of thing that actually ovals whatever they have different colors.",
                    "label": 0
                },
                {
                    "sent": "They could have different shading, and there's some concept that's defined in this low dimensional discrete vector space.",
                    "label": 0
                },
                {
                    "sent": "Alright, so how might you apply a one of these nonparametric mixture model?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here, well, this is.",
                    "label": 0
                },
                {
                    "sent": "This is the way Anderson did it, and what's interesting is he he did it in the context of a a certain kind of approach to inference, which you could think of as a greedy sequential search, or as Sanborn and colleagues showed, you can think of it as a particle filter with one particle.",
                    "label": 1
                },
                {
                    "sent": "You probably don't.",
                    "label": 0
                },
                {
                    "sent": "I think you haven't yet learned what particle filters are.",
                    "label": 0
                },
                {
                    "sent": "There are kind of sequential Monte Carlo approach, but I think you will be learning about them very soon, right hope?",
                    "label": 0
                },
                {
                    "sent": "And they're particularly elegant approaches to approximate inference to apply in a setting like this where you get examples coming in sequentially one at a time, and you want to grow out your hypothesis as the data come in in ways where the complexity of your hypothesis grow sort of just when you need it.",
                    "label": 0
                },
                {
                    "sent": "So I'll just show this intuitively here, but hopefully this will be a useful intuition for when you.",
                    "label": 0
                },
                {
                    "sent": "See this more formally, so what's going on here?",
                    "label": 0
                },
                {
                    "sent": "This is what this tree is showing.",
                    "label": 0
                },
                {
                    "sent": "Is the process that Anderson says is going on in the human learners mind, but it's also an actual legitimate approximate inference scheme for growing your mixture model as as examples come in.",
                    "label": 0
                },
                {
                    "sent": "So these rows of these these tables are Boolean vectors specifying the features of the objects.",
                    "label": 0
                },
                {
                    "sent": "So here are the first object comes in and it has its features are 11111 and.",
                    "label": 0
                },
                {
                    "sent": "And what we're seeing is as new objects come in.",
                    "label": 0
                },
                {
                    "sent": "The learner has to decide.",
                    "label": 0
                },
                {
                    "sent": "Am I going to put this object in one of the clusters I already have, or am I going to make a new cluster?",
                    "label": 0
                },
                {
                    "sent": "And of course, you're not literally making a new cluster, because in one of these nonparametric models, there's actually an infinite number of classes.",
                    "label": 0
                },
                {
                    "sent": "You're just discovering one that you hadn't seen before, and what you're deciding is in the posterior.",
                    "label": 0
                },
                {
                    "sent": "What's more likely is this.",
                    "label": 0
                },
                {
                    "sent": "Is this a cluster that I just haven't seen yet, or is it?",
                    "label": 0
                },
                {
                    "sent": "Should I think of this as an instance of a cluster, or a mixture component that I've already seen?",
                    "label": 0
                },
                {
                    "sent": "And so you're always evaluating when a new object comes in.",
                    "label": 0
                },
                {
                    "sent": "You're always evaluating it with respect to the clusters.",
                    "label": 0
                },
                {
                    "sent": "You have an the new one, and the the way the Dirichlet process is defined.",
                    "label": 0
                },
                {
                    "sent": "Gives you a natural tradeoff between basically a prior on how objects should be partitioned, or you know which which are the draws from the base distribution.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure exactly which terminology you've seen yet.",
                    "label": 0
                },
                {
                    "sent": "Versus the kind of fit that's like that's one of the complexity terms, and then there's kind of the fit to the data where you want each cluster to be as clean as possible.",
                    "label": 0
                },
                {
                    "sent": "In.",
                    "label": 0
                },
                {
                    "sent": "The idea is that the probability is highest when all the things in a cluster are very similar, because then the cluster can very tightly concentrate its probability just on what those things look like, rather than spreading its probability mass over a very broad set of different things.",
                    "label": 0
                },
                {
                    "sent": "So you can see this going on here.",
                    "label": 0
                },
                {
                    "sent": "What the tree is showing is it's tracing out the map hypothesis, the.",
                    "label": 0
                },
                {
                    "sent": "That the single highest posterior probability hypothesis as examples come in the first example is this just of course in its own cluster.",
                    "label": 0
                },
                {
                    "sent": "Then you see 10101 and the question is, should I put that?",
                    "label": 0
                },
                {
                    "sent": "Should I think that's that's in the same mixture component as the first one, or is it in a new one and?",
                    "label": 0
                },
                {
                    "sent": "Here the best solution is to put it with the other one, basically because it has more features in common with that one.",
                    "label": 0
                },
                {
                    "sent": "Then not right if this one.",
                    "label": 0
                },
                {
                    "sent": "If the second, we're all zeros, it would probably be better to put it in a different cluster, because putting it in the same cluster as this one would require that cluster to have its probability mass spread out too much.",
                    "label": 0
                },
                {
                    "sent": "Now you get 10110, and again it seems to be slightly better to put it in with this one then to make a new one.",
                    "label": 0
                },
                {
                    "sent": "But here you get a case when now now you've seen basically you've seen the first 3 all have mostly ones and then you see one.",
                    "label": 0
                },
                {
                    "sent": "That's all zeros.",
                    "label": 0
                },
                {
                    "sent": "And now it's better to make a new cluster, because if you again, if you put that one in with the other with the previous examples, then that cluster would just have to get too broad, so it's better to make a very tightly focused sort of all zeros cluster.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "You know, basically we keep going here.",
                    "label": 0
                },
                {
                    "sent": "You get the 5th example is has just one one so again it gets put in with the all zeros.",
                    "label": 0
                },
                {
                    "sent": "So we sort of have a cluster with prototype is looking like it's sort of zeros and another one whose prototype is looking like it's once and so hopefully that's fairly intuitive and I think over the next couple of days you'll see the math necessary to understand why this is a greedy greedy meaning at each step.",
                    "label": 0
                },
                {
                    "sent": "As an example comes in, you're sort of making the best.",
                    "label": 0
                },
                {
                    "sent": "Choice the maximum posteriori choice, but it's also the kind of thing you would get if you were actually doing a sequential Monte Carlo or particle filter sample.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you'll see how that works.",
                    "label": 0
                },
                {
                    "sent": "I guess I'm just about to to end here.",
                    "label": 0
                },
                {
                    "sent": "I'll just show you an example of this that Griffiths at all showed elegantly explained some otherwise puzzling data in the psychology literature, and I think.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then I'll save the rest for next time.",
                    "label": 0
                },
                {
                    "sent": "So here's the example that they showed.",
                    "label": 0
                },
                {
                    "sent": "It's let me first set the context for this, which is.",
                    "label": 0
                },
                {
                    "sent": "Kind of like in the causal learning example that I showed at the beginning.",
                    "label": 0
                },
                {
                    "sent": "Cognitive psychologist love to have these debates between two models, each of which two machine learning people often look kind of trivial.",
                    "label": 0
                },
                {
                    "sent": "But the but Congress psychologists will debate about these for years or decades.",
                    "label": 0
                },
                {
                    "sent": "Which is it?",
                    "label": 0
                },
                {
                    "sent": "Is it this one, or that one and machine learning person might say, well, probably both of those are way too simple, and sometimes it's going to be one and sometimes going to the other, and usually in the real world you want something more complex that maybe generalizes both of them.",
                    "label": 0
                },
                {
                    "sent": "So here's another example of that kind of dynamic.",
                    "label": 0
                },
                {
                    "sent": "Two very popular models.",
                    "label": 0
                },
                {
                    "sent": "In psychology are what are called prototypes and exemplars, and each of them also corresponds to a textbook statistical pattern recognition model.",
                    "label": 0
                },
                {
                    "sent": "Prototypes are basically a naive Bayes model that says each category has a has a single prototype, which in this in the binary case you would just.",
                    "label": 0
                },
                {
                    "sent": "Correspond to the.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "If you think of it as a naive Bayes model, it's basically just specifying the probability of each of these features independently in the category, but then the prototype is just the thing that you know the the mean of that vector or the mode if you like.",
                    "label": 0
                },
                {
                    "sent": "So here for a category category A, which are these examples here?",
                    "label": 1
                },
                {
                    "sent": "It seems pretty clear the prototype should be this vector of all zeros and the prototype for B should be the vector of all ones, and if you imagine learning a naive Bayes classifier, that's.",
                    "label": 0
                },
                {
                    "sent": "You know what you get is 1 category.",
                    "label": 0
                },
                {
                    "sent": "A category would say for each feature independently, it's most likely to be 0 and for the one it's most likely be one.",
                    "label": 0
                },
                {
                    "sent": "An exemplar model is like a.",
                    "label": 0
                },
                {
                    "sent": "Well, there's different versions of it, but it's like a kernel density estimator for a Bayesian classifier.",
                    "label": 0
                },
                {
                    "sent": "It basically says in psychology what well.",
                    "label": 0
                },
                {
                    "sent": "I guess I should tell you what these things are to psychologist.",
                    "label": 0
                },
                {
                    "sent": "Psychologist prototype model is an image you have in your mind of the category prototype, and you judge whether something is in the category by comparing its similarity to that one prototype.",
                    "label": 0
                },
                {
                    "sent": "Psychologically and exemplar model is used or in memory instances of all the things you've ever seen tagged with which category they are?",
                    "label": 0
                },
                {
                    "sent": "And then when the new thing comes in, you compute its similarity to the examples of Category A and the examples of Category B and you say it's most likely to be a if it's more similar on average to the AIDS, then to the bees.",
                    "label": 0
                },
                {
                    "sent": "But of course if you think about it, that looks a lot like a kernel density estimate or taking a kernel density estimate of the density of category A in this feature space, and then also the density of category B and seeing.",
                    "label": 0
                },
                {
                    "sent": "Which is a new out is a new object more likely under the category a distribution of the Category B distribution and it's for formerly equivalent, so that formal equivalence had been established in the mathematical psychology literature between prototype models and essentially naive Bayes or mixture models and exemplar models.",
                    "label": 0
                },
                {
                    "sent": "And these kernel density classifiers.",
                    "label": 0
                },
                {
                    "sent": "But there are all sorts of puzzling phenomena of sometimes human learning looks better described by 1.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it looks better described by the other, and sometimes it seems to.",
                    "label": 0
                },
                {
                    "sent": "Change as learning goes on in ways that if you think about it, sort of seems like you have to be.",
                    "label": 0
                },
                {
                    "sent": "You have to see versions of this if you're dealing with learning over a long time scale for Occam's razor, sorts of reasons, exactly the kind of things that nonparametric Bayes was introduced into machine learning to try to solve.",
                    "label": 0
                },
                {
                    "sent": "So you know if you think about it, a prototype model is about the simplest possible model of what a category is.",
                    "label": 0
                },
                {
                    "sent": "It just says essentially a category of objects is just one single thing plus random variation.",
                    "label": 0
                },
                {
                    "sent": "So it's really really simple notion of the category, whereas exemplar models are quite complicated.",
                    "label": 0
                },
                {
                    "sent": "They could.",
                    "label": 0
                },
                {
                    "sent": "They could model, you know, various different.",
                    "label": 0
                },
                {
                    "sent": "If you think about it as a kernel density classifier, the density functions could be just about anything, right?",
                    "label": 0
                },
                {
                    "sent": "If you think about the discriminating boundaries that are induced by doing kernel density classification, there again can approximate any nonlinear discriminating function.",
                    "label": 0
                },
                {
                    "sent": "If you think about it psychologically.",
                    "label": 0
                },
                {
                    "sent": "A prototype model makes a very strong commitment to say what a dog is a dog is this thing.",
                    "label": 0
                },
                {
                    "sent": "I imagine I have this platonic dog in mind, where as an example, our model says almost nothing about what a dog is.",
                    "label": 0
                },
                {
                    "sent": "A dog is just all the dogs I've ever seen and a dog is just something like those, right?",
                    "label": 0
                },
                {
                    "sent": "So this is so in some sense, the difference between prototype and exemplar models are just a spectrum of the simplest possible representation to the most complex possible representation, and what you see as a human learning goes on is in some sense.",
                    "label": 0
                },
                {
                    "sent": "You often see things like this, which would show a shift from people starting off with the kind of a first simple approximation to a category, and then they overtime they learn a more complex, perhaps more accurate, hopefully more accurate, not overfit representation of the category, and that's illustrated in this in this sort of experiment where the stimuli were set up to have.",
                    "label": 0
                },
                {
                    "sent": "To basically fit a nice prototype structure except for one exception.",
                    "label": 0
                },
                {
                    "sent": "So notice this.",
                    "label": 0
                },
                {
                    "sent": "This white one here.",
                    "label": 0
                },
                {
                    "sent": "The exceptions are indicated by triangles.",
                    "label": 0
                },
                {
                    "sent": "The category is indicated by the color, so this white triangle is a white one that looks more like a black one, and the black triangle is a black one that looks more like a white one.",
                    "label": 0
                },
                {
                    "sent": "And basically you can see that it looks just as if these are all single one bit off from the category prototype, and then you just flopped sort of switched at birth.",
                    "label": 0
                },
                {
                    "sent": "One of the one of the.",
                    "label": 0
                },
                {
                    "sent": "White ones for the black ones and vice versa, right?",
                    "label": 0
                },
                {
                    "sent": "So what happens when you give these examples to people and the here?",
                    "label": 0
                },
                {
                    "sent": "What you do is you're just giving them examples to classify in an online setting and they have a number of different blocks of the experiment, so they're seeing the same stimuli over and over again, each curve going along the X axis corresponds to it.",
                    "label": 0
                },
                {
                    "sent": "Another run through the stimuli.",
                    "label": 0
                },
                {
                    "sent": "So if you like here at the beginning, they've only seen each exception once.",
                    "label": 0
                },
                {
                    "sent": "And over the course of the experience, they start to see all the objects, but in particular the exceptions more often.",
                    "label": 0
                },
                {
                    "sent": "So what happens is at the beginning of learning, the exceptions are just treated just like any other instance of the the prototype of the other category.",
                    "label": 0
                },
                {
                    "sent": "So this one, even though it's labeled B or black subjects, are categorizing it as an A or a white one, and similarly this one is being categorized with the bees and they're getting feedback saying no, that's wrong, but it doesn't sink in immediately, right?",
                    "label": 0
                },
                {
                    "sent": "So for the first few blocks of trials, there just subjects are just.",
                    "label": 0
                },
                {
                    "sent": "Basically ignoring the feedback, telling them that they were wrong on these things, but then overtime they start to they start to both solidify the prototypes of A&B categories, but also realize gradually that actually Nope, this one really is a B and this one really isn't a an innocence.",
                    "label": 0
                },
                {
                    "sent": "What or the way psychologists founded this before is that there were kind of two different things going on.",
                    "label": 0
                },
                {
                    "sent": "There was early extraction of a prototype of each category and then later on more of an exemplar representation that was developing, which is.",
                    "label": 0
                },
                {
                    "sent": "Basically, the exemplar model describes the final state and the prototype model looks most like the beginning state.",
                    "label": 0
                },
                {
                    "sent": "But what Tom and colleagues showed is that this could be very naturally captured with one of these Dirichlet process mixture models, where basically what's going on is that at the beginning you're only finding two categories, and as it goes on, it's becoming more and more likely that you're going to introduce a separate category just for the exception.",
                    "label": 0
                },
                {
                    "sent": "So at the end, you've actually got his four components in the mixture, two of which.",
                    "label": 0
                },
                {
                    "sent": "Are associated with the a label and one with the B label, but it takes awhile for the exceptions to become their own category, 'cause you need to see you know the prior in the richly process.",
                    "label": 0
                },
                {
                    "sent": "Initially favors just lumping them into a small number of categories.",
                    "label": 0
                },
                {
                    "sent": "That's the concentration feature of the directly process, but once you get once you see enough instances of these exceptions, then it becomes.",
                    "label": 0
                },
                {
                    "sent": "It becomes more and enough instances of the whole thing.",
                    "label": 0
                },
                {
                    "sent": "Then the general shape of the distribution says OK, Now it's better.",
                    "label": 0
                },
                {
                    "sent": "It's now.",
                    "label": 0
                },
                {
                    "sent": "It's more likely that there's actually a separate small category that's just generating those exceptions, and by carving them off from the rest of the objects in the A and the B set, you're able to get an overall much better fit.",
                    "label": 0
                },
                {
                    "sent": "It's essentially an automatic Occam's razor that's being illustrated over the course of this experiment, and it's just one of many places where this sort of idea that emerges very naturally from.",
                    "label": 0
                },
                {
                    "sent": "Modern Bayesian machine learning seems to also be built into our heads.",
                    "label": 0
                },
                {
                    "sent": "OK, I see we're pretty much overtime, so we should stop.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll pick up with categorization next time and I'll be happy to talk with you over the lunch break or anytime between now and then.",
                    "label": 0
                }
            ]
        }
    }
}