{
    "id": "dos62mm47vbpjafvibi2xtcvdjdbsiiy",
    "title": "Mining Topics in Documents: Standing on the Shoulders of Big Data",
    "info": {
        "author": [
            "Zhiyuan (Brett) Chen, Department of Computer Science, University of Illinois at Chicago"
        ],
        "published": "Oct. 8, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_chen_mining_topics/",
    "segmentation": [
        [
            "Today I'm going to talk about mining topics in documents, standing on the shoulders of big Data, and Jim Bratton, an advisor, is professor."
        ],
        [
            "Renew.",
            "So topic models have been widely used to discover topics in many applications and most of the topic models unsupervised."
        ],
        [
            "However, topic models require a large amount of documents in order to work.",
            "Usually thousands of documents, and without such a large amount of documents it may generate many incoherent."
        ],
        [
            "But not every task application has such a large amount document.",
            "For example, if we want to find out the product features from the review.",
            "For example, given the reviews of the camera, we may want to find the product features such as batteries, screen and lens.",
            "In this task, many if you go to Amazon.com ebay.com, you will find that many products do not even have more than 100 reviews and we found that with 100 reviews, LG performs very poorly.",
            "So."
        ],
        [
            "So here's the question.",
            "We want to ask since the data is so small in one domain, can we help the modeling using big data?"
        ],
        [
            "Here's our intuition, so let's go back to how human learn.",
            "Let's say a person sees a new situation and it will heal.",
            "She will use the previous experience to solve the problems in this new station.",
            "For example, if someone goes to a new country and it has no idea what language is spoken in that country but use his previous years of experience, he can.",
            "He can figure out that this is probably restaurant in this hotel, and this is the airport.",
            "So even if he has no idea what's going on in that particular.",
            "She."
        ],
        [
            "So we want to borrow the similar idea when model season new domain.",
            "You can also use the data of many previous domain to help the learning in our new domain which we refer us the big data.",
            "So here when the model see the laptop domain it should not be a totally new domain because email already see some related domains such as cell phone, desktop and so that's why we were trying to use the big data here refers to a large number of domains with wide variety.",
            "To help in the learning in a new domain."
        ],
        [
            "So here's our motivation.",
            "Want to propose a model that learns as humans do, which is also refers to lifelong learning?",
            "Sort of so it is the level learning is to return the result, and then in the past and use it to help.",
            "And learning in the future."
        ],
        [
            "So here's our proposed model flow.",
            "We want.",
            "First of all, we retain the topics learned in the previous domains and then we learn the knowledge in these topics.",
            "And at last we apply the knowledge to a new domain."
        ],
        [
            "So here's one question.",
            "What's the knowledge application in our knowledge?"
        ],
        [
            "Raise.",
            "So again we let's go back to human learning.",
            "How does a baby then or gain knowledge so one of the very basic knowledge is should also not so the baby learns the knowledge and knowledge that he or she should shoot.",
            "Also not do something based on the reaction of the parents.",
            "So we."
        ],
        [
            "Have the same idea here for the shoot we use the mass link that is the tools in the must link should be put in the same topic because they have the semantic correlation.",
            "An follow cannot link.",
            "That is to not let the tools in the Canonical sloppy to put together in the same topic."
        ],
        [
            "So how can we extract the such knowledge could automatically?",
            "So here's the motivation of person learns the knowledge when it happens repetitively.",
            "That is, when someone tells you something, you may get it, but when many people keep telling you that you will learn this knowledge and remember it.",
            "So that's how we extract our knowledge that we will instruct the new level knowledge that appears repentant frequently.",
            "So we apply the well known data mining techniques called frequent itemset mining."
        ],
        [
            "So it is to discover the Freedom water standard with the word happening happen frequently in the same topic, and we specifically we apply the multi minimum support, frequent itemset mining and it will be used to directly minor must links."
        ],
        [
            "However, cannot things are much more challenging because most words are not correlated with most other words.",
            "For example, only words that are related with price will be correlated with cheap and most other words are not tolerated correlated with cheap.",
            "So that's why if vocabulary sizes V we have all the square cannot links in total.",
            "But given a new domain, most of words do not appear in the new domain, new domain that.",
            "So that's why we don't need to extract canonic for.",
            "All the words we focused on those top topical words in that particular domain and check it cannot links for them."
        ],
        [
            "So here are some two related words about Canonical.",
            "There are several topic models that are proposed to deal with math link, but there only two topic models that will propose to deal with cannot type of link cannot type of knowledge.",
            "First one is dividend sequence.",
            "The other day, but."
        ],
        [
            "And assume their knowledge to be correct and manually provided.",
            "So you can see our knowledge is automatically extracted, so there's no guarantee that the knowledge has to be 100% correct."
        ],
        [
            "So that's why we need to do the knowledge verification, so here's a motivation of persons.",
            "Experience may not be applicable to a particular particular situation.",
            "So, for example, someone may have the knowledge of the meaning of 1 gesture, but when he goes to a new country, the meaning of that gesture may change due to the different culture.",
            "So so let's similar problem here.",
            "We need to verify the knowledge when we have the new domain."
        ],
        [
            "So to verify the mathlink, first we build a must link graph.",
            "That is, each vertex is a must link and the edge shows the relationship between the math link.",
            "For example the graph.",
            "Here we have bank money and banking finance.",
            "There's edge between them because the bank in both must links mean the same thing, while the bank River means a totally different thing.",
            "How can we construct such graph so it is that the must link bank money and must link bank front lines should be extracted from.",
            "Almost seems out of topics while the other one Bacon River should be extracted from a totally different set of topics."
        ],
        [
            "And then further more, we use pointwise mutual information to estimate the correctness of the mass link and the positive PMI indicates a positive semantic correlation and it will be used in the proposed Gibbs sampler later."
        ],
        [
            "And then for the Canonical again, Canadians are much more challenging due to the power law distribution of natural language words.",
            "Most word do not Co occur with most other words, so that's why local occurrence cannot be used to estimate the correctness of economic.",
            "Now that's why we dynamically verify the Canonical in inside the inference step."
        ],
        [
            "So here's our proposed Gibbs sampler for our topic model.",
            "It's called M GPU, multi generalized, polya model.",
            "The idea is therefore must link.",
            "We want to increase the probability of both words in the mouth link and follow Canonical.",
            "We want to decrease the probability of one of the words in the can."
        ],
        [
            "So here's one example.",
            "If you if we see the topic, if we see the word speed under topic zero given the mathlink less speed, faster correlated, which increase the probability of word fast under topic zero will decrease the probability of.",
            "Beauty under topic zero.",
            "Given Canonical app given controlling of speed and beauty."
        ],
        [
            "So here are the steps for the forum US link.",
            "So here's the how we increase the probability.",
            "So here's the conditional probability of the Gibbs sampler.",
            "It has two parts, the first part.",
            "This document."
        ],
        [
            "Distribution, the difference is the second one.",
            "We added summation here.",
            "That is we consider the related words that represented by month links and also."
        ],
        [
            "We have the Lambda matches here which is based on the PM value I introduced before."
        ],
        [
            "So for the Canonical we want to transfer the cannot link word into some other topics that have higher probability of this word."
        ],
        [
            "So that's why we have an indicator function here to constrain that the world has to be transferred further to one of those topics.",
            "But what if there is no topic that has higher probability of this word?"
        ],
        [
            "So MSMOD will increase the number of topics an create a new topic.",
            "We don't do that in this case because our cannot links may be incorrect.",
            "For example, economics may ask us to separate the battery in life, but in this case we don't follow chaotic when there's a conflict, because we assume that Canonical is not reliable."
        ],
        [
            "So here are some evaluations.",
            "We have 100 product domains instructive on Amazon.com, 50 of them about electronics such as computers, cell phone, camera and film and our non electronics such as clothes, diaper bikes.",
            "My stomach has 1000 reviews here, so each domain works as a test domain.",
            "We randomly sampled 100 views as a small data and feed it to all the topic models and the knowledge is checked from the big data.",
            "The from the 1000 reviews from the other domains.",
            "That is, we want to simulate the situation that we extract knowledge from big Data applied to the small data."
        ],
        [
            "So for a model comparison, so the first one MC is our proposed model with most Lincoln Alex Mercer Link only and the next four models are knowledge based topic model, they can use the most knowledge and some of them can use both must link and cannot link.",
            "So the last one is well known LDA model."
        ],
        [
            "So first of all, we use the topic coherence too.",
            "To do the measurement so topical as was proposed in 2011, an it shows it shows highly correlated with human judgment and the high score high topic current score indicates better or more coherent topics."
        ],
        [
            "So here is the 1st result.",
            "So the Y axis here is the topic occurrence.",
            "Again, the higher the better and left the most one is all proposed model MZ achieve the best achieved the highest topic coherence and the second one is MCM is our model with much link only and performs the second best and the others are all the baseline models.",
            "So first of all we can see that the second one MGM is already better than all the baseline models and with the cannot.",
            "Achieves the best, so here it shows that both mathlinks than Canadians are very helpful and very important."
        ],
        [
            "And then we also ask who also ask two human judges, too many label the topical words of 10 domains until we calculate the basically given the ranking of the words we label each word as correct or incorrect.",
            "Then we calculate the precision at 5 and 10.",
            "Also the number of coherent topics.",
            "So here is the result for 10 domains and the red bar is our proposed names in model.",
            "The blue bar is the LTM model and the green bar is LDA model.",
            "So we can see that the first top chart is the precision of five middle ones, precision obtained, and the last one is the number of coherent topics.",
            "We can see that in all three cases, or model performs better than the baseline models and the results are highly consistent with the topic.",
            "Topic coherence result I showed before."
        ],
        [
            "It's also interesting to see the difference of the domain and the impact of difference of domains on the model.",
            "So here we want to extract the knowledge from non from some domains and applied it to the electronic domains.",
            "So in this graph the Y axis is the topic coherence again and the higher the better.",
            "The rightmost one Saturday which does not use any knowledge and then we apply the knowledge.",
            "Actor from non electronic domains.",
            "I apply them to the electronic domain.",
            "We see some improvement and we also check the knowledge from electronic electronics, applied them to electronic electronics domain.",
            "We see a bigger improvement.",
            "So if we share knowledge from all domains and apply them to each of the electronics and we see the best improvement.",
            "So here it shows that our model can accumulate and deal with the knowledge deal with noise in the knowledge."
        ],
        [
            "So here are the conclusions in this paper we propose the model that learn as humans do, that is, return the result learned in the past and use it for future learning.",
            "And we also use the big data, helping and modeling in the small data.",
            "We propose a way to automatically check knowledge and verify knowledge.",
            "And last but not least, we propose the M GPU model that for the Gibbs sampling inference."
        ],
        [
            "So here's some interesting future work.",
            "The first one is the knowledge engineering.",
            "So how can we store and maintains knowledge?",
            "So what if we have millions of domains, so we have different types of knowledge.",
            "How can we maintain them?",
            "How can we merge them together?",
            "And the second one is the domain order and domain selection.",
            "So we think that the domain order definitely plays a role here.",
            "But how does it influence the algorithm?",
            "How can we select domain for example?",
            "Again, if we have millions of domains, we don't want to extract the knowledge from millions of domains.",
            "We may want to select the domains first, maybe select thousands of length and extract the focus knowledge from these domains."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I'm going to talk about mining topics in documents, standing on the shoulders of big Data, and Jim Bratton, an advisor, is professor.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Renew.",
                    "label": 0
                },
                {
                    "sent": "So topic models have been widely used to discover topics in many applications and most of the topic models unsupervised.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, topic models require a large amount of documents in order to work.",
                    "label": 0
                },
                {
                    "sent": "Usually thousands of documents, and without such a large amount of documents it may generate many incoherent.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But not every task application has such a large amount document.",
                    "label": 0
                },
                {
                    "sent": "For example, if we want to find out the product features from the review.",
                    "label": 0
                },
                {
                    "sent": "For example, given the reviews of the camera, we may want to find the product features such as batteries, screen and lens.",
                    "label": 0
                },
                {
                    "sent": "In this task, many if you go to Amazon.com ebay.com, you will find that many products do not even have more than 100 reviews and we found that with 100 reviews, LG performs very poorly.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the question.",
                    "label": 0
                },
                {
                    "sent": "We want to ask since the data is so small in one domain, can we help the modeling using big data?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's our intuition, so let's go back to how human learn.",
                    "label": 0
                },
                {
                    "sent": "Let's say a person sees a new situation and it will heal.",
                    "label": 1
                },
                {
                    "sent": "She will use the previous experience to solve the problems in this new station.",
                    "label": 0
                },
                {
                    "sent": "For example, if someone goes to a new country and it has no idea what language is spoken in that country but use his previous years of experience, he can.",
                    "label": 0
                },
                {
                    "sent": "He can figure out that this is probably restaurant in this hotel, and this is the airport.",
                    "label": 0
                },
                {
                    "sent": "So even if he has no idea what's going on in that particular.",
                    "label": 0
                },
                {
                    "sent": "She.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we want to borrow the similar idea when model season new domain.",
                    "label": 0
                },
                {
                    "sent": "You can also use the data of many previous domain to help the learning in our new domain which we refer us the big data.",
                    "label": 1
                },
                {
                    "sent": "So here when the model see the laptop domain it should not be a totally new domain because email already see some related domains such as cell phone, desktop and so that's why we were trying to use the big data here refers to a large number of domains with wide variety.",
                    "label": 0
                },
                {
                    "sent": "To help in the learning in a new domain.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's our motivation.",
                    "label": 0
                },
                {
                    "sent": "Want to propose a model that learns as humans do, which is also refers to lifelong learning?",
                    "label": 1
                },
                {
                    "sent": "Sort of so it is the level learning is to return the result, and then in the past and use it to help.",
                    "label": 0
                },
                {
                    "sent": "And learning in the future.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's our proposed model flow.",
                    "label": 1
                },
                {
                    "sent": "We want.",
                    "label": 0
                },
                {
                    "sent": "First of all, we retain the topics learned in the previous domains and then we learn the knowledge in these topics.",
                    "label": 1
                },
                {
                    "sent": "And at last we apply the knowledge to a new domain.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's one question.",
                    "label": 0
                },
                {
                    "sent": "What's the knowledge application in our knowledge?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Raise.",
                    "label": 0
                },
                {
                    "sent": "So again we let's go back to human learning.",
                    "label": 0
                },
                {
                    "sent": "How does a baby then or gain knowledge so one of the very basic knowledge is should also not so the baby learns the knowledge and knowledge that he or she should shoot.",
                    "label": 1
                },
                {
                    "sent": "Also not do something based on the reaction of the parents.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have the same idea here for the shoot we use the mass link that is the tools in the must link should be put in the same topic because they have the semantic correlation.",
                    "label": 0
                },
                {
                    "sent": "An follow cannot link.",
                    "label": 0
                },
                {
                    "sent": "That is to not let the tools in the Canonical sloppy to put together in the same topic.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how can we extract the such knowledge could automatically?",
                    "label": 0
                },
                {
                    "sent": "So here's the motivation of person learns the knowledge when it happens repetitively.",
                    "label": 1
                },
                {
                    "sent": "That is, when someone tells you something, you may get it, but when many people keep telling you that you will learn this knowledge and remember it.",
                    "label": 0
                },
                {
                    "sent": "So that's how we extract our knowledge that we will instruct the new level knowledge that appears repentant frequently.",
                    "label": 0
                },
                {
                    "sent": "So we apply the well known data mining techniques called frequent itemset mining.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it is to discover the Freedom water standard with the word happening happen frequently in the same topic, and we specifically we apply the multi minimum support, frequent itemset mining and it will be used to directly minor must links.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, cannot things are much more challenging because most words are not correlated with most other words.",
                    "label": 0
                },
                {
                    "sent": "For example, only words that are related with price will be correlated with cheap and most other words are not tolerated correlated with cheap.",
                    "label": 0
                },
                {
                    "sent": "So that's why if vocabulary sizes V we have all the square cannot links in total.",
                    "label": 0
                },
                {
                    "sent": "But given a new domain, most of words do not appear in the new domain, new domain that.",
                    "label": 1
                },
                {
                    "sent": "So that's why we don't need to extract canonic for.",
                    "label": 1
                },
                {
                    "sent": "All the words we focused on those top topical words in that particular domain and check it cannot links for them.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are some two related words about Canonical.",
                    "label": 0
                },
                {
                    "sent": "There are several topic models that are proposed to deal with math link, but there only two topic models that will propose to deal with cannot type of link cannot type of knowledge.",
                    "label": 1
                },
                {
                    "sent": "First one is dividend sequence.",
                    "label": 0
                },
                {
                    "sent": "The other day, but.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And assume their knowledge to be correct and manually provided.",
                    "label": 0
                },
                {
                    "sent": "So you can see our knowledge is automatically extracted, so there's no guarantee that the knowledge has to be 100% correct.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's why we need to do the knowledge verification, so here's a motivation of persons.",
                    "label": 0
                },
                {
                    "sent": "Experience may not be applicable to a particular particular situation.",
                    "label": 1
                },
                {
                    "sent": "So, for example, someone may have the knowledge of the meaning of 1 gesture, but when he goes to a new country, the meaning of that gesture may change due to the different culture.",
                    "label": 0
                },
                {
                    "sent": "So so let's similar problem here.",
                    "label": 0
                },
                {
                    "sent": "We need to verify the knowledge when we have the new domain.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to verify the mathlink, first we build a must link graph.",
                    "label": 0
                },
                {
                    "sent": "That is, each vertex is a must link and the edge shows the relationship between the math link.",
                    "label": 0
                },
                {
                    "sent": "For example the graph.",
                    "label": 0
                },
                {
                    "sent": "Here we have bank money and banking finance.",
                    "label": 1
                },
                {
                    "sent": "There's edge between them because the bank in both must links mean the same thing, while the bank River means a totally different thing.",
                    "label": 0
                },
                {
                    "sent": "How can we construct such graph so it is that the must link bank money and must link bank front lines should be extracted from.",
                    "label": 0
                },
                {
                    "sent": "Almost seems out of topics while the other one Bacon River should be extracted from a totally different set of topics.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then further more, we use pointwise mutual information to estimate the correctness of the mass link and the positive PMI indicates a positive semantic correlation and it will be used in the proposed Gibbs sampler later.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then for the Canonical again, Canadians are much more challenging due to the power law distribution of natural language words.",
                    "label": 0
                },
                {
                    "sent": "Most word do not Co occur with most other words, so that's why local occurrence cannot be used to estimate the correctness of economic.",
                    "label": 1
                },
                {
                    "sent": "Now that's why we dynamically verify the Canonical in inside the inference step.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's our proposed Gibbs sampler for our topic model.",
                    "label": 1
                },
                {
                    "sent": "It's called M GPU, multi generalized, polya model.",
                    "label": 0
                },
                {
                    "sent": "The idea is therefore must link.",
                    "label": 0
                },
                {
                    "sent": "We want to increase the probability of both words in the mouth link and follow Canonical.",
                    "label": 1
                },
                {
                    "sent": "We want to decrease the probability of one of the words in the can.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's one example.",
                    "label": 0
                },
                {
                    "sent": "If you if we see the topic, if we see the word speed under topic zero given the mathlink less speed, faster correlated, which increase the probability of word fast under topic zero will decrease the probability of.",
                    "label": 1
                },
                {
                    "sent": "Beauty under topic zero.",
                    "label": 0
                },
                {
                    "sent": "Given Canonical app given controlling of speed and beauty.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the steps for the forum US link.",
                    "label": 0
                },
                {
                    "sent": "So here's the how we increase the probability.",
                    "label": 0
                },
                {
                    "sent": "So here's the conditional probability of the Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "It has two parts, the first part.",
                    "label": 0
                },
                {
                    "sent": "This document.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distribution, the difference is the second one.",
                    "label": 0
                },
                {
                    "sent": "We added summation here.",
                    "label": 0
                },
                {
                    "sent": "That is we consider the related words that represented by month links and also.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have the Lambda matches here which is based on the PM value I introduced before.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the Canonical we want to transfer the cannot link word into some other topics that have higher probability of this word.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's why we have an indicator function here to constrain that the world has to be transferred further to one of those topics.",
                    "label": 0
                },
                {
                    "sent": "But what if there is no topic that has higher probability of this word?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So MSMOD will increase the number of topics an create a new topic.",
                    "label": 1
                },
                {
                    "sent": "We don't do that in this case because our cannot links may be incorrect.",
                    "label": 0
                },
                {
                    "sent": "For example, economics may ask us to separate the battery in life, but in this case we don't follow chaotic when there's a conflict, because we assume that Canonical is not reliable.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are some evaluations.",
                    "label": 0
                },
                {
                    "sent": "We have 100 product domains instructive on Amazon.com, 50 of them about electronics such as computers, cell phone, camera and film and our non electronics such as clothes, diaper bikes.",
                    "label": 0
                },
                {
                    "sent": "My stomach has 1000 reviews here, so each domain works as a test domain.",
                    "label": 1
                },
                {
                    "sent": "We randomly sampled 100 views as a small data and feed it to all the topic models and the knowledge is checked from the big data.",
                    "label": 0
                },
                {
                    "sent": "The from the 1000 reviews from the other domains.",
                    "label": 1
                },
                {
                    "sent": "That is, we want to simulate the situation that we extract knowledge from big Data applied to the small data.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for a model comparison, so the first one MC is our proposed model with most Lincoln Alex Mercer Link only and the next four models are knowledge based topic model, they can use the most knowledge and some of them can use both must link and cannot link.",
                    "label": 0
                },
                {
                    "sent": "So the last one is well known LDA model.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, we use the topic coherence too.",
                    "label": 0
                },
                {
                    "sent": "To do the measurement so topical as was proposed in 2011, an it shows it shows highly correlated with human judgment and the high score high topic current score indicates better or more coherent topics.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the 1st result.",
                    "label": 0
                },
                {
                    "sent": "So the Y axis here is the topic occurrence.",
                    "label": 0
                },
                {
                    "sent": "Again, the higher the better and left the most one is all proposed model MZ achieve the best achieved the highest topic coherence and the second one is MCM is our model with much link only and performs the second best and the others are all the baseline models.",
                    "label": 0
                },
                {
                    "sent": "So first of all we can see that the second one MGM is already better than all the baseline models and with the cannot.",
                    "label": 0
                },
                {
                    "sent": "Achieves the best, so here it shows that both mathlinks than Canadians are very helpful and very important.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we also ask who also ask two human judges, too many label the topical words of 10 domains until we calculate the basically given the ranking of the words we label each word as correct or incorrect.",
                    "label": 0
                },
                {
                    "sent": "Then we calculate the precision at 5 and 10.",
                    "label": 0
                },
                {
                    "sent": "Also the number of coherent topics.",
                    "label": 0
                },
                {
                    "sent": "So here is the result for 10 domains and the red bar is our proposed names in model.",
                    "label": 0
                },
                {
                    "sent": "The blue bar is the LTM model and the green bar is LDA model.",
                    "label": 0
                },
                {
                    "sent": "So we can see that the first top chart is the precision of five middle ones, precision obtained, and the last one is the number of coherent topics.",
                    "label": 0
                },
                {
                    "sent": "We can see that in all three cases, or model performs better than the baseline models and the results are highly consistent with the topic.",
                    "label": 0
                },
                {
                    "sent": "Topic coherence result I showed before.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's also interesting to see the difference of the domain and the impact of difference of domains on the model.",
                    "label": 0
                },
                {
                    "sent": "So here we want to extract the knowledge from non from some domains and applied it to the electronic domains.",
                    "label": 0
                },
                {
                    "sent": "So in this graph the Y axis is the topic coherence again and the higher the better.",
                    "label": 0
                },
                {
                    "sent": "The rightmost one Saturday which does not use any knowledge and then we apply the knowledge.",
                    "label": 0
                },
                {
                    "sent": "Actor from non electronic domains.",
                    "label": 0
                },
                {
                    "sent": "I apply them to the electronic domain.",
                    "label": 0
                },
                {
                    "sent": "We see some improvement and we also check the knowledge from electronic electronics, applied them to electronic electronics domain.",
                    "label": 0
                },
                {
                    "sent": "We see a bigger improvement.",
                    "label": 0
                },
                {
                    "sent": "So if we share knowledge from all domains and apply them to each of the electronics and we see the best improvement.",
                    "label": 0
                },
                {
                    "sent": "So here it shows that our model can accumulate and deal with the knowledge deal with noise in the knowledge.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are the conclusions in this paper we propose the model that learn as humans do, that is, return the result learned in the past and use it for future learning.",
                    "label": 1
                },
                {
                    "sent": "And we also use the big data, helping and modeling in the small data.",
                    "label": 1
                },
                {
                    "sent": "We propose a way to automatically check knowledge and verify knowledge.",
                    "label": 0
                },
                {
                    "sent": "And last but not least, we propose the M GPU model that for the Gibbs sampling inference.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's some interesting future work.",
                    "label": 1
                },
                {
                    "sent": "The first one is the knowledge engineering.",
                    "label": 1
                },
                {
                    "sent": "So how can we store and maintains knowledge?",
                    "label": 0
                },
                {
                    "sent": "So what if we have millions of domains, so we have different types of knowledge.",
                    "label": 0
                },
                {
                    "sent": "How can we maintain them?",
                    "label": 0
                },
                {
                    "sent": "How can we merge them together?",
                    "label": 1
                },
                {
                    "sent": "And the second one is the domain order and domain selection.",
                    "label": 0
                },
                {
                    "sent": "So we think that the domain order definitely plays a role here.",
                    "label": 0
                },
                {
                    "sent": "But how does it influence the algorithm?",
                    "label": 0
                },
                {
                    "sent": "How can we select domain for example?",
                    "label": 0
                },
                {
                    "sent": "Again, if we have millions of domains, we don't want to extract the knowledge from millions of domains.",
                    "label": 0
                },
                {
                    "sent": "We may want to select the domains first, maybe select thousands of length and extract the focus knowledge from these domains.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}