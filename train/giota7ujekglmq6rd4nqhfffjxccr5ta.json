{
    "id": "giota7ujekglmq6rd4nqhfffjxccr5ta",
    "title": "Frequency-aware Truncated methods for Sparse Online Learning",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Hidekazu Oiwa, Graduate School of Information Science and Technology, University of Tokyo"
        ],
        "published": "Nov. 29, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_oiwa_learning/",
    "segmentation": [
        [
            "OK, thank you Gemma.",
            "And how about everyone?",
            "My name is Aurora.",
            "I'm a graduate student at the University of Tokyo in Japan.",
            "Today I'd like to introduce my research into sparse online learning.",
            "Kate.",
            "Fur."
        ],
        [
            "We introduce the problem setting of our research.",
            "Our research focuses on supervised learning.",
            "So far.",
            "So for predicting an output from a given input.",
            "We want to leave an optimal function F. The search the optimal structure of a function F, we use a writing about of data.",
            "This is the framework of supervised learning.",
            "Supervise running it's applicable to many tasks such as regression and classification."
        ],
        [
            "And the next we proceed to introduce the notation of our research.",
            "Redescribe input as a any dimensional vector X.",
            "And also describe output as Oscar, why?",
            "For predicting out of it from an input, we use a rate vector in the framework of linear prediction.",
            "Alright, vector is described at any dimensional vector the same dimensions.",
            "As input vector.",
            "And we will predict an output using the value of inner product of a weight vector and input vector.",
            "And also in the predicted value is described as white hat."
        ],
        [
            "So we want to drape the optimal parameters of a way to vector W. To drive optimal rate vector.",
            "Riversong vision problem described in this ride.",
            "This optimization problem consists of the sum of two functions, the faster an intro function and the second one is a regular ice time.",
            "By solving this optimization problem.",
            "We can do a simple and very predictable rate vector as a result.",
            "So in the following discussion, we will focus on how to obtain optimal weight vector."
        ],
        [
            "To obtain optimal weight vector we use online learning framework.",
            "In our lease at our research.",
            "In the optimum online learning framework.",
            "We update our way to vector using only one data on each update.",
            "So we don't have to put more data into a memory at a time.",
            "So it'll require a smaller memory space than the budget pipelining.",
            "And also we should note the form of optimization problem is changed.",
            "In this framework we update rate vector to minimize a lots and regularization of next coming data.",
            "So is that a dead ID integer optimization problem weight vector?",
            "And next we introduce the detail of loss function."
        ],
        [
            "And regularization electrolytes down.",
            "The first is rose function.",
            "The function is a function to evaluate the prediction accuracy over weight vector.",
            "For example.",
            "Hinge loss function is a famous example of loss function.",
            "Enter a function.",
            "If a predicted value is not appropriate to the true output.",
            "It's a value of function becomes large.",
            "In our research we assume a loss functions gradient is proportional to an input vector.",
            "And this assumption is satisfied in many loss functions in the linear prediction settings."
        ],
        [
            "And the next little right stuff.",
            "Little lights time is used to prevent overheating.",
            "Everyone regularization also no address so.",
            "It's an example of level right stuff.",
            "And that so consists of everyone know overweight vector.",
            "If a weight vector becomes complicated, the value of regularised term, also large, also becomes large.",
            "But inserting decorates them into optimization problem.",
            "Arata Victor will stay simple.",
            "So re compliment overheating to previous years data."
        ],
        [
            "And in addition.",
            "Not so hot.",
            "Another important property.",
            "By applying Russell.",
            "Without deteriorating prediction accuracy, we can obtain as fast weight vector by truncating the redundant parameters.",
            "At described in this ride.",
            "If we apply lasso to await vector through this update formula.",
            "Components 0.5 or less or become zero.",
            "Dearth the number of their components will increase and can drive great arse pathway to vector.",
            "By making a way to vectors first, we can compute an inner product faster."
        ],
        [
            "And there are two famous previous work combining online learning with Russell.",
            "WordPress coming through frameworks advantages.",
            "The first one is forward.",
            "And the second one is RDA.",
            "How much is the algorithm that splits update procedure into 2 steps?",
            "The first step is lost in migration step and the second one is regularization step.",
            "RDA is the algorithm that introduce wrestle into their operating framework.",
            "However, did the rock has a critical disadvantage."
        ],
        [
            "Each.",
            "We indicate a disadvantage of previous work in this trial.",
            "In previous works algorithms.",
            "No free country occurrence features are truncated on a priority basis.",
            "So indeed, previous work.",
            "It is difficult to use low frequently or current features for prediction.",
            "It did cause truncation is performed regardless of feature frequency.",
            "So low frequency features will be creeped around 0.",
            "By applying right?",
            "So we want to eliminate only uninformative features.",
            "However, this algorithm also truncate in haunted features.",
            "If they are loving, go free country or currency."
        ],
        [
            "So.",
            "Food attaining informative but roughly 3 occurrence features.",
            "We propose the new approach of lasso in online learning framework.",
            "Proposed method integrate the information of feature frequency with Russell.",
            "For capturing low frequency features, we adjust the truncation intensity.",
            "According to feature frequency.",
            "Separate methods such as the IDF, similar to our intention.",
            "But this method cannot be applied in online learning framework."
        ],
        [
            "Some light previous discussion in this slide.",
            "First, previous work for both is the learning method that combines online learning with Russell while preserving jets the frameworks merit.",
            "However.",
            "This previous work had the disadvantage that low frequency features are truncated on a priority basis.",
            "So to solve this problem, we propose the learning methods of Russell with feature frequency.",
            "This extra extensional algorithm is named frequency or truncated methods for sparse online learning.",
            "In short, if the focus."
        ],
        [
            "OK, next we precede the the explain the detail structure of our proposed algorithm.",
            "In previous work, truncation is done regardless of feature frequency.",
            "So low frequency parameters.",
            "Truncated on priority basis.",
            "To retain."
        ],
        [
            "Hopefully you can see features we introduce a vector HT and vector.",
            "HD has correlation with feature frequency into Russell.",
            "By inserting HP into Russell, we will adjust the intensity of truncation.",
            "To retain low frequency features in online matter."
        ],
        [
            "However.",
            "This method is not good where Vector HD is.",
            "Set at simply proportional to frequency.",
            "This is because the value range of weight vector.",
            "Depends more on update frequency than feature frequency.",
            "There's remake the Vector HD as our correlation rate update frequency to satisfy the intention."
        ],
        [
            "And next we explain the algorithm of 84.",
            "This algorithm perform this step parameter parameter update process.",
            "It is the same framework as focus, first in last minimizations step.",
            "For predicting that appropriately.",
            "Update rate vector using only loss function.",
            "In this algorithm we can apply subgradient methods.",
            "In last minimization step.",
            "Subgradient method is written by the formula in this slide.",
            "In subgradient Method update array to vector into a reverse direction of subgradient.",
            "And it is a direction to minimize the loss functions value."
        ],
        [
            "In the next update parameters 80.",
            "HD is defined at this right?",
            "Each component of HT is corresponding to each feature.",
            "And.",
            "Each component consists of Arpino.",
            "Update step sites at each round intros minimization step.",
            "Because of this definition.",
            "If ice components over weight vector nearly update.",
            "Only a small number of subgradients is only the only non 0.",
            "So the value of AC becomes small."
        ],
        [
            "An in Russell step.",
            "Integrate the vector HD.",
            "As matrix form.",
            "By modifying our last step as this ride.",
            "We can rewrite previous described intention.",
            "Adjust the intensity of truncation according to feature update frequency.",
            "We can some light jets to step into one cross form update formula.",
            "This update formula can be processed in order.",
            "The number of nonzero in an input data or sub gradient.",
            "And this computational cost is the same order as follows."
        ],
        [
            "And we also can write our proposed algorithms theoretical convergence properties.",
            "In our research we use the notion of direct bound.",
            "Regret bound is defined as this formula.",
            "And the first time of the Great Mound, it's cumulative loss and regularizations values.",
            "Produced by this algorithm in all rounds.",
            "The second one is the value of loss and minimal regularizations term.",
            "There are eight vector is set to make this summation minimum.",
            "If a regret bound is restaurant order, the number of data.",
            "Liberat by data converged zero at data increase.",
            "So await vector converge to optimal solution."
        ],
        [
            "And we proved.",
            "That other algorithms regret bound it's order, root key or other order root.",
            "The number of data.",
            "And that's specific conditions.",
            "So Rick improved, our algorithm can obtain an optimal solution at that increased.",
            "This figure is same as follows."
        ],
        [
            "And Rust research experimental results.",
            "We ever added the performance of each algorithm using seven classification tasks.",
            "Experimental results are shown in this slide."
        ],
        [
            "First, we compare the result when changing parameter P in FT for us.",
            "Parameter."
        ],
        [
            "We adjust the vector 80 disparity.",
            "So if we get smaller.",
            "The difference between components of Vector HD becomes large."
        ],
        [
            "And from the result.",
            "We can say 50 for both equal to achieve the best performance."
        ],
        [
            "And the next we compare the result of FT forward speak ordeal rates for both and RDN.",
            "From the result HD for both out of performance for both in all seven dead datasets.",
            "In terms of precision.",
            "However.",
            "Are the IT gets better performance than FD for both in six datasets?",
            "Richard, this is be cause RDA had a smaller regret bound in order.",
            "In terms of collections."
        ],
        [
            "And also we compare the sparseness of each algorithm where they obtain the best accuracy.",
            "Parameter result.",
            "FT for us.",
            "Got that similar result as forwards.",
            "So we can say everything for both improve accuracy while obtaining almost the same sparseness.",
            "On the other hand, RDA is not good in terms of sparseness."
        ],
        [
            "Finally, we conclude our discussion.",
            "We propose a new algorithm name SD photos.",
            "This algorithm or integrate the information of feature update frequency into Russell.",
            "To retain informative but low frequency features.",
            "And also we proved the regret bound of this algorithm at ordered T. Moreover, we cannot discuss in this presentation.",
            "We also propose FT for both, with cumulative penalty model.",
            "To deliver more robust model or noise data.",
            "Last, we evaluate the performance of this algorithm and such such right that FT forwards outperforms forwards in all datasets.",
            "We're preserving the shamelessness.",
            "This presentation is and thank you.",
            "So algorithm.",
            "In the algorithm.",
            "And in the algorithm, the 1st update step is uses the subgradient but the subgradient does not unique.",
            "So is your algorithm deterministic?",
            "No.",
            "In this algorithm, or we we can any subgradient.",
            "Which can be used and.",
            "If any we use any subgradient we can prove.",
            "It's a regret bound.",
            "Which subgradient do you actually use?",
            "I use or are into Hindi, Rose, Virgil.",
            "The subgradient is.",
            "Not not OR.",
            "Absolute value of subgradient is most bigger is used.",
            "Other questions.",
            "Let's"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, thank you Gemma.",
                    "label": 0
                },
                {
                    "sent": "And how about everyone?",
                    "label": 0
                },
                {
                    "sent": "My name is Aurora.",
                    "label": 0
                },
                {
                    "sent": "I'm a graduate student at the University of Tokyo in Japan.",
                    "label": 1
                },
                {
                    "sent": "Today I'd like to introduce my research into sparse online learning.",
                    "label": 0
                },
                {
                    "sent": "Kate.",
                    "label": 0
                },
                {
                    "sent": "Fur.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We introduce the problem setting of our research.",
                    "label": 1
                },
                {
                    "sent": "Our research focuses on supervised learning.",
                    "label": 1
                },
                {
                    "sent": "So far.",
                    "label": 0
                },
                {
                    "sent": "So for predicting an output from a given input.",
                    "label": 1
                },
                {
                    "sent": "We want to leave an optimal function F. The search the optimal structure of a function F, we use a writing about of data.",
                    "label": 0
                },
                {
                    "sent": "This is the framework of supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Supervise running it's applicable to many tasks such as regression and classification.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the next we proceed to introduce the notation of our research.",
                    "label": 0
                },
                {
                    "sent": "Redescribe input as a any dimensional vector X.",
                    "label": 0
                },
                {
                    "sent": "And also describe output as Oscar, why?",
                    "label": 0
                },
                {
                    "sent": "For predicting out of it from an input, we use a rate vector in the framework of linear prediction.",
                    "label": 0
                },
                {
                    "sent": "Alright, vector is described at any dimensional vector the same dimensions.",
                    "label": 0
                },
                {
                    "sent": "As input vector.",
                    "label": 0
                },
                {
                    "sent": "And we will predict an output using the value of inner product of a weight vector and input vector.",
                    "label": 1
                },
                {
                    "sent": "And also in the predicted value is described as white hat.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we want to drape the optimal parameters of a way to vector W. To drive optimal rate vector.",
                    "label": 0
                },
                {
                    "sent": "Riversong vision problem described in this ride.",
                    "label": 0
                },
                {
                    "sent": "This optimization problem consists of the sum of two functions, the faster an intro function and the second one is a regular ice time.",
                    "label": 1
                },
                {
                    "sent": "By solving this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We can do a simple and very predictable rate vector as a result.",
                    "label": 1
                },
                {
                    "sent": "So in the following discussion, we will focus on how to obtain optimal weight vector.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To obtain optimal weight vector we use online learning framework.",
                    "label": 0
                },
                {
                    "sent": "In our lease at our research.",
                    "label": 0
                },
                {
                    "sent": "In the optimum online learning framework.",
                    "label": 1
                },
                {
                    "sent": "We update our way to vector using only one data on each update.",
                    "label": 0
                },
                {
                    "sent": "So we don't have to put more data into a memory at a time.",
                    "label": 0
                },
                {
                    "sent": "So it'll require a smaller memory space than the budget pipelining.",
                    "label": 0
                },
                {
                    "sent": "And also we should note the form of optimization problem is changed.",
                    "label": 0
                },
                {
                    "sent": "In this framework we update rate vector to minimize a lots and regularization of next coming data.",
                    "label": 1
                },
                {
                    "sent": "So is that a dead ID integer optimization problem weight vector?",
                    "label": 0
                },
                {
                    "sent": "And next we introduce the detail of loss function.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And regularization electrolytes down.",
                    "label": 0
                },
                {
                    "sent": "The first is rose function.",
                    "label": 0
                },
                {
                    "sent": "The function is a function to evaluate the prediction accuracy over weight vector.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Hinge loss function is a famous example of loss function.",
                    "label": 1
                },
                {
                    "sent": "Enter a function.",
                    "label": 0
                },
                {
                    "sent": "If a predicted value is not appropriate to the true output.",
                    "label": 0
                },
                {
                    "sent": "It's a value of function becomes large.",
                    "label": 0
                },
                {
                    "sent": "In our research we assume a loss functions gradient is proportional to an input vector.",
                    "label": 1
                },
                {
                    "sent": "And this assumption is satisfied in many loss functions in the linear prediction settings.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the next little right stuff.",
                    "label": 0
                },
                {
                    "sent": "Little lights time is used to prevent overheating.",
                    "label": 0
                },
                {
                    "sent": "Everyone regularization also no address so.",
                    "label": 0
                },
                {
                    "sent": "It's an example of level right stuff.",
                    "label": 0
                },
                {
                    "sent": "And that so consists of everyone know overweight vector.",
                    "label": 0
                },
                {
                    "sent": "If a weight vector becomes complicated, the value of regularised term, also large, also becomes large.",
                    "label": 0
                },
                {
                    "sent": "But inserting decorates them into optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Arata Victor will stay simple.",
                    "label": 0
                },
                {
                    "sent": "So re compliment overheating to previous years data.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in addition.",
                    "label": 0
                },
                {
                    "sent": "Not so hot.",
                    "label": 0
                },
                {
                    "sent": "Another important property.",
                    "label": 0
                },
                {
                    "sent": "By applying Russell.",
                    "label": 0
                },
                {
                    "sent": "Without deteriorating prediction accuracy, we can obtain as fast weight vector by truncating the redundant parameters.",
                    "label": 0
                },
                {
                    "sent": "At described in this ride.",
                    "label": 0
                },
                {
                    "sent": "If we apply lasso to await vector through this update formula.",
                    "label": 1
                },
                {
                    "sent": "Components 0.5 or less or become zero.",
                    "label": 0
                },
                {
                    "sent": "Dearth the number of their components will increase and can drive great arse pathway to vector.",
                    "label": 0
                },
                {
                    "sent": "By making a way to vectors first, we can compute an inner product faster.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are two famous previous work combining online learning with Russell.",
                    "label": 1
                },
                {
                    "sent": "WordPress coming through frameworks advantages.",
                    "label": 0
                },
                {
                    "sent": "The first one is forward.",
                    "label": 0
                },
                {
                    "sent": "And the second one is RDA.",
                    "label": 0
                },
                {
                    "sent": "How much is the algorithm that splits update procedure into 2 steps?",
                    "label": 0
                },
                {
                    "sent": "The first step is lost in migration step and the second one is regularization step.",
                    "label": 0
                },
                {
                    "sent": "RDA is the algorithm that introduce wrestle into their operating framework.",
                    "label": 0
                },
                {
                    "sent": "However, did the rock has a critical disadvantage.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each.",
                    "label": 0
                },
                {
                    "sent": "We indicate a disadvantage of previous work in this trial.",
                    "label": 1
                },
                {
                    "sent": "In previous works algorithms.",
                    "label": 0
                },
                {
                    "sent": "No free country occurrence features are truncated on a priority basis.",
                    "label": 0
                },
                {
                    "sent": "So indeed, previous work.",
                    "label": 0
                },
                {
                    "sent": "It is difficult to use low frequently or current features for prediction.",
                    "label": 1
                },
                {
                    "sent": "It did cause truncation is performed regardless of feature frequency.",
                    "label": 0
                },
                {
                    "sent": "So low frequency features will be creeped around 0.",
                    "label": 0
                },
                {
                    "sent": "By applying right?",
                    "label": 0
                },
                {
                    "sent": "So we want to eliminate only uninformative features.",
                    "label": 0
                },
                {
                    "sent": "However, this algorithm also truncate in haunted features.",
                    "label": 0
                },
                {
                    "sent": "If they are loving, go free country or currency.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Food attaining informative but roughly 3 occurrence features.",
                    "label": 1
                },
                {
                    "sent": "We propose the new approach of lasso in online learning framework.",
                    "label": 0
                },
                {
                    "sent": "Proposed method integrate the information of feature frequency with Russell.",
                    "label": 0
                },
                {
                    "sent": "For capturing low frequency features, we adjust the truncation intensity.",
                    "label": 0
                },
                {
                    "sent": "According to feature frequency.",
                    "label": 0
                },
                {
                    "sent": "Separate methods such as the IDF, similar to our intention.",
                    "label": 0
                },
                {
                    "sent": "But this method cannot be applied in online learning framework.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some light previous discussion in this slide.",
                    "label": 0
                },
                {
                    "sent": "First, previous work for both is the learning method that combines online learning with Russell while preserving jets the frameworks merit.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "This previous work had the disadvantage that low frequency features are truncated on a priority basis.",
                    "label": 0
                },
                {
                    "sent": "So to solve this problem, we propose the learning methods of Russell with feature frequency.",
                    "label": 0
                },
                {
                    "sent": "This extra extensional algorithm is named frequency or truncated methods for sparse online learning.",
                    "label": 1
                },
                {
                    "sent": "In short, if the focus.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, next we precede the the explain the detail structure of our proposed algorithm.",
                    "label": 0
                },
                {
                    "sent": "In previous work, truncation is done regardless of feature frequency.",
                    "label": 1
                },
                {
                    "sent": "So low frequency parameters.",
                    "label": 0
                },
                {
                    "sent": "Truncated on priority basis.",
                    "label": 0
                },
                {
                    "sent": "To retain.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hopefully you can see features we introduce a vector HT and vector.",
                    "label": 0
                },
                {
                    "sent": "HD has correlation with feature frequency into Russell.",
                    "label": 1
                },
                {
                    "sent": "By inserting HP into Russell, we will adjust the intensity of truncation.",
                    "label": 0
                },
                {
                    "sent": "To retain low frequency features in online matter.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "This method is not good where Vector HD is.",
                    "label": 0
                },
                {
                    "sent": "Set at simply proportional to frequency.",
                    "label": 1
                },
                {
                    "sent": "This is because the value range of weight vector.",
                    "label": 0
                },
                {
                    "sent": "Depends more on update frequency than feature frequency.",
                    "label": 1
                },
                {
                    "sent": "There's remake the Vector HD as our correlation rate update frequency to satisfy the intention.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And next we explain the algorithm of 84.",
                    "label": 1
                },
                {
                    "sent": "This algorithm perform this step parameter parameter update process.",
                    "label": 0
                },
                {
                    "sent": "It is the same framework as focus, first in last minimizations step.",
                    "label": 0
                },
                {
                    "sent": "For predicting that appropriately.",
                    "label": 0
                },
                {
                    "sent": "Update rate vector using only loss function.",
                    "label": 0
                },
                {
                    "sent": "In this algorithm we can apply subgradient methods.",
                    "label": 0
                },
                {
                    "sent": "In last minimization step.",
                    "label": 0
                },
                {
                    "sent": "Subgradient method is written by the formula in this slide.",
                    "label": 0
                },
                {
                    "sent": "In subgradient Method update array to vector into a reverse direction of subgradient.",
                    "label": 1
                },
                {
                    "sent": "And it is a direction to minimize the loss functions value.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the next update parameters 80.",
                    "label": 0
                },
                {
                    "sent": "HD is defined at this right?",
                    "label": 0
                },
                {
                    "sent": "Each component of HT is corresponding to each feature.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Each component consists of Arpino.",
                    "label": 0
                },
                {
                    "sent": "Update step sites at each round intros minimization step.",
                    "label": 0
                },
                {
                    "sent": "Because of this definition.",
                    "label": 0
                },
                {
                    "sent": "If ice components over weight vector nearly update.",
                    "label": 0
                },
                {
                    "sent": "Only a small number of subgradients is only the only non 0.",
                    "label": 1
                },
                {
                    "sent": "So the value of AC becomes small.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An in Russell step.",
                    "label": 0
                },
                {
                    "sent": "Integrate the vector HD.",
                    "label": 0
                },
                {
                    "sent": "As matrix form.",
                    "label": 0
                },
                {
                    "sent": "By modifying our last step as this ride.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite previous described intention.",
                    "label": 0
                },
                {
                    "sent": "Adjust the intensity of truncation according to feature update frequency.",
                    "label": 0
                },
                {
                    "sent": "We can some light jets to step into one cross form update formula.",
                    "label": 0
                },
                {
                    "sent": "This update formula can be processed in order.",
                    "label": 1
                },
                {
                    "sent": "The number of nonzero in an input data or sub gradient.",
                    "label": 1
                },
                {
                    "sent": "And this computational cost is the same order as follows.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we also can write our proposed algorithms theoretical convergence properties.",
                    "label": 0
                },
                {
                    "sent": "In our research we use the notion of direct bound.",
                    "label": 0
                },
                {
                    "sent": "Regret bound is defined as this formula.",
                    "label": 1
                },
                {
                    "sent": "And the first time of the Great Mound, it's cumulative loss and regularizations values.",
                    "label": 1
                },
                {
                    "sent": "Produced by this algorithm in all rounds.",
                    "label": 0
                },
                {
                    "sent": "The second one is the value of loss and minimal regularizations term.",
                    "label": 0
                },
                {
                    "sent": "There are eight vector is set to make this summation minimum.",
                    "label": 1
                },
                {
                    "sent": "If a regret bound is restaurant order, the number of data.",
                    "label": 0
                },
                {
                    "sent": "Liberat by data converged zero at data increase.",
                    "label": 0
                },
                {
                    "sent": "So await vector converge to optimal solution.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we proved.",
                    "label": 0
                },
                {
                    "sent": "That other algorithms regret bound it's order, root key or other order root.",
                    "label": 0
                },
                {
                    "sent": "The number of data.",
                    "label": 0
                },
                {
                    "sent": "And that's specific conditions.",
                    "label": 0
                },
                {
                    "sent": "So Rick improved, our algorithm can obtain an optimal solution at that increased.",
                    "label": 0
                },
                {
                    "sent": "This figure is same as follows.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Rust research experimental results.",
                    "label": 0
                },
                {
                    "sent": "We ever added the performance of each algorithm using seven classification tasks.",
                    "label": 0
                },
                {
                    "sent": "Experimental results are shown in this slide.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, we compare the result when changing parameter P in FT for us.",
                    "label": 0
                },
                {
                    "sent": "Parameter.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We adjust the vector 80 disparity.",
                    "label": 0
                },
                {
                    "sent": "So if we get smaller.",
                    "label": 0
                },
                {
                    "sent": "The difference between components of Vector HD becomes large.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And from the result.",
                    "label": 0
                },
                {
                    "sent": "We can say 50 for both equal to achieve the best performance.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the next we compare the result of FT forward speak ordeal rates for both and RDN.",
                    "label": 0
                },
                {
                    "sent": "From the result HD for both out of performance for both in all seven dead datasets.",
                    "label": 0
                },
                {
                    "sent": "In terms of precision.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "Are the IT gets better performance than FD for both in six datasets?",
                    "label": 0
                },
                {
                    "sent": "Richard, this is be cause RDA had a smaller regret bound in order.",
                    "label": 0
                },
                {
                    "sent": "In terms of collections.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also we compare the sparseness of each algorithm where they obtain the best accuracy.",
                    "label": 0
                },
                {
                    "sent": "Parameter result.",
                    "label": 0
                },
                {
                    "sent": "FT for us.",
                    "label": 0
                },
                {
                    "sent": "Got that similar result as forwards.",
                    "label": 0
                },
                {
                    "sent": "So we can say everything for both improve accuracy while obtaining almost the same sparseness.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, RDA is not good in terms of sparseness.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, we conclude our discussion.",
                    "label": 0
                },
                {
                    "sent": "We propose a new algorithm name SD photos.",
                    "label": 0
                },
                {
                    "sent": "This algorithm or integrate the information of feature update frequency into Russell.",
                    "label": 0
                },
                {
                    "sent": "To retain informative but low frequency features.",
                    "label": 1
                },
                {
                    "sent": "And also we proved the regret bound of this algorithm at ordered T. Moreover, we cannot discuss in this presentation.",
                    "label": 1
                },
                {
                    "sent": "We also propose FT for both, with cumulative penalty model.",
                    "label": 1
                },
                {
                    "sent": "To deliver more robust model or noise data.",
                    "label": 0
                },
                {
                    "sent": "Last, we evaluate the performance of this algorithm and such such right that FT forwards outperforms forwards in all datasets.",
                    "label": 0
                },
                {
                    "sent": "We're preserving the shamelessness.",
                    "label": 0
                },
                {
                    "sent": "This presentation is and thank you.",
                    "label": 0
                },
                {
                    "sent": "So algorithm.",
                    "label": 0
                },
                {
                    "sent": "In the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And in the algorithm, the 1st update step is uses the subgradient but the subgradient does not unique.",
                    "label": 0
                },
                {
                    "sent": "So is your algorithm deterministic?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "In this algorithm, or we we can any subgradient.",
                    "label": 0
                },
                {
                    "sent": "Which can be used and.",
                    "label": 0
                },
                {
                    "sent": "If any we use any subgradient we can prove.",
                    "label": 0
                },
                {
                    "sent": "It's a regret bound.",
                    "label": 0
                },
                {
                    "sent": "Which subgradient do you actually use?",
                    "label": 0
                },
                {
                    "sent": "I use or are into Hindi, Rose, Virgil.",
                    "label": 0
                },
                {
                    "sent": "The subgradient is.",
                    "label": 0
                },
                {
                    "sent": "Not not OR.",
                    "label": 0
                },
                {
                    "sent": "Absolute value of subgradient is most bigger is used.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Let's",
                    "label": 0
                }
            ]
        }
    }
}