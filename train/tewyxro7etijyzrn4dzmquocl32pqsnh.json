{
    "id": "tewyxro7etijyzrn4dzmquocl32pqsnh",
    "title": "Regularized Sparse Kernel Slow Feature Analysis",
    "info": {
        "author": [
            "Wendelin B\u00f6hmer, Department of Software Engineering and Theoretical Computer Science, Faculty VI Electrical Engineering and Computer Sciences, TU Berlin"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_boehmer_regularized/",
    "segmentation": [
        [
            "I will talk about regularised sparse kernel feature analysis and I know it's early, but I'm curious how many of you have ever heard the term slow feature analysis.",
            "Please raise your hand.",
            "I thought so so so feature analysis is an unsupervised learning mechanism.",
            "And it's about time series, and it's very strongly correlated or related.",
            "To some methods in graph spectral analysis.",
            "However.",
            "So because of this, it would have fitted in lots of slots.",
            "However, it is actually fitting quite well here, because what?"
        ],
        [
            "Actually want to talk about is how to apply linear algorithms to complicated nonlinear datasets.",
            "So if we for example have a classification or regression problem with respect to latent variables.",
            "I will explain this at the example of a lower classification task where like people say things like head and heat.",
            "And the interesting bit is.",
            "The middle vowel.",
            "So what we have is.",
            "We we just have the.",
            "The sound file and we want to have in the end some discriminative function.",
            "However, the problem is where to get there and the first thing, OK, it's like a technical step.",
            "It's not that."
        ],
        [
            "Essential for the method.",
            "It's that we need to have a proper multi dimensional feature space.",
            "So what we do is we take a sliding window and move it over it and what we get is then this kind of delayed embedding of the original sound signal.",
            "So that's not very surprising.",
            "This many people have done that before and it's also not doesn't work that great.",
            "It doesn't work that great cause.",
            "We forgot to apply a linear algorithm in this setup.",
            "We would actually need.",
            "Or we need to consider two things.",
            "The first thing is that our latent variables might be non linearly embedded.",
            "In this.",
            "Space.",
            "So a linear method might just not do the trick at all.",
            "The second thing is that the solution that is in this case this discrimination function, might be nonlinear in this space of the latent variables as well.",
            "So the question is how do we treat this kind of thing in the classical approach?",
            "Well, if you don't want to go into support vector machine."
        ],
        [
            "Of these things is to just construct a feature space.",
            "Like this?",
            "And then run the the linear algorithm in this feature space.",
            "So however, this feature space again needs to have some properties to work sufficiently.",
            "The first is this feature space needs needs to be nonlinear in the data, obviously because if the space of latent variables is nonlinear, embedded in the data.",
            "The feature space needs to do to respect that.",
            "The second thing is that Phi or this feature space needs to be a functional basis in the space of latent variables.",
            "So it's not just sufficient to extract all of the latent variables, you also need to construct a functional basis in this space.",
            "And the last one is that the feature space needs to be low dimensional.",
            "That's very important because the more dimensions we have, the most samples we need for a good.",
            "For a good approximation stochastically.",
            "So all of this is.",
            "Usually done by hand, so people just construct their feature space by some heuristics, and that's actually the way to go.",
            "If you have information."
        ],
        [
            "About your domain, however.",
            "What if this is not the case?",
            "So what if you need to construct your nonlinear feature space from data?",
            "And in this talk I will specifically look into the possibilities of construct, so use unlabeled data.",
            "In other words, to construct the feature space.",
            "With an unsupervised learning method and.",
            "If we think about unsupervised learning methods, the first thing that comes to mind is always PCA.",
            "I will not ask you if you have heard the term PCA before, 'cause it's probably everybody.",
            "However, there are some problems that come into play when you look in nonlinear PCA.",
            "There is a very famous paper about kernel principal component analysis.",
            "And that is that PCA.",
            "In principle, is a reconstruction algorithm is tries to compress the data.",
            "So if you blow up.",
            "The function space in which you look for the highest variance.",
            "Then Anne.",
            "In turn you get a reconstruction of that function space.",
            "So basically what you are what PCA is extracting for you has not necessarily anything to do with the latent variables.",
            "That's to say, construct any kind of function space and.",
            "In them, however, there is another.",
            "Another unsupervised learning mechanism which is called slow feature analysis, and this has some really interesting properties which are very suddenly known to anybody.",
            "And that is that if you.",
            "Do a nonlinear version of this algorithm.",
            "Your features actually converge.",
            "To something independent of the function class.",
            "Given that the function class is high enough."
        ],
        [
            "So what is this mysterious thing?",
            "First of all, it's about temporal or time series just works on time series.",
            "So the goal is to filter temporally coherent signals out of this time series.",
            "And they with temporal coherence signals, we just mean that the temporal derivative.",
            "Of the feature on this case, a vector of features.",
            "Is as small as possible with respect to the L2 norm.",
            "And temporal derivative is really just like the difference between time T and T + 1.",
            "Because the slowest and this is where the slow comes from.",
            "So where the slowest possible feature is basically a zero vector.",
            "To prevent that, we need to do to add some constraints like 0 mycamine constraint.",
            "This is basically also the same for PCA, but what we actually need is a unit variance and correlation constraint.",
            "The Decorrelation is again pretty obvious because we're talking about functional basis later and functional basis are optimally decorrelated and the unit variance is just like to let things do have a change, they just change slowly.",
            "So the original algorithm, which was proposed by Elvis Cut and Terrance Junior Ski.",
            "Works the following.",
            "In a linear setup.",
            "So just imagine this solid line to be the isoline of the covariance matrix of the data.",
            "And that dotted ellipse to be the isoline of the covariance matrix of the derivative of the data to the temporal derivative.",
            "What happens is first we fulfill the zero mean constraint, but just moving the whole thing to the center.",
            "Then we decorrelate by just running a PCA, and then rotate this the data in the directions of the eigenvectors.",
            "Now we're leaving the PCA track because we fear it, or we fulfill the unit variance constraint, which means that we just like re scale all of the dimensions so that you have variance one in all directions.",
            "And now you take the dotted.",
            "Isoline to rotate the whole system in the direction of the slowest change.",
            "So this is just didn't really mesh or doesn't.",
            "Didn't really do all the things that I promised in the beginning.",
            "However, there is an analysis that says if you have like an.",
            "An infinite time series and the non restricted function class.",
            "Then you can actually say something about the features that that will emerge and the features are actually four year bases in the space of the latent variables.",
            "And the interesting bit is that you do not need to know anything about this base of latent variables.",
            "The only thing that you will get is.",
            "This four year basis is ordered according to the.",
            "Speed of change in the space of latent variables.",
            "So it's actually what you get is the first features will encode the slowest changing latent variables.",
            "And the idea is that all of those noise things that have nothing to do with the latent variable space.",
            "Are so fast that they are basically encoded in feature 10,000,000, so they will never appear.",
            "All of this is conditioned on the assumption that you have an unrestricted function class.",
            "Basically a very powerful one.",
            "So what we did is we were not the first one to do that, we took the."
        ],
        [
            "Function class of the reproducing kernel Hilbert spaces.",
            "So we used.",
            "We did the kernel trick.",
            "And.",
            "We formulated the whole algorithm which is very easy because in the function class here, I guess that most of you are familiar with it.",
            "You just express the function as a linear weighting of kernel functions which are parameterized by the training data.",
            "However, by all of the training data, so this is a sum from 1, two the number of training samples.",
            "The advantage is that now we can basically post the whole the whole nonlinear optimization problem in such a way that it is linear in the kernel matrix.",
            "So here you just say, OK, this is the.",
            "Derivative of the kernel matrix temporal derivative.",
            "And these are again, this is like the covariance matrix of the kernel matrix and so forth.",
            "And that these countries, as we are now again linear, we can apply the original linear is of algorithm.",
            "This whole thing has the complexity of a cube, so it's quite pet, but also quite obvious.",
            "I mean everybody who works with kernels knows how this works.",
            "However, there is actually something that was very unexpected to us and that is that the.",
            "Algorithm exhibits, Interestingly, overfitting and numerical instability.",
            "Why is that so surprising?",
            "I mean overfitting happens all the time.",
            "It does not happen with nonlinear or specifically with kernel PCA.",
            "Kernel PCA is extremely stable.",
            "God knows why.",
            "So The thing is, in this plot here I plotted the slowness on a previously unseen test set.",
            "So the test slowness.",
            "Over different Gaussian kernels with different kind of width and you see that here is like a huge.",
            "Bump where there shouldn't be one.",
            "So by the way, lower is better because you want to get slower.",
            "And in this area we actually have an overfitting regime.",
            "Now the.",
            "Even more surprising thing is that if you now look at the training variance, this is the variance on the training set.",
            "By definition, this is supposed to be 1.",
            "However, if you look into the variance, or if you just measure it afterwards, you find that.",
            "The real variance that is found by the algorithm is actually deviating from that and deviates more the wider the kernel width becomes an interesting Lee.",
            "This is something that has been analytically proven for a very related algorithm.",
            "This is the kernel Canonical correlation analysis.",
            "2007 yeah, so the question was how to overcome this because we still wanted to use this algorithm."
        ],
        [
            "And the answer was OK. First, the first thing that you can do is.",
            "Your function classes is to big.",
            "You put a penalising term on it.",
            "Basically what we did here is simply you.",
            "Penalized the Hilbert norm of the function of the of the feature.",
            "So you're.",
            "Your objective.",
            "Here in blue just gets a.",
            "Second part edit here in green.",
            "Which is weighted by some regularization factor Lambda Lambda.",
            "And the big advantage of this approach is that if you do all the math again, you end up with an objective which looks like this.",
            "This is almost the same objective like before.",
            "The objective before had only the blue part.",
            "And this addition here of this kernel matrix is.",
            "Well, in the greater scheme of things it is just for free, so it's the computer.",
            "Computational overhead is really marginal.",
            "And it works at the first glance surprisingly well.",
            "So in this this is basically the same plot as before, but the black line was the original, where the regularization parameter was zero.",
            "And the other lines are various.",
            "Regularization parameters.",
            "And you see that you reach.",
            "Really, a significantly better slowness.",
            "But you also basically need for every kernel.",
            "To select matching regularization parameter.",
            "So again you have to do a lot of cross validation and so forth to get this done.",
            "That's not really a good thing, and another thing is that also appears is that this Lambda can become extremely small, so.",
            "This Lambda here is if I make it so I can make it slightly smaller, but at some point it just hits computational precision and then it's gone.",
            "That's specifically bad for large kernel width."
        ],
        [
            "There's a second option to do regularization, and that's like more like in an indirect approach, because you could also say my function classes to big, so I just restrict the function classes instead of just penalising it.",
            "In this case we just say OK, instead of using all of the training, the kernel functions of the training data.",
            "For linear expression, we just use a subset of those.",
            "So in other words we create a sparse version of the algorithm.",
            "This is an interesting way.",
            "This can be seen as an implicit regularization of the function complexity itself.",
            "So what we do is like this is almost the same equation as before.",
            "The only difference is that now we don't have the training data X, but instead some subset that and the subset is really like EM Domino's.",
            "We consists of M samples and M is supposed to be much smaller than obviously.",
            "And the only difference that we have to make or the only change that we have to make the algorithm is basically to.",
            "Exchange the definition of the kernel matrix.",
            "And then all of those KK transposed, and so that you've seen before makes much more sense.",
            "So the good news is it reduces the complexity to M squared North, which is linear in the number of training data.",
            "Which is great.",
            "It is also if you look at the.",
            "At the graphs again, these are the same graphs is that you've seen before.",
            "Really efficient, so the black line is without additional regularization.",
            "That means with a regular regularization parameter of 0.",
            "And it's actually achieving the best tests loaners for all tested kernels and very very reliable fashion.",
            "The numerical instability in the training variance is also gone, almost gone.",
            "I mean, this is really numerical stuff.",
            "So in other words.",
            "This is.",
            "Actually, a much better way to regularize the problem.",
            "However, it's it is also sensitive to the selection of the subset.",
            "So basically the question which of my axis are that this is a classical support vector selection problem and we actually did a lot of.",
            "We did a lot of work.",
            "In the paper and derived another new algorithm and so forth.",
            "But I don't want to talk about that because of the time constraint.",
            "And."
        ],
        [
            "Instead, I want to show you that.",
            "These features actually contain.",
            "Information about the latent variables.",
            "This is relatively new stuff, so it's not in the paper.",
            "It will be hopefully in an extended Journal version.",
            "And I just so, so don't expect too smooth curves or anything.",
            "What I do here is I do have a lower classification example from the beginning, so I start with my spoken word.",
            "Then I do adelaid embedding or just like have a window run over the data stream.",
            "I extract my features.",
            "And then I do a quadratic discriminant analysis on this.",
            "Just this analyzes analysis basically just estimates the mean and the covariance matrix of the labeled data.",
            "So for each, for each of the goals, you have your own mean and covariance matrix and then construct the discriminant function out of them.",
            "So this is really a very straightforward and very easy way to classify, and it's by far not the best.",
            "It is just to show that you can apply any algorithms.",
            "And this discriminative function.",
            "Here again, it's something overtime.",
            "But since we want to rate the whole world in the sense of which role was in the center, we just integrate all of the red part and all of the all of the red path and all of the green part.",
            "And then we decide whether it was the word he tore the warhead.",
            "So."
        ],
        [
            "The results.",
            "Are here this is the classification accuracy over are plotted as a function of the feature space that we used or the dimensionality of the feature space to be exact.",
            "Mind that this is a log scale, so.",
            "Yeah.",
            "The dotted line that you see here is actually the classification accuracy.",
            "If you would use the original space, so just the delayed embedding and it's still kind of good because the problem is not that hard.",
            "It's two classes and so forth.",
            "However, the black line here represents the same.",
            "The same accuracy if you use.",
            "Kind of each analysis features instead, and the interesting bit is here, and this is because the log scale is important that.",
            "Roughly 5 kernels fluffy channel assist features are or contain more information about the latent variable or about the specific problem, then the original data stream of which was 500 dimensional.",
            "And as a comparison, and also we reached well, perfect, but it's pretty good considering that we really did use algorithms that were not really optimized for the problem as a comparison to show you that this is not what every unsupervised method does, I plotted the same curve here in blue for.",
            "For kernel PCA.",
            "So this is basically PCA on the same function class that we used for the slow feature analysis.",
            "And you see, OK, the first thing is you need more features to encode the same thing however, but I want actually want to point out what I find much more interesting.",
            "Here is.",
            "This dent in the performance, which basically means that what slow feature analysis is extracting, are really latent variables and.",
            "You just get more and more latent variables in it.",
            "They might be useless, but they're still kind of like what you wanted.",
            "PCA constructs something completely different and thus introduces can introduce at least very useless information into your encoding.",
            "So basic."
        ],
        [
            "Lee the take home message is when you are doing or if you want to do linear classification or regression with some latent variables and you have a very complicated and complex time series data, but you have a reasonable kernel for them then and you have no idea how to construct this feature space that I'm talking about then just try kernel SFA and this should be able to approximate some kind of a free basis in your space of latent."
        ],
        [
            "Evans, thank you very much for your attention.",
            "How about computing power?",
            "That's the question.",
            "Compute with computing power you mean real time, yeah or complexity?",
            "I mean, how much more effort?",
            "Our Chief this Queen yeah."
        ],
        [
            "Lots.",
            "So this is nothing you can actually answer directly, because you can basically shrink and grow your function space just as you like it.",
            "But in this setup we had to do eigenvalue decomposition on a. I think 1500 * 1500 metrics.",
            "In comparison to, just like estimating the mean and the cover ions of a 500.",
            "Dimensional vector, so this is.",
            "It's really in a more complex problem, but the complexity."
        ],
        [
            "The whole thing is basically.",
            "Your N ^2 N so it is linear in the training data is just like if you use lots of support vectors or large subset then the complexity is very high.",
            "However, if you somehow managed to do the same thing with a small number then your complexity can also be much much lower than the original problem now.",
            "Well let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will talk about regularised sparse kernel feature analysis and I know it's early, but I'm curious how many of you have ever heard the term slow feature analysis.",
                    "label": 1
                },
                {
                    "sent": "Please raise your hand.",
                    "label": 0
                },
                {
                    "sent": "I thought so so so feature analysis is an unsupervised learning mechanism.",
                    "label": 0
                },
                {
                    "sent": "And it's about time series, and it's very strongly correlated or related.",
                    "label": 0
                },
                {
                    "sent": "To some methods in graph spectral analysis.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "So because of this, it would have fitted in lots of slots.",
                    "label": 0
                },
                {
                    "sent": "However, it is actually fitting quite well here, because what?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually want to talk about is how to apply linear algorithms to complicated nonlinear datasets.",
                    "label": 0
                },
                {
                    "sent": "So if we for example have a classification or regression problem with respect to latent variables.",
                    "label": 1
                },
                {
                    "sent": "I will explain this at the example of a lower classification task where like people say things like head and heat.",
                    "label": 0
                },
                {
                    "sent": "And the interesting bit is.",
                    "label": 0
                },
                {
                    "sent": "The middle vowel.",
                    "label": 0
                },
                {
                    "sent": "So what we have is.",
                    "label": 0
                },
                {
                    "sent": "We we just have the.",
                    "label": 0
                },
                {
                    "sent": "The sound file and we want to have in the end some discriminative function.",
                    "label": 0
                },
                {
                    "sent": "However, the problem is where to get there and the first thing, OK, it's like a technical step.",
                    "label": 0
                },
                {
                    "sent": "It's not that.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Essential for the method.",
                    "label": 0
                },
                {
                    "sent": "It's that we need to have a proper multi dimensional feature space.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we take a sliding window and move it over it and what we get is then this kind of delayed embedding of the original sound signal.",
                    "label": 0
                },
                {
                    "sent": "So that's not very surprising.",
                    "label": 0
                },
                {
                    "sent": "This many people have done that before and it's also not doesn't work that great.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work that great cause.",
                    "label": 0
                },
                {
                    "sent": "We forgot to apply a linear algorithm in this setup.",
                    "label": 0
                },
                {
                    "sent": "We would actually need.",
                    "label": 0
                },
                {
                    "sent": "Or we need to consider two things.",
                    "label": 0
                },
                {
                    "sent": "The first thing is that our latent variables might be non linearly embedded.",
                    "label": 0
                },
                {
                    "sent": "In this.",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "So a linear method might just not do the trick at all.",
                    "label": 0
                },
                {
                    "sent": "The second thing is that the solution that is in this case this discrimination function, might be nonlinear in this space of the latent variables as well.",
                    "label": 1
                },
                {
                    "sent": "So the question is how do we treat this kind of thing in the classical approach?",
                    "label": 0
                },
                {
                    "sent": "Well, if you don't want to go into support vector machine.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of these things is to just construct a feature space.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "And then run the the linear algorithm in this feature space.",
                    "label": 0
                },
                {
                    "sent": "So however, this feature space again needs to have some properties to work sufficiently.",
                    "label": 0
                },
                {
                    "sent": "The first is this feature space needs needs to be nonlinear in the data, obviously because if the space of latent variables is nonlinear, embedded in the data.",
                    "label": 1
                },
                {
                    "sent": "The feature space needs to do to respect that.",
                    "label": 0
                },
                {
                    "sent": "The second thing is that Phi or this feature space needs to be a functional basis in the space of latent variables.",
                    "label": 1
                },
                {
                    "sent": "So it's not just sufficient to extract all of the latent variables, you also need to construct a functional basis in this space.",
                    "label": 0
                },
                {
                    "sent": "And the last one is that the feature space needs to be low dimensional.",
                    "label": 0
                },
                {
                    "sent": "That's very important because the more dimensions we have, the most samples we need for a good.",
                    "label": 0
                },
                {
                    "sent": "For a good approximation stochastically.",
                    "label": 0
                },
                {
                    "sent": "So all of this is.",
                    "label": 0
                },
                {
                    "sent": "Usually done by hand, so people just construct their feature space by some heuristics, and that's actually the way to go.",
                    "label": 0
                },
                {
                    "sent": "If you have information.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About your domain, however.",
                    "label": 0
                },
                {
                    "sent": "What if this is not the case?",
                    "label": 0
                },
                {
                    "sent": "So what if you need to construct your nonlinear feature space from data?",
                    "label": 1
                },
                {
                    "sent": "And in this talk I will specifically look into the possibilities of construct, so use unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "In other words, to construct the feature space.",
                    "label": 0
                },
                {
                    "sent": "With an unsupervised learning method and.",
                    "label": 0
                },
                {
                    "sent": "If we think about unsupervised learning methods, the first thing that comes to mind is always PCA.",
                    "label": 0
                },
                {
                    "sent": "I will not ask you if you have heard the term PCA before, 'cause it's probably everybody.",
                    "label": 0
                },
                {
                    "sent": "However, there are some problems that come into play when you look in nonlinear PCA.",
                    "label": 1
                },
                {
                    "sent": "There is a very famous paper about kernel principal component analysis.",
                    "label": 0
                },
                {
                    "sent": "And that is that PCA.",
                    "label": 0
                },
                {
                    "sent": "In principle, is a reconstruction algorithm is tries to compress the data.",
                    "label": 0
                },
                {
                    "sent": "So if you blow up.",
                    "label": 0
                },
                {
                    "sent": "The function space in which you look for the highest variance.",
                    "label": 1
                },
                {
                    "sent": "Then Anne.",
                    "label": 0
                },
                {
                    "sent": "In turn you get a reconstruction of that function space.",
                    "label": 0
                },
                {
                    "sent": "So basically what you are what PCA is extracting for you has not necessarily anything to do with the latent variables.",
                    "label": 1
                },
                {
                    "sent": "That's to say, construct any kind of function space and.",
                    "label": 0
                },
                {
                    "sent": "In them, however, there is another.",
                    "label": 0
                },
                {
                    "sent": "Another unsupervised learning mechanism which is called slow feature analysis, and this has some really interesting properties which are very suddenly known to anybody.",
                    "label": 0
                },
                {
                    "sent": "And that is that if you.",
                    "label": 1
                },
                {
                    "sent": "Do a nonlinear version of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Your features actually converge.",
                    "label": 0
                },
                {
                    "sent": "To something independent of the function class.",
                    "label": 0
                },
                {
                    "sent": "Given that the function class is high enough.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is this mysterious thing?",
                    "label": 0
                },
                {
                    "sent": "First of all, it's about temporal or time series just works on time series.",
                    "label": 0
                },
                {
                    "sent": "So the goal is to filter temporally coherent signals out of this time series.",
                    "label": 1
                },
                {
                    "sent": "And they with temporal coherence signals, we just mean that the temporal derivative.",
                    "label": 0
                },
                {
                    "sent": "Of the feature on this case, a vector of features.",
                    "label": 0
                },
                {
                    "sent": "Is as small as possible with respect to the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "And temporal derivative is really just like the difference between time T and T + 1.",
                    "label": 0
                },
                {
                    "sent": "Because the slowest and this is where the slow comes from.",
                    "label": 0
                },
                {
                    "sent": "So where the slowest possible feature is basically a zero vector.",
                    "label": 0
                },
                {
                    "sent": "To prevent that, we need to do to add some constraints like 0 mycamine constraint.",
                    "label": 0
                },
                {
                    "sent": "This is basically also the same for PCA, but what we actually need is a unit variance and correlation constraint.",
                    "label": 0
                },
                {
                    "sent": "The Decorrelation is again pretty obvious because we're talking about functional basis later and functional basis are optimally decorrelated and the unit variance is just like to let things do have a change, they just change slowly.",
                    "label": 0
                },
                {
                    "sent": "So the original algorithm, which was proposed by Elvis Cut and Terrance Junior Ski.",
                    "label": 0
                },
                {
                    "sent": "Works the following.",
                    "label": 0
                },
                {
                    "sent": "In a linear setup.",
                    "label": 0
                },
                {
                    "sent": "So just imagine this solid line to be the isoline of the covariance matrix of the data.",
                    "label": 0
                },
                {
                    "sent": "And that dotted ellipse to be the isoline of the covariance matrix of the derivative of the data to the temporal derivative.",
                    "label": 0
                },
                {
                    "sent": "What happens is first we fulfill the zero mean constraint, but just moving the whole thing to the center.",
                    "label": 0
                },
                {
                    "sent": "Then we decorrelate by just running a PCA, and then rotate this the data in the directions of the eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Now we're leaving the PCA track because we fear it, or we fulfill the unit variance constraint, which means that we just like re scale all of the dimensions so that you have variance one in all directions.",
                    "label": 0
                },
                {
                    "sent": "And now you take the dotted.",
                    "label": 0
                },
                {
                    "sent": "Isoline to rotate the whole system in the direction of the slowest change.",
                    "label": 0
                },
                {
                    "sent": "So this is just didn't really mesh or doesn't.",
                    "label": 0
                },
                {
                    "sent": "Didn't really do all the things that I promised in the beginning.",
                    "label": 0
                },
                {
                    "sent": "However, there is an analysis that says if you have like an.",
                    "label": 0
                },
                {
                    "sent": "An infinite time series and the non restricted function class.",
                    "label": 1
                },
                {
                    "sent": "Then you can actually say something about the features that that will emerge and the features are actually four year bases in the space of the latent variables.",
                    "label": 0
                },
                {
                    "sent": "And the interesting bit is that you do not need to know anything about this base of latent variables.",
                    "label": 0
                },
                {
                    "sent": "The only thing that you will get is.",
                    "label": 0
                },
                {
                    "sent": "This four year basis is ordered according to the.",
                    "label": 0
                },
                {
                    "sent": "Speed of change in the space of latent variables.",
                    "label": 0
                },
                {
                    "sent": "So it's actually what you get is the first features will encode the slowest changing latent variables.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that all of those noise things that have nothing to do with the latent variable space.",
                    "label": 0
                },
                {
                    "sent": "Are so fast that they are basically encoded in feature 10,000,000, so they will never appear.",
                    "label": 0
                },
                {
                    "sent": "All of this is conditioned on the assumption that you have an unrestricted function class.",
                    "label": 0
                },
                {
                    "sent": "Basically a very powerful one.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we were not the first one to do that, we took the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Function class of the reproducing kernel Hilbert spaces.",
                    "label": 1
                },
                {
                    "sent": "So we used.",
                    "label": 0
                },
                {
                    "sent": "We did the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We formulated the whole algorithm which is very easy because in the function class here, I guess that most of you are familiar with it.",
                    "label": 0
                },
                {
                    "sent": "You just express the function as a linear weighting of kernel functions which are parameterized by the training data.",
                    "label": 0
                },
                {
                    "sent": "However, by all of the training data, so this is a sum from 1, two the number of training samples.",
                    "label": 0
                },
                {
                    "sent": "The advantage is that now we can basically post the whole the whole nonlinear optimization problem in such a way that it is linear in the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "So here you just say, OK, this is the.",
                    "label": 0
                },
                {
                    "sent": "Derivative of the kernel matrix temporal derivative.",
                    "label": 0
                },
                {
                    "sent": "And these are again, this is like the covariance matrix of the kernel matrix and so forth.",
                    "label": 0
                },
                {
                    "sent": "And that these countries, as we are now again linear, we can apply the original linear is of algorithm.",
                    "label": 0
                },
                {
                    "sent": "This whole thing has the complexity of a cube, so it's quite pet, but also quite obvious.",
                    "label": 0
                },
                {
                    "sent": "I mean everybody who works with kernels knows how this works.",
                    "label": 0
                },
                {
                    "sent": "However, there is actually something that was very unexpected to us and that is that the.",
                    "label": 0
                },
                {
                    "sent": "Algorithm exhibits, Interestingly, overfitting and numerical instability.",
                    "label": 0
                },
                {
                    "sent": "Why is that so surprising?",
                    "label": 0
                },
                {
                    "sent": "I mean overfitting happens all the time.",
                    "label": 0
                },
                {
                    "sent": "It does not happen with nonlinear or specifically with kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "Kernel PCA is extremely stable.",
                    "label": 0
                },
                {
                    "sent": "God knows why.",
                    "label": 0
                },
                {
                    "sent": "So The thing is, in this plot here I plotted the slowness on a previously unseen test set.",
                    "label": 0
                },
                {
                    "sent": "So the test slowness.",
                    "label": 0
                },
                {
                    "sent": "Over different Gaussian kernels with different kind of width and you see that here is like a huge.",
                    "label": 0
                },
                {
                    "sent": "Bump where there shouldn't be one.",
                    "label": 0
                },
                {
                    "sent": "So by the way, lower is better because you want to get slower.",
                    "label": 0
                },
                {
                    "sent": "And in this area we actually have an overfitting regime.",
                    "label": 0
                },
                {
                    "sent": "Now the.",
                    "label": 0
                },
                {
                    "sent": "Even more surprising thing is that if you now look at the training variance, this is the variance on the training set.",
                    "label": 0
                },
                {
                    "sent": "By definition, this is supposed to be 1.",
                    "label": 0
                },
                {
                    "sent": "However, if you look into the variance, or if you just measure it afterwards, you find that.",
                    "label": 0
                },
                {
                    "sent": "The real variance that is found by the algorithm is actually deviating from that and deviates more the wider the kernel width becomes an interesting Lee.",
                    "label": 0
                },
                {
                    "sent": "This is something that has been analytically proven for a very related algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is the kernel Canonical correlation analysis.",
                    "label": 0
                },
                {
                    "sent": "2007 yeah, so the question was how to overcome this because we still wanted to use this algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the answer was OK. First, the first thing that you can do is.",
                    "label": 0
                },
                {
                    "sent": "Your function classes is to big.",
                    "label": 0
                },
                {
                    "sent": "You put a penalising term on it.",
                    "label": 0
                },
                {
                    "sent": "Basically what we did here is simply you.",
                    "label": 0
                },
                {
                    "sent": "Penalized the Hilbert norm of the function of the of the feature.",
                    "label": 0
                },
                {
                    "sent": "So you're.",
                    "label": 0
                },
                {
                    "sent": "Your objective.",
                    "label": 0
                },
                {
                    "sent": "Here in blue just gets a.",
                    "label": 0
                },
                {
                    "sent": "Second part edit here in green.",
                    "label": 0
                },
                {
                    "sent": "Which is weighted by some regularization factor Lambda Lambda.",
                    "label": 0
                },
                {
                    "sent": "And the big advantage of this approach is that if you do all the math again, you end up with an objective which looks like this.",
                    "label": 0
                },
                {
                    "sent": "This is almost the same objective like before.",
                    "label": 0
                },
                {
                    "sent": "The objective before had only the blue part.",
                    "label": 0
                },
                {
                    "sent": "And this addition here of this kernel matrix is.",
                    "label": 0
                },
                {
                    "sent": "Well, in the greater scheme of things it is just for free, so it's the computer.",
                    "label": 0
                },
                {
                    "sent": "Computational overhead is really marginal.",
                    "label": 1
                },
                {
                    "sent": "And it works at the first glance surprisingly well.",
                    "label": 0
                },
                {
                    "sent": "So in this this is basically the same plot as before, but the black line was the original, where the regularization parameter was zero.",
                    "label": 0
                },
                {
                    "sent": "And the other lines are various.",
                    "label": 0
                },
                {
                    "sent": "Regularization parameters.",
                    "label": 0
                },
                {
                    "sent": "And you see that you reach.",
                    "label": 0
                },
                {
                    "sent": "Really, a significantly better slowness.",
                    "label": 0
                },
                {
                    "sent": "But you also basically need for every kernel.",
                    "label": 0
                },
                {
                    "sent": "To select matching regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "So again you have to do a lot of cross validation and so forth to get this done.",
                    "label": 0
                },
                {
                    "sent": "That's not really a good thing, and another thing is that also appears is that this Lambda can become extremely small, so.",
                    "label": 1
                },
                {
                    "sent": "This Lambda here is if I make it so I can make it slightly smaller, but at some point it just hits computational precision and then it's gone.",
                    "label": 0
                },
                {
                    "sent": "That's specifically bad for large kernel width.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a second option to do regularization, and that's like more like in an indirect approach, because you could also say my function classes to big, so I just restrict the function classes instead of just penalising it.",
                    "label": 0
                },
                {
                    "sent": "In this case we just say OK, instead of using all of the training, the kernel functions of the training data.",
                    "label": 0
                },
                {
                    "sent": "For linear expression, we just use a subset of those.",
                    "label": 1
                },
                {
                    "sent": "So in other words we create a sparse version of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is an interesting way.",
                    "label": 0
                },
                {
                    "sent": "This can be seen as an implicit regularization of the function complexity itself.",
                    "label": 1
                },
                {
                    "sent": "So what we do is like this is almost the same equation as before.",
                    "label": 0
                },
                {
                    "sent": "The only difference is that now we don't have the training data X, but instead some subset that and the subset is really like EM Domino's.",
                    "label": 0
                },
                {
                    "sent": "We consists of M samples and M is supposed to be much smaller than obviously.",
                    "label": 0
                },
                {
                    "sent": "And the only difference that we have to make or the only change that we have to make the algorithm is basically to.",
                    "label": 0
                },
                {
                    "sent": "Exchange the definition of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "And then all of those KK transposed, and so that you've seen before makes much more sense.",
                    "label": 1
                },
                {
                    "sent": "So the good news is it reduces the complexity to M squared North, which is linear in the number of training data.",
                    "label": 0
                },
                {
                    "sent": "Which is great.",
                    "label": 0
                },
                {
                    "sent": "It is also if you look at the.",
                    "label": 0
                },
                {
                    "sent": "At the graphs again, these are the same graphs is that you've seen before.",
                    "label": 0
                },
                {
                    "sent": "Really efficient, so the black line is without additional regularization.",
                    "label": 1
                },
                {
                    "sent": "That means with a regular regularization parameter of 0.",
                    "label": 0
                },
                {
                    "sent": "And it's actually achieving the best tests loaners for all tested kernels and very very reliable fashion.",
                    "label": 0
                },
                {
                    "sent": "The numerical instability in the training variance is also gone, almost gone.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is really numerical stuff.",
                    "label": 1
                },
                {
                    "sent": "So in other words.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Actually, a much better way to regularize the problem.",
                    "label": 0
                },
                {
                    "sent": "However, it's it is also sensitive to the selection of the subset.",
                    "label": 0
                },
                {
                    "sent": "So basically the question which of my axis are that this is a classical support vector selection problem and we actually did a lot of.",
                    "label": 0
                },
                {
                    "sent": "We did a lot of work.",
                    "label": 0
                },
                {
                    "sent": "In the paper and derived another new algorithm and so forth.",
                    "label": 0
                },
                {
                    "sent": "But I don't want to talk about that because of the time constraint.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instead, I want to show you that.",
                    "label": 0
                },
                {
                    "sent": "These features actually contain.",
                    "label": 0
                },
                {
                    "sent": "Information about the latent variables.",
                    "label": 0
                },
                {
                    "sent": "This is relatively new stuff, so it's not in the paper.",
                    "label": 0
                },
                {
                    "sent": "It will be hopefully in an extended Journal version.",
                    "label": 0
                },
                {
                    "sent": "And I just so, so don't expect too smooth curves or anything.",
                    "label": 0
                },
                {
                    "sent": "What I do here is I do have a lower classification example from the beginning, so I start with my spoken word.",
                    "label": 0
                },
                {
                    "sent": "Then I do adelaid embedding or just like have a window run over the data stream.",
                    "label": 0
                },
                {
                    "sent": "I extract my features.",
                    "label": 0
                },
                {
                    "sent": "And then I do a quadratic discriminant analysis on this.",
                    "label": 1
                },
                {
                    "sent": "Just this analyzes analysis basically just estimates the mean and the covariance matrix of the labeled data.",
                    "label": 0
                },
                {
                    "sent": "So for each, for each of the goals, you have your own mean and covariance matrix and then construct the discriminant function out of them.",
                    "label": 0
                },
                {
                    "sent": "So this is really a very straightforward and very easy way to classify, and it's by far not the best.",
                    "label": 0
                },
                {
                    "sent": "It is just to show that you can apply any algorithms.",
                    "label": 0
                },
                {
                    "sent": "And this discriminative function.",
                    "label": 0
                },
                {
                    "sent": "Here again, it's something overtime.",
                    "label": 0
                },
                {
                    "sent": "But since we want to rate the whole world in the sense of which role was in the center, we just integrate all of the red part and all of the all of the red path and all of the green part.",
                    "label": 0
                },
                {
                    "sent": "And then we decide whether it was the word he tore the warhead.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The results.",
                    "label": 0
                },
                {
                    "sent": "Are here this is the classification accuracy over are plotted as a function of the feature space that we used or the dimensionality of the feature space to be exact.",
                    "label": 1
                },
                {
                    "sent": "Mind that this is a log scale, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The dotted line that you see here is actually the classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "If you would use the original space, so just the delayed embedding and it's still kind of good because the problem is not that hard.",
                    "label": 0
                },
                {
                    "sent": "It's two classes and so forth.",
                    "label": 0
                },
                {
                    "sent": "However, the black line here represents the same.",
                    "label": 0
                },
                {
                    "sent": "The same accuracy if you use.",
                    "label": 0
                },
                {
                    "sent": "Kind of each analysis features instead, and the interesting bit is here, and this is because the log scale is important that.",
                    "label": 0
                },
                {
                    "sent": "Roughly 5 kernels fluffy channel assist features are or contain more information about the latent variable or about the specific problem, then the original data stream of which was 500 dimensional.",
                    "label": 0
                },
                {
                    "sent": "And as a comparison, and also we reached well, perfect, but it's pretty good considering that we really did use algorithms that were not really optimized for the problem as a comparison to show you that this is not what every unsupervised method does, I plotted the same curve here in blue for.",
                    "label": 0
                },
                {
                    "sent": "For kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "So this is basically PCA on the same function class that we used for the slow feature analysis.",
                    "label": 0
                },
                {
                    "sent": "And you see, OK, the first thing is you need more features to encode the same thing however, but I want actually want to point out what I find much more interesting.",
                    "label": 0
                },
                {
                    "sent": "Here is.",
                    "label": 0
                },
                {
                    "sent": "This dent in the performance, which basically means that what slow feature analysis is extracting, are really latent variables and.",
                    "label": 0
                },
                {
                    "sent": "You just get more and more latent variables in it.",
                    "label": 0
                },
                {
                    "sent": "They might be useless, but they're still kind of like what you wanted.",
                    "label": 0
                },
                {
                    "sent": "PCA constructs something completely different and thus introduces can introduce at least very useless information into your encoding.",
                    "label": 0
                },
                {
                    "sent": "So basic.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lee the take home message is when you are doing or if you want to do linear classification or regression with some latent variables and you have a very complicated and complex time series data, but you have a reasonable kernel for them then and you have no idea how to construct this feature space that I'm talking about then just try kernel SFA and this should be able to approximate some kind of a free basis in your space of latent.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evans, thank you very much for your attention.",
                    "label": 0
                },
                {
                    "sent": "How about computing power?",
                    "label": 0
                },
                {
                    "sent": "That's the question.",
                    "label": 0
                },
                {
                    "sent": "Compute with computing power you mean real time, yeah or complexity?",
                    "label": 0
                },
                {
                    "sent": "I mean, how much more effort?",
                    "label": 0
                },
                {
                    "sent": "Our Chief this Queen yeah.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lots.",
                    "label": 0
                },
                {
                    "sent": "So this is nothing you can actually answer directly, because you can basically shrink and grow your function space just as you like it.",
                    "label": 0
                },
                {
                    "sent": "But in this setup we had to do eigenvalue decomposition on a. I think 1500 * 1500 metrics.",
                    "label": 0
                },
                {
                    "sent": "In comparison to, just like estimating the mean and the cover ions of a 500.",
                    "label": 0
                },
                {
                    "sent": "Dimensional vector, so this is.",
                    "label": 0
                },
                {
                    "sent": "It's really in a more complex problem, but the complexity.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The whole thing is basically.",
                    "label": 0
                },
                {
                    "sent": "Your N ^2 N so it is linear in the training data is just like if you use lots of support vectors or large subset then the complexity is very high.",
                    "label": 0
                },
                {
                    "sent": "However, if you somehow managed to do the same thing with a small number then your complexity can also be much much lower than the original problem now.",
                    "label": 0
                },
                {
                    "sent": "Well let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}