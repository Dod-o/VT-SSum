{
    "id": "funu6z4w2hmeamafk4mcfhx4amdd4z6i",
    "title": "Category Detection Using Hierarchical Mean Shift",
    "info": {
        "author": [
            "Weng-Keen Wong, School of Electrical Engineering and Computer Science, Oregon State University"
        ],
        "published": "Aug. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Data Mining->Anomaly Detection"
        ]
    },
    "url": "http://videolectures.net/kdd09_wong_cduhms/",
    "segmentation": [
        [
            "So today's talk is on category detection using hierarchal mean shift and this is joint work with my student Pavan Rotary who couldn't be here because of visa issues and I'm linking long.",
            "I'm going to be presenting."
        ],
        [
            "This work?",
            "So in many applications that perform surveillance or monitoring or scientific discovery and data cleaning, it's really important to discover the anomalies because there are things of interest and in order to discover these anomalies you need to first form a statistical model of your data and then identify data points that are unusual according to this statistical model and one problem with this approach is that in many cases the anomalies that are discovered to be statistically significant aren't terribly interesting to you because.",
            "They correspond to known sources or noise, or they are combinations of features that are just not."
        ],
        [
            "Interesting, and here's an example thanks to Dan Pelikan.",
            "Also the slow Digital Sky survey for these pictures.",
            "If you were to take a look at some of these pictures of galaxies, it turns out that in many cases 99.9% of the data.",
            "These are known galaxies that aren't very interesting, and they're sort of the common or normal cases in the data.",
            "About 1% of the data corresponds to anomalies.",
            "Now if you can draw your attention to the bottom 2 boxes there, the box no left hand side corresponds to.",
            "A satellite trail, and if you're an astrophysicist, you're not going to win a Nobel Prize for discovering a satellite trail.",
            "It's what would be considered a boring anomaly.",
            "On the other hand, the bottom right hand corner corresponds to a very strange hydrogen region, and this is an interesting anomaly.",
            "And notice that it is exactly about 1% of the point 1% of the anomalies are actually interesting."
        ],
        [
            "So one way of dealing with this problem, this is a fairly novel approach that was proposed by Pelican more in 2004.",
            "It's known as category detection.",
            "It's basically a Explorer process for exploratory data analysis that involves a human in the loop and the basic idea in this process is first you take your data set.",
            "You build a statistical model of your data.",
            "You then identify the anomalies and then you present certain anomalies to the user to label.",
            "And the user then labels these anomalies.",
            "You update your model with the labels and the loop continues again.",
            "Now this is not quite active, learn."
        ],
        [
            "There's one little twist to this.",
            "When the user is labeling the query, the user can either label the data point under an existing class or category, or the user can declare the data point to belong to a completely new undiscovered category, and you basically keep going until you discover all the classes of interest.",
            "Now, of course, if the user discovers a data point that represents a cluster of interest, the user may stop and dig a little bit deeper into that cluster."
        ],
        [
            "So the goal of category detection is the following.",
            "You want to present to the user a single representative instance from each class or category in your data set in as few few queries as possible.",
            "Now this turns out to be fairly straightforward if your data is perfectly balanced in terms of the size of your classes.",
            "However, in many cases there is a severe class imbalance in your data, and this makes it very difficult to find these rare categories, and that's why category detection is often referred to as.",
            "Rare category detection."
        ],
        [
            "OK, so that's the introduction in my talk.",
            "I'm just going to briefly cover related work and then die."
        ],
        [
            "Into the methodology in terms of related work, the paper that first proposed category detection presented the Interleaf algorithm, which is an algorithm based on M that proposes data points that are leased, owned by the mixture model components as a query data points.",
            "There's a later paper last year that presented a nearest neighbor type of approach that query data points that had the biggest difference in terms of the local density around it, and there's a nice theoretical paper for identifying multiple outputs in a function.",
            "For specific types of functions, and it had a nice theoretical analysis for these specific cases.",
            "The first 2 algorithms innerleithen MDM, both required the user to know something about the data set.",
            "For example, they interleave requires the user to know how many classes there are in total in a data set, and MDM requires the user to have some notion of the prior probabilities of a data point belonging to each class."
        ],
        [
            "OK, now in order to describe my algorithm I need to give a little bit of background about the mean shift algorithm, which a lot of people envision know about this, but it's not so well known in machine learning.",
            "It mean shift.",
            "You have what's known as a reference data set.",
            "This is the data set that you want to cluster and mean shift is a clustering algorithm.",
            "You also have another data set known as the query data set.",
            "This corresponds to the data set of coordinates that you would like to assign to cluster centers, and the way mean shift works is as follows.",
            "Suppose the + there, that's red.",
            "Is your query data point what you're going to do is you're going to draw a circle around it, and the radius of the circle is determined by a bandwidth parameter.",
            "You will then compute the center of mass in that circle shown by that purple + and you're going to compute what's known as the mean shift vector, which basically follows the gradient of the density function and what you will then do is you're going to shift that query point to the purple + Now, if you look at the formula down there, which I don't have much time to explain, that's the formula for the mean shift vector.",
            "It looks a lot like a kernel density estimation formula an it's very similar.",
            "The one difference here is that you're dealing with K prime, which is the derivative of the kernel function.",
            "So if you do this.",
            "You'll end up doing the following.",
            "You will shift that query point to where the center of mass is, and then."
        ],
        [
            "Repeat the process over again.",
            "You'll find a new center of mass Calculator, new mean shift vector and shift it once more, so you'll keep doing this until you converge to a cluster mode or close."
        ],
        [
            "Center now there is 1 little additional thing that you can do with mean Shift, which is known as blurring and this is what happens if your query points are in fact the reference data points in your data set.",
            "So when you're in fact you're in effect doing is you are moving your reference data points towards these basins of attraction and you are progressively blurring your data set.",
            "So if you imagine that your data set is like an image, you're progressively blurring that image."
        ],
        [
            "Let me give you a quick sort of visual picture of what it looks like when you finish running mean shift.",
            "What's happening here is if you imagine the plus signs that are red being your original data points, you'll notice that they all get sort of pulled into these basins of attraction where the cluster centers are shown by these green triangles."
        ],
        [
            "So onto my algorithm.",
            "Now the algorithm consists of three parts.",
            "First off you need to sphere your data.",
            "This is fairly standard thing which I'm not going to cover.",
            "The second thing is you need to run hierarchical mean shift.",
            "I'll explain what that means and the third thing consists of the querying process that you're going to present data points that user for late."
        ],
        [
            "Going.",
            "K now hierarchical mean shift consists of progressively blurring your data.",
            "So what you're going to do is you'll start out with all your data points in the reference data set.",
            "You're going to run blurring with some bandwidth value, and that's going to form a bunch of cluster centers.",
            "Those cluster centers then become data points for the next time you'll run mean shift, except this time you're going to increase the bandwidth by multiplying your old bandwidth by some constant factor K, where K is bigger than one.",
            "And you'll keep doing this until you eventually get everything sort of clumping together into one big cluster.",
            "And once you're done that, you're going to go back and take a look at how all these cluster centers joined together and you'll basically produce a dendrogram very much like that one.",
            "Now you'll notice that in this dendrogram, whenever you see sort of two branches joining together into one, you're forming a new cluster at that point at each level.",
            "This is important and also this is actually a fairly computationally expensive process and there are tricks that are described in the paper and I can talk to you about this as my poster for speeding this up using KD trees."
        ],
        [
            "OK, now once you've got that dendrogram, you're actually going to go through each cluster in that Venn diagram and present it for querying to the user.",
            "And you're not actually going to present the entire cluster.",
            "You're going to present a representative data point, which is defined to be the data points that moves the least to get to that cluster center.",
            "Now, in order to rank these representative data points, we tried two separate criterion, the first of which is known as Outlier knus.",
            "The Outlier Ness is defined as the lifetime of a cluster in that dendrogram divided by the number of data points in that cluster.",
            "So you can see from this formula that the smaller that cluster is, the higher the outlier NIS value and the lifetime of the cluster is very simply how long it stays alive and under gram.",
            "So from the moment that it gets created to the point where it gets merged with another cluster, if you take the log of a difference of the bandwidths.",
            "That's the lifetime of the cluster.",
            "The second part."
        ],
        [
            "Hearing that we tried was compactness isolation.",
            "I wanted to describe this, but think of this as Inter cluster distance, an intra cluster distance an it's very related to that compactness and isolation is a little more computationally expensive to compute, but you actually get much better results with this than Outlier Ness."
        ],
        [
            "OK, one final little heuristic that we threw in here.",
            "There are times where you have data points that are tide in terms of either outlier, Anissa compactness isolation and in order to resolve these ties we add a heuristic known as highest average distance which basically says queried the representative data point that is the highest average distance from all the user labeled data points."
        ],
        [
            "OK, so I'm going to show you results very briefly in order to evaluate this algorithm we used it on six different UCI datasets an in some of these datasets we actually had to subsample the classes in order to create a severe class imbalance.",
            "How do you evaluate these algorithms?",
            "Well, there's a bit of a diff."
        ],
        [
            "Print process here is an example of what's known as a category detection curve in a category detection curve.",
            "You have the number of categories of classes discovered on the Y axis and you also have the number of queries to the user on the X axis, and obviously what you want to do is you want to discover as many categories in the data as possible in as few queries that user as possible.",
            "So a curve that clients very quickly upward and to the right is considered to be very good.",
            "There's two metrics that you can use to evaluate.",
            "To summarize these curves, one is the number of queries presented to the user to discover all the categories in the data."
        ],
        [
            "And that's shown right here, so you can see from this set of results anything shaded blue is considered to be the winner and compared to.",
            "So if you take the HMS methods, which are hierarchical mean shift methods with either CI, compactness, isolation or outlier Ness eauty, they tend to do much better than MDM or interleave.",
            "There's one exception here, which is there's a bit of a weird situation.",
            "With abalone we can talk about that in my poster.",
            "The second sort of metric for evaluating these algorithms is also the area under the category detection curve."
        ],
        [
            "Which I present up here as well, and you can see that HMS with compactness isolation plus the tiebreaker condition tends to do very, very well on these datasets.",
            "The only data set that actually does worse on an NDM does better than HMS is yeast.",
            "So."
        ],
        [
            "So I've got one more minute conclusions.",
            "These hierarchical mean shift methods are much better and consistently discover all the classes in the data set and much fewer queries than existing methods, and they don't need a priori knowledge of the data set properties."
        ],
        [
            "I think I'm actually at 12 minutes, so I'll just take any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today's talk is on category detection using hierarchal mean shift and this is joint work with my student Pavan Rotary who couldn't be here because of visa issues and I'm linking long.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be presenting.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This work?",
                    "label": 0
                },
                {
                    "sent": "So in many applications that perform surveillance or monitoring or scientific discovery and data cleaning, it's really important to discover the anomalies because there are things of interest and in order to discover these anomalies you need to first form a statistical model of your data and then identify data points that are unusual according to this statistical model and one problem with this approach is that in many cases the anomalies that are discovered to be statistically significant aren't terribly interesting to you because.",
                    "label": 0
                },
                {
                    "sent": "They correspond to known sources or noise, or they are combinations of features that are just not.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interesting, and here's an example thanks to Dan Pelikan.",
                    "label": 0
                },
                {
                    "sent": "Also the slow Digital Sky survey for these pictures.",
                    "label": 1
                },
                {
                    "sent": "If you were to take a look at some of these pictures of galaxies, it turns out that in many cases 99.9% of the data.",
                    "label": 0
                },
                {
                    "sent": "These are known galaxies that aren't very interesting, and they're sort of the common or normal cases in the data.",
                    "label": 0
                },
                {
                    "sent": "About 1% of the data corresponds to anomalies.",
                    "label": 1
                },
                {
                    "sent": "Now if you can draw your attention to the bottom 2 boxes there, the box no left hand side corresponds to.",
                    "label": 0
                },
                {
                    "sent": "A satellite trail, and if you're an astrophysicist, you're not going to win a Nobel Prize for discovering a satellite trail.",
                    "label": 0
                },
                {
                    "sent": "It's what would be considered a boring anomaly.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the bottom right hand corner corresponds to a very strange hydrogen region, and this is an interesting anomaly.",
                    "label": 0
                },
                {
                    "sent": "And notice that it is exactly about 1% of the point 1% of the anomalies are actually interesting.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one way of dealing with this problem, this is a fairly novel approach that was proposed by Pelican more in 2004.",
                    "label": 0
                },
                {
                    "sent": "It's known as category detection.",
                    "label": 1
                },
                {
                    "sent": "It's basically a Explorer process for exploratory data analysis that involves a human in the loop and the basic idea in this process is first you take your data set.",
                    "label": 1
                },
                {
                    "sent": "You build a statistical model of your data.",
                    "label": 1
                },
                {
                    "sent": "You then identify the anomalies and then you present certain anomalies to the user to label.",
                    "label": 0
                },
                {
                    "sent": "And the user then labels these anomalies.",
                    "label": 0
                },
                {
                    "sent": "You update your model with the labels and the loop continues again.",
                    "label": 0
                },
                {
                    "sent": "Now this is not quite active, learn.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's one little twist to this.",
                    "label": 0
                },
                {
                    "sent": "When the user is labeling the query, the user can either label the data point under an existing class or category, or the user can declare the data point to belong to a completely new undiscovered category, and you basically keep going until you discover all the classes of interest.",
                    "label": 1
                },
                {
                    "sent": "Now, of course, if the user discovers a data point that represents a cluster of interest, the user may stop and dig a little bit deeper into that cluster.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the goal of category detection is the following.",
                    "label": 0
                },
                {
                    "sent": "You want to present to the user a single representative instance from each class or category in your data set in as few few queries as possible.",
                    "label": 1
                },
                {
                    "sent": "Now this turns out to be fairly straightforward if your data is perfectly balanced in terms of the size of your classes.",
                    "label": 0
                },
                {
                    "sent": "However, in many cases there is a severe class imbalance in your data, and this makes it very difficult to find these rare categories, and that's why category detection is often referred to as.",
                    "label": 0
                },
                {
                    "sent": "Rare category detection.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's the introduction in my talk.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to briefly cover related work and then die.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into the methodology in terms of related work, the paper that first proposed category detection presented the Interleaf algorithm, which is an algorithm based on M that proposes data points that are leased, owned by the mixture model components as a query data points.",
                    "label": 0
                },
                {
                    "sent": "There's a later paper last year that presented a nearest neighbor type of approach that query data points that had the biggest difference in terms of the local density around it, and there's a nice theoretical paper for identifying multiple outputs in a function.",
                    "label": 0
                },
                {
                    "sent": "For specific types of functions, and it had a nice theoretical analysis for these specific cases.",
                    "label": 0
                },
                {
                    "sent": "The first 2 algorithms innerleithen MDM, both required the user to know something about the data set.",
                    "label": 0
                },
                {
                    "sent": "For example, they interleave requires the user to know how many classes there are in total in a data set, and MDM requires the user to have some notion of the prior probabilities of a data point belonging to each class.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now in order to describe my algorithm I need to give a little bit of background about the mean shift algorithm, which a lot of people envision know about this, but it's not so well known in machine learning.",
                    "label": 0
                },
                {
                    "sent": "It mean shift.",
                    "label": 0
                },
                {
                    "sent": "You have what's known as a reference data set.",
                    "label": 1
                },
                {
                    "sent": "This is the data set that you want to cluster and mean shift is a clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "You also have another data set known as the query data set.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to the data set of coordinates that you would like to assign to cluster centers, and the way mean shift works is as follows.",
                    "label": 0
                },
                {
                    "sent": "Suppose the + there, that's red.",
                    "label": 0
                },
                {
                    "sent": "Is your query data point what you're going to do is you're going to draw a circle around it, and the radius of the circle is determined by a bandwidth parameter.",
                    "label": 0
                },
                {
                    "sent": "You will then compute the center of mass in that circle shown by that purple + and you're going to compute what's known as the mean shift vector, which basically follows the gradient of the density function and what you will then do is you're going to shift that query point to the purple + Now, if you look at the formula down there, which I don't have much time to explain, that's the formula for the mean shift vector.",
                    "label": 1
                },
                {
                    "sent": "It looks a lot like a kernel density estimation formula an it's very similar.",
                    "label": 0
                },
                {
                    "sent": "The one difference here is that you're dealing with K prime, which is the derivative of the kernel function.",
                    "label": 0
                },
                {
                    "sent": "So if you do this.",
                    "label": 0
                },
                {
                    "sent": "You'll end up doing the following.",
                    "label": 0
                },
                {
                    "sent": "You will shift that query point to where the center of mass is, and then.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Repeat the process over again.",
                    "label": 0
                },
                {
                    "sent": "You'll find a new center of mass Calculator, new mean shift vector and shift it once more, so you'll keep doing this until you converge to a cluster mode or close.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Center now there is 1 little additional thing that you can do with mean Shift, which is known as blurring and this is what happens if your query points are in fact the reference data points in your data set.",
                    "label": 1
                },
                {
                    "sent": "So when you're in fact you're in effect doing is you are moving your reference data points towards these basins of attraction and you are progressively blurring your data set.",
                    "label": 0
                },
                {
                    "sent": "So if you imagine that your data set is like an image, you're progressively blurring that image.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me give you a quick sort of visual picture of what it looks like when you finish running mean shift.",
                    "label": 0
                },
                {
                    "sent": "What's happening here is if you imagine the plus signs that are red being your original data points, you'll notice that they all get sort of pulled into these basins of attraction where the cluster centers are shown by these green triangles.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So onto my algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now the algorithm consists of three parts.",
                    "label": 0
                },
                {
                    "sent": "First off you need to sphere your data.",
                    "label": 0
                },
                {
                    "sent": "This is fairly standard thing which I'm not going to cover.",
                    "label": 0
                },
                {
                    "sent": "The second thing is you need to run hierarchical mean shift.",
                    "label": 1
                },
                {
                    "sent": "I'll explain what that means and the third thing consists of the querying process that you're going to present data points that user for late.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going.",
                    "label": 0
                },
                {
                    "sent": "K now hierarchical mean shift consists of progressively blurring your data.",
                    "label": 1
                },
                {
                    "sent": "So what you're going to do is you'll start out with all your data points in the reference data set.",
                    "label": 0
                },
                {
                    "sent": "You're going to run blurring with some bandwidth value, and that's going to form a bunch of cluster centers.",
                    "label": 0
                },
                {
                    "sent": "Those cluster centers then become data points for the next time you'll run mean shift, except this time you're going to increase the bandwidth by multiplying your old bandwidth by some constant factor K, where K is bigger than one.",
                    "label": 0
                },
                {
                    "sent": "And you'll keep doing this until you eventually get everything sort of clumping together into one big cluster.",
                    "label": 0
                },
                {
                    "sent": "And once you're done that, you're going to go back and take a look at how all these cluster centers joined together and you'll basically produce a dendrogram very much like that one.",
                    "label": 0
                },
                {
                    "sent": "Now you'll notice that in this dendrogram, whenever you see sort of two branches joining together into one, you're forming a new cluster at that point at each level.",
                    "label": 0
                },
                {
                    "sent": "This is important and also this is actually a fairly computationally expensive process and there are tricks that are described in the paper and I can talk to you about this as my poster for speeding this up using KD trees.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now once you've got that dendrogram, you're actually going to go through each cluster in that Venn diagram and present it for querying to the user.",
                    "label": 1
                },
                {
                    "sent": "And you're not actually going to present the entire cluster.",
                    "label": 0
                },
                {
                    "sent": "You're going to present a representative data point, which is defined to be the data points that moves the least to get to that cluster center.",
                    "label": 1
                },
                {
                    "sent": "Now, in order to rank these representative data points, we tried two separate criterion, the first of which is known as Outlier knus.",
                    "label": 0
                },
                {
                    "sent": "The Outlier Ness is defined as the lifetime of a cluster in that dendrogram divided by the number of data points in that cluster.",
                    "label": 0
                },
                {
                    "sent": "So you can see from this formula that the smaller that cluster is, the higher the outlier NIS value and the lifetime of the cluster is very simply how long it stays alive and under gram.",
                    "label": 1
                },
                {
                    "sent": "So from the moment that it gets created to the point where it gets merged with another cluster, if you take the log of a difference of the bandwidths.",
                    "label": 0
                },
                {
                    "sent": "That's the lifetime of the cluster.",
                    "label": 0
                },
                {
                    "sent": "The second part.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hearing that we tried was compactness isolation.",
                    "label": 0
                },
                {
                    "sent": "I wanted to describe this, but think of this as Inter cluster distance, an intra cluster distance an it's very related to that compactness and isolation is a little more computationally expensive to compute, but you actually get much better results with this than Outlier Ness.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, one final little heuristic that we threw in here.",
                    "label": 0
                },
                {
                    "sent": "There are times where you have data points that are tide in terms of either outlier, Anissa compactness isolation and in order to resolve these ties we add a heuristic known as highest average distance which basically says queried the representative data point that is the highest average distance from all the user labeled data points.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to show you results very briefly in order to evaluate this algorithm we used it on six different UCI datasets an in some of these datasets we actually had to subsample the classes in order to create a severe class imbalance.",
                    "label": 0
                },
                {
                    "sent": "How do you evaluate these algorithms?",
                    "label": 0
                },
                {
                    "sent": "Well, there's a bit of a diff.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Print process here is an example of what's known as a category detection curve in a category detection curve.",
                    "label": 1
                },
                {
                    "sent": "You have the number of categories of classes discovered on the Y axis and you also have the number of queries to the user on the X axis, and obviously what you want to do is you want to discover as many categories in the data as possible in as few queries that user as possible.",
                    "label": 0
                },
                {
                    "sent": "So a curve that clients very quickly upward and to the right is considered to be very good.",
                    "label": 0
                },
                {
                    "sent": "There's two metrics that you can use to evaluate.",
                    "label": 0
                },
                {
                    "sent": "To summarize these curves, one is the number of queries presented to the user to discover all the categories in the data.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's shown right here, so you can see from this set of results anything shaded blue is considered to be the winner and compared to.",
                    "label": 0
                },
                {
                    "sent": "So if you take the HMS methods, which are hierarchical mean shift methods with either CI, compactness, isolation or outlier Ness eauty, they tend to do much better than MDM or interleave.",
                    "label": 0
                },
                {
                    "sent": "There's one exception here, which is there's a bit of a weird situation.",
                    "label": 0
                },
                {
                    "sent": "With abalone we can talk about that in my poster.",
                    "label": 0
                },
                {
                    "sent": "The second sort of metric for evaluating these algorithms is also the area under the category detection curve.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which I present up here as well, and you can see that HMS with compactness isolation plus the tiebreaker condition tends to do very, very well on these datasets.",
                    "label": 0
                },
                {
                    "sent": "The only data set that actually does worse on an NDM does better than HMS is yeast.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I've got one more minute conclusions.",
                    "label": 0
                },
                {
                    "sent": "These hierarchical mean shift methods are much better and consistently discover all the classes in the data set and much fewer queries than existing methods, and they don't need a priori knowledge of the data set properties.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think I'm actually at 12 minutes, so I'll just take any questions.",
                    "label": 0
                }
            ]
        }
    }
}