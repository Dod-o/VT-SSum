{
    "id": "q4zpyujq47itll47gbf2vf3huit2kmzs",
    "title": "A Bayesian approach to Word Segmentation: Theoretical and Experimental results",
    "info": {
        "author": [
            "Sharon Goldwater, Department of Linguistics, Stanford University"
        ],
        "published": "Oct. 31, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Natural Language Processing",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlcs07_goldwater_bat/",
    "segmentation": [
        [
            "OK, thanks a lot.",
            "So, given that there's a very broad audience here, I sort of put two talks together into one talk to try and address as many different interests as possible, But that means necessarily that they're both going to be kind of brief, so hopefully it will still be."
        ],
        [
            "Understandable.",
            "As bye bye now.",
            "I'm sure most of you no word segmentation is one of the first problems that infants need to solve as their learning language.",
            "And as we've already heard, infants make use of a number of different kinds of cues to solve this problem.",
            "The one that I'm going to be focusing on here is the kinds of cues that are available in this statistical regularity's that occur in the sequences of syllables that happen in fluent speech.",
            "So these kinds of statistical regularity's have.",
            "Generated a lot of interest in the research community for a couple of reasons.",
            "One, they are language independent, sort of Q, and Secondly because they are used very early, they seem to come online earlier than a lot of the other kinds of cues and taken together, these two facts suggest that these sorts of statistics may actually provide an initial method into that starts off the bootstrapping process, leading to some use of the other cues."
        ],
        [
            "So there's been actually a lot of work on modeling statistical segmentation.",
            "Previous work has often focused on how statistical information such as transitional probabilities, which I'll get back to you a little bit later, can be used in order to segment speech.",
            "But in this talk I'm going to be looking at a Bayesian approach, which instead is going to be asking what sort of information should be used by successful learner.",
            "So that means we're going to be looking at what kinds of statistics should be collected by the learner, and also what kinds of assumptions need to be made in order to constrain the possible.",
            "Generalizations to come up with a linguistically reasonable solution to the problem."
        ],
        [
            "So my outline basically as you can tell from the title, I have two parts to this talk.",
            "In the first part I'm going to be presenting a computational model and some theoretical results from that model.",
            "Looking at the question of what the consequences are of using different kinds of information in an ideal learning situation to do word segmentation, this is work that I've done with Tom Griffiths and Mark Johnson as part of my thesis work and then in the second part of the talk I'm going to be addressing the question of whether humans actually behave in a way that seems to be consistent with this kind of ideal learning assumption.",
            "And that's joint work that I've done with Mike Frank, who's talking after me, as well as a."
        ],
        [
            "Of other folks.",
            "OK, so work on statistical segmentation.",
            "As I mentioned already, often discusses the concept of transitional probabilities, so this is basically the idea that the probability of a syllable given the previous syllable is often lower at word boundaries.",
            "But if we want to think about this in terms of a Bayesian approach, we need to think about what this idea of transitional probabilities actually says about words and about the assumptions that the learner is making.",
            "It turns out that that's really.",
            "Two different assumptions that could be made that are consistent with the observation of transitional probabilities.",
            "One is that a word is a unit whose beginning predicts it's and, but it does not predict other words.",
            "Or we could assume as the learner, that a word is a unit whose beginning predicts its end, and it also predicts future words simply to a lesser extent than the beginning of the word predicts the."
        ],
        [
            "End of the word.",
            "And it turns out that if we look at most of the previous work in this area, that previous work has assumed in general that words are in fact statistically independent.",
            "So if you look at experimental work.",
            "This was originally started off by Jenny Saffran, but there's a lot of other people who followed in her footsteps.",
            "Looking at how you can generate.",
            "Experimental stimuli, these stimuli are created by.",
            "Randomly concatenating together words in a small artificial language and that random concatenation essentially introduces this statistical independence.",
            "Similarly, there's been a lot of computational work, much of which is also assumed statistical independence between words.",
            "So what I want to look at here is what happens if we also assume that words."
        ],
        [
            "Dict other words.",
            "So the question is going to be addressing here are first of all, if a learner assumes that words are in fact independent units, what is learned from more realistic input than the kinds of input that are typically seen in these experiments with humans and the way that I'm going to be looking at that is by using a unigram model, A model which assumes that each sentence is generated by generating the words in the sentence independently.",
            "And then second, I'm going to look at the question of what happens if the learner assumes instead that words are units that help predict other units.",
            "In that case, I'm going to be looking at a bigram model, in which each word, sorry, each sentence is generated by generating the words in the sentence conditioned on the previous word.",
            "And the approach that I'll be taking as I've already alluded to, is to use in a Bayesian ideal observer model and examine the consequences of."
        ],
        [
            "In each of these two different assumptions.",
            "Again, probably by this time we're all familiar with Bayesian learning, at least to some extent, so the Bayesian learner is going to try and identify an explanatory linguistic hypothesis that both accounts for the observed data as well as conforming to prior expectations.",
            "Accounting for the observed data can be.",
            "We can determine that by looking at the probability of the data given the hypothesis or the likelihood, and we can then compute the probability of each hypothesis as the prior.",
            "An as a in this Bayesian approach, we're going to assume that the focus is on the goal of the computation, i.e.",
            "Trying to identify a high posterior probability hypothesis rather than on the algorithm that's used to achieve that goal, so I won't be talking much about the algorithm today."
        ],
        [
            "OK, so in the particular realm of segmentation we have a set of data that consists of UN segmented corpus.",
            "In this case it's going to be phonemic.",
            "Lee transcribed child directed speech and we have a set of hypothesis that consist of different possible sequences of word tokens.",
            "Now.",
            "If we think about what that means in terms of the likelihood in the prior.",
            "It turns out that the likelihood actually is going to be either zero or one, so it's going to be a zero if a particular sequence of tokens in our hypothesis is inconsistent with the UN segmented corpus that we've observed, and it's going to be one if concatenating together those words actually forms that corpus.",
            "What that means, of course, is that it's actually the prior that's going to determine whether or not we choose a particular hypothesis.",
            "That prior is what we're going to use to encode the unigram assumption, or the bigram assumption, as well as some other assumptions that I'll get to a little bit.",
            "Later.",
            "And as I said, the optimal solution here, which would be the one that."
        ],
        [
            "The highest prior probability.",
            "So before I get to our own model, I just want to very briefly mentioned that there has been some previous work that's kind of similar in some ways to our own Michael Brandt in 1999 developed a Bayesian model for segmentation that was a unigram model as I mentioned.",
            "It has actually some similar mathematical properties to our own, however there are a couple of problems with the system.",
            "First of all, the algorithm that Brent used was an approximate learning algorithm, and it turns out we've actually shown and I'll get to this a little bit later that the solutions that were found.",
            "By his system, we're not actually close to the optimal solutions.",
            "And Secondly, his system is hard to extend in order to incorporate bigram information, which is of course what we wanted to do, so instead."
        ],
        [
            "We developed our own model.",
            "This is a model based on the Dursley process from Bayesian statistics, and this model assumes that the words in the corpus are generated as a sequence of words, and so we're going to generate the F word in the corpus in two steps.",
            "First, we're going to decide whether that word is a novel lexical item or not, so the probability that it is novel is going to be proportional to some constant parameter Alpha, and the probability that it's not novel will be proportional to N, where N is the number of word tokens that have already been generated.",
            "So what this means is that as we generate more tokens, the probability of generating a novel lexical item will slowly decrease.",
            "And the result of that is that segmentations that contain fewer word types will have higher probability than those that contain."
        ],
        [
            "More word types.",
            "Step 2 In this generative process is if this word is novel, we need to generate a phonemic form for the word, and we do that by just choosing a word with probability that's proportional to the unigram phoneme.",
            "Probability of that word.",
            "We are generating a word that we've already generated before.",
            "Then we need to choose the lexical identity of that word from the previously occurring words, and we do that with probability that's proportional to the number of times we've seen each of those words before.",
            "So what that means is that the more frequently we've seen a word, the more frequently we will, the more probable it will become to generate that word in the future, and this kind of rich get richer process of generating words.",
            "Actually, it turns out creates a power law distribution on word frequencies.",
            "I should have mentioned this before, sorry.",
            "So because of the way that this phoneme unigram phony model works, we assign a higher probability to shorter words.",
            "And because of the.",
            "This process of the way that we choose words that we've already chosen before.",
            "We assume that segmentations that contain a power law distribution on word frequencies will have higher probability than other kinds of segmentations."
        ],
        [
            "OK, so using this model we ran a number of simulations using the same corpus that was used by Michael Brent.",
            "This is a corpus that came from childless and it contains about 10,000 utterances of phonemic Lee transcribed child directed speech for children between 19 and 20 three months old.",
            "This is some example input, probably a lot of you can't actually read what this says.",
            "It's in an ASCII transcription, but it says you want to see the book look, there's a boy with his hat and a doggy.",
            "You want to look at this.",
            "Basically, the only important point here is that utterance boundaries are observed because there are pauses and word boundaries within.",
            "The utterances are not observed, and that's what the algorithm the system needs to discover."
        ],
        [
            "OK, so here are some example results that we got using our unigram model, so errors are shown in red.",
            "What you'll notice here is that when the system proposes a word boundary, it's almost always right, but it's just not proposing an afford boundary, so there's a lot of cases where multiple words are sort of concatenated into single."
        ],
        [
            "Items, and in fact, if you look at the.",
            "Quantitative results that confirms that.",
            "So this is measured in terms of precision and recall, which also known as accuracy and completeness.",
            "But basically what this shows is that, as I said, proposed boundaries are more accurate than previous work, but there are fewer proposals being made and the result is that overall the F score, which is an average of precision recall on word tokens, is."
        ],
        [
            "Lower than brents.",
            "So why is that?",
            "It's actually not that surprising if you think about it.",
            "This model makes a false assumption, which is that words have the same probability regardless of their context.",
            "And if you actually look at the corpus, it's clearly not the case.",
            "So the probability of the word that is about .02 the probability of the word that, given that it followed the word Watts, is much higher and much lower if it followed the word too.",
            "So it actually varies by several orders of magnitude.",
            "This is something that this model is not able to capture in any way other than by.",
            "Proposing that these very strong words were dependencies are actually within individual work."
        ],
        [
            "It's.",
            "But that raises the question of why previous unigram models didn't have the same problem, and I believe that the reason for that is actually because their search algorithms just didn't show this problem.",
            "The algorithm was not sufficient to identify the optimal segmentation.",
            "So if we actually compute the probability of the solution that was found by our unigram model under Brents model, we find that it actually has higher probability under his model than his own solution does.",
            "Moreover, we can actually create a corpus that adheres to the unigram assumption made by both of these models.",
            "By just randomly permuting all of the words in the corpus, when we do that, we find that our system then achieves 96% accuracy, whereas brents model actually does improve, but it's still only getting 81%.",
            "Again, showing that his search procedure just isn't sufficient and actually, my coauthor Tom Griffiths has also done some formal analysis, showing that this kind of under segmentation is really the optimal solution for any sort of reasonable unigram."
        ],
        [
            "Little.",
            "So that's a fairly strong argument, and about unigram models and under segmentation.",
            "That brings me to the bigram model, which I don't have time to talk about in detail.",
            "It's actually very similar conceptually to the unigram model, except now.",
            "Instead of generating each unigram one at a time in sequence, we're going to actually now generate bigrams so we have to decide whether each bigram is novel or not, and then if it is, we need to sort of back off to the unigram model to generate the second word.",
            "And if not, we're going to go back to the accounts that we already have."
        ],
        [
            "Previous bigrams.",
            "These are some example results from the bigram model.",
            "You'll notice that there is a lot less red up there, so we're doing much better."
        ],
        [
            "In fact, if you look at the quantitative evaluation, it turns out that compared to the unigram model, we're actually doing just as well in terms of precision.",
            "So when we propose a boundary, it's just as accurate as it was before.",
            "But we're proposing a lot more boundaries now, and so overall the result of that is that we've improved both the score on word tokens as well as the score on word types above the level of previous models.",
            "Both brands as well as other published models that are out."
        ],
        [
            "There.",
            "So, just to summarize, this part of the talk.",
            "I said in the beginning that the idea of transitional probabilities that it's been shown behaviourally that people are sensitive to these, but there's really two different kinds of assumptions about what defines a word that are consistent with these behavioral.",
            "This behavioral evidence, and I've shown that those two different assumptions actually lead to very different results.",
            "So if we assume that the beginning of the word predicts the end of the word, then the optimal solution isn't one that under segments the corpus finding.",
            "Multi word units and identifying those individual words.",
            "But if we also assume that words predict that a word predicts the next word, then we come up with a much more accurate."
        ],
        [
            "More adult like segmentation.",
            "So there's a lot of remaining questions, of course, from this work I haven't.",
            "Obviously I don't have time to address all of these, and I haven't even had a chance to do research on all of these, but the one that I will be addressing here is the question of whether there's actually even any evidence to suggest that humans behave in a way that's consistent with these predictions.",
            "So it's all very well to make ideal Lerner arguments, but you know, there are more convincing in terms of human psychology if we can show that humans behave in ways that are."
        ],
        [
            "Consistent with ideal learners, So what we wanted to do was actually test the predictions of this Bayesian model as well as some other models in comparing them to human performance in a saffron style experiment.",
            "Now the problem with these kinds of experiments is it turns out that all of these models have close to perfect accuracy on all the experimental stimuli, so you can't just check are they doing?",
            "Are they doing the segmentation correctly?",
            "Instead what we wanted to do is compare the changes in the model performance.",
            "As we varied the task difficulty and compare that to how human performance."
        ],
        [
            "Varied with task difficulty.",
            "So the way that we varied the task difficulty was by looking at different utterance length conditions.",
            "So this is an example of a lexecon that was used for one of the subjects we actually generated these lexicons at random, so each subject had a different lexicons, but they always had two 2 syllable words, 2 three syllable words, and too sorry, and 2 four syllable words.",
            "And then the different conditions ranged from one word per utterance, up to 24 words per utterance, and actually not all of my audio clips are working, but hopefully I can show you the one and two word utterances.",
            "So that's a one word utterance.",
            "OK, so these are produced by speech synthesizer.",
            "But I can't play the 12 ordinances 'cause they start to sound like complete nonsense.",
            "But people are actually able to do this."
        ],
        [
            "And so the way that we did these experiments is by training subjects to listen to these synthesized utterances for about 15 minutes.",
            "Each subject got one life condition to listen to.",
            "There were no pauses between the syllables within the utterances, but there were half second pauses between utterances and then for testing, we presented each subject with a bunch of pairs.",
            "They had to make a 2 alternative forced choice.",
            "Each pair consisted of a word, an apart word.",
            "This is just like the previous talk.",
            "So the words here are.",
            "Words that actually appeared in the lexecon and the part words are contained one the end of one word in the beginning of another word, and they just had to say which one sounded more like a word to them."
        ],
        [
            "And this is what you see in the human performance.",
            "So on the X axis is sentence length ranging from 1 to 24 words, and this is their performance.",
            "And not surprisingly they do much better on the shorter utterances than they do on the longer ones.",
            "But this is not a linear effect.",
            "There's definitely a steeper drop off and then kind of a shallower drop off towards the end, so this is the data that we're going to try and model."
        ],
        [
            "We evaluated six different models on this data set.",
            "Each of them was trained and tested on the same stimuli that the humans had.",
            "We needed to simulate the two alternative forced choice and so we had each of the models produce a score for each of the words in the choice pair, and then we use this loose choice rule to produce a probability from that.",
            "And then we computed the best linear fit.",
            "As I mentioned earlier and computed the correlation between the fitted data and the.",
            "Human data."
        ],
        [
            "The models that we used here, three of them are actually very similar, very much like transitional probabilities, so I'll just refer to these as transitional probability models model.",
            "They're all based on local statistics.",
            "Basic idea is just a segment at the minimum of the transitional probability.",
            "We also used a model by Dan Swingly that was developed in 2005.",
            "This is a model that actually is based on these kinds of local statistics as well as on some frequency thresholds so it actually builds up a lexecon.",
            "And the score is just the maximum threshold at which a particular word appears in the lexecon.",
            "We then used a model called parser which is intended to incorporate some principles of lexical competition and memory decay, and this one actually has probabilities on each word which we used as the score and then finally our own Bayesian model, which again has probabilities, which we."
        ],
        [
            "Used as the scores.",
            "OK, so these are our results in terms of linear fit.",
            "First, the transitional probability models you'll notice that actually these do surprisingly well.",
            "They capture that drop off pretty well in the short utterance range.",
            "What they don't seem to be capturing is that sort of continued drop off between utterance length 12 and 24, but overall it's actually not a bad fit.",
            "Answering these model doesn't do quite as well.",
            "It seems to be kind of sensitive to specifics of the individual corpora, and so overall it's a little bit noisier and doesn't have as good of a fit.",
            "And the parser model.",
            "Actually, we averaged over a whole bunch of runs of parser, but we still got this really noisy effect here, so it's not doing very well.",
            "And finally we have the Bayesian model, which has a surprisingly good fit here.",
            "It's rare that you get such a good fit between a model in data, so in addition to modeling that initial dropoff that we got in the transitional probability models, we also get sort of this continued drop off with the longer utterances.",
            "So we're kind of interested in looking at.",
            "What's causing this?",
            "Why are we getting this effect in the Bayesian model that we're not getting?"
        ],
        [
            "The others, and So what?",
            "We then looked at was actually the scores that were assigned by each of those models to the words and the part words.",
            "And what you'll see is actually sort of two different patterns that are going on here in the transitional probability and the dance wingly, which is also sort of based on the similar statistics.",
            "You'll notice that the scores of the words don't really vary with utterance length, or at least not very much, while it's actually the change in scores of the distractors that's causing the increased difficulty.",
            "On the other hand, in these two models, we actually have a drop off in the probabilities of the words.",
            "As utterance length increases, and the reason that that's happening certainly in the Bayesian model, is actually sort of a competition effect.",
            "So as utterance length increases.",
            "The probabilities, so we there's there are more different hypothesis that are under consideration and as a result the probability mass kind of needs to be spread out among more different hypothesis and what that means is that there's just less probability mass assigned to the correct hypothesis.",
            "And so that leads to this drop off in the scores of the correct words, and similarly we actually get a slight increase in distracter scores because they are.",
            "Are actually more of those hypothesis that contain this."
        ],
        [
            "Tractors.",
            "OK, so just to summarize in this part of the talk, basically I've shown that statistical segmentation is more difficult when utterances contain more words.",
            "This is not a terribly surprising result, but actually I think this is the first time that it's been shown experimentally.",
            "And I've also shown that this kind of gradual decay in performance is predicted by the Bayesian model as I've just shown, but not by the other models that were tested, and as I've just explained, the Bayesian model actually suggests or predicts that this difficulty is primarily due to the effects of competition, wherein longer utterances correct words are less probable just because there are more other other possibilities to consider.",
            "The local statistical approaches don't model this kind of."
        ],
        [
            "Competition.",
            "We're actually still working on some additional experiments in this similar vein, so we'd like to look at some other kinds of predictions that the model makes.",
            "So one of those is varying the length of exposure to the training stimulus, so the Bayesian model actually predicts that longer exposure should lead to better performance.",
            "This is not terribly surprising.",
            "Transitional probabilities actually suggest that there really shouldn't be any effective exposure, so we're hoping to show this.",
            "Experimentally, which would again confirm the Bayesian model predictions and the Secondly we also want to vary the number of lexical items so the Bayesian model again intuitively suggests that the larger lexecon would lead to worse performance, while the transitional probability model actually has a counter intuitive prediction which goes the other way, suggesting that a larger lexecon would lead to better performance.",
            "So those are two things that we're working on right now, but we're still running subjects in terms."
        ],
        [
            "It takes a lot of subjects to run these things.",
            "And just to conclude, basically what I've talked about today are two different parts of a research.",
            "Looking at both computer simulations as well as experimental work.",
            "And in the task of word segmentation.",
            "I've shown using a Bayesian model an that the unigram assumption that's made by a lot of previous work actually causes ideal learners to under segment fluent speech.",
            "And then I've also tried to show that this kind of theoretical work actually does have implications for human learning because, at least in this one case that I've talked about, human word segmentation does actually seem to approximate the Bayesian ideal learning scenario that I talked about in the first part of the talk.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thanks a lot.",
                    "label": 0
                },
                {
                    "sent": "So, given that there's a very broad audience here, I sort of put two talks together into one talk to try and address as many different interests as possible, But that means necessarily that they're both going to be kind of brief, so hopefully it will still be.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Understandable.",
                    "label": 0
                },
                {
                    "sent": "As bye bye now.",
                    "label": 0
                },
                {
                    "sent": "I'm sure most of you no word segmentation is one of the first problems that infants need to solve as their learning language.",
                    "label": 1
                },
                {
                    "sent": "And as we've already heard, infants make use of a number of different kinds of cues to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "The one that I'm going to be focusing on here is the kinds of cues that are available in this statistical regularity's that occur in the sequences of syllables that happen in fluent speech.",
                    "label": 0
                },
                {
                    "sent": "So these kinds of statistical regularity's have.",
                    "label": 0
                },
                {
                    "sent": "Generated a lot of interest in the research community for a couple of reasons.",
                    "label": 0
                },
                {
                    "sent": "One, they are language independent, sort of Q, and Secondly because they are used very early, they seem to come online earlier than a lot of the other kinds of cues and taken together, these two facts suggest that these sorts of statistics may actually provide an initial method into that starts off the bootstrapping process, leading to some use of the other cues.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's been actually a lot of work on modeling statistical segmentation.",
                    "label": 0
                },
                {
                    "sent": "Previous work has often focused on how statistical information such as transitional probabilities, which I'll get back to you a little bit later, can be used in order to segment speech.",
                    "label": 1
                },
                {
                    "sent": "But in this talk I'm going to be looking at a Bayesian approach, which instead is going to be asking what sort of information should be used by successful learner.",
                    "label": 0
                },
                {
                    "sent": "So that means we're going to be looking at what kinds of statistics should be collected by the learner, and also what kinds of assumptions need to be made in order to constrain the possible.",
                    "label": 0
                },
                {
                    "sent": "Generalizations to come up with a linguistically reasonable solution to the problem.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So my outline basically as you can tell from the title, I have two parts to this talk.",
                    "label": 0
                },
                {
                    "sent": "In the first part I'm going to be presenting a computational model and some theoretical results from that model.",
                    "label": 0
                },
                {
                    "sent": "Looking at the question of what the consequences are of using different kinds of information in an ideal learning situation to do word segmentation, this is work that I've done with Tom Griffiths and Mark Johnson as part of my thesis work and then in the second part of the talk I'm going to be addressing the question of whether humans actually behave in a way that seems to be consistent with this kind of ideal learning assumption.",
                    "label": 1
                },
                {
                    "sent": "And that's joint work that I've done with Mike Frank, who's talking after me, as well as a.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of other folks.",
                    "label": 0
                },
                {
                    "sent": "OK, so work on statistical segmentation.",
                    "label": 1
                },
                {
                    "sent": "As I mentioned already, often discusses the concept of transitional probabilities, so this is basically the idea that the probability of a syllable given the previous syllable is often lower at word boundaries.",
                    "label": 0
                },
                {
                    "sent": "But if we want to think about this in terms of a Bayesian approach, we need to think about what this idea of transitional probabilities actually says about words and about the assumptions that the learner is making.",
                    "label": 0
                },
                {
                    "sent": "It turns out that that's really.",
                    "label": 0
                },
                {
                    "sent": "Two different assumptions that could be made that are consistent with the observation of transitional probabilities.",
                    "label": 0
                },
                {
                    "sent": "One is that a word is a unit whose beginning predicts it's and, but it does not predict other words.",
                    "label": 1
                },
                {
                    "sent": "Or we could assume as the learner, that a word is a unit whose beginning predicts its end, and it also predicts future words simply to a lesser extent than the beginning of the word predicts the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "End of the word.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that if we look at most of the previous work in this area, that previous work has assumed in general that words are in fact statistically independent.",
                    "label": 1
                },
                {
                    "sent": "So if you look at experimental work.",
                    "label": 0
                },
                {
                    "sent": "This was originally started off by Jenny Saffran, but there's a lot of other people who followed in her footsteps.",
                    "label": 0
                },
                {
                    "sent": "Looking at how you can generate.",
                    "label": 0
                },
                {
                    "sent": "Experimental stimuli, these stimuli are created by.",
                    "label": 0
                },
                {
                    "sent": "Randomly concatenating together words in a small artificial language and that random concatenation essentially introduces this statistical independence.",
                    "label": 0
                },
                {
                    "sent": "Similarly, there's been a lot of computational work, much of which is also assumed statistical independence between words.",
                    "label": 0
                },
                {
                    "sent": "So what I want to look at here is what happens if we also assume that words.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dict other words.",
                    "label": 0
                },
                {
                    "sent": "So the question is going to be addressing here are first of all, if a learner assumes that words are in fact independent units, what is learned from more realistic input than the kinds of input that are typically seen in these experiments with humans and the way that I'm going to be looking at that is by using a unigram model, A model which assumes that each sentence is generated by generating the words in the sentence independently.",
                    "label": 1
                },
                {
                    "sent": "And then second, I'm going to look at the question of what happens if the learner assumes instead that words are units that help predict other units.",
                    "label": 1
                },
                {
                    "sent": "In that case, I'm going to be looking at a bigram model, in which each word, sorry, each sentence is generated by generating the words in the sentence conditioned on the previous word.",
                    "label": 0
                },
                {
                    "sent": "And the approach that I'll be taking as I've already alluded to, is to use in a Bayesian ideal observer model and examine the consequences of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In each of these two different assumptions.",
                    "label": 0
                },
                {
                    "sent": "Again, probably by this time we're all familiar with Bayesian learning, at least to some extent, so the Bayesian learner is going to try and identify an explanatory linguistic hypothesis that both accounts for the observed data as well as conforming to prior expectations.",
                    "label": 1
                },
                {
                    "sent": "Accounting for the observed data can be.",
                    "label": 0
                },
                {
                    "sent": "We can determine that by looking at the probability of the data given the hypothesis or the likelihood, and we can then compute the probability of each hypothesis as the prior.",
                    "label": 0
                },
                {
                    "sent": "An as a in this Bayesian approach, we're going to assume that the focus is on the goal of the computation, i.e.",
                    "label": 0
                },
                {
                    "sent": "Trying to identify a high posterior probability hypothesis rather than on the algorithm that's used to achieve that goal, so I won't be talking much about the algorithm today.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in the particular realm of segmentation we have a set of data that consists of UN segmented corpus.",
                    "label": 1
                },
                {
                    "sent": "In this case it's going to be phonemic.",
                    "label": 1
                },
                {
                    "sent": "Lee transcribed child directed speech and we have a set of hypothesis that consist of different possible sequences of word tokens.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "If we think about what that means in terms of the likelihood in the prior.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the likelihood actually is going to be either zero or one, so it's going to be a zero if a particular sequence of tokens in our hypothesis is inconsistent with the UN segmented corpus that we've observed, and it's going to be one if concatenating together those words actually forms that corpus.",
                    "label": 0
                },
                {
                    "sent": "What that means, of course, is that it's actually the prior that's going to determine whether or not we choose a particular hypothesis.",
                    "label": 0
                },
                {
                    "sent": "That prior is what we're going to use to encode the unigram assumption, or the bigram assumption, as well as some other assumptions that I'll get to a little bit.",
                    "label": 0
                },
                {
                    "sent": "Later.",
                    "label": 0
                },
                {
                    "sent": "And as I said, the optimal solution here, which would be the one that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The highest prior probability.",
                    "label": 0
                },
                {
                    "sent": "So before I get to our own model, I just want to very briefly mentioned that there has been some previous work that's kind of similar in some ways to our own Michael Brandt in 1999 developed a Bayesian model for segmentation that was a unigram model as I mentioned.",
                    "label": 1
                },
                {
                    "sent": "It has actually some similar mathematical properties to our own, however there are a couple of problems with the system.",
                    "label": 0
                },
                {
                    "sent": "First of all, the algorithm that Brent used was an approximate learning algorithm, and it turns out we've actually shown and I'll get to this a little bit later that the solutions that were found.",
                    "label": 0
                },
                {
                    "sent": "By his system, we're not actually close to the optimal solutions.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, his system is hard to extend in order to incorporate bigram information, which is of course what we wanted to do, so instead.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We developed our own model.",
                    "label": 0
                },
                {
                    "sent": "This is a model based on the Dursley process from Bayesian statistics, and this model assumes that the words in the corpus are generated as a sequence of words, and so we're going to generate the F word in the corpus in two steps.",
                    "label": 0
                },
                {
                    "sent": "First, we're going to decide whether that word is a novel lexical item or not, so the probability that it is novel is going to be proportional to some constant parameter Alpha, and the probability that it's not novel will be proportional to N, where N is the number of word tokens that have already been generated.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that as we generate more tokens, the probability of generating a novel lexical item will slowly decrease.",
                    "label": 1
                },
                {
                    "sent": "And the result of that is that segmentations that contain fewer word types will have higher probability than those that contain.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More word types.",
                    "label": 0
                },
                {
                    "sent": "Step 2 In this generative process is if this word is novel, we need to generate a phonemic form for the word, and we do that by just choosing a word with probability that's proportional to the unigram phoneme.",
                    "label": 0
                },
                {
                    "sent": "Probability of that word.",
                    "label": 0
                },
                {
                    "sent": "We are generating a word that we've already generated before.",
                    "label": 0
                },
                {
                    "sent": "Then we need to choose the lexical identity of that word from the previously occurring words, and we do that with probability that's proportional to the number of times we've seen each of those words before.",
                    "label": 1
                },
                {
                    "sent": "So what that means is that the more frequently we've seen a word, the more frequently we will, the more probable it will become to generate that word in the future, and this kind of rich get richer process of generating words.",
                    "label": 0
                },
                {
                    "sent": "Actually, it turns out creates a power law distribution on word frequencies.",
                    "label": 0
                },
                {
                    "sent": "I should have mentioned this before, sorry.",
                    "label": 1
                },
                {
                    "sent": "So because of the way that this phoneme unigram phony model works, we assign a higher probability to shorter words.",
                    "label": 0
                },
                {
                    "sent": "And because of the.",
                    "label": 1
                },
                {
                    "sent": "This process of the way that we choose words that we've already chosen before.",
                    "label": 0
                },
                {
                    "sent": "We assume that segmentations that contain a power law distribution on word frequencies will have higher probability than other kinds of segmentations.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so using this model we ran a number of simulations using the same corpus that was used by Michael Brent.",
                    "label": 0
                },
                {
                    "sent": "This is a corpus that came from childless and it contains about 10,000 utterances of phonemic Lee transcribed child directed speech for children between 19 and 20 three months old.",
                    "label": 1
                },
                {
                    "sent": "This is some example input, probably a lot of you can't actually read what this says.",
                    "label": 0
                },
                {
                    "sent": "It's in an ASCII transcription, but it says you want to see the book look, there's a boy with his hat and a doggy.",
                    "label": 0
                },
                {
                    "sent": "You want to look at this.",
                    "label": 0
                },
                {
                    "sent": "Basically, the only important point here is that utterance boundaries are observed because there are pauses and word boundaries within.",
                    "label": 0
                },
                {
                    "sent": "The utterances are not observed, and that's what the algorithm the system needs to discover.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here are some example results that we got using our unigram model, so errors are shown in red.",
                    "label": 0
                },
                {
                    "sent": "What you'll notice here is that when the system proposes a word boundary, it's almost always right, but it's just not proposing an afford boundary, so there's a lot of cases where multiple words are sort of concatenated into single.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Items, and in fact, if you look at the.",
                    "label": 0
                },
                {
                    "sent": "Quantitative results that confirms that.",
                    "label": 0
                },
                {
                    "sent": "So this is measured in terms of precision and recall, which also known as accuracy and completeness.",
                    "label": 1
                },
                {
                    "sent": "But basically what this shows is that, as I said, proposed boundaries are more accurate than previous work, but there are fewer proposals being made and the result is that overall the F score, which is an average of precision recall on word tokens, is.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lower than brents.",
                    "label": 0
                },
                {
                    "sent": "So why is that?",
                    "label": 0
                },
                {
                    "sent": "It's actually not that surprising if you think about it.",
                    "label": 0
                },
                {
                    "sent": "This model makes a false assumption, which is that words have the same probability regardless of their context.",
                    "label": 1
                },
                {
                    "sent": "And if you actually look at the corpus, it's clearly not the case.",
                    "label": 0
                },
                {
                    "sent": "So the probability of the word that is about .02 the probability of the word that, given that it followed the word Watts, is much higher and much lower if it followed the word too.",
                    "label": 0
                },
                {
                    "sent": "So it actually varies by several orders of magnitude.",
                    "label": 0
                },
                {
                    "sent": "This is something that this model is not able to capture in any way other than by.",
                    "label": 0
                },
                {
                    "sent": "Proposing that these very strong words were dependencies are actually within individual work.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "But that raises the question of why previous unigram models didn't have the same problem, and I believe that the reason for that is actually because their search algorithms just didn't show this problem.",
                    "label": 0
                },
                {
                    "sent": "The algorithm was not sufficient to identify the optimal segmentation.",
                    "label": 1
                },
                {
                    "sent": "So if we actually compute the probability of the solution that was found by our unigram model under Brents model, we find that it actually has higher probability under his model than his own solution does.",
                    "label": 1
                },
                {
                    "sent": "Moreover, we can actually create a corpus that adheres to the unigram assumption made by both of these models.",
                    "label": 0
                },
                {
                    "sent": "By just randomly permuting all of the words in the corpus, when we do that, we find that our system then achieves 96% accuracy, whereas brents model actually does improve, but it's still only getting 81%.",
                    "label": 0
                },
                {
                    "sent": "Again, showing that his search procedure just isn't sufficient and actually, my coauthor Tom Griffiths has also done some formal analysis, showing that this kind of under segmentation is really the optimal solution for any sort of reasonable unigram.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Little.",
                    "label": 0
                },
                {
                    "sent": "So that's a fairly strong argument, and about unigram models and under segmentation.",
                    "label": 0
                },
                {
                    "sent": "That brings me to the bigram model, which I don't have time to talk about in detail.",
                    "label": 1
                },
                {
                    "sent": "It's actually very similar conceptually to the unigram model, except now.",
                    "label": 1
                },
                {
                    "sent": "Instead of generating each unigram one at a time in sequence, we're going to actually now generate bigrams so we have to decide whether each bigram is novel or not, and then if it is, we need to sort of back off to the unigram model to generate the second word.",
                    "label": 1
                },
                {
                    "sent": "And if not, we're going to go back to the accounts that we already have.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Previous bigrams.",
                    "label": 0
                },
                {
                    "sent": "These are some example results from the bigram model.",
                    "label": 0
                },
                {
                    "sent": "You'll notice that there is a lot less red up there, so we're doing much better.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In fact, if you look at the quantitative evaluation, it turns out that compared to the unigram model, we're actually doing just as well in terms of precision.",
                    "label": 1
                },
                {
                    "sent": "So when we propose a boundary, it's just as accurate as it was before.",
                    "label": 1
                },
                {
                    "sent": "But we're proposing a lot more boundaries now, and so overall the result of that is that we've improved both the score on word tokens as well as the score on word types above the level of previous models.",
                    "label": 0
                },
                {
                    "sent": "Both brands as well as other published models that are out.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "So, just to summarize, this part of the talk.",
                    "label": 0
                },
                {
                    "sent": "I said in the beginning that the idea of transitional probabilities that it's been shown behaviourally that people are sensitive to these, but there's really two different kinds of assumptions about what defines a word that are consistent with these behavioral.",
                    "label": 0
                },
                {
                    "sent": "This behavioral evidence, and I've shown that those two different assumptions actually lead to very different results.",
                    "label": 0
                },
                {
                    "sent": "So if we assume that the beginning of the word predicts the end of the word, then the optimal solution isn't one that under segments the corpus finding.",
                    "label": 0
                },
                {
                    "sent": "Multi word units and identifying those individual words.",
                    "label": 0
                },
                {
                    "sent": "But if we also assume that words predict that a word predicts the next word, then we come up with a much more accurate.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More adult like segmentation.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of remaining questions, of course, from this work I haven't.",
                    "label": 1
                },
                {
                    "sent": "Obviously I don't have time to address all of these, and I haven't even had a chance to do research on all of these, but the one that I will be addressing here is the question of whether there's actually even any evidence to suggest that humans behave in a way that's consistent with these predictions.",
                    "label": 1
                },
                {
                    "sent": "So it's all very well to make ideal Lerner arguments, but you know, there are more convincing in terms of human psychology if we can show that humans behave in ways that are.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Consistent with ideal learners, So what we wanted to do was actually test the predictions of this Bayesian model as well as some other models in comparing them to human performance in a saffron style experiment.",
                    "label": 1
                },
                {
                    "sent": "Now the problem with these kinds of experiments is it turns out that all of these models have close to perfect accuracy on all the experimental stimuli, so you can't just check are they doing?",
                    "label": 1
                },
                {
                    "sent": "Are they doing the segmentation correctly?",
                    "label": 1
                },
                {
                    "sent": "Instead what we wanted to do is compare the changes in the model performance.",
                    "label": 0
                },
                {
                    "sent": "As we varied the task difficulty and compare that to how human performance.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Varied with task difficulty.",
                    "label": 0
                },
                {
                    "sent": "So the way that we varied the task difficulty was by looking at different utterance length conditions.",
                    "label": 1
                },
                {
                    "sent": "So this is an example of a lexecon that was used for one of the subjects we actually generated these lexicons at random, so each subject had a different lexicons, but they always had two 2 syllable words, 2 three syllable words, and too sorry, and 2 four syllable words.",
                    "label": 0
                },
                {
                    "sent": "And then the different conditions ranged from one word per utterance, up to 24 words per utterance, and actually not all of my audio clips are working, but hopefully I can show you the one and two word utterances.",
                    "label": 0
                },
                {
                    "sent": "So that's a one word utterance.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are produced by speech synthesizer.",
                    "label": 0
                },
                {
                    "sent": "But I can't play the 12 ordinances 'cause they start to sound like complete nonsense.",
                    "label": 0
                },
                {
                    "sent": "But people are actually able to do this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the way that we did these experiments is by training subjects to listen to these synthesized utterances for about 15 minutes.",
                    "label": 0
                },
                {
                    "sent": "Each subject got one life condition to listen to.",
                    "label": 0
                },
                {
                    "sent": "There were no pauses between the syllables within the utterances, but there were half second pauses between utterances and then for testing, we presented each subject with a bunch of pairs.",
                    "label": 1
                },
                {
                    "sent": "They had to make a 2 alternative forced choice.",
                    "label": 0
                },
                {
                    "sent": "Each pair consisted of a word, an apart word.",
                    "label": 0
                },
                {
                    "sent": "This is just like the previous talk.",
                    "label": 0
                },
                {
                    "sent": "So the words here are.",
                    "label": 0
                },
                {
                    "sent": "Words that actually appeared in the lexecon and the part words are contained one the end of one word in the beginning of another word, and they just had to say which one sounded more like a word to them.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is what you see in the human performance.",
                    "label": 0
                },
                {
                    "sent": "So on the X axis is sentence length ranging from 1 to 24 words, and this is their performance.",
                    "label": 0
                },
                {
                    "sent": "And not surprisingly they do much better on the shorter utterances than they do on the longer ones.",
                    "label": 0
                },
                {
                    "sent": "But this is not a linear effect.",
                    "label": 0
                },
                {
                    "sent": "There's definitely a steeper drop off and then kind of a shallower drop off towards the end, so this is the data that we're going to try and model.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We evaluated six different models on this data set.",
                    "label": 1
                },
                {
                    "sent": "Each of them was trained and tested on the same stimuli that the humans had.",
                    "label": 1
                },
                {
                    "sent": "We needed to simulate the two alternative forced choice and so we had each of the models produce a score for each of the words in the choice pair, and then we use this loose choice rule to produce a probability from that.",
                    "label": 1
                },
                {
                    "sent": "And then we computed the best linear fit.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned earlier and computed the correlation between the fitted data and the.",
                    "label": 0
                },
                {
                    "sent": "Human data.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The models that we used here, three of them are actually very similar, very much like transitional probabilities, so I'll just refer to these as transitional probability models model.",
                    "label": 0
                },
                {
                    "sent": "They're all based on local statistics.",
                    "label": 0
                },
                {
                    "sent": "Basic idea is just a segment at the minimum of the transitional probability.",
                    "label": 1
                },
                {
                    "sent": "We also used a model by Dan Swingly that was developed in 2005.",
                    "label": 0
                },
                {
                    "sent": "This is a model that actually is based on these kinds of local statistics as well as on some frequency thresholds so it actually builds up a lexecon.",
                    "label": 0
                },
                {
                    "sent": "And the score is just the maximum threshold at which a particular word appears in the lexecon.",
                    "label": 1
                },
                {
                    "sent": "We then used a model called parser which is intended to incorporate some principles of lexical competition and memory decay, and this one actually has probabilities on each word which we used as the score and then finally our own Bayesian model, which again has probabilities, which we.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Used as the scores.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are our results in terms of linear fit.",
                    "label": 0
                },
                {
                    "sent": "First, the transitional probability models you'll notice that actually these do surprisingly well.",
                    "label": 0
                },
                {
                    "sent": "They capture that drop off pretty well in the short utterance range.",
                    "label": 0
                },
                {
                    "sent": "What they don't seem to be capturing is that sort of continued drop off between utterance length 12 and 24, but overall it's actually not a bad fit.",
                    "label": 0
                },
                {
                    "sent": "Answering these model doesn't do quite as well.",
                    "label": 0
                },
                {
                    "sent": "It seems to be kind of sensitive to specifics of the individual corpora, and so overall it's a little bit noisier and doesn't have as good of a fit.",
                    "label": 0
                },
                {
                    "sent": "And the parser model.",
                    "label": 0
                },
                {
                    "sent": "Actually, we averaged over a whole bunch of runs of parser, but we still got this really noisy effect here, so it's not doing very well.",
                    "label": 0
                },
                {
                    "sent": "And finally we have the Bayesian model, which has a surprisingly good fit here.",
                    "label": 0
                },
                {
                    "sent": "It's rare that you get such a good fit between a model in data, so in addition to modeling that initial dropoff that we got in the transitional probability models, we also get sort of this continued drop off with the longer utterances.",
                    "label": 0
                },
                {
                    "sent": "So we're kind of interested in looking at.",
                    "label": 0
                },
                {
                    "sent": "What's causing this?",
                    "label": 0
                },
                {
                    "sent": "Why are we getting this effect in the Bayesian model that we're not getting?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The others, and So what?",
                    "label": 0
                },
                {
                    "sent": "We then looked at was actually the scores that were assigned by each of those models to the words and the part words.",
                    "label": 0
                },
                {
                    "sent": "And what you'll see is actually sort of two different patterns that are going on here in the transitional probability and the dance wingly, which is also sort of based on the similar statistics.",
                    "label": 0
                },
                {
                    "sent": "You'll notice that the scores of the words don't really vary with utterance length, or at least not very much, while it's actually the change in scores of the distractors that's causing the increased difficulty.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, in these two models, we actually have a drop off in the probabilities of the words.",
                    "label": 0
                },
                {
                    "sent": "As utterance length increases, and the reason that that's happening certainly in the Bayesian model, is actually sort of a competition effect.",
                    "label": 0
                },
                {
                    "sent": "So as utterance length increases.",
                    "label": 0
                },
                {
                    "sent": "The probabilities, so we there's there are more different hypothesis that are under consideration and as a result the probability mass kind of needs to be spread out among more different hypothesis and what that means is that there's just less probability mass assigned to the correct hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And so that leads to this drop off in the scores of the correct words, and similarly we actually get a slight increase in distracter scores because they are.",
                    "label": 0
                },
                {
                    "sent": "Are actually more of those hypothesis that contain this.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tractors.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to summarize in this part of the talk, basically I've shown that statistical segmentation is more difficult when utterances contain more words.",
                    "label": 1
                },
                {
                    "sent": "This is not a terribly surprising result, but actually I think this is the first time that it's been shown experimentally.",
                    "label": 1
                },
                {
                    "sent": "And I've also shown that this kind of gradual decay in performance is predicted by the Bayesian model as I've just shown, but not by the other models that were tested, and as I've just explained, the Bayesian model actually suggests or predicts that this difficulty is primarily due to the effects of competition, wherein longer utterances correct words are less probable just because there are more other other possibilities to consider.",
                    "label": 1
                },
                {
                    "sent": "The local statistical approaches don't model this kind of.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Competition.",
                    "label": 0
                },
                {
                    "sent": "We're actually still working on some additional experiments in this similar vein, so we'd like to look at some other kinds of predictions that the model makes.",
                    "label": 0
                },
                {
                    "sent": "So one of those is varying the length of exposure to the training stimulus, so the Bayesian model actually predicts that longer exposure should lead to better performance.",
                    "label": 1
                },
                {
                    "sent": "This is not terribly surprising.",
                    "label": 0
                },
                {
                    "sent": "Transitional probabilities actually suggest that there really shouldn't be any effective exposure, so we're hoping to show this.",
                    "label": 1
                },
                {
                    "sent": "Experimentally, which would again confirm the Bayesian model predictions and the Secondly we also want to vary the number of lexical items so the Bayesian model again intuitively suggests that the larger lexecon would lead to worse performance, while the transitional probability model actually has a counter intuitive prediction which goes the other way, suggesting that a larger lexecon would lead to better performance.",
                    "label": 0
                },
                {
                    "sent": "So those are two things that we're working on right now, but we're still running subjects in terms.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It takes a lot of subjects to run these things.",
                    "label": 0
                },
                {
                    "sent": "And just to conclude, basically what I've talked about today are two different parts of a research.",
                    "label": 0
                },
                {
                    "sent": "Looking at both computer simulations as well as experimental work.",
                    "label": 1
                },
                {
                    "sent": "And in the task of word segmentation.",
                    "label": 0
                },
                {
                    "sent": "I've shown using a Bayesian model an that the unigram assumption that's made by a lot of previous work actually causes ideal learners to under segment fluent speech.",
                    "label": 1
                },
                {
                    "sent": "And then I've also tried to show that this kind of theoretical work actually does have implications for human learning because, at least in this one case that I've talked about, human word segmentation does actually seem to approximate the Bayesian ideal learning scenario that I talked about in the first part of the talk.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}