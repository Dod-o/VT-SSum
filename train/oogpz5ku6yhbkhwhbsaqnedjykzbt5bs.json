{
    "id": "oogpz5ku6yhbkhwhbsaqnedjykzbt5bs",
    "title": "Practical Variational Inference for Neural Networks",
    "info": {
        "author": [
            "Alex Graves, University of Toronto"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Neural Networks",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_graves_networks/",
    "segmentation": [
        [
            "So variational inference is a very popular tool for performing approximate Bayesian inference in machine learning.",
            "But one class of kind of machine learning model that it isn't often applied to is neural networks, so neural networks are usually just trained with maximum likelihood plus early stopping and maybe some kind of regularization term.",
            "And I think that one of the main reasons that variational inference isn't often applied here is that neural networks are kind of by construction, very complex nonlinear functions for which is hard to derive good variational bounds, and in particular you tend to have to re derive these bounds every time you make a significant change to the network architecture, and this kind of goes against the grain of neural network research where people like to stick together kind of differentiable modules or subnetworks in a pretty arbitrary way.",
            "And then think about optimization afterwards.",
            "So for example that you might want to add an extra hidden layer or some new type of activation function or something like that, and you don't want to have to re derive your learning algorithm."
        ],
        [
            "So in general, doing variational inference with the neural network essentially means that instead of optimizing a single point in weight space, you're optimizing a, or you're optimizing the parameters of a distribution over weight space.",
            "And in our case, is a pretty simple distribution.",
            "It's a diagonal Gaussian.",
            "So basically just means that instead of a single parameter for every wait, there's two parameters, the mean and the variance.",
            "And this is basically equivalent to or.",
            "It's very similar to training a neural network with white noise.",
            "So when you're doing weight noise training, basically you you adapt the point in white space to which you're adding some fixed variance Gaussian noise.",
            "You know as a regularizer, but the difference is that here we're adapting the variance as well as the mean.",
            "And what this means is that the network can kind of choose to make some weights quite precise while making other weights more you know.",
            "Have a higher variance and therefore be less precisely specified.",
            "Now there's two ways of kind of interpreting the loss function that you get from this.",
            "You can think of it basically like as a form of stochastic variational inference with a loss function.",
            "Is the free energy that you're trying to the variational free energy that you're trying to minimize.",
            "Or you can rearrange it into a minimum description length loss function which breaks down into two parts.",
            "One of them is equivalent to the number of bits that are required to to the expected number of bits required to transmit a particular weight vector drawn from this distribution and wait space, and this is to transmit this to some receiver who already has a prior overweight space and the other term is just the the kind of the ordinary log loss, which is the number of bits it would the expected number of bits to transmit the data given the weights, and was kind of nice about that, it means that you can.",
            "Recast optimization as compression and you can really measure how much you're actually compressing the training data using this network.",
            "Which for one thing means that you can.",
            "You can compare it for example to the compression given by like an off the shelf compressor.",
            "So, but the real motivation for doing this, at least in my case, is to is to improve generalization to reduce overfitting and.",
            "The way the way, the way it does that is that in order to sort of memorize or or store more information about the training set, the network has to somehow store that information in the weights and it then has to pay in order to transmit these weights.",
            "So it's really very resistant to overfitting."
        ],
        [
            "So just to run through the main advantages of the method, so I guess the biggest one is that unlike these variational methods that have to be, you know, drive for some particular neural net class of neural networks.",
            "This thing is very generic.",
            "It applies to any differentiable log loss model, which includes basically any or most.",
            "Directed neural networks.",
            "Let's say it doesn't include energy energy based neural networks.",
            "Another big advantage I found that in practice, so that there are some caveats.",
            "Here you have you have to have kind of sensibly defined training targets and a few things like that, but basically in practice I don't need a validation set for early stopping anymore.",
            "Just train this thing on the training set and take the you know the set of weights that gives or the set of parameters that gives the best performance on the training set and then apply it to this unseen test set.",
            "You also get some kind of information about the network out of this thing, because the transmission costs breakdown into into a cost per weight, so you can kind of read something about, you know which weights the network really needs, in which weights it doesn't, and a kind of natural extension of that is you can prune away a lot of the network weights, in particular the ones that have a high probability at 0."
        ],
        [
            "OK, and maybe more importantly, it actually seems to work.",
            "So I took a very complex neural networks with the precise.",
            "It's a hierarchical multidimensional long short term memory networks, with the connectionist temporal classification output layer.",
            "So basically it's exactly the kind of network for which you wouldn't be able to derive a you know a more conventional variational bound and applied this method, and I find that you know this is basically that it works better than L2L1 or ordinary white noise and it also.",
            "Allowed me to prune away an awful lot of weights without really losing any important performance.",
            "In fact, it could even improve performance if the network was retrained following pruning, so please come to my poster for more details.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So variational inference is a very popular tool for performing approximate Bayesian inference in machine learning.",
                    "label": 1
                },
                {
                    "sent": "But one class of kind of machine learning model that it isn't often applied to is neural networks, so neural networks are usually just trained with maximum likelihood plus early stopping and maybe some kind of regularization term.",
                    "label": 0
                },
                {
                    "sent": "And I think that one of the main reasons that variational inference isn't often applied here is that neural networks are kind of by construction, very complex nonlinear functions for which is hard to derive good variational bounds, and in particular you tend to have to re derive these bounds every time you make a significant change to the network architecture, and this kind of goes against the grain of neural network research where people like to stick together kind of differentiable modules or subnetworks in a pretty arbitrary way.",
                    "label": 0
                },
                {
                    "sent": "And then think about optimization afterwards.",
                    "label": 0
                },
                {
                    "sent": "So for example that you might want to add an extra hidden layer or some new type of activation function or something like that, and you don't want to have to re derive your learning algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in general, doing variational inference with the neural network essentially means that instead of optimizing a single point in weight space, you're optimizing a, or you're optimizing the parameters of a distribution over weight space.",
                    "label": 0
                },
                {
                    "sent": "And in our case, is a pretty simple distribution.",
                    "label": 0
                },
                {
                    "sent": "It's a diagonal Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So basically just means that instead of a single parameter for every wait, there's two parameters, the mean and the variance.",
                    "label": 1
                },
                {
                    "sent": "And this is basically equivalent to or.",
                    "label": 1
                },
                {
                    "sent": "It's very similar to training a neural network with white noise.",
                    "label": 0
                },
                {
                    "sent": "So when you're doing weight noise training, basically you you adapt the point in white space to which you're adding some fixed variance Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "You know as a regularizer, but the difference is that here we're adapting the variance as well as the mean.",
                    "label": 0
                },
                {
                    "sent": "And what this means is that the network can kind of choose to make some weights quite precise while making other weights more you know.",
                    "label": 0
                },
                {
                    "sent": "Have a higher variance and therefore be less precisely specified.",
                    "label": 0
                },
                {
                    "sent": "Now there's two ways of kind of interpreting the loss function that you get from this.",
                    "label": 1
                },
                {
                    "sent": "You can think of it basically like as a form of stochastic variational inference with a loss function.",
                    "label": 0
                },
                {
                    "sent": "Is the free energy that you're trying to the variational free energy that you're trying to minimize.",
                    "label": 1
                },
                {
                    "sent": "Or you can rearrange it into a minimum description length loss function which breaks down into two parts.",
                    "label": 0
                },
                {
                    "sent": "One of them is equivalent to the number of bits that are required to to the expected number of bits required to transmit a particular weight vector drawn from this distribution and wait space, and this is to transmit this to some receiver who already has a prior overweight space and the other term is just the the kind of the ordinary log loss, which is the number of bits it would the expected number of bits to transmit the data given the weights, and was kind of nice about that, it means that you can.",
                    "label": 1
                },
                {
                    "sent": "Recast optimization as compression and you can really measure how much you're actually compressing the training data using this network.",
                    "label": 0
                },
                {
                    "sent": "Which for one thing means that you can.",
                    "label": 0
                },
                {
                    "sent": "You can compare it for example to the compression given by like an off the shelf compressor.",
                    "label": 0
                },
                {
                    "sent": "So, but the real motivation for doing this, at least in my case, is to is to improve generalization to reduce overfitting and.",
                    "label": 0
                },
                {
                    "sent": "The way the way, the way it does that is that in order to sort of memorize or or store more information about the training set, the network has to somehow store that information in the weights and it then has to pay in order to transmit these weights.",
                    "label": 0
                },
                {
                    "sent": "So it's really very resistant to overfitting.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to run through the main advantages of the method, so I guess the biggest one is that unlike these variational methods that have to be, you know, drive for some particular neural net class of neural networks.",
                    "label": 0
                },
                {
                    "sent": "This thing is very generic.",
                    "label": 0
                },
                {
                    "sent": "It applies to any differentiable log loss model, which includes basically any or most.",
                    "label": 1
                },
                {
                    "sent": "Directed neural networks.",
                    "label": 1
                },
                {
                    "sent": "Let's say it doesn't include energy energy based neural networks.",
                    "label": 0
                },
                {
                    "sent": "Another big advantage I found that in practice, so that there are some caveats.",
                    "label": 0
                },
                {
                    "sent": "Here you have you have to have kind of sensibly defined training targets and a few things like that, but basically in practice I don't need a validation set for early stopping anymore.",
                    "label": 0
                },
                {
                    "sent": "Just train this thing on the training set and take the you know the set of weights that gives or the set of parameters that gives the best performance on the training set and then apply it to this unseen test set.",
                    "label": 1
                },
                {
                    "sent": "You also get some kind of information about the network out of this thing, because the transmission costs breakdown into into a cost per weight, so you can kind of read something about, you know which weights the network really needs, in which weights it doesn't, and a kind of natural extension of that is you can prune away a lot of the network weights, in particular the ones that have a high probability at 0.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and maybe more importantly, it actually seems to work.",
                    "label": 0
                },
                {
                    "sent": "So I took a very complex neural networks with the precise.",
                    "label": 0
                },
                {
                    "sent": "It's a hierarchical multidimensional long short term memory networks, with the connectionist temporal classification output layer.",
                    "label": 0
                },
                {
                    "sent": "So basically it's exactly the kind of network for which you wouldn't be able to derive a you know a more conventional variational bound and applied this method, and I find that you know this is basically that it works better than L2L1 or ordinary white noise and it also.",
                    "label": 0
                },
                {
                    "sent": "Allowed me to prune away an awful lot of weights without really losing any important performance.",
                    "label": 0
                },
                {
                    "sent": "In fact, it could even improve performance if the network was retrained following pruning, so please come to my poster for more details.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}