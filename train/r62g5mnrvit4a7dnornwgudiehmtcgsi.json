{
    "id": "r62g5mnrvit4a7dnornwgudiehmtcgsi",
    "title": "SAKey: Scalable Almost Key discovery in RDF data",
    "info": {
        "author": [
            "Danai Symeonidou, Laboratory for Computer Science, University of Paris-Sud 11"
        ],
        "published": "Dec. 19, 2014",
        "recorded": "October 2014",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2014_symeonidou_sakey/",
    "segmentation": [
        [
            "Hello everybody, my name is and I smell needle and today I'm going to present you esaki scalable muskie discovery approach for RDF data.",
            "So this is a joint work with Wassermann Natalie Barney Lymphatic's eyes."
        ],
        [
            "So we have already heard in the previous talk the background about the linked open data cloud and data linking program.",
            "So just to introduce a little bit again, so we have a lot of RDF datasets on the web and we can have also links between them.",
            "So one of the most important links, as you can see here is the same as link, because actually it allows us to integrate information is coming from different data sources.",
            "So actually since we have more and more more and more RDF data published on the Web, the question is actually can we define manually such links?",
            "And the answer is no, it's very hard to define the manually therefore."
        ],
        [
            "As we have seen before, there exists already data linking approaches that work on this problem.",
            "So actually what we we have to see is that most of these approaches use rules actually to link, and these rules actually can be logical rules or complex rules.",
            "And as we can see here, for example, this logical rule states that if we have two people we have with that share the same Social Security number, then we can infer that these two people are the same person.",
            "In fact.",
            "So what we should notice at this point is that this kind of rules.",
            "Use very discriminative properties so properties that uniquely identify the instances in order to be able to connect them.",
            "So actually they make the use of keys so."
        ],
        [
            "Let's see now the problem of keys.",
            "So the keys are not so easily declared by an expert."
        ],
        [
            "And why somebody would say OK?",
            "I know, for example, that the Social Security number is a key for a person.",
            "It can uniquely identify a person, so why it's hard?",
            "Or for example, the ISBN which is an identifier of a book.",
            "But when it?"
        ],
        [
            "Times two more composite combinations of properties actually.",
            "So for example name, date of birth and burning.",
            "For for a person the question is can we be sure that it's a key?",
            "I ask you a, can you be sure?",
            "No, the answer is no.",
            "We cannot be sure so."
        ],
        [
            "That's why experts many times introduce errors in the in the key, so introduce erroneous keys in the data."
        ],
        [
            "And another point that is very important is that the more keys we have, the more links we can construct because we can have more rules."
        ],
        [
            "So actually the goal of our work and we.",
            "Is on the discovery of keys in an automatic way in order to be used in a data linking process or in other process.",
            "But the main goal was initial."
        ],
        [
            "Before the delete.",
            "So let's see little bit the definition of the key.",
            "So all two has already defined a key for a class which actually represents a combination of properties that uniquely identify each instance of class, and we can define in the ontology using this construct that a set of objects and data type properties are a key for a class."
        ],
        [
            "So let's see an example."
        ],
        [
            "It's sufficient in this case to have one author that is the same and one title that is common to infer that two books are the same.",
            "So this is the what old allows us to do with this construct."
        ],
        [
            "So the key discovery problem is not knew it has been already studied in the relational database issan in the semantic Web.",
            "But what we can observe actually is that."
        ],
        [
            "There is no approach until today that is able to discover the complete set of composite keys following the definition of all 2 using heuristics to be able to interpret the incompleteness of data and also being able to discover keys that can be approximate."
        ],
        [
            "So is where we want to attack actually, plus another problem that is very crucial is the scalability of this kind of approaches.",
            "So the question is."
        ],
        [
            "Talking, we discovered keys in RDF data when we have errors when we can have duplicates, which actually happens in the real life.",
            "It happens in the real data that we have and also how can we discover keys from data that can be numerous and can be described by many properties."
        ],
        [
            "So let's see an example about the errors in the duplicates.",
            "So considering the simple definition that a key, something that uniquely identifies every instance, for example of the class film, what we see here is that we wouldn't be able to discover any key in this example, because it's."
        ],
        [
            "People we just have to duplicate.",
            "We have two films that refer exactly to the same film.",
            "Therefore there is no combination of properties that can uniquely identify the film.",
            "And this is thanks actually to the errors in public."
        ],
        [
            "So the goal is to be able to discover keys even when the data can contain errors and duplicates."
        ],
        [
            "So that's why we introduce sake and scalable musky discovery approach which actually is able to discover sets of keys that are such properties that are not keys due to few exceptions."
        ],
        [
            "You know what is an exception?",
            "An exception for a key for a set of properties P is an instance that shares values with another instance for a given set of properties."
        ],
        [
            "So let's see here this example.",
            "What we observe here is that we have descriptions of the class film.",
            "And there we can see, we can observe that there exist two films sharing the same name.",
            "Therefore the film two and the film 6 represent exceptions that do not allow us to consider the property name is a key."
        ],
        [
            "Therefore, we introduced the notion of exception set, which actually represents a set of exceptions, and in this case the exception set for the property name is film two and film 6."
        ],
        [
            "So we arrive to our definition of almost keys, so an enormous key represents a set of properties that has at most as an exceptions.",
            "So in this case the name is Atul Musky, which means that it's a key.",
            "If we are removing.",
            "Actually, if we don't see this two exceptions that we have in the data."
        ],
        [
            "So let's see now why this problem is hard to do so, and I have an automatic way to discover almost keys would be first to examine all the possible combinations of properties so we have to create all the combinations of properties and then for each of these combinations what we have to do is to scan all the instances to validate if this set of properties refers to a real almost key or not.",
            "So for example, given a class inscribed by 15 properties, we have more than 32,000 candidate keys that we're supposed to validate.",
            "So what we can see from this small set of properties is that this problem is a very time consuming problem and it's very hard to solve there."
        ],
        [
            "Before.",
            "What we should do is to be able to discover almost keys in an efficient way by both reducing the number of combinations and by also partially scanning our data."
        ],
        [
            "So let's see how we do it.",
            "So to partially scan the data, we introduce the notion of unknown keys, which actually represents a set of properties that has at least an exceptions so."
        ],
        [
            "Let's see why discovering any keys first is better than discovering almost keys.",
            "So for example, to verify that the website is a zero almost Kiseki with no exceptions, we need to check all the websites and be sure that every website is distinct.",
            "So OK, we scan, we are able to validate website is zero musky."
        ],
        [
            "Let's now proceed to the case of director.",
            "What do we see?",
            "We see that by checking only the two first films, we're able to identify that the director is a turnkey.",
            "So it has two exceptions inside and it's enough.",
            "We can stop.",
            "We don't need to continue.",
            "Therefore discovering first non keys.",
            "It's much faster than discovering keys."
        ],
        [
            "So the idea here is not by discovering all the maximal unknown keys we're able after to derive easily the N -- 1 almost kiss."
        ],
        [
            "So let's say a little bit.",
            "What is the general idea of their of our approach?",
            "So we have initially the data that we actually are able to filter and we are able to represent them in a final map, which is a very we are actually able to filter out a lot of information that are not necessary."
        ],
        [
            "I'm sorry I did a mistake so we have an expert that tells us the number of exceptions that he wants for the almost keys and we're able in this step to filter out combinations of properties that should not be explored because they will never refer to non keys.",
            "Given these two ingredients, we proceed to the non key discovery and once all the maximal anarchies are discovered we're able then to derive the almost keys.",
            "So let's proceed now to the data filtering step."
        ],
        [
            "So as we've seen them in, these are the data represented, the representing the films of the previous example.",
            "So what we see here is that we represent the data in a map called initial map, where actually its role of the map represents values of the properties and what we see here is that we actually group the films according to their sharing values.",
            "So for example, here F1F2 are together because Julia Roberts plays in F1F2 and so on.",
            "So actually we group the information because we want to see.",
            "Movies are the values that are shared.",
            "So."
        ],
        [
            "The question at this point was should we keep all this information or their information that we can eliminate that can never lead us to non keys and the answer is yes, there are information that can be eliminated at this step and for example what we can see an very naive filtering here is that for example, the fact that Leman plays only film 5 can never lead us to a non key.",
            "So we eliminate this information because actually this person is not shared in different films.",
            "He's not an actor in different films so he can never lead us to an Anki.",
            "And thanks to these filtering's we are able also to eliminate properties that refer to single keys, as we've seen.",
            "For example the website.",
            "So the website is a key, and since each film as a unique website, we are able to discover directly by this filtering is that it's a key and to eliminate it, so to eliminate also a number of combinations of properties action."
        ],
        [
            "So what we see here is that by applying this filtering's we are able to filter out a very big part of the data which allows us in the end to discover what we want.",
            "Only this sub part of the data which we call final map."
        ],
        [
            "So we've seen the filtering step and now what I've told you is that the problem is so hard because actually we have to do all these combinations of properties and validate if they are non keys or not.",
            "So actually."
        ],
        [
            "What we've seen is that there exists combinations of properties that should not be explored, thanks to the incompleteness of data, and thanks to the fact that properties can refer to different classes, and the idea is that we are able using a greedy algorithm to construct set of properties that possibly refer to a non key.",
            "So actually we're going to explore only a subpart of the combinations of properties that exist."
        ],
        [
            "So proceeding now to the Nonkey discovery part."
        ],
        [
            "So what we've seen before is that for the property name, there are two films, F2 and F6, that share the same name.",
            "Therefore, we said that it's a turnkey, but the question that we have now is OK, how do we?",
            "How are we able to discover composite Anon keys?",
            "So to do so?",
            "Actually what we have to do is to do intersections between the sets of different properties and let's see here in."
        ],
        [
            "Example so."
        ],
        [
            "To discover that his actor director is a 3 nonkey what we have to do is to intersect the sets here and what we're able in fact to discover in the end is that there are three films that share on the same time.",
            "Has actor and director so F1F2 share actors and directors.",
            "Therefore it's A3 non key."
        ],
        [
            "So as I said before, we have to do all the combinations of properties to be able to discover the complete set of maximal anarchies.",
            "So it's what we represent actually here.",
            "But as I said before, this is very time consuming, so actually."
        ],
        [
            "We have proposed in this approach different kinds of prunings that allows us to filter out a lot of combinations of properties and a lot of computations."
        ],
        [
            "But allow this approach to go even faster."
        ],
        [
            "So proceeding to the last step."
        ],
        [
            "The algorithm, once all the maximal Anon keys are discovered, we want to derive the almost keys.",
            "As I've said before.",
            "And this problem has been already treated by different approaches, but the way that it was being done until now, it was very, very complicated and time consuming.",
            "Therefore we have proposed in this paper.",
            "In this work a very efficient derivation of minimal keys from minimal energy and almost keys from maximal N + 1 keys.",
            "And this algorithm is based on the frequencies of properties."
        ],
        [
            "So let's proceed now to the experiments and evaluation of the our approach.",
            "So we have done so.",
            "The experiments can be split into big parts, so the first part is dattoli data linking using almost keys and the second part is the scalability of this approach.",
            "So we have actually done a lot of experiments and psyche has been validated and tested in actually all the two classes of the pedia in classes from Jago and from benchmarks for data linking that I'm going to explain after."
        ],
        [
            "So let's go first to the data linking problem that we were trying to tackle using almost keys.",
            "So the goal here was to compare linking results using almost keys with different N values and see actually if the almost kiss can help with the data linking problem."
        ],
        [
            "So to do so, we use the recall precision and F measure."
        ],
        [
            "And we have used datasets coming from the ontology Alignment Evaluation Initiative, which actually are benchmarks for data linking, so."
        ],
        [
            "Where the ghosts down there actually is provided, so the conclusion for this experiment is that the linking results using an almost keys were always better than using simple keys and let's see and if."
        ],
        [
            "People, so in the case of the 8013 for the class person, what we see that we discovered, for example, that the combination of birthday than a word is a key and we see that the measure here is 70%.",
            "By allowing two exceptions, we see that the birth date becomes a key actually and we are able to increase the measure by 30%.",
            "At this point I want to underline the to do we see of course that the recall and F measure are low in any case, but we have used.",
            "A very strict similarity measure.",
            "So."
        ],
        [
            "Proceeding through the scalability, actually, what we have done is that we have compared Katie to our which is an old approach to the discoveries, keys with sake and the into steps actually in the non key discovery part and in the key derivation steps apart by setting the number of exceptions to 0."
        ],
        [
            "So we have used a different date."
        ],
        [
            "It's it's in the conclusion is that Psyche was much more scalable both in the key derivation and key discovery part."
        ],
        [
            "So that is depicted also in this in this slide, where we can see, for example, that when the number of properties increases very much get it.",
            "Why is out of memory?",
            "Why for us?",
            "For example it stays always low, 337 seconds in one minute, so going."
        ],
        [
            "To the kid derivation part, what we see is for example in the class body body of water key.",
            "Detour takes more than one day to derive the keys.",
            "Why for us it's only 62 seconds using the new key derivation approach that we have introduced."
        ],
        [
            "So moving now to the scalability of Katy to our alone, we wanted to see actually go very fast.",
            "I go very fast.",
            "The data filtering.",
            "Actually the pruning set that we have done.",
            "So I proceed fast."
        ],
        [
            "Because."
        ],
        [
            "You make me a sign, so actually what we've seen is that."
        ],
        [
            "All these things that we do are very, very efficient and what we can see here very fast is that using them the.",
            "Data filtering we are able to filter out in the classes of the pedia a 90% of the data which is actually very very important and also a very very big number of properties is able to be eliminated because they refer to single keys."
        ],
        [
            "So also the prunings help us to go much faster.",
            "So for example, from 4 minutes we jump to 40 seconds, which actually is a very significant result."
        ],
        [
            "And in this last slide about the experiments, what we wanted to see is what is the reaction of the this approach when the number of exceptions was increasing and what we can say is that actually we are able to be efficient even when a lot of exceptions are allowed."
        ],
        [
            "To conclude, we have introduced it's not working.",
            "So what we have introduced today is Psyche enormous key discovery approach in RDF data, which actually is able to treat aroni errors and duplicates.",
            "We have introduced the notion of an almost keys which actually refer to keys with at most and exceptions.",
            "We have a lot of very good scalability thanks to filterings and pruning strategies, and also thanks to a very scalable key derivation approaches approach and the experiment has shown actually the relevance of anonymous keys in the data linking.",
            "And also the scalability of this approach and as a future work fast what we want to do is to be able to set the number of exceptions automatically without using an expert and to update keys when the data involved.",
            "Thanks a lot.",
            "Let's thank the speaker.",
            "OK, so just to confirm the N equal to 0 is quadratic.",
            "Is that the N equal to 0 that you evaluated against as your baseline?",
            "Is that quadratic like how?",
            "What is what does that do exactly?",
            "Say again the question then the end your essay key your baseline when you when you when you have an equal to zero, yes, so actually that is like a real key and the zero almost keys are key.",
            "But in many datasets you won't find that at all.",
            "Like there is no are.",
            "So what I'm trying to say is that there are other baselines where you have you have blocking and you have similarity like it's very old problem.",
            "So why?",
            "Why about the linking now?",
            "Yes yeah.",
            "So why do we need to use almost keys?",
            "So it's why because actually what I've shown a lot of data set that have errors inside.",
            "So actually what you discover is something that, for example you can lose for example the Social Security number because we have only two people inside your data set that share the Social Security number.",
            "Because it's erroneous information, so you lose that.",
            "Social Security number is a key, and you're not going to link people because you are not actually exploiting the correct information.",
            "You see what I mean you?",
            "Thanks to the errors.",
            "You're you're losing information that are significant.",
            "So by allowing some exceptions, you're able to discover keys that were lost.",
            "Actually thanks to duplicate or text when running with data.",
            "So it's actually why we want to relax a little bit because the data that we have.",
            "But but but but blocking does died.",
            "I'm sorry we don't have time for a follow up on.",
            "Fortunately, that's offline, thank you.",
            "Thanks speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everybody, my name is and I smell needle and today I'm going to present you esaki scalable muskie discovery approach for RDF data.",
                    "label": 0
                },
                {
                    "sent": "So this is a joint work with Wassermann Natalie Barney Lymphatic's eyes.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have already heard in the previous talk the background about the linked open data cloud and data linking program.",
                    "label": 1
                },
                {
                    "sent": "So just to introduce a little bit again, so we have a lot of RDF datasets on the web and we can have also links between them.",
                    "label": 0
                },
                {
                    "sent": "So one of the most important links, as you can see here is the same as link, because actually it allows us to integrate information is coming from different data sources.",
                    "label": 0
                },
                {
                    "sent": "So actually since we have more and more more and more RDF data published on the Web, the question is actually can we define manually such links?",
                    "label": 1
                },
                {
                    "sent": "And the answer is no, it's very hard to define the manually therefore.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As we have seen before, there exists already data linking approaches that work on this problem.",
                    "label": 1
                },
                {
                    "sent": "So actually what we we have to see is that most of these approaches use rules actually to link, and these rules actually can be logical rules or complex rules.",
                    "label": 1
                },
                {
                    "sent": "And as we can see here, for example, this logical rule states that if we have two people we have with that share the same Social Security number, then we can infer that these two people are the same person.",
                    "label": 0
                },
                {
                    "sent": "In fact.",
                    "label": 0
                },
                {
                    "sent": "So what we should notice at this point is that this kind of rules.",
                    "label": 0
                },
                {
                    "sent": "Use very discriminative properties so properties that uniquely identify the instances in order to be able to connect them.",
                    "label": 0
                },
                {
                    "sent": "So actually they make the use of keys so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see now the problem of keys.",
                    "label": 0
                },
                {
                    "sent": "So the keys are not so easily declared by an expert.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And why somebody would say OK?",
                    "label": 0
                },
                {
                    "sent": "I know, for example, that the Social Security number is a key for a person.",
                    "label": 0
                },
                {
                    "sent": "It can uniquely identify a person, so why it's hard?",
                    "label": 0
                },
                {
                    "sent": "Or for example, the ISBN which is an identifier of a book.",
                    "label": 0
                },
                {
                    "sent": "But when it?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Times two more composite combinations of properties actually.",
                    "label": 0
                },
                {
                    "sent": "So for example name, date of birth and burning.",
                    "label": 0
                },
                {
                    "sent": "For for a person the question is can we be sure that it's a key?",
                    "label": 1
                },
                {
                    "sent": "I ask you a, can you be sure?",
                    "label": 0
                },
                {
                    "sent": "No, the answer is no.",
                    "label": 0
                },
                {
                    "sent": "We cannot be sure so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's why experts many times introduce errors in the in the key, so introduce erroneous keys in the data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And another point that is very important is that the more keys we have, the more links we can construct because we can have more rules.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So actually the goal of our work and we.",
                    "label": 0
                },
                {
                    "sent": "Is on the discovery of keys in an automatic way in order to be used in a data linking process or in other process.",
                    "label": 0
                },
                {
                    "sent": "But the main goal was initial.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before the delete.",
                    "label": 0
                },
                {
                    "sent": "So let's see little bit the definition of the key.",
                    "label": 0
                },
                {
                    "sent": "So all two has already defined a key for a class which actually represents a combination of properties that uniquely identify each instance of class, and we can define in the ontology using this construct that a set of objects and data type properties are a key for a class.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see an example.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's sufficient in this case to have one author that is the same and one title that is common to infer that two books are the same.",
                    "label": 0
                },
                {
                    "sent": "So this is the what old allows us to do with this construct.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the key discovery problem is not knew it has been already studied in the relational database issan in the semantic Web.",
                    "label": 0
                },
                {
                    "sent": "But what we can observe actually is that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is no approach until today that is able to discover the complete set of composite keys following the definition of all 2 using heuristics to be able to interpret the incompleteness of data and also being able to discover keys that can be approximate.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So is where we want to attack actually, plus another problem that is very crucial is the scalability of this kind of approaches.",
                    "label": 0
                },
                {
                    "sent": "So the question is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talking, we discovered keys in RDF data when we have errors when we can have duplicates, which actually happens in the real life.",
                    "label": 0
                },
                {
                    "sent": "It happens in the real data that we have and also how can we discover keys from data that can be numerous and can be described by many properties.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see an example about the errors in the duplicates.",
                    "label": 0
                },
                {
                    "sent": "So considering the simple definition that a key, something that uniquely identifies every instance, for example of the class film, what we see here is that we wouldn't be able to discover any key in this example, because it's.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People we just have to duplicate.",
                    "label": 0
                },
                {
                    "sent": "We have two films that refer exactly to the same film.",
                    "label": 0
                },
                {
                    "sent": "Therefore there is no combination of properties that can uniquely identify the film.",
                    "label": 0
                },
                {
                    "sent": "And this is thanks actually to the errors in public.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the goal is to be able to discover keys even when the data can contain errors and duplicates.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's why we introduce sake and scalable musky discovery approach which actually is able to discover sets of keys that are such properties that are not keys due to few exceptions.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know what is an exception?",
                    "label": 0
                },
                {
                    "sent": "An exception for a key for a set of properties P is an instance that shares values with another instance for a given set of properties.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see here this example.",
                    "label": 0
                },
                {
                    "sent": "What we observe here is that we have descriptions of the class film.",
                    "label": 0
                },
                {
                    "sent": "And there we can see, we can observe that there exist two films sharing the same name.",
                    "label": 0
                },
                {
                    "sent": "Therefore the film two and the film 6 represent exceptions that do not allow us to consider the property name is a key.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Therefore, we introduced the notion of exception set, which actually represents a set of exceptions, and in this case the exception set for the property name is film two and film 6.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we arrive to our definition of almost keys, so an enormous key represents a set of properties that has at most as an exceptions.",
                    "label": 0
                },
                {
                    "sent": "So in this case the name is Atul Musky, which means that it's a key.",
                    "label": 0
                },
                {
                    "sent": "If we are removing.",
                    "label": 0
                },
                {
                    "sent": "Actually, if we don't see this two exceptions that we have in the data.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's see now why this problem is hard to do so, and I have an automatic way to discover almost keys would be first to examine all the possible combinations of properties so we have to create all the combinations of properties and then for each of these combinations what we have to do is to scan all the instances to validate if this set of properties refers to a real almost key or not.",
                    "label": 1
                },
                {
                    "sent": "So for example, given a class inscribed by 15 properties, we have more than 32,000 candidate keys that we're supposed to validate.",
                    "label": 0
                },
                {
                    "sent": "So what we can see from this small set of properties is that this problem is a very time consuming problem and it's very hard to solve there.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before.",
                    "label": 0
                },
                {
                    "sent": "What we should do is to be able to discover almost keys in an efficient way by both reducing the number of combinations and by also partially scanning our data.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see how we do it.",
                    "label": 0
                },
                {
                    "sent": "So to partially scan the data, we introduce the notion of unknown keys, which actually represents a set of properties that has at least an exceptions so.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see why discovering any keys first is better than discovering almost keys.",
                    "label": 0
                },
                {
                    "sent": "So for example, to verify that the website is a zero almost Kiseki with no exceptions, we need to check all the websites and be sure that every website is distinct.",
                    "label": 0
                },
                {
                    "sent": "So OK, we scan, we are able to validate website is zero musky.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's now proceed to the case of director.",
                    "label": 0
                },
                {
                    "sent": "What do we see?",
                    "label": 0
                },
                {
                    "sent": "We see that by checking only the two first films, we're able to identify that the director is a turnkey.",
                    "label": 0
                },
                {
                    "sent": "So it has two exceptions inside and it's enough.",
                    "label": 0
                },
                {
                    "sent": "We can stop.",
                    "label": 0
                },
                {
                    "sent": "We don't need to continue.",
                    "label": 0
                },
                {
                    "sent": "Therefore discovering first non keys.",
                    "label": 0
                },
                {
                    "sent": "It's much faster than discovering keys.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea here is not by discovering all the maximal unknown keys we're able after to derive easily the N -- 1 almost kiss.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's say a little bit.",
                    "label": 0
                },
                {
                    "sent": "What is the general idea of their of our approach?",
                    "label": 0
                },
                {
                    "sent": "So we have initially the data that we actually are able to filter and we are able to represent them in a final map, which is a very we are actually able to filter out a lot of information that are not necessary.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm sorry I did a mistake so we have an expert that tells us the number of exceptions that he wants for the almost keys and we're able in this step to filter out combinations of properties that should not be explored because they will never refer to non keys.",
                    "label": 0
                },
                {
                    "sent": "Given these two ingredients, we proceed to the non key discovery and once all the maximal anarchies are discovered we're able then to derive the almost keys.",
                    "label": 0
                },
                {
                    "sent": "So let's proceed now to the data filtering step.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as we've seen them in, these are the data represented, the representing the films of the previous example.",
                    "label": 0
                },
                {
                    "sent": "So what we see here is that we represent the data in a map called initial map, where actually its role of the map represents values of the properties and what we see here is that we actually group the films according to their sharing values.",
                    "label": 0
                },
                {
                    "sent": "So for example, here F1F2 are together because Julia Roberts plays in F1F2 and so on.",
                    "label": 0
                },
                {
                    "sent": "So actually we group the information because we want to see.",
                    "label": 0
                },
                {
                    "sent": "Movies are the values that are shared.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The question at this point was should we keep all this information or their information that we can eliminate that can never lead us to non keys and the answer is yes, there are information that can be eliminated at this step and for example what we can see an very naive filtering here is that for example, the fact that Leman plays only film 5 can never lead us to a non key.",
                    "label": 0
                },
                {
                    "sent": "So we eliminate this information because actually this person is not shared in different films.",
                    "label": 0
                },
                {
                    "sent": "He's not an actor in different films so he can never lead us to an Anki.",
                    "label": 0
                },
                {
                    "sent": "And thanks to these filtering's we are able also to eliminate properties that refer to single keys, as we've seen.",
                    "label": 0
                },
                {
                    "sent": "For example the website.",
                    "label": 0
                },
                {
                    "sent": "So the website is a key, and since each film as a unique website, we are able to discover directly by this filtering is that it's a key and to eliminate it, so to eliminate also a number of combinations of properties action.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we see here is that by applying this filtering's we are able to filter out a very big part of the data which allows us in the end to discover what we want.",
                    "label": 0
                },
                {
                    "sent": "Only this sub part of the data which we call final map.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we've seen the filtering step and now what I've told you is that the problem is so hard because actually we have to do all these combinations of properties and validate if they are non keys or not.",
                    "label": 0
                },
                {
                    "sent": "So actually.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we've seen is that there exists combinations of properties that should not be explored, thanks to the incompleteness of data, and thanks to the fact that properties can refer to different classes, and the idea is that we are able using a greedy algorithm to construct set of properties that possibly refer to a non key.",
                    "label": 0
                },
                {
                    "sent": "So actually we're going to explore only a subpart of the combinations of properties that exist.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So proceeding now to the Nonkey discovery part.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we've seen before is that for the property name, there are two films, F2 and F6, that share the same name.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we said that it's a turnkey, but the question that we have now is OK, how do we?",
                    "label": 0
                },
                {
                    "sent": "How are we able to discover composite Anon keys?",
                    "label": 0
                },
                {
                    "sent": "So to do so?",
                    "label": 0
                },
                {
                    "sent": "Actually what we have to do is to do intersections between the sets of different properties and let's see here in.",
                    "label": 1
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example so.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To discover that his actor director is a 3 nonkey what we have to do is to intersect the sets here and what we're able in fact to discover in the end is that there are three films that share on the same time.",
                    "label": 0
                },
                {
                    "sent": "Has actor and director so F1F2 share actors and directors.",
                    "label": 0
                },
                {
                    "sent": "Therefore it's A3 non key.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I said before, we have to do all the combinations of properties to be able to discover the complete set of maximal anarchies.",
                    "label": 0
                },
                {
                    "sent": "So it's what we represent actually here.",
                    "label": 0
                },
                {
                    "sent": "But as I said before, this is very time consuming, so actually.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have proposed in this approach different kinds of prunings that allows us to filter out a lot of combinations of properties and a lot of computations.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But allow this approach to go even faster.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So proceeding to the last step.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The algorithm, once all the maximal Anon keys are discovered, we want to derive the almost keys.",
                    "label": 1
                },
                {
                    "sent": "As I've said before.",
                    "label": 0
                },
                {
                    "sent": "And this problem has been already treated by different approaches, but the way that it was being done until now, it was very, very complicated and time consuming.",
                    "label": 0
                },
                {
                    "sent": "Therefore we have proposed in this paper.",
                    "label": 0
                },
                {
                    "sent": "In this work a very efficient derivation of minimal keys from minimal energy and almost keys from maximal N + 1 keys.",
                    "label": 1
                },
                {
                    "sent": "And this algorithm is based on the frequencies of properties.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's proceed now to the experiments and evaluation of the our approach.",
                    "label": 0
                },
                {
                    "sent": "So we have done so.",
                    "label": 0
                },
                {
                    "sent": "The experiments can be split into big parts, so the first part is dattoli data linking using almost keys and the second part is the scalability of this approach.",
                    "label": 1
                },
                {
                    "sent": "So we have actually done a lot of experiments and psyche has been validated and tested in actually all the two classes of the pedia in classes from Jago and from benchmarks for data linking that I'm going to explain after.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's go first to the data linking problem that we were trying to tackle using almost keys.",
                    "label": 0
                },
                {
                    "sent": "So the goal here was to compare linking results using almost keys with different N values and see actually if the almost kiss can help with the data linking problem.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to do so, we use the recall precision and F measure.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have used datasets coming from the ontology Alignment Evaluation Initiative, which actually are benchmarks for data linking, so.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where the ghosts down there actually is provided, so the conclusion for this experiment is that the linking results using an almost keys were always better than using simple keys and let's see and if.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People, so in the case of the 8013 for the class person, what we see that we discovered, for example, that the combination of birthday than a word is a key and we see that the measure here is 70%.",
                    "label": 0
                },
                {
                    "sent": "By allowing two exceptions, we see that the birth date becomes a key actually and we are able to increase the measure by 30%.",
                    "label": 0
                },
                {
                    "sent": "At this point I want to underline the to do we see of course that the recall and F measure are low in any case, but we have used.",
                    "label": 0
                },
                {
                    "sent": "A very strict similarity measure.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Proceeding through the scalability, actually, what we have done is that we have compared Katie to our which is an old approach to the discoveries, keys with sake and the into steps actually in the non key discovery part and in the key derivation steps apart by setting the number of exceptions to 0.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have used a different date.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's it's in the conclusion is that Psyche was much more scalable both in the key derivation and key discovery part.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that is depicted also in this in this slide, where we can see, for example, that when the number of properties increases very much get it.",
                    "label": 0
                },
                {
                    "sent": "Why is out of memory?",
                    "label": 0
                },
                {
                    "sent": "Why for us?",
                    "label": 0
                },
                {
                    "sent": "For example it stays always low, 337 seconds in one minute, so going.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the kid derivation part, what we see is for example in the class body body of water key.",
                    "label": 0
                },
                {
                    "sent": "Detour takes more than one day to derive the keys.",
                    "label": 0
                },
                {
                    "sent": "Why for us it's only 62 seconds using the new key derivation approach that we have introduced.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So moving now to the scalability of Katy to our alone, we wanted to see actually go very fast.",
                    "label": 0
                },
                {
                    "sent": "I go very fast.",
                    "label": 0
                },
                {
                    "sent": "The data filtering.",
                    "label": 0
                },
                {
                    "sent": "Actually the pruning set that we have done.",
                    "label": 0
                },
                {
                    "sent": "So I proceed fast.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You make me a sign, so actually what we've seen is that.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All these things that we do are very, very efficient and what we can see here very fast is that using them the.",
                    "label": 0
                },
                {
                    "sent": "Data filtering we are able to filter out in the classes of the pedia a 90% of the data which is actually very very important and also a very very big number of properties is able to be eliminated because they refer to single keys.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So also the prunings help us to go much faster.",
                    "label": 0
                },
                {
                    "sent": "So for example, from 4 minutes we jump to 40 seconds, which actually is a very significant result.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this last slide about the experiments, what we wanted to see is what is the reaction of the this approach when the number of exceptions was increasing and what we can say is that actually we are able to be efficient even when a lot of exceptions are allowed.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To conclude, we have introduced it's not working.",
                    "label": 0
                },
                {
                    "sent": "So what we have introduced today is Psyche enormous key discovery approach in RDF data, which actually is able to treat aroni errors and duplicates.",
                    "label": 0
                },
                {
                    "sent": "We have introduced the notion of an almost keys which actually refer to keys with at most and exceptions.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of very good scalability thanks to filterings and pruning strategies, and also thanks to a very scalable key derivation approaches approach and the experiment has shown actually the relevance of anonymous keys in the data linking.",
                    "label": 0
                },
                {
                    "sent": "And also the scalability of this approach and as a future work fast what we want to do is to be able to set the number of exceptions automatically without using an expert and to update keys when the data involved.",
                    "label": 0
                },
                {
                    "sent": "Thanks a lot.",
                    "label": 0
                },
                {
                    "sent": "Let's thank the speaker.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to confirm the N equal to 0 is quadratic.",
                    "label": 0
                },
                {
                    "sent": "Is that the N equal to 0 that you evaluated against as your baseline?",
                    "label": 0
                },
                {
                    "sent": "Is that quadratic like how?",
                    "label": 0
                },
                {
                    "sent": "What is what does that do exactly?",
                    "label": 0
                },
                {
                    "sent": "Say again the question then the end your essay key your baseline when you when you when you have an equal to zero, yes, so actually that is like a real key and the zero almost keys are key.",
                    "label": 0
                },
                {
                    "sent": "But in many datasets you won't find that at all.",
                    "label": 0
                },
                {
                    "sent": "Like there is no are.",
                    "label": 0
                },
                {
                    "sent": "So what I'm trying to say is that there are other baselines where you have you have blocking and you have similarity like it's very old problem.",
                    "label": 0
                },
                {
                    "sent": "So why?",
                    "label": 0
                },
                {
                    "sent": "Why about the linking now?",
                    "label": 0
                },
                {
                    "sent": "Yes yeah.",
                    "label": 0
                },
                {
                    "sent": "So why do we need to use almost keys?",
                    "label": 0
                },
                {
                    "sent": "So it's why because actually what I've shown a lot of data set that have errors inside.",
                    "label": 0
                },
                {
                    "sent": "So actually what you discover is something that, for example you can lose for example the Social Security number because we have only two people inside your data set that share the Social Security number.",
                    "label": 0
                },
                {
                    "sent": "Because it's erroneous information, so you lose that.",
                    "label": 0
                },
                {
                    "sent": "Social Security number is a key, and you're not going to link people because you are not actually exploiting the correct information.",
                    "label": 0
                },
                {
                    "sent": "You see what I mean you?",
                    "label": 0
                },
                {
                    "sent": "Thanks to the errors.",
                    "label": 0
                },
                {
                    "sent": "You're you're losing information that are significant.",
                    "label": 0
                },
                {
                    "sent": "So by allowing some exceptions, you're able to discover keys that were lost.",
                    "label": 0
                },
                {
                    "sent": "Actually thanks to duplicate or text when running with data.",
                    "label": 0
                },
                {
                    "sent": "So it's actually why we want to relax a little bit because the data that we have.",
                    "label": 0
                },
                {
                    "sent": "But but but but blocking does died.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry we don't have time for a follow up on.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, that's offline, thank you.",
                    "label": 0
                },
                {
                    "sent": "Thanks speaker again.",
                    "label": 0
                }
            ]
        }
    }
}