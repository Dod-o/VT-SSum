{
    "id": "sp24f2lcsyd7kf2v446amvydkghylrnd",
    "title": "Bounding Excess Risk in Machine Learning",
    "info": {
        "author": [
            "Vladimir Koltchinskii, School of Mathematics, Georgia Institute of Technology"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss09us_koltchinskii_berml/",
    "segmentation": [
        [
            "And we are very happy to have Vladimir culture in ski from Georgia Tech.",
            "Who will talk about risk bounds and machine learning?",
            "Lower the floor.",
            "Just a second.",
            "Is it working?",
            "15 minutes please.",
            "OK, thank you very much and I will be talking about.",
            "As part of I said it will be able to access against Bones in machine learning, so it will be.",
            "Probably a little bit more and more probabilistic tutorials, and then we will be talking about mostly about some some probabilistic inequality, so it will be much more probabilistic than than most of us are struggles that you heard it before.",
            "So what I will be?"
        ],
        [
            "We're doing.",
            "I will start.",
            "It will take probably me one hour to go through some basic properties of empirical process is mostly these are certain inequalities that describes the size of empirical process and this is the main tool used when you are trying to bound the generalization error is the accessory skin machine learning OK, and in you know in kind of standard model.",
            "Of learning.",
            "And then probably after this it will be a time for a break and then after after the break I will talk about distribution independent bounds on excess risk.",
            "So it will be a senchal a certain technologies that allows to find such bones at a reasonably tight.",
            "It will wait very general technology.",
            "And then.",
            "There will be probably another break and then the last two topics about data dependent bounds on excess risk and Oracle inequality is in model in model selection problems.",
            "Also in a rather abstract framework of empirical risk minimization.",
            "This is what we will do, you know, closer to then.",
            "OK so."
        ],
        [
            "You know the problem.",
            "We are going to deal with is sort of abstract, abstract version of empirical risk minimization problem.",
            "So we will assume that that there is certain data X1, XN that consist of independent, identically distributed points sampled in some Space S, and there's a underlying distribution of the sample.",
            "I don't know what it be OK, so we have this type of data and then there is a class of functions defined on S, mostly for technical reason I will assume that this functions are that takes values in 01K, so it will be uniformly bounded class and this allows to write nice exponential bones.",
            "OK, so one can do something in the case when classes are unbounded and this is very important, but.",
            "You know many kind of the the final picture.",
            "It is clear in the in the in the case of bone that classes and I will just stick to this case here without talking about the situation.",
            "Once the classes are unbounded now the function sends a class in machine learning framework.",
            "They would represent losses of certain decision rules, for instance, OK all it can be a loss of a classifier, say OK or losses associated with certain actions, say OK. Can be a loss of a classifier, so usually you know points in this Space S will help structure.",
            "So in fact it will be about what I want to emphasize is this structure will be will be irrelevant for us, at least for a while.",
            "OK, so say point space S. In fact it will be in in many applications it will be not as but something like as cross minus one one.",
            "If we are talking about binary classification.",
            "OK, and then function F on the space.",
            "Say.",
            "Function F on this space can be.",
            "It can be just, you know, it will depend now on X&Y and it will be just just.",
            "There's a sink, so it will be $1 if we misclassify certain if classifier e.g misclassified certain example and $0.00 if if it.",
            "If it classifieds correctly.",
            "OK so this is what we are paying for misclassification, for instance.",
            "Or if you if you want to think of something like boosting, it might be something like this, OK?",
            "So there will be some real valued classifier, say and it will be exponential loss associated with this classifier.",
            "Or if you're you're thinking about.",
            "Delta regression, it can be just something like this.",
            "Let me put here G. OK, so this is FK this other function F. So it will represent loss.",
            "OK but for a while at least it will be completely irrelevant for me that the space has the structure so I will just denote it S and will use this as functions defined on an abstract space S and in this space I will be sampling points.",
            "OK, so this mode is happening.",
            "No.",
            "Based on this data, one can construct what is called empirical distribution, which is a natural natural estimate of the true distribution of our data P. So it just does a average of N point masses located at the points of the data.",
            "I will get your friend Iraq measures OK. OK, that's the point of the data and."
        ],
        [
            "Then if we start integrating losses with respect to unknown distribution, if we integrate loss with respect to unknown distribution, then we are we are.",
            "We are computing risk right?",
            "It will be risk associated with various sort and.",
            "Action, for instance, but this is.",
            "This is in the background, right?",
            "So we don't care about this, we're just leveraging loss and getting risk right?",
            "Getting risk and then.",
            "If we if we ever Julie Sconza data right so integrate average loss on the data, integrate loss function with respect to empirical distribution where computing empirical risk right there, computing empirical risk and then we are ending up with two problems that we want to compare.",
            "We in principle what we want to do.",
            "We want to minimize the true risk, right?",
            "But we don't have access to the true risk we are replacing.",
            "The truly is by.",
            "By the empirical risk and minimizing instead empirical risk.",
            "And this is a solution that we are getting offended scared.",
            "And then.",
            "The quantities that is of interest in this case that characterizes how well we are doing is accessing disk OK.",
            "The access code we want to be.",
            "We are replacing one optimization problem by another optimization problem.",
            "The ones that we.",
            "You know, I'm not really sold because we don't have access to the true risk we are replacing.",
            "Bye bye bye and approximation.",
            "And then the question is how well we are approximating and for a particular solution what we should look at?",
            "We should look at the difference between the the risk of this particular solution and minimal possible risk.",
            "And this is what is called the accessories.",
            "OK in this problem, so PF minus.",
            "BFF's in East Golf Course you know of F minus the.",
            "Smallest possible risk in this problem, right is what will be called the accessories OK in in, in, in, in in, Bayesian statistics is a similar quantity, usually is called base regret.",
            "OK, it's just just another terminology.",
            "OK, so essentially what we are going to be interested in.",
            "We are going to be interested in controlling with high probability the accessory risk by the accessories comes empirical risk minimizer, right?",
            "So the accessories come this, yeah?",
            "Talk about the empirical risk or you actually want the actor in the last line.",
            "This is.",
            "No, I won't be off if I want I want I want I want to know when I plug in this thing here, right?",
            "I want to know how small this quantities, right?",
            "How close I am coming to the minimum, right?",
            "This is what I want to achieve, right?",
            "This is what I want to know right?",
            "Basically right, I don't want to.",
            "Well I want to know this, but mostly I want to know where this thing is minimized, right?",
            "For which?",
            "After the sync is minimized, right?",
            "This is what I want to know and I want to know a function that that comes close right to this minimum right?",
            "And this is a candidate right solution of this problem is a candidate and then I want to stick it here right?",
            "To plug in it here and to know how large or how small this quantities right?",
            "And this will tell me how Khloe and this will tell me how well I'm solving the empirical risk minimization problem, right?",
            "Well, that's why I thought maybe you want empirical risk for your first term.",
            "But maybe you want to know why?",
            "Why would I want this?",
            "The difference was between your empirical risk and the best you could do.",
            "No, because because because because we know no becausw becausw at the end, I don't care about empirical risk at all right?",
            "Why should we care about empirical risk?",
            "Empirical risk, you know, is something that can become small, but it means nothing.",
            "For instance, you know.",
            "Our goal, a couple of examples that Rob Schapire you know talked about last time.",
            "You know there can be a message that that that drives empirical risk 0, but it does not mean anything, right?",
            "So we don't care about comparing empirical risk of F with this information, right?",
            "What we really care is is to know how large or how small the accessories great defined by this lion is for.",
            "For a friend with her you know.",
            "For instance, if in the context of we will maybe arrive alright, just a little bit later, but in the context, for instance, of regression problems, right?",
            "The accessory risk with auto loss, right?",
            "This quadratic loss accessories becomes as the square of the L2 distance right between the.",
            "Between our estimates and the progression function.",
            "OK, so this is what we care about.",
            "OK, so."
        ],
        [
            "Now again, what will be the problems?",
            "OK, precisely the problems we will be talking about.",
            "First its development of certain techniques that would allow us to construct upper bones upper bounds on the excess risk of empirical risk minimizer.",
            "OK, so we want to know upper bounds on excess risk of empirical risk minimizer, and we will will care about 2.",
            "Types of our bones.",
            "First distribution independent upper bounds.",
            "OK, that will will be somehow related to the unknown distribution and also to some geometric properties of the class and some complexity of the class F over which we are minimizing the empirical risk.",
            "OK, so this will be one question.",
            "Another question is we also care about about empirical versions of this bound.",
            "So we want to have.",
            "Copper confidence, bones and accessories that depend on the data so that using the data we can say how close, how well we are solving the problem and this is needed in particular in model selection problems in model selection problems there are many situations of course in which in which empirical risk minimization is is an ingredient in our problem, right?",
            "So it's always almost always and most of the learning problems.",
            "It's an important ingredient of certain learning algorithms, but.",
            "Maybe not the cold, Grissom Gaser.",
            "Maybe situations in which we have very huge.",
            "A glass of functions, for instance losses associated with some huge neural network.",
            "OK or huge family of neural networks OK, and this is can be very large class but in fact.",
            "It would be useless to minimize empirical risk over the whole class if the class is huge, it will lead to overfitting.",
            "OK will lead to overfitting and what is done instead.",
            "Instead you're looking at some.",
            "Subclasses of this plus some, maybe maybe families of functions of varying complexity neural networks, whereas you know different numbers of neutrons.",
            "For instance, different numbers of layers.",
            "Things like this, right?",
            "We have these subfamilies and then you are.",
            "You are solving empirical risk minimization on on each class FK over there some family right?",
            "And then you want to find the best of the solutions, right?",
            "This is a model selection problem, right?",
            "And model selection problem in this context.",
            "So you have many empirical risk minimization problems.",
            "And then you want to find based on the data.",
            "Find caves had such that they access risk of what you are of.",
            "What you found will be as close as possible to the minimal in this problem, right?",
            "There's a minimum of the access risks of.",
            "You know for all these bunch of pink coleus minimization problems this model selection problems and in in this framework it's.",
            "Really, very important to have some tight enough data dependent upper bounds on the accessories, because this helps you to make to make the selection of model.",
            "In this problem."
        ],
        [
            "OK.",
            "So let me start imagining that that that that you know nothing about empirical risk minimization or had nothing to do not.",
            "No.",
            "Nothing about Vapnik and Sherman in kiss, just nothing.",
            "OK, how would you approach?",
            "Well, this problem of bondings accessories.",
            "OK there is.",
            "You know the following very trivial computations at anybody who would spend several minutes on this question would inevitably do OK.",
            "So you want to control this quantity.",
            "This quantity is the difference between the risk of offend with head and the infamous right at the infamous.",
            "Imagine you know it's it's not important.",
            "But imagine that the infamous attained somewhere OK and this is this function F's bar so we can write P. Of a friend with Scott minus PP office bar.",
            "OK, so then you owe.",
            "It is natural to do is to replace key RP by PN right and then to compensate for for this replacement actually right?",
            "So we did this and then the first term here right?",
            "This term is less of equals in zero right?",
            "It's clear be cause FN Wiskott is aware the minimum of the empirical risk is attained, right?",
            "There is a minimum is a day, and so these these should be smaller than this right?",
            "Lesser equals and this happens.",
            "Bar is also in our class, so there's different should be negative.",
            "So in the upper bound we can drop it, right?",
            "We can drop it and then we are upper bounding this by this quantity right?",
            "By this quantity.",
            "So we're looking at the difference between P&P.",
            "We are computing this difference at the difference of functions F&G right FG and we are sleeping.",
            "Overall functions FNG in our Class F right in our class at this is less or equals and twice they supremums.",
            "The difference between P&P and NP right BNP NF and PM.",
            "So the difference between between average and expectation right we are shipping this differences over Ochoa losses in our class.",
            "All functions in our class and as a result we are getting this upper bound and.",
            "In many reasonable situations, this upper bound will be over the water.",
            "Oh capital of into the power minus 1/2 OK and this is big cause because of the central limit theorem, right?",
            "For instance, if you take F justifying it class of functions, right?",
            "It's definitely will be of the order and to the power minus 1/2.",
            "Just 'cause this is, this is the case for a single function, right?",
            "The difference between between average the sample mean and the population mean right?",
            "And the expectation right is.",
            "By the central Limit Theorem is order 1 / sqrt N right over 1 / sqrt N and if your class F is not too large and what does it mean to large is described very well in the theory of empirical processes.",
            "It has been understood very well because this was the basic problem, right?",
            "If it's not too large in the sense that the central limit theorem still holds on the whole class in some uniform sense, right in some uniform sentence is what is usually called.",
            "Dark loss, right?",
            "And in particular, it can be, for instance, such things as well.",
            "Picture one and kiss glasses.",
            "OK?",
            "Is IRP dansko OK in this case?",
            "You will cover this upper bound on this phone accessories gate.",
            "This is not a very good upper bone so right, and if you think for for several seconds you will tell me in which of these lines are several lion cereals are can be a big gap actually where we are losing key again this inequality as well.",
            "Where is the place where we are doing not well.",
            "In which way?",
            "Sorry.",
            "He has this is the police but but but this is not really OK to use the truth.",
            "It's not the main problem with this bull you know, OK, so now now since I told you that this is not the real cause of trouble, you know it's easy to identify the lion right?",
            "Cause in the first line we have equality, right?",
            "So we can't lose anything here, right?",
            "So it can be only only here when we go from this line.",
            "From this slide into this line.",
            "So when we are controlling this sink right by this supermom, it's not real.",
            "Is that important that here we are?",
            "We are replacing something negative might be important sometimes, but here we are replacing something negative by by zero right?",
            "And also losing a little bit.",
            "But the main source of trouble is is actually here right?",
            "When we go from this line to this line and this is where we are, where we are losing and the reason is very simple.",
            "OK, the games on this work is simple and let me explain it now, but then we will get back to this after awhile.",
            "OK, because this is a way also to understand this is also the way to improve the bound.",
            "OK the reason is simple.",
            "Becausw as soon as we went here right?",
            "We already know that the access risk of offend with hat is smaller than N to the power minus 1/2, right?",
            "OK is order enters the power minus 1/2 say right.",
            "We know this so we know that it's more right.",
            "Accessories of F as head of Miss Bar right of 1st Bar.",
            "By its very definition is 0 right?",
            "Because at this bar is where the minimum of the objectives that ain't right.",
            "So now we have two functions for which they access.",
            "Risk is small, right?",
            "For which the accessories gets more, which means that in this supremum right in this super album I don't have to use actual as a whole class.",
            "It would be enough for me to soup only over a part of the class that consists of functions with small enough accessories.",
            "And I'm still getting a legitimate bone, right?",
            "As a class of functions wags accessing skills small right words.",
            "Access English.",
            "Yes, Moe can be much smaller than the whole class of functions, right?",
            "Can be much smaller than the whole class of functions.",
            "And because of this, for instance, in the case of donsker classes always you will get not this, but at least a little bit better.",
            "You will get all small offensive power minus 1/2 right in a typical situation for a dance class.",
            "And we will see this.",
            "And if you know more about your class and the fact that the classes don't care if you know some, set up some better control of complexity of the class, then you can get bounds that will will be as good as end to the power minus one.",
            "For instance, OK.",
            "Here OK so we will get much better grades that will be in statistical sense and so it's sort of a mini Mac.",
            "Statistical sense would will, you know would be optimal bones?",
            "OK, but this should be based on a little bit more subtle analysis.",
            "OK then, as I said, what we are doing here and we will get through this.",
            "It's very very simple also.",
            "But we will get through this a little later.",
            "But anyway, in order to do this in order to do this what we will have to do, inevitably we will have to understand how control.",
            "So the size of such superhuman, right?",
            "How we depends on the complexity of the class and how to know what the size of the supremum is, right this apartment?",
            "Because even if we do this computation in a more subtle way and replace F by a part of F, still will be left with the problem.",
            "How to control the same type of quantity on a part of F right locali say and still we have to know certain tools that would allow us to do this.",
            "So because of this first we will stick.",
            "Just adjust to this quantity right?",
            "And try to.",
            "The stand.",
            "What are reasonable ways to control the size of this quantity?",
            "OK, the size of this quantity.",
            "OK, so."
        ],
        [
            "So what we will be talking about is the subject of what is called empirical processes.",
            "Cearley and this part of empirical processes series.",
            "At that we will be talking about was it has been developed.",
            "You know, for for now it's for 40 years.",
            "OK, many people were involved in this, but it's probably important to emphasize the contributions of Mapnik.",
            "Insure one Kiss feels that were really 40 years ago.",
            "Start it over updating first.",
            "Long trivial results.",
            "OK on this type of problems in a general framework.",
            "OK then.",
            "Dick Dudley developed in the in the mid 70s developed really nice see of empirical processes that included central limit theorems and things like this.",
            "OK, so he was.",
            "Keeley most of terminologies that we are using now.",
            "You know come from his work you know he he was the one who who started talking about.",
            "We see classes and things like this OK it's all started in his work and then.",
            "Another important name is really important.",
            "Name is Telegram Cabie cause most of the important structural results that we are using now were developed by Telegram in the in the 90s.",
            "OK and we will talk about some of them, but some of them now.",
            "Basically there is also will be talking about.",
            "It's a mixture of something something where you simple case there are certain inequalities that are very simple.",
            "So simple that I will be able to show them here on the board.",
            "OK, and there are some that are much much deeper and much more more sophisticated.",
            "OK, and but but still I think it will be possible to understand the meaning of this inequality is OK, but I would without discussing the proofs.",
            "OK, and.",
            "I already said what is written here, so I'm not going to get back to this, so it was already discussed."
        ],
        [
            "OK, so now what is our purpose for the next?",
            "I don't know half an hour or a little more.",
            "So we will be looking at this object.",
            "The difference between between the the average PNF is the average of our function at end points, right at 10 data points you adjust the sample mean right and the corresponding expectation.",
            "So it's just we have many sample means, right?",
            "And they're expectations right?",
            "And we want to compare them.",
            "This is what is called empirical process.",
            "Usually it's normalized normalized by square root of N. Right, it's multiplied by square root of N, but it won't be important for me, so I'm not going to normalize it, OK?",
            "Then the basic question we we want to look at is what is the size of this supermom right?",
            "Obviously problem so we have again.",
            "A large, maybe infinite, number of sample means and how they deviate in simulataneously right in a uniform sense from their expectations, which is actually, you know, maybe maybe one of the most basic questions in statistics if you start thinking about this right, at least in statistics of IID data, right of IID data?",
            "And, well, maybe maybe one of the most basic questions and empirical Sciences.",
            "If you want.",
            "OK, because when we have repeated measurements, we always, you know if we.",
            "If we are, if we are looking at some completely complex enough problems, we are often running in this type of difficulties.",
            "We have to know the size of this quantity.",
            "Then so basically what we want to know how this quantity depends on certain measures of complexity of the class, how to introduce nature, all matters of complexities that would allow us to control this quantity to control this quantity and also how to construct data dependent bounds or on this quantities that will be of correct size at least up to constants of correct size.",
            "OK, so."
        ],
        [
            "Who?",
            "An important object to look at when when one deals with this type of processes is another stochastic process called rademeyer process.",
            "Rather, my hair process and is defined here and essentially what is written here process.",
            "It's also an average of the values of the functions at data points right there computing average, but this time we are.",
            "We are putting random science.",
            "Does this sum right?",
            "We are we are flipping coin right independently of the date, of course, right?",
            "Obtaining a sequence of plus and minus ones and putting science at random in this sum, right?",
            "Randomizing some in this way, right?",
            "So just introducing additional randomization in the sun?",
            "This removes this process.",
            "This quantity is super normal.",
            "This process is something that often is called Rademacher complexity, and it's a global version of random here complexity, because in in the problems of controlling the size of accessories we often have to localize order my hair complexity, and there are also things called local market complexities.",
            "This is a global version.",
            "So here's a very simple statistical meaning of async right, and it's very easy to understand at heuristic level why.",
            "Why is this thing might be might be a reasonable measure of complexity in the cases when you, for instance, are looking at some some large function class and are trying to use it for certain purposes.",
            "For instance to feed to feed regression model or something like this, right?",
            "And.",
            "Uh, and you want to know whether this class is not too large so that you know you're not going to have our feet using this bunch of functions, right?",
            "There's a bunch of functions.",
            "So if you look at this expression, then basically what it is, it's something like empirical correlation, coefficient sample, correlation coefficient between the values of the function F right?",
            "We have function F. We are computing this function at data points, right?",
            "And then we have a random noise plus and minus one with probability 1/2 of them.",
            "I hear noise.",
            "If you want right then what we are computing when you're computing the server that you are computing, sort of a sample correlation coefficient between the values of the function at.",
            "Points right and random noise, right random noise now.",
            "You started souping device correlation coefficient over your class.",
            "Right or your class of function and this even quantities that you are getting right is large.",
            "It would mean that.",
            "For this random noise right?",
            "For this random noise at you, for instance, simulated on your computer, right simulated on your computer, you can always find a function in your class.",
            "Existing is large, it means that there is a function in this class that will set the values of this function and data points are highly correlated, right, highly correlated with the noise.",
            "Right, if we have such a clause, that is that for for such noise, like or, they might hear noise just for this, you know, flipping coins and writing plus and minus one, you can always find a function that is, we have high correlation with this.",
            "So you feed the noise well, right?",
            "This is a good sign that we are going to have our feet.",
            "If you use this class.",
            "If you use this class.",
            "So this is one of the reasons why.",
            "This quantities is a natural measure of complexity in many problems.",
            "Of course one has to sync.",
            "What does it mean large and what does it mean?",
            "Small here, right?",
            "If for instance, instead of FI plug in a constant.",
            "If I take F equal to 1 right?",
            "I'm getting just one over and some of epsilon ice.",
            "What is the nature of size of the sink?",
            "One over and some of epsilon nice.",
            "What is the natural size of six is it?",
            "Is it?",
            "And one over N 1 / sqrt N or N ^3.",
            "The choice is yours.",
            "Wanna work square root of all right?",
            "So by the central limit theorem, for instance right?",
            "If you.",
            "If you look if you look at average of IID random variables with like this, right?",
            "Just just plus and minus one with probability 1/2.",
            "If you computed variance, it will be precisely one over again, so the standard deviation is 1 / sqrt N right one over square root of it.",
            "So 1 / sqrt N is not something large in this context, right?",
            "On the other hand.",
            "See if.",
            "If, for instance, you can find in your class of function F right side just F of XI is just equal to epsilon I.",
            "Right, if you have leaks, then there's this super mobile become as like just one right?",
            "We will get one for this function, right?",
            "So the supremum will be at least one in this case, right?",
            "This is already large, so roughly not very large means something on the order of 1 / sqrt N this is good enough, right?",
            "This is good enough and large is something of the order over constant.",
            "Say great, this is already wagging like short too like Shaggy because for example this.",
            "OK so this is just Java meaning zoo meaning of this quantity knows real is underway.",
            "Just wanted."
        ],
        [
            "Use of important is because of certain inequalities that can be proved.",
            "As it relates to the size of the empirical process, ascential is the accuracy of approximation of the true distribution by the empirical distribution on the on the class F to the size of Rademacher Process, and this is very nice.",
            "So very simple inequality is that tells that essentially up to up to constant 2 1/2.",
            "Right on average, these two things are the same.",
            "Yeah.",
            "So.",
            "Sweetheart, ball or continuous?",
            "Random variable like is.",
            "It doesn't make sense to do the same thing with something that abortion noises.",
            "Would this thing makes sense with Gaussian noise instead of Rademacher noises?",
            "York way, yes.",
            "Yeah it makes.",
            "It makes perfect sense.",
            "You can replace the randomization by Gaussian randomization.",
            "This things will be about the same order actually, you know, since you mentioned this, I will.",
            "I will, I promise you, I will use it today.",
            "OK I will.",
            "I will replace Gaussian by randomizing show you how to do it OK later today.",
            "So.",
            "You know, so it's only imperfection in this inequality is that in the upper bound I put F in the lower bound.",
            "I put center Class FC and this is needed in the lower bound and upper bound.",
            "But you know what is trivial, almost very easy to see that in the upper bound you can.",
            "I can also write FC and this is just gives me, you know, kind of a back from bound right will be 2 sided inequality with we just adjust the trolls that these two things are actually same up to factor or factor two 1/2.",
            "OK, up to factor two 1/2.",
            "I am not writing here.",
            "If she just 'cause this is what we need.",
            "OK but both are true.",
            "OK, in the lower bound FC is needed.",
            "KFC is need.",
            "This inequality is actually.",
            "So easy to prove that I can't, you know, always when I talk about these things and have time.",
            "I never never.",
            "Can you know just the temptation to prove it?",
            "OK, because it's just very, very simple.",
            "And on the other hand it's it's very simple.",
            "You know, after it has been understood.",
            "Be cause I still remember time when I when I was, say a graduate student, I still remember Diamons are where the whole conference is dedicated to this type of symmetry.",
            "Caitians with random uh-huh random variables.",
            "It's actually a trick that has been used for long for a long while.",
            "You know to to replace in sums like this just to multiply by randomly chosen science.",
            "OK, it's something that that was that was equally used in.",
            "In probability, see you again in harmonic analysis for a number of years.",
            "Actually, you know.",
            "Or or the market is the name of the microwave.",
            "Gables comes from her functions that are used in in in analysis to construct Walsh basis.",
            "For instance right.",
            "And this is very nice family or so normal functions that can be defined explicitly on the interval 01 right on the interval 01 and that become just IID random variables taking values plus and minus one with probability 1/2.",
            "If you interval 01 was laid back measure as.",
            "Probably just space.",
            "OK, so it comes from there and it comes from harmonic analysis, actualizar use of this type of things and there was in the mid 60s.",
            "There was a book of Kahan on random functional see I guess where he already you know extensively used this technique.",
            "Actually you know for various purposes, OK for analyzing the sizes of certain sums of of stochastic processes say OK, so let me let me show you the proof of this.",
            "Only only holds upper bound, which is which is very simple.",
            "So.",
            "What we have to do?",
            "We have to create.",
            "Another bunch.",
            "Of random variables that are just, you know we want to create this guy, which will be a copy independent copy.",
            "Box 1 Excel so it's convenient to sync over this as just a bunch of random variables that are probabilistically the same, but they're independent.",
            "They, for instance are defying somewhere on the on the Mars.",
            "OK, so X1 XN are on the Earth 6 one we still the extent we still there are probabilistically the same, but they are on the market and they just don't know each other.",
            "OK, so they are independent.",
            "OK then PN with steel there will be empirical distribution based on these guys with steel duh.",
            "OK based on this guys were still there then or what we want to know.",
            "We want to know the expectation of the supremum.",
            "We want to know the expectation of the supremum, sorry.",
            "We want to know this right?",
            "This is what we want to know, right?",
            "But it's clear that if I, you know, look at P and we still do off F and applies this expectation, which is the expectation on the Mars.",
            "OK, I will get here right?",
            "'cause those guys were still have the same distribution rights also have distribution be, so the expectation will be the expectation of their average right which began with steel days.",
            "Will be just adjust BF right?",
            "So it means that I can replace this PDF, write and write write it this way.",
            "And put here expectation.",
            "Of.",
            "P and we still deaf right now for this expectation, right?",
            "For this expectation?",
            "This thing this is on there's right.",
            "This is on the on the marks OK, this is on there's this thing, for this expectation is just a constant right?",
            "It's like a constant, so it means that I can take this expectation and move it.",
            "You know just write it here, OK?",
            "This expectation is here and it applies to the difference between P&F&P and we still do off F, right?",
            "I can do this because expectation of constant is just this constant itself, right?",
            "So just just using the properties of expectations here OK?",
            "And then there's this expectation was still days inside the Super Moon, but I can put it outside.",
            "I am only increasing then they the size of the expectation, right?",
            "So we will get an equality.",
            "So I will write here less or equal and put this expectation old site.",
            "Somewhere here see.",
            "OK, all this application is here now.",
            "I don't have this.",
            "If we still hear what I have inside, let me write it OK.",
            "So I can use.",
            "Two expectations, and then I will have here Supreme over my class and I will have to let me write.",
            "Just explicitly what I have.",
            "Hey, basically you know this is average of function F at data points on the Earth, right?",
            "Another one is the average on the marks right?",
            "The average on the Mars, so I can.",
            "I can write them together.",
            "L'eglise can ride some legs.",
            "Now what I have here this call sync right is just certain function.",
            "That depends.",
            "Onto an independent identically distributed random variables right and the main property of independent identically distributed random variables is that they are exchangeable.",
            "Boat so if you if you switch this guy's right but I'm use them in an arbitrary way, you're not changing the distribution right?",
            "If you're not changing the distribution, we are not changing the expectation.",
            "Also right?",
            "Which means that I can now play this game.",
            "I can take one of those guys, for instance.",
            "You know whatever X1 say and send it to the errors and this guy will go to the marks right?",
            "And this is equivalent to.",
            "Putting minus right somewhere in this right.",
            "I'm just going to change change scion of one of these terms, right?",
            "If I'm doing this and what I am changing the sign I'm doing just permutation of my random variable.",
            "So I'm not changing expectations, right?",
            "It means that I can put in this sum in an arbitrary away plus and pluses and minuses right?",
            "And I'm not changing expectation right for any combination of pluses and minuses.",
            "The expectation will be the same right?",
            "Then?",
            "Of course it means that I can put random pluses and minuses with, you know justice right there.",
            "Paradise, right?",
            "This are random pluses and minuses and take expectation was expected.",
            "Them I'm not changing anything by doing season OK so this is what I am doing.",
            "I am putting here Epsilon Dre and add one more expectation.",
            "This average with respect to epsilons, several spectral epsilons and then what remains is just to split this sum into two right one.",
            "On the address and another on the marks right and bound supremum of some of these two things by some of their suprema, right?",
            "And you will get expectation of each Suprema, right?",
            "Some of the expectations of the Supreme and this expectations are the same because the guys have the same distribution and you are getting twice expectation observed on my hair process, right getting twice expectation observed on my airport, so it's a very very simple proof, so there's nothing complicated here, but it's sort of a smart.",
            "Move.",
            "Those are the logs to get this border.",
            "It's a simple exercise to reduce this in the case of the lower bound.",
            "OK, so this is a similar argument that leads to the lower bound here.",
            "OK, OK, so this is 1 inequality is that that is very important and I will explain."
        ],
        [
            "A little late or why it is important gay then The thing is another inequality, it's.",
            "It's a little more complicated to prove this would require more work.",
            "I'm not going to do this, it's all called contraction inequality due to telegram.",
            "And this is very nice property of over there.",
            "Michael processes essentially tells us the following is that if you have a certain function class, but then you want transforms its functions do a non linear transformation of this functions right and apply the same function Phi right to all functions in our in your class, right?",
            "So we are getting a transform class like this right?",
            "And you want to control the size of the.",
            "So my hair process, right?",
            "The expectation of this supremum for this class OK, or this class then provided that this this function is is a contraction is Lipschitz function with constant one, you just just have you know whenever you have this function, you can just drop it and write the constant.",
            "Right multiply by right.",
            "This was only thing you are losing right?",
            "So it's controlled by the Democratic Process Index by F right index by F and.",
            "That's all that you need, right?",
            "In the case, if of course you know by by simple or scaling right it follows immediately.",
            "Is that if you have a function five with Lipschitz constant daily, you will have to multiply here by this Lipschitz constant.",
            "OK, this is only difference that will happen here.",
            "So for instance, if you.",
            "You know, recalls that in applications right in applications functions F will have some structure.",
            "They are losses, right?",
            "For instance, it can be F can be why minus G square, right?",
            "In the case of regression y -- G of X squared this is your F, right?",
            "This URF in this case when you're controlling the size of random ahead process for such for such a class, right?",
            "For such a class you want to drop the square.",
            "Right, we want to get rid of the square can be done provided that the range of your functions is bounded.",
            "In this case, square is going to be Lipschitz right?",
            "And then you can do it right?",
            "If the range is unbounded, it's impossible to do it this way.",
            "OK, it will be impossible to do it this way, so this is a very convenient technical tool in many cases and use the log."
        ],
        [
            "No OK. What I will do now, I will try to show you.",
            "Several.",
            "Several bones on the.",
            "Size of Rademacher processes.",
            "At the same time these are bones on the size of empirical process by size.",
            "At the moment I mean just expectation right of the Super norm.",
            "I will be looking at expectation of the supplier.",
            "Will discuss a little later what to do, what to do if.",
            "If you want to.",
            "Oh no, how?",
            "This random variable itself, Supreme with empirical or under my care process, how it's related to its expectation.",
            "OK, we will look at this a little bit later.",
            "OK, but at the moment we will care about expectation only.",
            "So several bones on the expectation of the Supreme observed by her process.",
            "So we will use the following notations, right?",
            "Sigma Square roughly will be the maximal valiance right or function in the class, right?",
            "I'm not centering it, it's this function is not important here, so it's just the maximum of the of the square observed to norm.",
            "So Sigma is the maximal L2 norm of the function in the class, right?",
            "So this is what you owe me.",
            "Annals of quantities that is of interest here.",
            "Was it characterizes some towels a size of the class right.",
            "The size of the functions of class, so we will need another quantities.",
            "This will be as a as like just super norm right of the functions in the class.",
            "So you is is a constant that uniformly bounds all functions in our class, right?",
            "There's another characteristic that we will use then sometimes since we will care a lot about classes that are small.",
            "Actually, our classes of small functions sometimes it's beneficial to have better control of functions and this control by you and this is done by by by something called envelope of the functions of the class.",
            "So this is just one single function right?",
            "One single functions at bounds.",
            "All the functions in the class.",
            "OK, if we are considering functions that are uniformly bounded, so this will be dominated by you, but.",
            "In some cases it can be as a nice examples in which in which it's important to take into account.",
            "This envelopes also OK, so those are characteristics that we will use and then we will use in each example different ways to characterize complexity of the class itself.",
            "There will be several ways I will look at and I will show you several bones.",
            "OK, here is."
        ],
        [
            "One of the simplest bones.",
            "One of the simplest bones.",
            "This isn't the case when our class is finite, right?",
            "Our class is finite and consist of capital and functions OK. What is going to happen in this case?",
            "In this case, as a expectation is controlled is controlled by two by the maximum of two terms right by the maximum of two terms and the first term depends on Sigma on the maximal variance of function in the class.",
            "If you want right and it's you know the Sigma is divided by square root of N. It's natural in this situation right?",
            "Or the microprocessors average right?",
            "So we should be obvious.",
            "Size 1 / sqrt N approximately right.",
            "About who was respectable just to the sample size, and then it's multiplied by square root of the logarithm of the cardinality of the class, right?",
            "Absolutely some of the cardinality of the class.",
            "Well, let me ask you one question.",
            "So if G1G NRIAD normal random variable was mean zero and variance Sigma Square, what is the size of this expectation?",
            "How it depends on on Sigma and on the number of random variables kept on capital N. Mask.",
            "Sorry.",
            "Sqrt 2 log in right?",
            "OK thank you.",
            "OK so it will be roughly like this.",
            "You know, I don't care about constant so I will write just six.",
            "OK, so to be of the order.",
            "Sigma Times Square root of the logarithm of capital M. It's a simple probability exercise to prove this.",
            "So if you have not done this yet at the right time, you know during the break to do it OK.",
            "So basically, if you're Sturm here and you will see this pattern in other bones that I will show you is the term that is.",
            "It is similar to some kind of Gaussian behavior.",
            "OK, it's it's A kind of comes from certain Gaussian tails, right?",
            "Gaussian tails of our random variables, right so?",
            "If we had only the 1st right, it will tell us that that that our buncher for adima here sums right by Jeffrey Dahmer here some.",
            "So we are looking at this order.",
            "My hair process right?",
            "Right, Bunch right behaves approximately as as ID as the maximum of iid Gaussian random variables.",
            "Right capital N of them right with with variant Sigma divided by square root of N right Sigma divided by square.",
            "This was the first term is about right now.",
            "Of which random variables I should do?",
            "Another question which random variables I should use here instead of Gaussian in order to have here, you know instead of these two have here, which will be something of the order logarithm of N. What kind of random variable should be used here?",
            "Yeah, something something like exponential random variables, right?",
            "So if you if you use exponential random variables right with exponential tails, high Square is.",
            "Alright, for from this point of view, if you use exponential random variables, right exponential random variables with exponential tails right?",
            "So so the tails decay not as it was a power minus minus X square, but as each of the power minus X right?",
            "You would end up here with with Logan is right, it will say the expectation will grow as logarithm, not as square root of logarithm, right?",
            "So the second term comes from some.",
            "Sub exponential behavior of our random variables right?",
            "And this this combination of sub Gaussian and sub conventional type of behavior is typical when we talk about sums of independent random variables who does not know Bernstein's inequality in probability?",
            "See again.",
            "Who does not know?",
            "K who knows?",
            "But in science, inequality in probability, CA.",
            "Who does not know whether you know or not?",
            "OK, anyway I'm not.",
            "I'm not going to talk about Bernstein's inequality, but I will talk about about telegrams inequality a little later.",
            "I will have it OK, which is which is a generalization of Bernstein's inequality.",
            "So we will learn this way about birthday and equality as well, But basically this type of behavior is that we see here comes from Bernstein's inequality in a way, right?",
            "And Bernstein's inequality science in equality controls.",
            "This sum of independent random variables with mean zero and finite variance and they are bounded, say write it controls, controls it by two exponents, right?",
            "And depending you're looking at probabilities that this sum is bigger than X, say right and depending on whether X is relatively small or relatively large, you will have in the first case when it's relatively well, will help sub Gaussian type of behavior and effects is relatively large.",
            "You have to switch to sub exponential type of behavior both both.",
            "Sales are sharp actual.",
            "Then you can construct examples that show that this type of inequality sarshar right.",
            "If you translate this in the in the language of expectations, right?",
            "How expectations of of the maximum of many random variables of these types that have the structure of the sum of independent random variables.",
            "This is what we are dealing with here.",
            "If you translate it into the language of expectations, you will end up with these two terms right.",
            "One term is Coco responds to something Gaussian, another something exponential.",
            "OK, something exponential.",
            "OK so this is 1."
        ],
        [
            "Or one inequality is on another inequality.",
            "It's it's something more interesting and related to the old work of Applicant Chairman, Yankees.",
            "Is this is the example in which is a class F consist of of binary functions, right functions that takes value zero and one something that is of importance in classification?",
            "Of course, right in classification problems?",
            "And in this case what wapnick insured when you suggested they suggest that there will be a natural way to control the complexity of such classes of functions.",
            "So what they do they basically.",
            "Project of this class of functions project this class of binary functions into.",
            "The entire company Toryal Cube.",
            "Right, So what will this guy ski right?",
            "The components of this vector?",
            "Here we are computing the values of the function at points X1, XN, and the components of this vector's components of these vectors are zero and one, so we are practices of combinatorial cuprite of just rightness of 01 to the power and right 01 to survive again of this cube and then.",
            "We are counting how many work this is we encounter right when we when I France or the whole class right.",
            "We are counting how many vertices are there right?",
            "And this is something that is called often shattering number tracking number.",
            "So we are kind of.",
            "If you UFS file functions, afinsa classes, classifiers, you're counting in how many ways your set of classifiers can split the data right?",
            "Can label the data with zeros and ones right, and this is what the cardinality of this class is about, right?",
            "For the cardinality of this class is about and then the expectation of the Supreme observed Emma here process right abdominal process.",
            "In this case it's controlled.",
            "Also there will be a maximum of two terms.",
            "One terms again corresponds to something sub Gaussian and you have here the.",
            "Lucy, which is a largest variance into class right?",
            "So that was a sick muscularis supremum of PF Square.",
            "In this particular case, it will be kind of the largest probability to take one, right?",
            "This is what Sigma Square is, right?",
            "Cause big cause you are functions are essentially indicators, right?",
            "So they take only way Lou Zero and one right?",
            "So when you're computing PF squared, you're computing probabilities that F is equal to 1.",
            "Right, the definite equal to 1, and this is the largest probability here, right?",
            "And then you have expectation of the square root of the logarithm of the shattering number divided by, and this logging some replaces what was logarithm of capital N before right in the case of finite class we had just learning some of capital in here instead of this.",
            "Now we have learning some of the shattering number.",
            "Agree some of the shorter exam button.",
            "Here we have kind of exponential or pop or Sony and type of behavior and we have.",
            "Expectation of the Organism itself instead of of.",
            "The office, for instance.",
            "Proof.",
            "It's immediate right to see that.",
            "That if.",
            "Expectation of the logarithm.",
            "Is small comparing was then great if Expectational doing some small comparing gives ends and this implies that expectation.",
            "Obviously supermom tends to 0 right?",
            "Some idiot employees at expectation of this to promote and Susie, or just use Symmetrization inequality, right?",
            "Reduce empirical processed order marker process and then use this bound right?",
            "Use this bowl and then we will after we discuss a little bit more of tools including concentration inequality is we will see that this.",
            "This also implies this that this will.",
            "Hold with probability one, for instance right one can do this.",
            "Also right that this converges almost surely.",
            "And this was the initial serum of Botnick.",
            "Insure one in case proved 40 years ago with different set of tools back then move in a more complicated ways and what we can do now right?",
            "In fact they show that this is equivalent.",
            "OK, this is the what what nature when kislov large numbers.",
            "If you want OK, actually you know one can over one can do more.",
            "Isn't this one can show that if this is of the order.",
            "For all four, four of between 1/2 and one, then this will be.",
            "Will be observ order.",
            "Of the order or small of fantasy power Alpha minus one, and this is also equivalent.",
            "OK, there's another result that was proved in the 80s by, you know.",
            "I was working on so only stuff so I had I had.",
            "Kind of immersion of these wings was is being sufficient condition and I had a necessary condition that was a little bit worse and this was with some additional logarithm and then Michel Talagrand couple of years later got a clean result that those that you see here actually OK and the proof of the necessary part is very complicated so it says it was very hard.",
            "With our problem to go another way and to show this so to prove this with convergence rate, this type of results with convergence rate it's more complicated than just using this inequality.",
            "It requires some other tool sexually when even for for sufficiency part which is easier.",
            "But anyway this is.",
            "Bones in terms of shattering numbers, that is."
        ],
        [
            "Very useful in classification problems.",
            "OK, let me I have to go."
        ],
        [
            "A little frustration.",
            "Oh OK, now in many cases.",
            "Or what people are using to control the complexity of the class?",
            "Is the notion of metric entropy is a notion of metric entropy and I will show you a couple of results of this nature.",
            "So this number capital N here right is just the epsilon entropy of the epsilon covering number, right?",
            "So epsilon covering number was a class F in the space L 2:00 PM, right?",
            "So it's you know it uses.",
            "Random.",
            "Distance actually write random distances.",
            "Definition of entropy so.",
            "Basically what we are looking at we are looking at the minimal number of balls of radius epsilon in this space that are covering the class right?",
            "And we have certain control on the minimal number of balls, right it's?",
            "It's enough to write it for empirical distribution itself, but it's useless actually to write it in terms of empirical distribution itself cause becausw, since I want to control it almost surely right?",
            "Almost surely I can never control it almost surely, unless I know that I can can actually put outside Supreme over all distributions, right?",
            "I can replace PN by Q and take supremum overall distribution and these are.",
            "So called uniform covering numbers, right?",
            "So you are sleeping.",
            "The entropy in the space L2 Q.",
            "In fact right?",
            "And this is enough to do this, just to have this bound for any Q right?",
            "And then of course you have at 4:00 PM right?",
            "You have it for P and and many things like like you know what nature wankas classes offsets are in this framework.",
            "That's why I'm calling this PC type classes.",
            "Also some extensions of public share one kiss.",
            "Property to the classes of functions like the other various notions of we see some graph classes and things like this OK and things like this.",
            "So basically what is important here is that there is 111 important parameter, this number V which is a picture when kids dimension.",
            "OK, it in in the in the case of market share and kissed losses this becomes an ocean of both nature and anchors dimension right?",
            "And the size of the supremum again is controlled by by two terms right.",
            "One term corresponds to some kind of sub Gaussian behavior and the variance parameter Sigma is involved there, right?",
            "And up to sort an extra logarithmic factor which is sharp actually right it's needed.",
            "Here, right?",
            "It's just the main part.",
            "Is Sigma Times Square root of the dimension divided by N, right?",
            "This is the type of the bound we are getting right as under his series.",
            "Additional term which is observed that one over and right.",
            "It's controlled by the uniform norm of U and this is this corresponds to the sub exponential type of behavior in all these bones is very similar pattern.",
            "OK, when they are done correctly.",
            "OK.",
            "This is."
        ],
        [
            "Election overseas in the case when entropies large much larger than four, about Nick in Sherwin and KISS classes.",
            "So now I'm controlling, not not the covering number.",
            "I'm controlling the logarithm of the covering number, right?",
            "So look very similar, covering number so the covering number becomes very large in this case, right?",
            "Very large in this case.",
            "But still, if this exponent roll right that describes the growth of the entropy right when epsilon tends to 0, right?",
            "How fast?",
            "Pros this parameter or is not supposed to be too large, so we throw is between zero and one then still you have this type of inequalities again of a very similar structure, but of course those are great.",
            "Ski are starting getting different right?",
            "You are not getting any longer one over and in the first term still you have 1 / sqrt N But here you are getting something a little bit different it's.",
            "Related those a, it was the fact that as entropies is like in this case is large.",
            "OK, so"
        ],
        [
            "This is a somewhat different example.",
            "It's something that is.",
            "That is useful in the sea of kernel machines.",
            "OK, in this area of Carlisle machine.",
            "So what we are looking at we're looking at at the symmetric non negative definite kernel semantic network, negatively definite kernel and it defines in a natural way of producing kernel Hilbert space, right?",
            "Reproducing kernel Hilbert space and then what we are interested in.",
            "We are interested in subset.",
            "Hourglass is a subset of observable in this space.",
            "You know for simplicity I'm taking unit bull.",
            "OK, unit ball and then.",
            "Oh oh, if we can, of course, associated with Karen, OK, an integral operator right in the usual way that acts from the space L2PP's our unknown distribution into the space L2P.",
            "Right into itself and then Lambda, J are eigenvalues of this operator and this eigen values depend on the distribution P. Of course right there distribution dependent quantities.",
            "OK, and then what is happening?",
            "Is a desais of the observer Democratic process and this is very sharp.",
            "Bones is a lower bound.",
            "You know that essentially the same order is controlled by this interesting some, in which you're basically truncating eigenvalues, right?",
            "Truncating eigenvalues of the integral operator at Level Sigma Square.",
            "OK, you're gonna have some threshold Sigma squared.",
            "This is a maximal wagons.",
            "The maximal L2 norm of functions in your class, and you're truncating eigenvalues, right?",
            "You have still here and to the power minus one here 1/2, so it will be the order 1 / sqrt N. How precisely depends on Sigma.",
            "Depends on how fast eigenvalues tend to 0.",
            "Right, I'll precisely everything depends on Sigma.",
            "For instance, if you if you look at the case when when your kernel is of finite rank, OK, imagine that you are Colonel is of finite rank Sadie.",
            "Which means that.",
            "First D eigenvalues, right?",
            "First the eigenvalues are one.",
            "Then eigenvalue, subzero right.",
            "Then eigenvalue subzero.",
            "Then what you are going to end up with here you are going to end up with Sigma right?",
            "If you do a quick computation your head you're going to end up with Sigma multiplied divided by square root of N right and multiplied by square root of D. So we will get Sigma Times Square root of the divided by N. Right, so everything will be controlled in terms observer and D of the kernel.",
            "In this case right?",
            "And this bound?",
            "So this is what is happening in finite dimensional situation.",
            "This is something very simple to prove actually.",
            "I could leave it as an exercise to you to prove this bound.",
            "It's very simple.",
            "The idea is just to observe that.",
            "As at this thing, right, the unit ball in the reproducing kernel Hilbert space in the space L2P is what?",
            "What is the unit Bolinger producing kernel Hilbert space about?",
            "When we look at, you know?",
            "Look at this over when our view is the point of view of the object of the Hilbert space L2.",
            "Boards are producing kernel Hilbert space becomes world's balls are.",
            "What kind of sets?",
            "Come on, tell me said squares.",
            "You know rectangles of some guy and ellipsoids.",
            "Well, the the bolts in reducing kernel Hilbert space are ellipsoids, right are going to be ellipsoids, and the axis of this ellipsoids are the eigenvalues, precisely OK.",
            "This eigen values will be the access of this ellipsoids.",
            "Now what we are doing here, we are taking ellipsoid right?",
            "In this space L2P, which is our unit bowlines, reproducing kernel Hilbert space, it becomes just ellipsoid in the initial space.",
            "And then, since Sigma Square is the largest as a largest.",
            "So Sigma is like just L2 norm right of the function in our class.",
            "What we're doing, we're intersecting this ellipsoid with the bowl of Radio Sigma, right?",
            "So it was another ellipsoid whose access our Sigma right?",
            "And when you intersect this two things right when you intersect those two things, it's easy to see that up to a constant, right up to a constant.",
            "It will be about the same set, about the same set as ellipsoid.",
            "Whose axis are truncated this way?",
            "OK, so up to a constant.",
            "You can replace one set by another.",
            "OK, it's very easy to see.",
            "So, so if you take you take actually intersection of two ellipsoids, right, who's who's who's access are whose coordinates are aligned right, it should be ellipsoids with respect to the same coordinate system, right?",
            "You take intersection of two ellipsoids, right?",
            "Then said that you are getting right.",
            "It says that you are getting can be can be roughly represented as ellipsoid, right?",
            "Who's access are the minimum of the of the axis of this Tulip.",
            "So it's OK.",
            "This is what it says, right?",
            "And.",
            "Roughly means that this set can be included sayin twice right?",
            "Such ellipsoid, right?",
            "The interception can be included in price, so it and will contain 1/2 of this ellipsoid.",
            "OK, so it's going to be multiplied by constant writers, convex bodies.",
            "And this is what you are.",
            "What you are getting here.",
            "And if you do kind of very very direct computations and for ellipsoid things are easy to compute, you will end up with this bow.",
            "OK, well that was about.",
            "OK.",
            "I think I'm a little bit behind the schedule, but let."
        ],
        [
            "Let me make a break here.",
            "OK, 10 minutes break OK?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we are very happy to have Vladimir culture in ski from Georgia Tech.",
                    "label": 0
                },
                {
                    "sent": "Who will talk about risk bounds and machine learning?",
                    "label": 1
                },
                {
                    "sent": "Lower the floor.",
                    "label": 0
                },
                {
                    "sent": "Just a second.",
                    "label": 0
                },
                {
                    "sent": "Is it working?",
                    "label": 0
                },
                {
                    "sent": "15 minutes please.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much and I will be talking about.",
                    "label": 0
                },
                {
                    "sent": "As part of I said it will be able to access against Bones in machine learning, so it will be.",
                    "label": 0
                },
                {
                    "sent": "Probably a little bit more and more probabilistic tutorials, and then we will be talking about mostly about some some probabilistic inequality, so it will be much more probabilistic than than most of us are struggles that you heard it before.",
                    "label": 0
                },
                {
                    "sent": "So what I will be?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're doing.",
                    "label": 0
                },
                {
                    "sent": "I will start.",
                    "label": 0
                },
                {
                    "sent": "It will take probably me one hour to go through some basic properties of empirical process is mostly these are certain inequalities that describes the size of empirical process and this is the main tool used when you are trying to bound the generalization error is the accessory skin machine learning OK, and in you know in kind of standard model.",
                    "label": 0
                },
                {
                    "sent": "Of learning.",
                    "label": 0
                },
                {
                    "sent": "And then probably after this it will be a time for a break and then after after the break I will talk about distribution independent bounds on excess risk.",
                    "label": 0
                },
                {
                    "sent": "So it will be a senchal a certain technologies that allows to find such bones at a reasonably tight.",
                    "label": 0
                },
                {
                    "sent": "It will wait very general technology.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "There will be probably another break and then the last two topics about data dependent bounds on excess risk and Oracle inequality is in model in model selection problems.",
                    "label": 1
                },
                {
                    "sent": "Also in a rather abstract framework of empirical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "This is what we will do, you know, closer to then.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know the problem.",
                    "label": 0
                },
                {
                    "sent": "We are going to deal with is sort of abstract, abstract version of empirical risk minimization problem.",
                    "label": 0
                },
                {
                    "sent": "So we will assume that that there is certain data X1, XN that consist of independent, identically distributed points sampled in some Space S, and there's a underlying distribution of the sample.",
                    "label": 1
                },
                {
                    "sent": "I don't know what it be OK, so we have this type of data and then there is a class of functions defined on S, mostly for technical reason I will assume that this functions are that takes values in 01K, so it will be uniformly bounded class and this allows to write nice exponential bones.",
                    "label": 0
                },
                {
                    "sent": "OK, so one can do something in the case when classes are unbounded and this is very important, but.",
                    "label": 0
                },
                {
                    "sent": "You know many kind of the the final picture.",
                    "label": 0
                },
                {
                    "sent": "It is clear in the in the in the case of bone that classes and I will just stick to this case here without talking about the situation.",
                    "label": 0
                },
                {
                    "sent": "Once the classes are unbounded now the function sends a class in machine learning framework.",
                    "label": 0
                },
                {
                    "sent": "They would represent losses of certain decision rules, for instance, OK all it can be a loss of a classifier, say OK or losses associated with certain actions, say OK. Can be a loss of a classifier, so usually you know points in this Space S will help structure.",
                    "label": 0
                },
                {
                    "sent": "So in fact it will be about what I want to emphasize is this structure will be will be irrelevant for us, at least for a while.",
                    "label": 0
                },
                {
                    "sent": "OK, so say point space S. In fact it will be in in many applications it will be not as but something like as cross minus one one.",
                    "label": 0
                },
                {
                    "sent": "If we are talking about binary classification.",
                    "label": 0
                },
                {
                    "sent": "OK, and then function F on the space.",
                    "label": 0
                },
                {
                    "sent": "Say.",
                    "label": 0
                },
                {
                    "sent": "Function F on this space can be.",
                    "label": 0
                },
                {
                    "sent": "It can be just, you know, it will depend now on X&Y and it will be just just.",
                    "label": 0
                },
                {
                    "sent": "There's a sink, so it will be $1 if we misclassify certain if classifier e.g misclassified certain example and $0.00 if if it.",
                    "label": 0
                },
                {
                    "sent": "If it classifieds correctly.",
                    "label": 0
                },
                {
                    "sent": "OK so this is what we are paying for misclassification, for instance.",
                    "label": 0
                },
                {
                    "sent": "Or if you if you want to think of something like boosting, it might be something like this, OK?",
                    "label": 0
                },
                {
                    "sent": "So there will be some real valued classifier, say and it will be exponential loss associated with this classifier.",
                    "label": 0
                },
                {
                    "sent": "Or if you're you're thinking about.",
                    "label": 0
                },
                {
                    "sent": "Delta regression, it can be just something like this.",
                    "label": 0
                },
                {
                    "sent": "Let me put here G. OK, so this is FK this other function F. So it will represent loss.",
                    "label": 0
                },
                {
                    "sent": "OK but for a while at least it will be completely irrelevant for me that the space has the structure so I will just denote it S and will use this as functions defined on an abstract space S and in this space I will be sampling points.",
                    "label": 0
                },
                {
                    "sent": "OK, so this mode is happening.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Based on this data, one can construct what is called empirical distribution, which is a natural natural estimate of the true distribution of our data P. So it just does a average of N point masses located at the points of the data.",
                    "label": 0
                },
                {
                    "sent": "I will get your friend Iraq measures OK. OK, that's the point of the data and.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then if we start integrating losses with respect to unknown distribution, if we integrate loss with respect to unknown distribution, then we are we are.",
                    "label": 0
                },
                {
                    "sent": "We are computing risk right?",
                    "label": 0
                },
                {
                    "sent": "It will be risk associated with various sort and.",
                    "label": 0
                },
                {
                    "sent": "Action, for instance, but this is.",
                    "label": 0
                },
                {
                    "sent": "This is in the background, right?",
                    "label": 0
                },
                {
                    "sent": "So we don't care about this, we're just leveraging loss and getting risk right?",
                    "label": 0
                },
                {
                    "sent": "Getting risk and then.",
                    "label": 0
                },
                {
                    "sent": "If we if we ever Julie Sconza data right so integrate average loss on the data, integrate loss function with respect to empirical distribution where computing empirical risk right there, computing empirical risk and then we are ending up with two problems that we want to compare.",
                    "label": 0
                },
                {
                    "sent": "We in principle what we want to do.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize the true risk, right?",
                    "label": 0
                },
                {
                    "sent": "But we don't have access to the true risk we are replacing.",
                    "label": 0
                },
                {
                    "sent": "The truly is by.",
                    "label": 0
                },
                {
                    "sent": "By the empirical risk and minimizing instead empirical risk.",
                    "label": 1
                },
                {
                    "sent": "And this is a solution that we are getting offended scared.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "The quantities that is of interest in this case that characterizes how well we are doing is accessing disk OK.",
                    "label": 0
                },
                {
                    "sent": "The access code we want to be.",
                    "label": 0
                },
                {
                    "sent": "We are replacing one optimization problem by another optimization problem.",
                    "label": 0
                },
                {
                    "sent": "The ones that we.",
                    "label": 0
                },
                {
                    "sent": "You know, I'm not really sold because we don't have access to the true risk we are replacing.",
                    "label": 0
                },
                {
                    "sent": "Bye bye bye and approximation.",
                    "label": 0
                },
                {
                    "sent": "And then the question is how well we are approximating and for a particular solution what we should look at?",
                    "label": 0
                },
                {
                    "sent": "We should look at the difference between the the risk of this particular solution and minimal possible risk.",
                    "label": 0
                },
                {
                    "sent": "And this is what is called the accessories.",
                    "label": 0
                },
                {
                    "sent": "OK in this problem, so PF minus.",
                    "label": 0
                },
                {
                    "sent": "BFF's in East Golf Course you know of F minus the.",
                    "label": 0
                },
                {
                    "sent": "Smallest possible risk in this problem, right is what will be called the accessories OK in in, in, in, in in, Bayesian statistics is a similar quantity, usually is called base regret.",
                    "label": 0
                },
                {
                    "sent": "OK, it's just just another terminology.",
                    "label": 0
                },
                {
                    "sent": "OK, so essentially what we are going to be interested in.",
                    "label": 0
                },
                {
                    "sent": "We are going to be interested in controlling with high probability the accessory risk by the accessories comes empirical risk minimizer, right?",
                    "label": 0
                },
                {
                    "sent": "So the accessories come this, yeah?",
                    "label": 0
                },
                {
                    "sent": "Talk about the empirical risk or you actually want the actor in the last line.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "No, I won't be off if I want I want I want I want to know when I plug in this thing here, right?",
                    "label": 0
                },
                {
                    "sent": "I want to know how small this quantities, right?",
                    "label": 0
                },
                {
                    "sent": "How close I am coming to the minimum, right?",
                    "label": 0
                },
                {
                    "sent": "This is what I want to achieve, right?",
                    "label": 0
                },
                {
                    "sent": "This is what I want to know right?",
                    "label": 0
                },
                {
                    "sent": "Basically right, I don't want to.",
                    "label": 0
                },
                {
                    "sent": "Well I want to know this, but mostly I want to know where this thing is minimized, right?",
                    "label": 0
                },
                {
                    "sent": "For which?",
                    "label": 0
                },
                {
                    "sent": "After the sync is minimized, right?",
                    "label": 0
                },
                {
                    "sent": "This is what I want to know and I want to know a function that that comes close right to this minimum right?",
                    "label": 0
                },
                {
                    "sent": "And this is a candidate right solution of this problem is a candidate and then I want to stick it here right?",
                    "label": 0
                },
                {
                    "sent": "To plug in it here and to know how large or how small this quantities right?",
                    "label": 0
                },
                {
                    "sent": "And this will tell me how Khloe and this will tell me how well I'm solving the empirical risk minimization problem, right?",
                    "label": 0
                },
                {
                    "sent": "Well, that's why I thought maybe you want empirical risk for your first term.",
                    "label": 0
                },
                {
                    "sent": "But maybe you want to know why?",
                    "label": 0
                },
                {
                    "sent": "Why would I want this?",
                    "label": 0
                },
                {
                    "sent": "The difference was between your empirical risk and the best you could do.",
                    "label": 0
                },
                {
                    "sent": "No, because because because because we know no becausw becausw at the end, I don't care about empirical risk at all right?",
                    "label": 0
                },
                {
                    "sent": "Why should we care about empirical risk?",
                    "label": 0
                },
                {
                    "sent": "Empirical risk, you know, is something that can become small, but it means nothing.",
                    "label": 0
                },
                {
                    "sent": "For instance, you know.",
                    "label": 0
                },
                {
                    "sent": "Our goal, a couple of examples that Rob Schapire you know talked about last time.",
                    "label": 0
                },
                {
                    "sent": "You know there can be a message that that that drives empirical risk 0, but it does not mean anything, right?",
                    "label": 0
                },
                {
                    "sent": "So we don't care about comparing empirical risk of F with this information, right?",
                    "label": 0
                },
                {
                    "sent": "What we really care is is to know how large or how small the accessories great defined by this lion is for.",
                    "label": 0
                },
                {
                    "sent": "For a friend with her you know.",
                    "label": 0
                },
                {
                    "sent": "For instance, if in the context of we will maybe arrive alright, just a little bit later, but in the context, for instance, of regression problems, right?",
                    "label": 0
                },
                {
                    "sent": "The accessory risk with auto loss, right?",
                    "label": 0
                },
                {
                    "sent": "This quadratic loss accessories becomes as the square of the L2 distance right between the.",
                    "label": 0
                },
                {
                    "sent": "Between our estimates and the progression function.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what we care about.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now again, what will be the problems?",
                    "label": 0
                },
                {
                    "sent": "OK, precisely the problems we will be talking about.",
                    "label": 0
                },
                {
                    "sent": "First its development of certain techniques that would allow us to construct upper bones upper bounds on the excess risk of empirical risk minimizer.",
                    "label": 1
                },
                {
                    "sent": "OK, so we want to know upper bounds on excess risk of empirical risk minimizer, and we will will care about 2.",
                    "label": 0
                },
                {
                    "sent": "Types of our bones.",
                    "label": 0
                },
                {
                    "sent": "First distribution independent upper bounds.",
                    "label": 0
                },
                {
                    "sent": "OK, that will will be somehow related to the unknown distribution and also to some geometric properties of the class and some complexity of the class F over which we are minimizing the empirical risk.",
                    "label": 0
                },
                {
                    "sent": "OK, so this will be one question.",
                    "label": 0
                },
                {
                    "sent": "Another question is we also care about about empirical versions of this bound.",
                    "label": 0
                },
                {
                    "sent": "So we want to have.",
                    "label": 0
                },
                {
                    "sent": "Copper confidence, bones and accessories that depend on the data so that using the data we can say how close, how well we are solving the problem and this is needed in particular in model selection problems in model selection problems there are many situations of course in which in which empirical risk minimization is is an ingredient in our problem, right?",
                    "label": 0
                },
                {
                    "sent": "So it's always almost always and most of the learning problems.",
                    "label": 0
                },
                {
                    "sent": "It's an important ingredient of certain learning algorithms, but.",
                    "label": 0
                },
                {
                    "sent": "Maybe not the cold, Grissom Gaser.",
                    "label": 0
                },
                {
                    "sent": "Maybe situations in which we have very huge.",
                    "label": 0
                },
                {
                    "sent": "A glass of functions, for instance losses associated with some huge neural network.",
                    "label": 0
                },
                {
                    "sent": "OK or huge family of neural networks OK, and this is can be very large class but in fact.",
                    "label": 0
                },
                {
                    "sent": "It would be useless to minimize empirical risk over the whole class if the class is huge, it will lead to overfitting.",
                    "label": 0
                },
                {
                    "sent": "OK will lead to overfitting and what is done instead.",
                    "label": 0
                },
                {
                    "sent": "Instead you're looking at some.",
                    "label": 0
                },
                {
                    "sent": "Subclasses of this plus some, maybe maybe families of functions of varying complexity neural networks, whereas you know different numbers of neutrons.",
                    "label": 0
                },
                {
                    "sent": "For instance, different numbers of layers.",
                    "label": 0
                },
                {
                    "sent": "Things like this, right?",
                    "label": 0
                },
                {
                    "sent": "We have these subfamilies and then you are.",
                    "label": 1
                },
                {
                    "sent": "You are solving empirical risk minimization on on each class FK over there some family right?",
                    "label": 0
                },
                {
                    "sent": "And then you want to find the best of the solutions, right?",
                    "label": 1
                },
                {
                    "sent": "This is a model selection problem, right?",
                    "label": 0
                },
                {
                    "sent": "And model selection problem in this context.",
                    "label": 0
                },
                {
                    "sent": "So you have many empirical risk minimization problems.",
                    "label": 0
                },
                {
                    "sent": "And then you want to find based on the data.",
                    "label": 0
                },
                {
                    "sent": "Find caves had such that they access risk of what you are of.",
                    "label": 0
                },
                {
                    "sent": "What you found will be as close as possible to the minimal in this problem, right?",
                    "label": 0
                },
                {
                    "sent": "There's a minimum of the access risks of.",
                    "label": 0
                },
                {
                    "sent": "You know for all these bunch of pink coleus minimization problems this model selection problems and in in this framework it's.",
                    "label": 0
                },
                {
                    "sent": "Really, very important to have some tight enough data dependent upper bounds on the accessories, because this helps you to make to make the selection of model.",
                    "label": 0
                },
                {
                    "sent": "In this problem.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let me start imagining that that that that you know nothing about empirical risk minimization or had nothing to do not.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Nothing about Vapnik and Sherman in kiss, just nothing.",
                    "label": 0
                },
                {
                    "sent": "OK, how would you approach?",
                    "label": 0
                },
                {
                    "sent": "Well, this problem of bondings accessories.",
                    "label": 0
                },
                {
                    "sent": "OK there is.",
                    "label": 0
                },
                {
                    "sent": "You know the following very trivial computations at anybody who would spend several minutes on this question would inevitably do OK.",
                    "label": 0
                },
                {
                    "sent": "So you want to control this quantity.",
                    "label": 0
                },
                {
                    "sent": "This quantity is the difference between the risk of offend with head and the infamous right at the infamous.",
                    "label": 0
                },
                {
                    "sent": "Imagine you know it's it's not important.",
                    "label": 0
                },
                {
                    "sent": "But imagine that the infamous attained somewhere OK and this is this function F's bar so we can write P. Of a friend with Scott minus PP office bar.",
                    "label": 0
                },
                {
                    "sent": "OK, so then you owe.",
                    "label": 0
                },
                {
                    "sent": "It is natural to do is to replace key RP by PN right and then to compensate for for this replacement actually right?",
                    "label": 0
                },
                {
                    "sent": "So we did this and then the first term here right?",
                    "label": 0
                },
                {
                    "sent": "This term is less of equals in zero right?",
                    "label": 0
                },
                {
                    "sent": "It's clear be cause FN Wiskott is aware the minimum of the empirical risk is attained, right?",
                    "label": 0
                },
                {
                    "sent": "There is a minimum is a day, and so these these should be smaller than this right?",
                    "label": 0
                },
                {
                    "sent": "Lesser equals and this happens.",
                    "label": 0
                },
                {
                    "sent": "Bar is also in our class, so there's different should be negative.",
                    "label": 0
                },
                {
                    "sent": "So in the upper bound we can drop it, right?",
                    "label": 0
                },
                {
                    "sent": "We can drop it and then we are upper bounding this by this quantity right?",
                    "label": 0
                },
                {
                    "sent": "By this quantity.",
                    "label": 0
                },
                {
                    "sent": "So we're looking at the difference between P&P.",
                    "label": 0
                },
                {
                    "sent": "We are computing this difference at the difference of functions F&G right FG and we are sleeping.",
                    "label": 0
                },
                {
                    "sent": "Overall functions FNG in our Class F right in our class at this is less or equals and twice they supremums.",
                    "label": 0
                },
                {
                    "sent": "The difference between P&P and NP right BNP NF and PM.",
                    "label": 0
                },
                {
                    "sent": "So the difference between between average and expectation right we are shipping this differences over Ochoa losses in our class.",
                    "label": 0
                },
                {
                    "sent": "All functions in our class and as a result we are getting this upper bound and.",
                    "label": 0
                },
                {
                    "sent": "In many reasonable situations, this upper bound will be over the water.",
                    "label": 0
                },
                {
                    "sent": "Oh capital of into the power minus 1/2 OK and this is big cause because of the central limit theorem, right?",
                    "label": 0
                },
                {
                    "sent": "For instance, if you take F justifying it class of functions, right?",
                    "label": 0
                },
                {
                    "sent": "It's definitely will be of the order and to the power minus 1/2.",
                    "label": 0
                },
                {
                    "sent": "Just 'cause this is, this is the case for a single function, right?",
                    "label": 0
                },
                {
                    "sent": "The difference between between average the sample mean and the population mean right?",
                    "label": 0
                },
                {
                    "sent": "And the expectation right is.",
                    "label": 0
                },
                {
                    "sent": "By the central Limit Theorem is order 1 / sqrt N right over 1 / sqrt N and if your class F is not too large and what does it mean to large is described very well in the theory of empirical processes.",
                    "label": 0
                },
                {
                    "sent": "It has been understood very well because this was the basic problem, right?",
                    "label": 0
                },
                {
                    "sent": "If it's not too large in the sense that the central limit theorem still holds on the whole class in some uniform sense, right in some uniform sentence is what is usually called.",
                    "label": 0
                },
                {
                    "sent": "Dark loss, right?",
                    "label": 0
                },
                {
                    "sent": "And in particular, it can be, for instance, such things as well.",
                    "label": 0
                },
                {
                    "sent": "Picture one and kiss glasses.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "Is IRP dansko OK in this case?",
                    "label": 0
                },
                {
                    "sent": "You will cover this upper bound on this phone accessories gate.",
                    "label": 0
                },
                {
                    "sent": "This is not a very good upper bone so right, and if you think for for several seconds you will tell me in which of these lines are several lion cereals are can be a big gap actually where we are losing key again this inequality as well.",
                    "label": 0
                },
                {
                    "sent": "Where is the place where we are doing not well.",
                    "label": 0
                },
                {
                    "sent": "In which way?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "He has this is the police but but but this is not really OK to use the truth.",
                    "label": 0
                },
                {
                    "sent": "It's not the main problem with this bull you know, OK, so now now since I told you that this is not the real cause of trouble, you know it's easy to identify the lion right?",
                    "label": 0
                },
                {
                    "sent": "Cause in the first line we have equality, right?",
                    "label": 0
                },
                {
                    "sent": "So we can't lose anything here, right?",
                    "label": 0
                },
                {
                    "sent": "So it can be only only here when we go from this line.",
                    "label": 0
                },
                {
                    "sent": "From this slide into this line.",
                    "label": 0
                },
                {
                    "sent": "So when we are controlling this sink right by this supermom, it's not real.",
                    "label": 0
                },
                {
                    "sent": "Is that important that here we are?",
                    "label": 0
                },
                {
                    "sent": "We are replacing something negative might be important sometimes, but here we are replacing something negative by by zero right?",
                    "label": 0
                },
                {
                    "sent": "And also losing a little bit.",
                    "label": 0
                },
                {
                    "sent": "But the main source of trouble is is actually here right?",
                    "label": 0
                },
                {
                    "sent": "When we go from this line to this line and this is where we are, where we are losing and the reason is very simple.",
                    "label": 0
                },
                {
                    "sent": "OK, the games on this work is simple and let me explain it now, but then we will get back to this after awhile.",
                    "label": 0
                },
                {
                    "sent": "OK, because this is a way also to understand this is also the way to improve the bound.",
                    "label": 0
                },
                {
                    "sent": "OK the reason is simple.",
                    "label": 0
                },
                {
                    "sent": "Becausw as soon as we went here right?",
                    "label": 0
                },
                {
                    "sent": "We already know that the access risk of offend with hat is smaller than N to the power minus 1/2, right?",
                    "label": 0
                },
                {
                    "sent": "OK is order enters the power minus 1/2 say right.",
                    "label": 0
                },
                {
                    "sent": "We know this so we know that it's more right.",
                    "label": 0
                },
                {
                    "sent": "Accessories of F as head of Miss Bar right of 1st Bar.",
                    "label": 0
                },
                {
                    "sent": "By its very definition is 0 right?",
                    "label": 0
                },
                {
                    "sent": "Because at this bar is where the minimum of the objectives that ain't right.",
                    "label": 0
                },
                {
                    "sent": "So now we have two functions for which they access.",
                    "label": 0
                },
                {
                    "sent": "Risk is small, right?",
                    "label": 0
                },
                {
                    "sent": "For which the accessories gets more, which means that in this supremum right in this super album I don't have to use actual as a whole class.",
                    "label": 0
                },
                {
                    "sent": "It would be enough for me to soup only over a part of the class that consists of functions with small enough accessories.",
                    "label": 0
                },
                {
                    "sent": "And I'm still getting a legitimate bone, right?",
                    "label": 0
                },
                {
                    "sent": "As a class of functions wags accessing skills small right words.",
                    "label": 0
                },
                {
                    "sent": "Access English.",
                    "label": 0
                },
                {
                    "sent": "Yes, Moe can be much smaller than the whole class of functions, right?",
                    "label": 0
                },
                {
                    "sent": "Can be much smaller than the whole class of functions.",
                    "label": 0
                },
                {
                    "sent": "And because of this, for instance, in the case of donsker classes always you will get not this, but at least a little bit better.",
                    "label": 0
                },
                {
                    "sent": "You will get all small offensive power minus 1/2 right in a typical situation for a dance class.",
                    "label": 0
                },
                {
                    "sent": "And we will see this.",
                    "label": 0
                },
                {
                    "sent": "And if you know more about your class and the fact that the classes don't care if you know some, set up some better control of complexity of the class, then you can get bounds that will will be as good as end to the power minus one.",
                    "label": 0
                },
                {
                    "sent": "For instance, OK.",
                    "label": 0
                },
                {
                    "sent": "Here OK so we will get much better grades that will be in statistical sense and so it's sort of a mini Mac.",
                    "label": 0
                },
                {
                    "sent": "Statistical sense would will, you know would be optimal bones?",
                    "label": 0
                },
                {
                    "sent": "OK, but this should be based on a little bit more subtle analysis.",
                    "label": 0
                },
                {
                    "sent": "OK then, as I said, what we are doing here and we will get through this.",
                    "label": 0
                },
                {
                    "sent": "It's very very simple also.",
                    "label": 0
                },
                {
                    "sent": "But we will get through this a little later.",
                    "label": 0
                },
                {
                    "sent": "But anyway, in order to do this in order to do this what we will have to do, inevitably we will have to understand how control.",
                    "label": 0
                },
                {
                    "sent": "So the size of such superhuman, right?",
                    "label": 0
                },
                {
                    "sent": "How we depends on the complexity of the class and how to know what the size of the supremum is, right this apartment?",
                    "label": 0
                },
                {
                    "sent": "Because even if we do this computation in a more subtle way and replace F by a part of F, still will be left with the problem.",
                    "label": 0
                },
                {
                    "sent": "How to control the same type of quantity on a part of F right locali say and still we have to know certain tools that would allow us to do this.",
                    "label": 0
                },
                {
                    "sent": "So because of this first we will stick.",
                    "label": 0
                },
                {
                    "sent": "Just adjust to this quantity right?",
                    "label": 0
                },
                {
                    "sent": "And try to.",
                    "label": 0
                },
                {
                    "sent": "The stand.",
                    "label": 0
                },
                {
                    "sent": "What are reasonable ways to control the size of this quantity?",
                    "label": 0
                },
                {
                    "sent": "OK, the size of this quantity.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we will be talking about is the subject of what is called empirical processes.",
                    "label": 0
                },
                {
                    "sent": "Cearley and this part of empirical processes series.",
                    "label": 0
                },
                {
                    "sent": "At that we will be talking about was it has been developed.",
                    "label": 0
                },
                {
                    "sent": "You know, for for now it's for 40 years.",
                    "label": 0
                },
                {
                    "sent": "OK, many people were involved in this, but it's probably important to emphasize the contributions of Mapnik.",
                    "label": 0
                },
                {
                    "sent": "Insure one Kiss feels that were really 40 years ago.",
                    "label": 0
                },
                {
                    "sent": "Start it over updating first.",
                    "label": 0
                },
                {
                    "sent": "Long trivial results.",
                    "label": 0
                },
                {
                    "sent": "OK on this type of problems in a general framework.",
                    "label": 0
                },
                {
                    "sent": "OK then.",
                    "label": 0
                },
                {
                    "sent": "Dick Dudley developed in the in the mid 70s developed really nice see of empirical processes that included central limit theorems and things like this.",
                    "label": 1
                },
                {
                    "sent": "OK, so he was.",
                    "label": 0
                },
                {
                    "sent": "Keeley most of terminologies that we are using now.",
                    "label": 0
                },
                {
                    "sent": "You know come from his work you know he he was the one who who started talking about.",
                    "label": 0
                },
                {
                    "sent": "We see classes and things like this OK it's all started in his work and then.",
                    "label": 0
                },
                {
                    "sent": "Another important name is really important.",
                    "label": 0
                },
                {
                    "sent": "Name is Telegram Cabie cause most of the important structural results that we are using now were developed by Telegram in the in the 90s.",
                    "label": 0
                },
                {
                    "sent": "OK and we will talk about some of them, but some of them now.",
                    "label": 0
                },
                {
                    "sent": "Basically there is also will be talking about.",
                    "label": 0
                },
                {
                    "sent": "It's a mixture of something something where you simple case there are certain inequalities that are very simple.",
                    "label": 1
                },
                {
                    "sent": "So simple that I will be able to show them here on the board.",
                    "label": 0
                },
                {
                    "sent": "OK, and there are some that are much much deeper and much more more sophisticated.",
                    "label": 0
                },
                {
                    "sent": "OK, and but but still I think it will be possible to understand the meaning of this inequality is OK, but I would without discussing the proofs.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "I already said what is written here, so I'm not going to get back to this, so it was already discussed.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now what is our purpose for the next?",
                    "label": 0
                },
                {
                    "sent": "I don't know half an hour or a little more.",
                    "label": 0
                },
                {
                    "sent": "So we will be looking at this object.",
                    "label": 0
                },
                {
                    "sent": "The difference between between the the average PNF is the average of our function at end points, right at 10 data points you adjust the sample mean right and the corresponding expectation.",
                    "label": 0
                },
                {
                    "sent": "So it's just we have many sample means, right?",
                    "label": 0
                },
                {
                    "sent": "And they're expectations right?",
                    "label": 0
                },
                {
                    "sent": "And we want to compare them.",
                    "label": 0
                },
                {
                    "sent": "This is what is called empirical process.",
                    "label": 1
                },
                {
                    "sent": "Usually it's normalized normalized by square root of N. Right, it's multiplied by square root of N, but it won't be important for me, so I'm not going to normalize it, OK?",
                    "label": 0
                },
                {
                    "sent": "Then the basic question we we want to look at is what is the size of this supermom right?",
                    "label": 1
                },
                {
                    "sent": "Obviously problem so we have again.",
                    "label": 1
                },
                {
                    "sent": "A large, maybe infinite, number of sample means and how they deviate in simulataneously right in a uniform sense from their expectations, which is actually, you know, maybe maybe one of the most basic questions in statistics if you start thinking about this right, at least in statistics of IID data, right of IID data?",
                    "label": 0
                },
                {
                    "sent": "And, well, maybe maybe one of the most basic questions and empirical Sciences.",
                    "label": 0
                },
                {
                    "sent": "If you want.",
                    "label": 0
                },
                {
                    "sent": "OK, because when we have repeated measurements, we always, you know if we.",
                    "label": 1
                },
                {
                    "sent": "If we are, if we are looking at some completely complex enough problems, we are often running in this type of difficulties.",
                    "label": 0
                },
                {
                    "sent": "We have to know the size of this quantity.",
                    "label": 0
                },
                {
                    "sent": "Then so basically what we want to know how this quantity depends on certain measures of complexity of the class, how to introduce nature, all matters of complexities that would allow us to control this quantity to control this quantity and also how to construct data dependent bounds or on this quantities that will be of correct size at least up to constants of correct size.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Who?",
                    "label": 0
                },
                {
                    "sent": "An important object to look at when when one deals with this type of processes is another stochastic process called rademeyer process.",
                    "label": 0
                },
                {
                    "sent": "Rather, my hair process and is defined here and essentially what is written here process.",
                    "label": 0
                },
                {
                    "sent": "It's also an average of the values of the functions at data points right there computing average, but this time we are.",
                    "label": 0
                },
                {
                    "sent": "We are putting random science.",
                    "label": 0
                },
                {
                    "sent": "Does this sum right?",
                    "label": 0
                },
                {
                    "sent": "We are we are flipping coin right independently of the date, of course, right?",
                    "label": 0
                },
                {
                    "sent": "Obtaining a sequence of plus and minus ones and putting science at random in this sum, right?",
                    "label": 0
                },
                {
                    "sent": "Randomizing some in this way, right?",
                    "label": 0
                },
                {
                    "sent": "So just introducing additional randomization in the sun?",
                    "label": 0
                },
                {
                    "sent": "This removes this process.",
                    "label": 0
                },
                {
                    "sent": "This quantity is super normal.",
                    "label": 0
                },
                {
                    "sent": "This process is something that often is called Rademacher complexity, and it's a global version of random here complexity, because in in the problems of controlling the size of accessories we often have to localize order my hair complexity, and there are also things called local market complexities.",
                    "label": 0
                },
                {
                    "sent": "This is a global version.",
                    "label": 0
                },
                {
                    "sent": "So here's a very simple statistical meaning of async right, and it's very easy to understand at heuristic level why.",
                    "label": 0
                },
                {
                    "sent": "Why is this thing might be might be a reasonable measure of complexity in the cases when you, for instance, are looking at some some large function class and are trying to use it for certain purposes.",
                    "label": 0
                },
                {
                    "sent": "For instance to feed to feed regression model or something like this, right?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Uh, and you want to know whether this class is not too large so that you know you're not going to have our feet using this bunch of functions, right?",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of functions.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this expression, then basically what it is, it's something like empirical correlation, coefficient sample, correlation coefficient between the values of the function F right?",
                    "label": 0
                },
                {
                    "sent": "We have function F. We are computing this function at data points, right?",
                    "label": 0
                },
                {
                    "sent": "And then we have a random noise plus and minus one with probability 1/2 of them.",
                    "label": 0
                },
                {
                    "sent": "I hear noise.",
                    "label": 0
                },
                {
                    "sent": "If you want right then what we are computing when you're computing the server that you are computing, sort of a sample correlation coefficient between the values of the function at.",
                    "label": 0
                },
                {
                    "sent": "Points right and random noise, right random noise now.",
                    "label": 0
                },
                {
                    "sent": "You started souping device correlation coefficient over your class.",
                    "label": 0
                },
                {
                    "sent": "Right or your class of function and this even quantities that you are getting right is large.",
                    "label": 0
                },
                {
                    "sent": "It would mean that.",
                    "label": 0
                },
                {
                    "sent": "For this random noise right?",
                    "label": 0
                },
                {
                    "sent": "For this random noise at you, for instance, simulated on your computer, right simulated on your computer, you can always find a function in your class.",
                    "label": 0
                },
                {
                    "sent": "Existing is large, it means that there is a function in this class that will set the values of this function and data points are highly correlated, right, highly correlated with the noise.",
                    "label": 0
                },
                {
                    "sent": "Right, if we have such a clause, that is that for for such noise, like or, they might hear noise just for this, you know, flipping coins and writing plus and minus one, you can always find a function that is, we have high correlation with this.",
                    "label": 0
                },
                {
                    "sent": "So you feed the noise well, right?",
                    "label": 0
                },
                {
                    "sent": "This is a good sign that we are going to have our feet.",
                    "label": 0
                },
                {
                    "sent": "If you use this class.",
                    "label": 0
                },
                {
                    "sent": "If you use this class.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the reasons why.",
                    "label": 0
                },
                {
                    "sent": "This quantities is a natural measure of complexity in many problems.",
                    "label": 0
                },
                {
                    "sent": "Of course one has to sync.",
                    "label": 0
                },
                {
                    "sent": "What does it mean large and what does it mean?",
                    "label": 0
                },
                {
                    "sent": "Small here, right?",
                    "label": 0
                },
                {
                    "sent": "If for instance, instead of FI plug in a constant.",
                    "label": 0
                },
                {
                    "sent": "If I take F equal to 1 right?",
                    "label": 0
                },
                {
                    "sent": "I'm getting just one over and some of epsilon ice.",
                    "label": 0
                },
                {
                    "sent": "What is the nature of size of the sink?",
                    "label": 0
                },
                {
                    "sent": "One over and some of epsilon nice.",
                    "label": 0
                },
                {
                    "sent": "What is the natural size of six is it?",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "And one over N 1 / sqrt N or N ^3.",
                    "label": 0
                },
                {
                    "sent": "The choice is yours.",
                    "label": 0
                },
                {
                    "sent": "Wanna work square root of all right?",
                    "label": 0
                },
                {
                    "sent": "So by the central limit theorem, for instance right?",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                },
                {
                    "sent": "If you look if you look at average of IID random variables with like this, right?",
                    "label": 1
                },
                {
                    "sent": "Just just plus and minus one with probability 1/2.",
                    "label": 0
                },
                {
                    "sent": "If you computed variance, it will be precisely one over again, so the standard deviation is 1 / sqrt N right one over square root of it.",
                    "label": 0
                },
                {
                    "sent": "So 1 / sqrt N is not something large in this context, right?",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 0
                },
                {
                    "sent": "See if.",
                    "label": 1
                },
                {
                    "sent": "If, for instance, you can find in your class of function F right side just F of XI is just equal to epsilon I.",
                    "label": 0
                },
                {
                    "sent": "Right, if you have leaks, then there's this super mobile become as like just one right?",
                    "label": 0
                },
                {
                    "sent": "We will get one for this function, right?",
                    "label": 0
                },
                {
                    "sent": "So the supremum will be at least one in this case, right?",
                    "label": 0
                },
                {
                    "sent": "This is already large, so roughly not very large means something on the order of 1 / sqrt N this is good enough, right?",
                    "label": 0
                },
                {
                    "sent": "This is good enough and large is something of the order over constant.",
                    "label": 0
                },
                {
                    "sent": "Say great, this is already wagging like short too like Shaggy because for example this.",
                    "label": 0
                },
                {
                    "sent": "OK so this is just Java meaning zoo meaning of this quantity knows real is underway.",
                    "label": 0
                },
                {
                    "sent": "Just wanted.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use of important is because of certain inequalities that can be proved.",
                    "label": 0
                },
                {
                    "sent": "As it relates to the size of the empirical process, ascential is the accuracy of approximation of the true distribution by the empirical distribution on the on the class F to the size of Rademacher Process, and this is very nice.",
                    "label": 0
                },
                {
                    "sent": "So very simple inequality is that tells that essentially up to up to constant 2 1/2.",
                    "label": 0
                },
                {
                    "sent": "Right on average, these two things are the same.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Sweetheart, ball or continuous?",
                    "label": 0
                },
                {
                    "sent": "Random variable like is.",
                    "label": 0
                },
                {
                    "sent": "It doesn't make sense to do the same thing with something that abortion noises.",
                    "label": 0
                },
                {
                    "sent": "Would this thing makes sense with Gaussian noise instead of Rademacher noises?",
                    "label": 0
                },
                {
                    "sent": "York way, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah it makes.",
                    "label": 0
                },
                {
                    "sent": "It makes perfect sense.",
                    "label": 0
                },
                {
                    "sent": "You can replace the randomization by Gaussian randomization.",
                    "label": 0
                },
                {
                    "sent": "This things will be about the same order actually, you know, since you mentioned this, I will.",
                    "label": 0
                },
                {
                    "sent": "I will, I promise you, I will use it today.",
                    "label": 0
                },
                {
                    "sent": "OK I will.",
                    "label": 0
                },
                {
                    "sent": "I will replace Gaussian by randomizing show you how to do it OK later today.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You know, so it's only imperfection in this inequality is that in the upper bound I put F in the lower bound.",
                    "label": 0
                },
                {
                    "sent": "I put center Class FC and this is needed in the lower bound and upper bound.",
                    "label": 0
                },
                {
                    "sent": "But you know what is trivial, almost very easy to see that in the upper bound you can.",
                    "label": 0
                },
                {
                    "sent": "I can also write FC and this is just gives me, you know, kind of a back from bound right will be 2 sided inequality with we just adjust the trolls that these two things are actually same up to factor or factor two 1/2.",
                    "label": 0
                },
                {
                    "sent": "OK, up to factor two 1/2.",
                    "label": 0
                },
                {
                    "sent": "I am not writing here.",
                    "label": 0
                },
                {
                    "sent": "If she just 'cause this is what we need.",
                    "label": 0
                },
                {
                    "sent": "OK but both are true.",
                    "label": 0
                },
                {
                    "sent": "OK, in the lower bound FC is needed.",
                    "label": 0
                },
                {
                    "sent": "KFC is need.",
                    "label": 0
                },
                {
                    "sent": "This inequality is actually.",
                    "label": 0
                },
                {
                    "sent": "So easy to prove that I can't, you know, always when I talk about these things and have time.",
                    "label": 0
                },
                {
                    "sent": "I never never.",
                    "label": 0
                },
                {
                    "sent": "Can you know just the temptation to prove it?",
                    "label": 0
                },
                {
                    "sent": "OK, because it's just very, very simple.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand it's it's very simple.",
                    "label": 0
                },
                {
                    "sent": "You know, after it has been understood.",
                    "label": 0
                },
                {
                    "sent": "Be cause I still remember time when I when I was, say a graduate student, I still remember Diamons are where the whole conference is dedicated to this type of symmetry.",
                    "label": 0
                },
                {
                    "sent": "Caitians with random uh-huh random variables.",
                    "label": 0
                },
                {
                    "sent": "It's actually a trick that has been used for long for a long while.",
                    "label": 0
                },
                {
                    "sent": "You know to to replace in sums like this just to multiply by randomly chosen science.",
                    "label": 0
                },
                {
                    "sent": "OK, it's something that that was that was equally used in.",
                    "label": 0
                },
                {
                    "sent": "In probability, see you again in harmonic analysis for a number of years.",
                    "label": 0
                },
                {
                    "sent": "Actually, you know.",
                    "label": 0
                },
                {
                    "sent": "Or or the market is the name of the microwave.",
                    "label": 0
                },
                {
                    "sent": "Gables comes from her functions that are used in in in analysis to construct Walsh basis.",
                    "label": 0
                },
                {
                    "sent": "For instance right.",
                    "label": 0
                },
                {
                    "sent": "And this is very nice family or so normal functions that can be defined explicitly on the interval 01 right on the interval 01 and that become just IID random variables taking values plus and minus one with probability 1/2.",
                    "label": 0
                },
                {
                    "sent": "If you interval 01 was laid back measure as.",
                    "label": 0
                },
                {
                    "sent": "Probably just space.",
                    "label": 0
                },
                {
                    "sent": "OK, so it comes from there and it comes from harmonic analysis, actualizar use of this type of things and there was in the mid 60s.",
                    "label": 0
                },
                {
                    "sent": "There was a book of Kahan on random functional see I guess where he already you know extensively used this technique.",
                    "label": 0
                },
                {
                    "sent": "Actually you know for various purposes, OK for analyzing the sizes of certain sums of of stochastic processes say OK, so let me let me show you the proof of this.",
                    "label": 0
                },
                {
                    "sent": "Only only holds upper bound, which is which is very simple.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we have to do?",
                    "label": 0
                },
                {
                    "sent": "We have to create.",
                    "label": 0
                },
                {
                    "sent": "Another bunch.",
                    "label": 0
                },
                {
                    "sent": "Of random variables that are just, you know we want to create this guy, which will be a copy independent copy.",
                    "label": 0
                },
                {
                    "sent": "Box 1 Excel so it's convenient to sync over this as just a bunch of random variables that are probabilistically the same, but they're independent.",
                    "label": 0
                },
                {
                    "sent": "They, for instance are defying somewhere on the on the Mars.",
                    "label": 0
                },
                {
                    "sent": "OK, so X1 XN are on the Earth 6 one we still the extent we still there are probabilistically the same, but they are on the market and they just don't know each other.",
                    "label": 0
                },
                {
                    "sent": "OK, so they are independent.",
                    "label": 0
                },
                {
                    "sent": "OK then PN with steel there will be empirical distribution based on these guys with steel duh.",
                    "label": 0
                },
                {
                    "sent": "OK based on this guys were still there then or what we want to know.",
                    "label": 0
                },
                {
                    "sent": "We want to know the expectation of the supremum.",
                    "label": 0
                },
                {
                    "sent": "We want to know the expectation of the supremum, sorry.",
                    "label": 0
                },
                {
                    "sent": "We want to know this right?",
                    "label": 0
                },
                {
                    "sent": "This is what we want to know, right?",
                    "label": 0
                },
                {
                    "sent": "But it's clear that if I, you know, look at P and we still do off F and applies this expectation, which is the expectation on the Mars.",
                    "label": 0
                },
                {
                    "sent": "OK, I will get here right?",
                    "label": 0
                },
                {
                    "sent": "'cause those guys were still have the same distribution rights also have distribution be, so the expectation will be the expectation of their average right which began with steel days.",
                    "label": 0
                },
                {
                    "sent": "Will be just adjust BF right?",
                    "label": 0
                },
                {
                    "sent": "So it means that I can replace this PDF, write and write write it this way.",
                    "label": 0
                },
                {
                    "sent": "And put here expectation.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "P and we still deaf right now for this expectation, right?",
                    "label": 0
                },
                {
                    "sent": "For this expectation?",
                    "label": 0
                },
                {
                    "sent": "This thing this is on there's right.",
                    "label": 0
                },
                {
                    "sent": "This is on the on the marks OK, this is on there's this thing, for this expectation is just a constant right?",
                    "label": 0
                },
                {
                    "sent": "It's like a constant, so it means that I can take this expectation and move it.",
                    "label": 0
                },
                {
                    "sent": "You know just write it here, OK?",
                    "label": 0
                },
                {
                    "sent": "This expectation is here and it applies to the difference between P&F&P and we still do off F, right?",
                    "label": 0
                },
                {
                    "sent": "I can do this because expectation of constant is just this constant itself, right?",
                    "label": 0
                },
                {
                    "sent": "So just just using the properties of expectations here OK?",
                    "label": 0
                },
                {
                    "sent": "And then there's this expectation was still days inside the Super Moon, but I can put it outside.",
                    "label": 0
                },
                {
                    "sent": "I am only increasing then they the size of the expectation, right?",
                    "label": 0
                },
                {
                    "sent": "So we will get an equality.",
                    "label": 0
                },
                {
                    "sent": "So I will write here less or equal and put this expectation old site.",
                    "label": 0
                },
                {
                    "sent": "Somewhere here see.",
                    "label": 0
                },
                {
                    "sent": "OK, all this application is here now.",
                    "label": 0
                },
                {
                    "sent": "I don't have this.",
                    "label": 0
                },
                {
                    "sent": "If we still hear what I have inside, let me write it OK.",
                    "label": 0
                },
                {
                    "sent": "So I can use.",
                    "label": 0
                },
                {
                    "sent": "Two expectations, and then I will have here Supreme over my class and I will have to let me write.",
                    "label": 0
                },
                {
                    "sent": "Just explicitly what I have.",
                    "label": 0
                },
                {
                    "sent": "Hey, basically you know this is average of function F at data points on the Earth, right?",
                    "label": 0
                },
                {
                    "sent": "Another one is the average on the marks right?",
                    "label": 0
                },
                {
                    "sent": "The average on the Mars, so I can.",
                    "label": 0
                },
                {
                    "sent": "I can write them together.",
                    "label": 0
                },
                {
                    "sent": "L'eglise can ride some legs.",
                    "label": 0
                },
                {
                    "sent": "Now what I have here this call sync right is just certain function.",
                    "label": 0
                },
                {
                    "sent": "That depends.",
                    "label": 0
                },
                {
                    "sent": "Onto an independent identically distributed random variables right and the main property of independent identically distributed random variables is that they are exchangeable.",
                    "label": 0
                },
                {
                    "sent": "Boat so if you if you switch this guy's right but I'm use them in an arbitrary way, you're not changing the distribution right?",
                    "label": 0
                },
                {
                    "sent": "If you're not changing the distribution, we are not changing the expectation.",
                    "label": 0
                },
                {
                    "sent": "Also right?",
                    "label": 0
                },
                {
                    "sent": "Which means that I can now play this game.",
                    "label": 0
                },
                {
                    "sent": "I can take one of those guys, for instance.",
                    "label": 0
                },
                {
                    "sent": "You know whatever X1 say and send it to the errors and this guy will go to the marks right?",
                    "label": 0
                },
                {
                    "sent": "And this is equivalent to.",
                    "label": 0
                },
                {
                    "sent": "Putting minus right somewhere in this right.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to change change scion of one of these terms, right?",
                    "label": 0
                },
                {
                    "sent": "If I'm doing this and what I am changing the sign I'm doing just permutation of my random variable.",
                    "label": 0
                },
                {
                    "sent": "So I'm not changing expectations, right?",
                    "label": 0
                },
                {
                    "sent": "It means that I can put in this sum in an arbitrary away plus and pluses and minuses right?",
                    "label": 0
                },
                {
                    "sent": "And I'm not changing expectation right for any combination of pluses and minuses.",
                    "label": 0
                },
                {
                    "sent": "The expectation will be the same right?",
                    "label": 0
                },
                {
                    "sent": "Then?",
                    "label": 0
                },
                {
                    "sent": "Of course it means that I can put random pluses and minuses with, you know justice right there.",
                    "label": 0
                },
                {
                    "sent": "Paradise, right?",
                    "label": 0
                },
                {
                    "sent": "This are random pluses and minuses and take expectation was expected.",
                    "label": 0
                },
                {
                    "sent": "Them I'm not changing anything by doing season OK so this is what I am doing.",
                    "label": 0
                },
                {
                    "sent": "I am putting here Epsilon Dre and add one more expectation.",
                    "label": 0
                },
                {
                    "sent": "This average with respect to epsilons, several spectral epsilons and then what remains is just to split this sum into two right one.",
                    "label": 0
                },
                {
                    "sent": "On the address and another on the marks right and bound supremum of some of these two things by some of their suprema, right?",
                    "label": 0
                },
                {
                    "sent": "And you will get expectation of each Suprema, right?",
                    "label": 0
                },
                {
                    "sent": "Some of the expectations of the Supreme and this expectations are the same because the guys have the same distribution and you are getting twice expectation observed on my hair process, right getting twice expectation observed on my airport, so it's a very very simple proof, so there's nothing complicated here, but it's sort of a smart.",
                    "label": 0
                },
                {
                    "sent": "Move.",
                    "label": 0
                },
                {
                    "sent": "Those are the logs to get this border.",
                    "label": 0
                },
                {
                    "sent": "It's a simple exercise to reduce this in the case of the lower bound.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a similar argument that leads to the lower bound here.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, so this is 1 inequality is that that is very important and I will explain.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little late or why it is important gay then The thing is another inequality, it's.",
                    "label": 0
                },
                {
                    "sent": "It's a little more complicated to prove this would require more work.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to do this, it's all called contraction inequality due to telegram.",
                    "label": 1
                },
                {
                    "sent": "And this is very nice property of over there.",
                    "label": 0
                },
                {
                    "sent": "Michael processes essentially tells us the following is that if you have a certain function class, but then you want transforms its functions do a non linear transformation of this functions right and apply the same function Phi right to all functions in our in your class, right?",
                    "label": 0
                },
                {
                    "sent": "So we are getting a transform class like this right?",
                    "label": 0
                },
                {
                    "sent": "And you want to control the size of the.",
                    "label": 0
                },
                {
                    "sent": "So my hair process, right?",
                    "label": 0
                },
                {
                    "sent": "The expectation of this supremum for this class OK, or this class then provided that this this function is is a contraction is Lipschitz function with constant one, you just just have you know whenever you have this function, you can just drop it and write the constant.",
                    "label": 0
                },
                {
                    "sent": "Right multiply by right.",
                    "label": 0
                },
                {
                    "sent": "This was only thing you are losing right?",
                    "label": 0
                },
                {
                    "sent": "So it's controlled by the Democratic Process Index by F right index by F and.",
                    "label": 0
                },
                {
                    "sent": "That's all that you need, right?",
                    "label": 0
                },
                {
                    "sent": "In the case, if of course you know by by simple or scaling right it follows immediately.",
                    "label": 1
                },
                {
                    "sent": "Is that if you have a function five with Lipschitz constant daily, you will have to multiply here by this Lipschitz constant.",
                    "label": 0
                },
                {
                    "sent": "OK, this is only difference that will happen here.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you.",
                    "label": 0
                },
                {
                    "sent": "You know, recalls that in applications right in applications functions F will have some structure.",
                    "label": 0
                },
                {
                    "sent": "They are losses, right?",
                    "label": 0
                },
                {
                    "sent": "For instance, it can be F can be why minus G square, right?",
                    "label": 0
                },
                {
                    "sent": "In the case of regression y -- G of X squared this is your F, right?",
                    "label": 0
                },
                {
                    "sent": "This URF in this case when you're controlling the size of random ahead process for such for such a class, right?",
                    "label": 0
                },
                {
                    "sent": "For such a class you want to drop the square.",
                    "label": 0
                },
                {
                    "sent": "Right, we want to get rid of the square can be done provided that the range of your functions is bounded.",
                    "label": 0
                },
                {
                    "sent": "In this case, square is going to be Lipschitz right?",
                    "label": 0
                },
                {
                    "sent": "And then you can do it right?",
                    "label": 0
                },
                {
                    "sent": "If the range is unbounded, it's impossible to do it this way.",
                    "label": 0
                },
                {
                    "sent": "OK, it will be impossible to do it this way, so this is a very convenient technical tool in many cases and use the log.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No OK. What I will do now, I will try to show you.",
                    "label": 0
                },
                {
                    "sent": "Several.",
                    "label": 0
                },
                {
                    "sent": "Several bones on the.",
                    "label": 0
                },
                {
                    "sent": "Size of Rademacher processes.",
                    "label": 0
                },
                {
                    "sent": "At the same time these are bones on the size of empirical process by size.",
                    "label": 0
                },
                {
                    "sent": "At the moment I mean just expectation right of the Super norm.",
                    "label": 0
                },
                {
                    "sent": "I will be looking at expectation of the supplier.",
                    "label": 0
                },
                {
                    "sent": "Will discuss a little later what to do, what to do if.",
                    "label": 0
                },
                {
                    "sent": "If you want to.",
                    "label": 0
                },
                {
                    "sent": "Oh no, how?",
                    "label": 0
                },
                {
                    "sent": "This random variable itself, Supreme with empirical or under my care process, how it's related to its expectation.",
                    "label": 0
                },
                {
                    "sent": "OK, we will look at this a little bit later.",
                    "label": 0
                },
                {
                    "sent": "OK, but at the moment we will care about expectation only.",
                    "label": 0
                },
                {
                    "sent": "So several bones on the expectation of the Supreme observed by her process.",
                    "label": 0
                },
                {
                    "sent": "So we will use the following notations, right?",
                    "label": 0
                },
                {
                    "sent": "Sigma Square roughly will be the maximal valiance right or function in the class, right?",
                    "label": 0
                },
                {
                    "sent": "I'm not centering it, it's this function is not important here, so it's just the maximum of the of the square observed to norm.",
                    "label": 0
                },
                {
                    "sent": "So Sigma is the maximal L2 norm of the function in the class, right?",
                    "label": 0
                },
                {
                    "sent": "So this is what you owe me.",
                    "label": 0
                },
                {
                    "sent": "Annals of quantities that is of interest here.",
                    "label": 0
                },
                {
                    "sent": "Was it characterizes some towels a size of the class right.",
                    "label": 0
                },
                {
                    "sent": "The size of the functions of class, so we will need another quantities.",
                    "label": 0
                },
                {
                    "sent": "This will be as a as like just super norm right of the functions in the class.",
                    "label": 0
                },
                {
                    "sent": "So you is is a constant that uniformly bounds all functions in our class, right?",
                    "label": 0
                },
                {
                    "sent": "There's another characteristic that we will use then sometimes since we will care a lot about classes that are small.",
                    "label": 0
                },
                {
                    "sent": "Actually, our classes of small functions sometimes it's beneficial to have better control of functions and this control by you and this is done by by by something called envelope of the functions of the class.",
                    "label": 0
                },
                {
                    "sent": "So this is just one single function right?",
                    "label": 0
                },
                {
                    "sent": "One single functions at bounds.",
                    "label": 0
                },
                {
                    "sent": "All the functions in the class.",
                    "label": 0
                },
                {
                    "sent": "OK, if we are considering functions that are uniformly bounded, so this will be dominated by you, but.",
                    "label": 0
                },
                {
                    "sent": "In some cases it can be as a nice examples in which in which it's important to take into account.",
                    "label": 0
                },
                {
                    "sent": "This envelopes also OK, so those are characteristics that we will use and then we will use in each example different ways to characterize complexity of the class itself.",
                    "label": 0
                },
                {
                    "sent": "There will be several ways I will look at and I will show you several bones.",
                    "label": 0
                },
                {
                    "sent": "OK, here is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the simplest bones.",
                    "label": 0
                },
                {
                    "sent": "One of the simplest bones.",
                    "label": 0
                },
                {
                    "sent": "This isn't the case when our class is finite, right?",
                    "label": 0
                },
                {
                    "sent": "Our class is finite and consist of capital and functions OK. What is going to happen in this case?",
                    "label": 0
                },
                {
                    "sent": "In this case, as a expectation is controlled is controlled by two by the maximum of two terms right by the maximum of two terms and the first term depends on Sigma on the maximal variance of function in the class.",
                    "label": 0
                },
                {
                    "sent": "If you want right and it's you know the Sigma is divided by square root of N. It's natural in this situation right?",
                    "label": 0
                },
                {
                    "sent": "Or the microprocessors average right?",
                    "label": 0
                },
                {
                    "sent": "So we should be obvious.",
                    "label": 0
                },
                {
                    "sent": "Size 1 / sqrt N approximately right.",
                    "label": 0
                },
                {
                    "sent": "About who was respectable just to the sample size, and then it's multiplied by square root of the logarithm of the cardinality of the class, right?",
                    "label": 0
                },
                {
                    "sent": "Absolutely some of the cardinality of the class.",
                    "label": 0
                },
                {
                    "sent": "Well, let me ask you one question.",
                    "label": 0
                },
                {
                    "sent": "So if G1G NRIAD normal random variable was mean zero and variance Sigma Square, what is the size of this expectation?",
                    "label": 0
                },
                {
                    "sent": "How it depends on on Sigma and on the number of random variables kept on capital N. Mask.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Sqrt 2 log in right?",
                    "label": 0
                },
                {
                    "sent": "OK thank you.",
                    "label": 0
                },
                {
                    "sent": "OK so it will be roughly like this.",
                    "label": 0
                },
                {
                    "sent": "You know, I don't care about constant so I will write just six.",
                    "label": 0
                },
                {
                    "sent": "OK, so to be of the order.",
                    "label": 0
                },
                {
                    "sent": "Sigma Times Square root of the logarithm of capital M. It's a simple probability exercise to prove this.",
                    "label": 0
                },
                {
                    "sent": "So if you have not done this yet at the right time, you know during the break to do it OK.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you're Sturm here and you will see this pattern in other bones that I will show you is the term that is.",
                    "label": 0
                },
                {
                    "sent": "It is similar to some kind of Gaussian behavior.",
                    "label": 0
                },
                {
                    "sent": "OK, it's it's A kind of comes from certain Gaussian tails, right?",
                    "label": 0
                },
                {
                    "sent": "Gaussian tails of our random variables, right so?",
                    "label": 0
                },
                {
                    "sent": "If we had only the 1st right, it will tell us that that that our buncher for adima here sums right by Jeffrey Dahmer here some.",
                    "label": 0
                },
                {
                    "sent": "So we are looking at this order.",
                    "label": 0
                },
                {
                    "sent": "My hair process right?",
                    "label": 0
                },
                {
                    "sent": "Right, Bunch right behaves approximately as as ID as the maximum of iid Gaussian random variables.",
                    "label": 0
                },
                {
                    "sent": "Right capital N of them right with with variant Sigma divided by square root of N right Sigma divided by square.",
                    "label": 0
                },
                {
                    "sent": "This was the first term is about right now.",
                    "label": 0
                },
                {
                    "sent": "Of which random variables I should do?",
                    "label": 0
                },
                {
                    "sent": "Another question which random variables I should use here instead of Gaussian in order to have here, you know instead of these two have here, which will be something of the order logarithm of N. What kind of random variable should be used here?",
                    "label": 0
                },
                {
                    "sent": "Yeah, something something like exponential random variables, right?",
                    "label": 0
                },
                {
                    "sent": "So if you if you use exponential random variables right with exponential tails, high Square is.",
                    "label": 0
                },
                {
                    "sent": "Alright, for from this point of view, if you use exponential random variables, right exponential random variables with exponential tails right?",
                    "label": 0
                },
                {
                    "sent": "So so the tails decay not as it was a power minus minus X square, but as each of the power minus X right?",
                    "label": 0
                },
                {
                    "sent": "You would end up here with with Logan is right, it will say the expectation will grow as logarithm, not as square root of logarithm, right?",
                    "label": 0
                },
                {
                    "sent": "So the second term comes from some.",
                    "label": 0
                },
                {
                    "sent": "Sub exponential behavior of our random variables right?",
                    "label": 0
                },
                {
                    "sent": "And this this combination of sub Gaussian and sub conventional type of behavior is typical when we talk about sums of independent random variables who does not know Bernstein's inequality in probability?",
                    "label": 0
                },
                {
                    "sent": "See again.",
                    "label": 0
                },
                {
                    "sent": "Who does not know?",
                    "label": 0
                },
                {
                    "sent": "K who knows?",
                    "label": 0
                },
                {
                    "sent": "But in science, inequality in probability, CA.",
                    "label": 0
                },
                {
                    "sent": "Who does not know whether you know or not?",
                    "label": 0
                },
                {
                    "sent": "OK, anyway I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about Bernstein's inequality, but I will talk about about telegrams inequality a little later.",
                    "label": 0
                },
                {
                    "sent": "I will have it OK, which is which is a generalization of Bernstein's inequality.",
                    "label": 0
                },
                {
                    "sent": "So we will learn this way about birthday and equality as well, But basically this type of behavior is that we see here comes from Bernstein's inequality in a way, right?",
                    "label": 0
                },
                {
                    "sent": "And Bernstein's inequality science in equality controls.",
                    "label": 0
                },
                {
                    "sent": "This sum of independent random variables with mean zero and finite variance and they are bounded, say write it controls, controls it by two exponents, right?",
                    "label": 0
                },
                {
                    "sent": "And depending you're looking at probabilities that this sum is bigger than X, say right and depending on whether X is relatively small or relatively large, you will have in the first case when it's relatively well, will help sub Gaussian type of behavior and effects is relatively large.",
                    "label": 0
                },
                {
                    "sent": "You have to switch to sub exponential type of behavior both both.",
                    "label": 0
                },
                {
                    "sent": "Sales are sharp actual.",
                    "label": 0
                },
                {
                    "sent": "Then you can construct examples that show that this type of inequality sarshar right.",
                    "label": 0
                },
                {
                    "sent": "If you translate this in the in the language of expectations, right?",
                    "label": 0
                },
                {
                    "sent": "How expectations of of the maximum of many random variables of these types that have the structure of the sum of independent random variables.",
                    "label": 0
                },
                {
                    "sent": "This is what we are dealing with here.",
                    "label": 0
                },
                {
                    "sent": "If you translate it into the language of expectations, you will end up with these two terms right.",
                    "label": 0
                },
                {
                    "sent": "One term is Coco responds to something Gaussian, another something exponential.",
                    "label": 0
                },
                {
                    "sent": "OK, something exponential.",
                    "label": 0
                },
                {
                    "sent": "OK so this is 1.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or one inequality is on another inequality.",
                    "label": 0
                },
                {
                    "sent": "It's it's something more interesting and related to the old work of Applicant Chairman, Yankees.",
                    "label": 0
                },
                {
                    "sent": "Is this is the example in which is a class F consist of of binary functions, right functions that takes value zero and one something that is of importance in classification?",
                    "label": 0
                },
                {
                    "sent": "Of course, right in classification problems?",
                    "label": 0
                },
                {
                    "sent": "And in this case what wapnick insured when you suggested they suggest that there will be a natural way to control the complexity of such classes of functions.",
                    "label": 0
                },
                {
                    "sent": "So what they do they basically.",
                    "label": 0
                },
                {
                    "sent": "Project of this class of functions project this class of binary functions into.",
                    "label": 1
                },
                {
                    "sent": "The entire company Toryal Cube.",
                    "label": 0
                },
                {
                    "sent": "Right, So what will this guy ski right?",
                    "label": 0
                },
                {
                    "sent": "The components of this vector?",
                    "label": 0
                },
                {
                    "sent": "Here we are computing the values of the function at points X1, XN, and the components of this vector's components of these vectors are zero and one, so we are practices of combinatorial cuprite of just rightness of 01 to the power and right 01 to survive again of this cube and then.",
                    "label": 0
                },
                {
                    "sent": "We are counting how many work this is we encounter right when we when I France or the whole class right.",
                    "label": 0
                },
                {
                    "sent": "We are counting how many vertices are there right?",
                    "label": 0
                },
                {
                    "sent": "And this is something that is called often shattering number tracking number.",
                    "label": 0
                },
                {
                    "sent": "So we are kind of.",
                    "label": 0
                },
                {
                    "sent": "If you UFS file functions, afinsa classes, classifiers, you're counting in how many ways your set of classifiers can split the data right?",
                    "label": 0
                },
                {
                    "sent": "Can label the data with zeros and ones right, and this is what the cardinality of this class is about, right?",
                    "label": 0
                },
                {
                    "sent": "For the cardinality of this class is about and then the expectation of the Supreme observed Emma here process right abdominal process.",
                    "label": 0
                },
                {
                    "sent": "In this case it's controlled.",
                    "label": 0
                },
                {
                    "sent": "Also there will be a maximum of two terms.",
                    "label": 0
                },
                {
                    "sent": "One terms again corresponds to something sub Gaussian and you have here the.",
                    "label": 1
                },
                {
                    "sent": "Lucy, which is a largest variance into class right?",
                    "label": 0
                },
                {
                    "sent": "So that was a sick muscularis supremum of PF Square.",
                    "label": 0
                },
                {
                    "sent": "In this particular case, it will be kind of the largest probability to take one, right?",
                    "label": 0
                },
                {
                    "sent": "This is what Sigma Square is, right?",
                    "label": 0
                },
                {
                    "sent": "Cause big cause you are functions are essentially indicators, right?",
                    "label": 1
                },
                {
                    "sent": "So they take only way Lou Zero and one right?",
                    "label": 0
                },
                {
                    "sent": "So when you're computing PF squared, you're computing probabilities that F is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Right, the definite equal to 1, and this is the largest probability here, right?",
                    "label": 0
                },
                {
                    "sent": "And then you have expectation of the square root of the logarithm of the shattering number divided by, and this logging some replaces what was logarithm of capital N before right in the case of finite class we had just learning some of capital in here instead of this.",
                    "label": 0
                },
                {
                    "sent": "Now we have learning some of the shattering number.",
                    "label": 0
                },
                {
                    "sent": "Agree some of the shorter exam button.",
                    "label": 0
                },
                {
                    "sent": "Here we have kind of exponential or pop or Sony and type of behavior and we have.",
                    "label": 0
                },
                {
                    "sent": "Expectation of the Organism itself instead of of.",
                    "label": 0
                },
                {
                    "sent": "The office, for instance.",
                    "label": 0
                },
                {
                    "sent": "Proof.",
                    "label": 0
                },
                {
                    "sent": "It's immediate right to see that.",
                    "label": 0
                },
                {
                    "sent": "That if.",
                    "label": 0
                },
                {
                    "sent": "Expectation of the logarithm.",
                    "label": 0
                },
                {
                    "sent": "Is small comparing was then great if Expectational doing some small comparing gives ends and this implies that expectation.",
                    "label": 0
                },
                {
                    "sent": "Obviously supermom tends to 0 right?",
                    "label": 0
                },
                {
                    "sent": "Some idiot employees at expectation of this to promote and Susie, or just use Symmetrization inequality, right?",
                    "label": 0
                },
                {
                    "sent": "Reduce empirical processed order marker process and then use this bound right?",
                    "label": 0
                },
                {
                    "sent": "Use this bowl and then we will after we discuss a little bit more of tools including concentration inequality is we will see that this.",
                    "label": 0
                },
                {
                    "sent": "This also implies this that this will.",
                    "label": 0
                },
                {
                    "sent": "Hold with probability one, for instance right one can do this.",
                    "label": 0
                },
                {
                    "sent": "Also right that this converges almost surely.",
                    "label": 0
                },
                {
                    "sent": "And this was the initial serum of Botnick.",
                    "label": 0
                },
                {
                    "sent": "Insure one in case proved 40 years ago with different set of tools back then move in a more complicated ways and what we can do now right?",
                    "label": 0
                },
                {
                    "sent": "In fact they show that this is equivalent.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the what what nature when kislov large numbers.",
                    "label": 0
                },
                {
                    "sent": "If you want OK, actually you know one can over one can do more.",
                    "label": 0
                },
                {
                    "sent": "Isn't this one can show that if this is of the order.",
                    "label": 0
                },
                {
                    "sent": "For all four, four of between 1/2 and one, then this will be.",
                    "label": 0
                },
                {
                    "sent": "Will be observ order.",
                    "label": 0
                },
                {
                    "sent": "Of the order or small of fantasy power Alpha minus one, and this is also equivalent.",
                    "label": 0
                },
                {
                    "sent": "OK, there's another result that was proved in the 80s by, you know.",
                    "label": 0
                },
                {
                    "sent": "I was working on so only stuff so I had I had.",
                    "label": 0
                },
                {
                    "sent": "Kind of immersion of these wings was is being sufficient condition and I had a necessary condition that was a little bit worse and this was with some additional logarithm and then Michel Talagrand couple of years later got a clean result that those that you see here actually OK and the proof of the necessary part is very complicated so it says it was very hard.",
                    "label": 0
                },
                {
                    "sent": "With our problem to go another way and to show this so to prove this with convergence rate, this type of results with convergence rate it's more complicated than just using this inequality.",
                    "label": 0
                },
                {
                    "sent": "It requires some other tool sexually when even for for sufficiency part which is easier.",
                    "label": 0
                },
                {
                    "sent": "But anyway this is.",
                    "label": 1
                },
                {
                    "sent": "Bones in terms of shattering numbers, that is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very useful in classification problems.",
                    "label": 0
                },
                {
                    "sent": "OK, let me I have to go.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A little frustration.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, now in many cases.",
                    "label": 0
                },
                {
                    "sent": "Or what people are using to control the complexity of the class?",
                    "label": 0
                },
                {
                    "sent": "Is the notion of metric entropy is a notion of metric entropy and I will show you a couple of results of this nature.",
                    "label": 0
                },
                {
                    "sent": "So this number capital N here right is just the epsilon entropy of the epsilon covering number, right?",
                    "label": 0
                },
                {
                    "sent": "So epsilon covering number was a class F in the space L 2:00 PM, right?",
                    "label": 0
                },
                {
                    "sent": "So it's you know it uses.",
                    "label": 0
                },
                {
                    "sent": "Random.",
                    "label": 0
                },
                {
                    "sent": "Distance actually write random distances.",
                    "label": 0
                },
                {
                    "sent": "Definition of entropy so.",
                    "label": 0
                },
                {
                    "sent": "Basically what we are looking at we are looking at the minimal number of balls of radius epsilon in this space that are covering the class right?",
                    "label": 0
                },
                {
                    "sent": "And we have certain control on the minimal number of balls, right it's?",
                    "label": 0
                },
                {
                    "sent": "It's enough to write it for empirical distribution itself, but it's useless actually to write it in terms of empirical distribution itself cause becausw, since I want to control it almost surely right?",
                    "label": 0
                },
                {
                    "sent": "Almost surely I can never control it almost surely, unless I know that I can can actually put outside Supreme over all distributions, right?",
                    "label": 0
                },
                {
                    "sent": "I can replace PN by Q and take supremum overall distribution and these are.",
                    "label": 0
                },
                {
                    "sent": "So called uniform covering numbers, right?",
                    "label": 0
                },
                {
                    "sent": "So you are sleeping.",
                    "label": 0
                },
                {
                    "sent": "The entropy in the space L2 Q.",
                    "label": 0
                },
                {
                    "sent": "In fact right?",
                    "label": 0
                },
                {
                    "sent": "And this is enough to do this, just to have this bound for any Q right?",
                    "label": 0
                },
                {
                    "sent": "And then of course you have at 4:00 PM right?",
                    "label": 0
                },
                {
                    "sent": "You have it for P and and many things like like you know what nature wankas classes offsets are in this framework.",
                    "label": 0
                },
                {
                    "sent": "That's why I'm calling this PC type classes.",
                    "label": 0
                },
                {
                    "sent": "Also some extensions of public share one kiss.",
                    "label": 0
                },
                {
                    "sent": "Property to the classes of functions like the other various notions of we see some graph classes and things like this OK and things like this.",
                    "label": 0
                },
                {
                    "sent": "So basically what is important here is that there is 111 important parameter, this number V which is a picture when kids dimension.",
                    "label": 0
                },
                {
                    "sent": "OK, it in in the in the case of market share and kissed losses this becomes an ocean of both nature and anchors dimension right?",
                    "label": 0
                },
                {
                    "sent": "And the size of the supremum again is controlled by by two terms right.",
                    "label": 0
                },
                {
                    "sent": "One term corresponds to some kind of sub Gaussian behavior and the variance parameter Sigma is involved there, right?",
                    "label": 0
                },
                {
                    "sent": "And up to sort an extra logarithmic factor which is sharp actually right it's needed.",
                    "label": 0
                },
                {
                    "sent": "Here, right?",
                    "label": 0
                },
                {
                    "sent": "It's just the main part.",
                    "label": 0
                },
                {
                    "sent": "Is Sigma Times Square root of the dimension divided by N, right?",
                    "label": 0
                },
                {
                    "sent": "This is the type of the bound we are getting right as under his series.",
                    "label": 0
                },
                {
                    "sent": "Additional term which is observed that one over and right.",
                    "label": 0
                },
                {
                    "sent": "It's controlled by the uniform norm of U and this is this corresponds to the sub exponential type of behavior in all these bones is very similar pattern.",
                    "label": 0
                },
                {
                    "sent": "OK, when they are done correctly.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Election overseas in the case when entropies large much larger than four, about Nick in Sherwin and KISS classes.",
                    "label": 0
                },
                {
                    "sent": "So now I'm controlling, not not the covering number.",
                    "label": 0
                },
                {
                    "sent": "I'm controlling the logarithm of the covering number, right?",
                    "label": 0
                },
                {
                    "sent": "So look very similar, covering number so the covering number becomes very large in this case, right?",
                    "label": 0
                },
                {
                    "sent": "Very large in this case.",
                    "label": 0
                },
                {
                    "sent": "But still, if this exponent roll right that describes the growth of the entropy right when epsilon tends to 0, right?",
                    "label": 0
                },
                {
                    "sent": "How fast?",
                    "label": 0
                },
                {
                    "sent": "Pros this parameter or is not supposed to be too large, so we throw is between zero and one then still you have this type of inequalities again of a very similar structure, but of course those are great.",
                    "label": 0
                },
                {
                    "sent": "Ski are starting getting different right?",
                    "label": 0
                },
                {
                    "sent": "You are not getting any longer one over and in the first term still you have 1 / sqrt N But here you are getting something a little bit different it's.",
                    "label": 0
                },
                {
                    "sent": "Related those a, it was the fact that as entropies is like in this case is large.",
                    "label": 0
                },
                {
                    "sent": "OK, so",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a somewhat different example.",
                    "label": 0
                },
                {
                    "sent": "It's something that is.",
                    "label": 0
                },
                {
                    "sent": "That is useful in the sea of kernel machines.",
                    "label": 0
                },
                {
                    "sent": "OK, in this area of Carlisle machine.",
                    "label": 0
                },
                {
                    "sent": "So what we are looking at we're looking at at the symmetric non negative definite kernel semantic network, negatively definite kernel and it defines in a natural way of producing kernel Hilbert space, right?",
                    "label": 0
                },
                {
                    "sent": "Reproducing kernel Hilbert space and then what we are interested in.",
                    "label": 1
                },
                {
                    "sent": "We are interested in subset.",
                    "label": 0
                },
                {
                    "sent": "Hourglass is a subset of observable in this space.",
                    "label": 0
                },
                {
                    "sent": "You know for simplicity I'm taking unit bull.",
                    "label": 0
                },
                {
                    "sent": "OK, unit ball and then.",
                    "label": 0
                },
                {
                    "sent": "Oh oh, if we can, of course, associated with Karen, OK, an integral operator right in the usual way that acts from the space L2PP's our unknown distribution into the space L2P.",
                    "label": 0
                },
                {
                    "sent": "Right into itself and then Lambda, J are eigenvalues of this operator and this eigen values depend on the distribution P. Of course right there distribution dependent quantities.",
                    "label": 0
                },
                {
                    "sent": "OK, and then what is happening?",
                    "label": 0
                },
                {
                    "sent": "Is a desais of the observer Democratic process and this is very sharp.",
                    "label": 0
                },
                {
                    "sent": "Bones is a lower bound.",
                    "label": 0
                },
                {
                    "sent": "You know that essentially the same order is controlled by this interesting some, in which you're basically truncating eigenvalues, right?",
                    "label": 0
                },
                {
                    "sent": "Truncating eigenvalues of the integral operator at Level Sigma Square.",
                    "label": 1
                },
                {
                    "sent": "OK, you're gonna have some threshold Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "This is a maximal wagons.",
                    "label": 0
                },
                {
                    "sent": "The maximal L2 norm of functions in your class, and you're truncating eigenvalues, right?",
                    "label": 0
                },
                {
                    "sent": "You have still here and to the power minus one here 1/2, so it will be the order 1 / sqrt N. How precisely depends on Sigma.",
                    "label": 0
                },
                {
                    "sent": "Depends on how fast eigenvalues tend to 0.",
                    "label": 0
                },
                {
                    "sent": "Right, I'll precisely everything depends on Sigma.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you if you look at the case when when your kernel is of finite rank, OK, imagine that you are Colonel is of finite rank Sadie.",
                    "label": 0
                },
                {
                    "sent": "Which means that.",
                    "label": 0
                },
                {
                    "sent": "First D eigenvalues, right?",
                    "label": 0
                },
                {
                    "sent": "First the eigenvalues are one.",
                    "label": 0
                },
                {
                    "sent": "Then eigenvalue, subzero right.",
                    "label": 0
                },
                {
                    "sent": "Then eigenvalue subzero.",
                    "label": 0
                },
                {
                    "sent": "Then what you are going to end up with here you are going to end up with Sigma right?",
                    "label": 0
                },
                {
                    "sent": "If you do a quick computation your head you're going to end up with Sigma multiplied divided by square root of N right and multiplied by square root of D. So we will get Sigma Times Square root of the divided by N. Right, so everything will be controlled in terms observer and D of the kernel.",
                    "label": 0
                },
                {
                    "sent": "In this case right?",
                    "label": 0
                },
                {
                    "sent": "And this bound?",
                    "label": 0
                },
                {
                    "sent": "So this is what is happening in finite dimensional situation.",
                    "label": 0
                },
                {
                    "sent": "This is something very simple to prove actually.",
                    "label": 0
                },
                {
                    "sent": "I could leave it as an exercise to you to prove this bound.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "The idea is just to observe that.",
                    "label": 0
                },
                {
                    "sent": "As at this thing, right, the unit ball in the reproducing kernel Hilbert space in the space L2P is what?",
                    "label": 0
                },
                {
                    "sent": "What is the unit Bolinger producing kernel Hilbert space about?",
                    "label": 0
                },
                {
                    "sent": "When we look at, you know?",
                    "label": 0
                },
                {
                    "sent": "Look at this over when our view is the point of view of the object of the Hilbert space L2.",
                    "label": 0
                },
                {
                    "sent": "Boards are producing kernel Hilbert space becomes world's balls are.",
                    "label": 0
                },
                {
                    "sent": "What kind of sets?",
                    "label": 0
                },
                {
                    "sent": "Come on, tell me said squares.",
                    "label": 0
                },
                {
                    "sent": "You know rectangles of some guy and ellipsoids.",
                    "label": 0
                },
                {
                    "sent": "Well, the the bolts in reducing kernel Hilbert space are ellipsoids, right are going to be ellipsoids, and the axis of this ellipsoids are the eigenvalues, precisely OK.",
                    "label": 0
                },
                {
                    "sent": "This eigen values will be the access of this ellipsoids.",
                    "label": 0
                },
                {
                    "sent": "Now what we are doing here, we are taking ellipsoid right?",
                    "label": 0
                },
                {
                    "sent": "In this space L2P, which is our unit bowlines, reproducing kernel Hilbert space, it becomes just ellipsoid in the initial space.",
                    "label": 0
                },
                {
                    "sent": "And then, since Sigma Square is the largest as a largest.",
                    "label": 0
                },
                {
                    "sent": "So Sigma is like just L2 norm right of the function in our class.",
                    "label": 0
                },
                {
                    "sent": "What we're doing, we're intersecting this ellipsoid with the bowl of Radio Sigma, right?",
                    "label": 0
                },
                {
                    "sent": "So it was another ellipsoid whose access our Sigma right?",
                    "label": 0
                },
                {
                    "sent": "And when you intersect this two things right when you intersect those two things, it's easy to see that up to a constant, right up to a constant.",
                    "label": 0
                },
                {
                    "sent": "It will be about the same set, about the same set as ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "Whose axis are truncated this way?",
                    "label": 0
                },
                {
                    "sent": "OK, so up to a constant.",
                    "label": 0
                },
                {
                    "sent": "You can replace one set by another.",
                    "label": 0
                },
                {
                    "sent": "OK, it's very easy to see.",
                    "label": 0
                },
                {
                    "sent": "So, so if you take you take actually intersection of two ellipsoids, right, who's who's who's access are whose coordinates are aligned right, it should be ellipsoids with respect to the same coordinate system, right?",
                    "label": 0
                },
                {
                    "sent": "You take intersection of two ellipsoids, right?",
                    "label": 0
                },
                {
                    "sent": "Then said that you are getting right.",
                    "label": 0
                },
                {
                    "sent": "It says that you are getting can be can be roughly represented as ellipsoid, right?",
                    "label": 0
                },
                {
                    "sent": "Who's access are the minimum of the of the axis of this Tulip.",
                    "label": 0
                },
                {
                    "sent": "So it's OK.",
                    "label": 0
                },
                {
                    "sent": "This is what it says, right?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Roughly means that this set can be included sayin twice right?",
                    "label": 0
                },
                {
                    "sent": "Such ellipsoid, right?",
                    "label": 0
                },
                {
                    "sent": "The interception can be included in price, so it and will contain 1/2 of this ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's going to be multiplied by constant writers, convex bodies.",
                    "label": 0
                },
                {
                    "sent": "And this is what you are.",
                    "label": 0
                },
                {
                    "sent": "What you are getting here.",
                    "label": 0
                },
                {
                    "sent": "And if you do kind of very very direct computations and for ellipsoid things are easy to compute, you will end up with this bow.",
                    "label": 0
                },
                {
                    "sent": "OK, well that was about.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I think I'm a little bit behind the schedule, but let.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me make a break here.",
                    "label": 0
                },
                {
                    "sent": "OK, 10 minutes break OK?",
                    "label": 0
                }
            ]
        }
    }
}