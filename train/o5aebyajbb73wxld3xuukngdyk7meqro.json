{
    "id": "o5aebyajbb73wxld3xuukngdyk7meqro",
    "title": "Exploring the space of coding matrix classifiers for hierarchical multiclass text categorization",
    "info": {
        "author": [
            "Janez Brank, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Nov. 4, 2011",
        "recorded": "October 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Ensemble Methods",
            "Top->Computer Science->Text Mining",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/sikdd2011_brank_hierarchical/",
    "segmentation": [
        [
            "OK, so sorry about the delay.",
            "That's the title of my talk, so I'll be talking about coding matrix classifiers and OK in the context of hierarchical multiclass text categorization.",
            "But really, this is an interesting question by itself, regardless of what specific kind of problems we apply them to, so first."
        ],
        [
            "Bit of a bit of introduction, so we'll be dealing with multiclass classification problems, and more specifically with single level multiclass problems, so there are multiple classes, let's say K classes, But we want to assign each instance only to one of them and.",
            "One way to deal with that kind of problems is to transform the original multiclass problem into several binary.",
            "That is to say, two class classification problems.",
            "So we define several new binary problems.",
            "So for each of these new binary problems, we have to define what is the positive class and what is the negative class for this particular binary problem.",
            "And we will use.",
            "Basically, we will take some of the classes of the original problem and form their union, and that will be our new positive class and likewise will take.",
            "Some other classes or the original problem inform their union and that will be our new negative class and so now we have a positive and a negative negative class for a new binary classification problem and potentially some of these original classes might actually remain UN used for this particular new binary problem.",
            "So we define several binary problems like this.",
            "We train a binary classifier for each of them and then we have to combine the predictions of these binary classifiers.",
            "Through some sort of voting, perhaps with some weights or something of that sort, and we can combine their predictions to obtain predictions for the original K class problem.",
            "So that's basically a way to transform a multiclass problem into several binary problems, and this can be useful because some machine learning methods are originally more suitable for binary problems, and so forth now."
        ],
        [
            "So we come to coding matrices.",
            "So basically a coding matrix is a kind of nice, formal, explicit way of describing the relationship between the original K classes of the multiclass problem that we started with, and our new binary problems.",
            "Let's say that we have M binary problems at the end and we will describe this mapping from the original classes to our new problems with the coding matrix.",
            "So basically the matrix has one row for each class and one column for each of the new problems.",
            "Maybe I can draw this.",
            "So basically we have.",
            "We have a K rows, one for each class and we have M columns, one for each new binary problem and the entry at the intersection of this this row in this column can have the value plus 1 -- 1 or 0.",
            "And it tells us how this particular original class was used in the definition of this particular new binary problem.",
            "So it was either used in the negative class or in the positive class, or it was not used at all, so that each each column describes one of the new binary problems, and each row tells us how a particular class of the original problem was used in these new problems that we have been developing.",
            "And.",
            "Just for illustration, some typical approaches for transforming a multiclass problem into several binary problems.",
            "So one well known approaches one versus others.",
            "So we're basically we have a kind of diagonal matrix.",
            "There is a plus one along the diagonal and minus ones everywhere else.",
            "So basically each column says one of the original classes is positive and everything else is negative, so that each of the new binary classifiers will help us distinguish one of the original classes from everything else.",
            "So that's one well known approach.",
            "So another well known approach is called one versus one.",
            "So here we have a lot more columns.",
            "Actually.",
            "Then there are classes we have basically one column for each pair of the original classes, and in that particular column one of the classes is used as positive, the other is used as negative and the remaining classes aren't used at all.",
            "So basically in this scenario, each of the new binary classifiers helps us distinguish two of the original classes and ignores the rest.",
            "And then we hope that their votes in the end will come up to useful prediction of the original problem.",
            "We could even be exhausted.",
            "We could have exponentially many columns in the matrix corresponding to each possible partition of the original set of classes into a positive or a negative.",
            "Or we can be more clever.",
            "People have used error correcting output codes from information theory, which have some desirable theoretical properties and so forth, but these are just illustrations.",
            "The point is the decoding matrices are basically generalization of all these approaches and allow us to describe them all in the same framework.",
            "But of course there are also.",
            "A lot more coding matrices than just the ones we've described here.",
            "The space of all possible matrices is exponentially large in the number of classes in the number of new problems, and we'd like to explore it a little bit.",
            "So each matrix gives rise to classifier of the original problem.",
            "Or in other words, each metrics gives rise to an ensemble of binary classifiers, which can then be used for the original problem, and so we'd like to get to know this space a little better and to see how the performance of the classifiers of the matrices.",
            "Is related if it is related to some other interesting and easily observable properties of the metrics.",
            "So that's basically what we're going to do, and we're going to do this empirically on one particular relatively small data set where we can investigate these things a bit more thoroughly."
        ],
        [
            "Oh, so here's the here's the scenario we'll be working on.",
            "We have a very small problem multiclass problem with seven classes arranged in a hierarchy.",
            "This is basically a small subset of the demos Open Directory project hierarchy selected so that it's all very manageable and.",
            "Because we have a hierarchy, we can introduce an additional constraint regarding our coding matrix.",
            "Basically, if we use a class as positive in a particular binary problem that we are designing, we don't want its subclasses, its descendants in the hierarchy to be negative, because that would be kind of conflict.",
            "So with the addition of this constraint, the number of possible different columns of the matrix is considerably reduced.",
            "So originally you might say we have if there are three possibilities for each entry in the column, and the column consists of.",
            "Of K entries, then we have three to the K possible columns and OK. Then he could discard the ones which contain only positive numbers are only negative numbers because those aren't really defining a useful problem and you could say that if two columns differ only in science, and everything else is the same, then we don't need to distinguish them, because it's really the same problem with labels reversed.",
            "But with this additional constraint, the number of possible columns is still further reduced, so that in the end for this particular small problem of seven classes we end up with just.",
            "36 different possible columns.",
            "So basically our metrics any of these, any of the columns of the matrix is one of the 36 possible columns, so for this particular small data set, things start to become a little bit more manageable.",
            "So if you Additionally say that we don't want multiple columns to be the same to each other, because then we'll just have multiple copies of the same classifier in our ensemble, and that doesn't really make sense.",
            "We could wait them later if we wanted them to.",
            "So basically that means they're just 36 choose M. Possible matrices of M columns and here.",
            "Here are just a few examples how these numbers grow.",
            "Of course they grow exponentially for awhile, but the idea is these numbers are still sort of manageable.",
            "We aren't talking about 10 to 100 matrixes, we're talking about a few millions.",
            "Mattress is so we can actually still afford with a bit of care to examine them exhaustively for quite a lot of values of M, Whereas for the kind of the intermediate values of M where the number does get intractable where we are talking about hundreds of millions at that point.",
            "We can still at least random randomly sample a decent subset of the matrices so that we'll get a rough idea of the statistical properties of the space behind this.",
            "We will evaluate each metrics that we're dealing with with with with the evaluation score called the average card score.",
            "Basically, since our predictions will be into a hierarchy, we don't just want to ask whether a certain instance has been classified correctly or not, because not all the mistakes are equally bad.",
            "So if you, for example, if a matrix belongs here, if an instance belongs here and we classified it into into apparent, that's less of a mistake, then if we classified it into a distant color into a distant cousin.",
            "So basically the Jaccard score takes that into account, but we can skip the details.",
            "In the end we get a measure between zero and one for the average kind of classification performance, or the metrics and higher measures are better.",
            "That's what we need to know.",
            "Oh OK."
        ],
        [
            "So let's let's start looking at some interesting results.",
            "We might ask ourselves if I look at if I look at all all matrices of M columns, what is the average performance over these matrices?",
            "Or what is the median performance?",
            "What is the best performance?",
            "So each of these numbers is interesting in a certain way.",
            "The best tells you what's possible if you really took the trouble to search through the space very carefully to find the very best metrics, the median tells you.",
            "Basically, if you choose the metrics at random, it's likely to be at least as good as this, so that's also an interesting number.",
            "Or maybe if you look at several matrices.",
            "And look at the average.",
            "The average is again something interesting.",
            "So here are some results.",
            "We have the number of columns on the on the horizontal axis, and here is the performance score that I mentioned earlier.",
            "The average Jaccard score.",
            "So here we have for each number of columns the average performance overall matrices of that of that number of columns, and the bars show the standard deviation.",
            "So that shows us how.",
            "OK if you exaggerate with really two few columns, you won't be able to achieve good performance, but pretty soon.",
            "The relatively small number of columns still smaller than the number of classes.",
            "Remember, we had seven classes.",
            "You can already start achieving some pretty good performance and what happens is you look at wider and wider metric is so as you allow more and more classifiers in your ensemble there.",
            "On average all of these matrices turn out to perform more and more similarly, so the standard deviation decreases.",
            "Here's a very interesting chart showing the maximum when the median performance over all columns.",
            "Overall matrices of EM columns.",
            "So that shows us that the median performance kinda gross for a while and eventually it gets to the point where pretty much any random metrics you would choose will perform pretty well, but the maximum is also very interesting.",
            "It shows us how already with four columns that's much fewer than the number of classes, right?",
            "We had seven classes, but already with four columns in the Matrix you can achieve performance that is almost as good as the best metrics altogether, which we have here at 7 or 8 columns.",
            "So basically tells you that a smaller number of columns is still enough to achieve good performance, but of course as it turns out, it's harder to find that very good metrics among those with four columns, because it's basically is 2 standard deviations above the average, so it's harder to find it, whereas the wider mattress is.",
            "You look at the easier it is to find something good, and it also shows us that if we look at too many, if we take too many columns in our metrics and the best possible performance we can achieve is actually a little worse.",
            "Then at this kind of intermediate at matrixes of intermediate with like 789 columns.",
            "So this is in a way somewhat optimistic result.",
            "I mean, we can see that good matrices with few columns are possible, and then that's desirable.",
            "You want to have few columns in your metrics because you don't want to have to train a huge ensemble, so.",
            "If you can only find it, it does exist.",
            "You know the other hand.",
            "If we can afford a lot of columns, then pretty much we don't have to worry.",
            "What sort of metrics we choose, we can just generate it randomly and almost all of them are pretty good.",
            "It would speculate.",
            "Would you say that this this would appear over several data sets the same?",
            "Yeah, I speculate that the rough ideas would probably be the same, but it's very interesting how the details match on other datasets.",
            "That's something that I will hopefully get to do at some point, because these results are kind of interesting.",
            "This reminds me on, you know this.",
            "Also interesting pretty much the same shape of the curve when they're doing active learning and with only extremely small number of answers we get already.",
            "We can get well in some cases.",
            "Extremely good classifier.",
            "Then it goes down the quality, but probably robustness is not really the.",
            "You might say that we have a certain robustness because with a lot of columns you can pick pretty much any metrics and it will be good.",
            "But with a small number of columns you have to choose the metrics very carefully if you want to perform well.",
            "So there is a certain robustness that you gain by allowing more colors on the left side.",
            "On the left or you don't get that.",
            "That's where you like the robustness exactly.",
            "So the idea is this was kind of I was basically the motivation was to ask yourself is it worth trying to, you know, go to the trouble of looking carefully for a metrics with a small number of columns and so the question is, does good metrics with a small number of columns even exist?",
            "So here we see that they do exist, but they are hard to find."
        ],
        [
            "Oh another yeah.",
            "So suppose that we would choose an M column metrics at random and observe its performance scores.",
            "So since we chose it at random, we can think of this score also was kind of random variable.",
            "Then we might ask ourselves what is the distribution of this of this random variable?",
            "Depending on the number of columns.",
            "So in the previous slide we saw actually some characteristics of those distributions we saw that averages standard deviations, medians and maximum.",
            "Here we see the distributions themselves as a histogram.",
            "For a few values of M so OK, this few histograms don't look nice and smooth simply because there are not that many matrices of this few columns to allow for a smooth histogram.",
            "But as we will take quite the metric, is there are more of them in the histogram get smoother, but even here we can already see certain trends for the more columns we allow the.",
            "So these are our scores of the of the performance of the mattress is and this is the probability density.",
            "Basically in our histogram.",
            "So the more columns you allow, the more this whole distribution.",
            "Is moving towards higher scores and the more frequently you encounter matrices with good scores, so actually there is a nice difference between the two.",
            "So this looks like once you're at five, well, that's in the yeah, you might say.",
            "Ability performance.",
            "Yeah, you move anymore.",
            "That's that's basically illustrates what we already saw in the previous slide.",
            "The maximum is practically achieved at four and doesn't doesn't increase past that, but more and more probability mass gets close to the maximum this mode is.",
            "This madisar peak is moving gradually more and more to the right, and it's higher and higher.",
            "We'll see."
        ],
        [
            "Other chart of that sort on the on the next slide.",
            "So here we have even wider metric is 1015 columns, 2030 columns and we see this same trend all the time, so the maximum doesn't budg.",
            "But this peak keeps moving to the right and more and more matrices are in this in this area of good performance.",
            "So here we see this sort of."
        ],
        [
            "Things quantitatively now I was I was looking for some sort of theoretically well defined distribution that would roughly match the shape of these empirical distributions that we saw in the previous slide, and I decided to try the better distribution.",
            "We can use the method of moments to estimate the better para meters Alpha and beta, which of course will depend on the number of columns we're looking at on M and here are.",
            "So here are examples.",
            "So here we have the empirical distribution.",
            "The blue line is the empirical distribution, the same as we saw in the previous slide, and this is the best beta distribution fitted over the empirical one.",
            "So as it turns out, for smaller values of N, the fit isn't that great.",
            "Basically the actual empirical distribution leans a bit more to the right, and it has a higher peak.",
            "But as you look at wider and wider matrices.",
            "The better fits increasingly good to the empirical distribution."
        ],
        [
            "And we can ask ourselves, how do the so we got this para meters Alpha and beta estimated for each value of M and we can look at them as a function of M. So this chart shows us know that there's a logarithmic scale here, so it's interesting that there are several pretty wide ranges where the.",
            "Para meters were, the line is basically almost a straight line, but with the logarithmic scale, that really means that over these ranges like this range.",
            "With this range, the Alpha and better parameters are approximately exponential functions of the number of columns.",
            "Higher values of health and better also reflect the fact that the standard deviation is narrower, and we saw that on the previous slides as well."
        ],
        [
            "OK, so another interesting question.",
            "So when you when you try to construct coding metrics you would ideally want some useful properties to guide you so that hopefully if you construct a matrix that has those properties, it will turn out to perform well as well.",
            "That's what we would like, and one well known property that is often recommended is that we don't want the rows of the columns.",
            "The rows of the matrix to be too similar to each other and we likewise don't want the columns of the matrix to be too similar to each other.",
            "There are some good theoretical arguments for that.",
            "So for example, if two rows are similar, well basically if you look if you have an instance from a certain class, then we would expect the binary classifiers in our ensemble to make predictions for this particular instance, which are similar to the values of the of the row in the in the code metrics that corresponds to the class from which this instance was taken, and so if several rows are very similar, then we would expect our ensemble to make very similar predictions.",
            "Two instances from those classes, and so we won't be able to tell them apart very efficiently.",
            "We will make many confusions between them and similarly if two columns are too similar, then we have two very similar binary problems and will get very similar binary classifiers.",
            "So when when one of them will make a mistake, probably the other one will also and that will again make decoding hardware make it more likely that there will be enough mistakes that we will end up matching our row of predictions to the wrong row of the matrix.",
            "So basically, it's well known that it's a desirable characteristic that the rows and columns aren't too similar to each other.",
            "We decided to measure this with basically the average Hamming distance over all pairs of rows, or the average Hamming distance over all pairs of columns.",
            "So we get something that we call the row separation or column separation.",
            "So the question is, how are these separations measures correlated to the performance of the metrics as measured by the Jaccard score that we described earlier?"
        ],
        [
            "So here we have a few charts about this, so here we have a scatter plot for metrics of four columns.",
            "Here we have the Jacquard scored, the classification performance of the metrics and here we have the column separation and here likewise for the row separation in the lower 2 charts show the same thing, but for metric is off 7 columns.",
            "Remember that's the ones where we notice that the best performing matrixes are also found.",
            "So the interesting observation here is on the one hand, it's certainly true that the metrics that have good.",
            "Column separation or good role separation tend to perform better than the ones which have poor column separation or poor low separation, but also very interesting Lee if you look at the matrices that maximize the column separation or the row separation, these are not actually the mattress.",
            "Is that maximize the performance because some of the best performing matrices are somewhat to the right of them.",
            "So basically they perform better even though they have poorer or color separation.",
            "Now OK here we don't have all the metrics is shown because there are too many of them, but it's a random sample of 10,000.",
            "Mattress is so I think it's still quite representative, so it tells us that on the one hand, certainly Roman current separation are good properties and good guidelines, but simply going out to maximize them is not actually what you if you really want to get maximum performance from your mattress is."
        ],
        [
            "Enter OK, so one thing that's pretty much the last thing I looked at, so we know that the matrix defines basically a set of new binary problems and then also set of new and a sample of new binary classifiers, and so the question that might occur to us is is there any relationship between how good this individual binary classifiers are and how good the ensemble as a whole is?",
            "So might we want to focus when we are constructing a metrics in a real problem?",
            "Where we can just exhaustively enumerate them all?",
            "Might it be good to focus on mattress is with columns that lead themselves to producing better individual binary classifiers.",
            "So for this test we computed the F1 measure and the area under the RC curve for each of the individual binary problems relative to its own individual binary classification task.",
            "And then we looked at the average of these numbers.",
            "The average F1 or the average area under curve over all the classifiers in the example.",
            "And we looked at whether this averages then correlated to the performance of the ensemble as a whole.",
            "The classification performance is measured by the Jaccard score that we mentioned earlier, and the result is not really correlated.",
            "So here are some correlation coefficients.",
            "This phone is practically completely unrelated.",
            "Here is an extremely weak correlation that isn't really useful.",
            "So in other words, when you're constructing a matrix, it isn't really particularly important that the individual classifiers the binary classifiers in the in the new problem defined biometrics.",
            "Are very good by themselves.",
            "What matters is how the matrix combines them.",
            "So that's kind of a bit of a negative result, but on the other hand, also good because it means you don't really have to pay it."
        ],
        [
            "Into this, when you're constructing your metrics so the conclusions.",
            "Basically, these are things that I've already said earlier during the individual slides, we saw that the best matrixes were found at 7 or 8 columns, but already with much fewer columns.",
            "We can still find metrics that are almost equally good, but there are much more rare so they're harder to find, but it's it's a desirable thing, because ideally you want an ensemble with few few classifiers, especially if your original set of classes is higher.",
            "So it might be worth looking for them as they do exist and we also saw that if you can afford a larger number of classifiers senior assemble, then you can pretty much pick any metrics you like, 'cause most of them are good and the standard deviation is very small.",
            "We saw that the distribution of metrics courses approximately like a beta distribution with a few aberrations for small values of M and the parameters are approximately exponentially in M over large values of M. We saw that row and column separation are useful properties, but.",
            "That actually maximizing them is not is not the best idea if you want to look for the metrics with the best performance.",
            "And we also saw the quality of individual classifiers in our ensemble.",
            "The individual binary classifiers is not actually correlated with the performance of the entire matrix.",
            "So for future work, like I said earlier, it would be good to test these things and more data sets, including some larger datasets with more classes where of course we couldn't afford to testing exhaustively like we did here, but would have to focus more on sampling and perhaps investigate some other easily computable metrics properties similar to the way we did with Rowan.",
            "Separation because ideally you want to find some matches and properties which you will be able to use while constructing the metrics, and that will hopefully lead you to good metrics is without you having to test things on your data all the time.",
            "That concludes my presentation, thank you.",
            "So this.",
            "This finding, I guess, relates with random projections in a way there.",
            "Basically the idea is just just go out there, find some stuff.",
            "And yeah, the idea is kind of the quality of the ensemble comes from the way that you are combining different predictions, not from the idea that the individual predictions are good.",
            "Basically the statistics will help you without the mistakes of the individual binary classifiers, so that's kind of there was this sort of single.",
            "This theoretically kind of known, but here we have some empirical confirmation of it.",
            "Another comment.",
            "So thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so sorry about the delay.",
                    "label": 0
                },
                {
                    "sent": "That's the title of my talk, so I'll be talking about coding matrix classifiers and OK in the context of hierarchical multiclass text categorization.",
                    "label": 1
                },
                {
                    "sent": "But really, this is an interesting question by itself, regardless of what specific kind of problems we apply them to, so first.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bit of a bit of introduction, so we'll be dealing with multiclass classification problems, and more specifically with single level multiclass problems, so there are multiple classes, let's say K classes, But we want to assign each instance only to one of them and.",
                    "label": 0
                },
                {
                    "sent": "One way to deal with that kind of problems is to transform the original multiclass problem into several binary.",
                    "label": 1
                },
                {
                    "sent": "That is to say, two class classification problems.",
                    "label": 0
                },
                {
                    "sent": "So we define several new binary problems.",
                    "label": 0
                },
                {
                    "sent": "So for each of these new binary problems, we have to define what is the positive class and what is the negative class for this particular binary problem.",
                    "label": 1
                },
                {
                    "sent": "And we will use.",
                    "label": 0
                },
                {
                    "sent": "Basically, we will take some of the classes of the original problem and form their union, and that will be our new positive class and likewise will take.",
                    "label": 0
                },
                {
                    "sent": "Some other classes or the original problem inform their union and that will be our new negative class and so now we have a positive and a negative negative class for a new binary classification problem and potentially some of these original classes might actually remain UN used for this particular new binary problem.",
                    "label": 0
                },
                {
                    "sent": "So we define several binary problems like this.",
                    "label": 1
                },
                {
                    "sent": "We train a binary classifier for each of them and then we have to combine the predictions of these binary classifiers.",
                    "label": 0
                },
                {
                    "sent": "Through some sort of voting, perhaps with some weights or something of that sort, and we can combine their predictions to obtain predictions for the original K class problem.",
                    "label": 0
                },
                {
                    "sent": "So that's basically a way to transform a multiclass problem into several binary problems, and this can be useful because some machine learning methods are originally more suitable for binary problems, and so forth now.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we come to coding matrices.",
                    "label": 0
                },
                {
                    "sent": "So basically a coding matrix is a kind of nice, formal, explicit way of describing the relationship between the original K classes of the multiclass problem that we started with, and our new binary problems.",
                    "label": 1
                },
                {
                    "sent": "Let's say that we have M binary problems at the end and we will describe this mapping from the original classes to our new problems with the coding matrix.",
                    "label": 1
                },
                {
                    "sent": "So basically the matrix has one row for each class and one column for each of the new problems.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can draw this.",
                    "label": 0
                },
                {
                    "sent": "So basically we have.",
                    "label": 1
                },
                {
                    "sent": "We have a K rows, one for each class and we have M columns, one for each new binary problem and the entry at the intersection of this this row in this column can have the value plus 1 -- 1 or 0.",
                    "label": 0
                },
                {
                    "sent": "And it tells us how this particular original class was used in the definition of this particular new binary problem.",
                    "label": 0
                },
                {
                    "sent": "So it was either used in the negative class or in the positive class, or it was not used at all, so that each each column describes one of the new binary problems, and each row tells us how a particular class of the original problem was used in these new problems that we have been developing.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Just for illustration, some typical approaches for transforming a multiclass problem into several binary problems.",
                    "label": 0
                },
                {
                    "sent": "So one well known approaches one versus others.",
                    "label": 0
                },
                {
                    "sent": "So we're basically we have a kind of diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "There is a plus one along the diagonal and minus ones everywhere else.",
                    "label": 0
                },
                {
                    "sent": "So basically each column says one of the original classes is positive and everything else is negative, so that each of the new binary classifiers will help us distinguish one of the original classes from everything else.",
                    "label": 0
                },
                {
                    "sent": "So that's one well known approach.",
                    "label": 0
                },
                {
                    "sent": "So another well known approach is called one versus one.",
                    "label": 0
                },
                {
                    "sent": "So here we have a lot more columns.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Then there are classes we have basically one column for each pair of the original classes, and in that particular column one of the classes is used as positive, the other is used as negative and the remaining classes aren't used at all.",
                    "label": 1
                },
                {
                    "sent": "So basically in this scenario, each of the new binary classifiers helps us distinguish two of the original classes and ignores the rest.",
                    "label": 0
                },
                {
                    "sent": "And then we hope that their votes in the end will come up to useful prediction of the original problem.",
                    "label": 0
                },
                {
                    "sent": "We could even be exhausted.",
                    "label": 1
                },
                {
                    "sent": "We could have exponentially many columns in the matrix corresponding to each possible partition of the original set of classes into a positive or a negative.",
                    "label": 0
                },
                {
                    "sent": "Or we can be more clever.",
                    "label": 0
                },
                {
                    "sent": "People have used error correcting output codes from information theory, which have some desirable theoretical properties and so forth, but these are just illustrations.",
                    "label": 0
                },
                {
                    "sent": "The point is the decoding matrices are basically generalization of all these approaches and allow us to describe them all in the same framework.",
                    "label": 1
                },
                {
                    "sent": "But of course there are also.",
                    "label": 1
                },
                {
                    "sent": "A lot more coding matrices than just the ones we've described here.",
                    "label": 0
                },
                {
                    "sent": "The space of all possible matrices is exponentially large in the number of classes in the number of new problems, and we'd like to explore it a little bit.",
                    "label": 0
                },
                {
                    "sent": "So each matrix gives rise to classifier of the original problem.",
                    "label": 0
                },
                {
                    "sent": "Or in other words, each metrics gives rise to an ensemble of binary classifiers, which can then be used for the original problem, and so we'd like to get to know this space a little better and to see how the performance of the classifiers of the matrices.",
                    "label": 0
                },
                {
                    "sent": "Is related if it is related to some other interesting and easily observable properties of the metrics.",
                    "label": 0
                },
                {
                    "sent": "So that's basically what we're going to do, and we're going to do this empirically on one particular relatively small data set where we can investigate these things a bit more thoroughly.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh, so here's the here's the scenario we'll be working on.",
                    "label": 0
                },
                {
                    "sent": "We have a very small problem multiclass problem with seven classes arranged in a hierarchy.",
                    "label": 0
                },
                {
                    "sent": "This is basically a small subset of the demos Open Directory project hierarchy selected so that it's all very manageable and.",
                    "label": 0
                },
                {
                    "sent": "Because we have a hierarchy, we can introduce an additional constraint regarding our coding matrix.",
                    "label": 0
                },
                {
                    "sent": "Basically, if we use a class as positive in a particular binary problem that we are designing, we don't want its subclasses, its descendants in the hierarchy to be negative, because that would be kind of conflict.",
                    "label": 1
                },
                {
                    "sent": "So with the addition of this constraint, the number of possible different columns of the matrix is considerably reduced.",
                    "label": 0
                },
                {
                    "sent": "So originally you might say we have if there are three possibilities for each entry in the column, and the column consists of.",
                    "label": 0
                },
                {
                    "sent": "Of K entries, then we have three to the K possible columns and OK. Then he could discard the ones which contain only positive numbers are only negative numbers because those aren't really defining a useful problem and you could say that if two columns differ only in science, and everything else is the same, then we don't need to distinguish them, because it's really the same problem with labels reversed.",
                    "label": 0
                },
                {
                    "sent": "But with this additional constraint, the number of possible columns is still further reduced, so that in the end for this particular small problem of seven classes we end up with just.",
                    "label": 0
                },
                {
                    "sent": "36 different possible columns.",
                    "label": 0
                },
                {
                    "sent": "So basically our metrics any of these, any of the columns of the matrix is one of the 36 possible columns, so for this particular small data set, things start to become a little bit more manageable.",
                    "label": 0
                },
                {
                    "sent": "So if you Additionally say that we don't want multiple columns to be the same to each other, because then we'll just have multiple copies of the same classifier in our ensemble, and that doesn't really make sense.",
                    "label": 0
                },
                {
                    "sent": "We could wait them later if we wanted them to.",
                    "label": 1
                },
                {
                    "sent": "So basically that means they're just 36 choose M. Possible matrices of M columns and here.",
                    "label": 0
                },
                {
                    "sent": "Here are just a few examples how these numbers grow.",
                    "label": 0
                },
                {
                    "sent": "Of course they grow exponentially for awhile, but the idea is these numbers are still sort of manageable.",
                    "label": 0
                },
                {
                    "sent": "We aren't talking about 10 to 100 matrixes, we're talking about a few millions.",
                    "label": 0
                },
                {
                    "sent": "Mattress is so we can actually still afford with a bit of care to examine them exhaustively for quite a lot of values of M, Whereas for the kind of the intermediate values of M where the number does get intractable where we are talking about hundreds of millions at that point.",
                    "label": 0
                },
                {
                    "sent": "We can still at least random randomly sample a decent subset of the matrices so that we'll get a rough idea of the statistical properties of the space behind this.",
                    "label": 0
                },
                {
                    "sent": "We will evaluate each metrics that we're dealing with with with with the evaluation score called the average card score.",
                    "label": 0
                },
                {
                    "sent": "Basically, since our predictions will be into a hierarchy, we don't just want to ask whether a certain instance has been classified correctly or not, because not all the mistakes are equally bad.",
                    "label": 0
                },
                {
                    "sent": "So if you, for example, if a matrix belongs here, if an instance belongs here and we classified it into into apparent, that's less of a mistake, then if we classified it into a distant color into a distant cousin.",
                    "label": 0
                },
                {
                    "sent": "So basically the Jaccard score takes that into account, but we can skip the details.",
                    "label": 0
                },
                {
                    "sent": "In the end we get a measure between zero and one for the average kind of classification performance, or the metrics and higher measures are better.",
                    "label": 0
                },
                {
                    "sent": "That's what we need to know.",
                    "label": 0
                },
                {
                    "sent": "Oh OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's let's start looking at some interesting results.",
                    "label": 0
                },
                {
                    "sent": "We might ask ourselves if I look at if I look at all all matrices of M columns, what is the average performance over these matrices?",
                    "label": 1
                },
                {
                    "sent": "Or what is the median performance?",
                    "label": 0
                },
                {
                    "sent": "What is the best performance?",
                    "label": 0
                },
                {
                    "sent": "So each of these numbers is interesting in a certain way.",
                    "label": 0
                },
                {
                    "sent": "The best tells you what's possible if you really took the trouble to search through the space very carefully to find the very best metrics, the median tells you.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you choose the metrics at random, it's likely to be at least as good as this, so that's also an interesting number.",
                    "label": 0
                },
                {
                    "sent": "Or maybe if you look at several matrices.",
                    "label": 0
                },
                {
                    "sent": "And look at the average.",
                    "label": 0
                },
                {
                    "sent": "The average is again something interesting.",
                    "label": 0
                },
                {
                    "sent": "So here are some results.",
                    "label": 0
                },
                {
                    "sent": "We have the number of columns on the on the horizontal axis, and here is the performance score that I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "The average Jaccard score.",
                    "label": 0
                },
                {
                    "sent": "So here we have for each number of columns the average performance overall matrices of that of that number of columns, and the bars show the standard deviation.",
                    "label": 0
                },
                {
                    "sent": "So that shows us how.",
                    "label": 0
                },
                {
                    "sent": "OK if you exaggerate with really two few columns, you won't be able to achieve good performance, but pretty soon.",
                    "label": 0
                },
                {
                    "sent": "The relatively small number of columns still smaller than the number of classes.",
                    "label": 0
                },
                {
                    "sent": "Remember, we had seven classes.",
                    "label": 0
                },
                {
                    "sent": "You can already start achieving some pretty good performance and what happens is you look at wider and wider metric is so as you allow more and more classifiers in your ensemble there.",
                    "label": 0
                },
                {
                    "sent": "On average all of these matrices turn out to perform more and more similarly, so the standard deviation decreases.",
                    "label": 0
                },
                {
                    "sent": "Here's a very interesting chart showing the maximum when the median performance over all columns.",
                    "label": 0
                },
                {
                    "sent": "Overall matrices of EM columns.",
                    "label": 0
                },
                {
                    "sent": "So that shows us that the median performance kinda gross for a while and eventually it gets to the point where pretty much any random metrics you would choose will perform pretty well, but the maximum is also very interesting.",
                    "label": 0
                },
                {
                    "sent": "It shows us how already with four columns that's much fewer than the number of classes, right?",
                    "label": 0
                },
                {
                    "sent": "We had seven classes, but already with four columns in the Matrix you can achieve performance that is almost as good as the best metrics altogether, which we have here at 7 or 8 columns.",
                    "label": 0
                },
                {
                    "sent": "So basically tells you that a smaller number of columns is still enough to achieve good performance, but of course as it turns out, it's harder to find that very good metrics among those with four columns, because it's basically is 2 standard deviations above the average, so it's harder to find it, whereas the wider mattress is.",
                    "label": 0
                },
                {
                    "sent": "You look at the easier it is to find something good, and it also shows us that if we look at too many, if we take too many columns in our metrics and the best possible performance we can achieve is actually a little worse.",
                    "label": 0
                },
                {
                    "sent": "Then at this kind of intermediate at matrixes of intermediate with like 789 columns.",
                    "label": 0
                },
                {
                    "sent": "So this is in a way somewhat optimistic result.",
                    "label": 0
                },
                {
                    "sent": "I mean, we can see that good matrices with few columns are possible, and then that's desirable.",
                    "label": 0
                },
                {
                    "sent": "You want to have few columns in your metrics because you don't want to have to train a huge ensemble, so.",
                    "label": 0
                },
                {
                    "sent": "If you can only find it, it does exist.",
                    "label": 0
                },
                {
                    "sent": "You know the other hand.",
                    "label": 0
                },
                {
                    "sent": "If we can afford a lot of columns, then pretty much we don't have to worry.",
                    "label": 0
                },
                {
                    "sent": "What sort of metrics we choose, we can just generate it randomly and almost all of them are pretty good.",
                    "label": 0
                },
                {
                    "sent": "It would speculate.",
                    "label": 0
                },
                {
                    "sent": "Would you say that this this would appear over several data sets the same?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I speculate that the rough ideas would probably be the same, but it's very interesting how the details match on other datasets.",
                    "label": 0
                },
                {
                    "sent": "That's something that I will hopefully get to do at some point, because these results are kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "This reminds me on, you know this.",
                    "label": 0
                },
                {
                    "sent": "Also interesting pretty much the same shape of the curve when they're doing active learning and with only extremely small number of answers we get already.",
                    "label": 0
                },
                {
                    "sent": "We can get well in some cases.",
                    "label": 0
                },
                {
                    "sent": "Extremely good classifier.",
                    "label": 0
                },
                {
                    "sent": "Then it goes down the quality, but probably robustness is not really the.",
                    "label": 0
                },
                {
                    "sent": "You might say that we have a certain robustness because with a lot of columns you can pick pretty much any metrics and it will be good.",
                    "label": 0
                },
                {
                    "sent": "But with a small number of columns you have to choose the metrics very carefully if you want to perform well.",
                    "label": 0
                },
                {
                    "sent": "So there is a certain robustness that you gain by allowing more colors on the left side.",
                    "label": 0
                },
                {
                    "sent": "On the left or you don't get that.",
                    "label": 0
                },
                {
                    "sent": "That's where you like the robustness exactly.",
                    "label": 0
                },
                {
                    "sent": "So the idea is this was kind of I was basically the motivation was to ask yourself is it worth trying to, you know, go to the trouble of looking carefully for a metrics with a small number of columns and so the question is, does good metrics with a small number of columns even exist?",
                    "label": 0
                },
                {
                    "sent": "So here we see that they do exist, but they are hard to find.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh another yeah.",
                    "label": 0
                },
                {
                    "sent": "So suppose that we would choose an M column metrics at random and observe its performance scores.",
                    "label": 1
                },
                {
                    "sent": "So since we chose it at random, we can think of this score also was kind of random variable.",
                    "label": 1
                },
                {
                    "sent": "Then we might ask ourselves what is the distribution of this of this random variable?",
                    "label": 1
                },
                {
                    "sent": "Depending on the number of columns.",
                    "label": 0
                },
                {
                    "sent": "So in the previous slide we saw actually some characteristics of those distributions we saw that averages standard deviations, medians and maximum.",
                    "label": 0
                },
                {
                    "sent": "Here we see the distributions themselves as a histogram.",
                    "label": 0
                },
                {
                    "sent": "For a few values of M so OK, this few histograms don't look nice and smooth simply because there are not that many matrices of this few columns to allow for a smooth histogram.",
                    "label": 0
                },
                {
                    "sent": "But as we will take quite the metric, is there are more of them in the histogram get smoother, but even here we can already see certain trends for the more columns we allow the.",
                    "label": 0
                },
                {
                    "sent": "So these are our scores of the of the performance of the mattress is and this is the probability density.",
                    "label": 0
                },
                {
                    "sent": "Basically in our histogram.",
                    "label": 0
                },
                {
                    "sent": "So the more columns you allow, the more this whole distribution.",
                    "label": 0
                },
                {
                    "sent": "Is moving towards higher scores and the more frequently you encounter matrices with good scores, so actually there is a nice difference between the two.",
                    "label": 0
                },
                {
                    "sent": "So this looks like once you're at five, well, that's in the yeah, you might say.",
                    "label": 0
                },
                {
                    "sent": "Ability performance.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you move anymore.",
                    "label": 0
                },
                {
                    "sent": "That's that's basically illustrates what we already saw in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "The maximum is practically achieved at four and doesn't doesn't increase past that, but more and more probability mass gets close to the maximum this mode is.",
                    "label": 0
                },
                {
                    "sent": "This madisar peak is moving gradually more and more to the right, and it's higher and higher.",
                    "label": 0
                },
                {
                    "sent": "We'll see.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other chart of that sort on the on the next slide.",
                    "label": 0
                },
                {
                    "sent": "So here we have even wider metric is 1015 columns, 2030 columns and we see this same trend all the time, so the maximum doesn't budg.",
                    "label": 0
                },
                {
                    "sent": "But this peak keeps moving to the right and more and more matrices are in this in this area of good performance.",
                    "label": 0
                },
                {
                    "sent": "So here we see this sort of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things quantitatively now I was I was looking for some sort of theoretically well defined distribution that would roughly match the shape of these empirical distributions that we saw in the previous slide, and I decided to try the better distribution.",
                    "label": 1
                },
                {
                    "sent": "We can use the method of moments to estimate the better para meters Alpha and beta, which of course will depend on the number of columns we're looking at on M and here are.",
                    "label": 0
                },
                {
                    "sent": "So here are examples.",
                    "label": 1
                },
                {
                    "sent": "So here we have the empirical distribution.",
                    "label": 1
                },
                {
                    "sent": "The blue line is the empirical distribution, the same as we saw in the previous slide, and this is the best beta distribution fitted over the empirical one.",
                    "label": 0
                },
                {
                    "sent": "So as it turns out, for smaller values of N, the fit isn't that great.",
                    "label": 0
                },
                {
                    "sent": "Basically the actual empirical distribution leans a bit more to the right, and it has a higher peak.",
                    "label": 0
                },
                {
                    "sent": "But as you look at wider and wider matrices.",
                    "label": 0
                },
                {
                    "sent": "The better fits increasingly good to the empirical distribution.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can ask ourselves, how do the so we got this para meters Alpha and beta estimated for each value of M and we can look at them as a function of M. So this chart shows us know that there's a logarithmic scale here, so it's interesting that there are several pretty wide ranges where the.",
                    "label": 1
                },
                {
                    "sent": "Para meters were, the line is basically almost a straight line, but with the logarithmic scale, that really means that over these ranges like this range.",
                    "label": 0
                },
                {
                    "sent": "With this range, the Alpha and better parameters are approximately exponential functions of the number of columns.",
                    "label": 0
                },
                {
                    "sent": "Higher values of health and better also reflect the fact that the standard deviation is narrower, and we saw that on the previous slides as well.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so another interesting question.",
                    "label": 0
                },
                {
                    "sent": "So when you when you try to construct coding metrics you would ideally want some useful properties to guide you so that hopefully if you construct a matrix that has those properties, it will turn out to perform well as well.",
                    "label": 0
                },
                {
                    "sent": "That's what we would like, and one well known property that is often recommended is that we don't want the rows of the columns.",
                    "label": 0
                },
                {
                    "sent": "The rows of the matrix to be too similar to each other and we likewise don't want the columns of the matrix to be too similar to each other.",
                    "label": 1
                },
                {
                    "sent": "There are some good theoretical arguments for that.",
                    "label": 0
                },
                {
                    "sent": "So for example, if two rows are similar, well basically if you look if you have an instance from a certain class, then we would expect the binary classifiers in our ensemble to make predictions for this particular instance, which are similar to the values of the of the row in the in the code metrics that corresponds to the class from which this instance was taken, and so if several rows are very similar, then we would expect our ensemble to make very similar predictions.",
                    "label": 0
                },
                {
                    "sent": "Two instances from those classes, and so we won't be able to tell them apart very efficiently.",
                    "label": 0
                },
                {
                    "sent": "We will make many confusions between them and similarly if two columns are too similar, then we have two very similar binary problems and will get very similar binary classifiers.",
                    "label": 0
                },
                {
                    "sent": "So when when one of them will make a mistake, probably the other one will also and that will again make decoding hardware make it more likely that there will be enough mistakes that we will end up matching our row of predictions to the wrong row of the matrix.",
                    "label": 0
                },
                {
                    "sent": "So basically, it's well known that it's a desirable characteristic that the rows and columns aren't too similar to each other.",
                    "label": 1
                },
                {
                    "sent": "We decided to measure this with basically the average Hamming distance over all pairs of rows, or the average Hamming distance over all pairs of columns.",
                    "label": 0
                },
                {
                    "sent": "So we get something that we call the row separation or column separation.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how are these separations measures correlated to the performance of the metrics as measured by the Jaccard score that we described earlier?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we have a few charts about this, so here we have a scatter plot for metrics of four columns.",
                    "label": 0
                },
                {
                    "sent": "Here we have the Jacquard scored, the classification performance of the metrics and here we have the column separation and here likewise for the row separation in the lower 2 charts show the same thing, but for metric is off 7 columns.",
                    "label": 0
                },
                {
                    "sent": "Remember that's the ones where we notice that the best performing matrixes are also found.",
                    "label": 0
                },
                {
                    "sent": "So the interesting observation here is on the one hand, it's certainly true that the metrics that have good.",
                    "label": 0
                },
                {
                    "sent": "Column separation or good role separation tend to perform better than the ones which have poor column separation or poor low separation, but also very interesting Lee if you look at the matrices that maximize the column separation or the row separation, these are not actually the mattress.",
                    "label": 0
                },
                {
                    "sent": "Is that maximize the performance because some of the best performing matrices are somewhat to the right of them.",
                    "label": 0
                },
                {
                    "sent": "So basically they perform better even though they have poorer or color separation.",
                    "label": 0
                },
                {
                    "sent": "Now OK here we don't have all the metrics is shown because there are too many of them, but it's a random sample of 10,000.",
                    "label": 0
                },
                {
                    "sent": "Mattress is so I think it's still quite representative, so it tells us that on the one hand, certainly Roman current separation are good properties and good guidelines, but simply going out to maximize them is not actually what you if you really want to get maximum performance from your mattress is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Enter OK, so one thing that's pretty much the last thing I looked at, so we know that the matrix defines basically a set of new binary problems and then also set of new and a sample of new binary classifiers, and so the question that might occur to us is is there any relationship between how good this individual binary classifiers are and how good the ensemble as a whole is?",
                    "label": 0
                },
                {
                    "sent": "So might we want to focus when we are constructing a metrics in a real problem?",
                    "label": 0
                },
                {
                    "sent": "Where we can just exhaustively enumerate them all?",
                    "label": 0
                },
                {
                    "sent": "Might it be good to focus on mattress is with columns that lead themselves to producing better individual binary classifiers.",
                    "label": 0
                },
                {
                    "sent": "So for this test we computed the F1 measure and the area under the RC curve for each of the individual binary problems relative to its own individual binary classification task.",
                    "label": 1
                },
                {
                    "sent": "And then we looked at the average of these numbers.",
                    "label": 1
                },
                {
                    "sent": "The average F1 or the average area under curve over all the classifiers in the example.",
                    "label": 0
                },
                {
                    "sent": "And we looked at whether this averages then correlated to the performance of the ensemble as a whole.",
                    "label": 0
                },
                {
                    "sent": "The classification performance is measured by the Jaccard score that we mentioned earlier, and the result is not really correlated.",
                    "label": 0
                },
                {
                    "sent": "So here are some correlation coefficients.",
                    "label": 0
                },
                {
                    "sent": "This phone is practically completely unrelated.",
                    "label": 0
                },
                {
                    "sent": "Here is an extremely weak correlation that isn't really useful.",
                    "label": 0
                },
                {
                    "sent": "So in other words, when you're constructing a matrix, it isn't really particularly important that the individual classifiers the binary classifiers in the in the new problem defined biometrics.",
                    "label": 0
                },
                {
                    "sent": "Are very good by themselves.",
                    "label": 0
                },
                {
                    "sent": "What matters is how the matrix combines them.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a bit of a negative result, but on the other hand, also good because it means you don't really have to pay it.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Into this, when you're constructing your metrics so the conclusions.",
                    "label": 0
                },
                {
                    "sent": "Basically, these are things that I've already said earlier during the individual slides, we saw that the best matrixes were found at 7 or 8 columns, but already with much fewer columns.",
                    "label": 1
                },
                {
                    "sent": "We can still find metrics that are almost equally good, but there are much more rare so they're harder to find, but it's it's a desirable thing, because ideally you want an ensemble with few few classifiers, especially if your original set of classes is higher.",
                    "label": 1
                },
                {
                    "sent": "So it might be worth looking for them as they do exist and we also saw that if you can afford a larger number of classifiers senior assemble, then you can pretty much pick any metrics you like, 'cause most of them are good and the standard deviation is very small.",
                    "label": 1
                },
                {
                    "sent": "We saw that the distribution of metrics courses approximately like a beta distribution with a few aberrations for small values of M and the parameters are approximately exponentially in M over large values of M. We saw that row and column separation are useful properties, but.",
                    "label": 1
                },
                {
                    "sent": "That actually maximizing them is not is not the best idea if you want to look for the metrics with the best performance.",
                    "label": 0
                },
                {
                    "sent": "And we also saw the quality of individual classifiers in our ensemble.",
                    "label": 0
                },
                {
                    "sent": "The individual binary classifiers is not actually correlated with the performance of the entire matrix.",
                    "label": 0
                },
                {
                    "sent": "So for future work, like I said earlier, it would be good to test these things and more data sets, including some larger datasets with more classes where of course we couldn't afford to testing exhaustively like we did here, but would have to focus more on sampling and perhaps investigate some other easily computable metrics properties similar to the way we did with Rowan.",
                    "label": 0
                },
                {
                    "sent": "Separation because ideally you want to find some matches and properties which you will be able to use while constructing the metrics, and that will hopefully lead you to good metrics is without you having to test things on your data all the time.",
                    "label": 0
                },
                {
                    "sent": "That concludes my presentation, thank you.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "This finding, I guess, relates with random projections in a way there.",
                    "label": 0
                },
                {
                    "sent": "Basically the idea is just just go out there, find some stuff.",
                    "label": 0
                },
                {
                    "sent": "And yeah, the idea is kind of the quality of the ensemble comes from the way that you are combining different predictions, not from the idea that the individual predictions are good.",
                    "label": 0
                },
                {
                    "sent": "Basically the statistics will help you without the mistakes of the individual binary classifiers, so that's kind of there was this sort of single.",
                    "label": 0
                },
                {
                    "sent": "This theoretically kind of known, but here we have some empirical confirmation of it.",
                    "label": 0
                },
                {
                    "sent": "Another comment.",
                    "label": 0
                },
                {
                    "sent": "So thanks.",
                    "label": 0
                }
            ]
        }
    }
}