{
    "id": "5pzpq4izxuboaogklauqx6ez72ige5ij",
    "title": "Machine learning for cognitive science 3: Kernel methods and Bayesian methods",
    "info": {
        "author": [
            "Bernhard Sch\u00f6lkopf, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "June 15, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2010_scholkopf_mlfcs3/",
    "segmentation": [
        [
            "Just to summarize our results from yesterday, so this is one of the.",
            "This is one of the bonds that we can derive now.",
            "It tells us that for any function that can be implemented by the learning machine or by our class of functions, that the learning machine is allowed to choose from.",
            "With the probability or let's say with large probabilities.",
            "So I think of Delta is some small number.",
            "The test error of the learning machine is upper bounded by the training error plus some term that depends on the VC dimension.",
            "And when I say with a high probability, I mean with high probability over sampling training sets.",
            "So I think I've said that yesterday, but it's important to stress that, so it means that.",
            "And it's not for one fixed training set.",
            "Typically in an application you have one given training set and you want to do the best possible job for that.",
            "So this statement is not quite the ideal thing for that.",
            "It doesn't speak about that given training set.",
            "So this is a slightly different story and the same qualification applies to what I said about training on the test set and things like that, so it's a slightly different statement.",
            "Still, a very useful statement.",
            "Just bear in mind what it means, so it means we are unlikely to find a training set for which this inequality is violated, and likewise if we talk about training on the test set or cheating as we talk college yesterday we are unlikely to find a test set.",
            "For which.",
            "Our estimate of the test error is far from the truth, but for one given test set we might be unlucky.",
            "So, so this was our bond.",
            "So here we have the tradeoff between trying to minimize the training error and trying to have a small complexity terms or trying to choose a function class with a smaller we see dimension.",
            "If we choose a function class with a small VC dimension, then typically it will be the case that we cannot achieve such a small training error, so we'd like to have a bigger function class to get a smaller training error.",
            "But then the VC dimension goes up, so there's a tradeoff between needs these two things.",
            "And this bound doesn't guarantee us that for any given problem, we will have a small error.",
            "It just tells us that for any given problem, if we choose a reasonably small VC dimension and we have a sort of compared to the sample size, then it's unlikely that our test error would be a lot larger than the training error.",
            "But it doesn't tell us how to choose function classes for which we can get small training error, even though the complex."
        ],
        [
            "Is more."
        ],
        [
            "OK.",
            "So function classes are important.",
            "We would like to find good function classes.",
            "Ideally even we would like to have function classes where we can compute the VC dimension or other capacity measures.",
            "That would be nice because the VC dimension is this complicated commute Oriel thing I've told you about.",
            "It's about the maximum number of points which can be separated in all possible ways.",
            "So I can imagine, or you can probably imagine if I give you some learning machine which can implement a function class, you might have a hard time computing the VC dimension, so that's that's difficult.",
            "We can do it for separating hyperplanes.",
            "We can't do it for just any function class.",
            "So from theoretical theoretical point of view, we would like to work with separating hyperplanes.",
            "We can handle those from the practical point of view.",
            "Maybe we don't want to wear with hyperplanes because they are quite limited, they only induce linear separations of the data.",
            "So we would like to use something more complicated, and that's what kernels are bought.",
            "We will try to do the hyperplanes in some other high dimensional space which can be non linearly related with the input space."
        ],
        [
            "So how do we construct this space?",
            "So the idea is that we pre process all data points with some fixed mapping.",
            "Fire takes our input domain so this X is our input domain.",
            "We haven't made any assumptions about it other that other than it being an nonempty set.",
            "So it could be a set of discrete objects.",
            "It could be a vector space.",
            "So far it takes our input domain into some dot product space, so that's a vector space with a dot product and.",
            "Are there people who don't know what the dot product is?",
            "Just maybe, maybe you've all heard of that.",
            "So the product is a very useful concept from geometry, which allows you to compute things like distances and angles.",
            "So this is a dot product space, A vector space with the dot product, and then once we've pre processed the points like that.",
            "And we can do whatever learning tasks we're interested in solving based on the fire of the input points rather than the input points directly.",
            "So typically this space is high dimensional.",
            "And.",
            "And we're not really scared of that, because now we have seen that the main issue in machine learning.",
            "If you want to generalize is the capacity and maybe not the dimensionality."
        ],
        [
            "So in some cases.",
            "Like what I told you about separating hyperplane standard VC dimension bound will be in plus one for in dimensional hyperplanes.",
            "So it looks like here that we see dimension is basically the same as the dimensionality of the space.",
            "So that means in high dimensional spaces we would be in trouble.",
            "We would have a very large money BC dimension.",
            "However, I also mentioned that if we look at hyperplanes that induce separations with large margin, then the VC dimension can have a much smaller bound.",
            "So in that case we shouldn't be scared."
        ],
        [
            "The high dimensionality.",
            "OK, so let's see.",
            "Here's an example.",
            "So suppose we have these two classes of data points that crosses and circles.",
            "We are only given the data points, but not this separation separating boundaries.",
            "We assume that the true optimal separation is this ellipse here.",
            "And we already given the points.",
            "Now suppose someone told you you should be pre processing your data points by computing all features of order two in the input coordinates.",
            "There's just two input coordinates, so there's only three possible features of order 2.",
            "Protective order Two is X1 squared, X2 squared and X 1 * X Two.",
            "And don't worry about this square root here for the second for the moment, so we compute these three features and now we think about finding the separation in that space.",
            "And if you think about it for a second.",
            "If you write the ellipse equation in terms of X1 squared and X2 squared, then for this simple ellipse here the.",
            "Separation boundary becomes hyper linear hyperplane and in fact in this case it's even a linear hyperplane that only depends on X1 squared and X2 squared.",
            "It doesn't depend on these mixed, so if this ellipse were not access aligned, it would be also depending on this term.",
            "But that doesn't matter.",
            "The point is, in this 3 dimensional space we can solve this problem with the linear hyperplane.",
            "So if we had known that this is our solution, then it would be a good idea to map into this 3 dimensional space and then instead do a hyperplane and from the point of view of learning theory, that's nice because hyperplanes are easier to handle.",
            "We can compute the VC dimension and things like that.",
            "So that's a nice."
        ],
        [
            "To do and actually we would like to do it with higher order features and higher dimensionality's.",
            "So we would like to do this for N dimensional inputs and this is a pretty old slide.",
            "I've shown it many times and these images look very low resolution and somehow I'm not sure it might be my memory, But this this data set used to be state of the art data set some years ago.",
            "Now it looks like a little bit ridiculous, but it doesn't matter.",
            "We can also hire take higher dimensional inputs, any dimensionality in, and consider products of order D if we wanted to work in that space then the dimensionality of the space would grow.",
            "Like into the power of the.",
            "So even for this low resolution images we already have 10 to that end."
        ],
        [
            "Mentions here so it looks like we're in trouble, but there's a nice trick which is referred to as the kernel trick.",
            "Which consists of mapping the points into such a feature space.",
            "And then taking the DOT product between two such points.",
            "So even if this mapping is in the high dimensional space, it turns out we can compute the dot product without actually carrying out this mapping.",
            "So to see that, let's do.",
            "Let's take two points and map them.",
            "First, into the three dimensional space.",
            "So this is our low dimensional example from before.",
            "So here's the first image of the first point.",
            "Here's the image of the second point, and remember that product the Canonical dot product is just.",
            "We take this vector times the transpose of this vector, right?",
            "So it's a sum over product of corresponding entries.",
            "And if you think about this, just look at this so we have X 1 ^2 * X one prime squared plus this thing here times this thing.",
            "Plus this thing times this thing.",
            "If you write this down and if you don't see it you can do it as a little exercise you will see it's a complete binomial formula which can be rewritten as the dot product in the original space raised to the power of tool.",
            "So that's nice.",
            "That's a simple function that we can compute in the original space.",
            "We don't have to carry out fire."
        ],
        [
            "And actually the same trick works more generally, and now we go backwards.",
            "Let's just assume we have N dimensional points and we start from this expression here.",
            "So we raised this to the power of the.",
            "So we write down our Canonical dot product again.",
            "This formula here we raised to the power of the so we have D such sums.",
            "And to work it out we use J1 for the first one and JD for the last one.",
            "Now if we multiply it out, we have this default sum and here we have the products of all these terms occurring back here and I have sorted them such that the X terms are first in the ex prime terms come second.",
            "So what you can see here.",
            "So this is a dot product in an end to the power of the dimensional space or or it's a it's a sum with into the power of the terms.",
            "And each term is a product of some function of X times the same function of X prime.",
            "So this can actually be interpreted as a dot product in an end to the power of the dimensional space of some nonlinear transformation of X&X prime.",
            "And we can read off the nonlinear transformation from this formula here.",
            "So this is just a product of D entries of the vector X.",
            "This is the same product of course.",
            "Likewise for the vector X prime.",
            "So this is a dot product in this space spent by all products of the input directions, and here are these are even ordered products because.",
            "Some of these because multiplication is commutative, so some products occur many times in this."
        ],
        [
            "And that's why I had this factor."
        ],
        [
            "Square root 2 in here.",
            "So some product occur many times, but let's not worry about that.",
            "It's a dot product in this space spent by all products of the input features."
        ],
        [
            "So.",
            "This quantity here, which is called a kernel computer product in a high dimensional space.",
            "The question is, are there other other functions?",
            "Other kernels that compute dot products in high dimensional spaces?",
            "After applying some nonlinearity to the input directions and the answer is yes and this can be seen in various ways, traditional way was to appeal to an old theorem."
        ],
        [
            "Functional analysis called.",
            "This is theorem and which talks about kernels of integral operators.",
            "And if you don't know what an integral operator is, don't worry.",
            "I'm not going to go in detail on that, so this theorem just tells us if we have a positive definite integral operator by positive definite, I mean this equality here is true for all functions.",
            "Then we can expand this kernel in such a infinite sum.",
            "It's a little bit like a matrix diagonalization.",
            "Where the lamp ties are non negative and if you look at this some you can probably already guessed.",
            "This also looks a little bit like a different product in a high dimensional space and the nonlinearity comes from this."
        ],
        [
            "Fire fun."
        ],
        [
            "Oceans.",
            "So."
        ],
        [
            "Anne.",
            "So."
        ],
        [
            "The more nowadays people use a slightly different class of kernels.",
            "And very slightly larger class of kernels which are called positive definite kernels.",
            "And it turns out that this is exactly the right class of kernels that do correspond to DOT products in high dimensional spaces.",
            "So what's a positive definite kernel?",
            "It's a kernel which function of two arguments which is symmetric in the two arguments and which has the property that whatever set of training points, I take whatever set of real coefficients A1 through AM.",
            "This inequality here is true, where K is the matrix that we get by substituting pairs of training points into the kernel.",
            "So this is also called the gram matrix of the kernel matrix.",
            "So this matrix is this saying nothing but that this matrix is positive definite.",
            "Some people call it positive semidefinite.",
            "So in the kernel community and also in some branches of approximation theory, this is called a positive definite matrix and the stronger requirement.",
            "So if we Additionally require that for pairwise distinct points.",
            "So if no training point appears twice in the set.",
            "This quantity should only be 0 if these coefficients are zero, so that's a slightly an additional requirement.",
            "Then we would call this matrix strictly positive definite in the kernel.",
            "Also strictly positive definite.",
            "So don't get confused about terminology.",
            "Some people refer to this as positive semidefinite and positive definite.",
            "Calling it positive definite and strictly positive definite and maybe the main reason is that we mostly talk about positive definite.",
            "And this is the short term then.",
            "OK, so this is a condition for a kernel and I'll show you how to prove that this then corresponds to a dot product in a high dimensional space.",
            "Or maybe we'll try to show it to."
        ],
        [
            "So what are the main?",
            "What's the point about this kernel trick?",
            "So if we have some algorithm, so maybe now."
        ],
        [
            "Don't get confused about the general case.",
            "Think of a kernel in this form again, so we said this kernel computes.",
            "A DOT product in some high dimensional space, so it's implicitly it's carry out, carries out this nonlinearity fire and then compute the dot product there.",
            "So if we have some algorithm that we can write in terms of DOT products only, so some algorithm that maybe only uses distances and angles in some space, then we can carry out this algorithm implicitly in the space that fire Maps into.",
            "By simply replacing all occurrences of dot products by such a kernel."
        ],
        [
            "So any algorithm that only depends on the products can be kernelized as people say, and this way we can apply linear methods even to non vectorial data because as I mentioned."
        ],
        [
            "Before there is no.",
            "Distance didn't say here, but there is no assumption on X other than it being a nonempty set.",
            "So if we have some set of discrete objects and we are able to define a kernel function which satisfies this requirement of positive definiteness, then we get our representation of the kernel as a dot product in a high dimensional dot product space in a vector space."
        ],
        [
            "For free, so we have a vectorial representation for the data automatically.",
            "We don't have to worry about the data being vectors to begin with.",
            "So we think of this kernel as some kind of similarity measure.",
            "Maybe it's a particular type similarity measure positive definite one.",
            "Anne and Neil Lawrence already mentioned yesterday.",
            "These kernels are in the field of Gaussian process regression, also known as covariate."
        ],
        [
            "Functions.",
            "OK, so.",
            "Maybe the most important thing about this definition here or in general, when you see a new definition in mathematics, is to play around a bit with it to understand it.",
            "So I would suggest to that we take a few minutes and everybody tries to play around with this and prove a few things which I have on this slide here.",
            "So I'll give you some hints, maybe so these are ordered in may be increasing difficulty, but all of them are.",
            "Reasonably easy to prove.",
            "So in the first thing I would like you to prove that.",
            "If maybe I have to write them down somewhere because I guess you want to know this.",
            "Definite maybe.",
            "Maybe you can copy this definition, or you can try to remember it.",
            "So some of IDAIAJ of.",
            "This thing here should be no negative, so this is what we know.",
            "If a kernel is positive definite and Moreover we know it's symmetric, so at least it leave this up for a few seconds and then I'll say something about the next slide."
        ],
        [
            "I'll move to the next slide, so here in the first point.",
            "I want you to prove that if we define our kernel to be equal to this thing where fire is some mapping into some dot product space.",
            "Then the set the dust defined kernel is positive definite, so this object here is a positive definite kernel, so that's the first first thing to prove.",
            "The second thing to prove is kernels are positive on the diagonal.",
            "So if a kernel is positive definite, so this is a little bit short, this statement.",
            "Here we assume the kernel is positive definite and we prove that this implies that K of X, X is non negative.",
            "3rd part is a generalized Cauchy Schwarz inequality, so again, we assume the kernel is positive definite and we want to prove that then this inequality here is true.",
            "And finally, again, we assume the kernel is positive definite and we want to prove that.",
            "If the diagonal is 0, so if K of XX is 0 for all X, then the kernel is actually zero everywhere, even if X&X prime are different.",
            "So again, reminder in the first one, we only assume Phi is some mapping and we want to prove that this is a positive definite kernel in.",
            "In the rest, we always assume K is a positive definite kernel.",
            "So I'll give you a few minutes now and then.",
            "We can do it together.",
            "OK, someone's got the second one here.",
            "Maybe keep going until you have all of them and then will do all of them together at the end.",
            "Thank you.",
            "Can I get a quick show of hands?",
            "How many people have solved one or more problems?",
            "How many have two or more?",
            "Three or more.",
            "OK, so maybe I'll give you a little bit more time and then we do it together.",
            "So let me see whether I should give specific hints about some of them, so maybe a show of hands who has solved the first one.",
            "And I'm not gonna ask you to get up now so you can.",
            "You can be honest if you think you've solved it.",
            "Just just raise your hand.",
            "OK, who has solved the second one?",
            "Third one.",
            "So the first one is a more difficult one.",
            "It seems in the 4th one.",
            "OK, so that's also a risk relatively small set, so the 4th one becomes easy if you use the result of the third one.",
            "So that's one hint.",
            "So then it will take 2 more minutes and then with them together.",
            "So what I'm going to do is, I'll ask for volunteers, so if possible will do it with volunteers.",
            "Otherwise I'll have to volunteer myself, but I would prefer some of you do it.",
            "So is there anyone who would be willing?",
            "With my help potentially to do the first one here on this projector.",
            "OK.",
            "Thank you very much.",
            "So we can right here.",
            "Yeah, yeah, right?",
            "It's it's on.",
            "Write down for the graphics.",
            "Yep.",
            "He's gonna use more space.",
            "What?",
            "Tickets well it is.",
            "X1X1 multi-purpose XMX 10.",
            "Divided minus X1.",
            "Actually this escalates to then minus X one X2 and X2X1.",
            "Then you can use the.",
            "Fish plus point India.",
            "Space.",
            "So you want to use this cozy Schwarz inequality, or, well, if it's a kernel.",
            "I already know that in that space you have and not the usual procedure works inequality.",
            "You can use right?",
            "You couldn't use this one.",
            "We don't.",
            "We don't know yet whether it's a kernel.",
            "Yeah, I can first prove that one that one is OK. Then why don't you prove this one and then?",
            "Well, we already know it's a positive, so it's just like.",
            "You can sit down if you like or up to you.",
            "OK, so basically the issue right up to battleground matrix.",
            "A lot.",
            "Yes, excellent.",
            "Because this thing is because KX1X2.",
            "Is the same as a 6 plus well, OK?",
            "It's well paid.",
            "Minus KX1X2 square then, because by definition this thing is.",
            "So why is it larger or equal to 0?",
            "Because?",
            "So the matrix is positive definite, and I mean one way to say it.",
            "For instance, is positive definite matrices have non negative eigenvalues and the determinant is the product of the eigenvalues.",
            "So the determinant is non negative.",
            "OK, so do people see that so?",
            "This thing here you can easily convert it into this one.",
            "OK so so maybe maybe we'll ask someone else to do the first one.",
            "So thank you very much for solving this one.",
            "And.",
            "Thank you.",
            "So and here, just in case people are copying this, we also have K. OK, so we've solved the third one.",
            "Just.",
            "It has to be about product because like the garden is positive definite.",
            "So what exactly happened in addition of the product and then put another product?",
            "Well, that's true if you know that the kernel corresponds to a dot product in some other space.",
            "Just ask.",
            "But do you have to prove that this is a dot product, right?",
            "I have this.",
            "Well, if you can prove these things then then you can do it like that.",
            "You know, I'm not sure whether it will be easier like this easier than this, but.",
            "Yeah, I mean if you prove that it's the dot product.",
            "Although actually I would be surprised if you could prove that this this is a dot product that I mean.",
            "OK, you're not going to say that this is a dot product.",
            "You could only say it corresponds to the door products.",
            "Another space, because this can be a nonlinear function of X next prime, right?",
            "And if you want to show that it corresponds to a dot product in some other space, you basically have to solve the whole problem.",
            "That will solve in the next three slides.",
            "So let's say if you find a simpler proof for that, then I would be.",
            "I would be happy to see it.",
            "And OK so because yeah, maybe I should have said that.",
            "So the reason why we want to prove all these things?",
            "Because once we know them, we will be able to construct the other space in which K computes a dot product or which K can be represented as a dot product.",
            "OK, so the so we've solved this one.",
            "Let's try to solve one of the other ones and.",
            "I was a little bit worried that this was starting to look a little bit complicated number one, so that's why I was happy that you also had #3, so maybe someone else wants to do #1 because there's a simpler, simple way for doing #1, yes?",
            "So.",
            "No, sorry.",
            "Write down.",
            "Maybe you can say living what what are you using here?",
            "I'm just using linearity of the.",
            "Right, so yeah, that's correct.",
            "So we were assuming that this thing with the angular brackets is dot product, so he can use linearity of that product.",
            "So.",
            "Then we.",
            "We can change it in less.",
            "I can collect Jay.",
            "This one is the norm.",
            "Exactly, thank you.",
            "OK, so this is the proof.",
            "Thank you very much.",
            "So any questions about this so he's using the fact that this object here is a dot product, so therefore it's by linear is linear in both arguments.",
            "Linearity means you can take these coefficients in and out and you can take the sums in and out.",
            "And then he's taking this.",
            "Some of the eyes in here, some over the days in here.",
            "Likewise with the coefficients, and then he sees that this is actually the same vector as this one.",
            "And so it's it's a norm of that vector is not negative OK?",
            "So how about #2?",
            "Thank you.",
            "Choosing so in this case, now we assume K is a positive definite kernel to begin with, right?",
            "We can eat from the automobile is like.",
            "And some heroes.",
            "Yeah.",
            "Basically, take the.",
            "OK. Just stand there.",
            "I bet yeah.",
            "So.",
            "OK, so just to make sure that everybody understands it, so he's saying this thing here is.",
            "Non negative, and therefore this also is not negative, and that's correct.",
            "It's possible to do it even simpler if you want, but doesn't make a big difference, so we can Even so he's saying we no matter what training set we have.",
            "If we.",
            "Take us in a vector.",
            "One of these Canonical basis vectors.",
            "Then this will pick out just one of the training points, and then we see that if X is the training point, then this inequality is true.",
            "That's correct, you could make it even simpler if you want, because in the definition it says up here for any set of training points.",
            "So I could even pick a set of training points which only contains that.",
            "One point X right and then I could say the corresponding coefficient.",
            "I'll call it a or.",
            "I could even say I'll use the coefficient one because this also says for any coefficients.",
            "And then this inequality here whoops up.",
            "This simply becomes some over one term 1 * 1 * K One one, and then it's the same by this, correct?",
            "OK, so so we've solved this one.",
            "Only one left number for anyone from before.",
            "OK, so the question is question is whether this should be proved for any days only.",
            "For one a the answer is, well, the statement only says OK.",
            "The statement.",
            "Let's put the complete statement again.",
            "Statement says that for all X."
        ],
        [
            "For all X this inequality is inequality is true, so we only have to prove it independent of X, and in the proof didn't make any restrictions or any assumptions about what is X.",
            "So this is true for any X. OK, so anyone for number 4.",
            "OK. You can also say it if you want, yeah.",
            "Yeah.",
            "OK, so he's saying that.",
            "We write XKXX prime squared.",
            "We know it's upper bounded.",
            "By this it's prime.",
            "Now, by assumption this is 0.",
            "This is also zero.",
            "So this is equal to 0.",
            "It's a square, so it's a non negative number which is upper bounded by zero, so therefore it is 0.",
            "OK, so we have all of them, any questions?",
            "OK, so let's go ahead and.",
            "Prove what we wanted to prove.",
            "Or construct the feature space.",
            "So.",
            "Oh, so for this one.",
            "OK, so here's the outline of how the construction will work, and I think it's nice to see this construction once because a lot of people work with kernels and use kernels in different domains.",
            "I mean, even if you're not doing support vector machines, some of the kernel trick is is a standard trick 9 machine learning.",
            "It's a little bit like using the chain rule.",
            "You should be able to use the kernel trick, 'cause you might one day come up with a linear algorithm that you want to make nonlinear, so a lot of people use it, but.",
            "A lot of people have never actually seen how to construct the feature space with it, and why positive definite kernels correspond to their products in other spaces.",
            "And actually, it's not so difficult to see, so I think you should.",
            "You should see this now in the next 10 or 15 minutes.",
            "So here's the idea.",
            "We have to construct some feature map with the into some vector space first and then in this vector space or linear space we have to construct a dot product, not just any dot product.",
            "We want to have a dot product such that the kernel is equal to this dot product in this sense here.",
            "So it's a special type of dot product.",
            "And so the main work will be to show that the thing that we will construct has the properties of that product.",
            "And then we're almost done, so let's see.",
            "So the our mapping will define first the fine mapping is taking a point from the input domain and mapping it to a function, and it will be the function that we get by substituting the point into one of the arguments that, let's say the second argument of the kernel and keeping the first argument open.",
            "OK, so this object here is a function because it is still varies or it still depends on this first argument.",
            "So for instance, if we use a Gaussian kernel function.",
            "Then this means a point X will be mapped to a Gaussian function which is centered on this point X.",
            "So let's say here is the X point and now this is a function around X and likewise appoint X prime will be mapped to a kernel function centered on X prime, so this is a. Mapping into space of functions and if you have never seen this notation, this notation just means all functions and mapping from X to R, so all real valued functions on this domain X. OK, so let's.",
            "Let's look at this mapping, so remember this mapping."
        ],
        [
            "And we will first turn it into a linear."
        ],
        [
            "Place into a vector space.",
            "So what is a vector space?",
            "The main property of a vector space?",
            "If is.",
            "If I take 2 elements of that space, I can add them up and I'm still in the space.",
            "This the second main properties if I'm multiplier an element of the vector space with a number.",
            "I still get an element of the vector space, so it's a.",
            "This is a non trivial thing.",
            "This is this is true and but actually there's a trivial way of ensuring that is."
        ],
        [
            "Or in our case, and in our case, we will simply declare that our space contains all functions of this form.",
            "So if I map individual points into the space, I get functions of this form and now I'll just say, well, I will just declare that all.",
            "If I multiply such a function with a number, I'm still in my space, or if I add up several such functions, I'm still in that space.",
            "So the typical elements of my vector space will have this form here, so this function F for this function G. Well, these are numbers and the exits are arbitrary points for my original set."
        ],
        [
            "So interrupt me at anytime if you like, so now it becomes a little bit more tricky.",
            "Now we have to construct the DOT product in this space.",
            "Well, we don't actually have a choice in control."
        ],
        [
            "In this talk product, because as you remember from before, we want the DOT product to satisfy this equality here.",
            "So if we plug in the mapping the way we defined it, so X goes over to this function sitting on X.",
            "Then we know that all the products would have this property here.",
            "Now, in addition, something that we used before we said before DOT products have to be by linear.",
            "So they have to have this property that if I have a number out here, I can move that number into the dot product without changing the value.",
            "Or if I have a some outside, I can move the sum inside.",
            "So this basically this condition here tells us what is the value of the dot product for such simple functions such as individual bumps.",
            "And then the value of the dot product for linear combinations of bumps already follows from the requirement that the DOT product should be bilinear."
        ],
        [
            "So we have no choice.",
            "We have to define.",
            "The drug product between two genera."
        ],
        [
            "Functions F&G, so these two functions here."
        ],
        [
            "Should be defined like this.",
            "Should be there some overall I know Jay Alpha, I better J and this quantity here.",
            "So if we define it like this it is a bilinear function and now slightly more subtle but still important point from the mathematical point of view is that this thing here should be well defined.",
            "So what do I mean by this?",
            "M."
        ],
        [
            "So if I write a function like this, so this is my function, there might be different ways of writing the same function, maybe in terms of different different coefficients and different centers here on which the functions are sitting.",
            "That's possible.",
            "In fact, for many kernels it will really be the case, so this is not a unique expansion of that function.",
            "So there's not a one to one mapping between the function and these coefficients."
        ],
        [
            "On the other hand, here it looks like the definition of the dot product depends on these coefficients, so it shouldn't be the case that.",
            "This thing here, which should be a property only of F&G depends on these alphas betterson this points that are using the expansion.",
            "That would be bad, but it turns out it doesn't depend on that and the way to see it is the following one.",
            "I can rewrite this quantity here.",
            "Two possible ways I can write it like this thing here so."
        ],
        [
            "Maybe if I go back to here so remember F is the expansion terms of the other fasten the excess.",
            "G is the expansion in terms of betterson X primes.",
            "OK, so try to put it into your.",
            "Men."
        ],
        [
            "So into your mental imagery for a second or your memory and.",
            "Therefore, so remember, G was in terms of better than X primes, so if you substitute gene here, you will see you recover this thing.",
            "Likewise, if you substitute if which had the alphas in the axis in here you again recover this formula.",
            "So this these are trivial identities.",
            "But from this as follows.",
            "So from the first one.",
            "It follows that this quantity here is independent of the bit as an independent of the ex primes, because better and explain doesn't appear in this formula.",
            "From the second equality formula follows that this definition here is independent of the alphas and of the axis, 'cause I'll find X doesn't appear here.",
            "So taken together, we see it's independent of the particular choice of Alpha, Beta, X&X prime.",
            "It only depends on F&G itself.",
            "OK, so and.",
            "So this is a nice thing.",
            "It's a symmetric bilinear form we know now."
        ],
        [
            "Anne.",
            "So let's continue a little bit and look at 2 interesting special cases.",
            "So one special case if."
        ],
        [
            "We take one of these functions, so here we have two Jenna."
        ],
        [
            "Functions F&G, which we defined like this.",
            "If I make one of these functions very simple, so I'll cross out all this and just have one individual kernel fun."
        ],
        [
            "So what do I get then?",
            "Then?",
            "Then I only have."
        ],
        [
            "Set of coefficients here.",
            "And only one sum.",
            "Anne and."
        ],
        [
            "So if I do it for F then the Alpha vanish is the ex vanishers, so I'm left with a single sum over the bed has in the ex prime."
        ],
        [
            "Times, which actually is nothing but G."
        ],
        [
            "So therefore if I take this dot product so this very simple case for F with GI actually recover G evaluated at the point X.",
            "So this is a.",
            "Sometimes this is paraphrased by saying the kernel is the representative of PT evaluation, so PT evaluation is an interesting operation.",
            "Enough in a vector space of functions.",
            "It's a linear operation.",
            "But in the general case, it's not necessarily continuous.",
            "So here in repetition kernel Hilbert spaces, it turns out this is a nice continuous functions and it makes sense to do this point evaluation, but that's just a side remark.",
            "So anyways, OK represents PT evaluation of a function.",
            "And if we specialize even more, let's both F&G we make both of them single kernel functions.",
            "Then we recover this equation here, which is the reason why K is sometimes called a reproducing kernel.",
            "So K with K makes another key."
        ],
        [
            "OK.",
            "So now let's conclude this proof, and to show this.",
            "To start to show that this thing this symmetric bilinear form is actually a dot product.",
            "Then we go in several steps.",
            "We first show that it's a positive definite kernel in its own right on a set of functions.",
            "So just to make sure I don't confuse you."
        ],
        [
            "So remember we started by assuming K is a positive definite kernel on the domain where this X points come from.",
            "So what I'm saying now is if we define this thing here with the angular brackets, which we know is a symmetric bilinear form if we define it in this way, then it turns out that this is also positive definite kernel.",
            "But this is a positive definite kernel on the set of functions.",
            "So to see that this is a positive definite kernel, we have to check the usual definition of positive definite kernel."
        ],
        [
            "So what we have to do is.",
            "We have to take a set of arbitrary functions FFJ arbitrary real coefficients, I, J.",
            "And we have to workout this quantity here.",
            "So let's do that and we want to show that this is non negative.",
            "So first of all we use linearity.",
            "Remember, we have already seen that this thing with the angular brackets is a bilinear form, and it's symmetric bilinear form.",
            "So we can use linearity.",
            "We take these coefficients into some insight to some inside the bilinear form.",
            "So just like before we take it inside again, now we can see here this thing.",
            "Here is the same as this one.",
            "So these are both functions, let's call them F. We don't know what exactly they look like, but it doesn't matter.",
            "There are functions from our space, we know it's a vector space, so these are still elements of our space.",
            "Since the elements of our space, we can write them in this form here with some other files in some X axis, so we'll do that.",
            "And now we will use by linearity again, and we take the Suns out again in the alphas.",
            "So now we have this thing here and it looks familiar 'cause this is exactly the quantity that we know is not negative because K is a positive definite kernel.",
            "So since K is positive definite kernel on the domain of the excess, this thing here is a positive definite kernel on the domain of the functions or on this set of functions.",
            "So that's nice we know is positive definite kernel.",
            "Therefore we can use all the things that we have proven before."
        ],
        [
            "So all these properties we can now use, it's a positive definite kernel on this function space."
        ],
        [
            "OK, so let's see how we can use them.",
            "Well, actually so the first step is now we will prove it's strictly positive definite.",
            "An and to show this.",
            "We are so we.",
            "We know it's a symmetric bilinear form to show that it's a dot product.",
            "We also have to show it's strictly positive definite.",
            "So to show this, we look at some arbitrary point F of X ^2.",
            "We can remember that the kernel is the representative point evaluation."
        ],
        [
            "So that was.",
            "On this slide here.",
            "Alright, so if we want to evaluate a function at some point, we just take the dot product with the."
        ],
        [
            "Journal.",
            "So we'll do that here.",
            "Take the dot product between F and the kernel centered on X.",
            "So now we can use the Cauchy Schwarz inequality.",
            "So where was the coach Schwarz inequality?"
        ],
        [
            "T. Tells us that this square here is upper bounded by this product.",
            "Of these two things."
        ],
        [
            "Sorry, we use the yeah so we we upper bound.",
            "Is this actually then the.",
            "The usual coaching swaps inequality we upper bound by this thing times this thing here.",
            "And if we look at this quantity here.",
            "So this is this is a.",
            "If with F itself and here we have the kernel with the kernel, so maybe this is a slightly confusing so you shouldn't get confuse."
        ],
        [
            "By the fact that here we have the coach about the equality for K for Colonel.",
            "But now on this other slide."
        ],
        [
            "So maybe I said this is a little bit confusing.",
            "The kernel that we are interested in is not this key, but this thing with the angular brackets.",
            "So we will use the kosher Schwarz inequality for kernels.",
            "But for this kernel with the angular brackets which we know is a positive definite kernel on the set of functions.",
            "So we upper bounded by the product of the first argument with the first one and the dot product at the second with the second one.",
            "And what we can see from here is this is a non negative number which is upper bounded by this.",
            "Therefore, we know that whenever F with F, the dot product has the value 0.",
            "Then the left hand side must also be 0 'cause it's a non negative number which is upper bounded by zero.",
            "This means that if the dot product of F with itself is 0, then F is identically 0 for all X is, so it's the zero function.",
            "And if you know the definition of a dot product then you know that this is the strict positive definiteness that needs to be proven in order to show that something is a dot product so that products of a function with itself takes the value 0 only if the function is 0.",
            "So now we know it's a dot product and then the last step is actually sort of trivial, one that we're not going to go into detail on.",
            "So now we have a vector space with the DOT product.",
            "We just have to complete the space in the norm induced by the DOT product to get a Hilbert space so so vector space with the dot product is also called a pre Hilbert space and Hilbert space is simply complete.",
            "So sequences that are convergent Cauchy sequences actually converge against some limit.",
            "OK, so now we know.",
            "So let's see what have we proven we have proven."
        ],
        [
            "We started with the kernel, assuming it's positive definite, we constructed a feature map for this kernel.",
            "Then we construct it an inner product.",
            "Functions of this space.",
            "We showed that it has the properties of the inner product and we showed that it satisfies this equality here.",
            "So the inner product in that space is identical to the kernel taken in the input domain.",
            "For this special points.",
            "OK, so now."
        ],
        [
            "It's nice because remember if we before we prove that if we start with some.",
            "Function into a dot product space Phi, then this quantity here is a positive definite kernel.",
            "And now in the last 10 minutes we proved if we start with a positive definite kernel then there exists some fire.",
            "In some Hilbert space such that this thing here corresponds to the kernel.",
            "So the set of positive definite kernels is identical to the set of.",
            "Things that can be written like that for some mapping.",
            "Fight into a Hilbert space.",
            "So this is the same thing we can represent all kernels in this way invite."
        ],
        [
            "Versa."
        ],
        [
            "OK, so.",
            "If you have questions to interrupt at any point, so now we're going to look at some examples of kernel algorithms.",
            "So, so we're going to look at some examples of Colonel algorithms.",
            "Questions about the construction of the Hilbert space, then now is a good time.",
            "OK, so let's look at some examples, so maybe this was a little bit tough idea.",
            "I hope I didn't.",
            "I didn't exaggerate the mathematics here, but now it's going to get simpler again.",
            "So what's the simplest possible algorithm we can do with such kernels?",
            "So this is actually an algorithm is trivial algorithm that I came up with when I was working on the 1st chapter of book on Kernel Machines.",
            "I was thinking what's the simplest thing one could do which has some machine learning flavor and uses kernels Now thought?",
            "Well, let's take a two class classification problem.",
            "We have these circles and we have the pluses.",
            "These are two classes and will simply a classification rule will say given the new point we will assign it to the class whose mean is closer.",
            "So I compute the mean of this class, compute the mean of this class.",
            "And for instance, with this point I'll be closer to here, so I'll say it belongs to this class.",
            "If you think about what kind of decision rule does this induce, the set of points that are closer to here than they are to here is actually a half space.",
            "So everything over here will be closer to that point, and you get the half space by just drawing this line between the two centroids of the two datasets, positive and negative, and then you take a hyperplane orthogonal to that vector here and intersecting it.",
            "Intersecting this line in the middle.",
            "So this is a simple hyperplane classifier is another support vector machine, but not so different now.",
            "The only nontrivial thing is we will do this in our reproducing kernel Hilbert space will do this in the future space, and surprisingly it turns out this gives something that's well known in statistics.",
            "So we do this in our kernel space.",
            "And the different ways of doing it.",
            "So remember, if we want to Colonel Eison algorithm.",
            "So this is a simple linear algorithm.",
            "So we can carry out this algorithm by computing dot products, because we only need things like angles and distances.",
            "So for instance, one thing would be we just compute given an arbitrary point of actually mapped into the feature space.",
            "And we do the same for the positive class, so that let's say these are positive points.",
            "We map them all into the feature space.",
            "So this is the sum of all positive points.",
            "We map them into feature space.",
            "We divide by the number of positive points, same for the negative points.",
            "So we compute all these quantities.",
            "We write them down formally, so now they could be vectors in an infinite dimensional space.",
            "But we don't worry because we just have to make sure that all these vectors pair up into products.",
            "And once they are dot products we can replace them by kernels.",
            "So we'll do that and we could, for instance, compute the distance between this one and this one and compare it to the distance between this and this.",
            "You know, distances are just norms of different vectors, so norms of difference vectors we can write down these things.",
            "Norms are just dot products, so everything can be replaced by dot products.",
            "That would be one way.",
            "There are other ways of doing this.",
            "We could also say we take this vector W and we also write down this vector X -- C, where C is the average of the two class means, and then we have to check.",
            "Whether these vectors enclose an angle larger or smaller than 90 degrees.",
            "If you remember a dot product computes the cosine of the angle, then you directly see.",
            "This actually can be read off, you just have to check whether the product is positive or negative.",
            "So there are different ways and maybe we don't have to go into details on that.",
            "It's a little exercise, so I don't want to.",
            "Have you all go quiet again for 10 minutes now?",
            "Because we don't have that much time left, but I recommend recommend you could do it afterwards."
        ],
        [
            "Just takes you a few minutes so we'll we'll put these slides on the web anyway, so you can also look this up in here, but it's a little exercise, but if you do this, it turns out you get a decision function.",
            "Which looks like like that.",
            "So this decision function has a term there."
        ],
        [
            "The term that comes from the mean of the positive class, so taking DOT products between this and some fire of X."
        ],
        [
            "Like terms, so it it gives us a term like this which has.",
            "DOT products between all positive points and five X we get a term similar term for all negative points and we get a term B which is all the quantities that don't depend on X on the test point.",
            "Now we replace these dot products by kernel functions and then we have this expression here.",
            "So basically what this tells us is that here.",
            "So let's consider the special case where K can be.",
            "Thought of as a density model.",
            "So for instance, if we use a Gaussian kernel which is normalized to have integral one But actually this is more general.",
            "This works for all positive definite kernels.",
            "And.",
            "So, but think of the Gaussian, then this thing.",
            "Here is something that statisticians call opposing Windows density estimates of the positive class.",
            "So you put one kernel on each positive point.",
            "You normalize by the number of positive points.",
            "So this has integral one.",
            "This density estimate of the positive class.",
            "This is the density estimate of the negative class.",
            "Here we have some offset.",
            "Let's not worry about that.",
            "But roughly speaking, this means we are checking whether it's more likely that the point comes from the positive class.",
            "All from the negative class, so this."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to summarize our results from yesterday, so this is one of the.",
                    "label": 0
                },
                {
                    "sent": "This is one of the bonds that we can derive now.",
                    "label": 1
                },
                {
                    "sent": "It tells us that for any function that can be implemented by the learning machine or by our class of functions, that the learning machine is allowed to choose from.",
                    "label": 0
                },
                {
                    "sent": "With the probability or let's say with large probabilities.",
                    "label": 0
                },
                {
                    "sent": "So I think of Delta is some small number.",
                    "label": 0
                },
                {
                    "sent": "The test error of the learning machine is upper bounded by the training error plus some term that depends on the VC dimension.",
                    "label": 1
                },
                {
                    "sent": "And when I say with a high probability, I mean with high probability over sampling training sets.",
                    "label": 0
                },
                {
                    "sent": "So I think I've said that yesterday, but it's important to stress that, so it means that.",
                    "label": 0
                },
                {
                    "sent": "And it's not for one fixed training set.",
                    "label": 0
                },
                {
                    "sent": "Typically in an application you have one given training set and you want to do the best possible job for that.",
                    "label": 0
                },
                {
                    "sent": "So this statement is not quite the ideal thing for that.",
                    "label": 0
                },
                {
                    "sent": "It doesn't speak about that given training set.",
                    "label": 0
                },
                {
                    "sent": "So this is a slightly different story and the same qualification applies to what I said about training on the test set and things like that, so it's a slightly different statement.",
                    "label": 0
                },
                {
                    "sent": "Still, a very useful statement.",
                    "label": 0
                },
                {
                    "sent": "Just bear in mind what it means, so it means we are unlikely to find a training set for which this inequality is violated, and likewise if we talk about training on the test set or cheating as we talk college yesterday we are unlikely to find a test set.",
                    "label": 0
                },
                {
                    "sent": "For which.",
                    "label": 0
                },
                {
                    "sent": "Our estimate of the test error is far from the truth, but for one given test set we might be unlucky.",
                    "label": 0
                },
                {
                    "sent": "So, so this was our bond.",
                    "label": 0
                },
                {
                    "sent": "So here we have the tradeoff between trying to minimize the training error and trying to have a small complexity terms or trying to choose a function class with a smaller we see dimension.",
                    "label": 0
                },
                {
                    "sent": "If we choose a function class with a small VC dimension, then typically it will be the case that we cannot achieve such a small training error, so we'd like to have a bigger function class to get a smaller training error.",
                    "label": 1
                },
                {
                    "sent": "But then the VC dimension goes up, so there's a tradeoff between needs these two things.",
                    "label": 0
                },
                {
                    "sent": "And this bound doesn't guarantee us that for any given problem, we will have a small error.",
                    "label": 0
                },
                {
                    "sent": "It just tells us that for any given problem, if we choose a reasonably small VC dimension and we have a sort of compared to the sample size, then it's unlikely that our test error would be a lot larger than the training error.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't tell us how to choose function classes for which we can get small training error, even though the complex.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is more.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So function classes are important.",
                    "label": 0
                },
                {
                    "sent": "We would like to find good function classes.",
                    "label": 0
                },
                {
                    "sent": "Ideally even we would like to have function classes where we can compute the VC dimension or other capacity measures.",
                    "label": 0
                },
                {
                    "sent": "That would be nice because the VC dimension is this complicated commute Oriel thing I've told you about.",
                    "label": 0
                },
                {
                    "sent": "It's about the maximum number of points which can be separated in all possible ways.",
                    "label": 0
                },
                {
                    "sent": "So I can imagine, or you can probably imagine if I give you some learning machine which can implement a function class, you might have a hard time computing the VC dimension, so that's that's difficult.",
                    "label": 1
                },
                {
                    "sent": "We can do it for separating hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "We can't do it for just any function class.",
                    "label": 0
                },
                {
                    "sent": "So from theoretical theoretical point of view, we would like to work with separating hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "We can handle those from the practical point of view.",
                    "label": 0
                },
                {
                    "sent": "Maybe we don't want to wear with hyperplanes because they are quite limited, they only induce linear separations of the data.",
                    "label": 0
                },
                {
                    "sent": "So we would like to use something more complicated, and that's what kernels are bought.",
                    "label": 0
                },
                {
                    "sent": "We will try to do the hyperplanes in some other high dimensional space which can be non linearly related with the input space.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we construct this space?",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we pre process all data points with some fixed mapping.",
                    "label": 0
                },
                {
                    "sent": "Fire takes our input domain so this X is our input domain.",
                    "label": 0
                },
                {
                    "sent": "We haven't made any assumptions about it other that other than it being an nonempty set.",
                    "label": 0
                },
                {
                    "sent": "So it could be a set of discrete objects.",
                    "label": 0
                },
                {
                    "sent": "It could be a vector space.",
                    "label": 0
                },
                {
                    "sent": "So far it takes our input domain into some dot product space, so that's a vector space with a dot product and.",
                    "label": 0
                },
                {
                    "sent": "Are there people who don't know what the dot product is?",
                    "label": 0
                },
                {
                    "sent": "Just maybe, maybe you've all heard of that.",
                    "label": 0
                },
                {
                    "sent": "So the product is a very useful concept from geometry, which allows you to compute things like distances and angles.",
                    "label": 0
                },
                {
                    "sent": "So this is a dot product space, A vector space with the dot product, and then once we've pre processed the points like that.",
                    "label": 1
                },
                {
                    "sent": "And we can do whatever learning tasks we're interested in solving based on the fire of the input points rather than the input points directly.",
                    "label": 0
                },
                {
                    "sent": "So typically this space is high dimensional.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And we're not really scared of that, because now we have seen that the main issue in machine learning.",
                    "label": 0
                },
                {
                    "sent": "If you want to generalize is the capacity and maybe not the dimensionality.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in some cases.",
                    "label": 0
                },
                {
                    "sent": "Like what I told you about separating hyperplane standard VC dimension bound will be in plus one for in dimensional hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "So it looks like here that we see dimension is basically the same as the dimensionality of the space.",
                    "label": 0
                },
                {
                    "sent": "So that means in high dimensional spaces we would be in trouble.",
                    "label": 0
                },
                {
                    "sent": "We would have a very large money BC dimension.",
                    "label": 0
                },
                {
                    "sent": "However, I also mentioned that if we look at hyperplanes that induce separations with large margin, then the VC dimension can have a much smaller bound.",
                    "label": 0
                },
                {
                    "sent": "So in that case we shouldn't be scared.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The high dimensionality.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see.",
                    "label": 0
                },
                {
                    "sent": "Here's an example.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have these two classes of data points that crosses and circles.",
                    "label": 0
                },
                {
                    "sent": "We are only given the data points, but not this separation separating boundaries.",
                    "label": 0
                },
                {
                    "sent": "We assume that the true optimal separation is this ellipse here.",
                    "label": 0
                },
                {
                    "sent": "And we already given the points.",
                    "label": 0
                },
                {
                    "sent": "Now suppose someone told you you should be pre processing your data points by computing all features of order two in the input coordinates.",
                    "label": 0
                },
                {
                    "sent": "There's just two input coordinates, so there's only three possible features of order 2.",
                    "label": 0
                },
                {
                    "sent": "Protective order Two is X1 squared, X2 squared and X 1 * X Two.",
                    "label": 0
                },
                {
                    "sent": "And don't worry about this square root here for the second for the moment, so we compute these three features and now we think about finding the separation in that space.",
                    "label": 0
                },
                {
                    "sent": "And if you think about it for a second.",
                    "label": 0
                },
                {
                    "sent": "If you write the ellipse equation in terms of X1 squared and X2 squared, then for this simple ellipse here the.",
                    "label": 0
                },
                {
                    "sent": "Separation boundary becomes hyper linear hyperplane and in fact in this case it's even a linear hyperplane that only depends on X1 squared and X2 squared.",
                    "label": 0
                },
                {
                    "sent": "It doesn't depend on these mixed, so if this ellipse were not access aligned, it would be also depending on this term.",
                    "label": 0
                },
                {
                    "sent": "But that doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "The point is, in this 3 dimensional space we can solve this problem with the linear hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So if we had known that this is our solution, then it would be a good idea to map into this 3 dimensional space and then instead do a hyperplane and from the point of view of learning theory, that's nice because hyperplanes are easier to handle.",
                    "label": 0
                },
                {
                    "sent": "We can compute the VC dimension and things like that.",
                    "label": 0
                },
                {
                    "sent": "So that's a nice.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do and actually we would like to do it with higher order features and higher dimensionality's.",
                    "label": 0
                },
                {
                    "sent": "So we would like to do this for N dimensional inputs and this is a pretty old slide.",
                    "label": 0
                },
                {
                    "sent": "I've shown it many times and these images look very low resolution and somehow I'm not sure it might be my memory, But this this data set used to be state of the art data set some years ago.",
                    "label": 0
                },
                {
                    "sent": "Now it looks like a little bit ridiculous, but it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "We can also hire take higher dimensional inputs, any dimensionality in, and consider products of order D if we wanted to work in that space then the dimensionality of the space would grow.",
                    "label": 0
                },
                {
                    "sent": "Like into the power of the.",
                    "label": 0
                },
                {
                    "sent": "So even for this low resolution images we already have 10 to that end.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mentions here so it looks like we're in trouble, but there's a nice trick which is referred to as the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "Which consists of mapping the points into such a feature space.",
                    "label": 0
                },
                {
                    "sent": "And then taking the DOT product between two such points.",
                    "label": 0
                },
                {
                    "sent": "So even if this mapping is in the high dimensional space, it turns out we can compute the dot product without actually carrying out this mapping.",
                    "label": 0
                },
                {
                    "sent": "So to see that, let's do.",
                    "label": 0
                },
                {
                    "sent": "Let's take two points and map them.",
                    "label": 0
                },
                {
                    "sent": "First, into the three dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So this is our low dimensional example from before.",
                    "label": 0
                },
                {
                    "sent": "So here's the first image of the first point.",
                    "label": 0
                },
                {
                    "sent": "Here's the image of the second point, and remember that product the Canonical dot product is just.",
                    "label": 0
                },
                {
                    "sent": "We take this vector times the transpose of this vector, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a sum over product of corresponding entries.",
                    "label": 0
                },
                {
                    "sent": "And if you think about this, just look at this so we have X 1 ^2 * X one prime squared plus this thing here times this thing.",
                    "label": 0
                },
                {
                    "sent": "Plus this thing times this thing.",
                    "label": 0
                },
                {
                    "sent": "If you write this down and if you don't see it you can do it as a little exercise you will see it's a complete binomial formula which can be rewritten as the dot product in the original space raised to the power of tool.",
                    "label": 0
                },
                {
                    "sent": "So that's nice.",
                    "label": 0
                },
                {
                    "sent": "That's a simple function that we can compute in the original space.",
                    "label": 0
                },
                {
                    "sent": "We don't have to carry out fire.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And actually the same trick works more generally, and now we go backwards.",
                    "label": 0
                },
                {
                    "sent": "Let's just assume we have N dimensional points and we start from this expression here.",
                    "label": 0
                },
                {
                    "sent": "So we raised this to the power of the.",
                    "label": 0
                },
                {
                    "sent": "So we write down our Canonical dot product again.",
                    "label": 0
                },
                {
                    "sent": "This formula here we raised to the power of the so we have D such sums.",
                    "label": 0
                },
                {
                    "sent": "And to work it out we use J1 for the first one and JD for the last one.",
                    "label": 0
                },
                {
                    "sent": "Now if we multiply it out, we have this default sum and here we have the products of all these terms occurring back here and I have sorted them such that the X terms are first in the ex prime terms come second.",
                    "label": 0
                },
                {
                    "sent": "So what you can see here.",
                    "label": 0
                },
                {
                    "sent": "So this is a dot product in an end to the power of the dimensional space or or it's a it's a sum with into the power of the terms.",
                    "label": 0
                },
                {
                    "sent": "And each term is a product of some function of X times the same function of X prime.",
                    "label": 0
                },
                {
                    "sent": "So this can actually be interpreted as a dot product in an end to the power of the dimensional space of some nonlinear transformation of X&X prime.",
                    "label": 0
                },
                {
                    "sent": "And we can read off the nonlinear transformation from this formula here.",
                    "label": 0
                },
                {
                    "sent": "So this is just a product of D entries of the vector X.",
                    "label": 0
                },
                {
                    "sent": "This is the same product of course.",
                    "label": 0
                },
                {
                    "sent": "Likewise for the vector X prime.",
                    "label": 0
                },
                {
                    "sent": "So this is a dot product in this space spent by all products of the input directions, and here are these are even ordered products because.",
                    "label": 1
                },
                {
                    "sent": "Some of these because multiplication is commutative, so some products occur many times in this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's why I had this factor.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Square root 2 in here.",
                    "label": 0
                },
                {
                    "sent": "So some product occur many times, but let's not worry about that.",
                    "label": 0
                },
                {
                    "sent": "It's a dot product in this space spent by all products of the input features.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This quantity here, which is called a kernel computer product in a high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "The question is, are there other other functions?",
                    "label": 0
                },
                {
                    "sent": "Other kernels that compute dot products in high dimensional spaces?",
                    "label": 0
                },
                {
                    "sent": "After applying some nonlinearity to the input directions and the answer is yes and this can be seen in various ways, traditional way was to appeal to an old theorem.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functional analysis called.",
                    "label": 0
                },
                {
                    "sent": "This is theorem and which talks about kernels of integral operators.",
                    "label": 0
                },
                {
                    "sent": "And if you don't know what an integral operator is, don't worry.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go in detail on that, so this theorem just tells us if we have a positive definite integral operator by positive definite, I mean this equality here is true for all functions.",
                    "label": 0
                },
                {
                    "sent": "Then we can expand this kernel in such a infinite sum.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit like a matrix diagonalization.",
                    "label": 0
                },
                {
                    "sent": "Where the lamp ties are non negative and if you look at this some you can probably already guessed.",
                    "label": 0
                },
                {
                    "sent": "This also looks a little bit like a different product in a high dimensional space and the nonlinearity comes from this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fire fun.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oceans.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The more nowadays people use a slightly different class of kernels.",
                    "label": 0
                },
                {
                    "sent": "And very slightly larger class of kernels which are called positive definite kernels.",
                    "label": 1
                },
                {
                    "sent": "And it turns out that this is exactly the right class of kernels that do correspond to DOT products in high dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "So what's a positive definite kernel?",
                    "label": 0
                },
                {
                    "sent": "It's a kernel which function of two arguments which is symmetric in the two arguments and which has the property that whatever set of training points, I take whatever set of real coefficients A1 through AM.",
                    "label": 1
                },
                {
                    "sent": "This inequality here is true, where K is the matrix that we get by substituting pairs of training points into the kernel.",
                    "label": 1
                },
                {
                    "sent": "So this is also called the gram matrix of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "So this matrix is this saying nothing but that this matrix is positive definite.",
                    "label": 0
                },
                {
                    "sent": "Some people call it positive semidefinite.",
                    "label": 1
                },
                {
                    "sent": "So in the kernel community and also in some branches of approximation theory, this is called a positive definite matrix and the stronger requirement.",
                    "label": 0
                },
                {
                    "sent": "So if we Additionally require that for pairwise distinct points.",
                    "label": 1
                },
                {
                    "sent": "So if no training point appears twice in the set.",
                    "label": 0
                },
                {
                    "sent": "This quantity should only be 0 if these coefficients are zero, so that's a slightly an additional requirement.",
                    "label": 0
                },
                {
                    "sent": "Then we would call this matrix strictly positive definite in the kernel.",
                    "label": 0
                },
                {
                    "sent": "Also strictly positive definite.",
                    "label": 0
                },
                {
                    "sent": "So don't get confused about terminology.",
                    "label": 0
                },
                {
                    "sent": "Some people refer to this as positive semidefinite and positive definite.",
                    "label": 0
                },
                {
                    "sent": "Calling it positive definite and strictly positive definite and maybe the main reason is that we mostly talk about positive definite.",
                    "label": 0
                },
                {
                    "sent": "And this is the short term then.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a condition for a kernel and I'll show you how to prove that this then corresponds to a dot product in a high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Or maybe we'll try to show it to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what are the main?",
                    "label": 0
                },
                {
                    "sent": "What's the point about this kernel trick?",
                    "label": 0
                },
                {
                    "sent": "So if we have some algorithm, so maybe now.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't get confused about the general case.",
                    "label": 0
                },
                {
                    "sent": "Think of a kernel in this form again, so we said this kernel computes.",
                    "label": 0
                },
                {
                    "sent": "A DOT product in some high dimensional space, so it's implicitly it's carry out, carries out this nonlinearity fire and then compute the dot product there.",
                    "label": 0
                },
                {
                    "sent": "So if we have some algorithm that we can write in terms of DOT products only, so some algorithm that maybe only uses distances and angles in some space, then we can carry out this algorithm implicitly in the space that fire Maps into.",
                    "label": 0
                },
                {
                    "sent": "By simply replacing all occurrences of dot products by such a kernel.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So any algorithm that only depends on the products can be kernelized as people say, and this way we can apply linear methods even to non vectorial data because as I mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before there is no.",
                    "label": 0
                },
                {
                    "sent": "Distance didn't say here, but there is no assumption on X other than it being a nonempty set.",
                    "label": 0
                },
                {
                    "sent": "So if we have some set of discrete objects and we are able to define a kernel function which satisfies this requirement of positive definiteness, then we get our representation of the kernel as a dot product in a high dimensional dot product space in a vector space.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For free, so we have a vectorial representation for the data automatically.",
                    "label": 0
                },
                {
                    "sent": "We don't have to worry about the data being vectors to begin with.",
                    "label": 0
                },
                {
                    "sent": "So we think of this kernel as some kind of similarity measure.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a particular type similarity measure positive definite one.",
                    "label": 0
                },
                {
                    "sent": "Anne and Neil Lawrence already mentioned yesterday.",
                    "label": 0
                },
                {
                    "sent": "These kernels are in the field of Gaussian process regression, also known as covariate.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Maybe the most important thing about this definition here or in general, when you see a new definition in mathematics, is to play around a bit with it to understand it.",
                    "label": 0
                },
                {
                    "sent": "So I would suggest to that we take a few minutes and everybody tries to play around with this and prove a few things which I have on this slide here.",
                    "label": 0
                },
                {
                    "sent": "So I'll give you some hints, maybe so these are ordered in may be increasing difficulty, but all of them are.",
                    "label": 0
                },
                {
                    "sent": "Reasonably easy to prove.",
                    "label": 0
                },
                {
                    "sent": "So in the first thing I would like you to prove that.",
                    "label": 0
                },
                {
                    "sent": "If maybe I have to write them down somewhere because I guess you want to know this.",
                    "label": 0
                },
                {
                    "sent": "Definite maybe.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can copy this definition, or you can try to remember it.",
                    "label": 0
                },
                {
                    "sent": "So some of IDAIAJ of.",
                    "label": 0
                },
                {
                    "sent": "This thing here should be no negative, so this is what we know.",
                    "label": 0
                },
                {
                    "sent": "If a kernel is positive definite and Moreover we know it's symmetric, so at least it leave this up for a few seconds and then I'll say something about the next slide.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll move to the next slide, so here in the first point.",
                    "label": 0
                },
                {
                    "sent": "I want you to prove that if we define our kernel to be equal to this thing where fire is some mapping into some dot product space.",
                    "label": 0
                },
                {
                    "sent": "Then the set the dust defined kernel is positive definite, so this object here is a positive definite kernel, so that's the first first thing to prove.",
                    "label": 0
                },
                {
                    "sent": "The second thing to prove is kernels are positive on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "So if a kernel is positive definite, so this is a little bit short, this statement.",
                    "label": 0
                },
                {
                    "sent": "Here we assume the kernel is positive definite and we prove that this implies that K of X, X is non negative.",
                    "label": 0
                },
                {
                    "sent": "3rd part is a generalized Cauchy Schwarz inequality, so again, we assume the kernel is positive definite and we want to prove that then this inequality here is true.",
                    "label": 0
                },
                {
                    "sent": "And finally, again, we assume the kernel is positive definite and we want to prove that.",
                    "label": 0
                },
                {
                    "sent": "If the diagonal is 0, so if K of XX is 0 for all X, then the kernel is actually zero everywhere, even if X&X prime are different.",
                    "label": 0
                },
                {
                    "sent": "So again, reminder in the first one, we only assume Phi is some mapping and we want to prove that this is a positive definite kernel in.",
                    "label": 0
                },
                {
                    "sent": "In the rest, we always assume K is a positive definite kernel.",
                    "label": 1
                },
                {
                    "sent": "So I'll give you a few minutes now and then.",
                    "label": 0
                },
                {
                    "sent": "We can do it together.",
                    "label": 0
                },
                {
                    "sent": "OK, someone's got the second one here.",
                    "label": 0
                },
                {
                    "sent": "Maybe keep going until you have all of them and then will do all of them together at the end.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Can I get a quick show of hands?",
                    "label": 0
                },
                {
                    "sent": "How many people have solved one or more problems?",
                    "label": 0
                },
                {
                    "sent": "How many have two or more?",
                    "label": 0
                },
                {
                    "sent": "Three or more.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe I'll give you a little bit more time and then we do it together.",
                    "label": 0
                },
                {
                    "sent": "So let me see whether I should give specific hints about some of them, so maybe a show of hands who has solved the first one.",
                    "label": 0
                },
                {
                    "sent": "And I'm not gonna ask you to get up now so you can.",
                    "label": 0
                },
                {
                    "sent": "You can be honest if you think you've solved it.",
                    "label": 0
                },
                {
                    "sent": "Just just raise your hand.",
                    "label": 0
                },
                {
                    "sent": "OK, who has solved the second one?",
                    "label": 0
                },
                {
                    "sent": "Third one.",
                    "label": 0
                },
                {
                    "sent": "So the first one is a more difficult one.",
                    "label": 0
                },
                {
                    "sent": "It seems in the 4th one.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's also a risk relatively small set, so the 4th one becomes easy if you use the result of the third one.",
                    "label": 0
                },
                {
                    "sent": "So that's one hint.",
                    "label": 0
                },
                {
                    "sent": "So then it will take 2 more minutes and then with them together.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is, I'll ask for volunteers, so if possible will do it with volunteers.",
                    "label": 0
                },
                {
                    "sent": "Otherwise I'll have to volunteer myself, but I would prefer some of you do it.",
                    "label": 0
                },
                {
                    "sent": "So is there anyone who would be willing?",
                    "label": 0
                },
                {
                    "sent": "With my help potentially to do the first one here on this projector.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So we can right here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, right?",
                    "label": 0
                },
                {
                    "sent": "It's it's on.",
                    "label": 0
                },
                {
                    "sent": "Write down for the graphics.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "He's gonna use more space.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Tickets well it is.",
                    "label": 0
                },
                {
                    "sent": "X1X1 multi-purpose XMX 10.",
                    "label": 0
                },
                {
                    "sent": "Divided minus X1.",
                    "label": 0
                },
                {
                    "sent": "Actually this escalates to then minus X one X2 and X2X1.",
                    "label": 0
                },
                {
                    "sent": "Then you can use the.",
                    "label": 0
                },
                {
                    "sent": "Fish plus point India.",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "So you want to use this cozy Schwarz inequality, or, well, if it's a kernel.",
                    "label": 0
                },
                {
                    "sent": "I already know that in that space you have and not the usual procedure works inequality.",
                    "label": 0
                },
                {
                    "sent": "You can use right?",
                    "label": 0
                },
                {
                    "sent": "You couldn't use this one.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "We don't know yet whether it's a kernel.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I can first prove that one that one is OK. Then why don't you prove this one and then?",
                    "label": 0
                },
                {
                    "sent": "Well, we already know it's a positive, so it's just like.",
                    "label": 0
                },
                {
                    "sent": "You can sit down if you like or up to you.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically the issue right up to battleground matrix.",
                    "label": 0
                },
                {
                    "sent": "A lot.",
                    "label": 0
                },
                {
                    "sent": "Yes, excellent.",
                    "label": 0
                },
                {
                    "sent": "Because this thing is because KX1X2.",
                    "label": 0
                },
                {
                    "sent": "Is the same as a 6 plus well, OK?",
                    "label": 0
                },
                {
                    "sent": "It's well paid.",
                    "label": 0
                },
                {
                    "sent": "Minus KX1X2 square then, because by definition this thing is.",
                    "label": 0
                },
                {
                    "sent": "So why is it larger or equal to 0?",
                    "label": 0
                },
                {
                    "sent": "Because?",
                    "label": 0
                },
                {
                    "sent": "So the matrix is positive definite, and I mean one way to say it.",
                    "label": 0
                },
                {
                    "sent": "For instance, is positive definite matrices have non negative eigenvalues and the determinant is the product of the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So the determinant is non negative.",
                    "label": 0
                },
                {
                    "sent": "OK, so do people see that so?",
                    "label": 0
                },
                {
                    "sent": "This thing here you can easily convert it into this one.",
                    "label": 0
                },
                {
                    "sent": "OK so so maybe maybe we'll ask someone else to do the first one.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much for solving this one.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So and here, just in case people are copying this, we also have K. OK, so we've solved the third one.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "It has to be about product because like the garden is positive definite.",
                    "label": 0
                },
                {
                    "sent": "So what exactly happened in addition of the product and then put another product?",
                    "label": 0
                },
                {
                    "sent": "Well, that's true if you know that the kernel corresponds to a dot product in some other space.",
                    "label": 0
                },
                {
                    "sent": "Just ask.",
                    "label": 0
                },
                {
                    "sent": "But do you have to prove that this is a dot product, right?",
                    "label": 0
                },
                {
                    "sent": "I have this.",
                    "label": 0
                },
                {
                    "sent": "Well, if you can prove these things then then you can do it like that.",
                    "label": 0
                },
                {
                    "sent": "You know, I'm not sure whether it will be easier like this easier than this, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean if you prove that it's the dot product.",
                    "label": 0
                },
                {
                    "sent": "Although actually I would be surprised if you could prove that this this is a dot product that I mean.",
                    "label": 0
                },
                {
                    "sent": "OK, you're not going to say that this is a dot product.",
                    "label": 0
                },
                {
                    "sent": "You could only say it corresponds to the door products.",
                    "label": 0
                },
                {
                    "sent": "Another space, because this can be a nonlinear function of X next prime, right?",
                    "label": 0
                },
                {
                    "sent": "And if you want to show that it corresponds to a dot product in some other space, you basically have to solve the whole problem.",
                    "label": 0
                },
                {
                    "sent": "That will solve in the next three slides.",
                    "label": 0
                },
                {
                    "sent": "So let's say if you find a simpler proof for that, then I would be.",
                    "label": 0
                },
                {
                    "sent": "I would be happy to see it.",
                    "label": 0
                },
                {
                    "sent": "And OK so because yeah, maybe I should have said that.",
                    "label": 0
                },
                {
                    "sent": "So the reason why we want to prove all these things?",
                    "label": 0
                },
                {
                    "sent": "Because once we know them, we will be able to construct the other space in which K computes a dot product or which K can be represented as a dot product.",
                    "label": 0
                },
                {
                    "sent": "OK, so the so we've solved this one.",
                    "label": 0
                },
                {
                    "sent": "Let's try to solve one of the other ones and.",
                    "label": 0
                },
                {
                    "sent": "I was a little bit worried that this was starting to look a little bit complicated number one, so that's why I was happy that you also had #3, so maybe someone else wants to do #1 because there's a simpler, simple way for doing #1, yes?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "No, sorry.",
                    "label": 0
                },
                {
                    "sent": "Write down.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can say living what what are you using here?",
                    "label": 0
                },
                {
                    "sent": "I'm just using linearity of the.",
                    "label": 0
                },
                {
                    "sent": "Right, so yeah, that's correct.",
                    "label": 0
                },
                {
                    "sent": "So we were assuming that this thing with the angular brackets is dot product, so he can use linearity of that product.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Then we.",
                    "label": 0
                },
                {
                    "sent": "We can change it in less.",
                    "label": 0
                },
                {
                    "sent": "I can collect Jay.",
                    "label": 0
                },
                {
                    "sent": "This one is the norm.",
                    "label": 0
                },
                {
                    "sent": "Exactly, thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the proof.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So any questions about this so he's using the fact that this object here is a dot product, so therefore it's by linear is linear in both arguments.",
                    "label": 0
                },
                {
                    "sent": "Linearity means you can take these coefficients in and out and you can take the sums in and out.",
                    "label": 0
                },
                {
                    "sent": "And then he's taking this.",
                    "label": 0
                },
                {
                    "sent": "Some of the eyes in here, some over the days in here.",
                    "label": 0
                },
                {
                    "sent": "Likewise with the coefficients, and then he sees that this is actually the same vector as this one.",
                    "label": 0
                },
                {
                    "sent": "And so it's it's a norm of that vector is not negative OK?",
                    "label": 0
                },
                {
                    "sent": "So how about #2?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Choosing so in this case, now we assume K is a positive definite kernel to begin with, right?",
                    "label": 0
                },
                {
                    "sent": "We can eat from the automobile is like.",
                    "label": 0
                },
                {
                    "sent": "And some heroes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Basically, take the.",
                    "label": 0
                },
                {
                    "sent": "OK. Just stand there.",
                    "label": 0
                },
                {
                    "sent": "I bet yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to make sure that everybody understands it, so he's saying this thing here is.",
                    "label": 0
                },
                {
                    "sent": "Non negative, and therefore this also is not negative, and that's correct.",
                    "label": 0
                },
                {
                    "sent": "It's possible to do it even simpler if you want, but doesn't make a big difference, so we can Even so he's saying we no matter what training set we have.",
                    "label": 0
                },
                {
                    "sent": "If we.",
                    "label": 0
                },
                {
                    "sent": "Take us in a vector.",
                    "label": 0
                },
                {
                    "sent": "One of these Canonical basis vectors.",
                    "label": 0
                },
                {
                    "sent": "Then this will pick out just one of the training points, and then we see that if X is the training point, then this inequality is true.",
                    "label": 0
                },
                {
                    "sent": "That's correct, you could make it even simpler if you want, because in the definition it says up here for any set of training points.",
                    "label": 1
                },
                {
                    "sent": "So I could even pick a set of training points which only contains that.",
                    "label": 0
                },
                {
                    "sent": "One point X right and then I could say the corresponding coefficient.",
                    "label": 0
                },
                {
                    "sent": "I'll call it a or.",
                    "label": 0
                },
                {
                    "sent": "I could even say I'll use the coefficient one because this also says for any coefficients.",
                    "label": 0
                },
                {
                    "sent": "And then this inequality here whoops up.",
                    "label": 0
                },
                {
                    "sent": "This simply becomes some over one term 1 * 1 * K One one, and then it's the same by this, correct?",
                    "label": 0
                },
                {
                    "sent": "OK, so so we've solved this one.",
                    "label": 0
                },
                {
                    "sent": "Only one left number for anyone from before.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is question is whether this should be proved for any days only.",
                    "label": 0
                },
                {
                    "sent": "For one a the answer is, well, the statement only says OK.",
                    "label": 0
                },
                {
                    "sent": "The statement.",
                    "label": 0
                },
                {
                    "sent": "Let's put the complete statement again.",
                    "label": 0
                },
                {
                    "sent": "Statement says that for all X.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For all X this inequality is inequality is true, so we only have to prove it independent of X, and in the proof didn't make any restrictions or any assumptions about what is X.",
                    "label": 1
                },
                {
                    "sent": "So this is true for any X. OK, so anyone for number 4.",
                    "label": 0
                },
                {
                    "sent": "OK. You can also say it if you want, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so he's saying that.",
                    "label": 0
                },
                {
                    "sent": "We write XKXX prime squared.",
                    "label": 0
                },
                {
                    "sent": "We know it's upper bounded.",
                    "label": 0
                },
                {
                    "sent": "By this it's prime.",
                    "label": 0
                },
                {
                    "sent": "Now, by assumption this is 0.",
                    "label": 0
                },
                {
                    "sent": "This is also zero.",
                    "label": 0
                },
                {
                    "sent": "So this is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "It's a square, so it's a non negative number which is upper bounded by zero, so therefore it is 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have all of them, any questions?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's go ahead and.",
                    "label": 0
                },
                {
                    "sent": "Prove what we wanted to prove.",
                    "label": 0
                },
                {
                    "sent": "Or construct the feature space.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Oh, so for this one.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the outline of how the construction will work, and I think it's nice to see this construction once because a lot of people work with kernels and use kernels in different domains.",
                    "label": 0
                },
                {
                    "sent": "I mean, even if you're not doing support vector machines, some of the kernel trick is is a standard trick 9 machine learning.",
                    "label": 1
                },
                {
                    "sent": "It's a little bit like using the chain rule.",
                    "label": 0
                },
                {
                    "sent": "You should be able to use the kernel trick, 'cause you might one day come up with a linear algorithm that you want to make nonlinear, so a lot of people use it, but.",
                    "label": 0
                },
                {
                    "sent": "A lot of people have never actually seen how to construct the feature space with it, and why positive definite kernels correspond to their products in other spaces.",
                    "label": 0
                },
                {
                    "sent": "And actually, it's not so difficult to see, so I think you should.",
                    "label": 0
                },
                {
                    "sent": "You should see this now in the next 10 or 15 minutes.",
                    "label": 0
                },
                {
                    "sent": "So here's the idea.",
                    "label": 0
                },
                {
                    "sent": "We have to construct some feature map with the into some vector space first and then in this vector space or linear space we have to construct a dot product, not just any dot product.",
                    "label": 0
                },
                {
                    "sent": "We want to have a dot product such that the kernel is equal to this dot product in this sense here.",
                    "label": 1
                },
                {
                    "sent": "So it's a special type of dot product.",
                    "label": 0
                },
                {
                    "sent": "And so the main work will be to show that the thing that we will construct has the properties of that product.",
                    "label": 0
                },
                {
                    "sent": "And then we're almost done, so let's see.",
                    "label": 0
                },
                {
                    "sent": "So the our mapping will define first the fine mapping is taking a point from the input domain and mapping it to a function, and it will be the function that we get by substituting the point into one of the arguments that, let's say the second argument of the kernel and keeping the first argument open.",
                    "label": 0
                },
                {
                    "sent": "OK, so this object here is a function because it is still varies or it still depends on this first argument.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if we use a Gaussian kernel function.",
                    "label": 0
                },
                {
                    "sent": "Then this means a point X will be mapped to a Gaussian function which is centered on this point X.",
                    "label": 0
                },
                {
                    "sent": "So let's say here is the X point and now this is a function around X and likewise appoint X prime will be mapped to a kernel function centered on X prime, so this is a. Mapping into space of functions and if you have never seen this notation, this notation just means all functions and mapping from X to R, so all real valued functions on this domain X. OK, so let's.",
                    "label": 0
                },
                {
                    "sent": "Let's look at this mapping, so remember this mapping.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we will first turn it into a linear.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Place into a vector space.",
                    "label": 1
                },
                {
                    "sent": "So what is a vector space?",
                    "label": 0
                },
                {
                    "sent": "The main property of a vector space?",
                    "label": 0
                },
                {
                    "sent": "If is.",
                    "label": 0
                },
                {
                    "sent": "If I take 2 elements of that space, I can add them up and I'm still in the space.",
                    "label": 0
                },
                {
                    "sent": "This the second main properties if I'm multiplier an element of the vector space with a number.",
                    "label": 0
                },
                {
                    "sent": "I still get an element of the vector space, so it's a.",
                    "label": 0
                },
                {
                    "sent": "This is a non trivial thing.",
                    "label": 0
                },
                {
                    "sent": "This is this is true and but actually there's a trivial way of ensuring that is.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or in our case, and in our case, we will simply declare that our space contains all functions of this form.",
                    "label": 0
                },
                {
                    "sent": "So if I map individual points into the space, I get functions of this form and now I'll just say, well, I will just declare that all.",
                    "label": 0
                },
                {
                    "sent": "If I multiply such a function with a number, I'm still in my space, or if I add up several such functions, I'm still in that space.",
                    "label": 0
                },
                {
                    "sent": "So the typical elements of my vector space will have this form here, so this function F for this function G. Well, these are numbers and the exits are arbitrary points for my original set.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So interrupt me at anytime if you like, so now it becomes a little bit more tricky.",
                    "label": 0
                },
                {
                    "sent": "Now we have to construct the DOT product in this space.",
                    "label": 0
                },
                {
                    "sent": "Well, we don't actually have a choice in control.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this talk product, because as you remember from before, we want the DOT product to satisfy this equality here.",
                    "label": 0
                },
                {
                    "sent": "So if we plug in the mapping the way we defined it, so X goes over to this function sitting on X.",
                    "label": 0
                },
                {
                    "sent": "Then we know that all the products would have this property here.",
                    "label": 0
                },
                {
                    "sent": "Now, in addition, something that we used before we said before DOT products have to be by linear.",
                    "label": 0
                },
                {
                    "sent": "So they have to have this property that if I have a number out here, I can move that number into the dot product without changing the value.",
                    "label": 0
                },
                {
                    "sent": "Or if I have a some outside, I can move the sum inside.",
                    "label": 0
                },
                {
                    "sent": "So this basically this condition here tells us what is the value of the dot product for such simple functions such as individual bumps.",
                    "label": 0
                },
                {
                    "sent": "And then the value of the dot product for linear combinations of bumps already follows from the requirement that the DOT product should be bilinear.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have no choice.",
                    "label": 0
                },
                {
                    "sent": "We have to define.",
                    "label": 0
                },
                {
                    "sent": "The drug product between two genera.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions F&G, so these two functions here.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Should be defined like this.",
                    "label": 0
                },
                {
                    "sent": "Should be there some overall I know Jay Alpha, I better J and this quantity here.",
                    "label": 0
                },
                {
                    "sent": "So if we define it like this it is a bilinear function and now slightly more subtle but still important point from the mathematical point of view is that this thing here should be well defined.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by this?",
                    "label": 0
                },
                {
                    "sent": "M.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if I write a function like this, so this is my function, there might be different ways of writing the same function, maybe in terms of different different coefficients and different centers here on which the functions are sitting.",
                    "label": 0
                },
                {
                    "sent": "That's possible.",
                    "label": 0
                },
                {
                    "sent": "In fact, for many kernels it will really be the case, so this is not a unique expansion of that function.",
                    "label": 0
                },
                {
                    "sent": "So there's not a one to one mapping between the function and these coefficients.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, here it looks like the definition of the dot product depends on these coefficients, so it shouldn't be the case that.",
                    "label": 0
                },
                {
                    "sent": "This thing here, which should be a property only of F&G depends on these alphas betterson this points that are using the expansion.",
                    "label": 0
                },
                {
                    "sent": "That would be bad, but it turns out it doesn't depend on that and the way to see it is the following one.",
                    "label": 0
                },
                {
                    "sent": "I can rewrite this quantity here.",
                    "label": 0
                },
                {
                    "sent": "Two possible ways I can write it like this thing here so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe if I go back to here so remember F is the expansion terms of the other fasten the excess.",
                    "label": 0
                },
                {
                    "sent": "G is the expansion in terms of betterson X primes.",
                    "label": 0
                },
                {
                    "sent": "OK, so try to put it into your.",
                    "label": 0
                },
                {
                    "sent": "Men.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So into your mental imagery for a second or your memory and.",
                    "label": 0
                },
                {
                    "sent": "Therefore, so remember, G was in terms of better than X primes, so if you substitute gene here, you will see you recover this thing.",
                    "label": 0
                },
                {
                    "sent": "Likewise, if you substitute if which had the alphas in the axis in here you again recover this formula.",
                    "label": 0
                },
                {
                    "sent": "So this these are trivial identities.",
                    "label": 0
                },
                {
                    "sent": "But from this as follows.",
                    "label": 0
                },
                {
                    "sent": "So from the first one.",
                    "label": 0
                },
                {
                    "sent": "It follows that this quantity here is independent of the bit as an independent of the ex primes, because better and explain doesn't appear in this formula.",
                    "label": 0
                },
                {
                    "sent": "From the second equality formula follows that this definition here is independent of the alphas and of the axis, 'cause I'll find X doesn't appear here.",
                    "label": 0
                },
                {
                    "sent": "So taken together, we see it's independent of the particular choice of Alpha, Beta, X&X prime.",
                    "label": 0
                },
                {
                    "sent": "It only depends on F&G itself.",
                    "label": 0
                },
                {
                    "sent": "OK, so and.",
                    "label": 0
                },
                {
                    "sent": "So this is a nice thing.",
                    "label": 0
                },
                {
                    "sent": "It's a symmetric bilinear form we know now.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So let's continue a little bit and look at 2 interesting special cases.",
                    "label": 0
                },
                {
                    "sent": "So one special case if.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We take one of these functions, so here we have two Jenna.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions F&G, which we defined like this.",
                    "label": 0
                },
                {
                    "sent": "If I make one of these functions very simple, so I'll cross out all this and just have one individual kernel fun.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what do I get then?",
                    "label": 0
                },
                {
                    "sent": "Then?",
                    "label": 0
                },
                {
                    "sent": "Then I only have.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set of coefficients here.",
                    "label": 0
                },
                {
                    "sent": "And only one sum.",
                    "label": 0
                },
                {
                    "sent": "Anne and.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if I do it for F then the Alpha vanish is the ex vanishers, so I'm left with a single sum over the bed has in the ex prime.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Times, which actually is nothing but G.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So therefore if I take this dot product so this very simple case for F with GI actually recover G evaluated at the point X.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "Sometimes this is paraphrased by saying the kernel is the representative of PT evaluation, so PT evaluation is an interesting operation.",
                    "label": 0
                },
                {
                    "sent": "Enough in a vector space of functions.",
                    "label": 0
                },
                {
                    "sent": "It's a linear operation.",
                    "label": 0
                },
                {
                    "sent": "But in the general case, it's not necessarily continuous.",
                    "label": 0
                },
                {
                    "sent": "So here in repetition kernel Hilbert spaces, it turns out this is a nice continuous functions and it makes sense to do this point evaluation, but that's just a side remark.",
                    "label": 0
                },
                {
                    "sent": "So anyways, OK represents PT evaluation of a function.",
                    "label": 0
                },
                {
                    "sent": "And if we specialize even more, let's both F&G we make both of them single kernel functions.",
                    "label": 0
                },
                {
                    "sent": "Then we recover this equation here, which is the reason why K is sometimes called a reproducing kernel.",
                    "label": 0
                },
                {
                    "sent": "So K with K makes another key.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now let's conclude this proof, and to show this.",
                    "label": 0
                },
                {
                    "sent": "To start to show that this thing this symmetric bilinear form is actually a dot product.",
                    "label": 0
                },
                {
                    "sent": "Then we go in several steps.",
                    "label": 0
                },
                {
                    "sent": "We first show that it's a positive definite kernel in its own right on a set of functions.",
                    "label": 0
                },
                {
                    "sent": "So just to make sure I don't confuse you.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So remember we started by assuming K is a positive definite kernel on the domain where this X points come from.",
                    "label": 0
                },
                {
                    "sent": "So what I'm saying now is if we define this thing here with the angular brackets, which we know is a symmetric bilinear form if we define it in this way, then it turns out that this is also positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "But this is a positive definite kernel on the set of functions.",
                    "label": 1
                },
                {
                    "sent": "So to see that this is a positive definite kernel, we have to check the usual definition of positive definite kernel.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we have to do is.",
                    "label": 0
                },
                {
                    "sent": "We have to take a set of arbitrary functions FFJ arbitrary real coefficients, I, J.",
                    "label": 0
                },
                {
                    "sent": "And we have to workout this quantity here.",
                    "label": 0
                },
                {
                    "sent": "So let's do that and we want to show that this is non negative.",
                    "label": 0
                },
                {
                    "sent": "So first of all we use linearity.",
                    "label": 0
                },
                {
                    "sent": "Remember, we have already seen that this thing with the angular brackets is a bilinear form, and it's symmetric bilinear form.",
                    "label": 0
                },
                {
                    "sent": "So we can use linearity.",
                    "label": 0
                },
                {
                    "sent": "We take these coefficients into some insight to some inside the bilinear form.",
                    "label": 0
                },
                {
                    "sent": "So just like before we take it inside again, now we can see here this thing.",
                    "label": 0
                },
                {
                    "sent": "Here is the same as this one.",
                    "label": 0
                },
                {
                    "sent": "So these are both functions, let's call them F. We don't know what exactly they look like, but it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "There are functions from our space, we know it's a vector space, so these are still elements of our space.",
                    "label": 0
                },
                {
                    "sent": "Since the elements of our space, we can write them in this form here with some other files in some X axis, so we'll do that.",
                    "label": 0
                },
                {
                    "sent": "And now we will use by linearity again, and we take the Suns out again in the alphas.",
                    "label": 0
                },
                {
                    "sent": "So now we have this thing here and it looks familiar 'cause this is exactly the quantity that we know is not negative because K is a positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "So since K is positive definite kernel on the domain of the excess, this thing here is a positive definite kernel on the domain of the functions or on this set of functions.",
                    "label": 0
                },
                {
                    "sent": "So that's nice we know is positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "Therefore we can use all the things that we have proven before.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So all these properties we can now use, it's a positive definite kernel on this function space.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's see how we can use them.",
                    "label": 0
                },
                {
                    "sent": "Well, actually so the first step is now we will prove it's strictly positive definite.",
                    "label": 0
                },
                {
                    "sent": "An and to show this.",
                    "label": 0
                },
                {
                    "sent": "We are so we.",
                    "label": 0
                },
                {
                    "sent": "We know it's a symmetric bilinear form to show that it's a dot product.",
                    "label": 0
                },
                {
                    "sent": "We also have to show it's strictly positive definite.",
                    "label": 0
                },
                {
                    "sent": "So to show this, we look at some arbitrary point F of X ^2.",
                    "label": 0
                },
                {
                    "sent": "We can remember that the kernel is the representative point evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that was.",
                    "label": 0
                },
                {
                    "sent": "On this slide here.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if we want to evaluate a function at some point, we just take the dot product with the.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Journal.",
                    "label": 0
                },
                {
                    "sent": "So we'll do that here.",
                    "label": 0
                },
                {
                    "sent": "Take the dot product between F and the kernel centered on X.",
                    "label": 0
                },
                {
                    "sent": "So now we can use the Cauchy Schwarz inequality.",
                    "label": 0
                },
                {
                    "sent": "So where was the coach Schwarz inequality?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "T. Tells us that this square here is upper bounded by this product.",
                    "label": 0
                },
                {
                    "sent": "Of these two things.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry, we use the yeah so we we upper bound.",
                    "label": 0
                },
                {
                    "sent": "Is this actually then the.",
                    "label": 0
                },
                {
                    "sent": "The usual coaching swaps inequality we upper bound by this thing times this thing here.",
                    "label": 0
                },
                {
                    "sent": "And if we look at this quantity here.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a.",
                    "label": 0
                },
                {
                    "sent": "If with F itself and here we have the kernel with the kernel, so maybe this is a slightly confusing so you shouldn't get confuse.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By the fact that here we have the coach about the equality for K for Colonel.",
                    "label": 0
                },
                {
                    "sent": "But now on this other slide.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So maybe I said this is a little bit confusing.",
                    "label": 0
                },
                {
                    "sent": "The kernel that we are interested in is not this key, but this thing with the angular brackets.",
                    "label": 0
                },
                {
                    "sent": "So we will use the kosher Schwarz inequality for kernels.",
                    "label": 0
                },
                {
                    "sent": "But for this kernel with the angular brackets which we know is a positive definite kernel on the set of functions.",
                    "label": 1
                },
                {
                    "sent": "So we upper bounded by the product of the first argument with the first one and the dot product at the second with the second one.",
                    "label": 0
                },
                {
                    "sent": "And what we can see from here is this is a non negative number which is upper bounded by this.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we know that whenever F with F, the dot product has the value 0.",
                    "label": 0
                },
                {
                    "sent": "Then the left hand side must also be 0 'cause it's a non negative number which is upper bounded by zero.",
                    "label": 0
                },
                {
                    "sent": "This means that if the dot product of F with itself is 0, then F is identically 0 for all X is, so it's the zero function.",
                    "label": 1
                },
                {
                    "sent": "And if you know the definition of a dot product then you know that this is the strict positive definiteness that needs to be proven in order to show that something is a dot product so that products of a function with itself takes the value 0 only if the function is 0.",
                    "label": 0
                },
                {
                    "sent": "So now we know it's a dot product and then the last step is actually sort of trivial, one that we're not going to go into detail on.",
                    "label": 0
                },
                {
                    "sent": "So now we have a vector space with the DOT product.",
                    "label": 0
                },
                {
                    "sent": "We just have to complete the space in the norm induced by the DOT product to get a Hilbert space so so vector space with the dot product is also called a pre Hilbert space and Hilbert space is simply complete.",
                    "label": 0
                },
                {
                    "sent": "So sequences that are convergent Cauchy sequences actually converge against some limit.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we know.",
                    "label": 0
                },
                {
                    "sent": "So let's see what have we proven we have proven.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We started with the kernel, assuming it's positive definite, we constructed a feature map for this kernel.",
                    "label": 0
                },
                {
                    "sent": "Then we construct it an inner product.",
                    "label": 0
                },
                {
                    "sent": "Functions of this space.",
                    "label": 0
                },
                {
                    "sent": "We showed that it has the properties of the inner product and we showed that it satisfies this equality here.",
                    "label": 0
                },
                {
                    "sent": "So the inner product in that space is identical to the kernel taken in the input domain.",
                    "label": 0
                },
                {
                    "sent": "For this special points.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's nice because remember if we before we prove that if we start with some.",
                    "label": 0
                },
                {
                    "sent": "Function into a dot product space Phi, then this quantity here is a positive definite kernel.",
                    "label": 1
                },
                {
                    "sent": "And now in the last 10 minutes we proved if we start with a positive definite kernel then there exists some fire.",
                    "label": 0
                },
                {
                    "sent": "In some Hilbert space such that this thing here corresponds to the kernel.",
                    "label": 0
                },
                {
                    "sent": "So the set of positive definite kernels is identical to the set of.",
                    "label": 0
                },
                {
                    "sent": "Things that can be written like that for some mapping.",
                    "label": 1
                },
                {
                    "sent": "Fight into a Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "So this is the same thing we can represent all kernels in this way invite.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Versa.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "If you have questions to interrupt at any point, so now we're going to look at some examples of kernel algorithms.",
                    "label": 0
                },
                {
                    "sent": "So, so we're going to look at some examples of Colonel algorithms.",
                    "label": 0
                },
                {
                    "sent": "Questions about the construction of the Hilbert space, then now is a good time.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's look at some examples, so maybe this was a little bit tough idea.",
                    "label": 0
                },
                {
                    "sent": "I hope I didn't.",
                    "label": 1
                },
                {
                    "sent": "I didn't exaggerate the mathematics here, but now it's going to get simpler again.",
                    "label": 0
                },
                {
                    "sent": "So what's the simplest possible algorithm we can do with such kernels?",
                    "label": 0
                },
                {
                    "sent": "So this is actually an algorithm is trivial algorithm that I came up with when I was working on the 1st chapter of book on Kernel Machines.",
                    "label": 0
                },
                {
                    "sent": "I was thinking what's the simplest thing one could do which has some machine learning flavor and uses kernels Now thought?",
                    "label": 0
                },
                {
                    "sent": "Well, let's take a two class classification problem.",
                    "label": 0
                },
                {
                    "sent": "We have these circles and we have the pluses.",
                    "label": 0
                },
                {
                    "sent": "These are two classes and will simply a classification rule will say given the new point we will assign it to the class whose mean is closer.",
                    "label": 0
                },
                {
                    "sent": "So I compute the mean of this class, compute the mean of this class.",
                    "label": 0
                },
                {
                    "sent": "And for instance, with this point I'll be closer to here, so I'll say it belongs to this class.",
                    "label": 1
                },
                {
                    "sent": "If you think about what kind of decision rule does this induce, the set of points that are closer to here than they are to here is actually a half space.",
                    "label": 1
                },
                {
                    "sent": "So everything over here will be closer to that point, and you get the half space by just drawing this line between the two centroids of the two datasets, positive and negative, and then you take a hyperplane orthogonal to that vector here and intersecting it.",
                    "label": 0
                },
                {
                    "sent": "Intersecting this line in the middle.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple hyperplane classifier is another support vector machine, but not so different now.",
                    "label": 0
                },
                {
                    "sent": "The only nontrivial thing is we will do this in our reproducing kernel Hilbert space will do this in the future space, and surprisingly it turns out this gives something that's well known in statistics.",
                    "label": 0
                },
                {
                    "sent": "So we do this in our kernel space.",
                    "label": 0
                },
                {
                    "sent": "And the different ways of doing it.",
                    "label": 0
                },
                {
                    "sent": "So remember, if we want to Colonel Eison algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple linear algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we can carry out this algorithm by computing dot products, because we only need things like angles and distances.",
                    "label": 0
                },
                {
                    "sent": "So for instance, one thing would be we just compute given an arbitrary point of actually mapped into the feature space.",
                    "label": 0
                },
                {
                    "sent": "And we do the same for the positive class, so that let's say these are positive points.",
                    "label": 0
                },
                {
                    "sent": "We map them all into the feature space.",
                    "label": 0
                },
                {
                    "sent": "So this is the sum of all positive points.",
                    "label": 0
                },
                {
                    "sent": "We map them into feature space.",
                    "label": 0
                },
                {
                    "sent": "We divide by the number of positive points, same for the negative points.",
                    "label": 0
                },
                {
                    "sent": "So we compute all these quantities.",
                    "label": 0
                },
                {
                    "sent": "We write them down formally, so now they could be vectors in an infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "But we don't worry because we just have to make sure that all these vectors pair up into products.",
                    "label": 0
                },
                {
                    "sent": "And once they are dot products we can replace them by kernels.",
                    "label": 1
                },
                {
                    "sent": "So we'll do that and we could, for instance, compute the distance between this one and this one and compare it to the distance between this and this.",
                    "label": 0
                },
                {
                    "sent": "You know, distances are just norms of different vectors, so norms of difference vectors we can write down these things.",
                    "label": 0
                },
                {
                    "sent": "Norms are just dot products, so everything can be replaced by dot products.",
                    "label": 0
                },
                {
                    "sent": "That would be one way.",
                    "label": 0
                },
                {
                    "sent": "There are other ways of doing this.",
                    "label": 0
                },
                {
                    "sent": "We could also say we take this vector W and we also write down this vector X -- C, where C is the average of the two class means, and then we have to check.",
                    "label": 0
                },
                {
                    "sent": "Whether these vectors enclose an angle larger or smaller than 90 degrees.",
                    "label": 1
                },
                {
                    "sent": "If you remember a dot product computes the cosine of the angle, then you directly see.",
                    "label": 0
                },
                {
                    "sent": "This actually can be read off, you just have to check whether the product is positive or negative.",
                    "label": 0
                },
                {
                    "sent": "So there are different ways and maybe we don't have to go into details on that.",
                    "label": 0
                },
                {
                    "sent": "It's a little exercise, so I don't want to.",
                    "label": 0
                },
                {
                    "sent": "Have you all go quiet again for 10 minutes now?",
                    "label": 0
                },
                {
                    "sent": "Because we don't have that much time left, but I recommend recommend you could do it afterwards.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just takes you a few minutes so we'll we'll put these slides on the web anyway, so you can also look this up in here, but it's a little exercise, but if you do this, it turns out you get a decision function.",
                    "label": 0
                },
                {
                    "sent": "Which looks like like that.",
                    "label": 0
                },
                {
                    "sent": "So this decision function has a term there.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The term that comes from the mean of the positive class, so taking DOT products between this and some fire of X.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like terms, so it it gives us a term like this which has.",
                    "label": 0
                },
                {
                    "sent": "DOT products between all positive points and five X we get a term similar term for all negative points and we get a term B which is all the quantities that don't depend on X on the test point.",
                    "label": 0
                },
                {
                    "sent": "Now we replace these dot products by kernel functions and then we have this expression here.",
                    "label": 0
                },
                {
                    "sent": "So basically what this tells us is that here.",
                    "label": 0
                },
                {
                    "sent": "So let's consider the special case where K can be.",
                    "label": 0
                },
                {
                    "sent": "Thought of as a density model.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if we use a Gaussian kernel which is normalized to have integral one But actually this is more general.",
                    "label": 0
                },
                {
                    "sent": "This works for all positive definite kernels.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So, but think of the Gaussian, then this thing.",
                    "label": 1
                },
                {
                    "sent": "Here is something that statisticians call opposing Windows density estimates of the positive class.",
                    "label": 0
                },
                {
                    "sent": "So you put one kernel on each positive point.",
                    "label": 0
                },
                {
                    "sent": "You normalize by the number of positive points.",
                    "label": 0
                },
                {
                    "sent": "So this has integral one.",
                    "label": 0
                },
                {
                    "sent": "This density estimate of the positive class.",
                    "label": 1
                },
                {
                    "sent": "This is the density estimate of the negative class.",
                    "label": 0
                },
                {
                    "sent": "Here we have some offset.",
                    "label": 0
                },
                {
                    "sent": "Let's not worry about that.",
                    "label": 0
                },
                {
                    "sent": "But roughly speaking, this means we are checking whether it's more likely that the point comes from the positive class.",
                    "label": 0
                },
                {
                    "sent": "All from the negative class, so this.",
                    "label": 0
                }
            ]
        }
    }
}