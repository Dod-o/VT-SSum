{
    "id": "jgarldojw7jgedzot653xkn7hvh3vde3",
    "title": "Theano",
    "info": {
        "author": [
            "Pascal Lamblin, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "July 27, 2017",
        "recorded": "June 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_lamblin_theano/",
    "segmentation": [
        [
            "So I'm going to start with the."
        ],
        [
            "An introduction to Theano.",
            "Try to outline the motivation and design.",
            "It's going to be mostly slides with short code snippets that are also in an Ipython notebook that you can follow along if you want.",
            "And then I'm going to.",
            "Switch to more hands-on examples so these are going to be Jupiter notebook hosted on Amazon on instances with a GPU.",
            "So you're going to be able to play directly on the GPU from your laptop.",
            "Don't have to install anything.",
            "An older material is on GitHub and you can clone it later or have a look later if you want, and so for the hands-on examples 1st, I'm going to walk you through a simple component example, so variant of the original Lynette trained on Nest and using lasagna.",
            "Dark above basic piano exercises.",
            "I won't go through them today, but if you want to check them out later, solutions are there as well, and the last hands-on example is how to build your own classifier or starting from a pre trained VGG network.",
            "So if you want to follow the presentation along with the companion notebook, you can already.",
            "Head to that address.",
            "Anne and request basically IP and port to connect to the.",
            "The remote's notebook, so I'm actually going to do that right now.",
            "It's not required that you follow through that presentation.",
            "I'm just going to start it now, and we're going to have some time after the slides to troubleshoot your connection, and so on, before the 2nd part."
        ],
        [
            "So this is where you get redirected.",
            "I enter my name.",
            "Email the password is here.",
            "And OK.",
            "I have an IP and port.",
            "Password is the same.",
            "And if you're already there, you can navigate to.",
            "Notebooks and intro theano.",
            "So these are going to be all the snippets that I have in the slides.",
            "That you can execute with control, enter or shift enter.",
            "OK, so let's get started.",
            "So first I'm going to talk about the motivation and design behind piano.",
            "Then we're going to go more in details about what happens when you define the model and train it and graze over some advanced topics.",
            "In the end, if I have time."
        ],
        [
            "The goals we had when designing piano at the beginning is to allow machine learning researcher to express their models as mathematical expressions.",
            "So have the full expressing power of math and not only a predefined collection of predefined layers and training algorithms and datasets, wrappers and things like that.",
            "We wanted to be able to do that from scripting language or an interpreted language.",
            "Anne.",
            "We wanted to be able to automatically derive gradients an not have researchers manually all through Mathematica or software like that, derive the specific gradients for every new block or new kind of things that they wanted to experiment with wanted.",
            "Everything should be done automatically if expressed from basic building blocks that have gradients.",
            "And we want it, of course, to have good speed for execution when actually training and experimenting with the model without having researchers writing C or C++ or CUDA code to have that benefit.",
            "So these were the initial requirements an if those seem familiar to what Tensorflow is now doing, that's pretty much where they got the idea."
        ],
        [
            "So how we did that in the end?",
            "Oh so to define expression we use Python And we want it to be able to express mathematical expressions for the model using a syntax really close to an empire.",
            "So a lot of things that you find in Empire have an equivalent in Theano.",
            "Once you define those expression, it's possible to manipulate them because you define the symbolic expression 1st and then you can actually play with it and handle it.",
            "You can use on the part of the graph you can substitute part of the graph.",
            "For another you can go through the graph and apply gradient reverse mode forward mode.",
            "We have optimizations that rewrite part of the graph to make it either more numerically stable or more efficient in terms of execution.",
            "Or in terms of memory.",
            "And from that graph we can generate code using fast backends if some are available.",
            "Initially it was blast and custom C code that will generate on the fly.",
            "Now it's good acquisition and things like that.",
            "And we have also built tools around that to help check for correctness and help debug and so on."
        ],
        [
            "So we are now at more than nine years old and the project released so the latest stable version a couple of months ago.",
            "We used to say it's driven hundreds of research paper.",
            "I think that now it's like 14 hundreds citations for articles, so that's probably a good estimate of the number of articles using it.",
            "We have hundreds of contributors from all around the world.",
            "We have hundreds of participants on the mailing list answering questions and so on.",
            "It's used for University classes for startups, for large companies, for research, and you can find out more documentation and tutorials online.",
            "These are the ones we developed are lots of other one."
        ],
        [
            "It's available as well.",
            "It's the foundation of the many software libraries and frameworks as well so.",
            "Examples I put here like blocks, kerosin, lasagna that are deep learning framework that some of you are probably familiar with that defined define building blocks that are usually used for deep learning.",
            "Our lab is a reinforcement learning framework using pianos are back, end by MC3 is a probabilistic programming project using another back end and then there are rappers that help with parallelism.",
            "Across GPU's on one machine or across machines sometimes."
        ],
        [
            "So in practice, tiano defines mostly three things at the same time.",
            "It's defines a language that, like you express symbolic mathematical expressions.",
            "A compiler that will transform that mathematical graph into a callable that you will be able to use to compute actual values, and the library that will actually perform that execution."
        ],
        [
            "So if we start from the beginning, the symbolic expressions first when building a new graph, you start with inputs.",
            "So for instance, here we define two vectors.",
            "That that are placeholder.",
            "They don't have values yet.",
            "These variables all have a type that you have to know in advance, so that graph is strongly type an in the type you have.",
            "The number of dimensions you have the data type like for instance.",
            "By default it's flow like float or double.",
            "You can also have integer data types and so on.",
            "Number of dimensions like victories.",
            "One matrix is too tense or four is 4 had broadcastable pattern that basically tells you.",
            "Which dimensions are required to be one and should be duplicated implicitly like rose or vectors, rows or columns rather than than vectors, and the device like is it on CPU and GPU, which GPU and so on, but things like the shape.",
            "Of a tensor, it's stripe pattern or representation in memory down, not part of the type, so they can change at runtime."
        ],
        [
            "Another type of symbolic variables that are used for inputs is called shared variables and those one or symbolic and they will be part of the symbolic expression, but a numerical value is associated with them, and this value is persistent.",
            "It's shared across all the users of the variable, and it can be updated, so one good use case for shared variables is parameters of your models 'cause you want.",
            "Them to be shared between, say, an evaluation and the training function you want them to be updatable by the training function, and you want them of course to be persistent across calls."
        ],
        [
            "So I'm from these inputs we can build an expression like for instance.",
            "Here we define a new variable dot.",
            "That's the result of the tensor dot operation between vector XN, matrix W call sigmoid and so on.",
            "So this is quite similar to NUM PY, except that your inputs are the predefined input variables that were on the previous slides and for instance, I don't think that.",
            "Mumbai has sigmoid, but doesn't really matter.",
            "You can name them for debugging purposes.",
            "So each of these calls each of these operations create a new variable like some are named and some not.",
            "For instance, dot plus B is not named, but it's still there in the structure that represents the.",
            "In the expression and so you have that structure that.",
            "Links all of that together, so here's a way."
        ],
        [
            "We are visualizing that so the green one is.",
            "Free, like unbounded input X that we defined earlier.",
            "The sign ones are shared variables.",
            "And the... represent the operations.",
            "So here you have the dot product.",
            "We define the output here.",
            "It's a name to just print the type element, wise addition and so on.",
            "The sigmoid and this is the color for the outputs.",
            "Usually we don't represent all the graph because unnamed intermediate variables are not that relevant, so that's the simplified version where you just."
        ],
        [
            "Directly the operations and just print out types here and you still have inputs and outputs explicitly there as rectangles."
        ],
        [
            "So.",
            "Once we have built part of our graph, one nice thing that we can do is backpropagated gradients.",
            "I won't go into details about the backpropagation algorithm shortly.",
            "Mass Johnson is going to talk about in much more detail.",
            "An more insight about that on such a day."
        ],
        [
            "But in practice in piano, what happens is that each operation defines a grad method, and when you call Tiano Dot grad this global function, it will work through the graph and output an expression.",
            "It's a symbolic expression that represents here the gradient of cost C, which has to be a skater with respect to input W. Here I do that for a WNBA.",
            "In practice, you want to have only one call to traverse the graph only once.",
            "So these new variables that we so the return values here they are actually symbolic variables as well, the same as if we had derived the mathematical formula by hand and then implemented that using Theano operations.",
            "So we have no numerical values at that point, and these new variables that they are part of the same computation graph.",
            "So calling DNO Dot grad actually walks through the graph, but then augments it with the new operations that compute the gradients.",
            "And since they are just symbolic variables, then we can continue building on top of them, like for instance here I compute update expressions for simple gradient descent with a fixed running rate, and I can do that multiplication which is piano operation and minus and so on.",
            "Then I name those new Bibles."
        ],
        [
            "So if you want to visualize that here, I'm printing Georgian graph for The CW and DC DB.",
            "So the gradients themselves.",
            "So you have still the inputs.",
            "Here we have a couple of constants that corresponds to like 21 running RAID 0.1 and so on.",
            "And you can kind of see the structure from the input to the cost that here and then from the cost back propagation, and like these links, when the backpropagation when the gradient expression needs things.",
            "That depends from the forward.",
            "Here you have the gradients with respect to B and the gradients with respect to W. That actually depends on the one with respect to be, which is why this one is both an output of the graph and an intermediate variable used by this one."
        ],
        [
            "If I want to."
        ],
        [
            "Print the update itself, so 0.1 minus and so on.",
            "It just extends the same thing again.",
            "You still have like the gradients he ran.",
            "He ran from there.",
            "We just multiply by zero point 1 and so on, and we have those new outputs."
        ],
        [
            "So when you have that big graph that represents what you want to compute.",
            "What we want is to actually be able to come."
        ],
        [
            "Do that.",
            "So how does that work?",
            "So the important part here is the call to Tiano dot function.",
            "We call that the compilation phase because, like in the compiler, you go from some representation of what you want to perform to something that you want to, that you actually can call or execute.",
            "So it you know function that will take as input symbolic variables that define what we want, and it returns a Python object that you can call.",
            "So here we define that we want for instance, predict function that goes from only one input X2 out.",
            "Simple random variable prints.",
            "We call the function and it should print that.",
            "We can define functions with more than one input and more than one output.",
            "For instance, here we go from X&Y to both the prediction and the cost.",
            "And again the same value.",
            "An as we.",
            "So here we didn't go all the way through the graph.",
            "We stopped at out, which was intermittent variable and the same way you don't have to start from the beginning.",
            "So for instance here I want if I have a function that takes only like.",
            "Pre computed output and the targets and computes the cost and call it with just the same numbers but not confident."
        ],
        [
            "Wasted all the way because I'm lazy and it gives a result that's close.",
            "So all of these you can do from part of the graph.",
            "Something else.",
            "Yeah, something else that I didn't mention is that here we only provide values for the free or placeholder variables because it's going to use the value from the shared variables that it needs implicitly.",
            "You don't have to provide them.",
            "But something else that we might want to do is to compute new values for those shared variables.",
            "For instance, if we want to train our model.",
            "So the way we do that is by adding another keyword argument update to channel function.",
            "So here we specify so inputs X&Y, output the costs and then we reuse the update W and update B that we defined a couple of slides earlier and say, well, actually I want that to be mine.",
            "You value for shared variable W and the same for be so.",
            "If we print the value of B before an after the call to train, you see that it has changed.",
            "So as W&B are implicit inputs of the channel function that gets executed, the update expressions update the value of B.",
            "They are implicit outputs, so they will be computed as outputs at the same time as C. And then after they get computed, the update of the value of W&B is actually performed."
        ],
        [
            "Something important that takes place during the compilation phase is graph optimizations, and So what an optimization does is basically replace one node or several nodes in the graph with.",
            "Another computation, and so the type has to match.",
            "You cannot replace like a vector by your matrix or something like that, and the values should be equivalent.",
            "So they can be used to merge equivalent computation.",
            "Don't want to compute the same thing twice, so you replace the one by the other.",
            "You want to simplify things like X / X will be replaced by one, so in that case the values are not strictly equivalent at zero, but it's really something that we that is desirable.",
            "Same for other numerical optimizations.",
            "Yes, numerical study optimizations like log 1P instead of log of 1 + X and so on.",
            "These kind of simplification optimizations they are usually, I mean they are made necessary by the fact that we define most of the things as elementary mathematical operations, and if you take the gradient through each of these individual ones, you will get things that can be simplified a lot.",
            "And then we have also optimizations for speed, like replace CPU OPS by GPU ones doing shape inference so that you avoid computing some complex expression just to get its shape in the end.",
            "Things like constant folding, if there are constants that are known at compile time, you can just compute them once instead of at each call, and so on."
        ],
        [
            "So you can.",
            "Play with the level optimizations that gets applied to your graph.",
            "The default is faster and which performs most of the speed optimizations and memory reducing optimizations as well.",
            "You can disable most of them but still have cogeneration and GPU use, but it's usually more memory inefficient.",
            "You have things like debug mode that will.",
            "Check your graph before and after each optimization and make sure that everything computes the same thing and this will check Python code against C code against CUDA code to make sure that everything is good.",
            "So usually you don't want to do that on your production code, but it's easy.",
            "It's useful for debugging and then you can also be more specific and exclude or include in particular some kind of optimizations that you need or don't want.",
            "You can do that globally for the program.",
            "You can do that."
        ],
        [
            "For each function as well.",
            "So let me just show you a couple of examples of what graduation do here is the graph we had earlier for variable out, so the output of the sigmoid.",
            "If."
        ],
        [
            "We compile it.",
            "This is what we get so.",
            "It uses JMV, so a blast operation instead of the regular dot product.",
            "That's more general but less optimized.",
            "This one requires W to be transposed so we have a transpose here and then you have the sigmoid is actually performed in place on the output of the dot product.",
            "You see the red arrow here shows that it's destructive and it reduces the Med and the input memory."
        ],
        [
            "Here is the graph that defines the update for WNB and."
        ],
        [
            "Here is the function train which also has the cost here.",
            "So you see that it's.",
            "Much more compact.",
            "You have operations that have been fused together.",
            "Like here it performs like 3 multiple four multiplications and those attraction in the same node and you have updates of the shared variables that are noted like that.",
            "So this is the shared table that's used in inputs.",
            "It's also used here for the update expression and then the output is actually performing the gradient descent in place on the same memory that was used for the original.",
            "Sean Bible."
        ],
        [
            "Debug print is another way of checking what is in the graph, because this one can be nice, but it's sometimes hard to understand, like which are all the inputs of this or that node.",
            "For this shows it as a graph form in text and you can grab easily in some.",
            "Now that the graph has been generated and optimized, let's see what happens for the execution."
        ],
        [
            "So we generate custom C++ code, some of which uses CUDA kernels and so on for efficiency and the way it works is that for each up, so each operation in Theano will define ways to produce C code that will get compiled, an imported and executed.",
            "It can fall back to a Python implementation if there's no C implementation.",
            "And it will interact with the arrays on CPU or GPU that represents the intermediate values.",
            "Ann, once you have all these.",
            "When you have all these executables or callable functions for the different parts in C, then there's runtime environment or what we call the VM that will basically call them in order and orchestrate the execution.",
            "Taking into account things like ordering constraints, part of the graph that don't need to be executed, and so on."
        ],
        [
            "Another important part of optimized execution is the GPU's.",
            "So we have a new GPU back end that's part of the latest release, and it's by default in the latest release, and we've even removed the old one from the development version.",
            "Supports things like different well different types on the GPU, including float 16 for storage.",
            "If you want to reduce your memory usage on GPU.",
            "Easier to interact with.",
            "Arrays on the GPU than than it used to be, so like you might be like, you might want to be able to use just regular in \u03c0 to handle values from your intermediate variables and so on over shared variables.",
            "Then now you can do that with GPU arrays from Python as well an.",
            "So if you want to use that.",
            "Just set a flag, device calls CUDA or who does 012 and so on.",
            "If you want to specify one specific GPU an if you toggle that switch then all share variables are by default will be by default created in GPU memory so that your parameters and so on don't have to be transferred over and over from CPU to GPU and Mac and then it enables optimizations that will replace the CPU operation with GPU ones.",
            "An for speed you want to make sure that you use float 32 or even float 16.",
            "By default most GPUs have really slow performance for 64."
        ],
        [
            "How to change those computations?",
            "Those configuration flags.",
            "There are basically three ways.",
            "The default is in configuration file.",
            "It's going to be set up in the VM as well.",
            "Don't have to worry about it right now.",
            "If you want to override those and relevant viable tiano flags will overwrite what is defined in the configuration file, and then it's always possible for most of the flags to just override them from Python And just set them.",
            "The device cannot be changed, but you can still call things to make 1 device than the default."
        ],
        [
            "Going to go."
        ],
        [
            "Clearly over couple of more advanced topics.",
            "So if we want to express symbolic loops in particular loops where you don't know in advance how many steps there will be, or if you want to express the breaking condition as a symbolic variable that will be computed during like at the same time as the step function, then you don't really have a choice but to use scan.",
            "Which will basically encapsulate a small piano graph that will compute the computation at each step and then take care of bookkeeping and interacting with the rest of the graph.",
            "Syntax is not that easy and I don't really have time to cover it for now, but he."
        ],
        [
            "Is a small example that you can see in the notebook as well and play with it later."
        ],
        [
            "We have visualizations and debugging and diagnostic tools, especially because the definition of ottino expression and its execution or distinct.",
            "So we need things to go back to the source expression if something goes wrong.",
            "We have a couple of those and also things that enable monitoring during execution.",
            "If some values get too high or if you have now.",
            "And or Infinity or things like that."
        ],
        [
            "It's possible to extend piano, common ways of doing it is creating a new app with Python codes with C. For instance, if you want to wrap something that like an external library that has Python bindings.",
            "You can use it.",
            "You can extend to using C or CUDA code directly.",
            "And adding optimizations is another way so it could be an optimization that simplifies the graph or that inserts one of your newly defined optimized apps instead of a more native way of expressing the same computation."
        ],
        [
            "I don't have any big announcement for like the new features.",
            "Sorry.",
            "But so in the latest release where we have the new GPU back end that I mentioned, which has in particular much simpler installation on Windows through the packages, you don't have to install MVC and make it work anymore.",
            "We have performance improvements.",
            "We wrap chuidian V6.",
            "Now that brings a lot of goods.",
            "We have ways of having faster optimization phase.",
            "Even if not as thorough as the more expensive one, new diagnostic tools as well."
        ],
        [
            "In the things that we're currently working on.",
            "More operation on GPU, particular things like indexing and argmax, and things like that.",
            "Faster reductions still working on faster optimization phase.",
            "New new operations will be wrapped so more variants of convolution of linear algebra operation and so on.",
            "Still working on data parallelism and.",
            "Up from graph is a way to encapsulate part like as a part of the graph, and if for instance you have a more efficient way of expressing the gradients that you derive manually, or that you know that is going to be making more memory efficient to the detriment of the speed or things like that, But you want to switch between them, that's a way to override the gradient of some part of the graph can do all the things as well."
        ],
        [
            "Time and in our longer road map well one things we would like to add is shaping fronts for constants at the time where we build the expression when we can infer that things are not going to change or are guaranteed to change faster.",
            "Compilation cache for the generated code.",
            "Still better graph optimization and faster.",
            "So we want basically the fast optimization to be better and the best optimization to be faster ways of reusing previously optimized sub graph flag to be deterministic, especially for debugging.",
            "And potentially use CPU memory to offload some intermediate results that we are going to need only later during the back prop.",
            "Anne."
        ],
        [
            "One last word about lasagna.",
            "Because we're going to use it's during the demo.",
            "It's a framework on top of piano.",
            "It doesn't really hide the fact that you are working with the new variables an expression, but it makes it easier to build larger graph, especially for common models for machine learning.",
            "So it has predefined layers that will just instantiate some sub graph.",
            "Of the nobles and operations, it has pretty much implemented things like standard losses, optimizers, thing like you'd like rather than Delta and so on at them.",
            "So that you don't have to redefine that each time that you want to try something new IT doesn't have a training loop, so you're free to call all the functions that you want in the order that you want, and monitor whatever you need."
        ],
        [
            "I'd like to thank all of my colleagues and former colleagues at Mila and all the developers.",
            "I won't go through the name of everyone because there are literally hundreds of them, even if I only mention a couple of them here and finding organisms and organizers for the summer school."
        ],
        [
            "So if you want to come back to this presentation, it's online on GitHub and here is where the companion notebook is.",
            "She wants more resources on piano online.",
            "I have a couple of links.",
            "Here are also lots of documentation on frameworks that use tiano lizine care as blogs and so on.",
            "You can that you can find online.",
            "And So what?"
        ],
        [
            "Like you to do for now is if you have not done that already, is navigate to that page.",
            "And put your name and so on to request machine on Amazon.",
            "And while you get that started, I can take some questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to start with the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An introduction to Theano.",
                    "label": 0
                },
                {
                    "sent": "Try to outline the motivation and design.",
                    "label": 1
                },
                {
                    "sent": "It's going to be mostly slides with short code snippets that are also in an Ipython notebook that you can follow along if you want.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Switch to more hands-on examples so these are going to be Jupiter notebook hosted on Amazon on instances with a GPU.",
                    "label": 1
                },
                {
                    "sent": "So you're going to be able to play directly on the GPU from your laptop.",
                    "label": 0
                },
                {
                    "sent": "Don't have to install anything.",
                    "label": 0
                },
                {
                    "sent": "An older material is on GitHub and you can clone it later or have a look later if you want, and so for the hands-on examples 1st, I'm going to walk you through a simple component example, so variant of the original Lynette trained on Nest and using lasagna.",
                    "label": 0
                },
                {
                    "sent": "Dark above basic piano exercises.",
                    "label": 1
                },
                {
                    "sent": "I won't go through them today, but if you want to check them out later, solutions are there as well, and the last hands-on example is how to build your own classifier or starting from a pre trained VGG network.",
                    "label": 0
                },
                {
                    "sent": "So if you want to follow the presentation along with the companion notebook, you can already.",
                    "label": 0
                },
                {
                    "sent": "Head to that address.",
                    "label": 0
                },
                {
                    "sent": "Anne and request basically IP and port to connect to the.",
                    "label": 0
                },
                {
                    "sent": "The remote's notebook, so I'm actually going to do that right now.",
                    "label": 0
                },
                {
                    "sent": "It's not required that you follow through that presentation.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to start it now, and we're going to have some time after the slides to troubleshoot your connection, and so on, before the 2nd part.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is where you get redirected.",
                    "label": 0
                },
                {
                    "sent": "I enter my name.",
                    "label": 0
                },
                {
                    "sent": "Email the password is here.",
                    "label": 0
                },
                {
                    "sent": "And OK.",
                    "label": 0
                },
                {
                    "sent": "I have an IP and port.",
                    "label": 0
                },
                {
                    "sent": "Password is the same.",
                    "label": 0
                },
                {
                    "sent": "And if you're already there, you can navigate to.",
                    "label": 0
                },
                {
                    "sent": "Notebooks and intro theano.",
                    "label": 0
                },
                {
                    "sent": "So these are going to be all the snippets that I have in the slides.",
                    "label": 0
                },
                {
                    "sent": "That you can execute with control, enter or shift enter.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's get started.",
                    "label": 0
                },
                {
                    "sent": "So first I'm going to talk about the motivation and design behind piano.",
                    "label": 1
                },
                {
                    "sent": "Then we're going to go more in details about what happens when you define the model and train it and graze over some advanced topics.",
                    "label": 0
                },
                {
                    "sent": "In the end, if I have time.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The goals we had when designing piano at the beginning is to allow machine learning researcher to express their models as mathematical expressions.",
                    "label": 1
                },
                {
                    "sent": "So have the full expressing power of math and not only a predefined collection of predefined layers and training algorithms and datasets, wrappers and things like that.",
                    "label": 0
                },
                {
                    "sent": "We wanted to be able to do that from scripting language or an interpreted language.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "We wanted to be able to automatically derive gradients an not have researchers manually all through Mathematica or software like that, derive the specific gradients for every new block or new kind of things that they wanted to experiment with wanted.",
                    "label": 0
                },
                {
                    "sent": "Everything should be done automatically if expressed from basic building blocks that have gradients.",
                    "label": 0
                },
                {
                    "sent": "And we want it, of course, to have good speed for execution when actually training and experimenting with the model without having researchers writing C or C++ or CUDA code to have that benefit.",
                    "label": 0
                },
                {
                    "sent": "So these were the initial requirements an if those seem familiar to what Tensorflow is now doing, that's pretty much where they got the idea.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how we did that in the end?",
                    "label": 0
                },
                {
                    "sent": "Oh so to define expression we use Python And we want it to be able to express mathematical expressions for the model using a syntax really close to an empire.",
                    "label": 1
                },
                {
                    "sent": "So a lot of things that you find in Empire have an equivalent in Theano.",
                    "label": 0
                },
                {
                    "sent": "Once you define those expression, it's possible to manipulate them because you define the symbolic expression 1st and then you can actually play with it and handle it.",
                    "label": 0
                },
                {
                    "sent": "You can use on the part of the graph you can substitute part of the graph.",
                    "label": 1
                },
                {
                    "sent": "For another you can go through the graph and apply gradient reverse mode forward mode.",
                    "label": 0
                },
                {
                    "sent": "We have optimizations that rewrite part of the graph to make it either more numerically stable or more efficient in terms of execution.",
                    "label": 0
                },
                {
                    "sent": "Or in terms of memory.",
                    "label": 0
                },
                {
                    "sent": "And from that graph we can generate code using fast backends if some are available.",
                    "label": 0
                },
                {
                    "sent": "Initially it was blast and custom C code that will generate on the fly.",
                    "label": 0
                },
                {
                    "sent": "Now it's good acquisition and things like that.",
                    "label": 0
                },
                {
                    "sent": "And we have also built tools around that to help check for correctness and help debug and so on.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we are now at more than nine years old and the project released so the latest stable version a couple of months ago.",
                    "label": 0
                },
                {
                    "sent": "We used to say it's driven hundreds of research paper.",
                    "label": 0
                },
                {
                    "sent": "I think that now it's like 14 hundreds citations for articles, so that's probably a good estimate of the number of articles using it.",
                    "label": 0
                },
                {
                    "sent": "We have hundreds of contributors from all around the world.",
                    "label": 0
                },
                {
                    "sent": "We have hundreds of participants on the mailing list answering questions and so on.",
                    "label": 0
                },
                {
                    "sent": "It's used for University classes for startups, for large companies, for research, and you can find out more documentation and tutorials online.",
                    "label": 0
                },
                {
                    "sent": "These are the ones we developed are lots of other one.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's available as well.",
                    "label": 0
                },
                {
                    "sent": "It's the foundation of the many software libraries and frameworks as well so.",
                    "label": 0
                },
                {
                    "sent": "Examples I put here like blocks, kerosin, lasagna that are deep learning framework that some of you are probably familiar with that defined define building blocks that are usually used for deep learning.",
                    "label": 0
                },
                {
                    "sent": "Our lab is a reinforcement learning framework using pianos are back, end by MC3 is a probabilistic programming project using another back end and then there are rappers that help with parallelism.",
                    "label": 0
                },
                {
                    "sent": "Across GPU's on one machine or across machines sometimes.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in practice, tiano defines mostly three things at the same time.",
                    "label": 0
                },
                {
                    "sent": "It's defines a language that, like you express symbolic mathematical expressions.",
                    "label": 0
                },
                {
                    "sent": "A compiler that will transform that mathematical graph into a callable that you will be able to use to compute actual values, and the library that will actually perform that execution.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we start from the beginning, the symbolic expressions first when building a new graph, you start with inputs.",
                    "label": 0
                },
                {
                    "sent": "So for instance, here we define two vectors.",
                    "label": 0
                },
                {
                    "sent": "That that are placeholder.",
                    "label": 0
                },
                {
                    "sent": "They don't have values yet.",
                    "label": 0
                },
                {
                    "sent": "These variables all have a type that you have to know in advance, so that graph is strongly type an in the type you have.",
                    "label": 0
                },
                {
                    "sent": "The number of dimensions you have the data type like for instance.",
                    "label": 0
                },
                {
                    "sent": "By default it's flow like float or double.",
                    "label": 0
                },
                {
                    "sent": "You can also have integer data types and so on.",
                    "label": 0
                },
                {
                    "sent": "Number of dimensions like victories.",
                    "label": 0
                },
                {
                    "sent": "One matrix is too tense or four is 4 had broadcastable pattern that basically tells you.",
                    "label": 0
                },
                {
                    "sent": "Which dimensions are required to be one and should be duplicated implicitly like rose or vectors, rows or columns rather than than vectors, and the device like is it on CPU and GPU, which GPU and so on, but things like the shape.",
                    "label": 0
                },
                {
                    "sent": "Of a tensor, it's stripe pattern or representation in memory down, not part of the type, so they can change at runtime.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another type of symbolic variables that are used for inputs is called shared variables and those one or symbolic and they will be part of the symbolic expression, but a numerical value is associated with them, and this value is persistent.",
                    "label": 1
                },
                {
                    "sent": "It's shared across all the users of the variable, and it can be updated, so one good use case for shared variables is parameters of your models 'cause you want.",
                    "label": 0
                },
                {
                    "sent": "Them to be shared between, say, an evaluation and the training function you want them to be updatable by the training function, and you want them of course to be persistent across calls.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm from these inputs we can build an expression like for instance.",
                    "label": 0
                },
                {
                    "sent": "Here we define a new variable dot.",
                    "label": 0
                },
                {
                    "sent": "That's the result of the tensor dot operation between vector XN, matrix W call sigmoid and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is quite similar to NUM PY, except that your inputs are the predefined input variables that were on the previous slides and for instance, I don't think that.",
                    "label": 0
                },
                {
                    "sent": "Mumbai has sigmoid, but doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "You can name them for debugging purposes.",
                    "label": 0
                },
                {
                    "sent": "So each of these calls each of these operations create a new variable like some are named and some not.",
                    "label": 0
                },
                {
                    "sent": "For instance, dot plus B is not named, but it's still there in the structure that represents the.",
                    "label": 0
                },
                {
                    "sent": "In the expression and so you have that structure that.",
                    "label": 0
                },
                {
                    "sent": "Links all of that together, so here's a way.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are visualizing that so the green one is.",
                    "label": 0
                },
                {
                    "sent": "Free, like unbounded input X that we defined earlier.",
                    "label": 0
                },
                {
                    "sent": "The sign ones are shared variables.",
                    "label": 0
                },
                {
                    "sent": "And the... represent the operations.",
                    "label": 0
                },
                {
                    "sent": "So here you have the dot product.",
                    "label": 0
                },
                {
                    "sent": "We define the output here.",
                    "label": 0
                },
                {
                    "sent": "It's a name to just print the type element, wise addition and so on.",
                    "label": 0
                },
                {
                    "sent": "The sigmoid and this is the color for the outputs.",
                    "label": 0
                },
                {
                    "sent": "Usually we don't represent all the graph because unnamed intermediate variables are not that relevant, so that's the simplified version where you just.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Directly the operations and just print out types here and you still have inputs and outputs explicitly there as rectangles.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Once we have built part of our graph, one nice thing that we can do is backpropagated gradients.",
                    "label": 0
                },
                {
                    "sent": "I won't go into details about the backpropagation algorithm shortly.",
                    "label": 0
                },
                {
                    "sent": "Mass Johnson is going to talk about in much more detail.",
                    "label": 0
                },
                {
                    "sent": "An more insight about that on such a day.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in practice in piano, what happens is that each operation defines a grad method, and when you call Tiano Dot grad this global function, it will work through the graph and output an expression.",
                    "label": 0
                },
                {
                    "sent": "It's a symbolic expression that represents here the gradient of cost C, which has to be a skater with respect to input W. Here I do that for a WNBA.",
                    "label": 0
                },
                {
                    "sent": "In practice, you want to have only one call to traverse the graph only once.",
                    "label": 0
                },
                {
                    "sent": "So these new variables that we so the return values here they are actually symbolic variables as well, the same as if we had derived the mathematical formula by hand and then implemented that using Theano operations.",
                    "label": 0
                },
                {
                    "sent": "So we have no numerical values at that point, and these new variables that they are part of the same computation graph.",
                    "label": 0
                },
                {
                    "sent": "So calling DNO Dot grad actually walks through the graph, but then augments it with the new operations that compute the gradients.",
                    "label": 0
                },
                {
                    "sent": "And since they are just symbolic variables, then we can continue building on top of them, like for instance here I compute update expressions for simple gradient descent with a fixed running rate, and I can do that multiplication which is piano operation and minus and so on.",
                    "label": 0
                },
                {
                    "sent": "Then I name those new Bibles.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you want to visualize that here, I'm printing Georgian graph for The CW and DC DB.",
                    "label": 0
                },
                {
                    "sent": "So the gradients themselves.",
                    "label": 0
                },
                {
                    "sent": "So you have still the inputs.",
                    "label": 0
                },
                {
                    "sent": "Here we have a couple of constants that corresponds to like 21 running RAID 0.1 and so on.",
                    "label": 0
                },
                {
                    "sent": "And you can kind of see the structure from the input to the cost that here and then from the cost back propagation, and like these links, when the backpropagation when the gradient expression needs things.",
                    "label": 0
                },
                {
                    "sent": "That depends from the forward.",
                    "label": 0
                },
                {
                    "sent": "Here you have the gradients with respect to B and the gradients with respect to W. That actually depends on the one with respect to be, which is why this one is both an output of the graph and an intermediate variable used by this one.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I want to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Print the update itself, so 0.1 minus and so on.",
                    "label": 0
                },
                {
                    "sent": "It just extends the same thing again.",
                    "label": 0
                },
                {
                    "sent": "You still have like the gradients he ran.",
                    "label": 0
                },
                {
                    "sent": "He ran from there.",
                    "label": 0
                },
                {
                    "sent": "We just multiply by zero point 1 and so on, and we have those new outputs.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when you have that big graph that represents what you want to compute.",
                    "label": 0
                },
                {
                    "sent": "What we want is to actually be able to come.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do that.",
                    "label": 0
                },
                {
                    "sent": "So how does that work?",
                    "label": 0
                },
                {
                    "sent": "So the important part here is the call to Tiano dot function.",
                    "label": 0
                },
                {
                    "sent": "We call that the compilation phase because, like in the compiler, you go from some representation of what you want to perform to something that you want to, that you actually can call or execute.",
                    "label": 0
                },
                {
                    "sent": "So it you know function that will take as input symbolic variables that define what we want, and it returns a Python object that you can call.",
                    "label": 0
                },
                {
                    "sent": "So here we define that we want for instance, predict function that goes from only one input X2 out.",
                    "label": 0
                },
                {
                    "sent": "Simple random variable prints.",
                    "label": 0
                },
                {
                    "sent": "We call the function and it should print that.",
                    "label": 0
                },
                {
                    "sent": "We can define functions with more than one input and more than one output.",
                    "label": 0
                },
                {
                    "sent": "For instance, here we go from X&Y to both the prediction and the cost.",
                    "label": 0
                },
                {
                    "sent": "And again the same value.",
                    "label": 0
                },
                {
                    "sent": "An as we.",
                    "label": 0
                },
                {
                    "sent": "So here we didn't go all the way through the graph.",
                    "label": 0
                },
                {
                    "sent": "We stopped at out, which was intermittent variable and the same way you don't have to start from the beginning.",
                    "label": 0
                },
                {
                    "sent": "So for instance here I want if I have a function that takes only like.",
                    "label": 0
                },
                {
                    "sent": "Pre computed output and the targets and computes the cost and call it with just the same numbers but not confident.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wasted all the way because I'm lazy and it gives a result that's close.",
                    "label": 0
                },
                {
                    "sent": "So all of these you can do from part of the graph.",
                    "label": 0
                },
                {
                    "sent": "Something else.",
                    "label": 0
                },
                {
                    "sent": "Yeah, something else that I didn't mention is that here we only provide values for the free or placeholder variables because it's going to use the value from the shared variables that it needs implicitly.",
                    "label": 0
                },
                {
                    "sent": "You don't have to provide them.",
                    "label": 0
                },
                {
                    "sent": "But something else that we might want to do is to compute new values for those shared variables.",
                    "label": 1
                },
                {
                    "sent": "For instance, if we want to train our model.",
                    "label": 0
                },
                {
                    "sent": "So the way we do that is by adding another keyword argument update to channel function.",
                    "label": 0
                },
                {
                    "sent": "So here we specify so inputs X&Y, output the costs and then we reuse the update W and update B that we defined a couple of slides earlier and say, well, actually I want that to be mine.",
                    "label": 0
                },
                {
                    "sent": "You value for shared variable W and the same for be so.",
                    "label": 0
                },
                {
                    "sent": "If we print the value of B before an after the call to train, you see that it has changed.",
                    "label": 0
                },
                {
                    "sent": "So as W&B are implicit inputs of the channel function that gets executed, the update expressions update the value of B.",
                    "label": 1
                },
                {
                    "sent": "They are implicit outputs, so they will be computed as outputs at the same time as C. And then after they get computed, the update of the value of W&B is actually performed.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something important that takes place during the compilation phase is graph optimizations, and So what an optimization does is basically replace one node or several nodes in the graph with.",
                    "label": 0
                },
                {
                    "sent": "Another computation, and so the type has to match.",
                    "label": 0
                },
                {
                    "sent": "You cannot replace like a vector by your matrix or something like that, and the values should be equivalent.",
                    "label": 0
                },
                {
                    "sent": "So they can be used to merge equivalent computation.",
                    "label": 0
                },
                {
                    "sent": "Don't want to compute the same thing twice, so you replace the one by the other.",
                    "label": 0
                },
                {
                    "sent": "You want to simplify things like X / X will be replaced by one, so in that case the values are not strictly equivalent at zero, but it's really something that we that is desirable.",
                    "label": 0
                },
                {
                    "sent": "Same for other numerical optimizations.",
                    "label": 0
                },
                {
                    "sent": "Yes, numerical study optimizations like log 1P instead of log of 1 + X and so on.",
                    "label": 0
                },
                {
                    "sent": "These kind of simplification optimizations they are usually, I mean they are made necessary by the fact that we define most of the things as elementary mathematical operations, and if you take the gradient through each of these individual ones, you will get things that can be simplified a lot.",
                    "label": 0
                },
                {
                    "sent": "And then we have also optimizations for speed, like replace CPU OPS by GPU ones doing shape inference so that you avoid computing some complex expression just to get its shape in the end.",
                    "label": 0
                },
                {
                    "sent": "Things like constant folding, if there are constants that are known at compile time, you can just compute them once instead of at each call, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "Play with the level optimizations that gets applied to your graph.",
                    "label": 0
                },
                {
                    "sent": "The default is faster and which performs most of the speed optimizations and memory reducing optimizations as well.",
                    "label": 0
                },
                {
                    "sent": "You can disable most of them but still have cogeneration and GPU use, but it's usually more memory inefficient.",
                    "label": 0
                },
                {
                    "sent": "You have things like debug mode that will.",
                    "label": 0
                },
                {
                    "sent": "Check your graph before and after each optimization and make sure that everything computes the same thing and this will check Python code against C code against CUDA code to make sure that everything is good.",
                    "label": 0
                },
                {
                    "sent": "So usually you don't want to do that on your production code, but it's easy.",
                    "label": 0
                },
                {
                    "sent": "It's useful for debugging and then you can also be more specific and exclude or include in particular some kind of optimizations that you need or don't want.",
                    "label": 0
                },
                {
                    "sent": "You can do that globally for the program.",
                    "label": 0
                },
                {
                    "sent": "You can do that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For each function as well.",
                    "label": 0
                },
                {
                    "sent": "So let me just show you a couple of examples of what graduation do here is the graph we had earlier for variable out, so the output of the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compile it.",
                    "label": 0
                },
                {
                    "sent": "This is what we get so.",
                    "label": 0
                },
                {
                    "sent": "It uses JMV, so a blast operation instead of the regular dot product.",
                    "label": 0
                },
                {
                    "sent": "That's more general but less optimized.",
                    "label": 0
                },
                {
                    "sent": "This one requires W to be transposed so we have a transpose here and then you have the sigmoid is actually performed in place on the output of the dot product.",
                    "label": 0
                },
                {
                    "sent": "You see the red arrow here shows that it's destructive and it reduces the Med and the input memory.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the graph that defines the update for WNB and.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the function train which also has the cost here.",
                    "label": 0
                },
                {
                    "sent": "So you see that it's.",
                    "label": 0
                },
                {
                    "sent": "Much more compact.",
                    "label": 0
                },
                {
                    "sent": "You have operations that have been fused together.",
                    "label": 0
                },
                {
                    "sent": "Like here it performs like 3 multiple four multiplications and those attraction in the same node and you have updates of the shared variables that are noted like that.",
                    "label": 0
                },
                {
                    "sent": "So this is the shared table that's used in inputs.",
                    "label": 0
                },
                {
                    "sent": "It's also used here for the update expression and then the output is actually performing the gradient descent in place on the same memory that was used for the original.",
                    "label": 0
                },
                {
                    "sent": "Sean Bible.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Debug print is another way of checking what is in the graph, because this one can be nice, but it's sometimes hard to understand, like which are all the inputs of this or that node.",
                    "label": 0
                },
                {
                    "sent": "For this shows it as a graph form in text and you can grab easily in some.",
                    "label": 0
                },
                {
                    "sent": "Now that the graph has been generated and optimized, let's see what happens for the execution.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we generate custom C++ code, some of which uses CUDA kernels and so on for efficiency and the way it works is that for each up, so each operation in Theano will define ways to produce C code that will get compiled, an imported and executed.",
                    "label": 0
                },
                {
                    "sent": "It can fall back to a Python implementation if there's no C implementation.",
                    "label": 0
                },
                {
                    "sent": "And it will interact with the arrays on CPU or GPU that represents the intermediate values.",
                    "label": 0
                },
                {
                    "sent": "Ann, once you have all these.",
                    "label": 0
                },
                {
                    "sent": "When you have all these executables or callable functions for the different parts in C, then there's runtime environment or what we call the VM that will basically call them in order and orchestrate the execution.",
                    "label": 0
                },
                {
                    "sent": "Taking into account things like ordering constraints, part of the graph that don't need to be executed, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another important part of optimized execution is the GPU's.",
                    "label": 0
                },
                {
                    "sent": "So we have a new GPU back end that's part of the latest release, and it's by default in the latest release, and we've even removed the old one from the development version.",
                    "label": 0
                },
                {
                    "sent": "Supports things like different well different types on the GPU, including float 16 for storage.",
                    "label": 0
                },
                {
                    "sent": "If you want to reduce your memory usage on GPU.",
                    "label": 0
                },
                {
                    "sent": "Easier to interact with.",
                    "label": 0
                },
                {
                    "sent": "Arrays on the GPU than than it used to be, so like you might be like, you might want to be able to use just regular in \u03c0 to handle values from your intermediate variables and so on over shared variables.",
                    "label": 0
                },
                {
                    "sent": "Then now you can do that with GPU arrays from Python as well an.",
                    "label": 0
                },
                {
                    "sent": "So if you want to use that.",
                    "label": 0
                },
                {
                    "sent": "Just set a flag, device calls CUDA or who does 012 and so on.",
                    "label": 0
                },
                {
                    "sent": "If you want to specify one specific GPU an if you toggle that switch then all share variables are by default will be by default created in GPU memory so that your parameters and so on don't have to be transferred over and over from CPU to GPU and Mac and then it enables optimizations that will replace the CPU operation with GPU ones.",
                    "label": 0
                },
                {
                    "sent": "An for speed you want to make sure that you use float 32 or even float 16.",
                    "label": 0
                },
                {
                    "sent": "By default most GPUs have really slow performance for 64.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How to change those computations?",
                    "label": 0
                },
                {
                    "sent": "Those configuration flags.",
                    "label": 0
                },
                {
                    "sent": "There are basically three ways.",
                    "label": 0
                },
                {
                    "sent": "The default is in configuration file.",
                    "label": 0
                },
                {
                    "sent": "It's going to be set up in the VM as well.",
                    "label": 0
                },
                {
                    "sent": "Don't have to worry about it right now.",
                    "label": 0
                },
                {
                    "sent": "If you want to override those and relevant viable tiano flags will overwrite what is defined in the configuration file, and then it's always possible for most of the flags to just override them from Python And just set them.",
                    "label": 1
                },
                {
                    "sent": "The device cannot be changed, but you can still call things to make 1 device than the default.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to go.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clearly over couple of more advanced topics.",
                    "label": 0
                },
                {
                    "sent": "So if we want to express symbolic loops in particular loops where you don't know in advance how many steps there will be, or if you want to express the breaking condition as a symbolic variable that will be computed during like at the same time as the step function, then you don't really have a choice but to use scan.",
                    "label": 0
                },
                {
                    "sent": "Which will basically encapsulate a small piano graph that will compute the computation at each step and then take care of bookkeeping and interacting with the rest of the graph.",
                    "label": 0
                },
                {
                    "sent": "Syntax is not that easy and I don't really have time to cover it for now, but he.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a small example that you can see in the notebook as well and play with it later.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have visualizations and debugging and diagnostic tools, especially because the definition of ottino expression and its execution or distinct.",
                    "label": 0
                },
                {
                    "sent": "So we need things to go back to the source expression if something goes wrong.",
                    "label": 0
                },
                {
                    "sent": "We have a couple of those and also things that enable monitoring during execution.",
                    "label": 0
                },
                {
                    "sent": "If some values get too high or if you have now.",
                    "label": 0
                },
                {
                    "sent": "And or Infinity or things like that.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's possible to extend piano, common ways of doing it is creating a new app with Python codes with C. For instance, if you want to wrap something that like an external library that has Python bindings.",
                    "label": 0
                },
                {
                    "sent": "You can use it.",
                    "label": 0
                },
                {
                    "sent": "You can extend to using C or CUDA code directly.",
                    "label": 0
                },
                {
                    "sent": "And adding optimizations is another way so it could be an optimization that simplifies the graph or that inserts one of your newly defined optimized apps instead of a more native way of expressing the same computation.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't have any big announcement for like the new features.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "But so in the latest release where we have the new GPU back end that I mentioned, which has in particular much simpler installation on Windows through the packages, you don't have to install MVC and make it work anymore.",
                    "label": 0
                },
                {
                    "sent": "We have performance improvements.",
                    "label": 0
                },
                {
                    "sent": "We wrap chuidian V6.",
                    "label": 0
                },
                {
                    "sent": "Now that brings a lot of goods.",
                    "label": 0
                },
                {
                    "sent": "We have ways of having faster optimization phase.",
                    "label": 0
                },
                {
                    "sent": "Even if not as thorough as the more expensive one, new diagnostic tools as well.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the things that we're currently working on.",
                    "label": 0
                },
                {
                    "sent": "More operation on GPU, particular things like indexing and argmax, and things like that.",
                    "label": 1
                },
                {
                    "sent": "Faster reductions still working on faster optimization phase.",
                    "label": 1
                },
                {
                    "sent": "New new operations will be wrapped so more variants of convolution of linear algebra operation and so on.",
                    "label": 0
                },
                {
                    "sent": "Still working on data parallelism and.",
                    "label": 1
                },
                {
                    "sent": "Up from graph is a way to encapsulate part like as a part of the graph, and if for instance you have a more efficient way of expressing the gradients that you derive manually, or that you know that is going to be making more memory efficient to the detriment of the speed or things like that, But you want to switch between them, that's a way to override the gradient of some part of the graph can do all the things as well.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time and in our longer road map well one things we would like to add is shaping fronts for constants at the time where we build the expression when we can infer that things are not going to change or are guaranteed to change faster.",
                    "label": 0
                },
                {
                    "sent": "Compilation cache for the generated code.",
                    "label": 0
                },
                {
                    "sent": "Still better graph optimization and faster.",
                    "label": 0
                },
                {
                    "sent": "So we want basically the fast optimization to be better and the best optimization to be faster ways of reusing previously optimized sub graph flag to be deterministic, especially for debugging.",
                    "label": 0
                },
                {
                    "sent": "And potentially use CPU memory to offload some intermediate results that we are going to need only later during the back prop.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One last word about lasagna.",
                    "label": 0
                },
                {
                    "sent": "Because we're going to use it's during the demo.",
                    "label": 0
                },
                {
                    "sent": "It's a framework on top of piano.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really hide the fact that you are working with the new variables an expression, but it makes it easier to build larger graph, especially for common models for machine learning.",
                    "label": 0
                },
                {
                    "sent": "So it has predefined layers that will just instantiate some sub graph.",
                    "label": 0
                },
                {
                    "sent": "Of the nobles and operations, it has pretty much implemented things like standard losses, optimizers, thing like you'd like rather than Delta and so on at them.",
                    "label": 0
                },
                {
                    "sent": "So that you don't have to redefine that each time that you want to try something new IT doesn't have a training loop, so you're free to call all the functions that you want in the order that you want, and monitor whatever you need.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd like to thank all of my colleagues and former colleagues at Mila and all the developers.",
                    "label": 0
                },
                {
                    "sent": "I won't go through the name of everyone because there are literally hundreds of them, even if I only mention a couple of them here and finding organisms and organizers for the summer school.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you want to come back to this presentation, it's online on GitHub and here is where the companion notebook is.",
                    "label": 0
                },
                {
                    "sent": "She wants more resources on piano online.",
                    "label": 0
                },
                {
                    "sent": "I have a couple of links.",
                    "label": 0
                },
                {
                    "sent": "Here are also lots of documentation on frameworks that use tiano lizine care as blogs and so on.",
                    "label": 0
                },
                {
                    "sent": "You can that you can find online.",
                    "label": 0
                },
                {
                    "sent": "And So what?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like you to do for now is if you have not done that already, is navigate to that page.",
                    "label": 0
                },
                {
                    "sent": "And put your name and so on to request machine on Amazon.",
                    "label": 0
                },
                {
                    "sent": "And while you get that started, I can take some questions.",
                    "label": 0
                }
            ]
        }
    }
}