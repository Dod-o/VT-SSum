{
    "id": "4wkddksl3twujzt2ap6jq522lwnozmhi",
    "title": "Adversarial Examples",
    "info": {
        "author": [
            "Ian Goodfellow, Google, Inc."
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_goodfellow_adversarial_examples/",
    "segmentation": [
        [
            "So my talk will be about adversarial examples, which I'm sure a lot of you have heard about.",
            "There are mistakes that are known network makes when you provide it with unusual inputs that are intentionally chosen to make the network produce the wrong answer."
        ],
        [
            "And.",
            "This talk will mostly be a summary of three different research papers that have come out over the past few years.",
            "The first one is called intriguing properties of neural networks by Christians Egadi of Inception Fame, and he first observed that these adversarial examples happen.",
            "The next research paper is one that I wrote with him and John Schlenz at Google.",
            "Called explaining and harnessing adversarial examples where we showed how to find these adversarial examples more quickly, we provide some explanations for why the most common kinds of them are happening in current neural Nets, and we also show how training on them can actually improve neural networks.",
            "And then finally there is a recent paper by Takara Miata and his collaborators, who have shown a different way of generating adversarial examples that can actually be used for semi supervised learning."
        ],
        [
            "One of the reasons that we're interested in adversarial examples is that we would really like to be able to do something called model based optimization.",
            "But to make it clear why model based optimization is so exciting, I'm going to rebrand it.",
            "The Universal engineering machine for the duration of this slide.",
            "The idea behind model based optimization is you want to solve an optimization problem, but you don't have any way of getting derivatives and maybe the.",
            "The input states are not something that you can easily add epsilon differences to.",
            "In order to do numerical differentiation.",
            "One example of this is when you actually want to build some kind of physical product or when you want to find a gene or a drug that will fulfill some task or cure some disease.",
            "In a lot of these cases, we can actually measure the performance of different products or genes or drugs, but we don't know how to take the derivative with respect to some piece of DNA or with respect to a molecule.",
            "Where you can do instead is you can build a model that predicts how well one of these objects will perform, and then you can solve for the point where the model predicts that you'll get the best possible performance.",
            "So you can imagine having trained a model on a bunch of different models of cars and learning to predict their speed.",
            "You could use optimization within the model to find a point that is predicted to have very high speed, and then you could build this theoretical extremely fast car.",
            "You can apply that to really any problem.",
            "Unfortunately, adversarial examples mean that when we try to extrapolate like this, we might not actually find a point that corresponds to an incredibly fast car, but rather to a point where our model makes an incredibly wrong prediction.",
            "So that's that's a lot of why we should care about adversarial examples as engineers.",
            "And."
        ],
        [
            "The reason to care about adversarial examples is from a more philosophical standpoint.",
            "People have often wondered whether AI machines really understand what they're doing, and for a long time we could obviously say that they did not really understand what they were doing because they did not perform as well as humans on benchmark tasks.",
            "Now, in many different tasks, humans have actually reached human level alot of the time.",
            "These benchmarks are somewhat contrived.",
            "For example, part of the reason that humans don't perform better on image net is that so much of it revolves around being able to distinguish obscur species of Stingrays, or being able to tell Siberian Huskies from Alaskan Huskies.",
            "That may not be the best way of measuring what people are good at.",
            "Likewise, on benchmarks of faces, people are not very good at recognizing strangers faces in photos.",
            "Computers are probably not as good at recognizing these photos as those of your immediate friends and family.",
            "But the point stands that on many benchmark tasks, computers have reached human level performance."
        ],
        [
            "We've now entered a territory where it's somewhat more philosophically difficult to understand.",
            "How well does the algorithm actually in some sense know what it is doing?",
            "And.",
            "These thoughts made me think of an old thought experiment proposed by the philosopher of mind, John Searle at Berkeley.",
            "The idea is that you could lock him John Searle in a room with a slot and passed notes in Chinese into the room.",
            "He doesn't speak Chinese at all, but if he was equipped with a sufficiently exhaustive instruction book telling him what sequence of symbols to write down in reply to each input sequence of symbols, he could then pass notes back out through the window and the room.",
            "Combined with John Searle and the instruction book would appear to understand Chinese.",
            "A lot of people have debated what exactly this thought experiment means and how to interpret it, but it seems like there is one way that as a non philosophical engineer, one can still draw some insight from the example.",
            "Which is you can see what happens if you put in a sentence for which there is no instruction book entry.",
            "For machine learning algorithms, we can think of the data generating distribution as being the instruction book that we're providing to machine learning.",
            "We're telling our machine learning algorithms.",
            "Learn to approximate this distribution, and then in the future fill in the missing variables according to the distribution that you've learned.",
            "If we don't go to unusual points where the data doesn't happen very often, we can see how the neural net extrapolates to these regions where the data has not provided much of a guide."
        ],
        [
            "Unfortunately, so far it turns out that the way that they extrapolate is very poorly Christian szegedy.",
            "First notice this effect while he was attempting to make visualizations of what convolutional networks have learned.",
            "The idea is we start with an input image of a ship, and then we use gradient descent to gradually perturb the image in a direction that causes the output of the model to believe that it's an airplane.",
            "So we're just doing gradient descent on log probability of airplane given image.",
            "With respect to image.",
            "After five steps, this is what we're left with.",
            "It looks like exactly the same boat, but now the network believes with nearly 100% confidence that this is an airplane.",
            "Similarly, when we transform this car, I can't even really see any difference, but at the end of this sequence of transformations, the network believes have nearly 100% confidence that this is an airplane.",
            "When we do the same thing to this cat, we can see that his face turns kind of blueish grey.",
            "Maybe that's the color of the metal on the fuselage of an airplane.",
            "Other than that, it still looks like a cat to me.",
            "And finally, this track really does not change at all, so it's possible that neural Nets have achieved superhuman performance, and there's a hidden airplane in there that were all blind too, but I think it's much more likely that the machine learning algorithm is just making a mistake.",
            "The original reason that we did this experiment, or that Christian specifically did this experiment was that he was expecting the boat to grow wings or the background to turn blue and look like this guy, and he thought that it would be a way of diagnosing what convolutional networks are using to make their classification and innocence.",
            "He was right, but the diagnosis was much worse than he expected it to be.",
            "The features they seem to be using are not even necessarily anything that corresponds to human perception at all.",
            "At first it yeah, go ahead.",
            "Well, we can definitely tell that this is not an airplane.",
            "Have you seen this image before?",
            "You were able to process it correctly despite not having seen that exact one.",
            "We expect.",
            "If you look at it.",
            "Birds.",
            "Killing.",
            "And they actually suffer evolutionarily for that they raised the cuckoo's egg instead of their own, and their offspring die.",
            "So I you saying that like we should, we should make our algorithms intentionally be as unintelligent as the birds.",
            "That file created the cuckoo.",
            "I like this, but it seems pretty clear to me that it's a flood to be fooled by the cuckoo egg.",
            "The other thing is, we expect our neural Nets to survive in the same environment that we survive in.",
            "And you can do this with more or less any image and more or less any class.",
            "So suppose that we were trying to make a pornography filter.",
            "And prevent pornographic images from being displayed on web pages that we want to have a professional appearance.",
            "If you could take a pornographic image and convince the computer that it's an airplane that it would go ahead and show that in a banner ad you don't want that to happen.",
            "So so yeah, I mean it is.",
            "It is a practical problem even if all you want to do is make web pages.",
            "And hopefully you're not having a computer choose which eggs to sit on."
        ],
        [
            "So first we thought that a lot of the problem came from convolutional Nets being very complicated.",
            "High capacity models and using this very deep nonlinear function to classify their inputs.",
            "It turns out that a linear model has exactly the same problem.",
            "So here we're using a softmax regression model that was trained on an M list.",
            "So it's literally just a matrix multiplication to give you the logits, and then if you really want to get probabilities, you apply the softmax function.",
            "You can actually classified just with the matrix multiply.",
            "Followed by argmax on the outputs.",
            "What we do here is we start off with a 9 and then we transform it until the model believes it's a 0.",
            "Lee Yellow box indicates that it successfully has been fooled with 99% probability to believe that this is a zero.",
            "Then we keep on optimizing and convince it instead.",
            "But it's A1A two A 34567 and eight.",
            "And well, it actually gets the nine right?",
            "But only because it started there.",
            "We actually managed to sweep through all of the 10 classes that the model knows about and it didn't end up producing anything that looks all that meaningful to me as a human observer.",
            "It's it's a gradient step divided by the mean absolute value or something like that.",
            "It's just designed to make it progress faster with a small number of steps so that the visualization is less tedious to look at.",
            "If you do pure gradient steps, then you have to do a few more steps and it's hard to fit all of them on one screen.",
            "So what exactly is going on here?",
            "My argument today is that."
        ],
        [
            "It's going to be about what's going on.",
            "Here is something similar to what happened with a horse named Clever Hans.",
            "I got this analogy for a paper called Clever Hans clever algorithms by Bob Sturm.",
            "So the analogy is that there is this horse who had a trainer and the trainer was not any kind of charlatan.",
            "He wasn't making money off of this.",
            "Probably the only thing he got out of this was attention, and as far as anybody knows, he really did believe that the horse could do what he said it could do.",
            "So when he said that his horse could do was arithmetic, you could ask the horse to compute 2 + 2 and then the horse would tap his foot four times.",
            "Huge crowds of people would gather and watch the horse perform, and as far as anyone could tell, it really was able to actually answer a wide variety of questions that people ask the horse.",
            "Later, the psychologist decided to examine the horse and he went to an enclosed area with no other people, and he wore a mask and he asked the horse to do arithmetic and the horse couldn't do it at all.",
            "It turned out what was happening was the horse had not learn arithmetic.",
            "The horse had learned how to read peoples emotional reactions.",
            "So you would ask the horse to say add 1 + 2 and he would tap his hoof once and you would stare at him and expectation and you tap his purpose second time.",
            "And and everybody would sit on the edge of the seat and then he would type with third time interval.",
            "Oh my God.",
            "He knows arithmetic and then he would stop tapping.",
            "So clever Hans was trained to answer these questions.",
            "And he found a way of doing it that made him appear to be successful by the metric of can he provide the right answer when there's a room full of people watching him, but he had accidentally queued onto something that wasn't quite the right underlying function.",
            "He hadn't really discovered arithmetic, and he couldn't generalize to unusual situations where there wasn't a room of people to provide the extra cues that he needed to solve the problem.",
            "So that's more or less what's happened here are machine learning algorithms aren't intentionally trying to trick us.",
            "But they've found cues that work very well on the training set where we train them, and those happen not to hold up when you intentionally analyze them and and try to step out of the area where they work.",
            "It's a little bit different from.",
            "Yeah, so they've.",
            "They've learned the data distribution essentially and and they've managed to find a function that works exactly on a manifold where the data lies but then doesn't behave as you would expect when you go off of that manifold."
        ],
        [
            "When I first learned about adversarial examples, we thought that they were mostly some kind of overfitting.",
            "Some kind of result of the model capacity being too high, and the model having too much ability to represent highly nonlinear functions.",
            "So the way that this cartoon picture works is suppose that we have these three training set.",
            "X is in these three training set.",
            "These for training setos.",
            "We want the model to learn a decision boundary that will generalize well.",
            "So let's represent this.",
            "This blue masses to represent weather model believes there will be more axes and the green mask represents what a model believes.",
            "There will be more owes.",
            "In the view of adversarial examples, is resulting from overfitting.",
            "We decided that the model just sort of randomly smears probability blobs all over the space and the training data isn't exhaustive enough to extinguish these random blobs, but they appeared more or less just by chance, and the vicissitudes of using extremely overparameterized distributions.",
            "So we just happen to throw down this blob of blue probability mass.",
            "We believe that.",
            "That inputs RX is.",
            "As a human observer, we can say, well, this point is really close to this point in the training set, and it would be most reasonable to predict that it would be an oh but the model mislabels it.",
            "So I've drawn it in red.",
            "Likewise, this is closest to the X is.",
            "There's not really any reason to think there would be no, but the model is labeled it.",
            "And oh, just kind of by chance.",
            "Oh, I'm sorry.",
            "I have a laser pen.",
            "Yeah, so first I was describing how this is mislabeled.",
            "It's believed to be an X by the model, but it's really close to the owes.",
            "This one is really close to the X is, but it's believed to be an oh by the model and in this view there's not any particular reason why the model chose to mislabel this one in the way it do it, or the way the reason it shows to disable this underrated, it's just the outcome of chance and bad luck."
        ],
        [
            "There's another view of how you could get adversarial examples, and that's the one that I'm advocating in this talk.",
            "In this view, adversary examples come from the model being two simple to describe the real task.",
            "It's able to find a way of jamming the data, and even the test distribution into what it's able to express, but it's not able to really describe the underlying dynamics.",
            "So here we're using a linear classifier to separate the for training, set those in the three training set X is.",
            "And you can see there's a lot of rich structure here that it's not capturing.",
            "It doesn't understand that those are arranged in a semi circular shape.",
            "It doesn't understand that the X is are highly linear.",
            "Well, those aren't, and so on.",
            "But it does manage to find a dividing hyperplane that separates the two of them.",
            "The problem is now we can make a new point here which is extremely close to these axes and obviously not on the semi circular manifold or those lie and it gets classified as being a know when it should be an X.",
            "Likewise over here.",
            "This point is obviously a continuation of the metaphor in which they always lie.",
            "But because it is crossed a linear boundary, it's now labeled as being an X, so I've got it in red to show that it's mislabeled."
        ],
        [
            "When understanding the effect of models with low capacity, you need to have the correct intuition for what a low capacity model actually means.",
            "A lot of the time people have a misleading intuition in which people believe that a low capacity model cannot make very different, very confident predictions about different points in space.",
            "So this is the weather.",
            "People usually conceptualise low capacity models.",
            "We have a bunch of training sets.",
            "The training set X is we have high confidence that their ex is near the training set points.",
            "We have a lot of training set, those we have high confidence that there arose.",
            "Near where the training set data lie, lay and then when you go out to the edges of the space, there's no data there, and the model has little confidence because it has had no incentive to place high confidence values there.",
            "So that's one way that a model can have low capacity.",
            "Your intuition is not entirely incorrect.",
            "We believe that these models exist.",
            "This happens if you have something like an RBF classifier that can just place one bump of probability, posterior probability per class.",
            "However, there are other ways of having low capacity, specifically, low capacity just means that you're forced to allocate your confidence about classes to different regions according to some predefined schedule rather than according to what you would like to do and what the data says you should do.",
            "So a linear model also has fairly low capacity.",
            "It's confidence has to be tide to this red line.",
            "It's confidence has to be linear and what we do is we then take the sigmoid of this red line to find the probability of being an 0.",
            "In order to have this probability be reasonably high on the training data on these training ozanne reasonably low, and these training X is it needs to be even higher when you go out to the extreme values on the right and even lower when you got to the extreme values on the left.",
            "So linear models actually become more confident when you force them to extrapolate and go into regions, but they never saw before when in fact they should become less confident according to most people's intuition.",
            "So what does this have to do with deep neural networks?",
            "We always hear how nonlinear deep neural networks are.",
            "But really there."
        ],
        [
            "A lot of extremely linear pieces.",
            "In fact, we intentionally designed them to be as linear as we can get away with one of the most popular units right now is the rectified linear unit, which has one linear piece with hard coded slope zero, and then one linear piece where the slope can be learned by adopting the weights.",
            "There's also Max out with many linear pieces that can all be adapted separately.",
            "Sigmoid units are not linear, but they often need to be carefully tuned an initialized so that you operate primarily in the linear regime when you don't do that.",
            "There are much harder to optimize, so to some extent even our sigmoid nuts have many pieces that behave extremely linearly.",
            "Finally, an LS, TM and recurrent net that accumulates information overtime uses addition of the state vectors to accumulate its state overtime.",
            "That's why they're able to back propagate through time so well.",
            "And this addition is of course just a linear operation.",
            "The main differences in this case it's been hard coded always use a coefficient of 1 rather than learning that particular coefficient.",
            "So we've built our neural Nets out of very linear pieces.",
            "Does that mean that the overall function they learn is very linear?"
        ],
        [
            "I'm not sure that it is in every case, but it is in many cases that I've tested, so I did.",
            "Here is, I load an image of a car.",
            "And then I add perturbations to the car as we move left to right in this graph, I'm adding more and more of this image into it, and then as you move to the left, I'm subtracting that same image off to get the negative image.",
            "So we're swapping out a linear path in the input of the network.",
            "On this graph I'm plotting the argument to the softmax.",
            "So the network is consisting of Max out units and the only non linear function with curvature rather than the Max over different linear functions is the soft Max at the end.",
            "So if I'd put everything up to the softmax we get this function where except for right immediately around the data, it's very linear and nonlinear log probability.",
            "The unnormalized log probabilities get more and more extreme as you go out to the far ends there is.",
            "I'm planning to argument to the softmax is that if I put it the softmax itself.",
            "It would just saturate immediately and would tell you that each of the classes has probability one or zero pretty much everywhere, but right around the data.",
            "So to study this better, we decided we were interested in a way of coming up with a formula that gives us a high confidence misclassified adversarial example with high probability.",
            "Yeah.",
            "What which point?",
            "Oh this is actually an adversary of perturbation.",
            "And yeah, I haven't described yet how I make them.",
            "It's possible, yeah.",
            "But basically this is the direction that you would go in if you wanted to cause a linear model to do this, and it does what a linear model would do.",
            "So I'm trying to find every solar perturbations one of the hardest things to deal with out.",
            "There is a question back.",
            "Send in high dimensional spaces.",
            "Most vectors that are randomly chosen are orthogonal to each other.",
            "So if you know that a certain class increases in probability if you move in direction D. Then when you generate a random vector that one random vector is probably going to be orthogonal to D, so it probably won't affect the probability of the class that's tide to direction D. If you choose something which you know is in the half space of our end where vectors have large dot product with D, then most of those perturbations do seem to actually increase.",
            "That class is probability, so it is half spaces.",
            "Then little pockets where adversarial examples occur.",
            "There are very large connected regions of mistakes, but they're hard to find just by random noise because you have, at least for things like cifar 10, you have relatively few classes compared to the number of input dimensions, so there's maybe something like 10 dimensions that strongly affect that most strongly affect the classes, and there's 3000 random directions you can move in.",
            "You've got to pick one of the 10 that really pushes your linear classifier toward one class or the other.",
            "I will be talking about that later.",
            "You don't need to have this specific network.",
            "If you perturb the perturbation.",
            "Or I mean.",
            "You saying like if you, what do you pretend the perturbation with it?",
            "Yeah, so if you add small noise to it then you're going small noise that you had is probably going to be orthogonal to the direction D that affects the classification.",
            "No.",
            "It's it's it's sensitive to specific directions.",
            "It's not really sensitive to exact points, and most randomly chosen directions are going to be orthogonal to the directions that sensitive too.",
            "If you use very large noise then you can you can mess up the original.",
            "That, or you could probably change the classification of the adversarial example.",
            "But as long as the noise is reasonably small, it shouldn't do a whole lot."
        ],
        [
            "So I'm trying to design A procedure for finding adversarial examples in closed form.",
            "One thing that's difficult is that you don't know whether a perturbation is going to consist of causing a mistake, or if legitimately changing the class ahead of time.",
            "So here we have an endless three.",
            "We add a perturbation to it.",
            "This perturbation as L2 run of 3.96.",
            "And we get something that's obviously A7.",
            "So here we made a perturbation and we really did change the class.",
            "So if the network changes its opinion about what the class was from here to here, that's not really a mistake.",
            "Here we add random noise with exactly the same L2 norm's here.",
            "And we get you know three on our sudden Piper background.",
            "So here the class has not changed at all despite the perturbation having the same magnitude is here.",
            "And then finally, at the end we have another perturbation where we just erased some of the pixels at the top of the three and we end up with something that's just garbage and doesn't look like a digit at all.",
            "All through these perturbations of the same L2 norm.",
            "And it's a relatively small L2 norm as well and experiments.",
            "I'll show you usually using L2 norm of seven, so this is a little bit more than half of the size of perturbation.",
            "Usually like to use.",
            "So how can we actually go into that?",
            "A perturbation won't change the class."
        ],
        [
            "Well, unfortunately that's a little bit problem dependent, but for computer vision.",
            "And specifically for recognizing object categories and images.",
            "You can assume that each pixel has a minimum precision, below which changing that pixel will not affect the class.",
            "So for example, image NET and M Nest are both stored as 8 bit integers, and you can imagine that if you change something that smaller that can be, then you make a change that is smaller than can be represented with eight bits, then that should not affect the class so embarrassed when we went through the network is usually represented with 32 or 64 bits.",
            "If we just make a change that's smaller than 1 / 255, it definitely shouldn't affect.",
            "The output.",
            "In practice, we can usually do a little bit better than that, we can say.",
            "That and this is mostly black and mostly white, so you could imagine that a perturbation of like .25 or so shouldn't really change something from black to white.",
            "In practice, humans can read perturbations up to about .25 and not beyond that.",
            "There are some Gray values in amnist, and those Gray values can become wider black pixels if you allow bigger perturbation than that.",
            "So in this slide what I do is I constrain the Max norm.",
            "The magnitude of the pixel that the magnitude of the largest pixel that changes.",
            "So to find the biggest perturbation under maximum constraint, you can actually have every single pixel change.",
            "The most that it's allowed to, because the maximum doesn't add up over different examples, it's just the Max across all of them, so all of them will be right at the border, doing as much damage as you can possibly get.",
            "What I did here was I took the perturbations in the previous slide.",
            "And I pushed them right up to the maximum boundary, but no, no further.",
            "And then, on pixels that didn't make any change before I had them do random sign changes just so that I could do as much damage as possible while still obeying the maximum constraint.",
            "So these all have the exact same L2 norm as on the previous slide, but they've been forced to obey a Max norm constraint, and you can see that now none of them changes the real class.",
            "So that tells us how we can actually make changes that we know are going to give us more things from the same class.",
            "For these object recognition type tasks."
        ],
        [
            "Given that we have this constraint region, we can say we want to find an adversary example that lies within this constraint region.",
            "But we want to maximize the cost that we have on the training set.",
            "Actually maximizing the cost would require doing some kind of gradient based search surrounding the original example, and that's what Christians egadi did in the 1st paper that we published.",
            "But to make things a lot faster, we can make a Taylor series approximation.",
            "We can say the cost added adversarial example.",
            "It is going to be pretty close to the cost at the clean example, plus the difference between the two multiplied by the gradient with respect to the input.",
            "We then left with this linear equation where we're trying to maximize this value subject to this constraint, and it has a very simple closed form solution.",
            "The adversarial example is equal to the clean example.",
            "Plus the precision of our features multiplied by the sine of the gradient of the cost with respect to the input.",
            "Essentially what we're doing is.",
            "We're assuming that the cost is linear, and then we're making the perturbation that hurts it the most under the belief that changing each pixel a small amount can change the class.",
            "So if this works, it's strong evidence that a lot of the problem with adversarial examples is due to the networks being highly linear.",
            "And."
        ],
        [
            "It indeed works.",
            "Here is a picture of a panda which was classified with 57.7%.",
            "Confidence is being a panda.",
            "We apply the fast gradient sign method to compute this gradient.",
            "The actual perturbation that were added is classified itself as being a nematode with eight point 2% confidence.",
            "And then the final classification.",
            "The final image, resulted from adding the two.",
            "Is classified as being given with 99.3% confidence.",
            "The images that you are seeing here and here are actually identical the.",
            "The keynote file is not capable of storing the difference between those two, and even if it was, the projector couldn't display it.",
            "But it changes us from believing that with 60% confidence it's the right class two with over 99% confidence.",
            "It's the wrong class.",
            "To get some intuition for exactly what this is doing."
        ],
        [
            "It's helpful to look at a shallow linear model where you can characterize the whole model just by looking at its weights.",
            "So this will clean the M list.",
            "3 is a nest sevens look like if we train and logistic regression model on them, we get something that looks kind of like the Fischer projection of the two classes.",
            "We get these weights here as we move along array that looks like this.",
            "We move from something that looks kind of like the centroid of all the threes to something that looks like the centroid of all the sevens.",
            "This particular image is not really recognizable to a human, but that's that's the core of what drives the classifier after it's been trained.",
            "If we want to find the worst thing you can do in the classifier under a maximum constraint, we take the sign of those weights.",
            "Make it this image here, which that looks even less meaningful to me as a human being, because we've destroyed a lot of the magnitude information in it.",
            "But we were just trying to find out what's the worst thing we could do to each pixel.",
            "If we don't take this image and add either this image or it's negative to the clean examples, we could address several examples that are misclassified with an error rate of over 99%.",
            "You could say this is just enlist, but Andre."
        ],
        [
            "Party at Stanford has actually.",
            "Implemented this for image net and an image net.",
            "You don't get confident enough predictions on either clean or adverse aerial examples to see a huge difference.",
            "You can see this original image was classified with eight point, 3% probability of being a goldfish.",
            "And then after adding an image of the weights of the linear model.",
            "We get that it's 12.5% likely to be a Daisy.",
            "So even in very large images you can confuse linear classifiers in this way.",
            "Yeah.",
            "Well, every everything I've attacked with it, although that's I have attacked with this method, have had a pretty high error rate on it unless they were explicitly trained to resist it.",
            "No, it it.",
            "I mean this particular experiment.",
            "Yeah, but I've done other experiments where I take that direction and."
        ],
        [
            "In this experiment I take that same direction and I go very far out and it actually becomes more linear the farther out I go.",
            "It's actually somewhat nonlinear near the data, but it is.",
            "It is sufficiently linear that I can succeed in getting the wrong class inside this area, and then once I'm outside this area, it's very conspicuously very linear.",
            "I will take that.",
            "Cost itself may not be."
        ],
        [
            "I'm actually going one step further and I'm linearizing the cost, and that has some chance of failing, but it doesn't really seem to.",
            "In practice.",
            "The other thing is sure neural Nets are locally linear, but there are locally linear on small enough regions that I probably am making different maxout units activate or different reluz activate.",
            "When I make these perturbations.",
            "Yeah."
        ],
        [
            "OK, so I spent a lot of time telling you how terrible when your models are.",
            "Is there an alternative?",
            "I'm not necessarily advocating that we use RBF since did, but just to show that there is something that doesn't have all these properties.",
            "This is what happens if we use gradient ascent on the input to start with a 9 and then transform the 9 into a zero and then A1A2A3 or 45678 and back to 9.",
            "We're not perfect, but.",
            "We can actually see what it's going for, so this is just an RBF classifier where the posterior gets one Gaussian bump per class and.",
            "It seems to be behave just fine far from the data, yeah?",
            "Yeah, it doesn't work.",
            "The neural net part poisons it.",
            "So the thing was kind of kind of cool and kind of frustrating about the adversarial setting is.",
            "We're used to thinking about average case performance and we just try to come up with defenses that work most of the time on the data and hope that the data rarely falls into the hole.",
            "In our defense, the adversarial setting looks for the gap in your armor and just shoot straight for that and.",
            "If there's a way to screw up your classifier by exploiting linearity in it, it's going to find that.",
            "So in this case, if you have the RBF classifier on top of a neural net, the neural net itself can output crazy values if you perturb it linearly.",
            "So we just make it produce a crazy hidden representation, and then the RBF reads out to the wrong class that's encoded in that hidden representation.",
            "Yeah, it's it's harder to give them very high confidence because.",
            "So the equation that I'm using here, the."
        ],
        [
            "The Epsilon Times sign of gradient equation.",
            "That works really well if you continually get worse the further you go in One Direction.",
            "But if there's a local maximum and your confidence, then it might overshoot it and that can happen with RBF notes because it's not.",
            "It's not adapting Epsilon to make sure that you stop right at the point it believes is the most like whatever class you're trying to go into.",
            "If you did it like a line search in this direction, you could probably get RBF sterba fairly high confidence.",
            "That's what I'm saying."
        ],
        [
            "So.",
            "If our beliefs are able to do is so much better and linear models chained together behaves so poorly, why are we always using linear pieces to build our neural Nets?",
            "The."
        ],
        [
            "Reason is that models that are easy to optimize are also easy to perturb, and vice versa.",
            "Andrej Karpathy called this wiggle free in his blog post as he was summarizing my idea, but I like his summary of my idea better than my original explanation.",
            "So the idea of wiggle theory is.",
            "In order to train the weights for this hidden unit, we need to be able to wiggle this hidden unit and get a change at the output.",
            "We need to build.",
            "Wiggle this weight and make this this hidden unit wiggle and get a change all the way up here.",
            "The problem is if wiggling this.",
            "Can cause sufficient upstream changes, that means that wiggling this.",
            "Must also do more or less the same thing, and less.",
            "This weight is very, very small.",
            "So essentially when you have different layers composed together, if if you become immune to perturbations at this point.",
            "It means you can no longer optimize anything below it, and when you apply that logic recursively, you end up having to be able to respond to perturbations all the way down yahshua.",
            "OK, so that's.",
            "Defense strategy that I call hide the gradient and it's it's a way of making my gradient sign thing not work as well.",
            "10 inch will actually still be vulnerable to the gradient sign equation, but you could imagine what if we go more aggressive and rather than 10 itch?",
            "What if we just have a threshold and return positive one if you're greater than zero and negative one if you're less than 0 so that threshold unit won't have a gradient anymore?",
            "But if you knew the direction of its weights, you could still do the same perturbation and flip it from negative one to positive one.",
            "So you can actually still make adversarial examples for non differentiable models that saturate as long as in the underlying distribution.",
            "They are driven by some linear weight vector that will be really confident when you move out to one side or out to the other side.",
            "I'll show some.",
            "I'll show an attack on a non differentiable model later on.",
            "It's a little bit more complicated."
        ],
        [
            "So the other thing is that space is full of mistakes, you just have to add components of the weight vectors that you'll get a mistake.",
            "They're really, really easy to find.",
            "This is taking ASIFA 10 convolutional net and looking for airplanes.",
            "The pink boxes indicate things that are classified as being an object of some kind.",
            "And since none of these things are objects that are run, the yellow boxes are classified specifically as airplanes.",
            "So the way that I did this was I generated Gaussian noise with unit covariance and then I did one step of the fast side and gradient perturbation toward the airplane class.",
            "So I said I'm going to take the derivative of probability of Y equals airplane given X with respect to X.",
            "And then I'm going to take the sign of that and add epsilon times that.",
            "And so this succeeded in getting airplanes about 1/4 of the time.",
            "So if you wanted to make a fake airplane that looks like no training data at all, you'd need about four samples on average before you get ahead."
        ],
        [
            "There are a lot of other papers out there about fooling.",
            "Well, not with images that don't correspond to any class.",
            "One thing that's in common between both of these is that they're usually relatively expensive to evaluate, and they usually have some kind of hand coded prior in them, so you'll see these images that look like really nice and semantically meaningful to people somehow.",
            "Obviously they don't look like the things that the network is classifying them as being, but it looks like there's some kind of structure and meaning in there.",
            "The reason there's structure and meaning in there is that these algorithms use searches with a strong prior for there to be some kind of structure and meaning.",
            "If you actually just optimize what the neural net response to you get."
        ],
        [
            "You get garbage like this.",
            "And the other thing is, a lot of these algorithms."
        ],
        [
            "To run like evolutionary algorithms for thousands of iterations before they find their high confidence fooling example, you don't need to do that.",
            "If you want to fool Internet, just draw a sample from a Gaussian distribution and for the average neural net that will be classified as belonging to some class with very high probability.",
            "If you wanted to come from a specific class, do one step side method and about 1/4 of the time you'll get it to be from that class.",
            "So that's that's fooling images.",
            "The next topic is something that some yeah.",
            "That's in the paper.",
            "There's still fold.",
            "Yeah.",
            "It still fills it with high probability.",
            "Even with just Gaussian samples I was getting amnist classifiers to report that there were digits present with high probability.",
            "I was counting it as a mistake.",
            "If they said probability greater than oh point 5 that there is something there.",
            "So if you think about it, any individual class only has about 50% probability of it being aligned with its weights.",
            "But then you've got 10 different classes, so you might lose the coin toss on one of them, but it's it's hard to lose 10 classes to lose 1020 to lose 10 coin tosses in a row.",
            "Or that's if you're looking at it from the perspective of the adversary.",
            "If you're looking at it from the perspective of the neural net, it's hard to win ten coin tosses in a row.",
            "Yeah.",
            "Oh oh, like how could?",
            "How could you prevent this?",
            "Hello.",
            "They are not objects there, and it mistakenly believes that there are.",
            "When you say how does it know that there are objects there?",
            "I'm not sure what you mean.",
            "They will look at it.",
            "It's it's a.",
            "It's a picture of random pixels, so there is no object there, but it is.",
            "The neural net is saying that there is an object there.",
            "Training on pictures that are not objects might be one way to fix this problem, but not every neural net need.",
            "Not every machine learning needs.",
            "Not every machine learning algorithm needs that.",
            "So I.",
            "Ideally it would say that it has no confidence that anyone of the 10 is present.",
            "It would, it would say it would say that there's a uniform distribution over the 10 classes because there's no evidence for any one of them in particular.",
            "So that's what happens with something like a Gaussian process with a.",
            "With an RBF kernel, is that as you move far from the data that Gaussian process becomes less and less confident about its prediction?",
            "Here you have the opposite effect.",
            "As you move further from the data that becomes more and more confident about one specific prediction, even though there's not really any evidence for that particular view.",
            "A relative threshold.",
            "Well, yeah, sure.",
            "If you impose a threshold you will throw out all the clean data.",
            "It is more confident about these that it is about the clean data.",
            "On the clean data it will say this is a panda with 70% probability on the garbage data.",
            "It'll say this is a panda with 99.3% probability.",
            "So if you try to threshold based on the confidence you will actually throw out the main thing that you're trying to do before you will throw out the garbage.",
            "It's shocking, I know, yeah.",
            "Biological neural network.",
            "It wouldn't have the same problem, so I guess I'm wondering.",
            "Your your bar would need to be large enough to undo the perturbation, I mean, so it's a linear model, right?",
            "Like, say that over here you get the right class and then as you follow my arm up till you come to a point where you get the wrong class.",
            "If you just draw a ball around it, it's going to be a pretty big ball before you come back to the good area and half of that ball is going to be even worse and half of the ball will be fixing.",
            "Yeah, I'm telling you is that.",
            "The shape of the bad area is defined by the geometry of the model itself, and the model is very linear, so there's like half spaces of madness.",
            "Yeah.",
            "Yeah, I think that's a promising direction.",
            "An if you were here for the morning talk.",
            "There was one thing I was saying Mark asked me like what would I like to see from optimization and I said being able to train some of the models that we just can't really fit all that well.",
            "Right now.",
            "Part of that is because I think that some of those models could be a way of solving adversarial examples.",
            "Yes, and the reason for that will become more clear pretty soon.",
            "Then they become more different as you propagate through the net and a lot of the time they become pretty different even before the first layer.",
            "I'll talk about that pretty soon, yeah?",
            "Oh I will also talk about that pretty soon here, yeah.",
            "Oh well, different datasets.",
            "I've only done amnesty for 10 and Imagenet then main reason is I had the slides earlier was talking about how I needed to design A perturbation where I know that the perturbation can't change the real class.",
            "Like so feminist, if I change a pixel by less than 1 / 255, I know that doesn't change the real class for Boston houses pricing data.",
            "I don't know enough about that kind of data to know like like if you get rid of five square feet, should your your house price plummet or not?",
            "I don't know.",
            "Yeah, so I think I'm going to move forward a few slides I see still some questions, but a lot of the questions I keep saying I'm going to tell you a few slides so will do."
        ],
        [
            "New slides and see if your questions diminish.",
            "So what are the other very startling properties of adversarial examples that Christian observed back in 2013?",
            "Is that they generalize across different models and also across even different training sets, as long as the training sets are drawn from the same class, this is one thing that we tried to explain in the explaining and harnessing paper.",
            "So our explanation is that machine learning algorithms generalize.",
            "You get samples from some distribution.",
            "You learn some kind of function from that distribution, and then it's more or less the right function to use on other data from the same distribution.",
            "That's why you can generalize from the train set to the test set.",
            "It also means that you can train on two different training sets drawn from the same distribution, and then you'll be able to do well on the test set regardless of the specific identity of the training set you used.",
            "In terms of thinking about the weights of a linear model that you learn, you can turn on emnace, threes and sevens anyone so it looks like the Fischer projection of the threes and sevens.",
            "These are two different training sets here, but we get weights that look more or less the same.",
            "And because the weights are more or less the same, the discriminative direction that they respond to is more or less the same.",
            "The two models are going to make more or less the same mistakes.",
            "This means that if you want models to make very different mistakes, they need to have fairly different classification functions, and so far I've been arguing that neural Nets tend to make very linear classification decisions.",
            "So this idea predicts that neural Nets and linear models will make some of the same mistakes as each other, but neural Nets and RBF will make different mistakes."
        ],
        [
            "Likewise, nearest neighbor should be a little bit different from neural Nets.",
            "Home safely training on that will generate adverse aerial examples for it, and then we feed those adversary examples to a nearest neighbor classifier.",
            "It gets about a 25% error rate.",
            "If we do a different kind of attack from one model to a more similar model, then we expect to see a higher error rate.",
            "So in this case what we did is we used smooth nearest neighbor where rather than just finding the single nearest neighbor, we take Gaussian weights around each training example and have the whole training set vote with Gaussian weights that's actually differentiable, so you can actually apply the fast gradient sign method to it and generate an attack on it.",
            "If you then feed that attack into a hardness neighbor classifier and it's a 4247.2 error rate.",
            "Because we're attacking it with a model that's much more similar to the final target, we've improved the success rate of the attack.",
            "And this is an example of how you can attack and nondifferentiable model, even when there's no gradient.",
            "You just you use the cross model generalization property, so that's what earlier Joshua asked a question about using 10 inch, because the gradient goes away at the margins, and I said that's a strategy that I call hide the gradient, so that works for turning off the function that I use, but you can still attack it via cross model generalization.",
            "If we take a neural net and we actually.",
            "Some of these are so these are less interesting.",
            "I actually maybe should have held some these results to later.",
            "If we take a maximum that we generate generate adversary examples for it, we attack it hyperbolic tangent.",
            "Note we get a 99.3% error rate, so different kinds of neural Nets actually make very similar mistakes.",
            "The other thing that's really cool is that now let's and linear classifiers agree on the class, so they make a mistake and it's not just that they both make a mistake, they actually agree on what the wrong class should be.",
            "So you show it a perturbed three, and they both say that it's a 7, even though it's conceivable that one could say it's a seven, and one could say that it's an 8.",
            "So for example, we take a max.net and we attack it and re feet it's examples at a softmax regression.",
            "Softmax regression gets an 88.9% error rate and the two models agree on the class.",
            "On somebody, 67% of the mistakes, so about 70% of mistakes overall.",
            "If we transfer adversary examples from Accent net to a shallow RBF model.",
            "First we get a lower error rate, only 36.8% error rate because the models are so different that the attacks aren't as transferable.",
            "But also they agree on the class less frequently when they do both, make a mistake.",
            "Only 40% of the time instead of 70.",
            "So we don't have the whole story.",
            "There's obviously something besides how linear or not linear are you that drives exactly how much an attack on one model generalizes to another model?",
            "And also that determines whether they will get the same class when they make a mistake, but it does seem like the degree to which models are linear gives us a lot of ability to predict how well.",
            "Those models will be able to attack each other.",
            "Example.",
            "Yes.",
            "That's a future slide, yeah?",
            "OK, so this is.",
            "This is, we've seen that adversary examples generalize from one model to another from one training set to another, and and that the degree to which they transfer ressemble resembles the degree to which the models have the same decision function, David.",
            "Here I'm using the frustrated sign method with epsilon equals .25 the.",
            "The 1st paper intriguing properties of neural Nets used a different way of generating them and found similar results.",
            "Except back then we weren't looking at whether they agree on the identity of the class.",
            "Not on this particular.",
            "Yeah, there's like so many different things I'm measuring that I can try multiple epsilon.",
            "All of them, yeah."
        ],
        [
            "OK, so does the human brain have adversarial examples?",
            "I don't know if this will work on you.",
            "This is a little bit scale dependent.",
            "If you look at this from the right distance, you should see illusory movement.",
            "There's not actually any movement there.",
            "And it also probably looks to you like there's a spiral.",
            "It's not a spiral, it's concentric circles.",
            "And I also I also often get.",
            "Did I trace it wrong?",
            "I probably traced it running today.",
            "But your brain makes mistakes too.",
            "We know from the cross model generalization property that your brain makes different mistakes then the neural Nets do.",
            "When I show you large adversarial examples on amnesty, you don't misclassify them.",
            "We don't really know for sure what our brain thinks of the image, not ever serve examples because of perturbations.",
            "There are so small that the monitor can't show them to you.",
            "But we do know that you're not bothered by the really big ones that we use honest.",
            "My purpose in showing this slide is to save it.",
            "Essentially every engineered system has some kind of weakness and if you go hunting for them, you're always going to find one.",
            "It's probably possible to construct some kind of no free lunch theorem argument saying that if you got rid of all the adversarial examples, you'd be worse at the original task or something like that.",
            "What we can do is try to understand exactly why some systems make the mistakes they do, and others make different mistakes, and we can also try to understand if there's a way to eliminate the mistakes that are neural Nets make in order to behave more like the human brain where we make this kind of mistake, but not the ones where we respond really strongly to small linear perturbations of the input.",
            "Yeah.",
            "Yeah.",
            "OK, so there's there's a.",
            "There's a more compelling but more distressing adversarial example that I can bring up if you're not sold on this one.",
            "So Elias sets Giver told me he was reading a book about electrophysiology in the 1950s.",
            "And in the 1950s, they found that if they've hooked up an EG to your brain and measure the frequency of your brain waves and flashes strobe light at you at the same frequency, you'll have a seizure so they can induce epilepsy and healthy people.",
            "Essentially, just by finding the.",
            "Time scale that you respond to and then making a signal at the same time scale.",
            "So we're sort of doing that same kind of attack to these neural Nets, but in the spatial domain we're finding the spatial frequency that the weights respond to, and plugging that in.",
            "But you can kind of do the same thing to the recurrent net in the human brain by using an EG in the strobe light.",
            "Yeah, well, I don't.",
            "I don't think we can conclude for sure what optimization algorithm the brain uses.",
            "I don't think there's enough information about that, but I do think that finding other means of optimizing models can open us up to having more defenses against adversarial examples.",
            "Kim.",
            "Completely wrong.",
            "Yeah, yeah, there's a lot of common logical fallacy's and psychological biases that you can think of as being adversary examples of some kind.",
            "I will be a slide on that in a few minutes."
        ],
        [
            "OK, so there's many different things you could do to try to defend against adversarial examples, and a lot of them have failed.",
            "And everybody's going to kind of start complaining, but if you did all these extra tweaks on them, I'm not saying that these techniques could never work.",
            "I'm just saying that a reasonable attempt to do them has been made in most cases by me in some cases by other people, and the first reasonable attempt has not really shown much effect.",
            "One thing is, ensembles don't really work if you have very very large ensembles, like those defined by dropout.",
            "They can reduce the actually get the class right.",
            "If you have small ensembles, like if you train 50 different nuts yourself with different random seeds on different subsets of the data, those do not really work at all because of the cross model transferred property you can generate every server examples against one ensemble member and then feed it to the whole ensemble.",
            "The whole ensemble will get it wrong.",
            "You can also back prop through the whole ensemble.",
            "And generate adversarial example and that one's even harder to get right.",
            "Anne.",
            "Then somebody asked about what if you shift the adversarial example by pixel, and there's also there's a popular comment on a blog post somewhere saying that they thought adversarial examples go away in the human brain because we make multiple saccades.",
            "So we tried an experiment where we just have one neural net vote across multiple SIM cards on the same image, and it had no discernible effect on the error rate of the model.",
            "That happens whether you backdrop through just once a card or through all of them, and if you back up the one the one that needs to be a smaller image that there's a lot of overlap between the SIM cards.",
            "Otherwise you might miss the object in once a cut.",
            "Other things that have not really worked are just adding noise to the training process.",
            "Most noise vectors are going to be orthogonal to the adversarial directions.",
            "I generative modeling using an unsupervised training criterion.",
            "I know you was going to complain about this, yeah?",
            "Yeah.",
            "Oh we are not sure even by using the noise as a rejection close.",
            "I will turn it to add, add an extra class and say recognize the noise is being extra cost.",
            "Yes I have tried that and I've not gotten it to work.",
            "I've tried a variety of different things.",
            "One thing I did was I tried generating like Gaussian noise and then doing one step of fast gradient sign on it and then saying train that this is class N plus one.",
            "I've also tried telling it to that didn't work.",
            "Now the there's a big, there's a big RN out there, there's a lot of possible garbage.",
            "And.",
            "I just want everything about the about the extra class option.",
            "You don't want to just reject adversarial examples.",
            "I know it better than being fooled by them.",
            "You do want to reject the fooling images that are very far from the input.",
            "If you just reject every single examples and that means, so you're trying to filter out spam.",
            "I'm not.",
            "I know you understand.",
            "Yeah, sure, I'm just telling the audience that this thought comes into my head.",
            "So you want to make a spam folder and say your spam folder can tell when there's an adversarial example coming in, and then you just say, well, this is this is garbage.",
            "I refused to process it.",
            "I don't know how to classify it.",
            "That isn't how spammers can shut down your spam detection system, it's it's a little bit better than just outputing the wrong thing all the time, but it's still not really that great from a security point of view.",
            "Yeah.",
            "Oh implicitly, the fast gradient sign method is doing that, 'cause it's you take the backdrop is going to take a bunch of hidden unit values and then multiply by the transpose of the way it's going into them.",
            "So you kind of you kind of are getting something that comes out of whether hidden unit values are.",
            "I'm not sure that's what you meant, but.",
            "OK, yeah yeah, I'll talk about that in a few minutes.",
            "There's also, there's a really cool paper that I like a lot from Panasonic Research where they train an autoencoder to turn the adversarial examples back into clean ones, and I thought there was a really cool idea and I spent longer trying to get this one to work than any of the others on here.",
            "They reported in their own paper that it actually does not work and actually makes the system more vulnerable to adverse aerial examples and.",
            "The way I started on this whole project was I read that I'm like that can't possibly be right and I started trying to implement their method and make it work.",
            "My conclusion is the reason it doesn't work is that your sensitivity that adversary examples is based on all these weight matrices being multiplied together, and their autoencoder just adds two more and a set of Jacobian can get bigger.",
            "There are some differences you can do that's all the other several task, but they make you lose performance on a clean task.",
            "One thing is you can use weight decay.",
            "L1 is better than L2.",
            "But usually if you add enough weight decay to lose confidence in every server, examples you have seriously damaged your ability to classify the clean ones.",
            "Same thing with double back prop where you regularize the Jacobian of the network to be small if you actually turn up the coefficient on the double backdrop high enough to confer resistance to adversarial examples usually destroy performance in the clean task and finally a deep RBF network is very resistant to adversarial examples.",
            "But usually has pretty high training error.",
            "Maybe somebody that's better training them.",
            "Then we could get it to have low training error, but I haven't had much success with that."
        ],
        [
            "Stochastic gradient descent and momentum.",
            "Yeah, for the most part.",
            "OK.",
            "Possibly, yeah, yeah, there we get to look into.",
            "So when perspective on how the word that adverse aerial training or.",
            "If I skip to slide by mistake, sorry."
        ],
        [
            "I will skip around a little bit 'cause I accidentally put my slides in order, so the Universal Approximator theorem tells us that neural Nets can actually implement this function to arbitrary accuracy, even though they're built.",
            "Are these layer pieces.",
            "And that means that to some extent this is all just a problem of designing the right loss function to make them implement that function.",
            "How it applies to relevance to yeah.",
            "But and then they might have different efficiency.",
            "So anyway, you can.",
            "You can shape the neural net to be relatively constant in the area surrounding a training example, you just, you feed adversarial examples back in, I believe you were asking about that earlier.",
            "So if we actually train."
        ],
        [
            "On both adversarial and clean examples.",
            "Then we can cause the network to become resistant to the adversarial examples.",
            "Here's a learning curve on the left side.",
            "I'm showing you the error rate on the clean task, like green learning curve is training with adversarial examples.",
            "The blue is without.",
            "And without we got down to something like 94.94% error rate, honest and with adversary examples.",
            "We got down to .78%.",
            "Since then, there's been a few new papers that did better.",
            "And.",
            "Had.",
            "Somebody was asking earlier about how sensitive are you to adversarial examples at the start of training?",
            "It's a little bit hard to really find this meaningful because you get about 90% error rate honest on clean examples, but both adversarial include examples start out at the same place and then all throughout training.",
            "You're constantly vulnerable to adversarial examples if you don't train on adversarial examples in your input.",
            "If you turn out ever so examples in the input, then you become resistant to them overtime, but not as resistant as you are not.",
            "Not not as good at classifying them as you are at classifying clean data.",
            "Will you take a clean input?",
            "You find the gradient of the cost with respect to the input, and then you add epsilon times this sign and run that through again.",
            "So now I'm going to skip back a few slides.",
            "This might be getting out what year?"
        ],
        [
            "Other fooling image things.",
            "I've I have not.",
            "I have not been able to train networks to be reliably resistant to fooling images.",
            "If you just train them to say this is nothing, then for most classes they'll be OK, but usually they have at least one class where there's still easily fooled into believing that that's present.",
            "I've not entirely sure why it skews that way.",
            "So there's several different ways of regularization Delta to output similar things in similar locations.",
            "One ways you can limit the total amount that can vary.",
            "That's what things like weight decay.",
            "Do you just say the parameters are not very big, so it can't really change that much over a small interval.",
            "And this often causes underfitting.",
            "If you have a training example, we want a low value here at a high value here.",
            "You might not be able to hit both of them if you're constrained to be linear and have a very small slope everywhere.",
            "Another thing you can do is you can have a very high capacity model and you can train it to resist infinitesimal perturbation.",
            "So this is what things like double backdrop and the CLU do.",
            "They become very flat surrounding training examples, but then they're not really saying what happens elsewhere, it can also be very hard to end up being as perfectly flat as that criterion would like you to be.",
            "Adverse aerial training is different because we're trying to get to resist perturbation to finite non infinitesimal perturbations.",
            "But we're also saying we don't really care what the slope is, we just want the actual value to not change all that much.",
            "So you're going to have really crazy slope and make a regular function that goes up and down a lot near your training example, as long as it wiggles continuously in the same general height.",
            "So it up with with bumpy but wide flat regions surrounding each training point question in the back.",
            "I've tried several different techniques.",
            "One is like you were suggesting using sigmoids for each class and started so just like every segment should be off, I've tried adding an extra class and saying this is garbage.",
            "Recognize this as being garbage.",
            "I've tried using only the original classes and telling it that on the garbage inputs it should try to output uniform over all the original classes.",
            "I haven't had any luck with all three of those approaches."
        ],
        [
            "One difference that hasn't worked very well is generative modeling, and I think there's there's actually like a an underlying conceptual reason why generative modeling isn't going to fix the problem.",
            "The reason is that classification is all about the posterior distribution over Y.",
            "Given X and generative modeling is shaping your marginal distribution over X.",
            "So let's suppose we have two different mixture models, with each mixture corresponding to one class.",
            "If we just use Gaussian for each class and we use the same covariance matrix for both Gaussians but give them different means.",
            "Then we can get a marginal distribution that looks like this with two little Gaussian bumps.",
            "And the posterior is a sigmoid distribution.",
            "Where the weights of the sigmoid come from, the means of the original mixture distribution.",
            "So this is bad when you go very far from where the data lies, you make extremely confident predictions.",
            "This is basically what I spent the whole lecture saying we don't want to do and this comes out of having a A2 class mixture model with a specific parameterization of P of X given Y.",
            "That's only rejection, though.",
            "Yeah.",
            "Here we have another model that implements almost exactly the same P of X.",
            "The differences in this one P of X given Y is different rather than being a single Gaussian, each of the class models has a mixture of two Gaussians, one with a very high probability, high prior probability of being sampled from the other, one with probability epsilon of being sampled from.",
            "So with probability epsilon we generate something from a garbage class or like a garbage version of the class.",
            "Something that hasn't been seen before in the training set.",
            "So if we've trained on 1000 examples we say, well maybe with probability 1000 out of 1002 we generate something that looks like the training data and with probability 2 out of 1002 we generate something that looks like this unseen thing we've ever seen before, so that second unseen component is just the Gaussian with very wide variance.",
            "We add that extra second component.",
            "It completely changes the posterior looks.",
            "The posteriors now says.",
            "Over here on the far end, we don't really know what the input is over here in the far end.",
            "We also don't really know what the input is, but near the training data was we're confident these training zeros or zeroes in these training ones or ones.",
            "But both of these models, despite having radically different behavior in the posterior, implement almost exactly the same marginal distribution.",
            "So just using a using a generative cost to train your probabilistic model is not going to make it perform correct posterior inference.",
            "It.",
            "You could get the same thing by fitting if you actually knew why you could actually get the same thing by fitting both of them.",
            "This model would actually be scored worse.",
            "But The thing is, this model is adding this extra noise component because I know that that extra noise component helps you.",
            "There's nothing in the data that tells you that it should be there.",
            "It's like a problem of maximum likelihood and just just doing maximum likelihood on X is as well as why isn't going to remove that problem with maximum likelihood.",
            "Yeah.",
            "So you can you can integrate a fix into a generative model, but the fix is not being generative.",
            "And by the way.",
            "This isn't adding new values of why this is adding new X values that haven't been seen before associated with both wise.",
            "It's a little bit different than adding a garbage class.",
            "If you added a garbage class, your posterior would end up saying out here.",
            "I'm confident that it's neither of the first 2 classes.",
            "This setup is saying out here.",
            "I have no confidence about what class it is.",
            "Yeah, I would be happy with that, but I haven't been able to make it happen reliably.",
            "There's there's just too much too many directions that you can screw it up.",
            "And just training on.",
            "Just training on synthetically generated fooling examples doesn't seem to be enough to make it learn an exhaustive garbage class, yeah?",
            "Alright, I'd like if you can get it to work.",
            "I agree that that's an acceptable solution.",
            "Maximum likelihood with the garbage class isn't really working either, so the problem is maximum likelihood with garbage class is only going to raise the probability of the garbage class on examples that it's seen.",
            "This is more of like incorporating a prior than changing the way that maximum likelihood proceeds from the data.",
            "Far away.",
            "Yeah.",
            "OK, I'd probably better get moving.",
            "We're running a little bit low on time, yeah?"
        ],
        [
            "So training another server examples can improve your error rate quite a lot on amnestic goes from close to 100% to down to about 3% on my best model right now.",
            "But there's still a lot of weaknesses.",
            "Here we take a truck and we start transforming it into the different classes.",
            "There's one I kind of like that's the bird class right there.",
            "It actually does kind of look like a bird, but I didn't really do anything that looked very reasonable for any of the others.",
            "And you can also see that this cat here we transform it all the other cats or all the other classes, and it at least quits looking like a cat now before it used to just not change.",
            "Now it now it turns into some kind of blob, but it doesn't look like the other classes.",
            "So there's a new a new way of doing adversarial training that was recently released and."
        ],
        [
            "To motivate this, let's look at what happens when we do an adversarial perturbation.",
            "An adversarially trained network.",
            "Or so this is actually the original network.",
            "So we haven't used any adversarial training we can feed in this clean amnist one and get this class response and we can feed in this adversarial.",
            "I'm just one and get this response an the biggest problem here is that we used to have a big peak around the one class.",
            "Each of these bars is showing the unnormalized log probability of a different class, so there is zero.",
            "There's one and it's pretty large.",
            "After the perturbation, we're now getting the seven class being really high.",
            "So adversary training tries to address specifically getting the right class.",
            "After we've done every single training.",
            "The clean example has a big peak at one, and the adversarial example is a big peak at one.",
            "But there's still a problem here, which is that the other class values have changed quite a bit.",
            "Like for example, we think it's much less likely to be a four then we used to think it was, and we also think it's much more likely to be a sudden than it was before.",
            "After you apply the softmax, you can barely see these changes anymore, but we know that these kinds of changes actually mean that something has gone wrong with the internal representation of the model.",
            "If you saw Jeff Hinton's dark knowledge paper recently, he showed how these kinds of distributions over the wrong classes actually tell you a lot about what the network thinks different things resemble, yeah?",
            "I don't, I don't see it, but maybe you are more robust classifier than me."
        ],
        [
            "Oh, but this net has been explicitly trained that whenever there's a light Gray pixel like that that it should ignore it.",
            "So if it had trained well, it should.",
            "It should know to disregard those things.",
            "OK.",
            "Sorry tycoon, Miotto and his collaborators took this observation that the distribution as a whole is getting shifted quite a lot, and they came up with a new training procedure."
        ],
        [
            "Well, you penalize the KL divergent's between the clean point and the modified point.",
            "And the really cool thing about this is it doesn't rely on the cost function anymore.",
            "The training procedure that I wrote about in my paper was based on the cost function.",
            "Given the inputs X, the class labels Y and the model parameters.",
            "This only needs the inputs X and the model parameters.",
            "It doesn't need why it just says R for some input point X, which we know is data perturbations near that data point shouldn't change the distribution over the class very much.",
            "The really cool thing is you can now use this for semi supervised learning if you have a large data set of unlabeled examples, you can just regularize the model to be smooth in their vicinity.",
            "At the time of this paper is published, the state of the Art, an MNIST with only 100 labeled examples, was three point 3% error using a variational autoencoder type model using virtual adversarial examples.",
            "They reduced the error rate to 2.12%.",
            "Since then, that's been beaten, but also using something that uses unsupervised learning, so this result is very interesting because it means a lot of the regularization that we see from training with unsupervised learning is.",
            "Not performing a whole lot better than what you can get with smoothness alone.",
            "This technique is really shown us the power of smoothness.",
            "It may not be completely state of the art, but it can get you very far.",
            "Yes, let's move this around the data, yeah?",
            "So I had a slide about that earlier.",
            "I'll revisit it quickly."
        ],
        [
            "So.",
            "Never mind, the latency is so bad on this."
        ],
        [
            "OK.",
            "If you said is Kirby is very flat, your current point, you're only resisting an infinitesimal perturbation.",
            "The Cal divergences orthogonal to this issue.",
            "What I'm saying I'm saying matters is.",
            "You can measure the Jacobian of the KL divergent's.",
            "I actually just got busy because you had a local minimum of it.",
            "You could you could measure like the eigenvalues of the Hessian of the KL divergent's and say that they should be very flat and that would say.",
            "When I make an infinitesimal perturbation, the KL divergent should not change very much.",
            "Adversarial training is saying when I make a perturbation of L2 norm 7 and Max norm .25, the KL divergent should not change very much.",
            "So it's it's actually feeding a new input into the model.",
            "It's not, it's not just analytically analyze grounds where feeding the new input in is important, because it allows you to use Universal approximator theorem type properties to resist the change.",
            "You can have totally different sets of units saturated and not saturating at that distant point.",
            "When you regularize the Jacobian, all you can really do innarelli net is make the weight smaller.",
            "If you have a sigmoid net you can.",
            "You can try to push things to wonder the sigmoid saturate, but forever that it's not really a very useful property."
        ],
        [
            "So I'm pretty near the end."
        ],
        [
            "My time, so we skip ahead a little bit.",
            "One thing I'd like to strongly recommend is that you benchmark adverse aerial resistance of your own models that you're studying, and the recommended way that I suggest you do this is with the fast gradient sign method.",
            "To do that, what you should report in your paper is it's getting kind of cut off much way.",
            "You choose an epsilon value and you then compute the fast gradient sign adversary examples using that epsilon value and you report the error rate on a test set that has been perturbed in that way.",
            "You also report the value of epsilon so other people can reproduce the results and make a comperable.",
            "Comparable benchmark to report on.",
            "A lot of the time we see these papers where people say I have a new method of regularization and regularising neural Nets is important because there are adversarial examples, but then they don't test if they have any effect on the ripples in this kind of thing is very easy to implement in Fano or even in torch.",
            "You just try to call back Prop, take the sign of the back prop with respect to the input, and then add it to the original example.",
            "There's another benchmark that's out there that you can use.",
            "It's the one of our first paper.",
            "In that case you use an optimizer to search for the smallest perturbation that causes a mistake, and you report the average size of that perturbation.",
            "I recommend not using that because it's very hard to make it optimizer that's reproducible enough that other people can do exactly the same thing you did.",
            "And also because sometimes the smallest mistake might actually correspond to flipping the class for real.",
            "So it might not really be a mistake."
        ],
        [
            "Likewise, if you want to benchmark your resistance to fooling images or rubbish class examples, I would suggest drawing samples or a Gaussian that you fit to the training inputs and then doing one step of the gradient sign method on that.",
            "Then just report the error rate at which your model thought something was present and report the error rate at which you were able to make it believe in one specific class."
        ],
        [
            "So in conclusion, many modern machine learning algorithms get the right answer on the training data or the test data, but they get it for the wrong reason and that causes them to make the wrong output on many unusual inputs.",
            "Deep learning is on the right course to overcome adversary examples.",
            "The Universal Approximator theorem says that we can definitely represent the right kind of function, provided that we make the network big enough and not every machine learning model has that advantage on its side.",
            "And if you think that you've come up with a way of improving resistance to adversarial examples, you should benchmark it and report your error rate under a reproducible class of adversary examples.",
            "If there's any questions left, I'm I'm done."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So my talk will be about adversarial examples, which I'm sure a lot of you have heard about.",
                    "label": 0
                },
                {
                    "sent": "There are mistakes that are known network makes when you provide it with unusual inputs that are intentionally chosen to make the network produce the wrong answer.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This talk will mostly be a summary of three different research papers that have come out over the past few years.",
                    "label": 0
                },
                {
                    "sent": "The first one is called intriguing properties of neural networks by Christians Egadi of Inception Fame, and he first observed that these adversarial examples happen.",
                    "label": 1
                },
                {
                    "sent": "The next research paper is one that I wrote with him and John Schlenz at Google.",
                    "label": 0
                },
                {
                    "sent": "Called explaining and harnessing adversarial examples where we showed how to find these adversarial examples more quickly, we provide some explanations for why the most common kinds of them are happening in current neural Nets, and we also show how training on them can actually improve neural networks.",
                    "label": 0
                },
                {
                    "sent": "And then finally there is a recent paper by Takara Miata and his collaborators, who have shown a different way of generating adversarial examples that can actually be used for semi supervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the reasons that we're interested in adversarial examples is that we would really like to be able to do something called model based optimization.",
                    "label": 0
                },
                {
                    "sent": "But to make it clear why model based optimization is so exciting, I'm going to rebrand it.",
                    "label": 0
                },
                {
                    "sent": "The Universal engineering machine for the duration of this slide.",
                    "label": 1
                },
                {
                    "sent": "The idea behind model based optimization is you want to solve an optimization problem, but you don't have any way of getting derivatives and maybe the.",
                    "label": 0
                },
                {
                    "sent": "The input states are not something that you can easily add epsilon differences to.",
                    "label": 0
                },
                {
                    "sent": "In order to do numerical differentiation.",
                    "label": 0
                },
                {
                    "sent": "One example of this is when you actually want to build some kind of physical product or when you want to find a gene or a drug that will fulfill some task or cure some disease.",
                    "label": 0
                },
                {
                    "sent": "In a lot of these cases, we can actually measure the performance of different products or genes or drugs, but we don't know how to take the derivative with respect to some piece of DNA or with respect to a molecule.",
                    "label": 0
                },
                {
                    "sent": "Where you can do instead is you can build a model that predicts how well one of these objects will perform, and then you can solve for the point where the model predicts that you'll get the best possible performance.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine having trained a model on a bunch of different models of cars and learning to predict their speed.",
                    "label": 0
                },
                {
                    "sent": "You could use optimization within the model to find a point that is predicted to have very high speed, and then you could build this theoretical extremely fast car.",
                    "label": 0
                },
                {
                    "sent": "You can apply that to really any problem.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, adversarial examples mean that when we try to extrapolate like this, we might not actually find a point that corresponds to an incredibly fast car, but rather to a point where our model makes an incredibly wrong prediction.",
                    "label": 0
                },
                {
                    "sent": "So that's that's a lot of why we should care about adversarial examples as engineers.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The reason to care about adversarial examples is from a more philosophical standpoint.",
                    "label": 0
                },
                {
                    "sent": "People have often wondered whether AI machines really understand what they're doing, and for a long time we could obviously say that they did not really understand what they were doing because they did not perform as well as humans on benchmark tasks.",
                    "label": 0
                },
                {
                    "sent": "Now, in many different tasks, humans have actually reached human level alot of the time.",
                    "label": 0
                },
                {
                    "sent": "These benchmarks are somewhat contrived.",
                    "label": 0
                },
                {
                    "sent": "For example, part of the reason that humans don't perform better on image net is that so much of it revolves around being able to distinguish obscur species of Stingrays, or being able to tell Siberian Huskies from Alaskan Huskies.",
                    "label": 0
                },
                {
                    "sent": "That may not be the best way of measuring what people are good at.",
                    "label": 0
                },
                {
                    "sent": "Likewise, on benchmarks of faces, people are not very good at recognizing strangers faces in photos.",
                    "label": 0
                },
                {
                    "sent": "Computers are probably not as good at recognizing these photos as those of your immediate friends and family.",
                    "label": 0
                },
                {
                    "sent": "But the point stands that on many benchmark tasks, computers have reached human level performance.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We've now entered a territory where it's somewhat more philosophically difficult to understand.",
                    "label": 0
                },
                {
                    "sent": "How well does the algorithm actually in some sense know what it is doing?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "These thoughts made me think of an old thought experiment proposed by the philosopher of mind, John Searle at Berkeley.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you could lock him John Searle in a room with a slot and passed notes in Chinese into the room.",
                    "label": 0
                },
                {
                    "sent": "He doesn't speak Chinese at all, but if he was equipped with a sufficiently exhaustive instruction book telling him what sequence of symbols to write down in reply to each input sequence of symbols, he could then pass notes back out through the window and the room.",
                    "label": 0
                },
                {
                    "sent": "Combined with John Searle and the instruction book would appear to understand Chinese.",
                    "label": 1
                },
                {
                    "sent": "A lot of people have debated what exactly this thought experiment means and how to interpret it, but it seems like there is one way that as a non philosophical engineer, one can still draw some insight from the example.",
                    "label": 0
                },
                {
                    "sent": "Which is you can see what happens if you put in a sentence for which there is no instruction book entry.",
                    "label": 1
                },
                {
                    "sent": "For machine learning algorithms, we can think of the data generating distribution as being the instruction book that we're providing to machine learning.",
                    "label": 0
                },
                {
                    "sent": "We're telling our machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Learn to approximate this distribution, and then in the future fill in the missing variables according to the distribution that you've learned.",
                    "label": 0
                },
                {
                    "sent": "If we don't go to unusual points where the data doesn't happen very often, we can see how the neural net extrapolates to these regions where the data has not provided much of a guide.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unfortunately, so far it turns out that the way that they extrapolate is very poorly Christian szegedy.",
                    "label": 0
                },
                {
                    "sent": "First notice this effect while he was attempting to make visualizations of what convolutional networks have learned.",
                    "label": 0
                },
                {
                    "sent": "The idea is we start with an input image of a ship, and then we use gradient descent to gradually perturb the image in a direction that causes the output of the model to believe that it's an airplane.",
                    "label": 0
                },
                {
                    "sent": "So we're just doing gradient descent on log probability of airplane given image.",
                    "label": 0
                },
                {
                    "sent": "With respect to image.",
                    "label": 0
                },
                {
                    "sent": "After five steps, this is what we're left with.",
                    "label": 0
                },
                {
                    "sent": "It looks like exactly the same boat, but now the network believes with nearly 100% confidence that this is an airplane.",
                    "label": 0
                },
                {
                    "sent": "Similarly, when we transform this car, I can't even really see any difference, but at the end of this sequence of transformations, the network believes have nearly 100% confidence that this is an airplane.",
                    "label": 0
                },
                {
                    "sent": "When we do the same thing to this cat, we can see that his face turns kind of blueish grey.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's the color of the metal on the fuselage of an airplane.",
                    "label": 0
                },
                {
                    "sent": "Other than that, it still looks like a cat to me.",
                    "label": 0
                },
                {
                    "sent": "And finally, this track really does not change at all, so it's possible that neural Nets have achieved superhuman performance, and there's a hidden airplane in there that were all blind too, but I think it's much more likely that the machine learning algorithm is just making a mistake.",
                    "label": 0
                },
                {
                    "sent": "The original reason that we did this experiment, or that Christian specifically did this experiment was that he was expecting the boat to grow wings or the background to turn blue and look like this guy, and he thought that it would be a way of diagnosing what convolutional networks are using to make their classification and innocence.",
                    "label": 0
                },
                {
                    "sent": "He was right, but the diagnosis was much worse than he expected it to be.",
                    "label": 0
                },
                {
                    "sent": "The features they seem to be using are not even necessarily anything that corresponds to human perception at all.",
                    "label": 0
                },
                {
                    "sent": "At first it yeah, go ahead.",
                    "label": 0
                },
                {
                    "sent": "Well, we can definitely tell that this is not an airplane.",
                    "label": 0
                },
                {
                    "sent": "Have you seen this image before?",
                    "label": 0
                },
                {
                    "sent": "You were able to process it correctly despite not having seen that exact one.",
                    "label": 0
                },
                {
                    "sent": "We expect.",
                    "label": 0
                },
                {
                    "sent": "If you look at it.",
                    "label": 0
                },
                {
                    "sent": "Birds.",
                    "label": 0
                },
                {
                    "sent": "Killing.",
                    "label": 0
                },
                {
                    "sent": "And they actually suffer evolutionarily for that they raised the cuckoo's egg instead of their own, and their offspring die.",
                    "label": 0
                },
                {
                    "sent": "So I you saying that like we should, we should make our algorithms intentionally be as unintelligent as the birds.",
                    "label": 0
                },
                {
                    "sent": "That file created the cuckoo.",
                    "label": 0
                },
                {
                    "sent": "I like this, but it seems pretty clear to me that it's a flood to be fooled by the cuckoo egg.",
                    "label": 0
                },
                {
                    "sent": "The other thing is, we expect our neural Nets to survive in the same environment that we survive in.",
                    "label": 0
                },
                {
                    "sent": "And you can do this with more or less any image and more or less any class.",
                    "label": 0
                },
                {
                    "sent": "So suppose that we were trying to make a pornography filter.",
                    "label": 0
                },
                {
                    "sent": "And prevent pornographic images from being displayed on web pages that we want to have a professional appearance.",
                    "label": 0
                },
                {
                    "sent": "If you could take a pornographic image and convince the computer that it's an airplane that it would go ahead and show that in a banner ad you don't want that to happen.",
                    "label": 0
                },
                {
                    "sent": "So so yeah, I mean it is.",
                    "label": 0
                },
                {
                    "sent": "It is a practical problem even if all you want to do is make web pages.",
                    "label": 0
                },
                {
                    "sent": "And hopefully you're not having a computer choose which eggs to sit on.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first we thought that a lot of the problem came from convolutional Nets being very complicated.",
                    "label": 0
                },
                {
                    "sent": "High capacity models and using this very deep nonlinear function to classify their inputs.",
                    "label": 0
                },
                {
                    "sent": "It turns out that a linear model has exactly the same problem.",
                    "label": 1
                },
                {
                    "sent": "So here we're using a softmax regression model that was trained on an M list.",
                    "label": 0
                },
                {
                    "sent": "So it's literally just a matrix multiplication to give you the logits, and then if you really want to get probabilities, you apply the softmax function.",
                    "label": 0
                },
                {
                    "sent": "You can actually classified just with the matrix multiply.",
                    "label": 0
                },
                {
                    "sent": "Followed by argmax on the outputs.",
                    "label": 0
                },
                {
                    "sent": "What we do here is we start off with a 9 and then we transform it until the model believes it's a 0.",
                    "label": 0
                },
                {
                    "sent": "Lee Yellow box indicates that it successfully has been fooled with 99% probability to believe that this is a zero.",
                    "label": 0
                },
                {
                    "sent": "Then we keep on optimizing and convince it instead.",
                    "label": 0
                },
                {
                    "sent": "But it's A1A two A 34567 and eight.",
                    "label": 0
                },
                {
                    "sent": "And well, it actually gets the nine right?",
                    "label": 0
                },
                {
                    "sent": "But only because it started there.",
                    "label": 0
                },
                {
                    "sent": "We actually managed to sweep through all of the 10 classes that the model knows about and it didn't end up producing anything that looks all that meaningful to me as a human observer.",
                    "label": 0
                },
                {
                    "sent": "It's it's a gradient step divided by the mean absolute value or something like that.",
                    "label": 0
                },
                {
                    "sent": "It's just designed to make it progress faster with a small number of steps so that the visualization is less tedious to look at.",
                    "label": 0
                },
                {
                    "sent": "If you do pure gradient steps, then you have to do a few more steps and it's hard to fit all of them on one screen.",
                    "label": 0
                },
                {
                    "sent": "So what exactly is going on here?",
                    "label": 0
                },
                {
                    "sent": "My argument today is that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's going to be about what's going on.",
                    "label": 0
                },
                {
                    "sent": "Here is something similar to what happened with a horse named Clever Hans.",
                    "label": 0
                },
                {
                    "sent": "I got this analogy for a paper called Clever Hans clever algorithms by Bob Sturm.",
                    "label": 1
                },
                {
                    "sent": "So the analogy is that there is this horse who had a trainer and the trainer was not any kind of charlatan.",
                    "label": 0
                },
                {
                    "sent": "He wasn't making money off of this.",
                    "label": 0
                },
                {
                    "sent": "Probably the only thing he got out of this was attention, and as far as anybody knows, he really did believe that the horse could do what he said it could do.",
                    "label": 0
                },
                {
                    "sent": "So when he said that his horse could do was arithmetic, you could ask the horse to compute 2 + 2 and then the horse would tap his foot four times.",
                    "label": 0
                },
                {
                    "sent": "Huge crowds of people would gather and watch the horse perform, and as far as anyone could tell, it really was able to actually answer a wide variety of questions that people ask the horse.",
                    "label": 0
                },
                {
                    "sent": "Later, the psychologist decided to examine the horse and he went to an enclosed area with no other people, and he wore a mask and he asked the horse to do arithmetic and the horse couldn't do it at all.",
                    "label": 0
                },
                {
                    "sent": "It turned out what was happening was the horse had not learn arithmetic.",
                    "label": 0
                },
                {
                    "sent": "The horse had learned how to read peoples emotional reactions.",
                    "label": 0
                },
                {
                    "sent": "So you would ask the horse to say add 1 + 2 and he would tap his hoof once and you would stare at him and expectation and you tap his purpose second time.",
                    "label": 0
                },
                {
                    "sent": "And and everybody would sit on the edge of the seat and then he would type with third time interval.",
                    "label": 0
                },
                {
                    "sent": "Oh my God.",
                    "label": 0
                },
                {
                    "sent": "He knows arithmetic and then he would stop tapping.",
                    "label": 0
                },
                {
                    "sent": "So clever Hans was trained to answer these questions.",
                    "label": 0
                },
                {
                    "sent": "And he found a way of doing it that made him appear to be successful by the metric of can he provide the right answer when there's a room full of people watching him, but he had accidentally queued onto something that wasn't quite the right underlying function.",
                    "label": 0
                },
                {
                    "sent": "He hadn't really discovered arithmetic, and he couldn't generalize to unusual situations where there wasn't a room of people to provide the extra cues that he needed to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "So that's more or less what's happened here are machine learning algorithms aren't intentionally trying to trick us.",
                    "label": 0
                },
                {
                    "sent": "But they've found cues that work very well on the training set where we train them, and those happen not to hold up when you intentionally analyze them and and try to step out of the area where they work.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit different from.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so they've.",
                    "label": 0
                },
                {
                    "sent": "They've learned the data distribution essentially and and they've managed to find a function that works exactly on a manifold where the data lies but then doesn't behave as you would expect when you go off of that manifold.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When I first learned about adversarial examples, we thought that they were mostly some kind of overfitting.",
                    "label": 0
                },
                {
                    "sent": "Some kind of result of the model capacity being too high, and the model having too much ability to represent highly nonlinear functions.",
                    "label": 0
                },
                {
                    "sent": "So the way that this cartoon picture works is suppose that we have these three training set.",
                    "label": 0
                },
                {
                    "sent": "X is in these three training set.",
                    "label": 1
                },
                {
                    "sent": "These for training setos.",
                    "label": 0
                },
                {
                    "sent": "We want the model to learn a decision boundary that will generalize well.",
                    "label": 0
                },
                {
                    "sent": "So let's represent this.",
                    "label": 0
                },
                {
                    "sent": "This blue masses to represent weather model believes there will be more axes and the green mask represents what a model believes.",
                    "label": 0
                },
                {
                    "sent": "There will be more owes.",
                    "label": 0
                },
                {
                    "sent": "In the view of adversarial examples, is resulting from overfitting.",
                    "label": 1
                },
                {
                    "sent": "We decided that the model just sort of randomly smears probability blobs all over the space and the training data isn't exhaustive enough to extinguish these random blobs, but they appeared more or less just by chance, and the vicissitudes of using extremely overparameterized distributions.",
                    "label": 0
                },
                {
                    "sent": "So we just happen to throw down this blob of blue probability mass.",
                    "label": 0
                },
                {
                    "sent": "We believe that.",
                    "label": 0
                },
                {
                    "sent": "That inputs RX is.",
                    "label": 0
                },
                {
                    "sent": "As a human observer, we can say, well, this point is really close to this point in the training set, and it would be most reasonable to predict that it would be an oh but the model mislabels it.",
                    "label": 0
                },
                {
                    "sent": "So I've drawn it in red.",
                    "label": 0
                },
                {
                    "sent": "Likewise, this is closest to the X is.",
                    "label": 0
                },
                {
                    "sent": "There's not really any reason to think there would be no, but the model is labeled it.",
                    "label": 0
                },
                {
                    "sent": "And oh, just kind of by chance.",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "I have a laser pen.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so first I was describing how this is mislabeled.",
                    "label": 0
                },
                {
                    "sent": "It's believed to be an X by the model, but it's really close to the owes.",
                    "label": 0
                },
                {
                    "sent": "This one is really close to the X is, but it's believed to be an oh by the model and in this view there's not any particular reason why the model chose to mislabel this one in the way it do it, or the way the reason it shows to disable this underrated, it's just the outcome of chance and bad luck.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's another view of how you could get adversarial examples, and that's the one that I'm advocating in this talk.",
                    "label": 1
                },
                {
                    "sent": "In this view, adversary examples come from the model being two simple to describe the real task.",
                    "label": 0
                },
                {
                    "sent": "It's able to find a way of jamming the data, and even the test distribution into what it's able to express, but it's not able to really describe the underlying dynamics.",
                    "label": 0
                },
                {
                    "sent": "So here we're using a linear classifier to separate the for training, set those in the three training set X is.",
                    "label": 0
                },
                {
                    "sent": "And you can see there's a lot of rich structure here that it's not capturing.",
                    "label": 0
                },
                {
                    "sent": "It doesn't understand that those are arranged in a semi circular shape.",
                    "label": 0
                },
                {
                    "sent": "It doesn't understand that the X is are highly linear.",
                    "label": 0
                },
                {
                    "sent": "Well, those aren't, and so on.",
                    "label": 0
                },
                {
                    "sent": "But it does manage to find a dividing hyperplane that separates the two of them.",
                    "label": 0
                },
                {
                    "sent": "The problem is now we can make a new point here which is extremely close to these axes and obviously not on the semi circular manifold or those lie and it gets classified as being a know when it should be an X.",
                    "label": 0
                },
                {
                    "sent": "Likewise over here.",
                    "label": 0
                },
                {
                    "sent": "This point is obviously a continuation of the metaphor in which they always lie.",
                    "label": 0
                },
                {
                    "sent": "But because it is crossed a linear boundary, it's now labeled as being an X, so I've got it in red to show that it's mislabeled.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When understanding the effect of models with low capacity, you need to have the correct intuition for what a low capacity model actually means.",
                    "label": 0
                },
                {
                    "sent": "A lot of the time people have a misleading intuition in which people believe that a low capacity model cannot make very different, very confident predictions about different points in space.",
                    "label": 0
                },
                {
                    "sent": "So this is the weather.",
                    "label": 0
                },
                {
                    "sent": "People usually conceptualise low capacity models.",
                    "label": 1
                },
                {
                    "sent": "We have a bunch of training sets.",
                    "label": 0
                },
                {
                    "sent": "The training set X is we have high confidence that their ex is near the training set points.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of training set, those we have high confidence that there arose.",
                    "label": 0
                },
                {
                    "sent": "Near where the training set data lie, lay and then when you go out to the edges of the space, there's no data there, and the model has little confidence because it has had no incentive to place high confidence values there.",
                    "label": 0
                },
                {
                    "sent": "So that's one way that a model can have low capacity.",
                    "label": 0
                },
                {
                    "sent": "Your intuition is not entirely incorrect.",
                    "label": 0
                },
                {
                    "sent": "We believe that these models exist.",
                    "label": 0
                },
                {
                    "sent": "This happens if you have something like an RBF classifier that can just place one bump of probability, posterior probability per class.",
                    "label": 0
                },
                {
                    "sent": "However, there are other ways of having low capacity, specifically, low capacity just means that you're forced to allocate your confidence about classes to different regions according to some predefined schedule rather than according to what you would like to do and what the data says you should do.",
                    "label": 0
                },
                {
                    "sent": "So a linear model also has fairly low capacity.",
                    "label": 1
                },
                {
                    "sent": "It's confidence has to be tide to this red line.",
                    "label": 0
                },
                {
                    "sent": "It's confidence has to be linear and what we do is we then take the sigmoid of this red line to find the probability of being an 0.",
                    "label": 0
                },
                {
                    "sent": "In order to have this probability be reasonably high on the training data on these training ozanne reasonably low, and these training X is it needs to be even higher when you go out to the extreme values on the right and even lower when you got to the extreme values on the left.",
                    "label": 0
                },
                {
                    "sent": "So linear models actually become more confident when you force them to extrapolate and go into regions, but they never saw before when in fact they should become less confident according to most people's intuition.",
                    "label": 0
                },
                {
                    "sent": "So what does this have to do with deep neural networks?",
                    "label": 0
                },
                {
                    "sent": "We always hear how nonlinear deep neural networks are.",
                    "label": 0
                },
                {
                    "sent": "But really there.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A lot of extremely linear pieces.",
                    "label": 0
                },
                {
                    "sent": "In fact, we intentionally designed them to be as linear as we can get away with one of the most popular units right now is the rectified linear unit, which has one linear piece with hard coded slope zero, and then one linear piece where the slope can be learned by adopting the weights.",
                    "label": 1
                },
                {
                    "sent": "There's also Max out with many linear pieces that can all be adapted separately.",
                    "label": 1
                },
                {
                    "sent": "Sigmoid units are not linear, but they often need to be carefully tuned an initialized so that you operate primarily in the linear regime when you don't do that.",
                    "label": 0
                },
                {
                    "sent": "There are much harder to optimize, so to some extent even our sigmoid nuts have many pieces that behave extremely linearly.",
                    "label": 0
                },
                {
                    "sent": "Finally, an LS, TM and recurrent net that accumulates information overtime uses addition of the state vectors to accumulate its state overtime.",
                    "label": 0
                },
                {
                    "sent": "That's why they're able to back propagate through time so well.",
                    "label": 0
                },
                {
                    "sent": "And this addition is of course just a linear operation.",
                    "label": 0
                },
                {
                    "sent": "The main differences in this case it's been hard coded always use a coefficient of 1 rather than learning that particular coefficient.",
                    "label": 0
                },
                {
                    "sent": "So we've built our neural Nets out of very linear pieces.",
                    "label": 0
                },
                {
                    "sent": "Does that mean that the overall function they learn is very linear?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm not sure that it is in every case, but it is in many cases that I've tested, so I did.",
                    "label": 0
                },
                {
                    "sent": "Here is, I load an image of a car.",
                    "label": 0
                },
                {
                    "sent": "And then I add perturbations to the car as we move left to right in this graph, I'm adding more and more of this image into it, and then as you move to the left, I'm subtracting that same image off to get the negative image.",
                    "label": 0
                },
                {
                    "sent": "So we're swapping out a linear path in the input of the network.",
                    "label": 0
                },
                {
                    "sent": "On this graph I'm plotting the argument to the softmax.",
                    "label": 1
                },
                {
                    "sent": "So the network is consisting of Max out units and the only non linear function with curvature rather than the Max over different linear functions is the soft Max at the end.",
                    "label": 0
                },
                {
                    "sent": "So if I'd put everything up to the softmax we get this function where except for right immediately around the data, it's very linear and nonlinear log probability.",
                    "label": 0
                },
                {
                    "sent": "The unnormalized log probabilities get more and more extreme as you go out to the far ends there is.",
                    "label": 0
                },
                {
                    "sent": "I'm planning to argument to the softmax is that if I put it the softmax itself.",
                    "label": 0
                },
                {
                    "sent": "It would just saturate immediately and would tell you that each of the classes has probability one or zero pretty much everywhere, but right around the data.",
                    "label": 0
                },
                {
                    "sent": "So to study this better, we decided we were interested in a way of coming up with a formula that gives us a high confidence misclassified adversarial example with high probability.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "What which point?",
                    "label": 0
                },
                {
                    "sent": "Oh this is actually an adversary of perturbation.",
                    "label": 0
                },
                {
                    "sent": "And yeah, I haven't described yet how I make them.",
                    "label": 0
                },
                {
                    "sent": "It's possible, yeah.",
                    "label": 0
                },
                {
                    "sent": "But basically this is the direction that you would go in if you wanted to cause a linear model to do this, and it does what a linear model would do.",
                    "label": 0
                },
                {
                    "sent": "So I'm trying to find every solar perturbations one of the hardest things to deal with out.",
                    "label": 0
                },
                {
                    "sent": "There is a question back.",
                    "label": 0
                },
                {
                    "sent": "Send in high dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "Most vectors that are randomly chosen are orthogonal to each other.",
                    "label": 0
                },
                {
                    "sent": "So if you know that a certain class increases in probability if you move in direction D. Then when you generate a random vector that one random vector is probably going to be orthogonal to D, so it probably won't affect the probability of the class that's tide to direction D. If you choose something which you know is in the half space of our end where vectors have large dot product with D, then most of those perturbations do seem to actually increase.",
                    "label": 0
                },
                {
                    "sent": "That class is probability, so it is half spaces.",
                    "label": 0
                },
                {
                    "sent": "Then little pockets where adversarial examples occur.",
                    "label": 0
                },
                {
                    "sent": "There are very large connected regions of mistakes, but they're hard to find just by random noise because you have, at least for things like cifar 10, you have relatively few classes compared to the number of input dimensions, so there's maybe something like 10 dimensions that strongly affect that most strongly affect the classes, and there's 3000 random directions you can move in.",
                    "label": 0
                },
                {
                    "sent": "You've got to pick one of the 10 that really pushes your linear classifier toward one class or the other.",
                    "label": 0
                },
                {
                    "sent": "I will be talking about that later.",
                    "label": 0
                },
                {
                    "sent": "You don't need to have this specific network.",
                    "label": 0
                },
                {
                    "sent": "If you perturb the perturbation.",
                    "label": 0
                },
                {
                    "sent": "Or I mean.",
                    "label": 0
                },
                {
                    "sent": "You saying like if you, what do you pretend the perturbation with it?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if you add small noise to it then you're going small noise that you had is probably going to be orthogonal to the direction D that affects the classification.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "It's it's it's sensitive to specific directions.",
                    "label": 0
                },
                {
                    "sent": "It's not really sensitive to exact points, and most randomly chosen directions are going to be orthogonal to the directions that sensitive too.",
                    "label": 0
                },
                {
                    "sent": "If you use very large noise then you can you can mess up the original.",
                    "label": 0
                },
                {
                    "sent": "That, or you could probably change the classification of the adversarial example.",
                    "label": 0
                },
                {
                    "sent": "But as long as the noise is reasonably small, it shouldn't do a whole lot.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm trying to design A procedure for finding adversarial examples in closed form.",
                    "label": 0
                },
                {
                    "sent": "One thing that's difficult is that you don't know whether a perturbation is going to consist of causing a mistake, or if legitimately changing the class ahead of time.",
                    "label": 0
                },
                {
                    "sent": "So here we have an endless three.",
                    "label": 0
                },
                {
                    "sent": "We add a perturbation to it.",
                    "label": 0
                },
                {
                    "sent": "This perturbation as L2 run of 3.96.",
                    "label": 0
                },
                {
                    "sent": "And we get something that's obviously A7.",
                    "label": 0
                },
                {
                    "sent": "So here we made a perturbation and we really did change the class.",
                    "label": 0
                },
                {
                    "sent": "So if the network changes its opinion about what the class was from here to here, that's not really a mistake.",
                    "label": 0
                },
                {
                    "sent": "Here we add random noise with exactly the same L2 norm's here.",
                    "label": 0
                },
                {
                    "sent": "And we get you know three on our sudden Piper background.",
                    "label": 0
                },
                {
                    "sent": "So here the class has not changed at all despite the perturbation having the same magnitude is here.",
                    "label": 0
                },
                {
                    "sent": "And then finally, at the end we have another perturbation where we just erased some of the pixels at the top of the three and we end up with something that's just garbage and doesn't look like a digit at all.",
                    "label": 0
                },
                {
                    "sent": "All through these perturbations of the same L2 norm.",
                    "label": 1
                },
                {
                    "sent": "And it's a relatively small L2 norm as well and experiments.",
                    "label": 0
                },
                {
                    "sent": "I'll show you usually using L2 norm of seven, so this is a little bit more than half of the size of perturbation.",
                    "label": 1
                },
                {
                    "sent": "Usually like to use.",
                    "label": 0
                },
                {
                    "sent": "So how can we actually go into that?",
                    "label": 1
                },
                {
                    "sent": "A perturbation won't change the class.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, unfortunately that's a little bit problem dependent, but for computer vision.",
                    "label": 0
                },
                {
                    "sent": "And specifically for recognizing object categories and images.",
                    "label": 0
                },
                {
                    "sent": "You can assume that each pixel has a minimum precision, below which changing that pixel will not affect the class.",
                    "label": 1
                },
                {
                    "sent": "So for example, image NET and M Nest are both stored as 8 bit integers, and you can imagine that if you change something that smaller that can be, then you make a change that is smaller than can be represented with eight bits, then that should not affect the class so embarrassed when we went through the network is usually represented with 32 or 64 bits.",
                    "label": 0
                },
                {
                    "sent": "If we just make a change that's smaller than 1 / 255, it definitely shouldn't affect.",
                    "label": 0
                },
                {
                    "sent": "The output.",
                    "label": 0
                },
                {
                    "sent": "In practice, we can usually do a little bit better than that, we can say.",
                    "label": 0
                },
                {
                    "sent": "That and this is mostly black and mostly white, so you could imagine that a perturbation of like .25 or so shouldn't really change something from black to white.",
                    "label": 0
                },
                {
                    "sent": "In practice, humans can read perturbations up to about .25 and not beyond that.",
                    "label": 0
                },
                {
                    "sent": "There are some Gray values in amnist, and those Gray values can become wider black pixels if you allow bigger perturbation than that.",
                    "label": 1
                },
                {
                    "sent": "So in this slide what I do is I constrain the Max norm.",
                    "label": 0
                },
                {
                    "sent": "The magnitude of the pixel that the magnitude of the largest pixel that changes.",
                    "label": 0
                },
                {
                    "sent": "So to find the biggest perturbation under maximum constraint, you can actually have every single pixel change.",
                    "label": 0
                },
                {
                    "sent": "The most that it's allowed to, because the maximum doesn't add up over different examples, it's just the Max across all of them, so all of them will be right at the border, doing as much damage as you can possibly get.",
                    "label": 0
                },
                {
                    "sent": "What I did here was I took the perturbations in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "And I pushed them right up to the maximum boundary, but no, no further.",
                    "label": 1
                },
                {
                    "sent": "And then, on pixels that didn't make any change before I had them do random sign changes just so that I could do as much damage as possible while still obeying the maximum constraint.",
                    "label": 0
                },
                {
                    "sent": "So these all have the exact same L2 norm as on the previous slide, but they've been forced to obey a Max norm constraint, and you can see that now none of them changes the real class.",
                    "label": 0
                },
                {
                    "sent": "So that tells us how we can actually make changes that we know are going to give us more things from the same class.",
                    "label": 0
                },
                {
                    "sent": "For these object recognition type tasks.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Given that we have this constraint region, we can say we want to find an adversary example that lies within this constraint region.",
                    "label": 0
                },
                {
                    "sent": "But we want to maximize the cost that we have on the training set.",
                    "label": 0
                },
                {
                    "sent": "Actually maximizing the cost would require doing some kind of gradient based search surrounding the original example, and that's what Christians egadi did in the 1st paper that we published.",
                    "label": 0
                },
                {
                    "sent": "But to make things a lot faster, we can make a Taylor series approximation.",
                    "label": 0
                },
                {
                    "sent": "We can say the cost added adversarial example.",
                    "label": 0
                },
                {
                    "sent": "It is going to be pretty close to the cost at the clean example, plus the difference between the two multiplied by the gradient with respect to the input.",
                    "label": 0
                },
                {
                    "sent": "We then left with this linear equation where we're trying to maximize this value subject to this constraint, and it has a very simple closed form solution.",
                    "label": 0
                },
                {
                    "sent": "The adversarial example is equal to the clean example.",
                    "label": 0
                },
                {
                    "sent": "Plus the precision of our features multiplied by the sine of the gradient of the cost with respect to the input.",
                    "label": 0
                },
                {
                    "sent": "Essentially what we're doing is.",
                    "label": 0
                },
                {
                    "sent": "We're assuming that the cost is linear, and then we're making the perturbation that hurts it the most under the belief that changing each pixel a small amount can change the class.",
                    "label": 0
                },
                {
                    "sent": "So if this works, it's strong evidence that a lot of the problem with adversarial examples is due to the networks being highly linear.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It indeed works.",
                    "label": 0
                },
                {
                    "sent": "Here is a picture of a panda which was classified with 57.7%.",
                    "label": 0
                },
                {
                    "sent": "Confidence is being a panda.",
                    "label": 0
                },
                {
                    "sent": "We apply the fast gradient sign method to compute this gradient.",
                    "label": 0
                },
                {
                    "sent": "The actual perturbation that were added is classified itself as being a nematode with eight point 2% confidence.",
                    "label": 0
                },
                {
                    "sent": "And then the final classification.",
                    "label": 0
                },
                {
                    "sent": "The final image, resulted from adding the two.",
                    "label": 0
                },
                {
                    "sent": "Is classified as being given with 99.3% confidence.",
                    "label": 0
                },
                {
                    "sent": "The images that you are seeing here and here are actually identical the.",
                    "label": 0
                },
                {
                    "sent": "The keynote file is not capable of storing the difference between those two, and even if it was, the projector couldn't display it.",
                    "label": 0
                },
                {
                    "sent": "But it changes us from believing that with 60% confidence it's the right class two with over 99% confidence.",
                    "label": 0
                },
                {
                    "sent": "It's the wrong class.",
                    "label": 0
                },
                {
                    "sent": "To get some intuition for exactly what this is doing.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's helpful to look at a shallow linear model where you can characterize the whole model just by looking at its weights.",
                    "label": 0
                },
                {
                    "sent": "So this will clean the M list.",
                    "label": 0
                },
                {
                    "sent": "3 is a nest sevens look like if we train and logistic regression model on them, we get something that looks kind of like the Fischer projection of the two classes.",
                    "label": 0
                },
                {
                    "sent": "We get these weights here as we move along array that looks like this.",
                    "label": 0
                },
                {
                    "sent": "We move from something that looks kind of like the centroid of all the threes to something that looks like the centroid of all the sevens.",
                    "label": 0
                },
                {
                    "sent": "This particular image is not really recognizable to a human, but that's that's the core of what drives the classifier after it's been trained.",
                    "label": 0
                },
                {
                    "sent": "If we want to find the worst thing you can do in the classifier under a maximum constraint, we take the sign of those weights.",
                    "label": 0
                },
                {
                    "sent": "Make it this image here, which that looks even less meaningful to me as a human being, because we've destroyed a lot of the magnitude information in it.",
                    "label": 0
                },
                {
                    "sent": "But we were just trying to find out what's the worst thing we could do to each pixel.",
                    "label": 0
                },
                {
                    "sent": "If we don't take this image and add either this image or it's negative to the clean examples, we could address several examples that are misclassified with an error rate of over 99%.",
                    "label": 0
                },
                {
                    "sent": "You could say this is just enlist, but Andre.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Party at Stanford has actually.",
                    "label": 0
                },
                {
                    "sent": "Implemented this for image net and an image net.",
                    "label": 0
                },
                {
                    "sent": "You don't get confident enough predictions on either clean or adverse aerial examples to see a huge difference.",
                    "label": 0
                },
                {
                    "sent": "You can see this original image was classified with eight point, 3% probability of being a goldfish.",
                    "label": 0
                },
                {
                    "sent": "And then after adding an image of the weights of the linear model.",
                    "label": 0
                },
                {
                    "sent": "We get that it's 12.5% likely to be a Daisy.",
                    "label": 0
                },
                {
                    "sent": "So even in very large images you can confuse linear classifiers in this way.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, every everything I've attacked with it, although that's I have attacked with this method, have had a pretty high error rate on it unless they were explicitly trained to resist it.",
                    "label": 0
                },
                {
                    "sent": "No, it it.",
                    "label": 0
                },
                {
                    "sent": "I mean this particular experiment.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I've done other experiments where I take that direction and.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this experiment I take that same direction and I go very far out and it actually becomes more linear the farther out I go.",
                    "label": 0
                },
                {
                    "sent": "It's actually somewhat nonlinear near the data, but it is.",
                    "label": 0
                },
                {
                    "sent": "It is sufficiently linear that I can succeed in getting the wrong class inside this area, and then once I'm outside this area, it's very conspicuously very linear.",
                    "label": 0
                },
                {
                    "sent": "I will take that.",
                    "label": 0
                },
                {
                    "sent": "Cost itself may not be.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm actually going one step further and I'm linearizing the cost, and that has some chance of failing, but it doesn't really seem to.",
                    "label": 0
                },
                {
                    "sent": "In practice.",
                    "label": 0
                },
                {
                    "sent": "The other thing is sure neural Nets are locally linear, but there are locally linear on small enough regions that I probably am making different maxout units activate or different reluz activate.",
                    "label": 0
                },
                {
                    "sent": "When I make these perturbations.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I spent a lot of time telling you how terrible when your models are.",
                    "label": 0
                },
                {
                    "sent": "Is there an alternative?",
                    "label": 0
                },
                {
                    "sent": "I'm not necessarily advocating that we use RBF since did, but just to show that there is something that doesn't have all these properties.",
                    "label": 0
                },
                {
                    "sent": "This is what happens if we use gradient ascent on the input to start with a 9 and then transform the 9 into a zero and then A1A2A3 or 45678 and back to 9.",
                    "label": 0
                },
                {
                    "sent": "We're not perfect, but.",
                    "label": 0
                },
                {
                    "sent": "We can actually see what it's going for, so this is just an RBF classifier where the posterior gets one Gaussian bump per class and.",
                    "label": 0
                },
                {
                    "sent": "It seems to be behave just fine far from the data, yeah?",
                    "label": 1
                },
                {
                    "sent": "Yeah, it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "The neural net part poisons it.",
                    "label": 0
                },
                {
                    "sent": "So the thing was kind of kind of cool and kind of frustrating about the adversarial setting is.",
                    "label": 0
                },
                {
                    "sent": "We're used to thinking about average case performance and we just try to come up with defenses that work most of the time on the data and hope that the data rarely falls into the hole.",
                    "label": 0
                },
                {
                    "sent": "In our defense, the adversarial setting looks for the gap in your armor and just shoot straight for that and.",
                    "label": 0
                },
                {
                    "sent": "If there's a way to screw up your classifier by exploiting linearity in it, it's going to find that.",
                    "label": 0
                },
                {
                    "sent": "So in this case, if you have the RBF classifier on top of a neural net, the neural net itself can output crazy values if you perturb it linearly.",
                    "label": 0
                },
                {
                    "sent": "So we just make it produce a crazy hidden representation, and then the RBF reads out to the wrong class that's encoded in that hidden representation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's it's harder to give them very high confidence because.",
                    "label": 0
                },
                {
                    "sent": "So the equation that I'm using here, the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Epsilon Times sign of gradient equation.",
                    "label": 0
                },
                {
                    "sent": "That works really well if you continually get worse the further you go in One Direction.",
                    "label": 0
                },
                {
                    "sent": "But if there's a local maximum and your confidence, then it might overshoot it and that can happen with RBF notes because it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not adapting Epsilon to make sure that you stop right at the point it believes is the most like whatever class you're trying to go into.",
                    "label": 0
                },
                {
                    "sent": "If you did it like a line search in this direction, you could probably get RBF sterba fairly high confidence.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm saying.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If our beliefs are able to do is so much better and linear models chained together behaves so poorly, why are we always using linear pieces to build our neural Nets?",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reason is that models that are easy to optimize are also easy to perturb, and vice versa.",
                    "label": 1
                },
                {
                    "sent": "Andrej Karpathy called this wiggle free in his blog post as he was summarizing my idea, but I like his summary of my idea better than my original explanation.",
                    "label": 0
                },
                {
                    "sent": "So the idea of wiggle theory is.",
                    "label": 0
                },
                {
                    "sent": "In order to train the weights for this hidden unit, we need to be able to wiggle this hidden unit and get a change at the output.",
                    "label": 0
                },
                {
                    "sent": "We need to build.",
                    "label": 0
                },
                {
                    "sent": "Wiggle this weight and make this this hidden unit wiggle and get a change all the way up here.",
                    "label": 0
                },
                {
                    "sent": "The problem is if wiggling this.",
                    "label": 0
                },
                {
                    "sent": "Can cause sufficient upstream changes, that means that wiggling this.",
                    "label": 0
                },
                {
                    "sent": "Must also do more or less the same thing, and less.",
                    "label": 0
                },
                {
                    "sent": "This weight is very, very small.",
                    "label": 0
                },
                {
                    "sent": "So essentially when you have different layers composed together, if if you become immune to perturbations at this point.",
                    "label": 0
                },
                {
                    "sent": "It means you can no longer optimize anything below it, and when you apply that logic recursively, you end up having to be able to respond to perturbations all the way down yahshua.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's.",
                    "label": 0
                },
                {
                    "sent": "Defense strategy that I call hide the gradient and it's it's a way of making my gradient sign thing not work as well.",
                    "label": 0
                },
                {
                    "sent": "10 inch will actually still be vulnerable to the gradient sign equation, but you could imagine what if we go more aggressive and rather than 10 itch?",
                    "label": 0
                },
                {
                    "sent": "What if we just have a threshold and return positive one if you're greater than zero and negative one if you're less than 0 so that threshold unit won't have a gradient anymore?",
                    "label": 0
                },
                {
                    "sent": "But if you knew the direction of its weights, you could still do the same perturbation and flip it from negative one to positive one.",
                    "label": 0
                },
                {
                    "sent": "So you can actually still make adversarial examples for non differentiable models that saturate as long as in the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "They are driven by some linear weight vector that will be really confident when you move out to one side or out to the other side.",
                    "label": 0
                },
                {
                    "sent": "I'll show some.",
                    "label": 0
                },
                {
                    "sent": "I'll show an attack on a non differentiable model later on.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit more complicated.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the other thing is that space is full of mistakes, you just have to add components of the weight vectors that you'll get a mistake.",
                    "label": 0
                },
                {
                    "sent": "They're really, really easy to find.",
                    "label": 0
                },
                {
                    "sent": "This is taking ASIFA 10 convolutional net and looking for airplanes.",
                    "label": 0
                },
                {
                    "sent": "The pink boxes indicate things that are classified as being an object of some kind.",
                    "label": 0
                },
                {
                    "sent": "And since none of these things are objects that are run, the yellow boxes are classified specifically as airplanes.",
                    "label": 0
                },
                {
                    "sent": "So the way that I did this was I generated Gaussian noise with unit covariance and then I did one step of the fast side and gradient perturbation toward the airplane class.",
                    "label": 0
                },
                {
                    "sent": "So I said I'm going to take the derivative of probability of Y equals airplane given X with respect to X.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to take the sign of that and add epsilon times that.",
                    "label": 0
                },
                {
                    "sent": "And so this succeeded in getting airplanes about 1/4 of the time.",
                    "label": 0
                },
                {
                    "sent": "So if you wanted to make a fake airplane that looks like no training data at all, you'd need about four samples on average before you get ahead.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are a lot of other papers out there about fooling.",
                    "label": 0
                },
                {
                    "sent": "Well, not with images that don't correspond to any class.",
                    "label": 0
                },
                {
                    "sent": "One thing that's in common between both of these is that they're usually relatively expensive to evaluate, and they usually have some kind of hand coded prior in them, so you'll see these images that look like really nice and semantically meaningful to people somehow.",
                    "label": 0
                },
                {
                    "sent": "Obviously they don't look like the things that the network is classifying them as being, but it looks like there's some kind of structure and meaning in there.",
                    "label": 0
                },
                {
                    "sent": "The reason there's structure and meaning in there is that these algorithms use searches with a strong prior for there to be some kind of structure and meaning.",
                    "label": 0
                },
                {
                    "sent": "If you actually just optimize what the neural net response to you get.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You get garbage like this.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is, a lot of these algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To run like evolutionary algorithms for thousands of iterations before they find their high confidence fooling example, you don't need to do that.",
                    "label": 0
                },
                {
                    "sent": "If you want to fool Internet, just draw a sample from a Gaussian distribution and for the average neural net that will be classified as belonging to some class with very high probability.",
                    "label": 0
                },
                {
                    "sent": "If you wanted to come from a specific class, do one step side method and about 1/4 of the time you'll get it to be from that class.",
                    "label": 0
                },
                {
                    "sent": "So that's that's fooling images.",
                    "label": 0
                },
                {
                    "sent": "The next topic is something that some yeah.",
                    "label": 0
                },
                {
                    "sent": "That's in the paper.",
                    "label": 0
                },
                {
                    "sent": "There's still fold.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It still fills it with high probability.",
                    "label": 0
                },
                {
                    "sent": "Even with just Gaussian samples I was getting amnist classifiers to report that there were digits present with high probability.",
                    "label": 0
                },
                {
                    "sent": "I was counting it as a mistake.",
                    "label": 0
                },
                {
                    "sent": "If they said probability greater than oh point 5 that there is something there.",
                    "label": 0
                },
                {
                    "sent": "So if you think about it, any individual class only has about 50% probability of it being aligned with its weights.",
                    "label": 0
                },
                {
                    "sent": "But then you've got 10 different classes, so you might lose the coin toss on one of them, but it's it's hard to lose 10 classes to lose 1020 to lose 10 coin tosses in a row.",
                    "label": 0
                },
                {
                    "sent": "Or that's if you're looking at it from the perspective of the adversary.",
                    "label": 0
                },
                {
                    "sent": "If you're looking at it from the perspective of the neural net, it's hard to win ten coin tosses in a row.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh oh, like how could?",
                    "label": 0
                },
                {
                    "sent": "How could you prevent this?",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "They are not objects there, and it mistakenly believes that there are.",
                    "label": 0
                },
                {
                    "sent": "When you say how does it know that there are objects there?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure what you mean.",
                    "label": 0
                },
                {
                    "sent": "They will look at it.",
                    "label": 0
                },
                {
                    "sent": "It's it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a picture of random pixels, so there is no object there, but it is.",
                    "label": 0
                },
                {
                    "sent": "The neural net is saying that there is an object there.",
                    "label": 0
                },
                {
                    "sent": "Training on pictures that are not objects might be one way to fix this problem, but not every neural net need.",
                    "label": 0
                },
                {
                    "sent": "Not every machine learning needs.",
                    "label": 0
                },
                {
                    "sent": "Not every machine learning algorithm needs that.",
                    "label": 0
                },
                {
                    "sent": "So I.",
                    "label": 0
                },
                {
                    "sent": "Ideally it would say that it has no confidence that anyone of the 10 is present.",
                    "label": 0
                },
                {
                    "sent": "It would, it would say it would say that there's a uniform distribution over the 10 classes because there's no evidence for any one of them in particular.",
                    "label": 0
                },
                {
                    "sent": "So that's what happens with something like a Gaussian process with a.",
                    "label": 0
                },
                {
                    "sent": "With an RBF kernel, is that as you move far from the data that Gaussian process becomes less and less confident about its prediction?",
                    "label": 0
                },
                {
                    "sent": "Here you have the opposite effect.",
                    "label": 0
                },
                {
                    "sent": "As you move further from the data that becomes more and more confident about one specific prediction, even though there's not really any evidence for that particular view.",
                    "label": 0
                },
                {
                    "sent": "A relative threshold.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "If you impose a threshold you will throw out all the clean data.",
                    "label": 0
                },
                {
                    "sent": "It is more confident about these that it is about the clean data.",
                    "label": 0
                },
                {
                    "sent": "On the clean data it will say this is a panda with 70% probability on the garbage data.",
                    "label": 0
                },
                {
                    "sent": "It'll say this is a panda with 99.3% probability.",
                    "label": 0
                },
                {
                    "sent": "So if you try to threshold based on the confidence you will actually throw out the main thing that you're trying to do before you will throw out the garbage.",
                    "label": 0
                },
                {
                    "sent": "It's shocking, I know, yeah.",
                    "label": 0
                },
                {
                    "sent": "Biological neural network.",
                    "label": 0
                },
                {
                    "sent": "It wouldn't have the same problem, so I guess I'm wondering.",
                    "label": 0
                },
                {
                    "sent": "Your your bar would need to be large enough to undo the perturbation, I mean, so it's a linear model, right?",
                    "label": 0
                },
                {
                    "sent": "Like, say that over here you get the right class and then as you follow my arm up till you come to a point where you get the wrong class.",
                    "label": 0
                },
                {
                    "sent": "If you just draw a ball around it, it's going to be a pretty big ball before you come back to the good area and half of that ball is going to be even worse and half of the ball will be fixing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm telling you is that.",
                    "label": 0
                },
                {
                    "sent": "The shape of the bad area is defined by the geometry of the model itself, and the model is very linear, so there's like half spaces of madness.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that's a promising direction.",
                    "label": 0
                },
                {
                    "sent": "An if you were here for the morning talk.",
                    "label": 0
                },
                {
                    "sent": "There was one thing I was saying Mark asked me like what would I like to see from optimization and I said being able to train some of the models that we just can't really fit all that well.",
                    "label": 0
                },
                {
                    "sent": "Right now.",
                    "label": 0
                },
                {
                    "sent": "Part of that is because I think that some of those models could be a way of solving adversarial examples.",
                    "label": 0
                },
                {
                    "sent": "Yes, and the reason for that will become more clear pretty soon.",
                    "label": 0
                },
                {
                    "sent": "Then they become more different as you propagate through the net and a lot of the time they become pretty different even before the first layer.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about that pretty soon, yeah?",
                    "label": 0
                },
                {
                    "sent": "Oh I will also talk about that pretty soon here, yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh well, different datasets.",
                    "label": 0
                },
                {
                    "sent": "I've only done amnesty for 10 and Imagenet then main reason is I had the slides earlier was talking about how I needed to design A perturbation where I know that the perturbation can't change the real class.",
                    "label": 0
                },
                {
                    "sent": "Like so feminist, if I change a pixel by less than 1 / 255, I know that doesn't change the real class for Boston houses pricing data.",
                    "label": 0
                },
                {
                    "sent": "I don't know enough about that kind of data to know like like if you get rid of five square feet, should your your house price plummet or not?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I think I'm going to move forward a few slides I see still some questions, but a lot of the questions I keep saying I'm going to tell you a few slides so will do.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "New slides and see if your questions diminish.",
                    "label": 0
                },
                {
                    "sent": "So what are the other very startling properties of adversarial examples that Christian observed back in 2013?",
                    "label": 0
                },
                {
                    "sent": "Is that they generalize across different models and also across even different training sets, as long as the training sets are drawn from the same class, this is one thing that we tried to explain in the explaining and harnessing paper.",
                    "label": 0
                },
                {
                    "sent": "So our explanation is that machine learning algorithms generalize.",
                    "label": 0
                },
                {
                    "sent": "You get samples from some distribution.",
                    "label": 0
                },
                {
                    "sent": "You learn some kind of function from that distribution, and then it's more or less the right function to use on other data from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "That's why you can generalize from the train set to the test set.",
                    "label": 0
                },
                {
                    "sent": "It also means that you can train on two different training sets drawn from the same distribution, and then you'll be able to do well on the test set regardless of the specific identity of the training set you used.",
                    "label": 0
                },
                {
                    "sent": "In terms of thinking about the weights of a linear model that you learn, you can turn on emnace, threes and sevens anyone so it looks like the Fischer projection of the threes and sevens.",
                    "label": 0
                },
                {
                    "sent": "These are two different training sets here, but we get weights that look more or less the same.",
                    "label": 0
                },
                {
                    "sent": "And because the weights are more or less the same, the discriminative direction that they respond to is more or less the same.",
                    "label": 0
                },
                {
                    "sent": "The two models are going to make more or less the same mistakes.",
                    "label": 0
                },
                {
                    "sent": "This means that if you want models to make very different mistakes, they need to have fairly different classification functions, and so far I've been arguing that neural Nets tend to make very linear classification decisions.",
                    "label": 0
                },
                {
                    "sent": "So this idea predicts that neural Nets and linear models will make some of the same mistakes as each other, but neural Nets and RBF will make different mistakes.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Likewise, nearest neighbor should be a little bit different from neural Nets.",
                    "label": 1
                },
                {
                    "sent": "Home safely training on that will generate adverse aerial examples for it, and then we feed those adversary examples to a nearest neighbor classifier.",
                    "label": 0
                },
                {
                    "sent": "It gets about a 25% error rate.",
                    "label": 1
                },
                {
                    "sent": "If we do a different kind of attack from one model to a more similar model, then we expect to see a higher error rate.",
                    "label": 0
                },
                {
                    "sent": "So in this case what we did is we used smooth nearest neighbor where rather than just finding the single nearest neighbor, we take Gaussian weights around each training example and have the whole training set vote with Gaussian weights that's actually differentiable, so you can actually apply the fast gradient sign method to it and generate an attack on it.",
                    "label": 0
                },
                {
                    "sent": "If you then feed that attack into a hardness neighbor classifier and it's a 4247.2 error rate.",
                    "label": 0
                },
                {
                    "sent": "Because we're attacking it with a model that's much more similar to the final target, we've improved the success rate of the attack.",
                    "label": 0
                },
                {
                    "sent": "And this is an example of how you can attack and nondifferentiable model, even when there's no gradient.",
                    "label": 0
                },
                {
                    "sent": "You just you use the cross model generalization property, so that's what earlier Joshua asked a question about using 10 inch, because the gradient goes away at the margins, and I said that's a strategy that I call hide the gradient, so that works for turning off the function that I use, but you can still attack it via cross model generalization.",
                    "label": 1
                },
                {
                    "sent": "If we take a neural net and we actually.",
                    "label": 0
                },
                {
                    "sent": "Some of these are so these are less interesting.",
                    "label": 0
                },
                {
                    "sent": "I actually maybe should have held some these results to later.",
                    "label": 0
                },
                {
                    "sent": "If we take a maximum that we generate generate adversary examples for it, we attack it hyperbolic tangent.",
                    "label": 0
                },
                {
                    "sent": "Note we get a 99.3% error rate, so different kinds of neural Nets actually make very similar mistakes.",
                    "label": 1
                },
                {
                    "sent": "The other thing that's really cool is that now let's and linear classifiers agree on the class, so they make a mistake and it's not just that they both make a mistake, they actually agree on what the wrong class should be.",
                    "label": 0
                },
                {
                    "sent": "So you show it a perturbed three, and they both say that it's a 7, even though it's conceivable that one could say it's a seven, and one could say that it's an 8.",
                    "label": 0
                },
                {
                    "sent": "So for example, we take a max.net and we attack it and re feet it's examples at a softmax regression.",
                    "label": 0
                },
                {
                    "sent": "Softmax regression gets an 88.9% error rate and the two models agree on the class.",
                    "label": 1
                },
                {
                    "sent": "On somebody, 67% of the mistakes, so about 70% of mistakes overall.",
                    "label": 0
                },
                {
                    "sent": "If we transfer adversary examples from Accent net to a shallow RBF model.",
                    "label": 1
                },
                {
                    "sent": "First we get a lower error rate, only 36.8% error rate because the models are so different that the attacks aren't as transferable.",
                    "label": 1
                },
                {
                    "sent": "But also they agree on the class less frequently when they do both, make a mistake.",
                    "label": 0
                },
                {
                    "sent": "Only 40% of the time instead of 70.",
                    "label": 0
                },
                {
                    "sent": "So we don't have the whole story.",
                    "label": 0
                },
                {
                    "sent": "There's obviously something besides how linear or not linear are you that drives exactly how much an attack on one model generalizes to another model?",
                    "label": 0
                },
                {
                    "sent": "And also that determines whether they will get the same class when they make a mistake, but it does seem like the degree to which models are linear gives us a lot of ability to predict how well.",
                    "label": 0
                },
                {
                    "sent": "Those models will be able to attack each other.",
                    "label": 0
                },
                {
                    "sent": "Example.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "That's a future slide, yeah?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 1
                },
                {
                    "sent": "This is, we've seen that adversary examples generalize from one model to another from one training set to another, and and that the degree to which they transfer ressemble resembles the degree to which the models have the same decision function, David.",
                    "label": 0
                },
                {
                    "sent": "Here I'm using the frustrated sign method with epsilon equals .25 the.",
                    "label": 0
                },
                {
                    "sent": "The 1st paper intriguing properties of neural Nets used a different way of generating them and found similar results.",
                    "label": 0
                },
                {
                    "sent": "Except back then we weren't looking at whether they agree on the identity of the class.",
                    "label": 0
                },
                {
                    "sent": "Not on this particular.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's like so many different things I'm measuring that I can try multiple epsilon.",
                    "label": 0
                },
                {
                    "sent": "All of them, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so does the human brain have adversarial examples?",
                    "label": 1
                },
                {
                    "sent": "I don't know if this will work on you.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit scale dependent.",
                    "label": 0
                },
                {
                    "sent": "If you look at this from the right distance, you should see illusory movement.",
                    "label": 0
                },
                {
                    "sent": "There's not actually any movement there.",
                    "label": 0
                },
                {
                    "sent": "And it also probably looks to you like there's a spiral.",
                    "label": 0
                },
                {
                    "sent": "It's not a spiral, it's concentric circles.",
                    "label": 0
                },
                {
                    "sent": "And I also I also often get.",
                    "label": 0
                },
                {
                    "sent": "Did I trace it wrong?",
                    "label": 0
                },
                {
                    "sent": "I probably traced it running today.",
                    "label": 0
                },
                {
                    "sent": "But your brain makes mistakes too.",
                    "label": 0
                },
                {
                    "sent": "We know from the cross model generalization property that your brain makes different mistakes then the neural Nets do.",
                    "label": 0
                },
                {
                    "sent": "When I show you large adversarial examples on amnesty, you don't misclassify them.",
                    "label": 0
                },
                {
                    "sent": "We don't really know for sure what our brain thinks of the image, not ever serve examples because of perturbations.",
                    "label": 0
                },
                {
                    "sent": "There are so small that the monitor can't show them to you.",
                    "label": 0
                },
                {
                    "sent": "But we do know that you're not bothered by the really big ones that we use honest.",
                    "label": 0
                },
                {
                    "sent": "My purpose in showing this slide is to save it.",
                    "label": 0
                },
                {
                    "sent": "Essentially every engineered system has some kind of weakness and if you go hunting for them, you're always going to find one.",
                    "label": 0
                },
                {
                    "sent": "It's probably possible to construct some kind of no free lunch theorem argument saying that if you got rid of all the adversarial examples, you'd be worse at the original task or something like that.",
                    "label": 0
                },
                {
                    "sent": "What we can do is try to understand exactly why some systems make the mistakes they do, and others make different mistakes, and we can also try to understand if there's a way to eliminate the mistakes that are neural Nets make in order to behave more like the human brain where we make this kind of mistake, but not the ones where we respond really strongly to small linear perturbations of the input.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a more compelling but more distressing adversarial example that I can bring up if you're not sold on this one.",
                    "label": 0
                },
                {
                    "sent": "So Elias sets Giver told me he was reading a book about electrophysiology in the 1950s.",
                    "label": 0
                },
                {
                    "sent": "And in the 1950s, they found that if they've hooked up an EG to your brain and measure the frequency of your brain waves and flashes strobe light at you at the same frequency, you'll have a seizure so they can induce epilepsy and healthy people.",
                    "label": 0
                },
                {
                    "sent": "Essentially, just by finding the.",
                    "label": 0
                },
                {
                    "sent": "Time scale that you respond to and then making a signal at the same time scale.",
                    "label": 0
                },
                {
                    "sent": "So we're sort of doing that same kind of attack to these neural Nets, but in the spatial domain we're finding the spatial frequency that the weights respond to, and plugging that in.",
                    "label": 0
                },
                {
                    "sent": "But you can kind of do the same thing to the recurrent net in the human brain by using an EG in the strobe light.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't think we can conclude for sure what optimization algorithm the brain uses.",
                    "label": 0
                },
                {
                    "sent": "I don't think there's enough information about that, but I do think that finding other means of optimizing models can open us up to having more defenses against adversarial examples.",
                    "label": 0
                },
                {
                    "sent": "Kim.",
                    "label": 0
                },
                {
                    "sent": "Completely wrong.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, there's a lot of common logical fallacy's and psychological biases that you can think of as being adversary examples of some kind.",
                    "label": 0
                },
                {
                    "sent": "I will be a slide on that in a few minutes.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there's many different things you could do to try to defend against adversarial examples, and a lot of them have failed.",
                    "label": 0
                },
                {
                    "sent": "And everybody's going to kind of start complaining, but if you did all these extra tweaks on them, I'm not saying that these techniques could never work.",
                    "label": 0
                },
                {
                    "sent": "I'm just saying that a reasonable attempt to do them has been made in most cases by me in some cases by other people, and the first reasonable attempt has not really shown much effect.",
                    "label": 0
                },
                {
                    "sent": "One thing is, ensembles don't really work if you have very very large ensembles, like those defined by dropout.",
                    "label": 0
                },
                {
                    "sent": "They can reduce the actually get the class right.",
                    "label": 0
                },
                {
                    "sent": "If you have small ensembles, like if you train 50 different nuts yourself with different random seeds on different subsets of the data, those do not really work at all because of the cross model transferred property you can generate every server examples against one ensemble member and then feed it to the whole ensemble.",
                    "label": 0
                },
                {
                    "sent": "The whole ensemble will get it wrong.",
                    "label": 0
                },
                {
                    "sent": "You can also back prop through the whole ensemble.",
                    "label": 0
                },
                {
                    "sent": "And generate adversarial example and that one's even harder to get right.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Then somebody asked about what if you shift the adversarial example by pixel, and there's also there's a popular comment on a blog post somewhere saying that they thought adversarial examples go away in the human brain because we make multiple saccades.",
                    "label": 0
                },
                {
                    "sent": "So we tried an experiment where we just have one neural net vote across multiple SIM cards on the same image, and it had no discernible effect on the error rate of the model.",
                    "label": 0
                },
                {
                    "sent": "That happens whether you backdrop through just once a card or through all of them, and if you back up the one the one that needs to be a smaller image that there's a lot of overlap between the SIM cards.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you might miss the object in once a cut.",
                    "label": 0
                },
                {
                    "sent": "Other things that have not really worked are just adding noise to the training process.",
                    "label": 0
                },
                {
                    "sent": "Most noise vectors are going to be orthogonal to the adversarial directions.",
                    "label": 0
                },
                {
                    "sent": "I generative modeling using an unsupervised training criterion.",
                    "label": 0
                },
                {
                    "sent": "I know you was going to complain about this, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh we are not sure even by using the noise as a rejection close.",
                    "label": 0
                },
                {
                    "sent": "I will turn it to add, add an extra class and say recognize the noise is being extra cost.",
                    "label": 0
                },
                {
                    "sent": "Yes I have tried that and I've not gotten it to work.",
                    "label": 0
                },
                {
                    "sent": "I've tried a variety of different things.",
                    "label": 0
                },
                {
                    "sent": "One thing I did was I tried generating like Gaussian noise and then doing one step of fast gradient sign on it and then saying train that this is class N plus one.",
                    "label": 0
                },
                {
                    "sent": "I've also tried telling it to that didn't work.",
                    "label": 0
                },
                {
                    "sent": "Now the there's a big, there's a big RN out there, there's a lot of possible garbage.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I just want everything about the about the extra class option.",
                    "label": 0
                },
                {
                    "sent": "You don't want to just reject adversarial examples.",
                    "label": 0
                },
                {
                    "sent": "I know it better than being fooled by them.",
                    "label": 0
                },
                {
                    "sent": "You do want to reject the fooling images that are very far from the input.",
                    "label": 0
                },
                {
                    "sent": "If you just reject every single examples and that means, so you're trying to filter out spam.",
                    "label": 0
                },
                {
                    "sent": "I'm not.",
                    "label": 0
                },
                {
                    "sent": "I know you understand.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure, I'm just telling the audience that this thought comes into my head.",
                    "label": 0
                },
                {
                    "sent": "So you want to make a spam folder and say your spam folder can tell when there's an adversarial example coming in, and then you just say, well, this is this is garbage.",
                    "label": 0
                },
                {
                    "sent": "I refused to process it.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to classify it.",
                    "label": 0
                },
                {
                    "sent": "That isn't how spammers can shut down your spam detection system, it's it's a little bit better than just outputing the wrong thing all the time, but it's still not really that great from a security point of view.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh implicitly, the fast gradient sign method is doing that, 'cause it's you take the backdrop is going to take a bunch of hidden unit values and then multiply by the transpose of the way it's going into them.",
                    "label": 0
                },
                {
                    "sent": "So you kind of you kind of are getting something that comes out of whether hidden unit values are.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure that's what you meant, but.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah yeah, I'll talk about that in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "There's also, there's a really cool paper that I like a lot from Panasonic Research where they train an autoencoder to turn the adversarial examples back into clean ones, and I thought there was a really cool idea and I spent longer trying to get this one to work than any of the others on here.",
                    "label": 0
                },
                {
                    "sent": "They reported in their own paper that it actually does not work and actually makes the system more vulnerable to adverse aerial examples and.",
                    "label": 0
                },
                {
                    "sent": "The way I started on this whole project was I read that I'm like that can't possibly be right and I started trying to implement their method and make it work.",
                    "label": 0
                },
                {
                    "sent": "My conclusion is the reason it doesn't work is that your sensitivity that adversary examples is based on all these weight matrices being multiplied together, and their autoencoder just adds two more and a set of Jacobian can get bigger.",
                    "label": 0
                },
                {
                    "sent": "There are some differences you can do that's all the other several task, but they make you lose performance on a clean task.",
                    "label": 0
                },
                {
                    "sent": "One thing is you can use weight decay.",
                    "label": 0
                },
                {
                    "sent": "L1 is better than L2.",
                    "label": 0
                },
                {
                    "sent": "But usually if you add enough weight decay to lose confidence in every server, examples you have seriously damaged your ability to classify the clean ones.",
                    "label": 0
                },
                {
                    "sent": "Same thing with double back prop where you regularize the Jacobian of the network to be small if you actually turn up the coefficient on the double backdrop high enough to confer resistance to adversarial examples usually destroy performance in the clean task and finally a deep RBF network is very resistant to adversarial examples.",
                    "label": 1
                },
                {
                    "sent": "But usually has pretty high training error.",
                    "label": 0
                },
                {
                    "sent": "Maybe somebody that's better training them.",
                    "label": 0
                },
                {
                    "sent": "Then we could get it to have low training error, but I haven't had much success with that.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stochastic gradient descent and momentum.",
                    "label": 0
                },
                {
                    "sent": "Yeah, for the most part.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Possibly, yeah, yeah, there we get to look into.",
                    "label": 0
                },
                {
                    "sent": "So when perspective on how the word that adverse aerial training or.",
                    "label": 0
                },
                {
                    "sent": "If I skip to slide by mistake, sorry.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will skip around a little bit 'cause I accidentally put my slides in order, so the Universal Approximator theorem tells us that neural Nets can actually implement this function to arbitrary accuracy, even though they're built.",
                    "label": 0
                },
                {
                    "sent": "Are these layer pieces.",
                    "label": 0
                },
                {
                    "sent": "And that means that to some extent this is all just a problem of designing the right loss function to make them implement that function.",
                    "label": 0
                },
                {
                    "sent": "How it applies to relevance to yeah.",
                    "label": 0
                },
                {
                    "sent": "But and then they might have different efficiency.",
                    "label": 0
                },
                {
                    "sent": "So anyway, you can.",
                    "label": 0
                },
                {
                    "sent": "You can shape the neural net to be relatively constant in the area surrounding a training example, you just, you feed adversarial examples back in, I believe you were asking about that earlier.",
                    "label": 0
                },
                {
                    "sent": "So if we actually train.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On both adversarial and clean examples.",
                    "label": 0
                },
                {
                    "sent": "Then we can cause the network to become resistant to the adversarial examples.",
                    "label": 0
                },
                {
                    "sent": "Here's a learning curve on the left side.",
                    "label": 0
                },
                {
                    "sent": "I'm showing you the error rate on the clean task, like green learning curve is training with adversarial examples.",
                    "label": 0
                },
                {
                    "sent": "The blue is without.",
                    "label": 0
                },
                {
                    "sent": "And without we got down to something like 94.94% error rate, honest and with adversary examples.",
                    "label": 0
                },
                {
                    "sent": "We got down to .78%.",
                    "label": 0
                },
                {
                    "sent": "Since then, there's been a few new papers that did better.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Had.",
                    "label": 0
                },
                {
                    "sent": "Somebody was asking earlier about how sensitive are you to adversarial examples at the start of training?",
                    "label": 0
                },
                {
                    "sent": "It's a little bit hard to really find this meaningful because you get about 90% error rate honest on clean examples, but both adversarial include examples start out at the same place and then all throughout training.",
                    "label": 0
                },
                {
                    "sent": "You're constantly vulnerable to adversarial examples if you don't train on adversarial examples in your input.",
                    "label": 0
                },
                {
                    "sent": "If you turn out ever so examples in the input, then you become resistant to them overtime, but not as resistant as you are not.",
                    "label": 0
                },
                {
                    "sent": "Not not as good at classifying them as you are at classifying clean data.",
                    "label": 0
                },
                {
                    "sent": "Will you take a clean input?",
                    "label": 0
                },
                {
                    "sent": "You find the gradient of the cost with respect to the input, and then you add epsilon times this sign and run that through again.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to skip back a few slides.",
                    "label": 0
                },
                {
                    "sent": "This might be getting out what year?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other fooling image things.",
                    "label": 0
                },
                {
                    "sent": "I've I have not.",
                    "label": 0
                },
                {
                    "sent": "I have not been able to train networks to be reliably resistant to fooling images.",
                    "label": 0
                },
                {
                    "sent": "If you just train them to say this is nothing, then for most classes they'll be OK, but usually they have at least one class where there's still easily fooled into believing that that's present.",
                    "label": 0
                },
                {
                    "sent": "I've not entirely sure why it skews that way.",
                    "label": 0
                },
                {
                    "sent": "So there's several different ways of regularization Delta to output similar things in similar locations.",
                    "label": 0
                },
                {
                    "sent": "One ways you can limit the total amount that can vary.",
                    "label": 0
                },
                {
                    "sent": "That's what things like weight decay.",
                    "label": 0
                },
                {
                    "sent": "Do you just say the parameters are not very big, so it can't really change that much over a small interval.",
                    "label": 0
                },
                {
                    "sent": "And this often causes underfitting.",
                    "label": 0
                },
                {
                    "sent": "If you have a training example, we want a low value here at a high value here.",
                    "label": 0
                },
                {
                    "sent": "You might not be able to hit both of them if you're constrained to be linear and have a very small slope everywhere.",
                    "label": 0
                },
                {
                    "sent": "Another thing you can do is you can have a very high capacity model and you can train it to resist infinitesimal perturbation.",
                    "label": 0
                },
                {
                    "sent": "So this is what things like double backdrop and the CLU do.",
                    "label": 0
                },
                {
                    "sent": "They become very flat surrounding training examples, but then they're not really saying what happens elsewhere, it can also be very hard to end up being as perfectly flat as that criterion would like you to be.",
                    "label": 0
                },
                {
                    "sent": "Adverse aerial training is different because we're trying to get to resist perturbation to finite non infinitesimal perturbations.",
                    "label": 0
                },
                {
                    "sent": "But we're also saying we don't really care what the slope is, we just want the actual value to not change all that much.",
                    "label": 0
                },
                {
                    "sent": "So you're going to have really crazy slope and make a regular function that goes up and down a lot near your training example, as long as it wiggles continuously in the same general height.",
                    "label": 0
                },
                {
                    "sent": "So it up with with bumpy but wide flat regions surrounding each training point question in the back.",
                    "label": 0
                },
                {
                    "sent": "I've tried several different techniques.",
                    "label": 0
                },
                {
                    "sent": "One is like you were suggesting using sigmoids for each class and started so just like every segment should be off, I've tried adding an extra class and saying this is garbage.",
                    "label": 0
                },
                {
                    "sent": "Recognize this as being garbage.",
                    "label": 0
                },
                {
                    "sent": "I've tried using only the original classes and telling it that on the garbage inputs it should try to output uniform over all the original classes.",
                    "label": 0
                },
                {
                    "sent": "I haven't had any luck with all three of those approaches.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One difference that hasn't worked very well is generative modeling, and I think there's there's actually like a an underlying conceptual reason why generative modeling isn't going to fix the problem.",
                    "label": 0
                },
                {
                    "sent": "The reason is that classification is all about the posterior distribution over Y.",
                    "label": 0
                },
                {
                    "sent": "Given X and generative modeling is shaping your marginal distribution over X.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose we have two different mixture models, with each mixture corresponding to one class.",
                    "label": 0
                },
                {
                    "sent": "If we just use Gaussian for each class and we use the same covariance matrix for both Gaussians but give them different means.",
                    "label": 0
                },
                {
                    "sent": "Then we can get a marginal distribution that looks like this with two little Gaussian bumps.",
                    "label": 0
                },
                {
                    "sent": "And the posterior is a sigmoid distribution.",
                    "label": 0
                },
                {
                    "sent": "Where the weights of the sigmoid come from, the means of the original mixture distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is bad when you go very far from where the data lies, you make extremely confident predictions.",
                    "label": 0
                },
                {
                    "sent": "This is basically what I spent the whole lecture saying we don't want to do and this comes out of having a A2 class mixture model with a specific parameterization of P of X given Y.",
                    "label": 0
                },
                {
                    "sent": "That's only rejection, though.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Here we have another model that implements almost exactly the same P of X.",
                    "label": 0
                },
                {
                    "sent": "The differences in this one P of X given Y is different rather than being a single Gaussian, each of the class models has a mixture of two Gaussians, one with a very high probability, high prior probability of being sampled from the other, one with probability epsilon of being sampled from.",
                    "label": 0
                },
                {
                    "sent": "So with probability epsilon we generate something from a garbage class or like a garbage version of the class.",
                    "label": 0
                },
                {
                    "sent": "Something that hasn't been seen before in the training set.",
                    "label": 0
                },
                {
                    "sent": "So if we've trained on 1000 examples we say, well maybe with probability 1000 out of 1002 we generate something that looks like the training data and with probability 2 out of 1002 we generate something that looks like this unseen thing we've ever seen before, so that second unseen component is just the Gaussian with very wide variance.",
                    "label": 0
                },
                {
                    "sent": "We add that extra second component.",
                    "label": 0
                },
                {
                    "sent": "It completely changes the posterior looks.",
                    "label": 0
                },
                {
                    "sent": "The posteriors now says.",
                    "label": 0
                },
                {
                    "sent": "Over here on the far end, we don't really know what the input is over here in the far end.",
                    "label": 0
                },
                {
                    "sent": "We also don't really know what the input is, but near the training data was we're confident these training zeros or zeroes in these training ones or ones.",
                    "label": 0
                },
                {
                    "sent": "But both of these models, despite having radically different behavior in the posterior, implement almost exactly the same marginal distribution.",
                    "label": 0
                },
                {
                    "sent": "So just using a using a generative cost to train your probabilistic model is not going to make it perform correct posterior inference.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "You could get the same thing by fitting if you actually knew why you could actually get the same thing by fitting both of them.",
                    "label": 0
                },
                {
                    "sent": "This model would actually be scored worse.",
                    "label": 0
                },
                {
                    "sent": "But The thing is, this model is adding this extra noise component because I know that that extra noise component helps you.",
                    "label": 0
                },
                {
                    "sent": "There's nothing in the data that tells you that it should be there.",
                    "label": 0
                },
                {
                    "sent": "It's like a problem of maximum likelihood and just just doing maximum likelihood on X is as well as why isn't going to remove that problem with maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So you can you can integrate a fix into a generative model, but the fix is not being generative.",
                    "label": 0
                },
                {
                    "sent": "And by the way.",
                    "label": 0
                },
                {
                    "sent": "This isn't adding new values of why this is adding new X values that haven't been seen before associated with both wise.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit different than adding a garbage class.",
                    "label": 0
                },
                {
                    "sent": "If you added a garbage class, your posterior would end up saying out here.",
                    "label": 0
                },
                {
                    "sent": "I'm confident that it's neither of the first 2 classes.",
                    "label": 0
                },
                {
                    "sent": "This setup is saying out here.",
                    "label": 0
                },
                {
                    "sent": "I have no confidence about what class it is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I would be happy with that, but I haven't been able to make it happen reliably.",
                    "label": 0
                },
                {
                    "sent": "There's there's just too much too many directions that you can screw it up.",
                    "label": 0
                },
                {
                    "sent": "And just training on.",
                    "label": 0
                },
                {
                    "sent": "Just training on synthetically generated fooling examples doesn't seem to be enough to make it learn an exhaustive garbage class, yeah?",
                    "label": 0
                },
                {
                    "sent": "Alright, I'd like if you can get it to work.",
                    "label": 0
                },
                {
                    "sent": "I agree that that's an acceptable solution.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood with the garbage class isn't really working either, so the problem is maximum likelihood with garbage class is only going to raise the probability of the garbage class on examples that it's seen.",
                    "label": 0
                },
                {
                    "sent": "This is more of like incorporating a prior than changing the way that maximum likelihood proceeds from the data.",
                    "label": 0
                },
                {
                    "sent": "Far away.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, I'd probably better get moving.",
                    "label": 0
                },
                {
                    "sent": "We're running a little bit low on time, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So training another server examples can improve your error rate quite a lot on amnestic goes from close to 100% to down to about 3% on my best model right now.",
                    "label": 0
                },
                {
                    "sent": "But there's still a lot of weaknesses.",
                    "label": 0
                },
                {
                    "sent": "Here we take a truck and we start transforming it into the different classes.",
                    "label": 0
                },
                {
                    "sent": "There's one I kind of like that's the bird class right there.",
                    "label": 0
                },
                {
                    "sent": "It actually does kind of look like a bird, but I didn't really do anything that looked very reasonable for any of the others.",
                    "label": 0
                },
                {
                    "sent": "And you can also see that this cat here we transform it all the other cats or all the other classes, and it at least quits looking like a cat now before it used to just not change.",
                    "label": 0
                },
                {
                    "sent": "Now it now it turns into some kind of blob, but it doesn't look like the other classes.",
                    "label": 0
                },
                {
                    "sent": "So there's a new a new way of doing adversarial training that was recently released and.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To motivate this, let's look at what happens when we do an adversarial perturbation.",
                    "label": 0
                },
                {
                    "sent": "An adversarially trained network.",
                    "label": 0
                },
                {
                    "sent": "Or so this is actually the original network.",
                    "label": 0
                },
                {
                    "sent": "So we haven't used any adversarial training we can feed in this clean amnist one and get this class response and we can feed in this adversarial.",
                    "label": 0
                },
                {
                    "sent": "I'm just one and get this response an the biggest problem here is that we used to have a big peak around the one class.",
                    "label": 0
                },
                {
                    "sent": "Each of these bars is showing the unnormalized log probability of a different class, so there is zero.",
                    "label": 0
                },
                {
                    "sent": "There's one and it's pretty large.",
                    "label": 0
                },
                {
                    "sent": "After the perturbation, we're now getting the seven class being really high.",
                    "label": 0
                },
                {
                    "sent": "So adversary training tries to address specifically getting the right class.",
                    "label": 0
                },
                {
                    "sent": "After we've done every single training.",
                    "label": 0
                },
                {
                    "sent": "The clean example has a big peak at one, and the adversarial example is a big peak at one.",
                    "label": 0
                },
                {
                    "sent": "But there's still a problem here, which is that the other class values have changed quite a bit.",
                    "label": 0
                },
                {
                    "sent": "Like for example, we think it's much less likely to be a four then we used to think it was, and we also think it's much more likely to be a sudden than it was before.",
                    "label": 0
                },
                {
                    "sent": "After you apply the softmax, you can barely see these changes anymore, but we know that these kinds of changes actually mean that something has gone wrong with the internal representation of the model.",
                    "label": 0
                },
                {
                    "sent": "If you saw Jeff Hinton's dark knowledge paper recently, he showed how these kinds of distributions over the wrong classes actually tell you a lot about what the network thinks different things resemble, yeah?",
                    "label": 0
                },
                {
                    "sent": "I don't, I don't see it, but maybe you are more robust classifier than me.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, but this net has been explicitly trained that whenever there's a light Gray pixel like that that it should ignore it.",
                    "label": 0
                },
                {
                    "sent": "So if it had trained well, it should.",
                    "label": 0
                },
                {
                    "sent": "It should know to disregard those things.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Sorry tycoon, Miotto and his collaborators took this observation that the distribution as a whole is getting shifted quite a lot, and they came up with a new training procedure.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, you penalize the KL divergent's between the clean point and the modified point.",
                    "label": 0
                },
                {
                    "sent": "And the really cool thing about this is it doesn't rely on the cost function anymore.",
                    "label": 0
                },
                {
                    "sent": "The training procedure that I wrote about in my paper was based on the cost function.",
                    "label": 0
                },
                {
                    "sent": "Given the inputs X, the class labels Y and the model parameters.",
                    "label": 0
                },
                {
                    "sent": "This only needs the inputs X and the model parameters.",
                    "label": 0
                },
                {
                    "sent": "It doesn't need why it just says R for some input point X, which we know is data perturbations near that data point shouldn't change the distribution over the class very much.",
                    "label": 0
                },
                {
                    "sent": "The really cool thing is you can now use this for semi supervised learning if you have a large data set of unlabeled examples, you can just regularize the model to be smooth in their vicinity.",
                    "label": 0
                },
                {
                    "sent": "At the time of this paper is published, the state of the Art, an MNIST with only 100 labeled examples, was three point 3% error using a variational autoencoder type model using virtual adversarial examples.",
                    "label": 0
                },
                {
                    "sent": "They reduced the error rate to 2.12%.",
                    "label": 0
                },
                {
                    "sent": "Since then, that's been beaten, but also using something that uses unsupervised learning, so this result is very interesting because it means a lot of the regularization that we see from training with unsupervised learning is.",
                    "label": 0
                },
                {
                    "sent": "Not performing a whole lot better than what you can get with smoothness alone.",
                    "label": 0
                },
                {
                    "sent": "This technique is really shown us the power of smoothness.",
                    "label": 0
                },
                {
                    "sent": "It may not be completely state of the art, but it can get you very far.",
                    "label": 0
                },
                {
                    "sent": "Yes, let's move this around the data, yeah?",
                    "label": 0
                },
                {
                    "sent": "So I had a slide about that earlier.",
                    "label": 0
                },
                {
                    "sent": "I'll revisit it quickly.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Never mind, the latency is so bad on this.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If you said is Kirby is very flat, your current point, you're only resisting an infinitesimal perturbation.",
                    "label": 0
                },
                {
                    "sent": "The Cal divergences orthogonal to this issue.",
                    "label": 0
                },
                {
                    "sent": "What I'm saying I'm saying matters is.",
                    "label": 0
                },
                {
                    "sent": "You can measure the Jacobian of the KL divergent's.",
                    "label": 0
                },
                {
                    "sent": "I actually just got busy because you had a local minimum of it.",
                    "label": 0
                },
                {
                    "sent": "You could you could measure like the eigenvalues of the Hessian of the KL divergent's and say that they should be very flat and that would say.",
                    "label": 0
                },
                {
                    "sent": "When I make an infinitesimal perturbation, the KL divergent should not change very much.",
                    "label": 0
                },
                {
                    "sent": "Adversarial training is saying when I make a perturbation of L2 norm 7 and Max norm .25, the KL divergent should not change very much.",
                    "label": 0
                },
                {
                    "sent": "So it's it's actually feeding a new input into the model.",
                    "label": 0
                },
                {
                    "sent": "It's not, it's not just analytically analyze grounds where feeding the new input in is important, because it allows you to use Universal approximator theorem type properties to resist the change.",
                    "label": 0
                },
                {
                    "sent": "You can have totally different sets of units saturated and not saturating at that distant point.",
                    "label": 0
                },
                {
                    "sent": "When you regularize the Jacobian, all you can really do innarelli net is make the weight smaller.",
                    "label": 0
                },
                {
                    "sent": "If you have a sigmoid net you can.",
                    "label": 0
                },
                {
                    "sent": "You can try to push things to wonder the sigmoid saturate, but forever that it's not really a very useful property.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm pretty near the end.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My time, so we skip ahead a little bit.",
                    "label": 0
                },
                {
                    "sent": "One thing I'd like to strongly recommend is that you benchmark adverse aerial resistance of your own models that you're studying, and the recommended way that I suggest you do this is with the fast gradient sign method.",
                    "label": 0
                },
                {
                    "sent": "To do that, what you should report in your paper is it's getting kind of cut off much way.",
                    "label": 0
                },
                {
                    "sent": "You choose an epsilon value and you then compute the fast gradient sign adversary examples using that epsilon value and you report the error rate on a test set that has been perturbed in that way.",
                    "label": 0
                },
                {
                    "sent": "You also report the value of epsilon so other people can reproduce the results and make a comperable.",
                    "label": 1
                },
                {
                    "sent": "Comparable benchmark to report on.",
                    "label": 1
                },
                {
                    "sent": "A lot of the time we see these papers where people say I have a new method of regularization and regularising neural Nets is important because there are adversarial examples, but then they don't test if they have any effect on the ripples in this kind of thing is very easy to implement in Fano or even in torch.",
                    "label": 0
                },
                {
                    "sent": "You just try to call back Prop, take the sign of the back prop with respect to the input, and then add it to the original example.",
                    "label": 0
                },
                {
                    "sent": "There's another benchmark that's out there that you can use.",
                    "label": 0
                },
                {
                    "sent": "It's the one of our first paper.",
                    "label": 0
                },
                {
                    "sent": "In that case you use an optimizer to search for the smallest perturbation that causes a mistake, and you report the average size of that perturbation.",
                    "label": 0
                },
                {
                    "sent": "I recommend not using that because it's very hard to make it optimizer that's reproducible enough that other people can do exactly the same thing you did.",
                    "label": 0
                },
                {
                    "sent": "And also because sometimes the smallest mistake might actually correspond to flipping the class for real.",
                    "label": 0
                },
                {
                    "sent": "So it might not really be a mistake.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Likewise, if you want to benchmark your resistance to fooling images or rubbish class examples, I would suggest drawing samples or a Gaussian that you fit to the training inputs and then doing one step of the gradient sign method on that.",
                    "label": 0
                },
                {
                    "sent": "Then just report the error rate at which your model thought something was present and report the error rate at which you were able to make it believe in one specific class.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in conclusion, many modern machine learning algorithms get the right answer on the training data or the test data, but they get it for the wrong reason and that causes them to make the wrong output on many unusual inputs.",
                    "label": 0
                },
                {
                    "sent": "Deep learning is on the right course to overcome adversary examples.",
                    "label": 0
                },
                {
                    "sent": "The Universal Approximator theorem says that we can definitely represent the right kind of function, provided that we make the network big enough and not every machine learning model has that advantage on its side.",
                    "label": 0
                },
                {
                    "sent": "And if you think that you've come up with a way of improving resistance to adversarial examples, you should benchmark it and report your error rate under a reproducible class of adversary examples.",
                    "label": 0
                },
                {
                    "sent": "If there's any questions left, I'm I'm done.",
                    "label": 0
                }
            ]
        }
    }
}