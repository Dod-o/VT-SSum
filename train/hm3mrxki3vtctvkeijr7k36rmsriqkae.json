{
    "id": "hm3mrxki3vtctvkeijr7k36rmsriqkae",
    "title": "Online Classification with Specificity Constraints",
    "info": {
        "author": [
            "Andrey Bernstein, Technion - Israel Institute of Technology"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_bernstein_ocs/",
    "segmentation": [
        [
            "So consider a binary classification problem where there are several metrics to measure the performance of classifiers in this setting and."
        ],
        [
            "The two standard are true positive rate, which is a fraction of positives correctly cross classified and that false positive rate, which is a fraction of negatives or incorrectly classified.",
            "So a standard or common.",
            "Usage of these metrics that we picked them on Arosi curve, which has its false positive rate on the X axis and true positive rate on the Y axis.",
            "And for example, in this example we have 5 classifiers and D is a perfect classifier which has true positive rate of 1 and false positive rate of 0.",
            "And we can compare NB and say that a is more conservative than B in the sense that it makes positive classifications only with strong evidence, which also results in bed or some true positive rate that is less than that of B&C is a random classifier which has the same true positive and false positive rate, while E here is worse than random, it has some incorrect info about the class.",
            "So a typical goal in designing a classifier is to maximize true positive rate under false positive rate constraint.",
            "And in this paper we"
        ],
        [
            "You're dealing with online classification problem, where is no training set is given in advance.",
            "So instead we're given M. We're given M classifiers, which you can think of them as of M experts, which at each stage North they produce the label of the input.",
            "Or more generally, they produce a probability.",
            "Of the input to belong to the positive class.",
            "So in an online unifying classification algorithm, combines the output of these classifiers in order to attain a certain goal.",
            "In fact, it produces a convex combination of the given EM classifiers."
        ],
        [
            "So our goal here is was to design an no regret unifying algorithm, which which has the following two properties.",
            "First is then essentially the no regret property leads average true positive rate is not worse than that of the best convex combination of the given M classifiers.",
            "In hindsight and the second property is that its average false positive rate is under some given threshold gamma and this should hold a synthetically for any possible sequence of.",
            "Classifiers, outputs and input labels.",
            "So in fact this is a special case of regret minimization problem with constraints because of the second property."
        ],
        [
            "So what we do in this paper we did formulate the online classification problem as a special case of the regret minimization problem with constraints, and in fact we know some results about this problem.",
            "We know that this strict goal is not attainable in the sense that there are some sequences that violate one of these properties, so it cannot be obtained for every possible sequence of classifiers and labels.",
            "So some relaxation relaxed goal is needed, and we propose the relaxed goal that leads.",
            "To a computationally efficient online unified algorithm, which is, to the best of our knowledge, is the first polynomial algorithm for the regret minimization problem with constraints.",
            "So for the details of the algorithm and some simulation results, you're invited to come to our poster, which is located at W. 97.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So consider a binary classification problem where there are several metrics to measure the performance of classifiers in this setting and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The two standard are true positive rate, which is a fraction of positives correctly cross classified and that false positive rate, which is a fraction of negatives or incorrectly classified.",
                    "label": 0
                },
                {
                    "sent": "So a standard or common.",
                    "label": 0
                },
                {
                    "sent": "Usage of these metrics that we picked them on Arosi curve, which has its false positive rate on the X axis and true positive rate on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "And for example, in this example we have 5 classifiers and D is a perfect classifier which has true positive rate of 1 and false positive rate of 0.",
                    "label": 0
                },
                {
                    "sent": "And we can compare NB and say that a is more conservative than B in the sense that it makes positive classifications only with strong evidence, which also results in bed or some true positive rate that is less than that of B&C is a random classifier which has the same true positive and false positive rate, while E here is worse than random, it has some incorrect info about the class.",
                    "label": 1
                },
                {
                    "sent": "So a typical goal in designing a classifier is to maximize true positive rate under false positive rate constraint.",
                    "label": 0
                },
                {
                    "sent": "And in this paper we",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You're dealing with online classification problem, where is no training set is given in advance.",
                    "label": 1
                },
                {
                    "sent": "So instead we're given M. We're given M classifiers, which you can think of them as of M experts, which at each stage North they produce the label of the input.",
                    "label": 0
                },
                {
                    "sent": "Or more generally, they produce a probability.",
                    "label": 1
                },
                {
                    "sent": "Of the input to belong to the positive class.",
                    "label": 0
                },
                {
                    "sent": "So in an online unifying classification algorithm, combines the output of these classifiers in order to attain a certain goal.",
                    "label": 0
                },
                {
                    "sent": "In fact, it produces a convex combination of the given EM classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our goal here is was to design an no regret unifying algorithm, which which has the following two properties.",
                    "label": 0
                },
                {
                    "sent": "First is then essentially the no regret property leads average true positive rate is not worse than that of the best convex combination of the given M classifiers.",
                    "label": 1
                },
                {
                    "sent": "In hindsight and the second property is that its average false positive rate is under some given threshold gamma and this should hold a synthetically for any possible sequence of.",
                    "label": 1
                },
                {
                    "sent": "Classifiers, outputs and input labels.",
                    "label": 1
                },
                {
                    "sent": "So in fact this is a special case of regret minimization problem with constraints because of the second property.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we do in this paper we did formulate the online classification problem as a special case of the regret minimization problem with constraints, and in fact we know some results about this problem.",
                    "label": 1
                },
                {
                    "sent": "We know that this strict goal is not attainable in the sense that there are some sequences that violate one of these properties, so it cannot be obtained for every possible sequence of classifiers and labels.",
                    "label": 1
                },
                {
                    "sent": "So some relaxation relaxed goal is needed, and we propose the relaxed goal that leads.",
                    "label": 0
                },
                {
                    "sent": "To a computationally efficient online unified algorithm, which is, to the best of our knowledge, is the first polynomial algorithm for the regret minimization problem with constraints.",
                    "label": 1
                },
                {
                    "sent": "So for the details of the algorithm and some simulation results, you're invited to come to our poster, which is located at W. 97.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}