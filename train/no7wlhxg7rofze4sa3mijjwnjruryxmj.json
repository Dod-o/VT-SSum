{
    "id": "no7wlhxg7rofze4sa3mijjwnjruryxmj",
    "title": "Getting at the Semantics of Texts",
    "info": {
        "author": [
            "Hans Uszkoreit, German Research Center for Artificial Intelligence (DFKI)"
        ],
        "published": "Nov. 24, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/estc08_uszkoreit_gst/",
    "segmentation": [
        [
            "It's now my pleasure to introduce the first keynote speaker of the conference, hence Oscar rights, Hanses scientific director at the German Research Center for Artificial Intelligence, known to many of you as DFKI.",
            "Of course, he's also head of the DFKI Language Technology Lab, and he's a professor of computational linguistics at silent University, and he's had a very distinguished career in the area of computational linguistics.",
            "Given the vast amount of texts out there that needs to be semantically analyzed, computational linguistics clearly has a key role to play in in semantic technologies, and Hans is going to talk to us about getting at the semantics of texts, so without any further delay, I'd like to hand over to Hans and look forward to his presentation.",
            "Thank you very much for inviting me to this conference.",
            "I last year already.",
            "I was tempted to come and then had something else.",
            "And this time I was tempted to stay.",
            "But tomorrow we will have their 20th anniversary of DFKI and unfortunately I have to be there, so otherwise I would have loved to stay here.",
            "So in the end, although I was planning to come here for the whole duration of the conference, I had to.",
            "Quiz it in.",
            "Becausw yesterday I was at a conference completely different.",
            "I did not expect semantic technologies to play a role there because it was a future planning conference of Aliens Group.",
            "So the largest German insurance company but.",
            "Actually, what we discussed over large parts of the day was semantic modeling of customer processes and customer needs so that shows right from the practice here from the from the commercial applicants of the technology.",
            "Now here to the borderline of research and application where door meets and then tomorrow.",
            "I think we can sell it."
        ],
        [
            "Great our 20th anniversary so I will start by looking back but not spend too much time on the past and then talk about the extraction of relation instances among them events, opinions, other types of relations.",
            "Then come into the field that I am actually from, namely deep semantic and syntactic processing, computational linguistics.",
            "But I'll make this rather.",
            "Digestible I hope and then talk about hybrid processing models that we apply these days and start then after looking at the past and the beginning into the."
        ],
        [
            "Looking into the future.",
            "So tomorrow we have this 20 year anniversary of DFKI and we still get research money because we can still claim that there is no artificial intelligent yet we need another 20 years.",
            "Yeah, just 20 years so but is there AI or isn't there there are there?",
            "Is there semantic technology in practice or isn't there?",
            "Is it really true?",
            "Semantic?",
            "So the same we ask for AI, so today I cannot simulate a four year old child.",
            "We cannot.",
            "Not even a 3 year old.",
            "Not even a 2 year old but.",
            "On the other hand, software has been built that beats the world champion in chess."
        ],
        [
            "So there's some products on going on coming to human language technology, also called HLT.",
            "No HLT program today can read a given three word sentence in a regretful tone.",
            "Let alone discovering that it should be read in a regretful tone by the contents.",
            "But we can build HLT programs that can read texts in 10 languages without major pronunciation errors and without a foreign accent.",
            "No human being can do that.",
            "Can read 10 in 10 languages without."
        ],
        [
            "Nixon HLT cannot understand the full meaning of any one sentence in the Charter of Human Rights or any other interesting text you may have.",
            "But each OT can find a single mentioning of some relevant event in millions of sentences within seconds.",
            "Again, no human being can do that, so we've come more to a redefinition of our task, and that applies very well to semantic technologies as well.",
            "We are not trying to take away this type of work that humans are good, and being creative really having human intelligence, but we try to extend their capabilities.",
            "We try to extend their cognitions.",
            "By modeling certain aspects of human cognition and magnifying them, yeah, this is, I think what we're going to do and.",
            "I will start with the last sentence here.",
            "How can we find a single relevant mentioning of some relevant event in millions of?"
        ],
        [
            "Distances within seconds.",
            "So in general we may see before I really start the talk that our success may not be sweeping.",
            "But it's creeping."
        ],
        [
            "Creeping in everywhere.",
            "So there are quite a number of types of information extraction, as most of you know, we can extract from text topics, terms named entities, binary relations, enoree relations and then certain sub.",
            "Parts subsets of Enoree relations are events.",
            "Anchored in time answers, maybe to a relational question.",
            "Opinions.",
            "That's also relation between the opinion Holder and something.",
            "The opinion is about and center."
        ],
        [
            "So, and all of these are types, the red ones are."
        ],
        [
            "Types of relation extraction.",
            "Now when you work on relation extraction there are cheap methods and expensive methods and the cheap methods they get you a certain distance and I don't want to say anything bad about them because they are used every day and you can squeeze something out, some juice out of it, but when it really comes to getting semantics in a reliable way out of larger texts are getting at the semantics.",
            "All the cheap methods in the end fail.",
            "But even if we do not analyze every phrase or sentence, even if we concentrate on the ones where we think that the juices in we need some grammar or some equivalent classifier complex classifier to detect the relevant relations and context.",
            "So here we face the problem of whether you call it grammar or not.",
            "There are called classifier.",
            "We we need to acquire this here we need to acquire this the same way."
        ],
        [
            "We do it in language parsing.",
            "Now the main methods for writing such grammars and all of them are actually used in the field or.",
            "The old one introspective intellectual development.",
            "The next one corpus based intellectual development.",
            "The first one is just the grammar writer.",
            "Things what are ways of expressing that and try to write for everyone that comes to mind, right?",
            "A little grammar.",
            "The second one is go through large amounts of corpora.",
            "Huge amounts of texts and find all the patterns.",
            "Mark them by hand and then write rules for all of them.",
            "Or you may try supervised learning from annotated data.",
            "That's very much used in the field now, so you first annotate in the corpora.",
            "All the cases you say exactly which roles, which relations, and then you try to train classifiers on that.",
            "Or you do something I want to talk about today.",
            "Do minimally supervised learning from a few examples.",
            "Or you try completely unsupervised learning, which also I do not want to say very bad things about.",
            "On the other hand, completely unsupervised learning when it really comes to more complex semantic tasks is far from being solved and without giving it additional types of data it may never be solved completely unsupervised without additional types of data but minimally supervised learning I want to."
        ],
        [
            "Talk about today.",
            "So if you compare these methods now, you will see the first one is very expensive and incomplete, the other one is expensive and slow.",
            "Supervised learning from annotated data, still very expensive.",
            "They need to annotate all these data and then I try to show you that you get promising results out of minimally supervised learning.",
            "If you combine this method with linguistic analysis and not feasible for most tasks, I would still claim unless you show me the opposite.",
            "Is the completely unsupervised learning, at least for the more complex?"
        ],
        [
            "Yes, that I'm talking about, so the minimalize minimals least supervised learning has of course along history, and actually the type of work, the type of minimally supervised learning.",
            "I will say something about.",
            "What are the most interesting paper in this direction?",
            "Actually, maybe not the very first suggestion, but the most interesting paper that started this is by Sergey Brin.",
            "You know the name right today is Co.",
            "Owner of Google, and so why didn't he supervise and got an academic career in unsupervised learning or minimally supervised learning instead?",
            "So basically his results were actually not that good.",
            "The paper was brilliant and intriguing, but since he didn't have the same.",
            "Analysis methods that we are applying now.",
            "He could only see which words were next to each other.",
            "It didn't bear the fruit that he was hoping for.",
            "But then many other people from the field from the field of language processing tried their hands on the on similar methods, and here I list a long list of people without going into what in what each of them did there.",
            "Basically, the story is the following.",
            "You start and getting more and more complex.",
            "First people look for words that are next to each other.",
            "Then they look for words that are a little bit apart from each other.",
            "Then they look for phrases that are next to each other.",
            "Then for phrases that are a little bit apart from each other.",
            "Then they try to find out whether these phrases fill certain thematic roles.",
            "May be the subject of a sentence or object of a verb, and what the verb is so they look at pairs of phrases and their functions of phrases in the sentence and in the end, what I'm driving at is you analyze the sentence and then you get it all, and then you from this pattern of the completely analyzed sentence.",
            "Then you try to learn the relevant pattern and actually this in the end, not surprisingly, brings the best results.",
            "If and this is the big if you get an analysis method that gets you with a sufficient coverage and precision, the analysis of all these sentences.",
            "But I'll talk about this."
        ],
        [
            "So there are.",
            "Quite a number of approaches to seat construction by bootstrapping.",
            "The most well known that some of you may also notice the X disco system by young Garber already back in 2001.",
            "But in this case he is really following very, very special patterns.",
            "So that comes close to the full analysis, but doesn't quite get there.",
            "So he takes these triples of subject and verbs and objects and tries to get get along by these events.",
            "Now it turns out in the end, and that for a couple of relations that happened to be expressed.",
            "In this pattern you get a certain distance, but the majority of relations are not.",
            "So if you take for instance who invented what year you want to get the simple relation, who invented what, then very often the the mentionings we call them dimension in the text are in noun phrases for instance so and so the inventor, the 1981 inventor of blah blah.",
            "So the writers don't follow now the type of pattern that we think they should.",
            "Yeah, so they write these things and.",
            "Completely different ways.",
            "They're very creative.",
            "They use many different patterns, so that's why we were then driven to try to get to improve these results by."
        ],
        [
            "Getting full analysis.",
            "So we try to see driven and bottom of rule learning and a bootstrapping fashion so.",
            "We first discover some patterns.",
            "We do this bottom up and compositional.",
            "I'll show you some examples and then use a compression method for clustering and generalizing this rules, because otherwise you get too many rules.",
            "But we only considered subtrees containing arguments and pattern candidates.",
            "Arguments of the relations of the seeds.",
            "So we kind of cut out those parts of a tree that contain the elements we are interested in.",
            "The arguments of the relation.",
            "And the relation market could be a verb, could be an adjective and then try to generalize over many instances in order to get the inductive part the other generalize."
        ],
        [
            "Mission part.",
            "So it's C driven and bottom of rule learning and what we also do is we calculate the domain relevance by the terms that are there and the trustworthiness of the origin.",
            "So when we learn new rules from old patterns, then we look how, how much confidence did we have in the old one and that is inherited by the new ones and so on also.",
            "And the further it gets away from from something trustworthy, the less trustworthy."
        ],
        [
            "Get.",
            "So the novel thing is this compositional representation method, so I won't go through the subparts here."
        ],
        [
            "But give you some examples, then you see it very easy so.",
            "1.",
            "Domain that you may think is crazy for research.",
            "For information extraction, we worked first with was the Noble price domain.",
            "Why's that crazy?",
            "I mean, why would you ever want to extract Noble price winners?",
            "There's a whole table of them so you can get the whole thing.",
            "You populate the database by the cheapest rapper ever written now, so why should you do it?",
            "No, I mean you want to do it right for two reasons.",
            "One reason is exactly that you get this table.",
            "Because you first want to train a method for which you know you know the total.",
            "Recall that you could achieve and the second reason is and I'll show you later you want to get something that has a high redundancy in reporting where you get many, many reports, so you get a fair chance of finding many different patterns.",
            "And the third reason actually is, and I can also show you how we how we exploited.",
            "The third reason is that you may learn you may be able to show that from a very benign domain where you get lots of redundancy in the text.",
            "You can extract rules that you can then apply to less known prices where you don't get the full lists and actually all three of these things I will show now.",
            "So the other thing where that we try it and we tried it.",
            "Only because we wanted to get a paper accepted at the most ambitious young conference and language processing, yeah, so we try it also with the domain of which we knew before hand that it wouldn't work very well, but you need to apply.",
            "You need to follow certain evaluation schemes, otherwise you don't get it accepted.",
            "And in order to get an ACL paper out of that, we had to do that too, although we knew it wouldn't really work very well.",
            "But I show you.",
            "But then we could prove why it wouldn't work, and this is some eye opener for us, and I hope that you will find it.",
            "My."
        ],
        [
            "Really interesting yourself.",
            "So the Noble price target relations are pretty simple.",
            "You gotta recipient the prize, an area and a year and now what we try to do with with as little possible seeds and the seats are really just mean the tuple of the name of Noble peace in the year.",
            "So not not more.",
            "We want to give only the seat to a system, not more and everything else by magic happens by itself.",
            "And actually we tried it with one seed with five seats with 10 seeds.",
            "But actually, I mean we were surprised one seat already gives you.",
            "Vast amount of frequency, so and then you may get a sentence.",
            "Mohamed El Baradei one, the 2005 Nobel Peace Prize on Friday for his efforts to limit the spread."
        ],
        [
            "Atomic weapons so.",
            "Then we take two language processing tools.",
            "One is the tool, sprout.",
            "It's a named entity and morphology Analysis tool that we have built ourselves for about 11 languages.",
            "And then for this sentence analysis we in the beginning employ mini part by Dick and Lynn.",
            "Later we other process as well, but let's start with it and it's why did we take this Spicer?",
            "Because it's very robust.",
            "It gives you a price for everything.",
            "The parts may be wrong, but it gives you a price so it gives you it's very robust.",
            "Always gives you something and it's pretty good.",
            "Not the best."
        ],
        [
            "But one of the best.",
            "So and then we want to learn rules of that soil now.",
            "Don't try not to get too deep into the rules here.",
            "All you can see that in this attribute value matrix you get a rule name and you get a head so called head as we say in computational linguistics, which in this case is the noun and this is the prize.",
            "And then you may have daughters the year the prize, the area and so on."
        ],
        [
            "It's one rule that's a more difficult rule that has a verb in win, an active verb, and maybe a person in here and the lexical form and so on, but doesn't matter.",
            "I mean this is not computational linguistics, they have to deal with or we have to deal with it.",
            "That's our task to do.",
            "This is the type of grammatical representation that we use very often.",
            "So at first glance it looks a little bit like any kind of type definition or so, and it is a type definition, but offer little different has a little different semantics.",
            "From what used in knowledge technologies?"
        ],
        [
            "So those are the real components.",
            "Also it's an AVM format containing ahead daughters a rule and then down here the output and the output is actually the tuple.",
            "Now, one thing that we did from the beginning on that was different from what other people did before us.",
            "We not only looked at the at the full error enoree relation instances, but also at projections of the relation attornery and even at binary, and then try to find out whether we could also get something out of."
        ],
        [
            "Lowe's, and actually I'll show you later in a table that it really helps to do this.",
            "It really helps to do that.",
            "Also, it helps you to transport later the learned rules to other domains because you may abstract away from one of these, maybe from Noble price and apply it to other prices so the next is a pattern extraction step and that does exactly what I mentioned earlier.",
            "You identify these subtrees.",
            "Assume that this is a syntactic tree and whether you know I don't know whether any of yours and language processing.",
            "If not, you may not be able to imagine you maybe are not dreaming and trees and but linguists are dreaming and trees, and so this is a tree, and in there this is the abstraction of a tree and you only cut out those parts of the tree that contain the argument.",
            "And then you try to get up to the part where these paths of meat that contain the argument and you first think that everything in this domain is in fact part of a rule.",
            "But you get separate rules by going up, you get one rule for unary parts, one for binary, one for ternary, and then you go all the way up.",
            "And then you put them in a database and called."
        ],
        [
            "Pattern instances lots of pattern instances because you go up bottom up piece by piece first.",
            "The little rules that get the unary parts, the individual, then maybe binary worlds and ternary worlds, quaternary worlds all the way up until you have it, but then you get stacked you get."
        ],
        [
            "Recursive rules, and it turns out that by analyzing this domain and the same is true for other domains.",
            "You, if you first look by hand and find all the relevant sentences by, but if you get quaternary 94%.",
            "We find it in 94% of the relevant sentences.",
            "If ternary already 87, and then when you only get two of the instances, not surprisingly, you get lots of other things.",
            "If you just get physics and the name or the year and the name, so that may not be the full relation."
        ],
        [
            "That's not surprising, so we worked with the with the.",
            "We we try it, certain corpora that we did and we took a Noble price.",
            "We for the Noble Prizes we took took a huge news domain.",
            "Or you just not that you.",
            "It's not actually that big.",
            "Only has 12.6 megabytes and then another one of 5.8 megabytes.",
            "Why did we take two of them?",
            "Yeah, what?",
            "Because we wanted to see.",
            "First of all, if we get the Nobel Prize winners from this time year for which we or the news where or get things before and out of this time.",
            "So we wanted to run, rerun many more experiments, then I'm going to report.",
            "And Secondly we took the second domain.",
            "From the so called Mac 6 conference, that was the message extraction conference in in the US quite awhile ago, DARPA sponsored and one MB of text only from one newspaper from the New York Times and only on management succession only on people.",
            "Assuming certain positions and company other people going out, coming and going."
        ],
        [
            "So then we evaluate the whole thing on the Noble price domain and actually were quiet."
        ],
        [
            "This is jump over.",
            "We were quite happy that we got in the immediately results.",
            "With very few, with very few seeds that were.",
            "Pretty good, yeah.",
            "Compared to the picture, you may still think I don't know if you are not fun language processing you will say what those are good results 100 this good result, but not this year.",
            "But if you are from language processing you can appreciate the results.",
            "Also it's because with that method without giving any additional knowledge without giving any additional knowledge, it's quite a bit so we gave it.",
            "This is seats when we gave it one seat each, every seat delivers a slightly different result.",
            "So if you get up to five seats, yeah if you give it five seats you get a better result of course.",
            "But if you give it only one seed, one example.",
            "Everything else is learned.",
            "Then you get by precision, up to in the worst case by 71 in the best case we measured by 87 and recall is sometimes very good.",
            "Sometimes it isn't and I will come and discuss now in a graph based analysis why it's sometimes good and not good.",
            "Yeah, so we come to that point in a moment."
        ],
        [
            "So that's kind of interesting.",
            "So and actually the whole thing converges after a short time, so the bootstrapping works at first looks whether this seed is in there gets the patterns with the patterns, new seeds, new patterns, and you'll see it's new patterns and so on, and then it converges aft."
        ],
        [
            "About eight or nine iterations, usually with management succession much much worse.",
            "Yeah, you get look at these precision figures.",
            "You had to go up to 55 seeds to get 62 precision.",
            "But now and 48 recall, but now later I will tell you an application where we put in thousands of examples in order to get the one nugget out there.",
            "But now this is so 55.",
            "On the one hand, depending on the type of application, sometimes 55 is a lot, sometimes it isn't."
        ],
        [
            "Depending on what you want to achieve, so we compare this result so our results was then with 20 seeds.",
            "After four iterations we wanted to see how much, how many seats do we need to bypass the best other result in the field and the best other result at this point was Greenwood and Stevenson of 2006 with the link chain model.",
            "I won't tell you exactly how it works and they did handcrafted patterns, so this is exactly what we wanted to avoid.",
            "We didn't want handcrafting of Petra, no expensive him.",
            "So needed handcrafted patterns and after 190 iterations with handcrafted patterns, they achieved handcrafted patterns where their seeds, their seats were not the semantics, not the tuples, but the patterns and they achieved something smaller.",
            "So we just looked up how many seats do we need automatic seats in order to pass by?"
        ],
        [
            "Same domain.",
            "So we this I now the next thing is then we looked at what can we do now in domains that also talk about prices for which we do not have the tables and I don't know Pulitzer Prize.",
            "Probably we could have had the table but Turner Prize and many others and actually we found long long lists of prizes, prizes we had never heard of so we got hundreds of different prices that we.",
            "All over the world that we never had heard of her so so it was important."
        ],
        [
            "To other fields now the dream.",
            "Wouldn't it be wonderful if we could always automatically learn most or all relevant patterns from one single cement?",
            "Again, since it's too nice to be true, of course.",
            "Yeah, that's too good.",
            "I mean, I won't tell you this story because it's simply not right, but."
        ],
        [
            "As research questions, we want to know why does it work for some task?",
            "Why not for others, and how can we estimate how suitable a domain is for this method?",
            "And how can we deal with less suitable domain?"
        ],
        [
            "So let's in order to appreciate this question, let's look at the graph that we get by learning.",
            "So we start from one event at the bottom the msar, the mentionings.",
            "Where is it mentioned?",
            "Then we expect rules from the mentionings.",
            "From the patterns we go to rules, then we go to new mentionings to new events to new men on top, and so on.",
            "So now what type of graph do we get in order to do this, let's reduce this problem to a by by bipartite graph.",
            "And only have events and patterns.",
            "Leave out the steps in between so from which events do you get to?",
            "Which patterns and from which patterns to which events?"
        ],
        [
            "No, you could already see because the recall sometimes was 40, sometimes well, 70.",
            "It depends on where you start.",
            "You might may get some continents or islands in in, in, in, in your landscape of this graph.",
            "And now the interesting thing we know from graph theory is if we can really in a few iterations go through a huge graph.",
            "Then the graph should have a certain property that we all know.",
            "Namely it should be have the small world property.",
            "If it has the small world property, then in a few iterations we go through.",
            "But for that we need a certain type of degree distribution.",
            "Yeah, we need certain type of degree distribution and it would have to be a skewed long tail distribution.",
            "Now do we?"
        ],
        [
            "Edit Here or don't we?",
            "Yeah, we looked at the distributions.",
            "This distribution of patterns and texts and the distribution of mentioning to relation."
        ],
        [
            "Instances and what did we find for Noble Prize?",
            "Hey, there's this cute long tail distribution we found exactly what would give you what would give you a small world property and would give you a nice line."
        ],
        [
            "Property and then."
        ],
        [
            "We looked at this is this is the stuff you know.",
            "I mean, how are you?",
            "How you do, how you get scale free networks, right?",
            "It's the same thing like with social networks.",
            "And why do we?",
            "Why do we in a few steps I mean the ID."
        ],
        [
            "Here behind that is extremely simple.",
            "I show it to you here.",
            "The idea is if a network has this cute degree distribution, that means it has some hubs that are connected to a lot of things and they are like."
        ],
        [
            "Like in an airline."
        ],
        [
            "In system they are like this airline hubs.",
            "Now if you get there then you fly to some other place and everywhere in the world you can fly with just two stopovers or maybe 3.",
            "And this is exactly what we want for learning."
        ],
        [
            "Right, we want this and we don't want something like this year.",
            "There's the German railway system, so the German railway system.",
            "We would never learn this."
        ],
        [
            "So we would need we want the."
        ],
        [
            "Airline system so and now.",
            "The interesting thing is do we get it or do?"
        ],
        [
            "Once we get it."
        ],
        [
            "And the."
        ],
        [
            "Answer is actually."
        ],
        [
            "Here with the the the Purple one is the Noble price domain and the blue one, the blue one is the Mac domain.",
            "The management succession.",
            "So what happens?",
            "It's only one newspaper reporting and they do not really report usually more than once about one single relation instance about one event because they have reported about this guy now being replaced by the other guy.",
            "Why should they print it again?",
            "Usually so you get very little redundancy so you may get to and you mentioning but you.",
            "Don't get new patterns out of the new of the new instance, so you need a certain."
        ],
        [
            "Type of redundancy that's otherwise it wouldn't work, and again from rule to instance you see the same thing in the properly you see the small world property, you get this cute long tail distribution and in the in the management success."
        ],
        [
            "You don't.",
            "So what can we do if our domain or our data do not fit the nicely connected small work picture?",
            "Then we can give up and search for another domain.",
            "Or we can try to change the data or relations in order to get the never."
        ],
        [
            "And learning graphs.",
            "So try to get for instance additional data and that's what we started.",
            "Let's imagine that we would have started with a Pulitzer Prize.",
            "Or let's imagine we would have started with the German Fritz Winter price, for which you rarely ever get any mentionings more than one per price given out.",
            "Because I mean, most of you don't even know about this price.",
            "Then you would have to learn the patterns from another domain.",
            "You may call it a carrier domain and then apply it to the domain for which you don't have enough mentionings.",
            "So and exactly that.",
            "Now we come to the point where we try to Noble price domain becausw we exactly try to to use the normal price domain as the carrier domain."
        ],
        [
            "For other prices.",
            "Ended work, so this is just examples of other prices that we found the Blitz curve the ME, but then we also found other things that are not private prizes.",
            "But he won gold and then it turned out we looked at the text and it was a gold medal and he won a Tony and a Tony Award and so on.",
            "All prizes that we didn't know about but then also we get this stuff here, which is not really prizes becausw you may win or you may be awarded something that's not a price and it's very hard sometimes to separate this out.",
            "And this is a reason why you have to do a little more if you want to get high precision.",
            "But now it depends on your type of application.",
            "If you are in an intelligence application, you get a human finally evaluating this stuff, and you want a good recall in intelligence.",
            "Depending on the usually an intelligence, application, business, intelligence, technology, intelligence.",
            "Military intelligence you want you want not to miss this one event?",
            "You know that's very important, so you rather have people look through.",
            "So you want very high recall, maybe low precision, even if the even if you get only 1% precision, the person has to look through 100 things, but then it will not meant he will find this terrorist attack.",
            "Yeah, so it doesn't matter.",
            "So in this case so you should not always go by F measure.",
            "So to say if measures are very deceptive concept, so you should look at the."
        ],
        [
            "Individual application.",
            "So, but then there are other applications if you want fully automatically populate the database and you it doesn't really matter so much here in in a way how whether whether you immediately get all the instances or whether you have some time, whether you mainly you only need maybe 5 instances to win your game.",
            "Yeah, whatever your application is then of course you want to go by precision of its fully automatic and maybe 10% recall is enough because you get the five unit.",
            "So depending on what game you play so so we also tried to extend the New York Times mock data with other data.",
            "General Press corpora World Wide Web and immediately we got we got better results.",
            "So not surprise."
        ],
        [
            "Singly.",
            "So the next steps would be to go beyond the sentence, investigate properties of relations with respect to data, try to describe them as graph properties and so on.",
            "And we did some of that."
        ],
        [
            "Stuff.",
            "We experimented with other domains.",
            "One of our domains was pop artist gossip in a project funded by the EU and actually with two Austrian partners for the Austrians around, you know.",
            "So we what we try to do is we build a very nice system.",
            "Nifty.",
            "Everybody builds nice system.",
            "So no, I mean you do, but but we built a system that's fun for youngsters.",
            "You get a, you get an artificial character there and designed by a game company.",
            "And you can talk about pop gossip about the latest Bell boyfriends, girlfriends of your favorite pop artists, and what our system does.",
            "It starts with a huge seed from a database kept in Mountain View USA.",
            "NNDB look it up if you like database on famous people, and you start huge amount of seeds and this is an application where you use thousands and thousands of seats in order to find really the next nugget.",
            "The next piece of gossip there's the so one application, one seat, you find thousands of examples, maybe the other the other or hundreds and thousands of examples.",
            "The other one thousands of seats.",
            "In order to find the next business intelligence application.",
            "Typical intelligence application.",
            "Although this application does not have much intelligence.",
            "Well, the first experiment was used to learn patterns for detecting other price winning such as Grammy and Music Awards.",
            "And we did this with quite nice results that we reported on in several publications.",
            "But then.",
            "The next thing was also to learn the other type of gossip.",
            "Yeah, and with many many seeds.",
            "So, but this is."
        ],
        [
            "Now we want to go and improve recall and precision.",
            "So what did we do to in order to improve recall?",
            "We found out that many of the sentences had only anaphoric references to the real role fillers like he won the prize for his Earth shaking discoveries in genetic sequencing.",
            "You don't know who's he, and you don't know which price.",
            "But if you read some sentences back, it will be mentioned so many, many of these examples, especially in news.",
            "Of that sort, yeah, the first the elements are introduced the individuals and then you get a price with anaphoric references or in the same year the two biologists received the Nobel Prize in Medicine.",
            "So then we included sentences in which potential role fillers a curd, some sentences before or after the pattern.",
            "We did a study before and we found out that most of the most of the real role fillers where we call them enter seasons for anaphora.",
            "Where only three sentences before after, so we included four sentences to be on the safe side, but three would have been enough to.",
            "And then we use the domain ontology to determine whether the anaphoric ull phrase constituted a semantically suitable candidate for the relation and for the coreference.",
            "So we use that because we were not interested in all enough where we are only interested in those anaphora and that is different from earlier work and anaphora resolution, because we are only interested in these anaphora that really matter for our relation.",
            "So and then we were able and then we were able to improve these things and we had a paper at ICAI this year describing this at."
        ],
        [
            "European AI conference OK and then the other thing is improving precision.",
            "We found out that more than 40% of our errors could be attributed to shortcomings of this miniport parser.",
            "This mini parsers very good.",
            "It's very robust and eager, but it makes many mistakes.",
            "So one tempting alternative is to use.",
            "Use a more precise deep parser, but then you lose robustness again.",
            "Then you lose robustness, you get accuracy, but maybe only 40% of 1st, so we do not want to."
        ],
        [
            "Give up robustness.",
            "So now do I have if I have the time I would really like to share some in some reflections and I wonder whether you share them.",
            "There's something that has happened in the last 20 years.",
            "Now in knowledge representation that may be more your field and in language technology my field and that is pretty much alike.",
            "20 years ago we set influences computationally intractable inference is too inefficient for practical use.",
            "It was simply very inefficient.",
            "Yeah, I knew these early theorem provers wasn't terribly.",
            "Yeah, you could go out.",
            "If I had to evaluate projects we had to go out and have not just dinner, but we had to have a coffee afterwards and take a walk.",
            "And then maybe something was proven and and too much reliance on human knowledge engineering?",
            "Yeah, usually the people came and said, but look, who does all the knowledge engineering also?",
            "And that was the problem.",
            "And then in the end.",
            "Even if you had an ontology, there is this old world worth all ontologies leak."
        ],
        [
            "The same thing with formal grammars.",
            "Deep parsing is computationally intractable, too inefficient for practical use.",
            "You have to wait for a long time, too much reliance on human knowledge engineering.",
            "They have to write all these big grammars.",
            "All grammars leak.",
            "That was mentioned by Edward Sapir in 1921."
        ],
        [
            "But we carried it over to ontologies.",
            "So in the end what we had in the 90s was a sour grape philosophy that also psychologists called denial of desire, denial of desire.",
            "Very hungry.",
            "You know this table are very hungry.",
            "Fox walked into a vineyard where there was an ample supply of luscious looking grapes.",
            "However, the grapes hung higher than the Fox could reach.",
            "He jumped up and stretched and reached and jumped more to try to get those yummy grapes, but to no avail.",
            "Those grapes surely must be sour.",
            "He finally said I wouldn't eat them if they were served to me on a silver platter.",
            "Moral of the story, it's easy to hate what you cannot have so and actually in the in the 90s we had exactly that story in both fields and knowledge technologies and the language, technologies and people got the research funds that really did very cheap tricks and the people who tried to do it the principled way they had to survive.",
            "In somewhere, because they were called the non practical, the impractical, the dreamers.",
            "The ones who go for sour grapes."
        ],
        [
            "But why would it be good to drop deep processing in favor of shallow approaches?",
            "Because in this way we can build useful applications already today, so in a way we did the same thing.",
            "We also got research funding or funding.",
            "Didn't dry out.",
            "We did both.",
            "We on the one hand we did went for the shallow, non principled methods in order to keep people happy and deliver something to industry.",
            "On the other hand, we continued.",
            "But this is really hard to get money for that you try with the EU to get money for real principle research.",
            "So sometimes it works WHI.",
            "Is it good to continue with the processing because it's the ultimate goal of computational linguistics wire?",
            "I can tell you becausw if you work with these shallow and very shallow statistical systems.",
            "For each application you need to start again.",
            "You can you never have reusable knowledge, but what we want to get.",
            "We want to get some system like for a domain like for a knowledge domain.",
            "We want to get one system maybe for language, for the German language.",
            "For the English language that we can reuse for many different applications and that's kind of the goal.",
            "And also then we get consistency of the knowledge across.",
            "Applique"
        ],
        [
            "So what has changed for knowledge processing since then?",
            "You know what has changed, tractable subsets of 1st order logic?",
            "Sometimes that also called the German School of Description Logics.",
            "Something that DFK I was involved in and detailed catalogue of complexity of family.",
            "So the complexity of the family of logics.",
            "More efficient influencing technologies.",
            "So more intuitive notations, editing tools, better knowledge engineering methods, methods for learning automatic."
        ],
        [
            "Ugly and so on in more demand.",
            "What has changed for deep language processing?",
            "Computationally?",
            "More benign grammar formalisms.",
            "Much more efficient parsing technologies, more intuitive notations and editing tools.",
            "Better grammar engineering methods, machine learning, and stronger applications."
        ],
        [
            "The round was actually pretty much the same, so in deep language you get 3 different traditions, which I will not bother you with.",
            "But actually for all of these, for all of these areas there are now efficient and grammars and grammars with rather complex coverage.",
            "They are not yet as good as the best shallow grammars that get more than 90% ninety 2% or so.",
            "These are little below, but on the other hand they get you much more semantics.",
            "So the cheap grammars that win the DARPA contest this years here in grammar evaluations.",
            "They don't get you the real semantics, but these grammars that have a little bit.",
            "There's nothing to do with darker now.",
            "I mean, this is only with the type of people who run in these contests, and because they're different games for some things you may need just a higher recall.",
            "For others you need better precision."
        ],
        [
            "OK, and I won't go into this.",
            "This is now a good getting."
        ],
        [
            "Too much into nitty gritty.",
            "So the big.",
            "Difference.",
            "Oh no."
        ],
        [
            "I go into this so the dream of reusable linguistic knowledge has not been given up.",
            "Even if the great majority of papers at large conferences, now dedicated to shallow, including statistical end table look up systems, some researchers are still trying to solve the much harder problem.",
            "However, the problem is so complex that it takes large efforts and international collaboration.",
            "Now several of these collaborations have formed.",
            "One has formed starting from Palo Alto Research Center from Park in LFG and another one we formed today."
        ],
        [
            "It was Stanford and Tokyo in the area of another grammar with HP SG.",
            "But actually HPS.",
            "She was already around 20 years ago because when I started at the FKII left IBM and there I worked with the person whose name you may know who's not here.",
            "I think Cody Studer's here.",
            "No, I work with Woody Shooter together and we worked in the same project and he was more in the semantic technology site and I was now I'm was more on the language processing side and we tried to do exactly the same squeeze meaning out of sentences and in one of the projects we got up to sentence 14.",
            "With lots of handwork, but this only took three years and this was a different type of game from which we are now in.",
            "Because now we apply data intensive methods and really finally get up."
        ],
        [
            "22 speed so this is the."
        ],
        [
            "The participants in this, in this initiative, and it started with Stamfords, are booking and talk."
        ],
        [
            "Go and buy already buy.",
            "Combining methods from participants and having our own technology evaluation and contests, we could boost the efficiency by a factor 2000."
        ],
        [
            "So still not robust.",
            "Still not robust.",
            "We're still left with the problem of insufficient coverage.",
            "Two remedies, one combining the accurate deprocessing with robust, not robots.",
            "No, not with robots.",
            "With robust shallow processing.",
            "So combine the thing.",
            "Have one system that gets you if it applies.",
            "If it can parse, it gets you.",
            "The precise unit gets a high accuracy, maybe 40% another domain.",
            "60% and.",
            "If it doesn't parse, then you fall back on the mini part and still have a second shot.",
            "You know you still get it, so of course you do things in parallel in order to save time, so that's one thing we did and the other one is improving coverage through data and."
        ],
        [
            "Intensive learning methods and this is the end.",
            "I'll come to the end.",
            "In the first method in hybrid NLP we combined the processing with a number of shallow processing systems, put them together and this was a PhD thesis.",
            "I supervise parolee Schaefer of last year called Heart of Gold is disjoined.",
            "Architecture is now used at many sites and all components use the same annotation.",
            "Multilevel standoff annotation that is the interface.",
            "So you all all components one after the other, tries to annotate the sentence with the analysis and the language for annotation.",
            "And thus the interface language is called minimal recursion semantics.",
            "I won't say anything about it, but this is right now one of our major problems how to get this semantic formalism that was mainly developed at Stanford and is more suited for natural language to get this interface to the ontologies that we do in all this is quite a headache, but we're working on it.",
            "Yeah?",
            "So it's both semantics.",
            "Both are semantics, but if you look at it slightly there."
        ],
        [
            "And then the second thing is, another was another PhD thesis last year and there was a kind of a rather.",
            "This was quite an important result, Zanghi, who was able to prove 1st that the best English grammar in HP SG written by then flick, injure and others at Stanford.",
            "the English resource grammar.",
            "Showing that most of the missing coverage could be traced actually to the lexicon, even if it didn't look like it, because very often the word was in, but not quite in the same meaning in the same frame in the same subcategorization frame.",
            "So you could show that 84% of all missing sentences in coverage of this grammar could be attributed to.",
            "Missing lexical lexical types.",
            "Sometimes semantic types, so then he he learned these types by large corpora and was able to get the coverage up by another 20% and now the English Resource Grammar uses this method.",
            "Actually it's no use.",
            "But now, why didn't I put in the absolute numbers?",
            "Because they're still shameful.",
            "Of course we get.",
            "We get between 60 and 80%.",
            "We don't even get close to the 91% of the shallow parsing people.",
            "That's still true when we have a semantics as output and they don't.",
            "That's a difference and we get some relief."
        ],
        [
            "Things that they cannot get, so conclusion and outlook.",
            "And then let's finish with it.",
            "Yes, there has been progress, even if it has been slow.",
            "And doing things right and not giving up the more demanding, principled ways is in the end, paying off.",
            "Both in the area of knowledge technologies and linguistic technologies, the high hanging grapes actually are quite sweet.",
            "Quite sweet, but then the harvest season has not even started yet.",
            "Let's do this together.",
            "OK, thank you very much."
        ],
        [
            "OK, I think we have time for a couple of questions for hands so any questions.",
            "Actually, do we have a roving microphone?",
            "OK, great.",
            "Cancel.",
            "Any questions for hands?",
            "OK, so while people thinking of their questions let me start with the question.",
            "One of the reasons for taking the shadow or less principled approach is sometimes said to be the issue of scaleability.",
            "You know what?",
            "How watermarks would you make about about that?",
            "No, that's that's of course true.",
            "I mean, it's exactly the case if you want to.",
            "I'm not in other.",
            "As I mentioned, our funding has not dried out, so we have also used shallow methods quite extensively.",
            "If you get something that should scale very fast.",
            "That should learn very fast and you cannot really invest ways and doing it the principled way I think for the next few years I would still recommend to use shallower systems and depending on the type of task, I would either recommend to use a very shallow system or to use something like mini Park or the Stanford parser or the new dependency parsers.",
            "There's a whole generation of very exciting new things and pricing technologies over the last three years.",
            "And with very very good results, go to these things, use them and you get faster results because they are off the shelf, you can use them.",
            "They are distributed freely and so on.",
            "If you get if you get a domain and you know this domain is really important to you, you will be working on it for awhile.",
            "It's not some contract that you try to finish off in a 3 months and four months and so and if you have the time to do it right then I would say try it with a try.",
            "I mean I cannot promise that it would work for everything.",
            "It turns out we tried the English Resource Grammar and our version the we did a DFK the German to Japanese grammar that's now used at Nyct and entity with German and Japanese.",
            "And sometimes it works and their other domains.",
            "For which it doesn't work, and so we're not using it in all domains ourselves, because exactly or scalability, but sometimes it does.",
            "Yeah sure OK thanks.",
            "Any further questions for hands.",
            "Last chance.",
            "OK, in that case, thank you very much, Hans."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's now my pleasure to introduce the first keynote speaker of the conference, hence Oscar rights, Hanses scientific director at the German Research Center for Artificial Intelligence, known to many of you as DFKI.",
                    "label": 1
                },
                {
                    "sent": "Of course, he's also head of the DFKI Language Technology Lab, and he's a professor of computational linguistics at silent University, and he's had a very distinguished career in the area of computational linguistics.",
                    "label": 0
                },
                {
                    "sent": "Given the vast amount of texts out there that needs to be semantically analyzed, computational linguistics clearly has a key role to play in in semantic technologies, and Hans is going to talk to us about getting at the semantics of texts, so without any further delay, I'd like to hand over to Hans and look forward to his presentation.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for inviting me to this conference.",
                    "label": 0
                },
                {
                    "sent": "I last year already.",
                    "label": 0
                },
                {
                    "sent": "I was tempted to come and then had something else.",
                    "label": 0
                },
                {
                    "sent": "And this time I was tempted to stay.",
                    "label": 0
                },
                {
                    "sent": "But tomorrow we will have their 20th anniversary of DFKI and unfortunately I have to be there, so otherwise I would have loved to stay here.",
                    "label": 0
                },
                {
                    "sent": "So in the end, although I was planning to come here for the whole duration of the conference, I had to.",
                    "label": 0
                },
                {
                    "sent": "Quiz it in.",
                    "label": 0
                },
                {
                    "sent": "Becausw yesterday I was at a conference completely different.",
                    "label": 0
                },
                {
                    "sent": "I did not expect semantic technologies to play a role there because it was a future planning conference of Aliens Group.",
                    "label": 0
                },
                {
                    "sent": "So the largest German insurance company but.",
                    "label": 0
                },
                {
                    "sent": "Actually, what we discussed over large parts of the day was semantic modeling of customer processes and customer needs so that shows right from the practice here from the from the commercial applicants of the technology.",
                    "label": 0
                },
                {
                    "sent": "Now here to the borderline of research and application where door meets and then tomorrow.",
                    "label": 0
                },
                {
                    "sent": "I think we can sell it.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great our 20th anniversary so I will start by looking back but not spend too much time on the past and then talk about the extraction of relation instances among them events, opinions, other types of relations.",
                    "label": 1
                },
                {
                    "sent": "Then come into the field that I am actually from, namely deep semantic and syntactic processing, computational linguistics.",
                    "label": 0
                },
                {
                    "sent": "But I'll make this rather.",
                    "label": 1
                },
                {
                    "sent": "Digestible I hope and then talk about hybrid processing models that we apply these days and start then after looking at the past and the beginning into the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looking into the future.",
                    "label": 0
                },
                {
                    "sent": "So tomorrow we have this 20 year anniversary of DFKI and we still get research money because we can still claim that there is no artificial intelligent yet we need another 20 years.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just 20 years so but is there AI or isn't there there are there?",
                    "label": 0
                },
                {
                    "sent": "Is there semantic technology in practice or isn't there?",
                    "label": 0
                },
                {
                    "sent": "Is it really true?",
                    "label": 0
                },
                {
                    "sent": "Semantic?",
                    "label": 0
                },
                {
                    "sent": "So the same we ask for AI, so today I cannot simulate a four year old child.",
                    "label": 1
                },
                {
                    "sent": "We cannot.",
                    "label": 0
                },
                {
                    "sent": "Not even a 3 year old.",
                    "label": 0
                },
                {
                    "sent": "Not even a 2 year old but.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, software has been built that beats the world champion in chess.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's some products on going on coming to human language technology, also called HLT.",
                    "label": 0
                },
                {
                    "sent": "No HLT program today can read a given three word sentence in a regretful tone.",
                    "label": 1
                },
                {
                    "sent": "Let alone discovering that it should be read in a regretful tone by the contents.",
                    "label": 0
                },
                {
                    "sent": "But we can build HLT programs that can read texts in 10 languages without major pronunciation errors and without a foreign accent.",
                    "label": 1
                },
                {
                    "sent": "No human being can do that.",
                    "label": 0
                },
                {
                    "sent": "Can read 10 in 10 languages without.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nixon HLT cannot understand the full meaning of any one sentence in the Charter of Human Rights or any other interesting text you may have.",
                    "label": 1
                },
                {
                    "sent": "But each OT can find a single mentioning of some relevant event in millions of sentences within seconds.",
                    "label": 1
                },
                {
                    "sent": "Again, no human being can do that, so we've come more to a redefinition of our task, and that applies very well to semantic technologies as well.",
                    "label": 0
                },
                {
                    "sent": "We are not trying to take away this type of work that humans are good, and being creative really having human intelligence, but we try to extend their capabilities.",
                    "label": 0
                },
                {
                    "sent": "We try to extend their cognitions.",
                    "label": 0
                },
                {
                    "sent": "By modeling certain aspects of human cognition and magnifying them, yeah, this is, I think what we're going to do and.",
                    "label": 0
                },
                {
                    "sent": "I will start with the last sentence here.",
                    "label": 0
                },
                {
                    "sent": "How can we find a single relevant mentioning of some relevant event in millions of?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distances within seconds.",
                    "label": 0
                },
                {
                    "sent": "So in general we may see before I really start the talk that our success may not be sweeping.",
                    "label": 1
                },
                {
                    "sent": "But it's creeping.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Creeping in everywhere.",
                    "label": 0
                },
                {
                    "sent": "So there are quite a number of types of information extraction, as most of you know, we can extract from text topics, terms named entities, binary relations, enoree relations and then certain sub.",
                    "label": 1
                },
                {
                    "sent": "Parts subsets of Enoree relations are events.",
                    "label": 0
                },
                {
                    "sent": "Anchored in time answers, maybe to a relational question.",
                    "label": 0
                },
                {
                    "sent": "Opinions.",
                    "label": 0
                },
                {
                    "sent": "That's also relation between the opinion Holder and something.",
                    "label": 0
                },
                {
                    "sent": "The opinion is about and center.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, and all of these are types, the red ones are.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Types of relation extraction.",
                    "label": 0
                },
                {
                    "sent": "Now when you work on relation extraction there are cheap methods and expensive methods and the cheap methods they get you a certain distance and I don't want to say anything bad about them because they are used every day and you can squeeze something out, some juice out of it, but when it really comes to getting semantics in a reliable way out of larger texts are getting at the semantics.",
                    "label": 0
                },
                {
                    "sent": "All the cheap methods in the end fail.",
                    "label": 0
                },
                {
                    "sent": "But even if we do not analyze every phrase or sentence, even if we concentrate on the ones where we think that the juices in we need some grammar or some equivalent classifier complex classifier to detect the relevant relations and context.",
                    "label": 1
                },
                {
                    "sent": "So here we face the problem of whether you call it grammar or not.",
                    "label": 0
                },
                {
                    "sent": "There are called classifier.",
                    "label": 0
                },
                {
                    "sent": "We we need to acquire this here we need to acquire this the same way.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We do it in language parsing.",
                    "label": 0
                },
                {
                    "sent": "Now the main methods for writing such grammars and all of them are actually used in the field or.",
                    "label": 0
                },
                {
                    "sent": "The old one introspective intellectual development.",
                    "label": 1
                },
                {
                    "sent": "The next one corpus based intellectual development.",
                    "label": 0
                },
                {
                    "sent": "The first one is just the grammar writer.",
                    "label": 0
                },
                {
                    "sent": "Things what are ways of expressing that and try to write for everyone that comes to mind, right?",
                    "label": 0
                },
                {
                    "sent": "A little grammar.",
                    "label": 0
                },
                {
                    "sent": "The second one is go through large amounts of corpora.",
                    "label": 0
                },
                {
                    "sent": "Huge amounts of texts and find all the patterns.",
                    "label": 0
                },
                {
                    "sent": "Mark them by hand and then write rules for all of them.",
                    "label": 0
                },
                {
                    "sent": "Or you may try supervised learning from annotated data.",
                    "label": 1
                },
                {
                    "sent": "That's very much used in the field now, so you first annotate in the corpora.",
                    "label": 0
                },
                {
                    "sent": "All the cases you say exactly which roles, which relations, and then you try to train classifiers on that.",
                    "label": 0
                },
                {
                    "sent": "Or you do something I want to talk about today.",
                    "label": 1
                },
                {
                    "sent": "Do minimally supervised learning from a few examples.",
                    "label": 0
                },
                {
                    "sent": "Or you try completely unsupervised learning, which also I do not want to say very bad things about.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, completely unsupervised learning when it really comes to more complex semantic tasks is far from being solved and without giving it additional types of data it may never be solved completely unsupervised without additional types of data but minimally supervised learning I want to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk about today.",
                    "label": 0
                },
                {
                    "sent": "So if you compare these methods now, you will see the first one is very expensive and incomplete, the other one is expensive and slow.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning from annotated data, still very expensive.",
                    "label": 1
                },
                {
                    "sent": "They need to annotate all these data and then I try to show you that you get promising results out of minimally supervised learning.",
                    "label": 1
                },
                {
                    "sent": "If you combine this method with linguistic analysis and not feasible for most tasks, I would still claim unless you show me the opposite.",
                    "label": 0
                },
                {
                    "sent": "Is the completely unsupervised learning, at least for the more complex?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, that I'm talking about, so the minimalize minimals least supervised learning has of course along history, and actually the type of work, the type of minimally supervised learning.",
                    "label": 0
                },
                {
                    "sent": "I will say something about.",
                    "label": 0
                },
                {
                    "sent": "What are the most interesting paper in this direction?",
                    "label": 0
                },
                {
                    "sent": "Actually, maybe not the very first suggestion, but the most interesting paper that started this is by Sergey Brin.",
                    "label": 0
                },
                {
                    "sent": "You know the name right today is Co.",
                    "label": 0
                },
                {
                    "sent": "Owner of Google, and so why didn't he supervise and got an academic career in unsupervised learning or minimally supervised learning instead?",
                    "label": 0
                },
                {
                    "sent": "So basically his results were actually not that good.",
                    "label": 0
                },
                {
                    "sent": "The paper was brilliant and intriguing, but since he didn't have the same.",
                    "label": 0
                },
                {
                    "sent": "Analysis methods that we are applying now.",
                    "label": 0
                },
                {
                    "sent": "He could only see which words were next to each other.",
                    "label": 0
                },
                {
                    "sent": "It didn't bear the fruit that he was hoping for.",
                    "label": 0
                },
                {
                    "sent": "But then many other people from the field from the field of language processing tried their hands on the on similar methods, and here I list a long list of people without going into what in what each of them did there.",
                    "label": 0
                },
                {
                    "sent": "Basically, the story is the following.",
                    "label": 0
                },
                {
                    "sent": "You start and getting more and more complex.",
                    "label": 0
                },
                {
                    "sent": "First people look for words that are next to each other.",
                    "label": 0
                },
                {
                    "sent": "Then they look for words that are a little bit apart from each other.",
                    "label": 0
                },
                {
                    "sent": "Then they look for phrases that are next to each other.",
                    "label": 0
                },
                {
                    "sent": "Then for phrases that are a little bit apart from each other.",
                    "label": 0
                },
                {
                    "sent": "Then they try to find out whether these phrases fill certain thematic roles.",
                    "label": 0
                },
                {
                    "sent": "May be the subject of a sentence or object of a verb, and what the verb is so they look at pairs of phrases and their functions of phrases in the sentence and in the end, what I'm driving at is you analyze the sentence and then you get it all, and then you from this pattern of the completely analyzed sentence.",
                    "label": 0
                },
                {
                    "sent": "Then you try to learn the relevant pattern and actually this in the end, not surprisingly, brings the best results.",
                    "label": 0
                },
                {
                    "sent": "If and this is the big if you get an analysis method that gets you with a sufficient coverage and precision, the analysis of all these sentences.",
                    "label": 0
                },
                {
                    "sent": "But I'll talk about this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are.",
                    "label": 0
                },
                {
                    "sent": "Quite a number of approaches to seat construction by bootstrapping.",
                    "label": 1
                },
                {
                    "sent": "The most well known that some of you may also notice the X disco system by young Garber already back in 2001.",
                    "label": 0
                },
                {
                    "sent": "But in this case he is really following very, very special patterns.",
                    "label": 1
                },
                {
                    "sent": "So that comes close to the full analysis, but doesn't quite get there.",
                    "label": 1
                },
                {
                    "sent": "So he takes these triples of subject and verbs and objects and tries to get get along by these events.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out in the end, and that for a couple of relations that happened to be expressed.",
                    "label": 0
                },
                {
                    "sent": "In this pattern you get a certain distance, but the majority of relations are not.",
                    "label": 0
                },
                {
                    "sent": "So if you take for instance who invented what year you want to get the simple relation, who invented what, then very often the the mentionings we call them dimension in the text are in noun phrases for instance so and so the inventor, the 1981 inventor of blah blah.",
                    "label": 0
                },
                {
                    "sent": "So the writers don't follow now the type of pattern that we think they should.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so they write these things and.",
                    "label": 0
                },
                {
                    "sent": "Completely different ways.",
                    "label": 0
                },
                {
                    "sent": "They're very creative.",
                    "label": 0
                },
                {
                    "sent": "They use many different patterns, so that's why we were then driven to try to get to improve these results by.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Getting full analysis.",
                    "label": 0
                },
                {
                    "sent": "So we try to see driven and bottom of rule learning and a bootstrapping fashion so.",
                    "label": 1
                },
                {
                    "sent": "We first discover some patterns.",
                    "label": 1
                },
                {
                    "sent": "We do this bottom up and compositional.",
                    "label": 0
                },
                {
                    "sent": "I'll show you some examples and then use a compression method for clustering and generalizing this rules, because otherwise you get too many rules.",
                    "label": 0
                },
                {
                    "sent": "But we only considered subtrees containing arguments and pattern candidates.",
                    "label": 1
                },
                {
                    "sent": "Arguments of the relations of the seeds.",
                    "label": 1
                },
                {
                    "sent": "So we kind of cut out those parts of a tree that contain the elements we are interested in.",
                    "label": 0
                },
                {
                    "sent": "The arguments of the relation.",
                    "label": 0
                },
                {
                    "sent": "And the relation market could be a verb, could be an adjective and then try to generalize over many instances in order to get the inductive part the other generalize.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mission part.",
                    "label": 0
                },
                {
                    "sent": "So it's C driven and bottom of rule learning and what we also do is we calculate the domain relevance by the terms that are there and the trustworthiness of the origin.",
                    "label": 1
                },
                {
                    "sent": "So when we learn new rules from old patterns, then we look how, how much confidence did we have in the old one and that is inherited by the new ones and so on also.",
                    "label": 0
                },
                {
                    "sent": "And the further it gets away from from something trustworthy, the less trustworthy.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get.",
                    "label": 0
                },
                {
                    "sent": "So the novel thing is this compositional representation method, so I won't go through the subparts here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But give you some examples, then you see it very easy so.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "Domain that you may think is crazy for research.",
                    "label": 0
                },
                {
                    "sent": "For information extraction, we worked first with was the Noble price domain.",
                    "label": 0
                },
                {
                    "sent": "Why's that crazy?",
                    "label": 0
                },
                {
                    "sent": "I mean, why would you ever want to extract Noble price winners?",
                    "label": 0
                },
                {
                    "sent": "There's a whole table of them so you can get the whole thing.",
                    "label": 0
                },
                {
                    "sent": "You populate the database by the cheapest rapper ever written now, so why should you do it?",
                    "label": 0
                },
                {
                    "sent": "No, I mean you want to do it right for two reasons.",
                    "label": 0
                },
                {
                    "sent": "One reason is exactly that you get this table.",
                    "label": 0
                },
                {
                    "sent": "Because you first want to train a method for which you know you know the total.",
                    "label": 0
                },
                {
                    "sent": "Recall that you could achieve and the second reason is and I'll show you later you want to get something that has a high redundancy in reporting where you get many, many reports, so you get a fair chance of finding many different patterns.",
                    "label": 0
                },
                {
                    "sent": "And the third reason actually is, and I can also show you how we how we exploited.",
                    "label": 0
                },
                {
                    "sent": "The third reason is that you may learn you may be able to show that from a very benign domain where you get lots of redundancy in the text.",
                    "label": 0
                },
                {
                    "sent": "You can extract rules that you can then apply to less known prices where you don't get the full lists and actually all three of these things I will show now.",
                    "label": 0
                },
                {
                    "sent": "So the other thing where that we try it and we tried it.",
                    "label": 0
                },
                {
                    "sent": "Only because we wanted to get a paper accepted at the most ambitious young conference and language processing, yeah, so we try it also with the domain of which we knew before hand that it wouldn't work very well, but you need to apply.",
                    "label": 0
                },
                {
                    "sent": "You need to follow certain evaluation schemes, otherwise you don't get it accepted.",
                    "label": 0
                },
                {
                    "sent": "And in order to get an ACL paper out of that, we had to do that too, although we knew it wouldn't really work very well.",
                    "label": 0
                },
                {
                    "sent": "But I show you.",
                    "label": 0
                },
                {
                    "sent": "But then we could prove why it wouldn't work, and this is some eye opener for us, and I hope that you will find it.",
                    "label": 0
                },
                {
                    "sent": "My.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really interesting yourself.",
                    "label": 0
                },
                {
                    "sent": "So the Noble price target relations are pretty simple.",
                    "label": 0
                },
                {
                    "sent": "You gotta recipient the prize, an area and a year and now what we try to do with with as little possible seeds and the seats are really just mean the tuple of the name of Noble peace in the year.",
                    "label": 0
                },
                {
                    "sent": "So not not more.",
                    "label": 0
                },
                {
                    "sent": "We want to give only the seat to a system, not more and everything else by magic happens by itself.",
                    "label": 0
                },
                {
                    "sent": "And actually we tried it with one seed with five seats with 10 seeds.",
                    "label": 0
                },
                {
                    "sent": "But actually, I mean we were surprised one seat already gives you.",
                    "label": 0
                },
                {
                    "sent": "Vast amount of frequency, so and then you may get a sentence.",
                    "label": 0
                },
                {
                    "sent": "Mohamed El Baradei one, the 2005 Nobel Peace Prize on Friday for his efforts to limit the spread.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Atomic weapons so.",
                    "label": 0
                },
                {
                    "sent": "Then we take two language processing tools.",
                    "label": 0
                },
                {
                    "sent": "One is the tool, sprout.",
                    "label": 0
                },
                {
                    "sent": "It's a named entity and morphology Analysis tool that we have built ourselves for about 11 languages.",
                    "label": 0
                },
                {
                    "sent": "And then for this sentence analysis we in the beginning employ mini part by Dick and Lynn.",
                    "label": 1
                },
                {
                    "sent": "Later we other process as well, but let's start with it and it's why did we take this Spicer?",
                    "label": 0
                },
                {
                    "sent": "Because it's very robust.",
                    "label": 0
                },
                {
                    "sent": "It gives you a price for everything.",
                    "label": 0
                },
                {
                    "sent": "The parts may be wrong, but it gives you a price so it gives you it's very robust.",
                    "label": 0
                },
                {
                    "sent": "Always gives you something and it's pretty good.",
                    "label": 0
                },
                {
                    "sent": "Not the best.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But one of the best.",
                    "label": 0
                },
                {
                    "sent": "So and then we want to learn rules of that soil now.",
                    "label": 0
                },
                {
                    "sent": "Don't try not to get too deep into the rules here.",
                    "label": 0
                },
                {
                    "sent": "All you can see that in this attribute value matrix you get a rule name and you get a head so called head as we say in computational linguistics, which in this case is the noun and this is the prize.",
                    "label": 0
                },
                {
                    "sent": "And then you may have daughters the year the prize, the area and so on.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's one rule that's a more difficult rule that has a verb in win, an active verb, and maybe a person in here and the lexical form and so on, but doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "I mean this is not computational linguistics, they have to deal with or we have to deal with it.",
                    "label": 0
                },
                {
                    "sent": "That's our task to do.",
                    "label": 0
                },
                {
                    "sent": "This is the type of grammatical representation that we use very often.",
                    "label": 0
                },
                {
                    "sent": "So at first glance it looks a little bit like any kind of type definition or so, and it is a type definition, but offer little different has a little different semantics.",
                    "label": 0
                },
                {
                    "sent": "From what used in knowledge technologies?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So those are the real components.",
                    "label": 0
                },
                {
                    "sent": "Also it's an AVM format containing ahead daughters a rule and then down here the output and the output is actually the tuple.",
                    "label": 0
                },
                {
                    "sent": "Now, one thing that we did from the beginning on that was different from what other people did before us.",
                    "label": 0
                },
                {
                    "sent": "We not only looked at the at the full error enoree relation instances, but also at projections of the relation attornery and even at binary, and then try to find out whether we could also get something out of.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lowe's, and actually I'll show you later in a table that it really helps to do this.",
                    "label": 0
                },
                {
                    "sent": "It really helps to do that.",
                    "label": 0
                },
                {
                    "sent": "Also, it helps you to transport later the learned rules to other domains because you may abstract away from one of these, maybe from Noble price and apply it to other prices so the next is a pattern extraction step and that does exactly what I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "You identify these subtrees.",
                    "label": 0
                },
                {
                    "sent": "Assume that this is a syntactic tree and whether you know I don't know whether any of yours and language processing.",
                    "label": 0
                },
                {
                    "sent": "If not, you may not be able to imagine you maybe are not dreaming and trees and but linguists are dreaming and trees, and so this is a tree, and in there this is the abstraction of a tree and you only cut out those parts of the tree that contain the argument.",
                    "label": 0
                },
                {
                    "sent": "And then you try to get up to the part where these paths of meat that contain the argument and you first think that everything in this domain is in fact part of a rule.",
                    "label": 0
                },
                {
                    "sent": "But you get separate rules by going up, you get one rule for unary parts, one for binary, one for ternary, and then you go all the way up.",
                    "label": 0
                },
                {
                    "sent": "And then you put them in a database and called.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pattern instances lots of pattern instances because you go up bottom up piece by piece first.",
                    "label": 0
                },
                {
                    "sent": "The little rules that get the unary parts, the individual, then maybe binary worlds and ternary worlds, quaternary worlds all the way up until you have it, but then you get stacked you get.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recursive rules, and it turns out that by analyzing this domain and the same is true for other domains.",
                    "label": 0
                },
                {
                    "sent": "You, if you first look by hand and find all the relevant sentences by, but if you get quaternary 94%.",
                    "label": 0
                },
                {
                    "sent": "We find it in 94% of the relevant sentences.",
                    "label": 1
                },
                {
                    "sent": "If ternary already 87, and then when you only get two of the instances, not surprisingly, you get lots of other things.",
                    "label": 0
                },
                {
                    "sent": "If you just get physics and the name or the year and the name, so that may not be the full relation.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's not surprising, so we worked with the with the.",
                    "label": 0
                },
                {
                    "sent": "We we try it, certain corpora that we did and we took a Noble price.",
                    "label": 0
                },
                {
                    "sent": "We for the Noble Prizes we took took a huge news domain.",
                    "label": 0
                },
                {
                    "sent": "Or you just not that you.",
                    "label": 0
                },
                {
                    "sent": "It's not actually that big.",
                    "label": 0
                },
                {
                    "sent": "Only has 12.6 megabytes and then another one of 5.8 megabytes.",
                    "label": 0
                },
                {
                    "sent": "Why did we take two of them?",
                    "label": 0
                },
                {
                    "sent": "Yeah, what?",
                    "label": 0
                },
                {
                    "sent": "Because we wanted to see.",
                    "label": 0
                },
                {
                    "sent": "First of all, if we get the Nobel Prize winners from this time year for which we or the news where or get things before and out of this time.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to run, rerun many more experiments, then I'm going to report.",
                    "label": 0
                },
                {
                    "sent": "And Secondly we took the second domain.",
                    "label": 0
                },
                {
                    "sent": "From the so called Mac 6 conference, that was the message extraction conference in in the US quite awhile ago, DARPA sponsored and one MB of text only from one newspaper from the New York Times and only on management succession only on people.",
                    "label": 0
                },
                {
                    "sent": "Assuming certain positions and company other people going out, coming and going.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then we evaluate the whole thing on the Noble price domain and actually were quiet.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is jump over.",
                    "label": 0
                },
                {
                    "sent": "We were quite happy that we got in the immediately results.",
                    "label": 0
                },
                {
                    "sent": "With very few, with very few seeds that were.",
                    "label": 0
                },
                {
                    "sent": "Pretty good, yeah.",
                    "label": 0
                },
                {
                    "sent": "Compared to the picture, you may still think I don't know if you are not fun language processing you will say what those are good results 100 this good result, but not this year.",
                    "label": 0
                },
                {
                    "sent": "But if you are from language processing you can appreciate the results.",
                    "label": 0
                },
                {
                    "sent": "Also it's because with that method without giving any additional knowledge without giving any additional knowledge, it's quite a bit so we gave it.",
                    "label": 0
                },
                {
                    "sent": "This is seats when we gave it one seat each, every seat delivers a slightly different result.",
                    "label": 0
                },
                {
                    "sent": "So if you get up to five seats, yeah if you give it five seats you get a better result of course.",
                    "label": 0
                },
                {
                    "sent": "But if you give it only one seed, one example.",
                    "label": 0
                },
                {
                    "sent": "Everything else is learned.",
                    "label": 0
                },
                {
                    "sent": "Then you get by precision, up to in the worst case by 71 in the best case we measured by 87 and recall is sometimes very good.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it isn't and I will come and discuss now in a graph based analysis why it's sometimes good and not good.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we come to that point in a moment.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "So and actually the whole thing converges after a short time, so the bootstrapping works at first looks whether this seed is in there gets the patterns with the patterns, new seeds, new patterns, and you'll see it's new patterns and so on, and then it converges aft.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About eight or nine iterations, usually with management succession much much worse.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you get look at these precision figures.",
                    "label": 0
                },
                {
                    "sent": "You had to go up to 55 seeds to get 62 precision.",
                    "label": 0
                },
                {
                    "sent": "But now and 48 recall, but now later I will tell you an application where we put in thousands of examples in order to get the one nugget out there.",
                    "label": 0
                },
                {
                    "sent": "But now this is so 55.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, depending on the type of application, sometimes 55 is a lot, sometimes it isn't.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Depending on what you want to achieve, so we compare this result so our results was then with 20 seeds.",
                    "label": 0
                },
                {
                    "sent": "After four iterations we wanted to see how much, how many seats do we need to bypass the best other result in the field and the best other result at this point was Greenwood and Stevenson of 2006 with the link chain model.",
                    "label": 1
                },
                {
                    "sent": "I won't tell you exactly how it works and they did handcrafted patterns, so this is exactly what we wanted to avoid.",
                    "label": 0
                },
                {
                    "sent": "We didn't want handcrafting of Petra, no expensive him.",
                    "label": 0
                },
                {
                    "sent": "So needed handcrafted patterns and after 190 iterations with handcrafted patterns, they achieved handcrafted patterns where their seeds, their seats were not the semantics, not the tuples, but the patterns and they achieved something smaller.",
                    "label": 0
                },
                {
                    "sent": "So we just looked up how many seats do we need automatic seats in order to pass by?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same domain.",
                    "label": 0
                },
                {
                    "sent": "So we this I now the next thing is then we looked at what can we do now in domains that also talk about prices for which we do not have the tables and I don't know Pulitzer Prize.",
                    "label": 0
                },
                {
                    "sent": "Probably we could have had the table but Turner Prize and many others and actually we found long long lists of prizes, prizes we had never heard of so we got hundreds of different prices that we.",
                    "label": 0
                },
                {
                    "sent": "All over the world that we never had heard of her so so it was important.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To other fields now the dream.",
                    "label": 0
                },
                {
                    "sent": "Wouldn't it be wonderful if we could always automatically learn most or all relevant patterns from one single cement?",
                    "label": 1
                },
                {
                    "sent": "Again, since it's too nice to be true, of course.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's too good.",
                    "label": 0
                },
                {
                    "sent": "I mean, I won't tell you this story because it's simply not right, but.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As research questions, we want to know why does it work for some task?",
                    "label": 1
                },
                {
                    "sent": "Why not for others, and how can we estimate how suitable a domain is for this method?",
                    "label": 1
                },
                {
                    "sent": "And how can we deal with less suitable domain?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's in order to appreciate this question, let's look at the graph that we get by learning.",
                    "label": 0
                },
                {
                    "sent": "So we start from one event at the bottom the msar, the mentionings.",
                    "label": 0
                },
                {
                    "sent": "Where is it mentioned?",
                    "label": 0
                },
                {
                    "sent": "Then we expect rules from the mentionings.",
                    "label": 0
                },
                {
                    "sent": "From the patterns we go to rules, then we go to new mentionings to new events to new men on top, and so on.",
                    "label": 0
                },
                {
                    "sent": "So now what type of graph do we get in order to do this, let's reduce this problem to a by by bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "And only have events and patterns.",
                    "label": 0
                },
                {
                    "sent": "Leave out the steps in between so from which events do you get to?",
                    "label": 0
                },
                {
                    "sent": "Which patterns and from which patterns to which events?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, you could already see because the recall sometimes was 40, sometimes well, 70.",
                    "label": 0
                },
                {
                    "sent": "It depends on where you start.",
                    "label": 0
                },
                {
                    "sent": "You might may get some continents or islands in in, in, in, in your landscape of this graph.",
                    "label": 0
                },
                {
                    "sent": "And now the interesting thing we know from graph theory is if we can really in a few iterations go through a huge graph.",
                    "label": 0
                },
                {
                    "sent": "Then the graph should have a certain property that we all know.",
                    "label": 0
                },
                {
                    "sent": "Namely it should be have the small world property.",
                    "label": 0
                },
                {
                    "sent": "If it has the small world property, then in a few iterations we go through.",
                    "label": 0
                },
                {
                    "sent": "But for that we need a certain type of degree distribution.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we need certain type of degree distribution and it would have to be a skewed long tail distribution.",
                    "label": 0
                },
                {
                    "sent": "Now do we?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Edit Here or don't we?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we looked at the distributions.",
                    "label": 0
                },
                {
                    "sent": "This distribution of patterns and texts and the distribution of mentioning to relation.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instances and what did we find for Noble Prize?",
                    "label": 0
                },
                {
                    "sent": "Hey, there's this cute long tail distribution we found exactly what would give you what would give you a small world property and would give you a nice line.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Property and then.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We looked at this is this is the stuff you know.",
                    "label": 0
                },
                {
                    "sent": "I mean, how are you?",
                    "label": 0
                },
                {
                    "sent": "How you do, how you get scale free networks, right?",
                    "label": 0
                },
                {
                    "sent": "It's the same thing like with social networks.",
                    "label": 0
                },
                {
                    "sent": "And why do we?",
                    "label": 0
                },
                {
                    "sent": "Why do we in a few steps I mean the ID.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here behind that is extremely simple.",
                    "label": 0
                },
                {
                    "sent": "I show it to you here.",
                    "label": 0
                },
                {
                    "sent": "The idea is if a network has this cute degree distribution, that means it has some hubs that are connected to a lot of things and they are like.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like in an airline.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In system they are like this airline hubs.",
                    "label": 0
                },
                {
                    "sent": "Now if you get there then you fly to some other place and everywhere in the world you can fly with just two stopovers or maybe 3.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly what we want for learning.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, we want this and we don't want something like this year.",
                    "label": 0
                },
                {
                    "sent": "There's the German railway system, so the German railway system.",
                    "label": 0
                },
                {
                    "sent": "We would never learn this.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we would need we want the.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Airline system so and now.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is do we get it or do?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once we get it.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Answer is actually.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here with the the the Purple one is the Noble price domain and the blue one, the blue one is the Mac domain.",
                    "label": 0
                },
                {
                    "sent": "The management succession.",
                    "label": 0
                },
                {
                    "sent": "So what happens?",
                    "label": 0
                },
                {
                    "sent": "It's only one newspaper reporting and they do not really report usually more than once about one single relation instance about one event because they have reported about this guy now being replaced by the other guy.",
                    "label": 0
                },
                {
                    "sent": "Why should they print it again?",
                    "label": 0
                },
                {
                    "sent": "Usually so you get very little redundancy so you may get to and you mentioning but you.",
                    "label": 0
                },
                {
                    "sent": "Don't get new patterns out of the new of the new instance, so you need a certain.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Type of redundancy that's otherwise it wouldn't work, and again from rule to instance you see the same thing in the properly you see the small world property, you get this cute long tail distribution and in the in the management success.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You don't.",
                    "label": 0
                },
                {
                    "sent": "So what can we do if our domain or our data do not fit the nicely connected small work picture?",
                    "label": 0
                },
                {
                    "sent": "Then we can give up and search for another domain.",
                    "label": 0
                },
                {
                    "sent": "Or we can try to change the data or relations in order to get the never.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And learning graphs.",
                    "label": 0
                },
                {
                    "sent": "So try to get for instance additional data and that's what we started.",
                    "label": 1
                },
                {
                    "sent": "Let's imagine that we would have started with a Pulitzer Prize.",
                    "label": 1
                },
                {
                    "sent": "Or let's imagine we would have started with the German Fritz Winter price, for which you rarely ever get any mentionings more than one per price given out.",
                    "label": 0
                },
                {
                    "sent": "Because I mean, most of you don't even know about this price.",
                    "label": 1
                },
                {
                    "sent": "Then you would have to learn the patterns from another domain.",
                    "label": 0
                },
                {
                    "sent": "You may call it a carrier domain and then apply it to the domain for which you don't have enough mentionings.",
                    "label": 0
                },
                {
                    "sent": "So and exactly that.",
                    "label": 0
                },
                {
                    "sent": "Now we come to the point where we try to Noble price domain becausw we exactly try to to use the normal price domain as the carrier domain.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For other prices.",
                    "label": 0
                },
                {
                    "sent": "Ended work, so this is just examples of other prices that we found the Blitz curve the ME, but then we also found other things that are not private prizes.",
                    "label": 0
                },
                {
                    "sent": "But he won gold and then it turned out we looked at the text and it was a gold medal and he won a Tony and a Tony Award and so on.",
                    "label": 0
                },
                {
                    "sent": "All prizes that we didn't know about but then also we get this stuff here, which is not really prizes becausw you may win or you may be awarded something that's not a price and it's very hard sometimes to separate this out.",
                    "label": 0
                },
                {
                    "sent": "And this is a reason why you have to do a little more if you want to get high precision.",
                    "label": 0
                },
                {
                    "sent": "But now it depends on your type of application.",
                    "label": 0
                },
                {
                    "sent": "If you are in an intelligence application, you get a human finally evaluating this stuff, and you want a good recall in intelligence.",
                    "label": 0
                },
                {
                    "sent": "Depending on the usually an intelligence, application, business, intelligence, technology, intelligence.",
                    "label": 0
                },
                {
                    "sent": "Military intelligence you want you want not to miss this one event?",
                    "label": 0
                },
                {
                    "sent": "You know that's very important, so you rather have people look through.",
                    "label": 0
                },
                {
                    "sent": "So you want very high recall, maybe low precision, even if the even if you get only 1% precision, the person has to look through 100 things, but then it will not meant he will find this terrorist attack.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So in this case so you should not always go by F measure.",
                    "label": 0
                },
                {
                    "sent": "So to say if measures are very deceptive concept, so you should look at the.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Individual application.",
                    "label": 0
                },
                {
                    "sent": "So, but then there are other applications if you want fully automatically populate the database and you it doesn't really matter so much here in in a way how whether whether you immediately get all the instances or whether you have some time, whether you mainly you only need maybe 5 instances to win your game.",
                    "label": 0
                },
                {
                    "sent": "Yeah, whatever your application is then of course you want to go by precision of its fully automatic and maybe 10% recall is enough because you get the five unit.",
                    "label": 0
                },
                {
                    "sent": "So depending on what game you play so so we also tried to extend the New York Times mock data with other data.",
                    "label": 0
                },
                {
                    "sent": "General Press corpora World Wide Web and immediately we got we got better results.",
                    "label": 0
                },
                {
                    "sent": "So not surprise.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Singly.",
                    "label": 0
                },
                {
                    "sent": "So the next steps would be to go beyond the sentence, investigate properties of relations with respect to data, try to describe them as graph properties and so on.",
                    "label": 0
                },
                {
                    "sent": "And we did some of that.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stuff.",
                    "label": 0
                },
                {
                    "sent": "We experimented with other domains.",
                    "label": 0
                },
                {
                    "sent": "One of our domains was pop artist gossip in a project funded by the EU and actually with two Austrian partners for the Austrians around, you know.",
                    "label": 0
                },
                {
                    "sent": "So we what we try to do is we build a very nice system.",
                    "label": 0
                },
                {
                    "sent": "Nifty.",
                    "label": 0
                },
                {
                    "sent": "Everybody builds nice system.",
                    "label": 0
                },
                {
                    "sent": "So no, I mean you do, but but we built a system that's fun for youngsters.",
                    "label": 0
                },
                {
                    "sent": "You get a, you get an artificial character there and designed by a game company.",
                    "label": 0
                },
                {
                    "sent": "And you can talk about pop gossip about the latest Bell boyfriends, girlfriends of your favorite pop artists, and what our system does.",
                    "label": 0
                },
                {
                    "sent": "It starts with a huge seed from a database kept in Mountain View USA.",
                    "label": 0
                },
                {
                    "sent": "NNDB look it up if you like database on famous people, and you start huge amount of seeds and this is an application where you use thousands and thousands of seats in order to find really the next nugget.",
                    "label": 0
                },
                {
                    "sent": "The next piece of gossip there's the so one application, one seat, you find thousands of examples, maybe the other the other or hundreds and thousands of examples.",
                    "label": 0
                },
                {
                    "sent": "The other one thousands of seats.",
                    "label": 0
                },
                {
                    "sent": "In order to find the next business intelligence application.",
                    "label": 0
                },
                {
                    "sent": "Typical intelligence application.",
                    "label": 0
                },
                {
                    "sent": "Although this application does not have much intelligence.",
                    "label": 0
                },
                {
                    "sent": "Well, the first experiment was used to learn patterns for detecting other price winning such as Grammy and Music Awards.",
                    "label": 0
                },
                {
                    "sent": "And we did this with quite nice results that we reported on in several publications.",
                    "label": 0
                },
                {
                    "sent": "But then.",
                    "label": 0
                },
                {
                    "sent": "The next thing was also to learn the other type of gossip.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and with many many seeds.",
                    "label": 0
                },
                {
                    "sent": "So, but this is.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we want to go and improve recall and precision.",
                    "label": 0
                },
                {
                    "sent": "So what did we do to in order to improve recall?",
                    "label": 0
                },
                {
                    "sent": "We found out that many of the sentences had only anaphoric references to the real role fillers like he won the prize for his Earth shaking discoveries in genetic sequencing.",
                    "label": 0
                },
                {
                    "sent": "You don't know who's he, and you don't know which price.",
                    "label": 0
                },
                {
                    "sent": "But if you read some sentences back, it will be mentioned so many, many of these examples, especially in news.",
                    "label": 0
                },
                {
                    "sent": "Of that sort, yeah, the first the elements are introduced the individuals and then you get a price with anaphoric references or in the same year the two biologists received the Nobel Prize in Medicine.",
                    "label": 0
                },
                {
                    "sent": "So then we included sentences in which potential role fillers a curd, some sentences before or after the pattern.",
                    "label": 0
                },
                {
                    "sent": "We did a study before and we found out that most of the most of the real role fillers where we call them enter seasons for anaphora.",
                    "label": 0
                },
                {
                    "sent": "Where only three sentences before after, so we included four sentences to be on the safe side, but three would have been enough to.",
                    "label": 0
                },
                {
                    "sent": "And then we use the domain ontology to determine whether the anaphoric ull phrase constituted a semantically suitable candidate for the relation and for the coreference.",
                    "label": 0
                },
                {
                    "sent": "So we use that because we were not interested in all enough where we are only interested in those anaphora and that is different from earlier work and anaphora resolution, because we are only interested in these anaphora that really matter for our relation.",
                    "label": 0
                },
                {
                    "sent": "So and then we were able and then we were able to improve these things and we had a paper at ICAI this year describing this at.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "European AI conference OK and then the other thing is improving precision.",
                    "label": 0
                },
                {
                    "sent": "We found out that more than 40% of our errors could be attributed to shortcomings of this miniport parser.",
                    "label": 0
                },
                {
                    "sent": "This mini parsers very good.",
                    "label": 0
                },
                {
                    "sent": "It's very robust and eager, but it makes many mistakes.",
                    "label": 0
                },
                {
                    "sent": "So one tempting alternative is to use.",
                    "label": 0
                },
                {
                    "sent": "Use a more precise deep parser, but then you lose robustness again.",
                    "label": 0
                },
                {
                    "sent": "Then you lose robustness, you get accuracy, but maybe only 40% of 1st, so we do not want to.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give up robustness.",
                    "label": 0
                },
                {
                    "sent": "So now do I have if I have the time I would really like to share some in some reflections and I wonder whether you share them.",
                    "label": 0
                },
                {
                    "sent": "There's something that has happened in the last 20 years.",
                    "label": 0
                },
                {
                    "sent": "Now in knowledge representation that may be more your field and in language technology my field and that is pretty much alike.",
                    "label": 0
                },
                {
                    "sent": "20 years ago we set influences computationally intractable inference is too inefficient for practical use.",
                    "label": 0
                },
                {
                    "sent": "It was simply very inefficient.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I knew these early theorem provers wasn't terribly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you could go out.",
                    "label": 0
                },
                {
                    "sent": "If I had to evaluate projects we had to go out and have not just dinner, but we had to have a coffee afterwards and take a walk.",
                    "label": 0
                },
                {
                    "sent": "And then maybe something was proven and and too much reliance on human knowledge engineering?",
                    "label": 0
                },
                {
                    "sent": "Yeah, usually the people came and said, but look, who does all the knowledge engineering also?",
                    "label": 0
                },
                {
                    "sent": "And that was the problem.",
                    "label": 0
                },
                {
                    "sent": "And then in the end.",
                    "label": 0
                },
                {
                    "sent": "Even if you had an ontology, there is this old world worth all ontologies leak.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The same thing with formal grammars.",
                    "label": 0
                },
                {
                    "sent": "Deep parsing is computationally intractable, too inefficient for practical use.",
                    "label": 1
                },
                {
                    "sent": "You have to wait for a long time, too much reliance on human knowledge engineering.",
                    "label": 0
                },
                {
                    "sent": "They have to write all these big grammars.",
                    "label": 0
                },
                {
                    "sent": "All grammars leak.",
                    "label": 0
                },
                {
                    "sent": "That was mentioned by Edward Sapir in 1921.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we carried it over to ontologies.",
                    "label": 0
                },
                {
                    "sent": "So in the end what we had in the 90s was a sour grape philosophy that also psychologists called denial of desire, denial of desire.",
                    "label": 0
                },
                {
                    "sent": "Very hungry.",
                    "label": 0
                },
                {
                    "sent": "You know this table are very hungry.",
                    "label": 0
                },
                {
                    "sent": "Fox walked into a vineyard where there was an ample supply of luscious looking grapes.",
                    "label": 0
                },
                {
                    "sent": "However, the grapes hung higher than the Fox could reach.",
                    "label": 0
                },
                {
                    "sent": "He jumped up and stretched and reached and jumped more to try to get those yummy grapes, but to no avail.",
                    "label": 0
                },
                {
                    "sent": "Those grapes surely must be sour.",
                    "label": 0
                },
                {
                    "sent": "He finally said I wouldn't eat them if they were served to me on a silver platter.",
                    "label": 0
                },
                {
                    "sent": "Moral of the story, it's easy to hate what you cannot have so and actually in the in the 90s we had exactly that story in both fields and knowledge technologies and the language, technologies and people got the research funds that really did very cheap tricks and the people who tried to do it the principled way they had to survive.",
                    "label": 0
                },
                {
                    "sent": "In somewhere, because they were called the non practical, the impractical, the dreamers.",
                    "label": 0
                },
                {
                    "sent": "The ones who go for sour grapes.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But why would it be good to drop deep processing in favor of shallow approaches?",
                    "label": 0
                },
                {
                    "sent": "Because in this way we can build useful applications already today, so in a way we did the same thing.",
                    "label": 0
                },
                {
                    "sent": "We also got research funding or funding.",
                    "label": 0
                },
                {
                    "sent": "Didn't dry out.",
                    "label": 0
                },
                {
                    "sent": "We did both.",
                    "label": 0
                },
                {
                    "sent": "We on the one hand we did went for the shallow, non principled methods in order to keep people happy and deliver something to industry.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we continued.",
                    "label": 0
                },
                {
                    "sent": "But this is really hard to get money for that you try with the EU to get money for real principle research.",
                    "label": 0
                },
                {
                    "sent": "So sometimes it works WHI.",
                    "label": 0
                },
                {
                    "sent": "Is it good to continue with the processing because it's the ultimate goal of computational linguistics wire?",
                    "label": 0
                },
                {
                    "sent": "I can tell you becausw if you work with these shallow and very shallow statistical systems.",
                    "label": 0
                },
                {
                    "sent": "For each application you need to start again.",
                    "label": 0
                },
                {
                    "sent": "You can you never have reusable knowledge, but what we want to get.",
                    "label": 0
                },
                {
                    "sent": "We want to get some system like for a domain like for a knowledge domain.",
                    "label": 0
                },
                {
                    "sent": "We want to get one system maybe for language, for the German language.",
                    "label": 0
                },
                {
                    "sent": "For the English language that we can reuse for many different applications and that's kind of the goal.",
                    "label": 0
                },
                {
                    "sent": "And also then we get consistency of the knowledge across.",
                    "label": 0
                },
                {
                    "sent": "Applique",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what has changed for knowledge processing since then?",
                    "label": 0
                },
                {
                    "sent": "You know what has changed, tractable subsets of 1st order logic?",
                    "label": 0
                },
                {
                    "sent": "Sometimes that also called the German School of Description Logics.",
                    "label": 0
                },
                {
                    "sent": "Something that DFK I was involved in and detailed catalogue of complexity of family.",
                    "label": 0
                },
                {
                    "sent": "So the complexity of the family of logics.",
                    "label": 0
                },
                {
                    "sent": "More efficient influencing technologies.",
                    "label": 0
                },
                {
                    "sent": "So more intuitive notations, editing tools, better knowledge engineering methods, methods for learning automatic.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ugly and so on in more demand.",
                    "label": 0
                },
                {
                    "sent": "What has changed for deep language processing?",
                    "label": 1
                },
                {
                    "sent": "Computationally?",
                    "label": 0
                },
                {
                    "sent": "More benign grammar formalisms.",
                    "label": 0
                },
                {
                    "sent": "Much more efficient parsing technologies, more intuitive notations and editing tools.",
                    "label": 1
                },
                {
                    "sent": "Better grammar engineering methods, machine learning, and stronger applications.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The round was actually pretty much the same, so in deep language you get 3 different traditions, which I will not bother you with.",
                    "label": 0
                },
                {
                    "sent": "But actually for all of these, for all of these areas there are now efficient and grammars and grammars with rather complex coverage.",
                    "label": 0
                },
                {
                    "sent": "They are not yet as good as the best shallow grammars that get more than 90% ninety 2% or so.",
                    "label": 0
                },
                {
                    "sent": "These are little below, but on the other hand they get you much more semantics.",
                    "label": 0
                },
                {
                    "sent": "So the cheap grammars that win the DARPA contest this years here in grammar evaluations.",
                    "label": 0
                },
                {
                    "sent": "They don't get you the real semantics, but these grammars that have a little bit.",
                    "label": 0
                },
                {
                    "sent": "There's nothing to do with darker now.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is only with the type of people who run in these contests, and because they're different games for some things you may need just a higher recall.",
                    "label": 0
                },
                {
                    "sent": "For others you need better precision.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and I won't go into this.",
                    "label": 0
                },
                {
                    "sent": "This is now a good getting.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Too much into nitty gritty.",
                    "label": 0
                },
                {
                    "sent": "So the big.",
                    "label": 0
                },
                {
                    "sent": "Difference.",
                    "label": 0
                },
                {
                    "sent": "Oh no.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I go into this so the dream of reusable linguistic knowledge has not been given up.",
                    "label": 0
                },
                {
                    "sent": "Even if the great majority of papers at large conferences, now dedicated to shallow, including statistical end table look up systems, some researchers are still trying to solve the much harder problem.",
                    "label": 0
                },
                {
                    "sent": "However, the problem is so complex that it takes large efforts and international collaboration.",
                    "label": 0
                },
                {
                    "sent": "Now several of these collaborations have formed.",
                    "label": 0
                },
                {
                    "sent": "One has formed starting from Palo Alto Research Center from Park in LFG and another one we formed today.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It was Stanford and Tokyo in the area of another grammar with HP SG.",
                    "label": 0
                },
                {
                    "sent": "But actually HPS.",
                    "label": 0
                },
                {
                    "sent": "She was already around 20 years ago because when I started at the FKII left IBM and there I worked with the person whose name you may know who's not here.",
                    "label": 0
                },
                {
                    "sent": "I think Cody Studer's here.",
                    "label": 0
                },
                {
                    "sent": "No, I work with Woody Shooter together and we worked in the same project and he was more in the semantic technology site and I was now I'm was more on the language processing side and we tried to do exactly the same squeeze meaning out of sentences and in one of the projects we got up to sentence 14.",
                    "label": 0
                },
                {
                    "sent": "With lots of handwork, but this only took three years and this was a different type of game from which we are now in.",
                    "label": 0
                },
                {
                    "sent": "Because now we apply data intensive methods and really finally get up.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "22 speed so this is the.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The participants in this, in this initiative, and it started with Stamfords, are booking and talk.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go and buy already buy.",
                    "label": 0
                },
                {
                    "sent": "Combining methods from participants and having our own technology evaluation and contests, we could boost the efficiency by a factor 2000.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So still not robust.",
                    "label": 0
                },
                {
                    "sent": "Still not robust.",
                    "label": 0
                },
                {
                    "sent": "We're still left with the problem of insufficient coverage.",
                    "label": 0
                },
                {
                    "sent": "Two remedies, one combining the accurate deprocessing with robust, not robots.",
                    "label": 0
                },
                {
                    "sent": "No, not with robots.",
                    "label": 0
                },
                {
                    "sent": "With robust shallow processing.",
                    "label": 0
                },
                {
                    "sent": "So combine the thing.",
                    "label": 0
                },
                {
                    "sent": "Have one system that gets you if it applies.",
                    "label": 0
                },
                {
                    "sent": "If it can parse, it gets you.",
                    "label": 0
                },
                {
                    "sent": "The precise unit gets a high accuracy, maybe 40% another domain.",
                    "label": 0
                },
                {
                    "sent": "60% and.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't parse, then you fall back on the mini part and still have a second shot.",
                    "label": 0
                },
                {
                    "sent": "You know you still get it, so of course you do things in parallel in order to save time, so that's one thing we did and the other one is improving coverage through data and.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Intensive learning methods and this is the end.",
                    "label": 1
                },
                {
                    "sent": "I'll come to the end.",
                    "label": 0
                },
                {
                    "sent": "In the first method in hybrid NLP we combined the processing with a number of shallow processing systems, put them together and this was a PhD thesis.",
                    "label": 1
                },
                {
                    "sent": "I supervise parolee Schaefer of last year called Heart of Gold is disjoined.",
                    "label": 0
                },
                {
                    "sent": "Architecture is now used at many sites and all components use the same annotation.",
                    "label": 1
                },
                {
                    "sent": "Multilevel standoff annotation that is the interface.",
                    "label": 0
                },
                {
                    "sent": "So you all all components one after the other, tries to annotate the sentence with the analysis and the language for annotation.",
                    "label": 0
                },
                {
                    "sent": "And thus the interface language is called minimal recursion semantics.",
                    "label": 0
                },
                {
                    "sent": "I won't say anything about it, but this is right now one of our major problems how to get this semantic formalism that was mainly developed at Stanford and is more suited for natural language to get this interface to the ontologies that we do in all this is quite a headache, but we're working on it.",
                    "label": 0
                },
                {
                    "sent": "Yeah?",
                    "label": 0
                },
                {
                    "sent": "So it's both semantics.",
                    "label": 0
                },
                {
                    "sent": "Both are semantics, but if you look at it slightly there.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the second thing is, another was another PhD thesis last year and there was a kind of a rather.",
                    "label": 0
                },
                {
                    "sent": "This was quite an important result, Zanghi, who was able to prove 1st that the best English grammar in HP SG written by then flick, injure and others at Stanford.",
                    "label": 0
                },
                {
                    "sent": "the English resource grammar.",
                    "label": 0
                },
                {
                    "sent": "Showing that most of the missing coverage could be traced actually to the lexicon, even if it didn't look like it, because very often the word was in, but not quite in the same meaning in the same frame in the same subcategorization frame.",
                    "label": 0
                },
                {
                    "sent": "So you could show that 84% of all missing sentences in coverage of this grammar could be attributed to.",
                    "label": 0
                },
                {
                    "sent": "Missing lexical lexical types.",
                    "label": 0
                },
                {
                    "sent": "Sometimes semantic types, so then he he learned these types by large corpora and was able to get the coverage up by another 20% and now the English Resource Grammar uses this method.",
                    "label": 0
                },
                {
                    "sent": "Actually it's no use.",
                    "label": 0
                },
                {
                    "sent": "But now, why didn't I put in the absolute numbers?",
                    "label": 0
                },
                {
                    "sent": "Because they're still shameful.",
                    "label": 0
                },
                {
                    "sent": "Of course we get.",
                    "label": 0
                },
                {
                    "sent": "We get between 60 and 80%.",
                    "label": 0
                },
                {
                    "sent": "We don't even get close to the 91% of the shallow parsing people.",
                    "label": 0
                },
                {
                    "sent": "That's still true when we have a semantics as output and they don't.",
                    "label": 0
                },
                {
                    "sent": "That's a difference and we get some relief.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things that they cannot get, so conclusion and outlook.",
                    "label": 0
                },
                {
                    "sent": "And then let's finish with it.",
                    "label": 0
                },
                {
                    "sent": "Yes, there has been progress, even if it has been slow.",
                    "label": 0
                },
                {
                    "sent": "And doing things right and not giving up the more demanding, principled ways is in the end, paying off.",
                    "label": 0
                },
                {
                    "sent": "Both in the area of knowledge technologies and linguistic technologies, the high hanging grapes actually are quite sweet.",
                    "label": 0
                },
                {
                    "sent": "Quite sweet, but then the harvest season has not even started yet.",
                    "label": 0
                },
                {
                    "sent": "Let's do this together.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I think we have time for a couple of questions for hands so any questions.",
                    "label": 0
                },
                {
                    "sent": "Actually, do we have a roving microphone?",
                    "label": 0
                },
                {
                    "sent": "OK, great.",
                    "label": 0
                },
                {
                    "sent": "Cancel.",
                    "label": 0
                },
                {
                    "sent": "Any questions for hands?",
                    "label": 0
                },
                {
                    "sent": "OK, so while people thinking of their questions let me start with the question.",
                    "label": 0
                },
                {
                    "sent": "One of the reasons for taking the shadow or less principled approach is sometimes said to be the issue of scaleability.",
                    "label": 0
                },
                {
                    "sent": "You know what?",
                    "label": 0
                },
                {
                    "sent": "How watermarks would you make about about that?",
                    "label": 0
                },
                {
                    "sent": "No, that's that's of course true.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's exactly the case if you want to.",
                    "label": 0
                },
                {
                    "sent": "I'm not in other.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, our funding has not dried out, so we have also used shallow methods quite extensively.",
                    "label": 0
                },
                {
                    "sent": "If you get something that should scale very fast.",
                    "label": 0
                },
                {
                    "sent": "That should learn very fast and you cannot really invest ways and doing it the principled way I think for the next few years I would still recommend to use shallower systems and depending on the type of task, I would either recommend to use a very shallow system or to use something like mini Park or the Stanford parser or the new dependency parsers.",
                    "label": 0
                },
                {
                    "sent": "There's a whole generation of very exciting new things and pricing technologies over the last three years.",
                    "label": 0
                },
                {
                    "sent": "And with very very good results, go to these things, use them and you get faster results because they are off the shelf, you can use them.",
                    "label": 0
                },
                {
                    "sent": "They are distributed freely and so on.",
                    "label": 0
                },
                {
                    "sent": "If you get if you get a domain and you know this domain is really important to you, you will be working on it for awhile.",
                    "label": 0
                },
                {
                    "sent": "It's not some contract that you try to finish off in a 3 months and four months and so and if you have the time to do it right then I would say try it with a try.",
                    "label": 0
                },
                {
                    "sent": "I mean I cannot promise that it would work for everything.",
                    "label": 0
                },
                {
                    "sent": "It turns out we tried the English Resource Grammar and our version the we did a DFK the German to Japanese grammar that's now used at Nyct and entity with German and Japanese.",
                    "label": 0
                },
                {
                    "sent": "And sometimes it works and their other domains.",
                    "label": 0
                },
                {
                    "sent": "For which it doesn't work, and so we're not using it in all domains ourselves, because exactly or scalability, but sometimes it does.",
                    "label": 0
                },
                {
                    "sent": "Yeah sure OK thanks.",
                    "label": 0
                },
                {
                    "sent": "Any further questions for hands.",
                    "label": 0
                },
                {
                    "sent": "Last chance.",
                    "label": 0
                },
                {
                    "sent": "OK, in that case, thank you very much, Hans.",
                    "label": 0
                }
            ]
        }
    }
}