{
    "id": "zjmt5znh5hyeqh6tmhoylus7jkfwltfs",
    "title": "lil\u2019 UCB: An Optimal Exploration Algorithm for Multi-Armed Bandits",
    "info": {
        "author": [
            "Kevin Jamieson, Department of Electrical and Computer Engineering, University of Wisconsin-Madison"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_jamieson_ucb/",
    "segmentation": [
        [
            "So yeah, this is.",
            "Co authors are bare metal.",
            "Robert Novak social back, who is in the audience."
        ],
        [
            "Think about the best arm problem.",
            "This is stochastic band bandit problem.",
            "We have each pull of an arm is XIJI arm where each realization has mean UI and the sub Gaussian realization.",
            "So you can assume say a Gaussian distribution reach arm an.",
            "The game proceeds as at each time the algorithm will choose an action, some action within one through N an arm I receive their award and then the algorithm chooses to either stop.",
            "Or keep going and if it stops as the output some some arm that it thinks is the best arm.",
            "And so the goal for a given Delta a comparable to what you're trying to do, is come up with an algorithm, outputs and I hat that takes a few samples as possible.",
            "An output is the best drama, probably at least one minus Delta.",
            "So basically what we're doing is looking at all at the set of algorithms that output.",
            "The best arm was probably 1 minus Delta, and we're trying to find one that takes as few samples as possible.",
            "With loss of generality without loss of generality, assume that the means are ordered."
        ],
        [
            "Where the best arm is the first arm.",
            "So let's take a look at how hard this problem is.",
            "Let's just consider the two arm case.",
            "This is the case.",
            "The only cop and looked at yesterday, so go this this pretty quickly.",
            "After you take em Sammy, you sampled both arms just M times.",
            "Then we know that these they go down like 1 / sqrt M They converge there mean like that and if we knew the gap ahead of time we could choose M to be proportional to the inverse gap squared and we would know this is sufficiently number of samples to decide that fluctuations are small and we could accurately predict the best arm with this number of poles.",
            "Conversely, we know this scale is also necessary to just big weight between the arms.",
            "This policy Fano or by Emily results showing yesterday.",
            "In general, if you have an arms matters sequence showed that if to determine the best what the best arm it takes, the sum of the inverse squared times log one over Delta, where Delta I is.",
            "The gap between the best and the arm.",
            "However, this is pretty optimistic, because as I kind of alluded to on the 1st slide, it's a random stopping time.",
            "The algorithms to choose when to stop because it has to be correct for all possible problems.",
            "You're souping over all possible means it has to output the right arm.",
            "So that means that you cannot have a deterministic stopping time.",
            "And so, because you have to be adaptive, these unknown means.",
            "It seems a little harder and you can't just choose M like this beforehand, and so is there some price to being adaptive.",
            "Turns out that yes, there is."
        ],
        [
            "And the price is basically this log log factor, so this result is actually found by result by Ferrell in 60, four who was looking at sequential decision problems and basically was looking at a discrete random walk of a biased Gaussian increments.",
            "And he was just trying to figure out the sign of this bias and but basically it's a direct corollary of that result and it says that if the expected number of measurements does not scale, at least like inverse gap squared log, inverse gap squared, you will have.",
            "Probably very greater than Delta.",
            "It's a consequence of the Lavater logarithm, and this is a lot of results from coming out lately for finite versions of this, and we're going to get one ourselves.",
            "Basically what it says is that if you have your XYZ that are Gaussian 01 or really just mean zero and variance one, then this random walk.",
            "This somehow scales about like sort of two T log T. So recapping what we just saw these two lower bounds together.",
            "The man is equals result, which is basically a if you kind of know the gaps ahead of time.",
            "You're just trying to figure out what is low around more information theoretic in that sense versus this sequential caring about how much over this random stopping time we see that we should expect upper bounds to look more or less like inverse gap squared.",
            "Log log, inverse gap squared over Delta.",
            "This is what we would expect to see because in the case where any school 2.",
            "This is a lower bound."
        ],
        [
            "So in the literature there's been a bunch of work for this problem.",
            "Of course it's a very fundamental problem and the first solutions proposed back in 64 that I could find, and they looked at Gaussian arms and they basically they derive to propose an algorithm that is very much like successful nation of evendarr manner and in store, and they get this inverse squared log, inverse gap squared times N over Delta.",
            "Moody bear bubeck Ann Munoz came up an algorithm for successive jacks, but then they also proposed this USV type like algorithm and it's very easy to turn this into a fixed confidence algorithm.",
            "And if you do that, you get this.",
            "Basically the same rate up to scaling.",
            "And then calendar Krishnan, Toria or Stone came up LUCB, which is a really giant for the N best arms.",
            "But you can look at the first best arm and it works very well and they're basically the same rate, but all these results are pretty far from optimal because they're getting they're getting a result that scales with log in in all of them, and we're trying to get rid of that log in because it's just not there.",
            "So last year myself, and some coauthors came up with an algorithm that removes this log in and turned it basically into log log factors.",
            "That 2nd result there and around the same time conference MIC have a result that finally hit the lower bound and we were very happy.",
            "And yeah, we thought that was sort of the end of the story.",
            "However, when you simulate this and actually do this in simulation, you see that these constants that are in these last two algorithms are enormous.",
            "They both use an algorithm called Media nomination that was also drive in the evendarr Mr Result.",
            "It's been cited multiple times throughout this conference and they basically they problem with these logins.",
            "All these algorithms they get the reason that login shows up is because they're trying to do with large deviations of the empirical Max of Gaussians or endure realizations, and So what medium location does it uses?",
            "A better estimator that gets away from these these large deviations.",
            "Unfortunately at the cost of very large constants.",
            "So we want to do in this paper.",
            "In this work was basically come up with an algorithm that was achieved the optimal rate, but also was very effective in practice, meaning it was superior in both theory and practice and the goal and the idea was basically to start with a works really well in practice and then tweak it."
        ],
        [
            "Until it is optimal.",
            "So you see, probably doesn't need much introduction here, but I'll get there real quick.",
            "Basically, some in notation TFT is the number of times the other arm has been pulled up to time T an at each time T we're going to pull the arm that has highest empirical mean plus some confidence bound by TB.",
            "It is basically a confidence band that goes down more or less like 1 / sqrt T or square root of TOT and then some constant may depend on or T or TOT.",
            "So the contributions here are we're going to derive a new version of BT that actually takes into account the Lavater logarithm, because we know it's there.",
            "We know this inverse catalog is there, so let's let's use that in our comments found directly in.",
            "Second, we're going to cover the new stopping time.",
            "That's actually the most interesting part of this work.",
            "I think that gets around having to union bound over all the realizations of the arms that has been basically the other use of stopping times they.",
            "These analysis is typically look at.",
            "Saying that all the arms are well behaved around their means and then they union bound.",
            "But we're not going to do that.",
            "So to do this to come from the new BT, we had to come up with confidence bound and basically this is just a finite version of the Lavater logarithm we have for any epsilon we have a bound that looks very much like the limit for more than synthetic form of the library logarithm up to constants, and the epsilon just cancel it.",
            "Tough because it depends on T and Delta.",
            "So it really depends on the range they interested in.",
            "But if you pick 10 epsilon .01, you get this result that's basically off by, you know, the two and a half should be a 2.",
            "The 5000 should be a one.",
            "But you know it's inside a log, so it is actually quite tight.",
            "It works pretty well.",
            "Similar bounds are also independent drive by Kaufman at all, an mostly by about Soobramoney, which which actually send a lot of these results doing martingales and Bernstein type bounds.",
            "Very interesting stuff there."
        ],
        [
            "Then the archive.",
            "So OK, so now our algorithm is going to be inside.",
            "The BIT is going to be 2 times this U function.",
            "That is the conference boundary just drived OK.",
            "Note that there's no N in the upper confidence bound.",
            "There is no T in the upper.",
            "Confidence bound, is just, it's just use the number of times that arm has been pulled.",
            "It's really important.",
            "So like I mentioned before, the typical analysis in this area use a stopping criterion that looks at the highest empirical mean.",
            "He looks at a confidence bound when that confidence bound is higher than the upper confidence bounds of all the other algorithms.",
            "They stop very intuitive and successful approach makes a lot of sense, but unfortunately you can show you can argue that if you have Gaussian realizations, for instance.",
            "You cannot.",
            "You actually have to have these comments on scale.",
            "I swear to log in and to see this.",
            "The Max of an Gaussian skirts scales like spirit of log in.",
            "So if you just after the first time here, once you pull down their arms once, because if your means or bring zero and one I can pick.",
            "I can find and large enough so that squared login totally dwarfs any of the mean affects and so that's where does login in the confidence bounds will translate into a log in in the final round, maybe suboptimal.",
            "So we did the new thing here is basically we propose a new stopping time and the stopping time seems actually quite wasteful at first.",
            "Basically what it says is I want to find.",
            "I want to find an arm I such that it is pulled more than a constant times the sum of all the other arms combined.",
            "OK, this sounds really wasteful.",
            "It could take a long time for this happen, but in you know when you really think about it, which will go into the second.",
            "It really actually doesn't take that much.",
            "Not that wasteful with maybe like a constant effect, a constant factor off of the optimal."
        ],
        [
            "And so with we set our Lambda here as a stopping condition.",
            "You typically said like three or four times is the sum of all the other arms, and you get this theorem that is the optimal sample complexity that was the same result of the exponential gap of the.",
            "Karnan at all paper.",
            "The analysis is.",
            "Can broken up into 2 steps.",
            "Basically want to bound the number of measurements from the suboptimal arms?",
            "The intuition is here is basically recall that the ARM service the means are sorted.",
            "You condition on the best arm being well conditioned, meaning it's realizations are close to its mean, and then you look at the min S such that for the arm plus some confidence for the ice arm is less than or equal to mu one, so that mute you factor there is a little the confidence bound and you can show condition on the 1st arm realizations being well behaved that this behaves like an independent selection.",
            "In a variable and then you just apply a concentration of measure result for the sum of exponential IID.",
            "Select eventually.",
            "They are independent.",
            "Second step is to prove the stopping condition an intrusion here.",
            "This isn't exactly how the proof goes, but the intuition is that the since the first arm is the best mean.",
            "The probability that the second arm is pulled more than a constant times the first arm is less than Delta.",
            "The probability that the third arm is pulled more than the 1st and 2nd ARM is less than Delta squared.",
            "The probability that was pulled more than you know the 1st three arms, lesson, Delta cubes and so on and so forth, and so it sums over and you can get this final result that the sub optimal arm will never be pulled more than the sum of.",
            "All the other arms that have greater than or equal mean to it."
        ],
        [
            "So we get our result.",
            "So looking back again, we just fit in at the bottom here equal to the car and inquiring symmetras ult and so looking at this it's like what's the big deal with the same theory."
        ],
        [
            "Ann, really our whole motivation here was to come up with practical algorithm an these plots are small, so I just want to point out a couple of things not focus on these.",
            "Is that the nonadaptive here?",
            "This is just random sampling.",
            "This is blue.",
            "It's hard to see.",
            "But these curves are right.",
            "Here is blue.",
            "This is blue here.",
            "This is blue here and then the corner result using this mean elimination result is basically tracking the nonadaptive an the reason why this is just the constants in media management, huge and for any real problem it is not going to work.",
            "Now for a sensible donation.",
            "Is the red line there, and LUCB is the purple, an LCD performed surprisingly well.",
            "Very, very good, and the Scion curves are different variations of our algorithm.",
            "That the bottom line is doing the best.",
            "Is that really just heuristically setting?",
            "We know theory for it, but it works very well and basically the takeaway is that we're performing very comparably."
        ],
        [
            "The competition.",
            "We also looked at any time performance, so if you Dad put an arm anytime, how good are we doing?",
            "Is the probability of error out putting the wrong arm an?",
            "I'm sorry the colors have changed a little bit now.",
            "The LTV is now yellow and were purple and Gray apparently, and so we upper and lower bound the LCB algorithm, so we're very competitive with that."
        ],
        [
            "So in summary, we frozen algorithm is optimal in both theory and very compareable, and in practice, and we're pretty happy with it.",
            "Wasn't open questions.",
            "We look at the top arm just one best.",
            "What about top M arms like top five or 10?",
            "All the best algorithm for this problem?",
            "Have login factors in them and we'd like to come up with the procedure that can also remove login here.",
            "Given optimal sample complexity.",
            "And finally we just looked at the gaps and means for Gaussians.",
            "This is often more gallons of the same variance is optimal, but for.",
            "Renew is with small variances.",
            "There's things like this looking kill, diversions or variance can have a huge effect.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, this is.",
                    "label": 0
                },
                {
                    "sent": "Co authors are bare metal.",
                    "label": 0
                },
                {
                    "sent": "Robert Novak social back, who is in the audience.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think about the best arm problem.",
                    "label": 0
                },
                {
                    "sent": "This is stochastic band bandit problem.",
                    "label": 0
                },
                {
                    "sent": "We have each pull of an arm is XIJI arm where each realization has mean UI and the sub Gaussian realization.",
                    "label": 0
                },
                {
                    "sent": "So you can assume say a Gaussian distribution reach arm an.",
                    "label": 0
                },
                {
                    "sent": "The game proceeds as at each time the algorithm will choose an action, some action within one through N an arm I receive their award and then the algorithm chooses to either stop.",
                    "label": 0
                },
                {
                    "sent": "Or keep going and if it stops as the output some some arm that it thinks is the best arm.",
                    "label": 0
                },
                {
                    "sent": "And so the goal for a given Delta a comparable to what you're trying to do, is come up with an algorithm, outputs and I hat that takes a few samples as possible.",
                    "label": 0
                },
                {
                    "sent": "An output is the best drama, probably at least one minus Delta.",
                    "label": 0
                },
                {
                    "sent": "So basically what we're doing is looking at all at the set of algorithms that output.",
                    "label": 0
                },
                {
                    "sent": "The best arm was probably 1 minus Delta, and we're trying to find one that takes as few samples as possible.",
                    "label": 0
                },
                {
                    "sent": "With loss of generality without loss of generality, assume that the means are ordered.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where the best arm is the first arm.",
                    "label": 0
                },
                {
                    "sent": "So let's take a look at how hard this problem is.",
                    "label": 0
                },
                {
                    "sent": "Let's just consider the two arm case.",
                    "label": 0
                },
                {
                    "sent": "This is the case.",
                    "label": 0
                },
                {
                    "sent": "The only cop and looked at yesterday, so go this this pretty quickly.",
                    "label": 0
                },
                {
                    "sent": "After you take em Sammy, you sampled both arms just M times.",
                    "label": 0
                },
                {
                    "sent": "Then we know that these they go down like 1 / sqrt M They converge there mean like that and if we knew the gap ahead of time we could choose M to be proportional to the inverse gap squared and we would know this is sufficiently number of samples to decide that fluctuations are small and we could accurately predict the best arm with this number of poles.",
                    "label": 0
                },
                {
                    "sent": "Conversely, we know this scale is also necessary to just big weight between the arms.",
                    "label": 0
                },
                {
                    "sent": "This policy Fano or by Emily results showing yesterday.",
                    "label": 0
                },
                {
                    "sent": "In general, if you have an arms matters sequence showed that if to determine the best what the best arm it takes, the sum of the inverse squared times log one over Delta, where Delta I is.",
                    "label": 0
                },
                {
                    "sent": "The gap between the best and the arm.",
                    "label": 0
                },
                {
                    "sent": "However, this is pretty optimistic, because as I kind of alluded to on the 1st slide, it's a random stopping time.",
                    "label": 0
                },
                {
                    "sent": "The algorithms to choose when to stop because it has to be correct for all possible problems.",
                    "label": 0
                },
                {
                    "sent": "You're souping over all possible means it has to output the right arm.",
                    "label": 0
                },
                {
                    "sent": "So that means that you cannot have a deterministic stopping time.",
                    "label": 0
                },
                {
                    "sent": "And so, because you have to be adaptive, these unknown means.",
                    "label": 0
                },
                {
                    "sent": "It seems a little harder and you can't just choose M like this beforehand, and so is there some price to being adaptive.",
                    "label": 0
                },
                {
                    "sent": "Turns out that yes, there is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the price is basically this log log factor, so this result is actually found by result by Ferrell in 60, four who was looking at sequential decision problems and basically was looking at a discrete random walk of a biased Gaussian increments.",
                    "label": 0
                },
                {
                    "sent": "And he was just trying to figure out the sign of this bias and but basically it's a direct corollary of that result and it says that if the expected number of measurements does not scale, at least like inverse gap squared log, inverse gap squared, you will have.",
                    "label": 0
                },
                {
                    "sent": "Probably very greater than Delta.",
                    "label": 0
                },
                {
                    "sent": "It's a consequence of the Lavater logarithm, and this is a lot of results from coming out lately for finite versions of this, and we're going to get one ourselves.",
                    "label": 0
                },
                {
                    "sent": "Basically what it says is that if you have your XYZ that are Gaussian 01 or really just mean zero and variance one, then this random walk.",
                    "label": 0
                },
                {
                    "sent": "This somehow scales about like sort of two T log T. So recapping what we just saw these two lower bounds together.",
                    "label": 0
                },
                {
                    "sent": "The man is equals result, which is basically a if you kind of know the gaps ahead of time.",
                    "label": 0
                },
                {
                    "sent": "You're just trying to figure out what is low around more information theoretic in that sense versus this sequential caring about how much over this random stopping time we see that we should expect upper bounds to look more or less like inverse gap squared.",
                    "label": 0
                },
                {
                    "sent": "Log log, inverse gap squared over Delta.",
                    "label": 0
                },
                {
                    "sent": "This is what we would expect to see because in the case where any school 2.",
                    "label": 0
                },
                {
                    "sent": "This is a lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the literature there's been a bunch of work for this problem.",
                    "label": 1
                },
                {
                    "sent": "Of course it's a very fundamental problem and the first solutions proposed back in 64 that I could find, and they looked at Gaussian arms and they basically they derive to propose an algorithm that is very much like successful nation of evendarr manner and in store, and they get this inverse squared log, inverse gap squared times N over Delta.",
                    "label": 0
                },
                {
                    "sent": "Moody bear bubeck Ann Munoz came up an algorithm for successive jacks, but then they also proposed this USV type like algorithm and it's very easy to turn this into a fixed confidence algorithm.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, you get this.",
                    "label": 0
                },
                {
                    "sent": "Basically the same rate up to scaling.",
                    "label": 0
                },
                {
                    "sent": "And then calendar Krishnan, Toria or Stone came up LUCB, which is a really giant for the N best arms.",
                    "label": 0
                },
                {
                    "sent": "But you can look at the first best arm and it works very well and they're basically the same rate, but all these results are pretty far from optimal because they're getting they're getting a result that scales with log in in all of them, and we're trying to get rid of that log in because it's just not there.",
                    "label": 0
                },
                {
                    "sent": "So last year myself, and some coauthors came up with an algorithm that removes this log in and turned it basically into log log factors.",
                    "label": 0
                },
                {
                    "sent": "That 2nd result there and around the same time conference MIC have a result that finally hit the lower bound and we were very happy.",
                    "label": 0
                },
                {
                    "sent": "And yeah, we thought that was sort of the end of the story.",
                    "label": 0
                },
                {
                    "sent": "However, when you simulate this and actually do this in simulation, you see that these constants that are in these last two algorithms are enormous.",
                    "label": 0
                },
                {
                    "sent": "They both use an algorithm called Media nomination that was also drive in the evendarr Mr Result.",
                    "label": 0
                },
                {
                    "sent": "It's been cited multiple times throughout this conference and they basically they problem with these logins.",
                    "label": 0
                },
                {
                    "sent": "All these algorithms they get the reason that login shows up is because they're trying to do with large deviations of the empirical Max of Gaussians or endure realizations, and So what medium location does it uses?",
                    "label": 1
                },
                {
                    "sent": "A better estimator that gets away from these these large deviations.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately at the cost of very large constants.",
                    "label": 1
                },
                {
                    "sent": "So we want to do in this paper.",
                    "label": 0
                },
                {
                    "sent": "In this work was basically come up with an algorithm that was achieved the optimal rate, but also was very effective in practice, meaning it was superior in both theory and practice and the goal and the idea was basically to start with a works really well in practice and then tweak it.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Until it is optimal.",
                    "label": 0
                },
                {
                    "sent": "So you see, probably doesn't need much introduction here, but I'll get there real quick.",
                    "label": 0
                },
                {
                    "sent": "Basically, some in notation TFT is the number of times the other arm has been pulled up to time T an at each time T we're going to pull the arm that has highest empirical mean plus some confidence bound by TB.",
                    "label": 1
                },
                {
                    "sent": "It is basically a confidence band that goes down more or less like 1 / sqrt T or square root of TOT and then some constant may depend on or T or TOT.",
                    "label": 0
                },
                {
                    "sent": "So the contributions here are we're going to derive a new version of BT that actually takes into account the Lavater logarithm, because we know it's there.",
                    "label": 0
                },
                {
                    "sent": "We know this inverse catalog is there, so let's let's use that in our comments found directly in.",
                    "label": 1
                },
                {
                    "sent": "Second, we're going to cover the new stopping time.",
                    "label": 0
                },
                {
                    "sent": "That's actually the most interesting part of this work.",
                    "label": 0
                },
                {
                    "sent": "I think that gets around having to union bound over all the realizations of the arms that has been basically the other use of stopping times they.",
                    "label": 0
                },
                {
                    "sent": "These analysis is typically look at.",
                    "label": 1
                },
                {
                    "sent": "Saying that all the arms are well behaved around their means and then they union bound.",
                    "label": 0
                },
                {
                    "sent": "But we're not going to do that.",
                    "label": 0
                },
                {
                    "sent": "So to do this to come from the new BT, we had to come up with confidence bound and basically this is just a finite version of the Lavater logarithm we have for any epsilon we have a bound that looks very much like the limit for more than synthetic form of the library logarithm up to constants, and the epsilon just cancel it.",
                    "label": 0
                },
                {
                    "sent": "Tough because it depends on T and Delta.",
                    "label": 0
                },
                {
                    "sent": "So it really depends on the range they interested in.",
                    "label": 0
                },
                {
                    "sent": "But if you pick 10 epsilon .01, you get this result that's basically off by, you know, the two and a half should be a 2.",
                    "label": 0
                },
                {
                    "sent": "The 5000 should be a one.",
                    "label": 0
                },
                {
                    "sent": "But you know it's inside a log, so it is actually quite tight.",
                    "label": 0
                },
                {
                    "sent": "It works pretty well.",
                    "label": 0
                },
                {
                    "sent": "Similar bounds are also independent drive by Kaufman at all, an mostly by about Soobramoney, which which actually send a lot of these results doing martingales and Bernstein type bounds.",
                    "label": 0
                },
                {
                    "sent": "Very interesting stuff there.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the archive.",
                    "label": 0
                },
                {
                    "sent": "So OK, so now our algorithm is going to be inside.",
                    "label": 0
                },
                {
                    "sent": "The BIT is going to be 2 times this U function.",
                    "label": 0
                },
                {
                    "sent": "That is the conference boundary just drived OK.",
                    "label": 1
                },
                {
                    "sent": "Note that there's no N in the upper confidence bound.",
                    "label": 0
                },
                {
                    "sent": "There is no T in the upper.",
                    "label": 0
                },
                {
                    "sent": "Confidence bound, is just, it's just use the number of times that arm has been pulled.",
                    "label": 1
                },
                {
                    "sent": "It's really important.",
                    "label": 0
                },
                {
                    "sent": "So like I mentioned before, the typical analysis in this area use a stopping criterion that looks at the highest empirical mean.",
                    "label": 0
                },
                {
                    "sent": "He looks at a confidence bound when that confidence bound is higher than the upper confidence bounds of all the other algorithms.",
                    "label": 0
                },
                {
                    "sent": "They stop very intuitive and successful approach makes a lot of sense, but unfortunately you can show you can argue that if you have Gaussian realizations, for instance.",
                    "label": 0
                },
                {
                    "sent": "You cannot.",
                    "label": 0
                },
                {
                    "sent": "You actually have to have these comments on scale.",
                    "label": 0
                },
                {
                    "sent": "I swear to log in and to see this.",
                    "label": 0
                },
                {
                    "sent": "The Max of an Gaussian skirts scales like spirit of log in.",
                    "label": 0
                },
                {
                    "sent": "So if you just after the first time here, once you pull down their arms once, because if your means or bring zero and one I can pick.",
                    "label": 0
                },
                {
                    "sent": "I can find and large enough so that squared login totally dwarfs any of the mean affects and so that's where does login in the confidence bounds will translate into a log in in the final round, maybe suboptimal.",
                    "label": 0
                },
                {
                    "sent": "So we did the new thing here is basically we propose a new stopping time and the stopping time seems actually quite wasteful at first.",
                    "label": 0
                },
                {
                    "sent": "Basically what it says is I want to find.",
                    "label": 0
                },
                {
                    "sent": "I want to find an arm I such that it is pulled more than a constant times the sum of all the other arms combined.",
                    "label": 1
                },
                {
                    "sent": "OK, this sounds really wasteful.",
                    "label": 0
                },
                {
                    "sent": "It could take a long time for this happen, but in you know when you really think about it, which will go into the second.",
                    "label": 0
                },
                {
                    "sent": "It really actually doesn't take that much.",
                    "label": 0
                },
                {
                    "sent": "Not that wasteful with maybe like a constant effect, a constant factor off of the optimal.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so with we set our Lambda here as a stopping condition.",
                    "label": 0
                },
                {
                    "sent": "You typically said like three or four times is the sum of all the other arms, and you get this theorem that is the optimal sample complexity that was the same result of the exponential gap of the.",
                    "label": 0
                },
                {
                    "sent": "Karnan at all paper.",
                    "label": 0
                },
                {
                    "sent": "The analysis is.",
                    "label": 0
                },
                {
                    "sent": "Can broken up into 2 steps.",
                    "label": 0
                },
                {
                    "sent": "Basically want to bound the number of measurements from the suboptimal arms?",
                    "label": 0
                },
                {
                    "sent": "The intuition is here is basically recall that the ARM service the means are sorted.",
                    "label": 0
                },
                {
                    "sent": "You condition on the best arm being well conditioned, meaning it's realizations are close to its mean, and then you look at the min S such that for the arm plus some confidence for the ice arm is less than or equal to mu one, so that mute you factor there is a little the confidence bound and you can show condition on the 1st arm realizations being well behaved that this behaves like an independent selection.",
                    "label": 0
                },
                {
                    "sent": "In a variable and then you just apply a concentration of measure result for the sum of exponential IID.",
                    "label": 0
                },
                {
                    "sent": "Select eventually.",
                    "label": 0
                },
                {
                    "sent": "They are independent.",
                    "label": 0
                },
                {
                    "sent": "Second step is to prove the stopping condition an intrusion here.",
                    "label": 0
                },
                {
                    "sent": "This isn't exactly how the proof goes, but the intuition is that the since the first arm is the best mean.",
                    "label": 0
                },
                {
                    "sent": "The probability that the second arm is pulled more than a constant times the first arm is less than Delta.",
                    "label": 0
                },
                {
                    "sent": "The probability that the third arm is pulled more than the 1st and 2nd ARM is less than Delta squared.",
                    "label": 0
                },
                {
                    "sent": "The probability that was pulled more than you know the 1st three arms, lesson, Delta cubes and so on and so forth, and so it sums over and you can get this final result that the sub optimal arm will never be pulled more than the sum of.",
                    "label": 0
                },
                {
                    "sent": "All the other arms that have greater than or equal mean to it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we get our result.",
                    "label": 0
                },
                {
                    "sent": "So looking back again, we just fit in at the bottom here equal to the car and inquiring symmetras ult and so looking at this it's like what's the big deal with the same theory.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ann, really our whole motivation here was to come up with practical algorithm an these plots are small, so I just want to point out a couple of things not focus on these.",
                    "label": 0
                },
                {
                    "sent": "Is that the nonadaptive here?",
                    "label": 0
                },
                {
                    "sent": "This is just random sampling.",
                    "label": 0
                },
                {
                    "sent": "This is blue.",
                    "label": 0
                },
                {
                    "sent": "It's hard to see.",
                    "label": 0
                },
                {
                    "sent": "But these curves are right.",
                    "label": 0
                },
                {
                    "sent": "Here is blue.",
                    "label": 0
                },
                {
                    "sent": "This is blue here.",
                    "label": 0
                },
                {
                    "sent": "This is blue here and then the corner result using this mean elimination result is basically tracking the nonadaptive an the reason why this is just the constants in media management, huge and for any real problem it is not going to work.",
                    "label": 0
                },
                {
                    "sent": "Now for a sensible donation.",
                    "label": 0
                },
                {
                    "sent": "Is the red line there, and LUCB is the purple, an LCD performed surprisingly well.",
                    "label": 0
                },
                {
                    "sent": "Very, very good, and the Scion curves are different variations of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "That the bottom line is doing the best.",
                    "label": 0
                },
                {
                    "sent": "Is that really just heuristically setting?",
                    "label": 0
                },
                {
                    "sent": "We know theory for it, but it works very well and basically the takeaway is that we're performing very comparably.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The competition.",
                    "label": 0
                },
                {
                    "sent": "We also looked at any time performance, so if you Dad put an arm anytime, how good are we doing?",
                    "label": 0
                },
                {
                    "sent": "Is the probability of error out putting the wrong arm an?",
                    "label": 0
                },
                {
                    "sent": "I'm sorry the colors have changed a little bit now.",
                    "label": 0
                },
                {
                    "sent": "The LTV is now yellow and were purple and Gray apparently, and so we upper and lower bound the LCB algorithm, so we're very competitive with that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary, we frozen algorithm is optimal in both theory and very compareable, and in practice, and we're pretty happy with it.",
                    "label": 0
                },
                {
                    "sent": "Wasn't open questions.",
                    "label": 0
                },
                {
                    "sent": "We look at the top arm just one best.",
                    "label": 0
                },
                {
                    "sent": "What about top M arms like top five or 10?",
                    "label": 0
                },
                {
                    "sent": "All the best algorithm for this problem?",
                    "label": 1
                },
                {
                    "sent": "Have login factors in them and we'd like to come up with the procedure that can also remove login here.",
                    "label": 0
                },
                {
                    "sent": "Given optimal sample complexity.",
                    "label": 0
                },
                {
                    "sent": "And finally we just looked at the gaps and means for Gaussians.",
                    "label": 0
                },
                {
                    "sent": "This is often more gallons of the same variance is optimal, but for.",
                    "label": 0
                },
                {
                    "sent": "Renew is with small variances.",
                    "label": 1
                },
                {
                    "sent": "There's things like this looking kill, diversions or variance can have a huge effect.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}