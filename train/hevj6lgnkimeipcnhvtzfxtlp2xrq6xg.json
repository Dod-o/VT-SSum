{
    "id": "hevj6lgnkimeipcnhvtzfxtlp2xrq6xg",
    "title": "PRISM: PRincipled Implicit Shape Model",
    "info": {
        "author": [
            "Alain Lehmann, CALVIN, Computer Vision laboratory, ETH Zurich"
        ],
        "published": "Dec. 1, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/bmvc09_lehmann_prism/",
    "segmentation": [
        [
            "OK, welcome everybody to my presentation of praising the principled implicit shape model so this work is about optic class detection, where warnings given an image and the goal is to locate objects in the."
        ],
        [
            "Image so in other words, the goal is to find a more."
        ],
        [
            "Semantic description from a visual description of a scene.",
            "To do so, there are two commonly used paradigms.",
            "On the one hand, their assistance, inspired by the Hough transform and on the other hand there is the so-called sliding window paradigm.",
            "Now half transform systems for option class detection has been made popular by the implicit shape model.",
            "In these kind of system one starts with feature extraction in the image and each feature subsequently cast votes for possible object positions.",
            "Now these votes are accumulated in a voting space and the sum over all these vote."
        ],
        [
            "This indicates possible object positions.",
            "The with."
        ],
        [
            "Think this is rather natural procedure to detect objects.",
            "However, the model defined by SM is rather constraint.",
            "It builds on densities which do not allow for negative votes, which are important to discriminate certain positions more over the summation over all the votes is explained by marginalization, which is not really applicable since we do observe all the features.",
            "So they're all facts.",
            "On the other hand, we have.",
            "Sliding window were the idea is to slide windows of various sizes over every position in the image.",
            "Crop out a given sub image and decide for each individually whether it contains a car or not.",
            "So we think this is a rather clean reasoning.",
            "It also allows for very flexible models, including discriminative learning, so negative weights.",
            "However, this exhaustive search overall positions is not very natural.",
            "Now Prince."
        ],
        [
            "Is about combining this to worlds about giving a sound justification to the natural voting procedure by following the clean sliding window reasoning.",
            "In addition, we also get more flexibility in the model which allows for discriminative so negative votes.",
            "Now let's"
        ],
        [
            "Go a little bit more into detail or of the sliding window procedure, so the idea there is to fix the single window single hypothesis.",
            "Crop out the corresponding sub image and compare that to an object model which allows to compute a score.",
            "More generally, one can say that the idea is to compute a scene independent object description, which we're going to call the object footprint later on.",
            "Remark here that this scene independent description is important and it is not really explicitly defined in ISMA."
        ],
        [
            "Now is already set in half transform system.",
            "One starts with feature extraction shown at the top.",
            "In yellow we compute interest points.",
            "We can be characterized by XY position and the scale parameter and we compute SIFT descriptors which are quantized with which vocabulary which leads to a force index into this visual vocabulary for each feature.",
            "On the other hand, we also parametrize the hypothesis space, which is, in our case fixed aspect ratio bounding boxes, which can again be parameterized by XY position and scale.",
            "Now, an important element in prison is to combine the features and the hypothesis by invariants to compute this scene independent description.",
            "Now in this case, it's roughly speaking, taking the relative offset or feature and object position.",
            "They have to.",
            "However, to be normalized.",
            "To get to a true scale invariant object detector.",
            "Now already in this simple setup there are two possible ways to do this normalization and using one or the other has significant implications.",
            "In the final algorithm.",
            "So what are the two possible options on the right hand side on the left hand side with the?",
            "What happens if we normalize by the object scale?",
            "It's basically attaching a coordinate system to the object and record where are the features.",
            "On the other hand, normalizing by feature scale is like attaching coordinate systems to every feature and record.",
            "Where is the object center on the left hand side, there's more the sliding window view and the second one is more than half transformed."
        ],
        [
            "You now I said we do record either object object, center position or the feature position and this recording is actually the top of the object footprint.",
            "Now the object footprint is basically just a sum of direct pulses where each direct pulse records the value computed for an invariant and a fixed hypothesis.",
            "Now using linear object models which are compulsory for the half transform.",
            "Can compute and hypothesis score by taking the inner product of this sum of direct pulses with the weight function, which then leads to a sum of PT evaluation showing in red at the bottom.",
            "So it's basically a some overall features where we look up the weight function at the position of the visual word ID, so the appearance and the value of the invariant MODIS spatial information.",
            "Now up to now, we always kept the hypothesis single hypothesis fixed, which is the key idea of sliding window.",
            "And then we now go to make the step over to the half transform view.",
            "Now here."
        ],
        [
            "They have transformed view.",
            "The idea is basically not to fix a single hypothesis, but to evaluate all hypothesis in parallel.",
            "So the point evaluation that we had before, which was which was a scalar value, turns into a whole function which is known as the voting pattern.",
            "And this is a central element in half transform systems.",
            "Now this voting pattern is basically just a transformation of the object model.",
            "Wait where the transformation strongly depends.",
            "On the invariants.",
            "And of course, the position of the features.",
            "A remark here we make no constraints on the structure of the weight function, and it can also be positive and negative, which was not possible before in ISMA.",
            "We do not exploit this in this work, but we have a follow up paper at ICSE where we really showed that this is possible.",
            "Now the second view to see the difference between sliding window is more of algorithmic nature.",
            "So while sliding window first has a loop over all possible object hypothesis and then gathers together which feature give which contribution, the Hough transform interchanges these loops and has first loop or roll features and then these features cast vote for possible object positions and obviously in both cases we try about summing over features which have no contribution.",
            "So which are too far away.",
            "And that's basically it from the theoretical side.",
            "We now come."
        ],
        [
            "Go to a concrete algorithm derived from this system, which is strongly inspired by the implicit shape model, and we show on the right hand side what are the corresponding elements in Isma.",
            "So in Isma the idea is to learn the weights as occurrence distribution, which basically tell us where do we expect expect a certain visual word.",
            "Now in SM this is learned using kernels, kernel density estimators which whose memory usage and runtime scale linear with the training data size, which is not very nice.",
            "So instead we're going to use Goshen mixture models, which kind of compressed the training data and do not have this linear scaling behavior.",
            "Learning is done using expectation maximization algorithm.",
            "And as cautions allow for computing derivatives, we can actually do gradient based search instead of exhausting sliding window search which more or less corresponds to the idea of mean shift in ISMA."
        ],
        [
            "So I said before we have two possible invariants, and there is a significant difference in using one or the other, so.",
            "We here focus on what happens during the voting process to a single Goshen component of our mixture model, which is shown in the middle.",
            "So in this invariant space now on top with the what happens if we use the object centric invariant?",
            "In this case the voting leads to nonlinear distortions, which makes later stages more complicated.",
            "Now if we use instead the feature centric invariant, this voting is simply translation and scaling.",
            "So the result is still a Goshen and we can do this mapping off the voting into the search space explicitly, which make which has various advantages at detection time, and that's the invariant that we're going to use in our system.",
            "Now let's come to the result.",
            "So we did experiments on a pig."
        ],
        [
            "Western and multi motorbikes data set.",
            "For brevity we just showed the pedestrian data set here.",
            "What you see here is precision recall curve on this pedestrian data set and as a baseline we used the original implicit shape model, which is shown as red solid curve.",
            "Now, directly using our Gaussian mixture model leads to the black dashed curve, which is roughly the same but slightly worse at equal error rate compared to RSM.",
            "Now we experimented with a simple modification, so we're paravirtual word rescaling.",
            "Such that the vote the maximal vote of each virtual world is approximately 1 and this led to a significant improvement.",
            "As we can see at the black solid line, which even improves a little bit over the original yessum.",
            "So in conclusion, here we can say that our new theory, our new explanation for the voting procedure does not impair the result."
        ],
        [
            "Now, a second, very interesting experiment was about soft matching, which is commonly used in which the vocabulary system to increase the detection quality on.",
            "The disadvantage is that it causes more computation cost at detection time where speed is of prime importance.",
            "Now we argue that this is not needed and fast nearest neighbor matching is sufficient at detection time, which in our experiments led to about a factor.",
            "Or speed up compared to five nearest neighbor matching.",
            "So how should that be possible?",
            "The idea is that soft matching causes a blurring of the footprint of this direct policies that we introduced earlier, and the idea is instead of applying this blurring every time at detection time to every footprint we can move it over to the model side, where we can blur the model once offline during training.",
            "And we support this claim by an experiment at the bottom where the black curve on top is.",
            "Basically, if you do hard nearest neighbor matching and sufficiently strong soft matching.",
            "If you do hard matching during detection and soft matching, sufficiently strong soft matching during learning.",
            "Now as we can see, the red and the green curve are what happens if we add soft matching additional soft matching during detection time and with the.",
            "Performance just drops, so this is in line with our with our claim.",
            "With that"
        ],
        [
            "I would like to conclude my talk.",
            "We presented the principled implicit shape model which gives the sound justification for the HOF voting procedure.",
            "It thereby resolves theoretical problems of the original ISMA central elements of PRISM are the object footprint, object footprint, and invariants, which are used to related features.",
            "An object hypothesis and thereby decouples the modeling of geometric.",
            "Invariant of an object detector.",
            "Now the second point that comes out from our work is the duality of Hough transform and linear sliding Window system, which allows to combine advantages of both sides.",
            "We further argue that soft matching causes regularization, which can and should be moved to the training stage, which then allows for fast nearest neighbor matching at detection time.",
            "So let me make a little outlook to our work at ICC V, which is called feature centric, efficient subwindows search where."
        ],
        [
            "Are we on the one hand we show that these negative voting weights are possible, so we show a system based on support vector machines learning.",
            "On the other hand, we show that the feature centric view of this PRISM framework nicely combines with the branch and bound paradigm of efficient subwindow search.",
            "And this leads to advantages at detection time over the original SS.",
            "On the one hand, it leads to true scale invariant system.",
            "It uses less memory and it also avoids online pre processing.",
            "So computing integral images at detection time and for this branch and bound system we provide a demo code on our website.",
            "Thank you very much for your time."
        ],
        [
            "So one point is that the model itself does not make any probabilistic kind of interpretation.",
            "So what we did here is that we assumed that for each visual word we learn kind of a probability density, which would be, which is a probability density in the middle column.",
            "So in the invariant space.",
            "But as we do devoting into the search space, we basically have this rescaling and we no longer have probabilities then.",
            "So what we sum up is has no probabilistic interpretation anymore.",
            "Well, it was a.",
            "Simple heuristic to test it out."
        ],
        [
            "But it led to nice improvements, but there is actually some work at the CPR this year by Margie and Malik where they basically did so you're referring here to this modified GMM and their work is actually about tuning this Alpha parameters in a. Max Martin framework, so that's certainly more principled way to go.",
            "Would be nice to compare our really simply Ristic to what the sophisticated optimization framework leads to.",
            "Well, one point is so if one gets bigger, one gets more and more smaller scale features right?",
            "So and then this framework basically can set limits to what size feature, relative sized features we want to use, so that if we get much bigger images during detection, we basically ignore two small features because that would lead to.",
            "Basically a bias towards larger detections.",
            "And that's actually the thing that our ICC work.",
            "ICC work improves over the original ESS.",
            "You you can do that, so basically.",
            "If you look here at the invariant, so this double I, that's basically a 3 dimensional vector or 1 four XY and scale.",
            "And on the scale you can just add limits and afterwards the weight is just zero and so these things are ignored.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, welcome everybody to my presentation of praising the principled implicit shape model so this work is about optic class detection, where warnings given an image and the goal is to locate objects in the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Image so in other words, the goal is to find a more.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Semantic description from a visual description of a scene.",
                    "label": 0
                },
                {
                    "sent": "To do so, there are two commonly used paradigms.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, their assistance, inspired by the Hough transform and on the other hand there is the so-called sliding window paradigm.",
                    "label": 0
                },
                {
                    "sent": "Now half transform systems for option class detection has been made popular by the implicit shape model.",
                    "label": 1
                },
                {
                    "sent": "In these kind of system one starts with feature extraction in the image and each feature subsequently cast votes for possible object positions.",
                    "label": 0
                },
                {
                    "sent": "Now these votes are accumulated in a voting space and the sum over all these vote.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This indicates possible object positions.",
                    "label": 0
                },
                {
                    "sent": "The with.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think this is rather natural procedure to detect objects.",
                    "label": 0
                },
                {
                    "sent": "However, the model defined by SM is rather constraint.",
                    "label": 0
                },
                {
                    "sent": "It builds on densities which do not allow for negative votes, which are important to discriminate certain positions more over the summation over all the votes is explained by marginalization, which is not really applicable since we do observe all the features.",
                    "label": 0
                },
                {
                    "sent": "So they're all facts.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we have.",
                    "label": 0
                },
                {
                    "sent": "Sliding window were the idea is to slide windows of various sizes over every position in the image.",
                    "label": 0
                },
                {
                    "sent": "Crop out a given sub image and decide for each individually whether it contains a car or not.",
                    "label": 0
                },
                {
                    "sent": "So we think this is a rather clean reasoning.",
                    "label": 1
                },
                {
                    "sent": "It also allows for very flexible models, including discriminative learning, so negative weights.",
                    "label": 0
                },
                {
                    "sent": "However, this exhaustive search overall positions is not very natural.",
                    "label": 0
                },
                {
                    "sent": "Now Prince.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is about combining this to worlds about giving a sound justification to the natural voting procedure by following the clean sliding window reasoning.",
                    "label": 1
                },
                {
                    "sent": "In addition, we also get more flexibility in the model which allows for discriminative so negative votes.",
                    "label": 1
                },
                {
                    "sent": "Now let's",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go a little bit more into detail or of the sliding window procedure, so the idea there is to fix the single window single hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Crop out the corresponding sub image and compare that to an object model which allows to compute a score.",
                    "label": 1
                },
                {
                    "sent": "More generally, one can say that the idea is to compute a scene independent object description, which we're going to call the object footprint later on.",
                    "label": 1
                },
                {
                    "sent": "Remark here that this scene independent description is important and it is not really explicitly defined in ISMA.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now is already set in half transform system.",
                    "label": 0
                },
                {
                    "sent": "One starts with feature extraction shown at the top.",
                    "label": 0
                },
                {
                    "sent": "In yellow we compute interest points.",
                    "label": 0
                },
                {
                    "sent": "We can be characterized by XY position and the scale parameter and we compute SIFT descriptors which are quantized with which vocabulary which leads to a force index into this visual vocabulary for each feature.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we also parametrize the hypothesis space, which is, in our case fixed aspect ratio bounding boxes, which can again be parameterized by XY position and scale.",
                    "label": 1
                },
                {
                    "sent": "Now, an important element in prison is to combine the features and the hypothesis by invariants to compute this scene independent description.",
                    "label": 0
                },
                {
                    "sent": "Now in this case, it's roughly speaking, taking the relative offset or feature and object position.",
                    "label": 1
                },
                {
                    "sent": "They have to.",
                    "label": 0
                },
                {
                    "sent": "However, to be normalized.",
                    "label": 0
                },
                {
                    "sent": "To get to a true scale invariant object detector.",
                    "label": 0
                },
                {
                    "sent": "Now already in this simple setup there are two possible ways to do this normalization and using one or the other has significant implications.",
                    "label": 0
                },
                {
                    "sent": "In the final algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what are the two possible options on the right hand side on the left hand side with the?",
                    "label": 0
                },
                {
                    "sent": "What happens if we normalize by the object scale?",
                    "label": 1
                },
                {
                    "sent": "It's basically attaching a coordinate system to the object and record where are the features.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, normalizing by feature scale is like attaching coordinate systems to every feature and record.",
                    "label": 0
                },
                {
                    "sent": "Where is the object center on the left hand side, there's more the sliding window view and the second one is more than half transformed.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You now I said we do record either object object, center position or the feature position and this recording is actually the top of the object footprint.",
                    "label": 0
                },
                {
                    "sent": "Now the object footprint is basically just a sum of direct pulses where each direct pulse records the value computed for an invariant and a fixed hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Now using linear object models which are compulsory for the half transform.",
                    "label": 1
                },
                {
                    "sent": "Can compute and hypothesis score by taking the inner product of this sum of direct pulses with the weight function, which then leads to a sum of PT evaluation showing in red at the bottom.",
                    "label": 1
                },
                {
                    "sent": "So it's basically a some overall features where we look up the weight function at the position of the visual word ID, so the appearance and the value of the invariant MODIS spatial information.",
                    "label": 0
                },
                {
                    "sent": "Now up to now, we always kept the hypothesis single hypothesis fixed, which is the key idea of sliding window.",
                    "label": 0
                },
                {
                    "sent": "And then we now go to make the step over to the half transform view.",
                    "label": 0
                },
                {
                    "sent": "Now here.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They have transformed view.",
                    "label": 0
                },
                {
                    "sent": "The idea is basically not to fix a single hypothesis, but to evaluate all hypothesis in parallel.",
                    "label": 0
                },
                {
                    "sent": "So the point evaluation that we had before, which was which was a scalar value, turns into a whole function which is known as the voting pattern.",
                    "label": 1
                },
                {
                    "sent": "And this is a central element in half transform systems.",
                    "label": 0
                },
                {
                    "sent": "Now this voting pattern is basically just a transformation of the object model.",
                    "label": 1
                },
                {
                    "sent": "Wait where the transformation strongly depends.",
                    "label": 0
                },
                {
                    "sent": "On the invariants.",
                    "label": 0
                },
                {
                    "sent": "And of course, the position of the features.",
                    "label": 0
                },
                {
                    "sent": "A remark here we make no constraints on the structure of the weight function, and it can also be positive and negative, which was not possible before in ISMA.",
                    "label": 1
                },
                {
                    "sent": "We do not exploit this in this work, but we have a follow up paper at ICSE where we really showed that this is possible.",
                    "label": 0
                },
                {
                    "sent": "Now the second view to see the difference between sliding window is more of algorithmic nature.",
                    "label": 0
                },
                {
                    "sent": "So while sliding window first has a loop over all possible object hypothesis and then gathers together which feature give which contribution, the Hough transform interchanges these loops and has first loop or roll features and then these features cast vote for possible object positions and obviously in both cases we try about summing over features which have no contribution.",
                    "label": 0
                },
                {
                    "sent": "So which are too far away.",
                    "label": 0
                },
                {
                    "sent": "And that's basically it from the theoretical side.",
                    "label": 0
                },
                {
                    "sent": "We now come.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go to a concrete algorithm derived from this system, which is strongly inspired by the implicit shape model, and we show on the right hand side what are the corresponding elements in Isma.",
                    "label": 1
                },
                {
                    "sent": "So in Isma the idea is to learn the weights as occurrence distribution, which basically tell us where do we expect expect a certain visual word.",
                    "label": 0
                },
                {
                    "sent": "Now in SM this is learned using kernels, kernel density estimators which whose memory usage and runtime scale linear with the training data size, which is not very nice.",
                    "label": 1
                },
                {
                    "sent": "So instead we're going to use Goshen mixture models, which kind of compressed the training data and do not have this linear scaling behavior.",
                    "label": 0
                },
                {
                    "sent": "Learning is done using expectation maximization algorithm.",
                    "label": 0
                },
                {
                    "sent": "And as cautions allow for computing derivatives, we can actually do gradient based search instead of exhausting sliding window search which more or less corresponds to the idea of mean shift in ISMA.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I said before we have two possible invariants, and there is a significant difference in using one or the other, so.",
                    "label": 0
                },
                {
                    "sent": "We here focus on what happens during the voting process to a single Goshen component of our mixture model, which is shown in the middle.",
                    "label": 1
                },
                {
                    "sent": "So in this invariant space now on top with the what happens if we use the object centric invariant?",
                    "label": 1
                },
                {
                    "sent": "In this case the voting leads to nonlinear distortions, which makes later stages more complicated.",
                    "label": 0
                },
                {
                    "sent": "Now if we use instead the feature centric invariant, this voting is simply translation and scaling.",
                    "label": 1
                },
                {
                    "sent": "So the result is still a Goshen and we can do this mapping off the voting into the search space explicitly, which make which has various advantages at detection time, and that's the invariant that we're going to use in our system.",
                    "label": 0
                },
                {
                    "sent": "Now let's come to the result.",
                    "label": 0
                },
                {
                    "sent": "So we did experiments on a pig.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Western and multi motorbikes data set.",
                    "label": 0
                },
                {
                    "sent": "For brevity we just showed the pedestrian data set here.",
                    "label": 0
                },
                {
                    "sent": "What you see here is precision recall curve on this pedestrian data set and as a baseline we used the original implicit shape model, which is shown as red solid curve.",
                    "label": 0
                },
                {
                    "sent": "Now, directly using our Gaussian mixture model leads to the black dashed curve, which is roughly the same but slightly worse at equal error rate compared to RSM.",
                    "label": 0
                },
                {
                    "sent": "Now we experimented with a simple modification, so we're paravirtual word rescaling.",
                    "label": 0
                },
                {
                    "sent": "Such that the vote the maximal vote of each virtual world is approximately 1 and this led to a significant improvement.",
                    "label": 0
                },
                {
                    "sent": "As we can see at the black solid line, which even improves a little bit over the original yessum.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion, here we can say that our new theory, our new explanation for the voting procedure does not impair the result.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, a second, very interesting experiment was about soft matching, which is commonly used in which the vocabulary system to increase the detection quality on.",
                    "label": 1
                },
                {
                    "sent": "The disadvantage is that it causes more computation cost at detection time where speed is of prime importance.",
                    "label": 0
                },
                {
                    "sent": "Now we argue that this is not needed and fast nearest neighbor matching is sufficient at detection time, which in our experiments led to about a factor.",
                    "label": 1
                },
                {
                    "sent": "Or speed up compared to five nearest neighbor matching.",
                    "label": 0
                },
                {
                    "sent": "So how should that be possible?",
                    "label": 0
                },
                {
                    "sent": "The idea is that soft matching causes a blurring of the footprint of this direct policies that we introduced earlier, and the idea is instead of applying this blurring every time at detection time to every footprint we can move it over to the model side, where we can blur the model once offline during training.",
                    "label": 0
                },
                {
                    "sent": "And we support this claim by an experiment at the bottom where the black curve on top is.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you do hard nearest neighbor matching and sufficiently strong soft matching.",
                    "label": 1
                },
                {
                    "sent": "If you do hard matching during detection and soft matching, sufficiently strong soft matching during learning.",
                    "label": 0
                },
                {
                    "sent": "Now as we can see, the red and the green curve are what happens if we add soft matching additional soft matching during detection time and with the.",
                    "label": 0
                },
                {
                    "sent": "Performance just drops, so this is in line with our with our claim.",
                    "label": 0
                },
                {
                    "sent": "With that",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I would like to conclude my talk.",
                    "label": 0
                },
                {
                    "sent": "We presented the principled implicit shape model which gives the sound justification for the HOF voting procedure.",
                    "label": 1
                },
                {
                    "sent": "It thereby resolves theoretical problems of the original ISMA central elements of PRISM are the object footprint, object footprint, and invariants, which are used to related features.",
                    "label": 0
                },
                {
                    "sent": "An object hypothesis and thereby decouples the modeling of geometric.",
                    "label": 0
                },
                {
                    "sent": "Invariant of an object detector.",
                    "label": 0
                },
                {
                    "sent": "Now the second point that comes out from our work is the duality of Hough transform and linear sliding Window system, which allows to combine advantages of both sides.",
                    "label": 0
                },
                {
                    "sent": "We further argue that soft matching causes regularization, which can and should be moved to the training stage, which then allows for fast nearest neighbor matching at detection time.",
                    "label": 0
                },
                {
                    "sent": "So let me make a little outlook to our work at ICC V, which is called feature centric, efficient subwindows search where.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are we on the one hand we show that these negative voting weights are possible, so we show a system based on support vector machines learning.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we show that the feature centric view of this PRISM framework nicely combines with the branch and bound paradigm of efficient subwindow search.",
                    "label": 1
                },
                {
                    "sent": "And this leads to advantages at detection time over the original SS.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, it leads to true scale invariant system.",
                    "label": 0
                },
                {
                    "sent": "It uses less memory and it also avoids online pre processing.",
                    "label": 1
                },
                {
                    "sent": "So computing integral images at detection time and for this branch and bound system we provide a demo code on our website.",
                    "label": 1
                },
                {
                    "sent": "Thank you very much for your time.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one point is that the model itself does not make any probabilistic kind of interpretation.",
                    "label": 0
                },
                {
                    "sent": "So what we did here is that we assumed that for each visual word we learn kind of a probability density, which would be, which is a probability density in the middle column.",
                    "label": 0
                },
                {
                    "sent": "So in the invariant space.",
                    "label": 0
                },
                {
                    "sent": "But as we do devoting into the search space, we basically have this rescaling and we no longer have probabilities then.",
                    "label": 0
                },
                {
                    "sent": "So what we sum up is has no probabilistic interpretation anymore.",
                    "label": 0
                },
                {
                    "sent": "Well, it was a.",
                    "label": 0
                },
                {
                    "sent": "Simple heuristic to test it out.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it led to nice improvements, but there is actually some work at the CPR this year by Margie and Malik where they basically did so you're referring here to this modified GMM and their work is actually about tuning this Alpha parameters in a. Max Martin framework, so that's certainly more principled way to go.",
                    "label": 0
                },
                {
                    "sent": "Would be nice to compare our really simply Ristic to what the sophisticated optimization framework leads to.",
                    "label": 0
                },
                {
                    "sent": "Well, one point is so if one gets bigger, one gets more and more smaller scale features right?",
                    "label": 0
                },
                {
                    "sent": "So and then this framework basically can set limits to what size feature, relative sized features we want to use, so that if we get much bigger images during detection, we basically ignore two small features because that would lead to.",
                    "label": 0
                },
                {
                    "sent": "Basically a bias towards larger detections.",
                    "label": 0
                },
                {
                    "sent": "And that's actually the thing that our ICC work.",
                    "label": 0
                },
                {
                    "sent": "ICC work improves over the original ESS.",
                    "label": 0
                },
                {
                    "sent": "You you can do that, so basically.",
                    "label": 0
                },
                {
                    "sent": "If you look here at the invariant, so this double I, that's basically a 3 dimensional vector or 1 four XY and scale.",
                    "label": 0
                },
                {
                    "sent": "And on the scale you can just add limits and afterwards the weight is just zero and so these things are ignored.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}