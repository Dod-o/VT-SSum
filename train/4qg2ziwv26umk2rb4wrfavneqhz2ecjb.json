{
    "id": "4qg2ziwv26umk2rb4wrfavneqhz2ecjb",
    "title": "Nonlinear Mappings for Generative Kernels on Latent Variable Models",
    "info": {
        "author": [
            "Manuele Bicego, University of Verona"
        ],
        "published": "Sept. 13, 2010",
        "recorded": "August 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ssspr2010_bicego_nmg/",
    "segmentation": [
        [
            "My present."
        ],
        [
            "The idea is that I will start with the introduction to generative corners and they will try to give you the generative embedding POV, which is quite widely accepted up to now.",
            "Then I will move to the problem which are going to address which is the normalization problem and then I will try to tell you something about the nonlinear normalization.",
            "I will show you some results and some findings we have obtained from these results and then I will conclude and we'll discuss some open issues."
        ],
        [
            "So let's start from the very beginning.",
            "The idea is that there are two classes of approach for classification.",
            "The first is.",
            "Is based on generative models.",
            "In this case, the idea is that we want to model a class conditional likelihood and then do the classification, typically using the bias rule and the second.",
            "The second class is represented by discriminative methods where we think we're trying to do is just to define the separation bounded and the first case.",
            "The generative models have a better description capabilities and especially they can deal with.",
            "Non vectorial representations like sequences.",
            "Hmm is an example.",
            "In the second class.",
            "Typically the discriminative methods have better classification accuracies and supervector machine is a clear example is sick."
        ],
        [
            "Class so generative kernels are hybrid metals.",
            "The idea is that they can try to merge the description capability of generative models and classifications case over discriminate."
        ],
        [
            "Meters, so this is a standard IBM.",
            "The idea is that we can exploit a generative model in order to compute a kernel between objects, so this is an example.",
            "So here we have two sequences and then we have a trained generative model and through this generative model we are able to compute Academy.",
            "So a similarity measure between."
        ],
        [
            "Just so the main feature of these approaches is that they are very suitable for, as I told you, for nonbacterial object so that we can use these.",
            "For example the support vector machine classifier.",
            "With this kind of sequences string source sets.",
            "These are some example.",
            "Fisher Kernel is the first.",
            "Firstly introduce generative kernel is very widely known but also marginalized.",
            "Colonel Cooper Club Common and so on."
        ],
        [
            "This is an alternative point of view, which is in some sense.",
            "So generalizing this kind of general generative kernel here is the idea.",
            "We have some objects like sequences in this case.",
            "Then we have a generative model in HMM, in this case for example, and then we have a mapping which is able to project the sequences into a feature space and this feature space is called generative embedding or score space.",
            "And the idea is that if we compute the similarity in this space, we can have the generative, So in this sense we're just making explicit the presence of the feature space.",
            "And this is typically called generative embedding."
        ],
        [
            "It can be shown that many generative cabinets, not all, but many can be formulated, can be seen in this way in this from this point of view.",
            "For example, the well known Fisher kernel.",
            "In this case the generative embedding space is called Fisher Score space, and every object each object is represented by the derivative of the log likelihood of the model with respect to the parameters evaluated in that object, and the similarity in such cases just the inner product in the fission score space."
        ],
        [
            "Of course, different canvas may be defined depending of the different generative models we are using.",
            "The different mappings and the different similarities in the feature space and what we agree here is that we are using HMM based generative, so we're dealing with sequences and the kernel is the inner product in the obtained generative embedding space.",
            "Let me just say another thing here.",
            "Most of the results, most of this tab is.",
            "Other concentrating on this step.",
            "So how to project the sequences or the objects in feature space between the simple project we have studied?",
            "Also the best way to construct this generative models?",
            "So in this scenario the generative model is given, so we did some work with the Delta Group in in order to understand which is the best way to find this Model 1 model more Model 1 model per class and so on and also.",
            "In within the symbol projector with the mighty Figueredo Group, we also try to study the similarity in this space, so the idea is that we can also exploit.",
            "We can also have better performances if we are choosing a properly define the similarity in this space.",
            "So there was our paper hearing SPR was dealing with this problem."
        ],
        [
            "OK.",
            "So let's go to the problem.",
            "We are going to study here the normalization problem.",
            "It has been shown in different cases that the proper normalization of the generative's embedding space is crucial.",
            "The Fisher score space is the most important example, so if you don't normalize the space then you hardly get some good results.",
            "So there are some other evidences are also in our recent.",
            "Commission paper and also for the marginalized Colonel."
        ],
        [
            "Case, but I was observation is that in all these cases they apply normalization linear like a standardization.",
            "So making every direction of the space with zero mean unit variance and the question we are trying to address here is may unknowingly a normalization be useful."
        ],
        [
            "So what do we mean with Nolan?",
            "And normalization is applied to every component of the space to be featured in this space.",
            "Are nonlinear mapping like powering logarithm or logistic function?",
            "Here what we did there was just to try to answer to this question, so we apply different nonlinear mappings to different HMM based generative kernels in three applications."
        ],
        [
            "OK.",
            "So these are the details, so oh is just a generative object.",
            "In our case is a sequence.",
            "Then Lambda is the train generative model or a set of generative models.",
            "And then the generative embedding is just a mapping from the object to a vector space.",
            "GI is just one of the component of the vector space, and here we assume that these are all positive, as if not we can just shift that direction.",
            "So then linear normalization is just applying a non linear function.",
            "To every bad action of this space.",
            "So the new representation is just F of every feature of the space."
        ],
        [
            "So we we tried to investigate three different nonlinear mappings.",
            "The first is the powering operation, so just raising the the feature to some constant row.",
            "Then the natural logarithm, which.",
            "Does not have any parameter and then the logistic function.",
            "OK so.",
            "Forwarding and logistic functions have a parameter to be set.",
            "Of course it can be a problem the natural logarithm."
        ],
        [
            "Does not have.",
            "So this is our experiment, so we use.",
            "As I told you, we use either Markov models.",
            "They were fully regarded.",
            "The Markov models trained with standard language and then the number of states was decided depending on the particular application we were facing."
        ],
        [
            "And we studied four different generative embeddings.",
            "The Fisher score, which is of course the reference generative embedding.",
            "As I told you before, is GI is just the derivative of the log likelihood of HMM with respect to a given parameter.",
            "And this derivative is evaluated in the object, oh.",
            "Then there is another quite recent generative embedding.",
            "We propose a state space.",
            "In this case, GI is the average frequency of passing through a certain state of HMM while observing.",
            "Also, the idea is that we have a sequence.",
            "Then we feed to the model and then we're trying to understand how many times the model is passing through a given."
        ],
        [
            "State then order through the marginalized common space, even if this kernel has been introduced without an explicit generative embedding space, it is quite easy to transform it in such a way we can have a explicit score space, and in this case it is.",
            "Somehow it turns out that it is somehow similar to the state space and the final was the transition space, which is similar to the state space.",
            "But in this case we have just evaluating the.",
            "The evidence frequencies are passing through a given transition, so not through a given state, but we're given transition.",
            "You can find all the details of this.",
            "Embeddings in the SSP or paper."
        ],
        [
            "So these are the applications we tried to face to applications, in particular, the first is that to be shape recognition using our beloved chicken piece database and the idea is that here we have a parts of chicken.",
            "OK, then you have to recognize them starting from the control and then.",
            "Shapes I've described the boat with chaincode so in this case we're using discrete hmm and curvature, so in this case we're using Gaussian hmm, the second application was guest recognition using outlined data set, which is another reference data set.",
            "In this case we have Australian language science."
        ],
        [
            "Then the classification was performed with SVM.",
            "The color was just the inner product in the in the new space and see was optimized with the cross validation.",
            "We compute accuracies with K fold cross validation and our results were averaged over 20 repetitions and as I told you, some linear nonlinear mappings of the parameters and then what we did, we just try to try several values and then just the only the best results.",
            "Edit aborted"
        ],
        [
            "So before going to the numerical results, let me just tell you when it works.",
            "So we have found that it works only for generative embeddings, in which each direction summarizes information related to a single HMM state.",
            "So let me a little bit elaborate this concept.",
            "This is an example, so we have a two state hmm, and this is the state space.",
            "So every so is a big dimensional space in on this feature we have the average probability of being.",
            "In state one, given the model and the sequence, and here we have the same for the state.",
            "So so in this case, which is the case of state space and marginalized, space, the nonlinear mapping works in the official score space in the transition space, nonlinear mappings do not work in for all non linear mappings.",
            "Well do not work in the sense that the improvements are not very relevant."
        ],
        [
            "The second finding we got is that it works when nonlinear mapping is a concave with banishing banishing derivative of plus Infinity, so the powering operation Witherow greater than one does not work at all.",
            "And assessing the second characteristic is that this nonlinear mappings should be asymptotically nonexpansive, so they have to reduce the distances provided that the component has a large enough."
        ],
        [
            "OK, let me show you how it works so when it works it works very well.",
            "This is these are classification accuracies for state space embedding.",
            "Here you have the different kind of normalization.",
            "The first is the linear normalization and then the power in the lower right hand.",
            "The local logistic one.",
            "The powering Witherow less than one.",
            "Here we have the results for the 2D shape recognition with chaincode so today share potentially curvature and the guest of classification.",
            "As a general comment you can see that in most cases the nonlinear mapping.",
            "Improves the classification accuracies.",
            "In some cases it is quite large improvement."
        ],
        [
            "And this is the same.",
            "Kind of results for the marginalized camel embedding and also in this case we can see that in some application like this, the nonlinear mapping as let's say no effect, but in some other cases, like here, then the improvement is really really impressive."
        ],
        [
            "The best nonlinear mapping were found among all.",
            "Our nonlinear mapping is the powering operation, so this is this trying to explain that the effect of doing this kind of nonlinear mapping and it can be shown that the solid line is without any normalization and the dotted lines are powering operation with different values of the parameter, and the idea is that in all these cases the mappings.",
            "Reduce the contribution of larger components.",
            "OK, so they're putting that put down and the raise the contribution of smaller components so they are increased.",
            "So it seems to us that the effect is trying to re equilibrate.",
            "The contribution of each state of HMM, because these are states the generative embedding spaces where every direction is related to a single state."
        ],
        [
            "OK, so let me conclude what we've shown here is that no linear normalization of generative embedding spaces may be very useful.",
            "In some cases.",
            "It is really useful, but not in all cases, and not for nonlinear mappings.",
            "And of course the issue is White works and this is just a preliminary study, so we're trying to understand why it works and when it works in us in a more theoretical way.",
            "And One Direction we are investigating is that if you are applying the powering operation than what you are doing is kind of diagonalization of the camel metrics, which has been shown to be positive beneficial for for the application of support support vector machine like in it would be showing this paper by shock off at time.",
            "Another point is that of course what we are paying is that we can apply nonlinear mapping, but for some of them we have to choose the parameters and the choice of parameter is of course crucial, even if in some cases like when applying the logarithm, there is still an improvement and here we don't have any any parameter to be set.",
            "OK."
        ],
        [
            "OK, thank you, and let's conclude my talk.",
            "Thanks Noella we have time for a couple of questions first.",
            "So the question is this different embedding methods they have they lead to feature spaces of different dimensionality.",
            "Zvan is linear in the number of states of the HMM, the other quadratic, so this may be this may have an impact, say on the sparsity of the target space or it may lead to overfitting effects and so on teach you, could you find out anything.",
            "Would you comment on this problem?",
            "Is it really an issue or or not?",
            "Yeah of course of course this is a this is correct and also.",
            "I think that what we showed when presenting some different generative embeddings is that in some cases.",
            "I didn't dimensionality can be useful to explain to better characterize a pattern.",
            "So there are cases there are experiments in which a state space, which is a more compact space work works very well, and some other cases where this information is not enough, so they represent issues 2 poor.",
            "So we have to explode the presentation and going to, let's say greater level of detail.",
            "So this is mostly application dependent for the normalization issue.",
            "For the linear case there here is that it really depends on the application, so we cannot say that normalization is needed always or not needed always.",
            "So we're trying to understand why and when it is worth it to be done.",
            "Questions.",
            "Let me add on, did you consider nonhomogeneous normalization?",
            "Imagine you have a feature space and you can partition the feature space into areas where you want to do linear normalization.",
            "Other areas you have a particular function which works well, but only for this for particular types of features.",
            "We didn't consider that, but this is quite nice suggestion and even if in this case maybe the problem is that you are adding more degrees of freedom so you have a right to choose the para meters need to choose the known the linear mapping and where to apply it.",
            "So yeah, probably could learn what is the Graham Bell station for particular configuration of features.",
            "Thanks for this suggesting more questions.",
            "OK, so let's think Manuel again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My present.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The idea is that I will start with the introduction to generative corners and they will try to give you the generative embedding POV, which is quite widely accepted up to now.",
                    "label": 1
                },
                {
                    "sent": "Then I will move to the problem which are going to address which is the normalization problem and then I will try to tell you something about the nonlinear normalization.",
                    "label": 1
                },
                {
                    "sent": "I will show you some results and some findings we have obtained from these results and then I will conclude and we'll discuss some open issues.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start from the very beginning.",
                    "label": 0
                },
                {
                    "sent": "The idea is that there are two classes of approach for classification.",
                    "label": 0
                },
                {
                    "sent": "The first is.",
                    "label": 0
                },
                {
                    "sent": "Is based on generative models.",
                    "label": 0
                },
                {
                    "sent": "In this case, the idea is that we want to model a class conditional likelihood and then do the classification, typically using the bias rule and the second.",
                    "label": 0
                },
                {
                    "sent": "The second class is represented by discriminative methods where we think we're trying to do is just to define the separation bounded and the first case.",
                    "label": 0
                },
                {
                    "sent": "The generative models have a better description capabilities and especially they can deal with.",
                    "label": 1
                },
                {
                    "sent": "Non vectorial representations like sequences.",
                    "label": 0
                },
                {
                    "sent": "Hmm is an example.",
                    "label": 0
                },
                {
                    "sent": "In the second class.",
                    "label": 0
                },
                {
                    "sent": "Typically the discriminative methods have better classification accuracies and supervector machine is a clear example is sick.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Class so generative kernels are hybrid metals.",
                    "label": 0
                },
                {
                    "sent": "The idea is that they can try to merge the description capability of generative models and classifications case over discriminate.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Meters, so this is a standard IBM.",
                    "label": 0
                },
                {
                    "sent": "The idea is that we can exploit a generative model in order to compute a kernel between objects, so this is an example.",
                    "label": 1
                },
                {
                    "sent": "So here we have two sequences and then we have a trained generative model and through this generative model we are able to compute Academy.",
                    "label": 0
                },
                {
                    "sent": "So a similarity measure between.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just so the main feature of these approaches is that they are very suitable for, as I told you, for nonbacterial object so that we can use these.",
                    "label": 1
                },
                {
                    "sent": "For example the support vector machine classifier.",
                    "label": 0
                },
                {
                    "sent": "With this kind of sequences string source sets.",
                    "label": 0
                },
                {
                    "sent": "These are some example.",
                    "label": 1
                },
                {
                    "sent": "Fisher Kernel is the first.",
                    "label": 0
                },
                {
                    "sent": "Firstly introduce generative kernel is very widely known but also marginalized.",
                    "label": 0
                },
                {
                    "sent": "Colonel Cooper Club Common and so on.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is an alternative point of view, which is in some sense.",
                    "label": 1
                },
                {
                    "sent": "So generalizing this kind of general generative kernel here is the idea.",
                    "label": 0
                },
                {
                    "sent": "We have some objects like sequences in this case.",
                    "label": 1
                },
                {
                    "sent": "Then we have a generative model in HMM, in this case for example, and then we have a mapping which is able to project the sequences into a feature space and this feature space is called generative embedding or score space.",
                    "label": 1
                },
                {
                    "sent": "And the idea is that if we compute the similarity in this space, we can have the generative, So in this sense we're just making explicit the presence of the feature space.",
                    "label": 0
                },
                {
                    "sent": "And this is typically called generative embedding.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It can be shown that many generative cabinets, not all, but many can be formulated, can be seen in this way in this from this point of view.",
                    "label": 1
                },
                {
                    "sent": "For example, the well known Fisher kernel.",
                    "label": 0
                },
                {
                    "sent": "In this case the generative embedding space is called Fisher Score space, and every object each object is represented by the derivative of the log likelihood of the model with respect to the parameters evaluated in that object, and the similarity in such cases just the inner product in the fission score space.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course, different canvas may be defined depending of the different generative models we are using.",
                    "label": 1
                },
                {
                    "sent": "The different mappings and the different similarities in the feature space and what we agree here is that we are using HMM based generative, so we're dealing with sequences and the kernel is the inner product in the obtained generative embedding space.",
                    "label": 1
                },
                {
                    "sent": "Let me just say another thing here.",
                    "label": 0
                },
                {
                    "sent": "Most of the results, most of this tab is.",
                    "label": 0
                },
                {
                    "sent": "Other concentrating on this step.",
                    "label": 0
                },
                {
                    "sent": "So how to project the sequences or the objects in feature space between the simple project we have studied?",
                    "label": 0
                },
                {
                    "sent": "Also the best way to construct this generative models?",
                    "label": 0
                },
                {
                    "sent": "So in this scenario the generative model is given, so we did some work with the Delta Group in in order to understand which is the best way to find this Model 1 model more Model 1 model per class and so on and also.",
                    "label": 0
                },
                {
                    "sent": "In within the symbol projector with the mighty Figueredo Group, we also try to study the similarity in this space, so the idea is that we can also exploit.",
                    "label": 0
                },
                {
                    "sent": "We can also have better performances if we are choosing a properly define the similarity in this space.",
                    "label": 0
                },
                {
                    "sent": "So there was our paper hearing SPR was dealing with this problem.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's go to the problem.",
                    "label": 0
                },
                {
                    "sent": "We are going to study here the normalization problem.",
                    "label": 0
                },
                {
                    "sent": "It has been shown in different cases that the proper normalization of the generative's embedding space is crucial.",
                    "label": 1
                },
                {
                    "sent": "The Fisher score space is the most important example, so if you don't normalize the space then you hardly get some good results.",
                    "label": 0
                },
                {
                    "sent": "So there are some other evidences are also in our recent.",
                    "label": 0
                },
                {
                    "sent": "Commission paper and also for the marginalized Colonel.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case, but I was observation is that in all these cases they apply normalization linear like a standardization.",
                    "label": 0
                },
                {
                    "sent": "So making every direction of the space with zero mean unit variance and the question we are trying to address here is may unknowingly a normalization be useful.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we mean with Nolan?",
                    "label": 0
                },
                {
                    "sent": "And normalization is applied to every component of the space to be featured in this space.",
                    "label": 1
                },
                {
                    "sent": "Are nonlinear mapping like powering logarithm or logistic function?",
                    "label": 1
                },
                {
                    "sent": "Here what we did there was just to try to answer to this question, so we apply different nonlinear mappings to different HMM based generative kernels in three applications.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So these are the details, so oh is just a generative object.",
                    "label": 0
                },
                {
                    "sent": "In our case is a sequence.",
                    "label": 1
                },
                {
                    "sent": "Then Lambda is the train generative model or a set of generative models.",
                    "label": 1
                },
                {
                    "sent": "And then the generative embedding is just a mapping from the object to a vector space.",
                    "label": 1
                },
                {
                    "sent": "GI is just one of the component of the vector space, and here we assume that these are all positive, as if not we can just shift that direction.",
                    "label": 1
                },
                {
                    "sent": "So then linear normalization is just applying a non linear function.",
                    "label": 0
                },
                {
                    "sent": "To every bad action of this space.",
                    "label": 0
                },
                {
                    "sent": "So the new representation is just F of every feature of the space.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we we tried to investigate three different nonlinear mappings.",
                    "label": 1
                },
                {
                    "sent": "The first is the powering operation, so just raising the the feature to some constant row.",
                    "label": 1
                },
                {
                    "sent": "Then the natural logarithm, which.",
                    "label": 1
                },
                {
                    "sent": "Does not have any parameter and then the logistic function.",
                    "label": 1
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Forwarding and logistic functions have a parameter to be set.",
                    "label": 0
                },
                {
                    "sent": "Of course it can be a problem the natural logarithm.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Does not have.",
                    "label": 0
                },
                {
                    "sent": "So this is our experiment, so we use.",
                    "label": 0
                },
                {
                    "sent": "As I told you, we use either Markov models.",
                    "label": 0
                },
                {
                    "sent": "They were fully regarded.",
                    "label": 0
                },
                {
                    "sent": "The Markov models trained with standard language and then the number of states was decided depending on the particular application we were facing.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we studied four different generative embeddings.",
                    "label": 0
                },
                {
                    "sent": "The Fisher score, which is of course the reference generative embedding.",
                    "label": 0
                },
                {
                    "sent": "As I told you before, is GI is just the derivative of the log likelihood of HMM with respect to a given parameter.",
                    "label": 1
                },
                {
                    "sent": "And this derivative is evaluated in the object, oh.",
                    "label": 0
                },
                {
                    "sent": "Then there is another quite recent generative embedding.",
                    "label": 1
                },
                {
                    "sent": "We propose a state space.",
                    "label": 0
                },
                {
                    "sent": "In this case, GI is the average frequency of passing through a certain state of HMM while observing.",
                    "label": 1
                },
                {
                    "sent": "Also, the idea is that we have a sequence.",
                    "label": 0
                },
                {
                    "sent": "Then we feed to the model and then we're trying to understand how many times the model is passing through a given.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "State then order through the marginalized common space, even if this kernel has been introduced without an explicit generative embedding space, it is quite easy to transform it in such a way we can have a explicit score space, and in this case it is.",
                    "label": 0
                },
                {
                    "sent": "Somehow it turns out that it is somehow similar to the state space and the final was the transition space, which is similar to the state space.",
                    "label": 1
                },
                {
                    "sent": "But in this case we have just evaluating the.",
                    "label": 1
                },
                {
                    "sent": "The evidence frequencies are passing through a given transition, so not through a given state, but we're given transition.",
                    "label": 1
                },
                {
                    "sent": "You can find all the details of this.",
                    "label": 0
                },
                {
                    "sent": "Embeddings in the SSP or paper.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are the applications we tried to face to applications, in particular, the first is that to be shape recognition using our beloved chicken piece database and the idea is that here we have a parts of chicken.",
                    "label": 0
                },
                {
                    "sent": "OK, then you have to recognize them starting from the control and then.",
                    "label": 0
                },
                {
                    "sent": "Shapes I've described the boat with chaincode so in this case we're using discrete hmm and curvature, so in this case we're using Gaussian hmm, the second application was guest recognition using outlined data set, which is another reference data set.",
                    "label": 1
                },
                {
                    "sent": "In this case we have Australian language science.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the classification was performed with SVM.",
                    "label": 1
                },
                {
                    "sent": "The color was just the inner product in the in the new space and see was optimized with the cross validation.",
                    "label": 1
                },
                {
                    "sent": "We compute accuracies with K fold cross validation and our results were averaged over 20 repetitions and as I told you, some linear nonlinear mappings of the parameters and then what we did, we just try to try several values and then just the only the best results.",
                    "label": 0
                },
                {
                    "sent": "Edit aborted",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before going to the numerical results, let me just tell you when it works.",
                    "label": 0
                },
                {
                    "sent": "So we have found that it works only for generative embeddings, in which each direction summarizes information related to a single HMM state.",
                    "label": 1
                },
                {
                    "sent": "So let me a little bit elaborate this concept.",
                    "label": 0
                },
                {
                    "sent": "This is an example, so we have a two state hmm, and this is the state space.",
                    "label": 0
                },
                {
                    "sent": "So every so is a big dimensional space in on this feature we have the average probability of being.",
                    "label": 0
                },
                {
                    "sent": "In state one, given the model and the sequence, and here we have the same for the state.",
                    "label": 0
                },
                {
                    "sent": "So so in this case, which is the case of state space and marginalized, space, the nonlinear mapping works in the official score space in the transition space, nonlinear mappings do not work in for all non linear mappings.",
                    "label": 0
                },
                {
                    "sent": "Well do not work in the sense that the improvements are not very relevant.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second finding we got is that it works when nonlinear mapping is a concave with banishing banishing derivative of plus Infinity, so the powering operation Witherow greater than one does not work at all.",
                    "label": 0
                },
                {
                    "sent": "And assessing the second characteristic is that this nonlinear mappings should be asymptotically nonexpansive, so they have to reduce the distances provided that the component has a large enough.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let me show you how it works so when it works it works very well.",
                    "label": 1
                },
                {
                    "sent": "This is these are classification accuracies for state space embedding.",
                    "label": 1
                },
                {
                    "sent": "Here you have the different kind of normalization.",
                    "label": 0
                },
                {
                    "sent": "The first is the linear normalization and then the power in the lower right hand.",
                    "label": 1
                },
                {
                    "sent": "The local logistic one.",
                    "label": 0
                },
                {
                    "sent": "The powering Witherow less than one.",
                    "label": 0
                },
                {
                    "sent": "Here we have the results for the 2D shape recognition with chaincode so today share potentially curvature and the guest of classification.",
                    "label": 0
                },
                {
                    "sent": "As a general comment you can see that in most cases the nonlinear mapping.",
                    "label": 0
                },
                {
                    "sent": "Improves the classification accuracies.",
                    "label": 0
                },
                {
                    "sent": "In some cases it is quite large improvement.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the same.",
                    "label": 0
                },
                {
                    "sent": "Kind of results for the marginalized camel embedding and also in this case we can see that in some application like this, the nonlinear mapping as let's say no effect, but in some other cases, like here, then the improvement is really really impressive.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The best nonlinear mapping were found among all.",
                    "label": 1
                },
                {
                    "sent": "Our nonlinear mapping is the powering operation, so this is this trying to explain that the effect of doing this kind of nonlinear mapping and it can be shown that the solid line is without any normalization and the dotted lines are powering operation with different values of the parameter, and the idea is that in all these cases the mappings.",
                    "label": 0
                },
                {
                    "sent": "Reduce the contribution of larger components.",
                    "label": 1
                },
                {
                    "sent": "OK, so they're putting that put down and the raise the contribution of smaller components so they are increased.",
                    "label": 0
                },
                {
                    "sent": "So it seems to us that the effect is trying to re equilibrate.",
                    "label": 0
                },
                {
                    "sent": "The contribution of each state of HMM, because these are states the generative embedding spaces where every direction is related to a single state.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me conclude what we've shown here is that no linear normalization of generative embedding spaces may be very useful.",
                    "label": 1
                },
                {
                    "sent": "In some cases.",
                    "label": 1
                },
                {
                    "sent": "It is really useful, but not in all cases, and not for nonlinear mappings.",
                    "label": 0
                },
                {
                    "sent": "And of course the issue is White works and this is just a preliminary study, so we're trying to understand why it works and when it works in us in a more theoretical way.",
                    "label": 0
                },
                {
                    "sent": "And One Direction we are investigating is that if you are applying the powering operation than what you are doing is kind of diagonalization of the camel metrics, which has been shown to be positive beneficial for for the application of support support vector machine like in it would be showing this paper by shock off at time.",
                    "label": 0
                },
                {
                    "sent": "Another point is that of course what we are paying is that we can apply nonlinear mapping, but for some of them we have to choose the parameters and the choice of parameter is of course crucial, even if in some cases like when applying the logarithm, there is still an improvement and here we don't have any any parameter to be set.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you, and let's conclude my talk.",
                    "label": 0
                },
                {
                    "sent": "Thanks Noella we have time for a couple of questions first.",
                    "label": 0
                },
                {
                    "sent": "So the question is this different embedding methods they have they lead to feature spaces of different dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Zvan is linear in the number of states of the HMM, the other quadratic, so this may be this may have an impact, say on the sparsity of the target space or it may lead to overfitting effects and so on teach you, could you find out anything.",
                    "label": 0
                },
                {
                    "sent": "Would you comment on this problem?",
                    "label": 0
                },
                {
                    "sent": "Is it really an issue or or not?",
                    "label": 0
                },
                {
                    "sent": "Yeah of course of course this is a this is correct and also.",
                    "label": 0
                },
                {
                    "sent": "I think that what we showed when presenting some different generative embeddings is that in some cases.",
                    "label": 0
                },
                {
                    "sent": "I didn't dimensionality can be useful to explain to better characterize a pattern.",
                    "label": 0
                },
                {
                    "sent": "So there are cases there are experiments in which a state space, which is a more compact space work works very well, and some other cases where this information is not enough, so they represent issues 2 poor.",
                    "label": 0
                },
                {
                    "sent": "So we have to explode the presentation and going to, let's say greater level of detail.",
                    "label": 0
                },
                {
                    "sent": "So this is mostly application dependent for the normalization issue.",
                    "label": 0
                },
                {
                    "sent": "For the linear case there here is that it really depends on the application, so we cannot say that normalization is needed always or not needed always.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to understand why and when it is worth it to be done.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Let me add on, did you consider nonhomogeneous normalization?",
                    "label": 0
                },
                {
                    "sent": "Imagine you have a feature space and you can partition the feature space into areas where you want to do linear normalization.",
                    "label": 0
                },
                {
                    "sent": "Other areas you have a particular function which works well, but only for this for particular types of features.",
                    "label": 0
                },
                {
                    "sent": "We didn't consider that, but this is quite nice suggestion and even if in this case maybe the problem is that you are adding more degrees of freedom so you have a right to choose the para meters need to choose the known the linear mapping and where to apply it.",
                    "label": 0
                },
                {
                    "sent": "So yeah, probably could learn what is the Graham Bell station for particular configuration of features.",
                    "label": 0
                },
                {
                    "sent": "Thanks for this suggesting more questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's think Manuel again.",
                    "label": 0
                }
            ]
        }
    }
}