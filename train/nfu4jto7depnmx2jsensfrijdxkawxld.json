{
    "id": "nfu4jto7depnmx2jsensfrijdxkawxld",
    "title": "Supervised and Localized Dimensionality Reduction from Multiple Feature Representations or Kernels",
    "info": {
        "author": [
            "Ethem Alpaydin, Department of Computer Engineering, Bo\u011fazi\u00e7i University"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Manifold Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_alpaydin_sld/",
    "segmentation": [
        [
            "So let's say that we have a number of representations I."
        ],
        [
            "Going from, I'm going from one to P. And we want to combine them so this is the usual multiple kernel learning framework.",
            "You saw a number of times today.",
            "Our idea was to take this at times, not to treat them as constant, but it's some functions of the input.",
            "So when you have these multiple representations, what we do is that we concatenate all of them, get a large dimensional vector, and we define a function that takes it as.",
            "As its input, based on some parameters, generates a value between zero and one.",
            "So.",
            "What happens is that when you want to compute the distance between where computer kernel between XI and X if.",
            "Effects I chose this kernel an anifex also chooses this kernel then well then we apply this kernel and we take away.",
            "If either of these things is close to 0, done that.",
            "Corresponding kernel is not is not applied."
        ],
        [
            "Once you have these things localized, you immediately start thinking thinking about dimensionality reduction record also talked about this thing in the morning, so you have this original access.",
            "We have this projection matrices W which map down to a lower dimensional space, and that's where we apply the kernel.",
            "Similarly, in the gating space, when you have these X, you can first project them to a lower dimensional space and there you have this classifier we choose among these kernels.",
            "So now we have this new additional parameters W and the and the T is that matched is that does this projections?"
        ],
        [
            "Also, do you have problem formulation the same except that now instead of the usual kernel we have this knew locally defined kernel?",
            "We have all these parameters WV&T that we need to learn once you fix all these parameters and that we use the usual SPM and you can solve for it for the other parameters we used."
        ],
        [
            "So here you see."
        ],
        [
            "An example, this is the multi features data set from the user repository.",
            "We have six different representations.",
            "This is the first one.",
            "This is the gating space reduced to two dimensions.",
            "And here you see that out of the six representations, only four of them are used.",
            "These are the boundaries learned by the gating models and these are the two dimensional projections in each of the separate representations.",
            "So these are the instances that chooses representations, and this is the projection of the boundary.",
            "That's that's in that dimension.",
            "In that particular representation."
        ],
        [
            "So if you compare in terms of accuracy on the number of support vectors, this is SVM using the single best representation.",
            "This is SVM that takes all the concatenated form of all the representations together.",
            "This is multiple kernel learning.",
            "This is localized without any dimensionality reduction and this is with dimensionality reduction.",
            "So what we see over here is that without losing much from accuracy, actually this difference is not significant.",
            "They decrease significantly number of support vectors.",
            "And now we need to do more runs on other datasets to see if we always have get interesting results like this.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's say that we have a number of representations I.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going from, I'm going from one to P. And we want to combine them so this is the usual multiple kernel learning framework.",
                    "label": 0
                },
                {
                    "sent": "You saw a number of times today.",
                    "label": 0
                },
                {
                    "sent": "Our idea was to take this at times, not to treat them as constant, but it's some functions of the input.",
                    "label": 0
                },
                {
                    "sent": "So when you have these multiple representations, what we do is that we concatenate all of them, get a large dimensional vector, and we define a function that takes it as.",
                    "label": 0
                },
                {
                    "sent": "As its input, based on some parameters, generates a value between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What happens is that when you want to compute the distance between where computer kernel between XI and X if.",
                    "label": 0
                },
                {
                    "sent": "Effects I chose this kernel an anifex also chooses this kernel then well then we apply this kernel and we take away.",
                    "label": 0
                },
                {
                    "sent": "If either of these things is close to 0, done that.",
                    "label": 0
                },
                {
                    "sent": "Corresponding kernel is not is not applied.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once you have these things localized, you immediately start thinking thinking about dimensionality reduction record also talked about this thing in the morning, so you have this original access.",
                    "label": 0
                },
                {
                    "sent": "We have this projection matrices W which map down to a lower dimensional space, and that's where we apply the kernel.",
                    "label": 0
                },
                {
                    "sent": "Similarly, in the gating space, when you have these X, you can first project them to a lower dimensional space and there you have this classifier we choose among these kernels.",
                    "label": 0
                },
                {
                    "sent": "So now we have this new additional parameters W and the and the T is that matched is that does this projections?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, do you have problem formulation the same except that now instead of the usual kernel we have this knew locally defined kernel?",
                    "label": 0
                },
                {
                    "sent": "We have all these parameters WV&T that we need to learn once you fix all these parameters and that we use the usual SPM and you can solve for it for the other parameters we used.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here you see.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An example, this is the multi features data set from the user repository.",
                    "label": 1
                },
                {
                    "sent": "We have six different representations.",
                    "label": 0
                },
                {
                    "sent": "This is the first one.",
                    "label": 0
                },
                {
                    "sent": "This is the gating space reduced to two dimensions.",
                    "label": 0
                },
                {
                    "sent": "And here you see that out of the six representations, only four of them are used.",
                    "label": 0
                },
                {
                    "sent": "These are the boundaries learned by the gating models and these are the two dimensional projections in each of the separate representations.",
                    "label": 0
                },
                {
                    "sent": "So these are the instances that chooses representations, and this is the projection of the boundary.",
                    "label": 0
                },
                {
                    "sent": "That's that's in that dimension.",
                    "label": 0
                },
                {
                    "sent": "In that particular representation.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you compare in terms of accuracy on the number of support vectors, this is SVM using the single best representation.",
                    "label": 0
                },
                {
                    "sent": "This is SVM that takes all the concatenated form of all the representations together.",
                    "label": 0
                },
                {
                    "sent": "This is multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "This is localized without any dimensionality reduction and this is with dimensionality reduction.",
                    "label": 1
                },
                {
                    "sent": "So what we see over here is that without losing much from accuracy, actually this difference is not significant.",
                    "label": 0
                },
                {
                    "sent": "They decrease significantly number of support vectors.",
                    "label": 0
                },
                {
                    "sent": "And now we need to do more runs on other datasets to see if we always have get interesting results like this.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}