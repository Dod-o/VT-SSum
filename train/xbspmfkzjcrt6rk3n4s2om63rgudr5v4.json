{
    "id": "xbspmfkzjcrt6rk3n4s2om63rgudr5v4",
    "title": "Information-theoretic lower bounds on the oracle complexity of sparse convex optimization",
    "info": {
        "author": [
            "Alekh Agarwal, Microsoft Research"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_agarwal_itl/",
    "segmentation": [
        [
            "This is talk about computational lower bounds on the complexity of sparse convex optimization problems.",
            "An associate mentioned this is joint work with Peter Bartlet, Pradeep Ravikumar and Martin Wainwright so."
        ],
        [
            "So in the recent years, a lot of the focus in convex optimization has shifted to high dimensional problems, and there's a good reason for why this is happening, because there are more and more data sets where we do need to solve convex optimization problems on in very high dimensions.",
            "So one Canonical example is a lot of the work in the area of computational biology needs to solve optimization problems in order to perform statistical inference on.",
            "Very large number of variables problems such as collaborative filtering where you might be trying to solve high dimensional matrix completion problem, computational astronomy and so on and so forth.",
            "There are lots and lots of examples where these high dimensional optimization problems come up, and there's sort of 1 commonality to a lot of these different domains, which is somehow the solution space.",
            "The final optimum that we compute.",
            "We want the solution to be sparse.",
            "An often you would see that the algorithmics schemes that people come up with are actually.",
            "Once that they try to enforce even the sparsity explicitly on the final solution an there's a couple of good reasons for it.",
            "There's the obvious computational reason if you come up with a sparse optimum, then you know you can do future computation on it rather cheaply.",
            "For instance, dot products on sparse vectors are cheap, but there are also good statistical reasons for why you might want to do this so.",
            "If we take a very simple example of high dimensional sparse optimization problem that people often address that.",
            "That probably most of you are familiar with.",
            "By now there's a high dimensional linear regression problem, so you've got some high dimensional vectors XYZ and regression outputs Y eyes and you're trying to estimate a good mapping from the exercise to the wise and one popular estimator in the high dimension is the lawsuit estimator where you try to minimize the squared error between the observations and predictions subject to another non constraint.",
            "For this talk this.",
            "L1 norm constraint, for instance, is the is the is the part of interest because what it does is it enforces that the solution Theta hat that you compute to this problem is going to be sparse.",
            "Now as I said, having a sparse head ahead has obvious computational advantages, but in particular in this high dimensional setting even to show a basic statistical consistency of Terra Haute in estimating.",
            "Correct mapping, you need the sparsity properties of the estimator.",
            "Computed are the problem.",
            "So so these are sort of the two main advantages for for which people prefer these power solutions in high dimension problems."
        ],
        [
            "And as a result of this need, there's been a lot of work in the recent years in developing specialized algorithms to solve these problems fast to solve them efficiently.",
            "So of course the most classical work kind of predates all the recent flurry, so goes back to number of skin during in 1983 who first proposed the method of mirror descent, also studied more recently by backpack interval and extended in a lot of ways by lot of other people an.",
            "Is very suited to solving such problems.",
            "More recent work.",
            "So there were two very nice papers at NIPS last year.",
            "There was the forward, backward splitting of Duchenne singer.",
            "There was regularised dual averaging of Lenschow an.",
            "Since then we've seen accelerated variants of these algorithms.",
            "Extensions to mirror dissent Framework again an there's a lot of algorithms now available to solve such problems, and most of these algorithms they come with accompanying theory.",
            "So you can.",
            "Take any of these papers and for any of these specific methods you can conclude that given a desired accuracy level epsilon, how much computational labor you would have to put in to solve the optimization problem to an accuracy epsilon for any of these methods.",
            "Such bounds exist."
        ],
        [
            "However, to the best of my knowledge there is.",
            "Work in the area of sparse optimization and the fundamental hardness of these problems and by which what I mean is, suppose I want to take the best possible algorithm computationally.",
            "The most optimal algorithm to solve this problem, then how much labor would that algorithm have to spend in order to solve the problem to an accuracy epsilon?",
            "And the reason why you might want to understand this is cause you what you really want to get at is how far away from being optimal are the current or algorithms that we know.",
            "Are the optimal?",
            "Are the sub optimal and this is the question that we would try and address in this work.",
            "Then of course just stated this way.",
            "This is a pretty general question, so before we can go about trying to meaningfully answer this, we need to define what the precise nature of the optimization problem we want to deal with.",
            "The precise complexity model that we want to answer these questions, and so that's going to be the next."
        ],
        [
            "Two slides, so the convex optimization setup is fairly simple and probably most of you are familiar with it, so we're trying to optimize some convex function F over some convex subset of Rd.",
            "Now we're told.",
            "Oh OK, this is.",
            "This is a version of the slides from last night, so there should have been some updates here.",
            "So we are told that this function belongs to some class script F. And since we are interested in sparse optimization problems here in particular, we assume Script F is only consists of functions that have sparse Optima and I will make this a bit more precise in the later slides.",
            "But our optimization algorithm is given this information that it is dealing with the problem with the sparsity structure and it's us to come up with a point X which is at most epsilon sub optimal in the function values, right?",
            "So this is, this is what optimization algorithms do.",
            "And now how do we measure the complexity of these optimization algorithm?"
        ],
        [
            "The model that we propose to use is the Stochastic 1st order model of complexity, which was first looked at in.",
            "Again, the work of Nemerofsky ended in 1993, so this model applies to iterative optimization methods an every round.",
            "So the optimization goes for say fixed length of time T at every round.",
            "The optimization method queries the article with some point XD from the.",
            "The optimization set S. And the article interprets it as the algorithm's current guess towards the optimum of the function.",
            "Now what the article does is it response to the algorithm with some information about the behavior of the function at this point XD.",
            "So in particular, in the Stochastic 1st order model the Oracle returns a pair of random variables.",
            "F Hadden, ZE hat and.",
            "In particular."
        ],
        [
            "In particular, the random variable F hat is.",
            "My laptops totally decided to misbehave for this stock.",
            "So in particular that the random variable F hat is unbiased for the true function value at F. The random variable Z hat is unbiased for the sub gradient of F at XT, so we're going to be dealing with nonsmooth functions here, so I need to look at the sub derivative set, and we assume that there is bounded noise in the gradients gradient estimates that are given to us by the article, so the intuitive picture is that the optimization starts from some initial point X1.",
            "The article responds with some information about the gradient of F at X1.",
            "The algorithm takes this information into."
        ],
        [
            "Account and comes up with its next query point X."
        ],
        [
            "So receives information about the gradient X2 and so on so forth and aft."
        ],
        [
            "Sufficiently large number of queries it hopefully comes up to the point XT, which is a good enough estimate to the optimum of the function.",
            "Anne, this complexity model is of interest to us because all the methods that I mentioned up until so far, so gradient descent, mirror descent, stochastic gradient descent and mirror descent forward, backward splitting regularize, dual averaging, they cannot be implemented in this stochastic 1st order Article model.",
            "So if we derive any complexity, lower bounds on this model, they will indeed be applicable to all these methods that are actually being used to solve these problems."
        ],
        [
            "So the quantities of interest that we want to understand in this article model for a given method M and a given function F we want to look at the optimizer.",
            "The expected optimization error that the method incurs after T rounds.",
            "Now I have to take an expectation here because the article is giving me random responses, so there's some randomness in the algorithms.",
            ".3 points as well.",
            "So this expectation is over that randomness an we can Alternatively ask what's the article complexity which is the smallest number of queries that you have to make.",
            "To get to an expected error of at most epsilon in optimization.",
            "Now this of course makes sense once you have specified an optimization method, But what we want to look at is an algorithm independent lower bound on complexity.",
            "So we want to understand the minimum minimax complexity where you ask if you were to take the best algorithm and best in the sense that for that algorithm, if you take the worst case Oracle complexity overall functions in the class script F. So remember Script F is the class of.",
            "Is going to be this class of all sparse functions.",
            "So if you take the worst case Oracle complexity overall functions in that class, then what's the algorithm with the best worst case performance?",
            "And if you can lower bound the complexity of that algorithm, then you've have understood the fundamental hardness of the function class script F in the article complexity model so."
        ],
        [
            "So.",
            "This is this is of course, as I said, not the first time this class is being looked at, so the classical result goes back to Nemerofsky Newton, who showed that in the setting for of when you said script after the class of all convex Lipschitz functions, then the minimax complexities of one over epsilon squared.",
            "Now in the setting of interest here actually.",
            "What we can show is that the min Max complexity is going to be in D dimensions.",
            "Going to be as high as of D squared over epsilon squared and this was shown in some of our recent work and the exact assumptions will be will be clarified on the next slide under which this result holds.",
            "Now this is pretty pathetic in high dimensions.",
            "I mean, this tells you that if you want to solve the problem too and even moderate accuracy, the amount of computational complexity, the number of iterations you will have to perform is just prohibitively large.",
            "But we know that people solve these sparse optimization problems and they solve it in a reasonable amount of time, so.",
            "So there must be something that gives and what we have to notice that these results were developed for the case of general convex Lipschitz optimization, they do not take into account that functions that we're looking at have the sparsity structure.",
            "So the question we want to understand is if you impose this additional sparsity assumption on the optimum of the problem, then does that buy you anything?",
            "Um?",
            "Can you do something better when your optimization problem is sparse?",
            "OK, so."
        ],
        [
            "So to do that, we look at the class of all sparse convex functions.",
            "So these are functions for which there is at least one optimum XF that has at most K non zero entries.",
            "OK, so so these functions can have multiple Optima.",
            "You can have dense Optima.",
            "I don't care about that.",
            "What I want is there should be at least one good optimum, one optimum with at most K nonzero entries.",
            "In addition to that, I'm going to assume that the function is convex.",
            "And Lipschitz inappropriate norm.",
            "An under these these assumptions.",
            "First of all, the if you further assume that this set S can look like an L Infinity ball.",
            "So it's just a box, then the D squared over epsilon squared result that I showed on the previous slide without the sparsity assumption applies.",
            "Now when you add in the sparsity structure of the optimum."
        ],
        [
            "Then what you can show is the minimax complexity is actually this quantity shown here.",
            "So the key thing to note is that this is only logarithmic in the ambient dimension D, instead of the bad quadratic scaling, which does explain why you can solve these high dimensional problems when you have this additional sparsity assumption.",
            "Of course there is a scaling with the sparsity level of the optimum, because we know that without sparsity in general, we do not expect this log rhythmic scaling.",
            "Now lower bound by itself is kind of not that interesting unless you can understand how it exists, how it relates to the upper bounds on the existing methods.",
            "In this case, it turns out this lower bound is actually sharp.",
            "So if you take the class of methods of mirror descent and take a particular member from that family by tuning the prox function appropriately, then then this lower bound is actually exactly attained by a method of mirror descent.",
            "So that tells us that.",
            "Two things.",
            "First of all, this lower bound is unimprovable without further assumptions, because there's a method that meets the lower bound, and Secondly, it also establishes the optimality of that particular member from the class of methods of mirror descent, so we know now that there exists an optimal method to solve these problems."
        ],
        [
            "So in the remaining time I would like to give you a brief sketch of how such a result is proved.",
            "So the first step is to discard this entire function class and only look at a smaller, much smaller subset of it which is somehow which captures the true complexity of the class.",
            "So, so we try to identify a hard subset of functions in the class, and this hard subset is basically going to be a subset that has a lot of functions still, but each pair of functions in that class.",
            "Is well separated in a certain sense.",
            "What does bias as it allows us to establish a link from optimization to function identification.",
            "These functions are going to be separated enough that if you can optimize them, then you basically know which function that the optimum can only belong to.",
            "One of these functions, so you will be able to establish a one to one connection.",
            "Now if you recall the stochastic Oracle does not give you exact function gradient values, it only gives you noisy samples of those quantities.",
            "So now you're looking at these noisy samples generated according to a function.",
            "And you're solving a function identification problem from the data.",
            "So this almost smells like some sort of statistical estimation going on here an in fact.",
            "What?",
            "So let's go."
        ],
        [
            "So, so first thing I want to explain is the how much time do I have either way.",
            "OK, I'll try to be quick so OK, thanks.",
            "Sorry about the interruption.",
            "OK so the first thing I want to explain is the sense of separation that we use here to design our function class.",
            "And it turns out the standard notions of distance between functions kind of end up being unsuitable for us.",
            "So we end up defining our own notion of separation between convex functions.",
            "Which is denoted by this role symmetric and what it measures is the difference between the optimal function value, the smallest function value of the function F + G, and the smallest values of the functions FNG individually, so it measures this gap.",
            "And this is certain nice properties.",
            "You can show that it's strictly positive unless F&G are optimized on a common point, so as long as F&G do not share an optimum.",
            "This this separation is going to be non zero and intuitively what it tries to capture is how different FG look for optimization.",
            "So if if F&G have a structure where the optimum of G is somehow good enough optimum for F as well, then the separation will be small, otherwise it will be large.",
            "Um?"
        ],
        [
            "What does separation buys us?",
            "Is.",
            "What does separation buys us?",
            "Is it essentially allows us to make this connection from optimization to function identification, so So what you can show is that if two functions are at least Delta apart in the row sense, then any point that optimizes the function F well cannot optimize G well and vice versa.",
            "So in the picture here, if you take the point X one, then it's a good estimate of the optimum for the read function.",
            "But it's a terrible estimate of the optimum for the blue function.",
            "And similarly, X2 is a good approximate optimum for the blue function, but not for the read function.",
            "Now, once you get this property, you can bootstrap from 2 functions to the entire function class script F and say OK.",
            "So if I have a class script F where all the functions, every pair of functions is separated in the sense of row, then for any given point X there's going to be at least at most one function that is approximately optimized at that point.",
            "Now this is a very powerful thing because what it allows you to say is I can run my optimization algorithm to the end.",
            "Look at the final solution that's generated, which is an approximate optimum for the function being used by the article.",
            "I know that the point that the algorithm returns to me can be the approximate optimum for only one function in my class.",
            "If I have row separation, and so I know the function that the article used.",
            "So even even if the algorithm is not trying to identify the function, I can run it and.",
            "From the output I can."
        ],
        [
            "Reconstruct the articles function.",
            "The next step is to design this function class to be this hard subset of functions that we can embed into our function class, which will have this property that it's going to be optimized data, sparse point, and that's going to be well separated in the sense of row.",
            "So this class of functions we design is indexed by vectors in minus 101 to the D, and we in particular pick sparse vectors, so vectors that have at most at most K entries where you have plus minus one values.",
            "The rest are all zero.",
            "And with each such vector, you associate a function G Alpha an.",
            "The idea is that this IIS coordinate of the vector Alpha it supplies you.",
            "Basically with this averaging wait.",
            "So you use these words half plus Alpha, Delta in half minus Alpha Delta to average some base functions, and these basis functions they act only along one coordinate.",
            "So for each coordinate you take a particular base function and these have to be again picked appropriately and carefully.",
            "But you pick some functions of this form.",
            "And average them with these appropriately chosen weights.",
            "Now there are a couple of properties you have to ensure.",
            "First is we try to pick the base functions in such a way that sparsity of the vector Alpha implies sparsity of the optimum of G Alpha because we remember we want the Alpha to be optimized data, sparse point.",
            "The second thing we try to ensure is that we can, we can.",
            "We can keep these vectors G Alpha far.",
            "We can keep these functions G Alpha and beta far away for two different vectors Alpha and beta and the way we try to ensure this is we try to ensure that if if vector Alpha is very different from a vector beta in the sense of having distance then the corresponding functions are going to be far away as well.",
            "And again this this relies heavily on picking these base functions carefully, so module."
        ],
        [
            "So that then we can easily design a first order Oracle.",
            "Now for this problem, which is really the final ingredient in the proof where you interpret this averaging weight of half plus Alpha Phi Delta on the ice coordinate as the bias of a coin.",
            "And instead of actually averaging functions under this bias, you average them according to the outcome of the coin.",
            "So every every round the Oracle flips one coin per coordinate.",
            "Observe the outcome of the coin now.",
            "Averages them according to the outcome of the coin.",
            "So remember we are is going to be either zero or one, so the article is either going to pick for the Earth, coordinate the function FI plus, or pick the function FI minus and give you the function value and gradient in that coordinate according to pick function according to the outcome of the coin.",
            "So now we have made an explicit connection between.",
            "These these coin tosses and optimization, and in particular if you identify."
        ],
        [
            "Function G Alpha, which is equal into identifying the vector Alpha."
        ],
        [
            "Now we're trying to estimate this vector Alpha from the outcomes of coin tosses that we're observing.",
            "So you're observing these coin tosses, and you're trying to observe the you're trying to estimate the bias of the coins that are used to generate the tosses.",
            "So this is, I mean really, the most classical problem in statistics now, so you can easily come up with lower bounds on the sample complexity, the number of coin flips you need to see now, remember, every query to the article gives you a certain number of coin flips, so once you know the number of coin flips, you need to see, you can.",
            "You can directly come up with the number of queries that the algorithm will need to make an by following through this logic carefully, you can essentially prove the result that I showed."
        ],
        [
            "Earlier, so to wrap up what this work establishes, are sharp minimax lower bounds on the Oracle complexity of sparse convex optimization problems, and the key idea was to do this reduction from optimization to statistical estimation and then use information theoretic lower bounds on the sample complexity of estimation problems.",
            "In the process we identify this role symmetric.",
            "That actually seems like a rather natural distance measure for convex functions and might have other users in optimization.",
            "Problems and here I talked about the optimality of one of the methods mirror designed for for actually solving the problems, But you can actually show some of the other methods are also shown to be optimal VR results, so as a result our work also provides a certificate of optimality for some of the popularly used methods to solve these sparse convex optimization problems, so that's all."
        ],
        [
            "To say and thanks a lot in particular for bearing with all the heavy troubles.",
            "Thank you.",
            "Question.",
            "Has it been in similar work?",
            "I'm in emacs.",
            "Sounds for defining Matthews International interest.",
            "So in the.",
            "In the.",
            "Deterministic optimization setting I know of work for neutral methods in particular or not.",
            "Newton method for 2nd order optimization.",
            "So here remember, we're only we're associating methods with sort of the information they receive about the function, so you can talk about whether you receive gradient, whether you receive Hessian now distinguishing between quasi Newton and pure gradient methods is is somewhat hard here because Quasi Newton methods.",
            "We use the gradients but but in the deterministic setting there is some work on the complexity of 2nd order methods as well, again by number of scheduled in the.",
            "I think the 7th chapter of their book has certain results.",
            "I don't really know of something in the stochastic setting and partly this is becausw even from the point of view of upper bounds, there are very few settings in stochastic optimization where I'm familiar you can get.",
            "Much faster rates using second.",
            "There are indeed many settings in which second order information gives you much faster rates.",
            "To the best of my knowledge.",
            "Well.",
            "Oh so.",
            "Actually, your second order, no.",
            "I I I know of 1 setting where where where I only know a quasi Newton method to solve the problem optimally, but even there it's still 1st order information technically, so yeah.",
            "So is your lower bound type Wednesday about sleep?",
            "Yes, because as I well up to log rhythmic factors.",
            "So as I as I told you in, if you exactly follow through with the assumptions, then you can argue that in D dimensions the complexity is going to be squared over epsilon squared.",
            "Now what I'm establishing here is K squared log D over epsilon squared, so up to the log D factor, which becomes lose when.",
            "K becomes close to the.",
            "Log D or K. Oh, that's yeah, that's true.",
            "So that that that becomes.",
            "Yeah, OK, so you have to really keep K to be smaller than D. Yeah.",
            "So is there like a hardest possible stochastic sparse function?",
            "Or is that algorithm pendant?",
            "So first of all, because we are looking at?",
            "So, So what this tells you is that this is this class of functions, in particular, that that I never told you about.",
            "That I construct is going to be the hard class for any algorithm.",
            "Now the class is actually.",
            "It turns out very simple, it's just a linear function plus L1 norm term, and so.",
            "For that class, this lower bound applies, so it tells you even this very simple function.",
            "Now you don't know which candidate from this class is going to be hard for a particular method, but there is some function in this class which is hard for any given method.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is talk about computational lower bounds on the complexity of sparse convex optimization problems.",
                    "label": 0
                },
                {
                    "sent": "An associate mentioned this is joint work with Peter Bartlet, Pradeep Ravikumar and Martin Wainwright so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the recent years, a lot of the focus in convex optimization has shifted to high dimensional problems, and there's a good reason for why this is happening, because there are more and more data sets where we do need to solve convex optimization problems on in very high dimensions.",
                    "label": 1
                },
                {
                    "sent": "So one Canonical example is a lot of the work in the area of computational biology needs to solve optimization problems in order to perform statistical inference on.",
                    "label": 0
                },
                {
                    "sent": "Very large number of variables problems such as collaborative filtering where you might be trying to solve high dimensional matrix completion problem, computational astronomy and so on and so forth.",
                    "label": 1
                },
                {
                    "sent": "There are lots and lots of examples where these high dimensional optimization problems come up, and there's sort of 1 commonality to a lot of these different domains, which is somehow the solution space.",
                    "label": 0
                },
                {
                    "sent": "The final optimum that we compute.",
                    "label": 0
                },
                {
                    "sent": "We want the solution to be sparse.",
                    "label": 0
                },
                {
                    "sent": "An often you would see that the algorithmics schemes that people come up with are actually.",
                    "label": 0
                },
                {
                    "sent": "Once that they try to enforce even the sparsity explicitly on the final solution an there's a couple of good reasons for it.",
                    "label": 0
                },
                {
                    "sent": "There's the obvious computational reason if you come up with a sparse optimum, then you know you can do future computation on it rather cheaply.",
                    "label": 0
                },
                {
                    "sent": "For instance, dot products on sparse vectors are cheap, but there are also good statistical reasons for why you might want to do this so.",
                    "label": 0
                },
                {
                    "sent": "If we take a very simple example of high dimensional sparse optimization problem that people often address that.",
                    "label": 0
                },
                {
                    "sent": "That probably most of you are familiar with.",
                    "label": 0
                },
                {
                    "sent": "By now there's a high dimensional linear regression problem, so you've got some high dimensional vectors XYZ and regression outputs Y eyes and you're trying to estimate a good mapping from the exercise to the wise and one popular estimator in the high dimension is the lawsuit estimator where you try to minimize the squared error between the observations and predictions subject to another non constraint.",
                    "label": 0
                },
                {
                    "sent": "For this talk this.",
                    "label": 0
                },
                {
                    "sent": "L1 norm constraint, for instance, is the is the is the part of interest because what it does is it enforces that the solution Theta hat that you compute to this problem is going to be sparse.",
                    "label": 1
                },
                {
                    "sent": "Now as I said, having a sparse head ahead has obvious computational advantages, but in particular in this high dimensional setting even to show a basic statistical consistency of Terra Haute in estimating.",
                    "label": 0
                },
                {
                    "sent": "Correct mapping, you need the sparsity properties of the estimator.",
                    "label": 0
                },
                {
                    "sent": "Computed are the problem.",
                    "label": 0
                },
                {
                    "sent": "So so these are sort of the two main advantages for for which people prefer these power solutions in high dimension problems.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And as a result of this need, there's been a lot of work in the recent years in developing specialized algorithms to solve these problems fast to solve them efficiently.",
                    "label": 0
                },
                {
                    "sent": "So of course the most classical work kind of predates all the recent flurry, so goes back to number of skin during in 1983 who first proposed the method of mirror descent, also studied more recently by backpack interval and extended in a lot of ways by lot of other people an.",
                    "label": 0
                },
                {
                    "sent": "Is very suited to solving such problems.",
                    "label": 0
                },
                {
                    "sent": "More recent work.",
                    "label": 0
                },
                {
                    "sent": "So there were two very nice papers at NIPS last year.",
                    "label": 0
                },
                {
                    "sent": "There was the forward, backward splitting of Duchenne singer.",
                    "label": 0
                },
                {
                    "sent": "There was regularised dual averaging of Lenschow an.",
                    "label": 0
                },
                {
                    "sent": "Since then we've seen accelerated variants of these algorithms.",
                    "label": 0
                },
                {
                    "sent": "Extensions to mirror dissent Framework again an there's a lot of algorithms now available to solve such problems, and most of these algorithms they come with accompanying theory.",
                    "label": 1
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "Take any of these papers and for any of these specific methods you can conclude that given a desired accuracy level epsilon, how much computational labor you would have to put in to solve the optimization problem to an accuracy epsilon for any of these methods.",
                    "label": 0
                },
                {
                    "sent": "Such bounds exist.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, to the best of my knowledge there is.",
                    "label": 0
                },
                {
                    "sent": "Work in the area of sparse optimization and the fundamental hardness of these problems and by which what I mean is, suppose I want to take the best possible algorithm computationally.",
                    "label": 1
                },
                {
                    "sent": "The most optimal algorithm to solve this problem, then how much labor would that algorithm have to spend in order to solve the problem to an accuracy epsilon?",
                    "label": 0
                },
                {
                    "sent": "And the reason why you might want to understand this is cause you what you really want to get at is how far away from being optimal are the current or algorithms that we know.",
                    "label": 0
                },
                {
                    "sent": "Are the optimal?",
                    "label": 0
                },
                {
                    "sent": "Are the sub optimal and this is the question that we would try and address in this work.",
                    "label": 0
                },
                {
                    "sent": "Then of course just stated this way.",
                    "label": 0
                },
                {
                    "sent": "This is a pretty general question, so before we can go about trying to meaningfully answer this, we need to define what the precise nature of the optimization problem we want to deal with.",
                    "label": 0
                },
                {
                    "sent": "The precise complexity model that we want to answer these questions, and so that's going to be the next.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two slides, so the convex optimization setup is fairly simple and probably most of you are familiar with it, so we're trying to optimize some convex function F over some convex subset of Rd.",
                    "label": 1
                },
                {
                    "sent": "Now we're told.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, this is.",
                    "label": 1
                },
                {
                    "sent": "This is a version of the slides from last night, so there should have been some updates here.",
                    "label": 0
                },
                {
                    "sent": "So we are told that this function belongs to some class script F. And since we are interested in sparse optimization problems here in particular, we assume Script F is only consists of functions that have sparse Optima and I will make this a bit more precise in the later slides.",
                    "label": 0
                },
                {
                    "sent": "But our optimization algorithm is given this information that it is dealing with the problem with the sparsity structure and it's us to come up with a point X which is at most epsilon sub optimal in the function values, right?",
                    "label": 0
                },
                {
                    "sent": "So this is, this is what optimization algorithms do.",
                    "label": 0
                },
                {
                    "sent": "And now how do we measure the complexity of these optimization algorithm?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The model that we propose to use is the Stochastic 1st order model of complexity, which was first looked at in.",
                    "label": 0
                },
                {
                    "sent": "Again, the work of Nemerofsky ended in 1993, so this model applies to iterative optimization methods an every round.",
                    "label": 0
                },
                {
                    "sent": "So the optimization goes for say fixed length of time T at every round.",
                    "label": 1
                },
                {
                    "sent": "The optimization method queries the article with some point XD from the.",
                    "label": 0
                },
                {
                    "sent": "The optimization set S. And the article interprets it as the algorithm's current guess towards the optimum of the function.",
                    "label": 0
                },
                {
                    "sent": "Now what the article does is it response to the algorithm with some information about the behavior of the function at this point XD.",
                    "label": 0
                },
                {
                    "sent": "So in particular, in the Stochastic 1st order model the Oracle returns a pair of random variables.",
                    "label": 0
                },
                {
                    "sent": "F Hadden, ZE hat and.",
                    "label": 0
                },
                {
                    "sent": "In particular.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In particular, the random variable F hat is.",
                    "label": 0
                },
                {
                    "sent": "My laptops totally decided to misbehave for this stock.",
                    "label": 0
                },
                {
                    "sent": "So in particular that the random variable F hat is unbiased for the true function value at F. The random variable Z hat is unbiased for the sub gradient of F at XT, so we're going to be dealing with nonsmooth functions here, so I need to look at the sub derivative set, and we assume that there is bounded noise in the gradients gradient estimates that are given to us by the article, so the intuitive picture is that the optimization starts from some initial point X1.",
                    "label": 0
                },
                {
                    "sent": "The article responds with some information about the gradient of F at X1.",
                    "label": 0
                },
                {
                    "sent": "The algorithm takes this information into.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Account and comes up with its next query point X.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So receives information about the gradient X2 and so on so forth and aft.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sufficiently large number of queries it hopefully comes up to the point XT, which is a good enough estimate to the optimum of the function.",
                    "label": 0
                },
                {
                    "sent": "Anne, this complexity model is of interest to us because all the methods that I mentioned up until so far, so gradient descent, mirror descent, stochastic gradient descent and mirror descent forward, backward splitting regularize, dual averaging, they cannot be implemented in this stochastic 1st order Article model.",
                    "label": 1
                },
                {
                    "sent": "So if we derive any complexity, lower bounds on this model, they will indeed be applicable to all these methods that are actually being used to solve these problems.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the quantities of interest that we want to understand in this article model for a given method M and a given function F we want to look at the optimizer.",
                    "label": 0
                },
                {
                    "sent": "The expected optimization error that the method incurs after T rounds.",
                    "label": 0
                },
                {
                    "sent": "Now I have to take an expectation here because the article is giving me random responses, so there's some randomness in the algorithms.",
                    "label": 0
                },
                {
                    "sent": ".3 points as well.",
                    "label": 0
                },
                {
                    "sent": "So this expectation is over that randomness an we can Alternatively ask what's the article complexity which is the smallest number of queries that you have to make.",
                    "label": 0
                },
                {
                    "sent": "To get to an expected error of at most epsilon in optimization.",
                    "label": 0
                },
                {
                    "sent": "Now this of course makes sense once you have specified an optimization method, But what we want to look at is an algorithm independent lower bound on complexity.",
                    "label": 0
                },
                {
                    "sent": "So we want to understand the minimum minimax complexity where you ask if you were to take the best algorithm and best in the sense that for that algorithm, if you take the worst case Oracle complexity overall functions in the class script F. So remember Script F is the class of.",
                    "label": 1
                },
                {
                    "sent": "Is going to be this class of all sparse functions.",
                    "label": 0
                },
                {
                    "sent": "So if you take the worst case Oracle complexity overall functions in that class, then what's the algorithm with the best worst case performance?",
                    "label": 0
                },
                {
                    "sent": "And if you can lower bound the complexity of that algorithm, then you've have understood the fundamental hardness of the function class script F in the article complexity model so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is this is of course, as I said, not the first time this class is being looked at, so the classical result goes back to Nemerofsky Newton, who showed that in the setting for of when you said script after the class of all convex Lipschitz functions, then the minimax complexities of one over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "Now in the setting of interest here actually.",
                    "label": 0
                },
                {
                    "sent": "What we can show is that the min Max complexity is going to be in D dimensions.",
                    "label": 0
                },
                {
                    "sent": "Going to be as high as of D squared over epsilon squared and this was shown in some of our recent work and the exact assumptions will be will be clarified on the next slide under which this result holds.",
                    "label": 0
                },
                {
                    "sent": "Now this is pretty pathetic in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "I mean, this tells you that if you want to solve the problem too and even moderate accuracy, the amount of computational complexity, the number of iterations you will have to perform is just prohibitively large.",
                    "label": 0
                },
                {
                    "sent": "But we know that people solve these sparse optimization problems and they solve it in a reasonable amount of time, so.",
                    "label": 0
                },
                {
                    "sent": "So there must be something that gives and what we have to notice that these results were developed for the case of general convex Lipschitz optimization, they do not take into account that functions that we're looking at have the sparsity structure.",
                    "label": 0
                },
                {
                    "sent": "So the question we want to understand is if you impose this additional sparsity assumption on the optimum of the problem, then does that buy you anything?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Can you do something better when your optimization problem is sparse?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to do that, we look at the class of all sparse convex functions.",
                    "label": 1
                },
                {
                    "sent": "So these are functions for which there is at least one optimum XF that has at most K non zero entries.",
                    "label": 0
                },
                {
                    "sent": "OK, so so these functions can have multiple Optima.",
                    "label": 0
                },
                {
                    "sent": "You can have dense Optima.",
                    "label": 0
                },
                {
                    "sent": "I don't care about that.",
                    "label": 1
                },
                {
                    "sent": "What I want is there should be at least one good optimum, one optimum with at most K nonzero entries.",
                    "label": 0
                },
                {
                    "sent": "In addition to that, I'm going to assume that the function is convex.",
                    "label": 0
                },
                {
                    "sent": "And Lipschitz inappropriate norm.",
                    "label": 0
                },
                {
                    "sent": "An under these these assumptions.",
                    "label": 0
                },
                {
                    "sent": "First of all, the if you further assume that this set S can look like an L Infinity ball.",
                    "label": 0
                },
                {
                    "sent": "So it's just a box, then the D squared over epsilon squared result that I showed on the previous slide without the sparsity assumption applies.",
                    "label": 0
                },
                {
                    "sent": "Now when you add in the sparsity structure of the optimum.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then what you can show is the minimax complexity is actually this quantity shown here.",
                    "label": 0
                },
                {
                    "sent": "So the key thing to note is that this is only logarithmic in the ambient dimension D, instead of the bad quadratic scaling, which does explain why you can solve these high dimensional problems when you have this additional sparsity assumption.",
                    "label": 0
                },
                {
                    "sent": "Of course there is a scaling with the sparsity level of the optimum, because we know that without sparsity in general, we do not expect this log rhythmic scaling.",
                    "label": 0
                },
                {
                    "sent": "Now lower bound by itself is kind of not that interesting unless you can understand how it exists, how it relates to the upper bounds on the existing methods.",
                    "label": 0
                },
                {
                    "sent": "In this case, it turns out this lower bound is actually sharp.",
                    "label": 1
                },
                {
                    "sent": "So if you take the class of methods of mirror descent and take a particular member from that family by tuning the prox function appropriately, then then this lower bound is actually exactly attained by a method of mirror descent.",
                    "label": 1
                },
                {
                    "sent": "So that tells us that.",
                    "label": 0
                },
                {
                    "sent": "Two things.",
                    "label": 0
                },
                {
                    "sent": "First of all, this lower bound is unimprovable without further assumptions, because there's a method that meets the lower bound, and Secondly, it also establishes the optimality of that particular member from the class of methods of mirror descent, so we know now that there exists an optimal method to solve these problems.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the remaining time I would like to give you a brief sketch of how such a result is proved.",
                    "label": 0
                },
                {
                    "sent": "So the first step is to discard this entire function class and only look at a smaller, much smaller subset of it which is somehow which captures the true complexity of the class.",
                    "label": 0
                },
                {
                    "sent": "So, so we try to identify a hard subset of functions in the class, and this hard subset is basically going to be a subset that has a lot of functions still, but each pair of functions in that class.",
                    "label": 1
                },
                {
                    "sent": "Is well separated in a certain sense.",
                    "label": 0
                },
                {
                    "sent": "What does bias as it allows us to establish a link from optimization to function identification.",
                    "label": 1
                },
                {
                    "sent": "These functions are going to be separated enough that if you can optimize them, then you basically know which function that the optimum can only belong to.",
                    "label": 0
                },
                {
                    "sent": "One of these functions, so you will be able to establish a one to one connection.",
                    "label": 0
                },
                {
                    "sent": "Now if you recall the stochastic Oracle does not give you exact function gradient values, it only gives you noisy samples of those quantities.",
                    "label": 0
                },
                {
                    "sent": "So now you're looking at these noisy samples generated according to a function.",
                    "label": 0
                },
                {
                    "sent": "And you're solving a function identification problem from the data.",
                    "label": 0
                },
                {
                    "sent": "So this almost smells like some sort of statistical estimation going on here an in fact.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "So let's go.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so first thing I want to explain is the how much time do I have either way.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll try to be quick so OK, thanks.",
                    "label": 0
                },
                {
                    "sent": "Sorry about the interruption.",
                    "label": 0
                },
                {
                    "sent": "OK so the first thing I want to explain is the sense of separation that we use here to design our function class.",
                    "label": 0
                },
                {
                    "sent": "And it turns out the standard notions of distance between functions kind of end up being unsuitable for us.",
                    "label": 0
                },
                {
                    "sent": "So we end up defining our own notion of separation between convex functions.",
                    "label": 0
                },
                {
                    "sent": "Which is denoted by this role symmetric and what it measures is the difference between the optimal function value, the smallest function value of the function F + G, and the smallest values of the functions FNG individually, so it measures this gap.",
                    "label": 0
                },
                {
                    "sent": "And this is certain nice properties.",
                    "label": 0
                },
                {
                    "sent": "You can show that it's strictly positive unless F&G are optimized on a common point, so as long as F&G do not share an optimum.",
                    "label": 0
                },
                {
                    "sent": "This this separation is going to be non zero and intuitively what it tries to capture is how different FG look for optimization.",
                    "label": 1
                },
                {
                    "sent": "So if if F&G have a structure where the optimum of G is somehow good enough optimum for F as well, then the separation will be small, otherwise it will be large.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What does separation buys us?",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "What does separation buys us?",
                    "label": 0
                },
                {
                    "sent": "Is it essentially allows us to make this connection from optimization to function identification, so So what you can show is that if two functions are at least Delta apart in the row sense, then any point that optimizes the function F well cannot optimize G well and vice versa.",
                    "label": 1
                },
                {
                    "sent": "So in the picture here, if you take the point X one, then it's a good estimate of the optimum for the read function.",
                    "label": 0
                },
                {
                    "sent": "But it's a terrible estimate of the optimum for the blue function.",
                    "label": 0
                },
                {
                    "sent": "And similarly, X2 is a good approximate optimum for the blue function, but not for the read function.",
                    "label": 0
                },
                {
                    "sent": "Now, once you get this property, you can bootstrap from 2 functions to the entire function class script F and say OK.",
                    "label": 1
                },
                {
                    "sent": "So if I have a class script F where all the functions, every pair of functions is separated in the sense of row, then for any given point X there's going to be at least at most one function that is approximately optimized at that point.",
                    "label": 0
                },
                {
                    "sent": "Now this is a very powerful thing because what it allows you to say is I can run my optimization algorithm to the end.",
                    "label": 0
                },
                {
                    "sent": "Look at the final solution that's generated, which is an approximate optimum for the function being used by the article.",
                    "label": 0
                },
                {
                    "sent": "I know that the point that the algorithm returns to me can be the approximate optimum for only one function in my class.",
                    "label": 0
                },
                {
                    "sent": "If I have row separation, and so I know the function that the article used.",
                    "label": 0
                },
                {
                    "sent": "So even even if the algorithm is not trying to identify the function, I can run it and.",
                    "label": 0
                },
                {
                    "sent": "From the output I can.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reconstruct the articles function.",
                    "label": 0
                },
                {
                    "sent": "The next step is to design this function class to be this hard subset of functions that we can embed into our function class, which will have this property that it's going to be optimized data, sparse point, and that's going to be well separated in the sense of row.",
                    "label": 0
                },
                {
                    "sent": "So this class of functions we design is indexed by vectors in minus 101 to the D, and we in particular pick sparse vectors, so vectors that have at most at most K entries where you have plus minus one values.",
                    "label": 1
                },
                {
                    "sent": "The rest are all zero.",
                    "label": 0
                },
                {
                    "sent": "And with each such vector, you associate a function G Alpha an.",
                    "label": 0
                },
                {
                    "sent": "The idea is that this IIS coordinate of the vector Alpha it supplies you.",
                    "label": 0
                },
                {
                    "sent": "Basically with this averaging wait.",
                    "label": 0
                },
                {
                    "sent": "So you use these words half plus Alpha, Delta in half minus Alpha Delta to average some base functions, and these basis functions they act only along one coordinate.",
                    "label": 0
                },
                {
                    "sent": "So for each coordinate you take a particular base function and these have to be again picked appropriately and carefully.",
                    "label": 0
                },
                {
                    "sent": "But you pick some functions of this form.",
                    "label": 0
                },
                {
                    "sent": "And average them with these appropriately chosen weights.",
                    "label": 0
                },
                {
                    "sent": "Now there are a couple of properties you have to ensure.",
                    "label": 0
                },
                {
                    "sent": "First is we try to pick the base functions in such a way that sparsity of the vector Alpha implies sparsity of the optimum of G Alpha because we remember we want the Alpha to be optimized data, sparse point.",
                    "label": 1
                },
                {
                    "sent": "The second thing we try to ensure is that we can, we can.",
                    "label": 0
                },
                {
                    "sent": "We can keep these vectors G Alpha far.",
                    "label": 0
                },
                {
                    "sent": "We can keep these functions G Alpha and beta far away for two different vectors Alpha and beta and the way we try to ensure this is we try to ensure that if if vector Alpha is very different from a vector beta in the sense of having distance then the corresponding functions are going to be far away as well.",
                    "label": 0
                },
                {
                    "sent": "And again this this relies heavily on picking these base functions carefully, so module.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that then we can easily design a first order Oracle.",
                    "label": 0
                },
                {
                    "sent": "Now for this problem, which is really the final ingredient in the proof where you interpret this averaging weight of half plus Alpha Phi Delta on the ice coordinate as the bias of a coin.",
                    "label": 1
                },
                {
                    "sent": "And instead of actually averaging functions under this bias, you average them according to the outcome of the coin.",
                    "label": 0
                },
                {
                    "sent": "So every every round the Oracle flips one coin per coordinate.",
                    "label": 0
                },
                {
                    "sent": "Observe the outcome of the coin now.",
                    "label": 0
                },
                {
                    "sent": "Averages them according to the outcome of the coin.",
                    "label": 0
                },
                {
                    "sent": "So remember we are is going to be either zero or one, so the article is either going to pick for the Earth, coordinate the function FI plus, or pick the function FI minus and give you the function value and gradient in that coordinate according to pick function according to the outcome of the coin.",
                    "label": 1
                },
                {
                    "sent": "So now we have made an explicit connection between.",
                    "label": 0
                },
                {
                    "sent": "These these coin tosses and optimization, and in particular if you identify.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function G Alpha, which is equal into identifying the vector Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we're trying to estimate this vector Alpha from the outcomes of coin tosses that we're observing.",
                    "label": 0
                },
                {
                    "sent": "So you're observing these coin tosses, and you're trying to observe the you're trying to estimate the bias of the coins that are used to generate the tosses.",
                    "label": 0
                },
                {
                    "sent": "So this is, I mean really, the most classical problem in statistics now, so you can easily come up with lower bounds on the sample complexity, the number of coin flips you need to see now, remember, every query to the article gives you a certain number of coin flips, so once you know the number of coin flips, you need to see, you can.",
                    "label": 0
                },
                {
                    "sent": "You can directly come up with the number of queries that the algorithm will need to make an by following through this logic carefully, you can essentially prove the result that I showed.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Earlier, so to wrap up what this work establishes, are sharp minimax lower bounds on the Oracle complexity of sparse convex optimization problems, and the key idea was to do this reduction from optimization to statistical estimation and then use information theoretic lower bounds on the sample complexity of estimation problems.",
                    "label": 1
                },
                {
                    "sent": "In the process we identify this role symmetric.",
                    "label": 0
                },
                {
                    "sent": "That actually seems like a rather natural distance measure for convex functions and might have other users in optimization.",
                    "label": 0
                },
                {
                    "sent": "Problems and here I talked about the optimality of one of the methods mirror designed for for actually solving the problems, But you can actually show some of the other methods are also shown to be optimal VR results, so as a result our work also provides a certificate of optimality for some of the popularly used methods to solve these sparse convex optimization problems, so that's all.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To say and thanks a lot in particular for bearing with all the heavy troubles.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Has it been in similar work?",
                    "label": 0
                },
                {
                    "sent": "I'm in emacs.",
                    "label": 0
                },
                {
                    "sent": "Sounds for defining Matthews International interest.",
                    "label": 0
                },
                {
                    "sent": "So in the.",
                    "label": 0
                },
                {
                    "sent": "In the.",
                    "label": 0
                },
                {
                    "sent": "Deterministic optimization setting I know of work for neutral methods in particular or not.",
                    "label": 0
                },
                {
                    "sent": "Newton method for 2nd order optimization.",
                    "label": 0
                },
                {
                    "sent": "So here remember, we're only we're associating methods with sort of the information they receive about the function, so you can talk about whether you receive gradient, whether you receive Hessian now distinguishing between quasi Newton and pure gradient methods is is somewhat hard here because Quasi Newton methods.",
                    "label": 0
                },
                {
                    "sent": "We use the gradients but but in the deterministic setting there is some work on the complexity of 2nd order methods as well, again by number of scheduled in the.",
                    "label": 0
                },
                {
                    "sent": "I think the 7th chapter of their book has certain results.",
                    "label": 0
                },
                {
                    "sent": "I don't really know of something in the stochastic setting and partly this is becausw even from the point of view of upper bounds, there are very few settings in stochastic optimization where I'm familiar you can get.",
                    "label": 0
                },
                {
                    "sent": "Much faster rates using second.",
                    "label": 0
                },
                {
                    "sent": "There are indeed many settings in which second order information gives you much faster rates.",
                    "label": 0
                },
                {
                    "sent": "To the best of my knowledge.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Oh so.",
                    "label": 0
                },
                {
                    "sent": "Actually, your second order, no.",
                    "label": 0
                },
                {
                    "sent": "I I I know of 1 setting where where where I only know a quasi Newton method to solve the problem optimally, but even there it's still 1st order information technically, so yeah.",
                    "label": 0
                },
                {
                    "sent": "So is your lower bound type Wednesday about sleep?",
                    "label": 0
                },
                {
                    "sent": "Yes, because as I well up to log rhythmic factors.",
                    "label": 0
                },
                {
                    "sent": "So as I as I told you in, if you exactly follow through with the assumptions, then you can argue that in D dimensions the complexity is going to be squared over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "Now what I'm establishing here is K squared log D over epsilon squared, so up to the log D factor, which becomes lose when.",
                    "label": 0
                },
                {
                    "sent": "K becomes close to the.",
                    "label": 0
                },
                {
                    "sent": "Log D or K. Oh, that's yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "So that that that becomes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so you have to really keep K to be smaller than D. Yeah.",
                    "label": 0
                },
                {
                    "sent": "So is there like a hardest possible stochastic sparse function?",
                    "label": 0
                },
                {
                    "sent": "Or is that algorithm pendant?",
                    "label": 0
                },
                {
                    "sent": "So first of all, because we are looking at?",
                    "label": 0
                },
                {
                    "sent": "So, So what this tells you is that this is this class of functions, in particular, that that I never told you about.",
                    "label": 0
                },
                {
                    "sent": "That I construct is going to be the hard class for any algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now the class is actually.",
                    "label": 0
                },
                {
                    "sent": "It turns out very simple, it's just a linear function plus L1 norm term, and so.",
                    "label": 0
                },
                {
                    "sent": "For that class, this lower bound applies, so it tells you even this very simple function.",
                    "label": 0
                },
                {
                    "sent": "Now you don't know which candidate from this class is going to be hard for a particular method, but there is some function in this class which is hard for any given method.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}