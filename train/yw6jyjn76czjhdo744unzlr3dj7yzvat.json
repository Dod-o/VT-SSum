{
    "id": "yw6jyjn76czjhdo744unzlr3dj7yzvat",
    "title": "QueryPIE: Backward reasoning for OWL Horst over very large knowlege bases",
    "info": {
        "author": [
            "Jacopo Urbani, Vrije Universiteit Amsterdam (VU)"
        ],
        "published": "Nov. 25, 2011",
        "recorded": "October 2011",
        "category": [
            "Top->Computer Science->Semantic Web->OWL - Web Ontology Language"
        ]
    },
    "url": "http://videolectures.net/iswc2011_urbani_reasoning/",
    "segmentation": [
        [
            "Tom just going presenting one slight.",
            "OK, So what is website your query pie in one slight OK."
        ],
        [
            "So very simply, I read it is an hybrid reasoning engine that can scale to billion triples.",
            "OK, it does that because he basically.",
            "What it does is it reduces the complexity of pure backward chaining by doing some small precomputation.",
            "OK, so this precomputation is in the order of seconds against the hours of the you will need.",
            "If you do full materialization and the good thing is because we reduce the complexity backward chaining we keeps.",
            "The response time still in the order of milliseconds.",
            "So for you, for the user there is really not much difference.",
            "So this is query pieing once light so I see now that some of you are not really satisfied in my explanation and I have other 29 minutes left so I'm going to repeat exactly the same store."
        ],
        [
            "A little bit longer, let's say with 70 slides and this story start a little bit earlier."
        ],
        [
            "OK, start from the question what is rule based?",
            "Reason OK so quit Peiser who based reasoning so I assume you're all familiar with reasoning at the F and so on.",
            "So I just dedicated one of these 70s light to describe exactly what PewDiePie wants to do.",
            "So we have a set of data right?",
            "So billions of triples and we have a set of rules.",
            "OK, we exploit the semantics of our language while we want to do is apply this set of rules to our data.",
            "Studer Ave conclusions.",
            "Very simple.",
            "We can do this.",
            "This operation in two ways right?",
            "You can read the forward chaining way, so we apply all the rules at the beginning again with arrive everything we can or we can do the backward chaining way.",
            "So when the user queries our knowledge base we actually look for what which rules can actually derive some conclusions and so on in a backward chaining way, OK?"
        ],
        [
            "Each of these ways has advantages and disadvantages, so the advantage of classical forward chaining is that you could do it only once.",
            "OK, and after you did it while querying becomes just a kind of database problem, so you don't need to do reasoning anymore.",
            "The disadvantage is that if your data changes, you have to recompute entire closure, and it takes hours.",
            "On the other side, the advantage of backward chaining is actually we do reasoning only on the on the data that we care.",
            "OK, on the other data that the user is asking for and we've got everything else outside and the problem is that because the rules are kind of generic, even if you're interested in a small subset of our knowledge base, we might have to perform reasoning over much more in order to guarantee that our answer set is complete.",
            "OK, so each of these two methods have advantages disadvantage."
        ],
        [
            "So currently forward chaining has is the method that has shown the best capability in the last two years.",
            "We have been presenting these web Pi work which actually can scale quite well in our largest experiment we show it that you could perform the entire closure over 100 billion triples, which is like four times the entire semantic web now, so it scales well.",
            "But we still have to deal with intrinsic problems of forward chaining.",
            "So how do we do if our data changes very much, for example, and that's the motivation that drove us to come up with a different type of reasoning and that query pie.",
            "So we we we query pie what we try to do is we try to combine advantages of forward chaining and backward chaining in order to get closer to a kind of more query driven form of reasoning, more webby.",
            "So we can call it so kind of reasoning can deal better with updates and so."
        ],
        [
            "So query PY."
        ],
        [
            "I start by saying that the input of our reasoning algorithms is a triple pattern WHI.",
            "So users normally query knowledge base is using sparkle, right?",
            "So Sparkle is the factor language that we use.",
            "So if we look when reasoning is used in a sparkle query you will see that actually we need it the moment that we look up for data in our knowledge base.",
            "OK, we don't need it when we need to do perform the data joints, but we need it when we need to look up for data.",
            "Because of it, we decided to abstract our metal from the sparkled engine OK and focus on the problem where we actually have to look up for data in a knowledge base.",
            "And that's why our input is a triple pattern.",
            "So I said before they can qualify.",
            "We do a little bit of precomputation, So what is the precomputation about?",
            "So basically what we want to do, we want to perform the closure of the schema or terminological triples.",
            "OK, we do the closure over these triples before hand and reasoning about all other three post is left at query time, so the schema triples the kind of vague definition, so I will narrow it down to a more pragmatic one.",
            "So we consider in our work as terminological triples as old triples that have either an earlier fessor alter as a predicate, or objects that are actually used.",
            "Our inference rules.",
            "I report some examples to give you a better idea so.",
            "For example, X subclass of why this is a pattern that contains only schema triples, right?",
            "Or X property of or type transitive property.",
            "So these are all we consider these are all schema triples and what we want to do is compute the closure.",
            "So calculate all subclasses.",
            "Also properties all type transitive properties."
        ],
        [
            "So the question is, why do we do that?",
            "Three reasons, namely the first reason we observed the schema doesn't change that much, at least in web data.",
            "OK, so once we computed the closure of a schema, it's A kind of stable.",
            "We don't need to repeat this process many times.",
            "Secondly, we also observed that on web data, the schema is much smaller than the rest of the data, so it's few orders magnitude less than our data.",
            "So if we have billions of triples we can have.",
            "Few hundreds of classes or thousands much smaller.",
            "So if you compute the closure of this subset, it's kind of limited.",
            "And the third reason, even more important, because the schema triples are used very, very much in our reasoning process.",
            "So if we compute the closure over this week and save a lot of computation, and I'll show later exactly how much computationally save.",
            "So, more formally, in qualify we define two algorithms.",
            "First, we call it terminological closure, and this algorithm does exactly what he says.",
            "It computes the closure of the terminological triples.",
            "It's executed before query time.",
            "So before the user can actually use the knowledge base, the second algorithm is, we call it terminology independent reasoning, and this is actually the reasoning that we do at query time when the users ask for some data.",
            "So you expect me to explain before the terminological closure and then move to the second.",
            "I will do the other way around.",
            "I will start.",
            "I will first explain terminology, independent reasoning and then I'll move to the terminological closure and you will see later why I do that.",
            "OK, so let's focus on the second one, terminal."
        ],
        [
            "Edge independent reasoning.",
            "Very simply, this algorithm is nothing less than classical backward chaining.",
            "So if you have an input, a query name, a triple pattern as input, you look for what rule, which rules can actually derive triples that belong to that pattern.",
            "Then you look down the antecedents of these rules and you go down till you reach your knowledge base.",
            "When you reach a knowledge base then you are in a position to see whether your data matches and then you go up and you proceed with your derivation.",
            "So it's nothing else with then classical backward chaining with only one difference.",
            "Whenever we whenever we need to infer a schema, triples whenever in our reasoning three we have to infer for schema triples, we stop, we go immediately on the knowledge base.",
            "In other words, with this algorithm we assume that the schema is closed, so that we have all the schema.",
            "Triples are explicit in our knowledge base.",
            "We do not perform any reasoning.",
            "Those triples.",
            "Because we make this assumption, we can actually implement some really nice optimizations.",
            "OK, so for example, what we can do is that because we have, we assume that we have a schema triples whenever we have two rules that might arrive, one parents ruler, one child rule, and the child might arrive something for the parents.",
            "By looking at this scheme antecedents, we can look whether there is an intersection.",
            "If there is no intersections, the child ruquier never fire anything.",
            "To the parents so we can prove that branching tree these optimization aims to cut down further cut down the complexity of our reasoning tree.",
            "Another thing that we can do is that because our scheme we have our schema triples.",
            "We can replicate it to all the nodes in our cluster and whenever we have to execute his reasoning T reasoning three reason operation we can do locali OK because the schemas in memory.",
            "Obviously, this the assumption that we make is wrong, right?",
            "So it might not be the case that our schema is is is explicit in our knowledge base.",
            "OK, so in order to make this algorithm complete, we need to calculate all the schema triples and that's exactly the purpose of the previous algorithm, this terminological closure."
        ],
        [
            "So I'm not going to talk about it.",
            "There is a problem.",
            "The big one.",
            "We cannot use traditional forward chaining methods to derive a partial closure because the rules are so generic that we cannot identify a subset of our data or a subset of rules so that we can guarantee that our partial closure is complete.",
            "OK, so we can reuse tools like for example web, PY, becausw we can guarantee it will actually be complete.",
            "We have to come if we use whereby we have to do the full closure.",
            "OK, so this is a problem and the solution that we propose.",
            "Actually what we can do is to exploit these backward chaining.",
            "Reason that I just explained before for this task.",
            "So what do we do?",
            "We start assuming that our scheme is complete.",
            "Obviously this assumption is wrong.",
            "At the beginning we query our backward chaining algorithm, disturbing alot terminology, independent reasoning.",
            "Put schema I say give me your subclasses.",
            "This backward chaining will drive something, but not everything OK, because this assumption is wrong.",
            "So what we do we take our derivation, we put it back in a knowledge base and we repeat this process until we stop deriving anything.",
            "In other words, what we're doing, we're doing kind of forward chaining.",
            "So we repeating this process, but we use backward chaining steps and by using backward chaining steps.",
            "Watch what we're actually doing.",
            "We're touching only the data that we care of.",
            "So it's faster."
        ],
        [
            "OK, enough about content now implementation, so in theory the algorithms that that I just presented can work with whatever reason you have.",
            "But because we wanted to scale we came up with a distributed implementation.",
            "So what we did we wrote this prototype in Java and we use this framework that we develop our universities called Ibis.",
            "It's a framework that makes life easier if you have a cluster and notes.",
            "I have to talk to each other.",
            "And we indexed our triples with four indexes and we partition across the notes.",
            "OK, So what happens in our prototype is that.",
            "Once a node receives a query as an input, what he does he built this reasoning tree locally and then he looks which notes have the relevant data to resolve these three and send this to the note that has it.",
            "And this note receives this reason.",
            "This part of the reasoning tree executed and returned the Triple City user.",
            "So very simple, very simple.",
            "So now how does it perform?"
        ],
        [
            "Valuation of this method.",
            "The first claim I made in this presentation is that in query pie we can actually reduce the complexity of pure backward chaining.",
            "So exactly how much do we reduce this complexity?"
        ],
        [
            "Right, So what we did is that we took some.",
            "Oh, sorry before right before I go to that point, I must say we want to compare ourselves against the current state of the art.",
            "OK, so that's the target of our evaluation.",
            "The state of the art at the moment is forward chaining, so we took web PY one for all.",
            "The problem is that web PY is working with map reduce with a completely different technology.",
            "So we have to reduce this external overhead as much as we can.",
            "OK, So what we did?",
            "We consider two scenarios.",
            "The first one is this full materialization scenario.",
            "OK is a scenario where you have some data, you apply the foreclosure and then you just queried without reasoning.",
            "So we did.",
            "We just took our data set.",
            "We use web PY.",
            "We calculated everything and then we load it in our prototype and whenever we had to query the data we deactivated reasoning.",
            "OK, the second case is actually our case.",
            "You know this query pykes, so in this case what we did, we loaded the data in our prototype.",
            "We perform this partial closure and then whenever we have to ask it we we activated reasoning.",
            "Alright, we use exactly the same machines, so we use eight machines of our cluster an we consider three datasets LDS are, which is a nice cool fact forge, which is a generic data set.",
            "I linked life data and lubian.",
            "They're both, they all three around 1 billion triples.",
            "OK, so going back to what I said before our goal, our first experiment.",
            "The goal of our first experiment was to calculate how much we are able to reduce the complexity of pure backward chaining."
        ],
        [
            "So what we did, we took some queries.",
            "And we estimate it how.",
            "How large our reasoning tree will be if we perform the pure backward chaining.",
            "For example for is an estimation by the fact.",
            "So probably the.",
            "The real trees even bigger, so in the first query if we do backward chaining, we have to we have to resolve at three, which is 174 leaves.",
            "So we have to perform 174 look apps in order to derive everything with our methods.",
            "We were able to cut down the size of these 3 to 21 leaves, so depending on the query we were able to reduce the pure reasoning complexity of the best case one order of magnitude.",
            "So there is a significant significant reduction of the computational complexity.",
            "Alright, so now the second claim I made is that we do that paying the cost of a small precomputation.",
            "So how much is this cost?",
            "If we compare it to web?",
            "PY because if it is more or less the same, then the entire approach is pointless."
        ],
        [
            "OK, so in our datasets.",
            "Performing this terminological closure is in the order of seconds.",
            "So for Ruby and for example, it takes 8 seconds.",
            "If we perform the full materialization using web PY, it takes one hour and 15 minutes.",
            "So again, the price we pay is between one and two order of magnitude less than what we will have to pay.",
            "If we do the full materialization.",
            "So now there is one last question we need to solve.",
            "So far what I've shown is that yes, we are able to reduce the computational complexity by in the best case, one order of magnitude, the price we pay is very small, but still it can be the case that even we reduce the complexity of reasoning.",
            "When we compare it to the case where no reason is involved and that is the full materialization scenario, the difference is too big that it makes it not worth.",
            "Alright, So what we wanted to measure is exactly how much reasoning is slowing down our execution time.",
            "So I."
        ],
        [
            "Again, we took some queries in a paper that many mores he reports some as an example from the three data sets.",
            "Obviously reasoning is lower than just doing a data.",
            "Look up, you know there is no escape for that.",
            "But the good news is that in both cases the response time stays in the order of milliseconds.",
            "For example, the first query if we don't do any reasoning, it will take 3.32 milliseconds to answer it.",
            "OK, if we do our hybrid reasoning.",
            "It will take 3.38 so OK it's 2% slower, but for a user point of view there is basically no difference.",
            "OK.",
            "So."
        ],
        [
            "Take home message."
        ],
        [
            "Whatever like that you remember after this talk is that yes, on fly inference is possible, you can do that and you can do it on a large scale.",
            "You have to do things right.",
            "You have to do some kind of pre computation, but it's possible.",
            "This way of doing reason is actually much more convenient than performing a few metalization.",
            "Specially if your data changes very frequently, so it's more much more a webby way of performing reasoning.",
            "And even if there is also an ice cream pie, is the first prototype, so there is still too many open questions that need to be answered in the future.",
            "So for example.",
            "How does it work with Sparkle, right?",
            "So we buy purpose consider our as our input is triple patterns, But what happens the moment we put a sparkle engine on the top of it?",
            "So we implemented this prototype in a way that we could put whatever engine we wanted, but actually we have to see if the two things are compatible.",
            "Another thing that we plan to do is that in this prototype we consider only the old horse rules which are sufficiently complex, but they're not a standard in the last year, the Community came out with this out, where L fragment, which is a standard.",
            "So we plan to move.",
            "Our implementation to this new rule set.",
            "OK, so.",
            "I leave this like like this.",
            "OK before I give back to the chair.",
            "I thank you again for listening and I will be very happy to answer your questions.",
            "So thanks very much.",
            "Have you considered because I didn't see any definitions in your talk, so I'm not completely sure what do you mean by materializing everything in the schema.",
            "But have you considered that this could be infinite?",
            "That the material all the schema consequences that matter query answering could be an infinite set, so we consider these rulesets where it guarantees that there is a finite answer set.",
            "So yes, you're right in if we consider the whole logic, it's infinite, but in our ruleset it's it's in is finished.",
            "So in Horst it's guaranteed to finish.",
            "This is a bit.",
            "Kind of also like a commercial.",
            "There is another technique for semantic index that was introduced early this year.",
            "This is for our DFS and Alto QL is based on the same idea of computing the consequences of the terminology 1st and then doing something with that.",
            "However, there you don't have to pay any materialization cost of the data triples.",
            "You don't have to do forward chaining nor backward chaining.",
            "I think you should consider it also to combine it with what you're doing.",
            "It's it was introduced in the in the DL context.",
            "It was for the light, but the same idea.",
            "However, without the pay having to pay for even the backward chaining, that's OK.",
            "I was not aware of this work, thanks.",
            "OK so I will ask so just to minimize waste so give them.",
            "Thank you, I was wondering how the interplay of forward chaining and backward chaining relates to what is achieved with magic sets transformation.",
            "Yeah, so we one of the optimizations that we implemented is very close to what happens with magic sets.",
            "Is this binding propagation?",
            "So yeah, there are.",
            "There is related, but the novelty is that this is possible only because our schema is complete.",
            "So if we don't have the schema.",
            "So we cannot apply these optimizations.",
            "So the two things are related to each other, so yes, there is.",
            "There's definitely a correlation.",
            "It's likely different because the domain is different.",
            "Can you tell us a little bit about the partitioning on the cluster, I mean.",
            "Maybe some of the details there.",
            "Yes, so by purpose we didn't want to go into the storage problem because it's a really challenging problem.",
            "How to efficiently store the data?",
            "So what we did is just we went for the simplest way.",
            "So we index.",
            "We sort our triples according to its four indexes, and then we split it.",
            "In a cross in notes.",
            "So just wondering what the challenges would be moving forward to L2 or L. Overall, Horst.",
            "I don't.",
            "I don't see any.",
            "I don't see any challenge to be honest, but I don't know.",
            "I mean, in the rules are very similar, so I expect the performance to be the same.",
            "So without well you have rules with lists in the body which would cost probably some issues.",
            "Yes you have them also in Horst.",
            "No, no sorry water and the least you mean yes, yeah, yeah, yeah yeah, you're right yeah yeah yeah.",
            "New tricky."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tom just going presenting one slight.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is website your query pie in one slight OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So very simply, I read it is an hybrid reasoning engine that can scale to billion triples.",
                    "label": 1
                },
                {
                    "sent": "OK, it does that because he basically.",
                    "label": 0
                },
                {
                    "sent": "What it does is it reduces the complexity of pure backward chaining by doing some small precomputation.",
                    "label": 0
                },
                {
                    "sent": "OK, so this precomputation is in the order of seconds against the hours of the you will need.",
                    "label": 0
                },
                {
                    "sent": "If you do full materialization and the good thing is because we reduce the complexity backward chaining we keeps.",
                    "label": 1
                },
                {
                    "sent": "The response time still in the order of milliseconds.",
                    "label": 0
                },
                {
                    "sent": "So for you, for the user there is really not much difference.",
                    "label": 0
                },
                {
                    "sent": "So this is query pieing once light so I see now that some of you are not really satisfied in my explanation and I have other 29 minutes left so I'm going to repeat exactly the same store.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A little bit longer, let's say with 70 slides and this story start a little bit earlier.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, start from the question what is rule based?",
                    "label": 0
                },
                {
                    "sent": "Reason OK so quit Peiser who based reasoning so I assume you're all familiar with reasoning at the F and so on.",
                    "label": 0
                },
                {
                    "sent": "So I just dedicated one of these 70s light to describe exactly what PewDiePie wants to do.",
                    "label": 0
                },
                {
                    "sent": "So we have a set of data right?",
                    "label": 0
                },
                {
                    "sent": "So billions of triples and we have a set of rules.",
                    "label": 1
                },
                {
                    "sent": "OK, we exploit the semantics of our language while we want to do is apply this set of rules to our data.",
                    "label": 0
                },
                {
                    "sent": "Studer Ave conclusions.",
                    "label": 0
                },
                {
                    "sent": "Very simple.",
                    "label": 0
                },
                {
                    "sent": "We can do this.",
                    "label": 0
                },
                {
                    "sent": "This operation in two ways right?",
                    "label": 0
                },
                {
                    "sent": "You can read the forward chaining way, so we apply all the rules at the beginning again with arrive everything we can or we can do the backward chaining way.",
                    "label": 0
                },
                {
                    "sent": "So when the user queries our knowledge base we actually look for what which rules can actually derive some conclusions and so on in a backward chaining way, OK?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each of these ways has advantages and disadvantages, so the advantage of classical forward chaining is that you could do it only once.",
                    "label": 0
                },
                {
                    "sent": "OK, and after you did it while querying becomes just a kind of database problem, so you don't need to do reasoning anymore.",
                    "label": 1
                },
                {
                    "sent": "The disadvantage is that if your data changes, you have to recompute entire closure, and it takes hours.",
                    "label": 1
                },
                {
                    "sent": "On the other side, the advantage of backward chaining is actually we do reasoning only on the on the data that we care.",
                    "label": 0
                },
                {
                    "sent": "OK, on the other data that the user is asking for and we've got everything else outside and the problem is that because the rules are kind of generic, even if you're interested in a small subset of our knowledge base, we might have to perform reasoning over much more in order to guarantee that our answer set is complete.",
                    "label": 0
                },
                {
                    "sent": "OK, so each of these two methods have advantages disadvantage.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So currently forward chaining has is the method that has shown the best capability in the last two years.",
                    "label": 0
                },
                {
                    "sent": "We have been presenting these web Pi work which actually can scale quite well in our largest experiment we show it that you could perform the entire closure over 100 billion triples, which is like four times the entire semantic web now, so it scales well.",
                    "label": 0
                },
                {
                    "sent": "But we still have to deal with intrinsic problems of forward chaining.",
                    "label": 0
                },
                {
                    "sent": "So how do we do if our data changes very much, for example, and that's the motivation that drove us to come up with a different type of reasoning and that query pie.",
                    "label": 0
                },
                {
                    "sent": "So we we we query pie what we try to do is we try to combine advantages of forward chaining and backward chaining in order to get closer to a kind of more query driven form of reasoning, more webby.",
                    "label": 1
                },
                {
                    "sent": "So we can call it so kind of reasoning can deal better with updates and so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So query PY.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I start by saying that the input of our reasoning algorithms is a triple pattern WHI.",
                    "label": 1
                },
                {
                    "sent": "So users normally query knowledge base is using sparkle, right?",
                    "label": 0
                },
                {
                    "sent": "So Sparkle is the factor language that we use.",
                    "label": 0
                },
                {
                    "sent": "So if we look when reasoning is used in a sparkle query you will see that actually we need it the moment that we look up for data in our knowledge base.",
                    "label": 0
                },
                {
                    "sent": "OK, we don't need it when we need to do perform the data joints, but we need it when we need to look up for data.",
                    "label": 0
                },
                {
                    "sent": "Because of it, we decided to abstract our metal from the sparkled engine OK and focus on the problem where we actually have to look up for data in a knowledge base.",
                    "label": 0
                },
                {
                    "sent": "And that's why our input is a triple pattern.",
                    "label": 0
                },
                {
                    "sent": "So I said before they can qualify.",
                    "label": 0
                },
                {
                    "sent": "We do a little bit of precomputation, So what is the precomputation about?",
                    "label": 1
                },
                {
                    "sent": "So basically what we want to do, we want to perform the closure of the schema or terminological triples.",
                    "label": 1
                },
                {
                    "sent": "OK, we do the closure over these triples before hand and reasoning about all other three post is left at query time, so the schema triples the kind of vague definition, so I will narrow it down to a more pragmatic one.",
                    "label": 0
                },
                {
                    "sent": "So we consider in our work as terminological triples as old triples that have either an earlier fessor alter as a predicate, or objects that are actually used.",
                    "label": 0
                },
                {
                    "sent": "Our inference rules.",
                    "label": 0
                },
                {
                    "sent": "I report some examples to give you a better idea so.",
                    "label": 0
                },
                {
                    "sent": "For example, X subclass of why this is a pattern that contains only schema triples, right?",
                    "label": 0
                },
                {
                    "sent": "Or X property of or type transitive property.",
                    "label": 0
                },
                {
                    "sent": "So these are all we consider these are all schema triples and what we want to do is compute the closure.",
                    "label": 0
                },
                {
                    "sent": "So calculate all subclasses.",
                    "label": 0
                },
                {
                    "sent": "Also properties all type transitive properties.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is, why do we do that?",
                    "label": 0
                },
                {
                    "sent": "Three reasons, namely the first reason we observed the schema doesn't change that much, at least in web data.",
                    "label": 0
                },
                {
                    "sent": "OK, so once we computed the closure of a schema, it's A kind of stable.",
                    "label": 0
                },
                {
                    "sent": "We don't need to repeat this process many times.",
                    "label": 0
                },
                {
                    "sent": "Secondly, we also observed that on web data, the schema is much smaller than the rest of the data, so it's few orders magnitude less than our data.",
                    "label": 1
                },
                {
                    "sent": "So if we have billions of triples we can have.",
                    "label": 0
                },
                {
                    "sent": "Few hundreds of classes or thousands much smaller.",
                    "label": 0
                },
                {
                    "sent": "So if you compute the closure of this subset, it's kind of limited.",
                    "label": 0
                },
                {
                    "sent": "And the third reason, even more important, because the schema triples are used very, very much in our reasoning process.",
                    "label": 0
                },
                {
                    "sent": "So if we compute the closure over this week and save a lot of computation, and I'll show later exactly how much computationally save.",
                    "label": 1
                },
                {
                    "sent": "So, more formally, in qualify we define two algorithms.",
                    "label": 1
                },
                {
                    "sent": "First, we call it terminological closure, and this algorithm does exactly what he says.",
                    "label": 1
                },
                {
                    "sent": "It computes the closure of the terminological triples.",
                    "label": 0
                },
                {
                    "sent": "It's executed before query time.",
                    "label": 0
                },
                {
                    "sent": "So before the user can actually use the knowledge base, the second algorithm is, we call it terminology independent reasoning, and this is actually the reasoning that we do at query time when the users ask for some data.",
                    "label": 0
                },
                {
                    "sent": "So you expect me to explain before the terminological closure and then move to the second.",
                    "label": 1
                },
                {
                    "sent": "I will do the other way around.",
                    "label": 0
                },
                {
                    "sent": "I will start.",
                    "label": 0
                },
                {
                    "sent": "I will first explain terminology, independent reasoning and then I'll move to the terminological closure and you will see later why I do that.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's focus on the second one, terminal.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Edge independent reasoning.",
                    "label": 0
                },
                {
                    "sent": "Very simply, this algorithm is nothing less than classical backward chaining.",
                    "label": 0
                },
                {
                    "sent": "So if you have an input, a query name, a triple pattern as input, you look for what rule, which rules can actually derive triples that belong to that pattern.",
                    "label": 0
                },
                {
                    "sent": "Then you look down the antecedents of these rules and you go down till you reach your knowledge base.",
                    "label": 0
                },
                {
                    "sent": "When you reach a knowledge base then you are in a position to see whether your data matches and then you go up and you proceed with your derivation.",
                    "label": 0
                },
                {
                    "sent": "So it's nothing else with then classical backward chaining with only one difference.",
                    "label": 0
                },
                {
                    "sent": "Whenever we whenever we need to infer a schema, triples whenever in our reasoning three we have to infer for schema triples, we stop, we go immediately on the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "In other words, with this algorithm we assume that the schema is closed, so that we have all the schema.",
                    "label": 1
                },
                {
                    "sent": "Triples are explicit in our knowledge base.",
                    "label": 0
                },
                {
                    "sent": "We do not perform any reasoning.",
                    "label": 0
                },
                {
                    "sent": "Those triples.",
                    "label": 0
                },
                {
                    "sent": "Because we make this assumption, we can actually implement some really nice optimizations.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example, what we can do is that because we have, we assume that we have a schema triples whenever we have two rules that might arrive, one parents ruler, one child rule, and the child might arrive something for the parents.",
                    "label": 0
                },
                {
                    "sent": "By looking at this scheme antecedents, we can look whether there is an intersection.",
                    "label": 0
                },
                {
                    "sent": "If there is no intersections, the child ruquier never fire anything.",
                    "label": 0
                },
                {
                    "sent": "To the parents so we can prove that branching tree these optimization aims to cut down further cut down the complexity of our reasoning tree.",
                    "label": 0
                },
                {
                    "sent": "Another thing that we can do is that because our scheme we have our schema triples.",
                    "label": 0
                },
                {
                    "sent": "We can replicate it to all the nodes in our cluster and whenever we have to execute his reasoning T reasoning three reason operation we can do locali OK because the schemas in memory.",
                    "label": 0
                },
                {
                    "sent": "Obviously, this the assumption that we make is wrong, right?",
                    "label": 0
                },
                {
                    "sent": "So it might not be the case that our schema is is is explicit in our knowledge base.",
                    "label": 1
                },
                {
                    "sent": "OK, so in order to make this algorithm complete, we need to calculate all the schema triples and that's exactly the purpose of the previous algorithm, this terminological closure.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm not going to talk about it.",
                    "label": 0
                },
                {
                    "sent": "There is a problem.",
                    "label": 0
                },
                {
                    "sent": "The big one.",
                    "label": 0
                },
                {
                    "sent": "We cannot use traditional forward chaining methods to derive a partial closure because the rules are so generic that we cannot identify a subset of our data or a subset of rules so that we can guarantee that our partial closure is complete.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can reuse tools like for example web, PY, becausw we can guarantee it will actually be complete.",
                    "label": 0
                },
                {
                    "sent": "We have to come if we use whereby we have to do the full closure.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a problem and the solution that we propose.",
                    "label": 0
                },
                {
                    "sent": "Actually what we can do is to exploit these backward chaining.",
                    "label": 0
                },
                {
                    "sent": "Reason that I just explained before for this task.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "We start assuming that our scheme is complete.",
                    "label": 0
                },
                {
                    "sent": "Obviously this assumption is wrong.",
                    "label": 0
                },
                {
                    "sent": "At the beginning we query our backward chaining algorithm, disturbing alot terminology, independent reasoning.",
                    "label": 0
                },
                {
                    "sent": "Put schema I say give me your subclasses.",
                    "label": 0
                },
                {
                    "sent": "This backward chaining will drive something, but not everything OK, because this assumption is wrong.",
                    "label": 0
                },
                {
                    "sent": "So what we do we take our derivation, we put it back in a knowledge base and we repeat this process until we stop deriving anything.",
                    "label": 0
                },
                {
                    "sent": "In other words, what we're doing, we're doing kind of forward chaining.",
                    "label": 1
                },
                {
                    "sent": "So we repeating this process, but we use backward chaining steps and by using backward chaining steps.",
                    "label": 0
                },
                {
                    "sent": "Watch what we're actually doing.",
                    "label": 0
                },
                {
                    "sent": "We're touching only the data that we care of.",
                    "label": 0
                },
                {
                    "sent": "So it's faster.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, enough about content now implementation, so in theory the algorithms that that I just presented can work with whatever reason you have.",
                    "label": 0
                },
                {
                    "sent": "But because we wanted to scale we came up with a distributed implementation.",
                    "label": 0
                },
                {
                    "sent": "So what we did we wrote this prototype in Java and we use this framework that we develop our universities called Ibis.",
                    "label": 0
                },
                {
                    "sent": "It's a framework that makes life easier if you have a cluster and notes.",
                    "label": 0
                },
                {
                    "sent": "I have to talk to each other.",
                    "label": 0
                },
                {
                    "sent": "And we indexed our triples with four indexes and we partition across the notes.",
                    "label": 0
                },
                {
                    "sent": "OK, So what happens in our prototype is that.",
                    "label": 0
                },
                {
                    "sent": "Once a node receives a query as an input, what he does he built this reasoning tree locally and then he looks which notes have the relevant data to resolve these three and send this to the note that has it.",
                    "label": 1
                },
                {
                    "sent": "And this note receives this reason.",
                    "label": 1
                },
                {
                    "sent": "This part of the reasoning tree executed and returned the Triple City user.",
                    "label": 0
                },
                {
                    "sent": "So very simple, very simple.",
                    "label": 0
                },
                {
                    "sent": "So now how does it perform?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Valuation of this method.",
                    "label": 0
                },
                {
                    "sent": "The first claim I made in this presentation is that in query pie we can actually reduce the complexity of pure backward chaining.",
                    "label": 0
                },
                {
                    "sent": "So exactly how much do we reduce this complexity?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, So what we did is that we took some.",
                    "label": 0
                },
                {
                    "sent": "Oh, sorry before right before I go to that point, I must say we want to compare ourselves against the current state of the art.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's the target of our evaluation.",
                    "label": 0
                },
                {
                    "sent": "The state of the art at the moment is forward chaining, so we took web PY one for all.",
                    "label": 0
                },
                {
                    "sent": "The problem is that web PY is working with map reduce with a completely different technology.",
                    "label": 0
                },
                {
                    "sent": "So we have to reduce this external overhead as much as we can.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we did?",
                    "label": 0
                },
                {
                    "sent": "We consider two scenarios.",
                    "label": 1
                },
                {
                    "sent": "The first one is this full materialization scenario.",
                    "label": 0
                },
                {
                    "sent": "OK is a scenario where you have some data, you apply the foreclosure and then you just queried without reasoning.",
                    "label": 0
                },
                {
                    "sent": "So we did.",
                    "label": 0
                },
                {
                    "sent": "We just took our data set.",
                    "label": 0
                },
                {
                    "sent": "We use web PY.",
                    "label": 0
                },
                {
                    "sent": "We calculated everything and then we load it in our prototype and whenever we had to query the data we deactivated reasoning.",
                    "label": 1
                },
                {
                    "sent": "OK, the second case is actually our case.",
                    "label": 1
                },
                {
                    "sent": "You know this query pykes, so in this case what we did, we loaded the data in our prototype.",
                    "label": 0
                },
                {
                    "sent": "We perform this partial closure and then whenever we have to ask it we we activated reasoning.",
                    "label": 0
                },
                {
                    "sent": "Alright, we use exactly the same machines, so we use eight machines of our cluster an we consider three datasets LDS are, which is a nice cool fact forge, which is a generic data set.",
                    "label": 0
                },
                {
                    "sent": "I linked life data and lubian.",
                    "label": 0
                },
                {
                    "sent": "They're both, they all three around 1 billion triples.",
                    "label": 0
                },
                {
                    "sent": "OK, so going back to what I said before our goal, our first experiment.",
                    "label": 0
                },
                {
                    "sent": "The goal of our first experiment was to calculate how much we are able to reduce the complexity of pure backward chaining.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we did, we took some queries.",
                    "label": 0
                },
                {
                    "sent": "And we estimate it how.",
                    "label": 0
                },
                {
                    "sent": "How large our reasoning tree will be if we perform the pure backward chaining.",
                    "label": 0
                },
                {
                    "sent": "For example for is an estimation by the fact.",
                    "label": 0
                },
                {
                    "sent": "So probably the.",
                    "label": 0
                },
                {
                    "sent": "The real trees even bigger, so in the first query if we do backward chaining, we have to we have to resolve at three, which is 174 leaves.",
                    "label": 0
                },
                {
                    "sent": "So we have to perform 174 look apps in order to derive everything with our methods.",
                    "label": 0
                },
                {
                    "sent": "We were able to cut down the size of these 3 to 21 leaves, so depending on the query we were able to reduce the pure reasoning complexity of the best case one order of magnitude.",
                    "label": 0
                },
                {
                    "sent": "So there is a significant significant reduction of the computational complexity.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now the second claim I made is that we do that paying the cost of a small precomputation.",
                    "label": 0
                },
                {
                    "sent": "So how much is this cost?",
                    "label": 0
                },
                {
                    "sent": "If we compare it to web?",
                    "label": 0
                },
                {
                    "sent": "PY because if it is more or less the same, then the entire approach is pointless.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in our datasets.",
                    "label": 0
                },
                {
                    "sent": "Performing this terminological closure is in the order of seconds.",
                    "label": 1
                },
                {
                    "sent": "So for Ruby and for example, it takes 8 seconds.",
                    "label": 1
                },
                {
                    "sent": "If we perform the full materialization using web PY, it takes one hour and 15 minutes.",
                    "label": 0
                },
                {
                    "sent": "So again, the price we pay is between one and two order of magnitude less than what we will have to pay.",
                    "label": 0
                },
                {
                    "sent": "If we do the full materialization.",
                    "label": 1
                },
                {
                    "sent": "So now there is one last question we need to solve.",
                    "label": 0
                },
                {
                    "sent": "So far what I've shown is that yes, we are able to reduce the computational complexity by in the best case, one order of magnitude, the price we pay is very small, but still it can be the case that even we reduce the complexity of reasoning.",
                    "label": 0
                },
                {
                    "sent": "When we compare it to the case where no reason is involved and that is the full materialization scenario, the difference is too big that it makes it not worth.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what we wanted to measure is exactly how much reasoning is slowing down our execution time.",
                    "label": 0
                },
                {
                    "sent": "So I.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, we took some queries in a paper that many mores he reports some as an example from the three data sets.",
                    "label": 0
                },
                {
                    "sent": "Obviously reasoning is lower than just doing a data.",
                    "label": 0
                },
                {
                    "sent": "Look up, you know there is no escape for that.",
                    "label": 0
                },
                {
                    "sent": "But the good news is that in both cases the response time stays in the order of milliseconds.",
                    "label": 0
                },
                {
                    "sent": "For example, the first query if we don't do any reasoning, it will take 3.32 milliseconds to answer it.",
                    "label": 0
                },
                {
                    "sent": "OK, if we do our hybrid reasoning.",
                    "label": 0
                },
                {
                    "sent": "It will take 3.38 so OK it's 2% slower, but for a user point of view there is basically no difference.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take home message.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Whatever like that you remember after this talk is that yes, on fly inference is possible, you can do that and you can do it on a large scale.",
                    "label": 1
                },
                {
                    "sent": "You have to do things right.",
                    "label": 0
                },
                {
                    "sent": "You have to do some kind of pre computation, but it's possible.",
                    "label": 1
                },
                {
                    "sent": "This way of doing reason is actually much more convenient than performing a few metalization.",
                    "label": 1
                },
                {
                    "sent": "Specially if your data changes very frequently, so it's more much more a webby way of performing reasoning.",
                    "label": 0
                },
                {
                    "sent": "And even if there is also an ice cream pie, is the first prototype, so there is still too many open questions that need to be answered in the future.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "How does it work with Sparkle, right?",
                    "label": 1
                },
                {
                    "sent": "So we buy purpose consider our as our input is triple patterns, But what happens the moment we put a sparkle engine on the top of it?",
                    "label": 0
                },
                {
                    "sent": "So we implemented this prototype in a way that we could put whatever engine we wanted, but actually we have to see if the two things are compatible.",
                    "label": 0
                },
                {
                    "sent": "Another thing that we plan to do is that in this prototype we consider only the old horse rules which are sufficiently complex, but they're not a standard in the last year, the Community came out with this out, where L fragment, which is a standard.",
                    "label": 0
                },
                {
                    "sent": "So we plan to move.",
                    "label": 0
                },
                {
                    "sent": "Our implementation to this new rule set.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I leave this like like this.",
                    "label": 0
                },
                {
                    "sent": "OK before I give back to the chair.",
                    "label": 0
                },
                {
                    "sent": "I thank you again for listening and I will be very happy to answer your questions.",
                    "label": 0
                },
                {
                    "sent": "So thanks very much.",
                    "label": 0
                },
                {
                    "sent": "Have you considered because I didn't see any definitions in your talk, so I'm not completely sure what do you mean by materializing everything in the schema.",
                    "label": 0
                },
                {
                    "sent": "But have you considered that this could be infinite?",
                    "label": 0
                },
                {
                    "sent": "That the material all the schema consequences that matter query answering could be an infinite set, so we consider these rulesets where it guarantees that there is a finite answer set.",
                    "label": 0
                },
                {
                    "sent": "So yes, you're right in if we consider the whole logic, it's infinite, but in our ruleset it's it's in is finished.",
                    "label": 0
                },
                {
                    "sent": "So in Horst it's guaranteed to finish.",
                    "label": 0
                },
                {
                    "sent": "This is a bit.",
                    "label": 0
                },
                {
                    "sent": "Kind of also like a commercial.",
                    "label": 0
                },
                {
                    "sent": "There is another technique for semantic index that was introduced early this year.",
                    "label": 0
                },
                {
                    "sent": "This is for our DFS and Alto QL is based on the same idea of computing the consequences of the terminology 1st and then doing something with that.",
                    "label": 0
                },
                {
                    "sent": "However, there you don't have to pay any materialization cost of the data triples.",
                    "label": 0
                },
                {
                    "sent": "You don't have to do forward chaining nor backward chaining.",
                    "label": 0
                },
                {
                    "sent": "I think you should consider it also to combine it with what you're doing.",
                    "label": 0
                },
                {
                    "sent": "It's it was introduced in the in the DL context.",
                    "label": 0
                },
                {
                    "sent": "It was for the light, but the same idea.",
                    "label": 0
                },
                {
                    "sent": "However, without the pay having to pay for even the backward chaining, that's OK.",
                    "label": 0
                },
                {
                    "sent": "I was not aware of this work, thanks.",
                    "label": 0
                },
                {
                    "sent": "OK so I will ask so just to minimize waste so give them.",
                    "label": 0
                },
                {
                    "sent": "Thank you, I was wondering how the interplay of forward chaining and backward chaining relates to what is achieved with magic sets transformation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we one of the optimizations that we implemented is very close to what happens with magic sets.",
                    "label": 0
                },
                {
                    "sent": "Is this binding propagation?",
                    "label": 0
                },
                {
                    "sent": "So yeah, there are.",
                    "label": 0
                },
                {
                    "sent": "There is related, but the novelty is that this is possible only because our schema is complete.",
                    "label": 0
                },
                {
                    "sent": "So if we don't have the schema.",
                    "label": 0
                },
                {
                    "sent": "So we cannot apply these optimizations.",
                    "label": 0
                },
                {
                    "sent": "So the two things are related to each other, so yes, there is.",
                    "label": 0
                },
                {
                    "sent": "There's definitely a correlation.",
                    "label": 0
                },
                {
                    "sent": "It's likely different because the domain is different.",
                    "label": 0
                },
                {
                    "sent": "Can you tell us a little bit about the partitioning on the cluster, I mean.",
                    "label": 0
                },
                {
                    "sent": "Maybe some of the details there.",
                    "label": 0
                },
                {
                    "sent": "Yes, so by purpose we didn't want to go into the storage problem because it's a really challenging problem.",
                    "label": 0
                },
                {
                    "sent": "How to efficiently store the data?",
                    "label": 0
                },
                {
                    "sent": "So what we did is just we went for the simplest way.",
                    "label": 0
                },
                {
                    "sent": "So we index.",
                    "label": 0
                },
                {
                    "sent": "We sort our triples according to its four indexes, and then we split it.",
                    "label": 0
                },
                {
                    "sent": "In a cross in notes.",
                    "label": 0
                },
                {
                    "sent": "So just wondering what the challenges would be moving forward to L2 or L. Overall, Horst.",
                    "label": 0
                },
                {
                    "sent": "I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't see any.",
                    "label": 0
                },
                {
                    "sent": "I don't see any challenge to be honest, but I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, in the rules are very similar, so I expect the performance to be the same.",
                    "label": 0
                },
                {
                    "sent": "So without well you have rules with lists in the body which would cost probably some issues.",
                    "label": 0
                },
                {
                    "sent": "Yes you have them also in Horst.",
                    "label": 0
                },
                {
                    "sent": "No, no sorry water and the least you mean yes, yeah, yeah, yeah yeah, you're right yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "New tricky.",
                    "label": 0
                }
            ]
        }
    }
}