{
    "id": "dcaabzpmv5szxks3du4bqimobd72wuho",
    "title": "Relation-Prediction in Multi-Relational Domains using Matrix-Factorization",
    "info": {
        "author": [
            "Christoph Lippert, Max Planck Institute for Biological Cybernetics, Max Planck Institute",
            "Stefan-Hagen Weber, Siemens AG",
            "Yi Huang, Siemens AG",
            "Volker Tresp, Siemens AG",
            "Matthias Schubert, University of Munich",
            "Hans-Peter Kriegel, University of Munich"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Bioinformatics->Computational Systems Biology"
        ]
    },
    "url": "http://videolectures.net/siso08_lippert_rpimrd/",
    "segmentation": [
        [
            "Work that I've been doing during my time at Siemens.",
            "In collaboration with University of Munich.",
            "So."
        ],
        [
            "That we didn't short overview of the talk.",
            "I would start with introducing relational learning and show you how this can be done by matrix factorization.",
            "Then it's the main contribution of this work.",
            "Is the modulation matrix factorization, which which is essentially generalization of matrix factorization to multi relational domains.",
            "Of course we tried to prove our findings with some experiments, one's recommendation system and the other one is bio informatics domain.",
            "So we did gene function prediction.",
            "Will be finished by some concluding."
        ],
        [
            "And."
        ],
        [
            "Now let's go to traditional learning.",
            "So one finding is that real world data often originates from many heterogeneous sources.",
            "For example, in bioinformatics you have like sequence informations of interactions, your structures you have on the logical informations.",
            "Experiments and functions of genes, whatever.",
            "And we can say that this data forms modulation networks of entities that are connected by relations.",
            "So here we have such a graph where we have entity classes like gene, function and so on, and these are connected by relations.",
            "So the genes mutually interact interact.",
            "So actually the proteins but.",
            "And genes are functions and so."
        ],
        [
            "And what we see is like we have this data in a database, for example as a set of entities and we have some observed relations between these in our database.",
            "And the task is to predict new relations between these entities.",
            "Of course, there are many applications, so in functional prediction.",
            "Social network analysis and so on."
        ],
        [
            "This can be done with matrix factorization."
        ],
        [
            "This is an example from the collaborative filtering filtering domain, so we have people who read books and if they like after reading the book they rated so they give a numerical numerical expression of how they liked the book.",
            "More formally, this is like a relation between two entity classes E. Andd and.",
            "They connected by relation class which has a numerical value that expressed the strength of their relation.",
            "And another way to view this as a bipartite graph.",
            "So we have the two nodes from the two entity classes.",
            "And they are connected by some soft relations weighted by this relational value and come away to write this thing up is by matrix.",
            "So this is a sparse matrix where we have here the entities from EA.",
            "Here the entities from EP and the.",
            "Values in these mattress is are the relational values."
        ],
        [
            "As I said, we want to predict new relations.",
            "So this is essentially matrix completion task.",
            "And a common way to do this is by using a low rank matrix factorization, where the big matrix is factorized into two smaller matrices, U&V that I will call now the entity factor matches of EA&B respectively, and the assumption behind this is that the relations that these objects do is are just like there are few factors that are responsible for the behavior."
        ],
        [
            "And one of the leading approaches in the Netflix contest does like such a thing.",
            "So they use gradient decent to solve these metrics.",
            "Factorizations on like this very huge mattress is like in the Netflix contest.",
            "And they essentially state an optimization criterion.",
            "Overall, the observed values in the in the relation metrics.",
            "And.",
            "Which so this is the squared value and all the audio.",
            "Between the approximation and the observed values, and this is a regularization term involving the Frobenius norm of the two smaller matrices.",
            "So as I said, this is a gradient decent approach, so they calculate the gradient for like fixing one of these metrics is calculating the decent credit on the other, but just for one of the relational values at a time.",
            "And then they update the two matrices according to this.",
            "So this is the algorithm.",
            "So you in VR like randomly initially initialized.",
            "And yeah and cycles overall the observed values in the sparse matrix.",
            "Calculating the gradient, updating the two matrices until convergence.",
            "Well."
        ],
        [
            "Yeah, we don't have any constraints over there like.",
            "Ultimately, the of the.",
            "The factors and this is like a sparse matrix, so we don't want to take consider the UN observed values and in this relation matrix.",
            "So we don't have.",
            "We don't factorize the zeros in there.",
            "OK."
        ],
        [
            "Um?",
            "Of course, as I mentioned."
        ],
        [
            "We not interested.",
            "We are not interested in like these simulations but in.",
            "Huge networks of entities that are connected by relations.",
            "Work because we assume that they are somehow correlated usually.",
            "So.",
            "We have large M of these possibly sparse relation matrices.",
            "And these connect large entity.",
            "Entity classes and for each entity class.",
            "Now we produce one entity factor metrics.",
            "And.",
            "Where we want to arrive is like we want to predict new relations.",
            "So again we want to complete these relational matrices.",
            "And the trick is."
        ],
        [
            "That each of these relation matrix is again the factorization of the two entity matrix is that are involved in the relation.",
            "As one entity class can be involved in more than one relation, so also in this these are not just.",
            "Many factorization of single metrics is so, but the same entity factor metrics can be involved in more than one factorization.",
            "OK. And so this is the optimization criterion for this problem.",
            "Where we have again the squared error on all the observed values in all the.",
            "The relation mattress is and the Frobenius norm of the factor mattress is here.",
            "And yeah, I want to mention again this is not just a sum over individual factorizations.",
            "But they are connected by having the same entity factor matrices, so they appear more than one time in this thing.",
            "So yeah."
        ],
        [
            "Again, we use the gradient decent algorithm, so similar to the one that I mentioned before, so we have this optimization criterion.",
            "And yeah, for each of these matches and for each of the.",
            "The values we calculate the gradient where we fix one of the metrics and update the other one essentially.",
            "So.",
            "Here we initialize all the.",
            "Affect the mattress is and cycle.",
            "Overall these mattress is until this whole thing converges."
        ],
        [
            "Before doing this, we have to do normalization because we're like if we combine these different sources of information, we will fear the fear.",
            "Several problems when 'cause like some of these relations may be binary.",
            "So like essentially 10 or 1 -- 1 values and some maybe like for example ratings from one to five and so on.",
            "And in order to.",
            "2.",
            "Not run into these numerical problems we we use this term in order to normalize all these individual relation matches beforehand, and what this does is essentially it waits the.",
            "Some over so robots and columnwise we.",
            "Way the some of the.",
            "The observed values of 1 object time, the with towel times the mean of the whole of the whole metrics.",
            "So all the observed values in this matrix.",
            "And by this we we make use of like that we may have more information about some objects then then about other objects, because then we will.",
            "This sum will get larger and the mean over the row or column respectively will be like taking more into account than the mean of the whole."
        ],
        [
            "Metrics.",
            "OK, let's come to our expense."
        ],
        [
            "So first we analyzed the movielens data.",
            "This is pretty well known, so there are movie ratings from one to five and for the movies we have information about the genre of the movie and for the.",
            "Users we have like age, gender and occupation of them, and these form this kind of relational network."
        ],
        [
            "So what we did with 10 fold cross and we measured the arm.",
            "See and we compared our method to the single relational matrix factorization which used the same.",
            "In this case, the same normalization beforehand.",
            "In order to make things fair.",
            "OK, these are like to plot our plots of the armsy times the rank and armsy times the norm parameter.",
            "And what we can see is that once K is high enough, the.",
            "The armsy converges.",
            "And here we see that like choosing the right panelization of the of the norm of the model is quite essential for for this approach.",
            "And here are the results of the two algorithms and we were able to show using a student T test.",
            "As this is indeed.",
            "Yeah.",
            "Better than the other."
        ],
        [
            "So the second one was jinfeng."
        ],
        [
            "Prediction and East.",
            "So used.",
            "Has more than 6000 predicted genes in it and we downloaded this CYT with which has annotations for almost 5000 of these jeans.",
            "And this is already has been decried described by focus.",
            "These annotations are.",
            "Follow a hierarchy, so this is the fan cat where like we have a general to specific relation with in this tree.",
            "And overall there are 506 functions annotated in this Journal in the CBD, and 17 are on this most topmost level, so on the most general.",
            "The database also has a lot of additional information that we used to build a relational graph, very similar to."
        ],
        [
            "So.",
            "We measured the performance in two ways.",
            "One time for the first time we measured like we predicted just the most general.",
            "Fun card categories and the second one we tried to break all 506 funk categories.",
            "We performed some.",
            "Kind of five.",
            "Repeat all but one sampling where we for for each stream we randomly removed one of the 17 top most functional categories, including its whole dependent subtree and put it in the test set in order not to like peak.",
            "By having like this on the training set and test set or something like that.",
            "And we compared to a support vector machine using a linear kernel, which was essentially the best performing in this case.",
            "And as we make like a predict.",
            "More classes we trained one SVM for class and the single relational matrix factorization.",
            "This is pretty interesting because it's unusual to just use.",
            "Functions in order to predict new functions and not like all the other stuff you have in your database.",
            "So here are RC curves for the 17 categories experiment and four.",
            "The 500 and six categories experiment.",
            "This is a log scale as otherwise everything would have been chunked at the left side and you wouldn't see any anything.",
            "The blue line is the MF the the green dot here corresponds to the support vector machines.",
            "And the red line to the single relational matrix factorization.",
            "And what we can see that the confidence intervals are nicely separated, so these are quite good results."
        ],
        [
            "I don't want to go too much into details here, just want to point out the F measures and in terms of ash and F, the MF outperforms the other two methods quite nicely."
        ],
        [
            "So let's conclude."
        ],
        [
            "My talk.",
            "I tried to to explain that real world data often forms modulation networks of entities connected by relation and this makes mighty relational learning useful.",
            "And to tackle this we tried metrics factorization, so we extended metrics factorization too much relational domains.",
            "By stating an intuitive optimization criterion involving the squared errors on the observed values and Frobenius norm penalization term.",
            "And in our experiments we showed that using multiple relation.",
            "Clearly helps to improve the predictive performance."
        ],
        [
            "So thank you very much and yeah."
        ],
        [
            "So there have been several proposals before the relational learning.",
            "Biometrics I transition using exactly this type of joint effect arising matrices by the images.",
            "They can think so what's different in your approach?",
            "Yeah, there are some some spectral clustering approaches to do this.",
            "But there there wasn't any like that.",
            "Tried to predict.",
            "The like just take the computer, complete the metrics and to take this at the predictions there was like in Katyn KDD there was.",
            "A proposal that was very similar to this one from from sing.",
            "And yeah yeah, it's just like I.",
            "Just recently got aware of this, and yeah, it's pretty pretty similar actually, yeah.",
            "Yeah.",
            "Relations involving more than two.",
            "OK, so.",
            "We have this can just handle Dyadic relations.",
            "So what you would have to do is you would have to introduce a new set of entities and like have relations to this set of entities.",
            "So here is obviously out of product of our product.",
            "Small matrices gives you the big matrix right?",
            "Yeah so right now either.",
            "Three cases we get 3 dimensional matrix.",
            "Yeah, no, no I would I would I would means like if you have like like for example complexes like many proteins that somehow interact.",
            "And we use this information also, but we just.",
            "Produce like a new entity class which was a complex and then all the complexes at the proteins that interacted in this complex had relation to this complex, not to each other.",
            "So in this case, this becomes a tensor factorization problem, right?",
            "Essentially about singing Gordon not working.",
            "It's a lot harder.",
            "So right now we cannot take a tenses with this, yeah?",
            "No more questions.",
            "Have you involved with speaker?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work that I've been doing during my time at Siemens.",
                    "label": 0
                },
                {
                    "sent": "In collaboration with University of Munich.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That we didn't short overview of the talk.",
                    "label": 1
                },
                {
                    "sent": "I would start with introducing relational learning and show you how this can be done by matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "Then it's the main contribution of this work.",
                    "label": 0
                },
                {
                    "sent": "Is the modulation matrix factorization, which which is essentially generalization of matrix factorization to multi relational domains.",
                    "label": 1
                },
                {
                    "sent": "Of course we tried to prove our findings with some experiments, one's recommendation system and the other one is bio informatics domain.",
                    "label": 1
                },
                {
                    "sent": "So we did gene function prediction.",
                    "label": 0
                },
                {
                    "sent": "Will be finished by some concluding.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's go to traditional learning.",
                    "label": 0
                },
                {
                    "sent": "So one finding is that real world data often originates from many heterogeneous sources.",
                    "label": 1
                },
                {
                    "sent": "For example, in bioinformatics you have like sequence informations of interactions, your structures you have on the logical informations.",
                    "label": 0
                },
                {
                    "sent": "Experiments and functions of genes, whatever.",
                    "label": 0
                },
                {
                    "sent": "And we can say that this data forms modulation networks of entities that are connected by relations.",
                    "label": 1
                },
                {
                    "sent": "So here we have such a graph where we have entity classes like gene, function and so on, and these are connected by relations.",
                    "label": 0
                },
                {
                    "sent": "So the genes mutually interact interact.",
                    "label": 0
                },
                {
                    "sent": "So actually the proteins but.",
                    "label": 0
                },
                {
                    "sent": "And genes are functions and so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we see is like we have this data in a database, for example as a set of entities and we have some observed relations between these in our database.",
                    "label": 1
                },
                {
                    "sent": "And the task is to predict new relations between these entities.",
                    "label": 0
                },
                {
                    "sent": "Of course, there are many applications, so in functional prediction.",
                    "label": 1
                },
                {
                    "sent": "Social network analysis and so on.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This can be done with matrix factorization.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is an example from the collaborative filtering filtering domain, so we have people who read books and if they like after reading the book they rated so they give a numerical numerical expression of how they liked the book.",
                    "label": 0
                },
                {
                    "sent": "More formally, this is like a relation between two entity classes E. Andd and.",
                    "label": 0
                },
                {
                    "sent": "They connected by relation class which has a numerical value that expressed the strength of their relation.",
                    "label": 1
                },
                {
                    "sent": "And another way to view this as a bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "So we have the two nodes from the two entity classes.",
                    "label": 0
                },
                {
                    "sent": "And they are connected by some soft relations weighted by this relational value and come away to write this thing up is by matrix.",
                    "label": 0
                },
                {
                    "sent": "So this is a sparse matrix where we have here the entities from EA.",
                    "label": 0
                },
                {
                    "sent": "Here the entities from EP and the.",
                    "label": 0
                },
                {
                    "sent": "Values in these mattress is are the relational values.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I said, we want to predict new relations.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially matrix completion task.",
                    "label": 1
                },
                {
                    "sent": "And a common way to do this is by using a low rank matrix factorization, where the big matrix is factorized into two smaller matrices, U&V that I will call now the entity factor matches of EA&B respectively, and the assumption behind this is that the relations that these objects do is are just like there are few factors that are responsible for the behavior.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one of the leading approaches in the Netflix contest does like such a thing.",
                    "label": 1
                },
                {
                    "sent": "So they use gradient decent to solve these metrics.",
                    "label": 0
                },
                {
                    "sent": "Factorizations on like this very huge mattress is like in the Netflix contest.",
                    "label": 0
                },
                {
                    "sent": "And they essentially state an optimization criterion.",
                    "label": 0
                },
                {
                    "sent": "Overall, the observed values in the in the relation metrics.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Which so this is the squared value and all the audio.",
                    "label": 0
                },
                {
                    "sent": "Between the approximation and the observed values, and this is a regularization term involving the Frobenius norm of the two smaller matrices.",
                    "label": 0
                },
                {
                    "sent": "So as I said, this is a gradient decent approach, so they calculate the gradient for like fixing one of these metrics is calculating the decent credit on the other, but just for one of the relational values at a time.",
                    "label": 0
                },
                {
                    "sent": "And then they update the two matrices according to this.",
                    "label": 0
                },
                {
                    "sent": "So this is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you in VR like randomly initially initialized.",
                    "label": 0
                },
                {
                    "sent": "And yeah and cycles overall the observed values in the sparse matrix.",
                    "label": 0
                },
                {
                    "sent": "Calculating the gradient, updating the two matrices until convergence.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, we don't have any constraints over there like.",
                    "label": 0
                },
                {
                    "sent": "Ultimately, the of the.",
                    "label": 0
                },
                {
                    "sent": "The factors and this is like a sparse matrix, so we don't want to take consider the UN observed values and in this relation matrix.",
                    "label": 0
                },
                {
                    "sent": "So we don't have.",
                    "label": 0
                },
                {
                    "sent": "We don't factorize the zeros in there.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Of course, as I mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We not interested.",
                    "label": 0
                },
                {
                    "sent": "We are not interested in like these simulations but in.",
                    "label": 0
                },
                {
                    "sent": "Huge networks of entities that are connected by relations.",
                    "label": 0
                },
                {
                    "sent": "Work because we assume that they are somehow correlated usually.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have large M of these possibly sparse relation matrices.",
                    "label": 1
                },
                {
                    "sent": "And these connect large entity.",
                    "label": 1
                },
                {
                    "sent": "Entity classes and for each entity class.",
                    "label": 1
                },
                {
                    "sent": "Now we produce one entity factor metrics.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Where we want to arrive is like we want to predict new relations.",
                    "label": 0
                },
                {
                    "sent": "So again we want to complete these relational matrices.",
                    "label": 0
                },
                {
                    "sent": "And the trick is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That each of these relation matrix is again the factorization of the two entity matrix is that are involved in the relation.",
                    "label": 0
                },
                {
                    "sent": "As one entity class can be involved in more than one relation, so also in this these are not just.",
                    "label": 0
                },
                {
                    "sent": "Many factorization of single metrics is so, but the same entity factor metrics can be involved in more than one factorization.",
                    "label": 0
                },
                {
                    "sent": "OK. And so this is the optimization criterion for this problem.",
                    "label": 1
                },
                {
                    "sent": "Where we have again the squared error on all the observed values in all the.",
                    "label": 1
                },
                {
                    "sent": "The relation mattress is and the Frobenius norm of the factor mattress is here.",
                    "label": 0
                },
                {
                    "sent": "And yeah, I want to mention again this is not just a sum over individual factorizations.",
                    "label": 0
                },
                {
                    "sent": "But they are connected by having the same entity factor matrices, so they appear more than one time in this thing.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, we use the gradient decent algorithm, so similar to the one that I mentioned before, so we have this optimization criterion.",
                    "label": 0
                },
                {
                    "sent": "And yeah, for each of these matches and for each of the.",
                    "label": 0
                },
                {
                    "sent": "The values we calculate the gradient where we fix one of the metrics and update the other one essentially.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here we initialize all the.",
                    "label": 0
                },
                {
                    "sent": "Affect the mattress is and cycle.",
                    "label": 0
                },
                {
                    "sent": "Overall these mattress is until this whole thing converges.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before doing this, we have to do normalization because we're like if we combine these different sources of information, we will fear the fear.",
                    "label": 0
                },
                {
                    "sent": "Several problems when 'cause like some of these relations may be binary.",
                    "label": 1
                },
                {
                    "sent": "So like essentially 10 or 1 -- 1 values and some maybe like for example ratings from one to five and so on.",
                    "label": 0
                },
                {
                    "sent": "And in order to.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Not run into these numerical problems we we use this term in order to normalize all these individual relation matches beforehand, and what this does is essentially it waits the.",
                    "label": 0
                },
                {
                    "sent": "Some over so robots and columnwise we.",
                    "label": 0
                },
                {
                    "sent": "Way the some of the.",
                    "label": 0
                },
                {
                    "sent": "The observed values of 1 object time, the with towel times the mean of the whole of the whole metrics.",
                    "label": 0
                },
                {
                    "sent": "So all the observed values in this matrix.",
                    "label": 1
                },
                {
                    "sent": "And by this we we make use of like that we may have more information about some objects then then about other objects, because then we will.",
                    "label": 0
                },
                {
                    "sent": "This sum will get larger and the mean over the row or column respectively will be like taking more into account than the mean of the whole.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Metrics.",
                    "label": 0
                },
                {
                    "sent": "OK, let's come to our expense.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first we analyzed the movielens data.",
                    "label": 0
                },
                {
                    "sent": "This is pretty well known, so there are movie ratings from one to five and for the movies we have information about the genre of the movie and for the.",
                    "label": 1
                },
                {
                    "sent": "Users we have like age, gender and occupation of them, and these form this kind of relational network.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we did with 10 fold cross and we measured the arm.",
                    "label": 1
                },
                {
                    "sent": "See and we compared our method to the single relational matrix factorization which used the same.",
                    "label": 0
                },
                {
                    "sent": "In this case, the same normalization beforehand.",
                    "label": 0
                },
                {
                    "sent": "In order to make things fair.",
                    "label": 0
                },
                {
                    "sent": "OK, these are like to plot our plots of the armsy times the rank and armsy times the norm parameter.",
                    "label": 0
                },
                {
                    "sent": "And what we can see is that once K is high enough, the.",
                    "label": 0
                },
                {
                    "sent": "The armsy converges.",
                    "label": 0
                },
                {
                    "sent": "And here we see that like choosing the right panelization of the of the norm of the model is quite essential for for this approach.",
                    "label": 0
                },
                {
                    "sent": "And here are the results of the two algorithms and we were able to show using a student T test.",
                    "label": 0
                },
                {
                    "sent": "As this is indeed.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Better than the other.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the second one was jinfeng.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prediction and East.",
                    "label": 0
                },
                {
                    "sent": "So used.",
                    "label": 0
                },
                {
                    "sent": "Has more than 6000 predicted genes in it and we downloaded this CYT with which has annotations for almost 5000 of these jeans.",
                    "label": 0
                },
                {
                    "sent": "And this is already has been decried described by focus.",
                    "label": 0
                },
                {
                    "sent": "These annotations are.",
                    "label": 0
                },
                {
                    "sent": "Follow a hierarchy, so this is the fan cat where like we have a general to specific relation with in this tree.",
                    "label": 0
                },
                {
                    "sent": "And overall there are 506 functions annotated in this Journal in the CBD, and 17 are on this most topmost level, so on the most general.",
                    "label": 0
                },
                {
                    "sent": "The database also has a lot of additional information that we used to build a relational graph, very similar to.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We measured the performance in two ways.",
                    "label": 0
                },
                {
                    "sent": "One time for the first time we measured like we predicted just the most general.",
                    "label": 0
                },
                {
                    "sent": "Fun card categories and the second one we tried to break all 506 funk categories.",
                    "label": 0
                },
                {
                    "sent": "We performed some.",
                    "label": 0
                },
                {
                    "sent": "Kind of five.",
                    "label": 0
                },
                {
                    "sent": "Repeat all but one sampling where we for for each stream we randomly removed one of the 17 top most functional categories, including its whole dependent subtree and put it in the test set in order not to like peak.",
                    "label": 0
                },
                {
                    "sent": "By having like this on the training set and test set or something like that.",
                    "label": 0
                },
                {
                    "sent": "And we compared to a support vector machine using a linear kernel, which was essentially the best performing in this case.",
                    "label": 0
                },
                {
                    "sent": "And as we make like a predict.",
                    "label": 0
                },
                {
                    "sent": "More classes we trained one SVM for class and the single relational matrix factorization.",
                    "label": 1
                },
                {
                    "sent": "This is pretty interesting because it's unusual to just use.",
                    "label": 0
                },
                {
                    "sent": "Functions in order to predict new functions and not like all the other stuff you have in your database.",
                    "label": 0
                },
                {
                    "sent": "So here are RC curves for the 17 categories experiment and four.",
                    "label": 0
                },
                {
                    "sent": "The 500 and six categories experiment.",
                    "label": 0
                },
                {
                    "sent": "This is a log scale as otherwise everything would have been chunked at the left side and you wouldn't see any anything.",
                    "label": 0
                },
                {
                    "sent": "The blue line is the MF the the green dot here corresponds to the support vector machines.",
                    "label": 0
                },
                {
                    "sent": "And the red line to the single relational matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "And what we can see that the confidence intervals are nicely separated, so these are quite good results.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't want to go too much into details here, just want to point out the F measures and in terms of ash and F, the MF outperforms the other two methods quite nicely.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's conclude.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My talk.",
                    "label": 0
                },
                {
                    "sent": "I tried to to explain that real world data often forms modulation networks of entities connected by relation and this makes mighty relational learning useful.",
                    "label": 1
                },
                {
                    "sent": "And to tackle this we tried metrics factorization, so we extended metrics factorization too much relational domains.",
                    "label": 1
                },
                {
                    "sent": "By stating an intuitive optimization criterion involving the squared errors on the observed values and Frobenius norm penalization term.",
                    "label": 1
                },
                {
                    "sent": "And in our experiments we showed that using multiple relation.",
                    "label": 0
                },
                {
                    "sent": "Clearly helps to improve the predictive performance.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you very much and yeah.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there have been several proposals before the relational learning.",
                    "label": 0
                },
                {
                    "sent": "Biometrics I transition using exactly this type of joint effect arising matrices by the images.",
                    "label": 0
                },
                {
                    "sent": "They can think so what's different in your approach?",
                    "label": 0
                },
                {
                    "sent": "Yeah, there are some some spectral clustering approaches to do this.",
                    "label": 0
                },
                {
                    "sent": "But there there wasn't any like that.",
                    "label": 0
                },
                {
                    "sent": "Tried to predict.",
                    "label": 0
                },
                {
                    "sent": "The like just take the computer, complete the metrics and to take this at the predictions there was like in Katyn KDD there was.",
                    "label": 0
                },
                {
                    "sent": "A proposal that was very similar to this one from from sing.",
                    "label": 0
                },
                {
                    "sent": "And yeah yeah, it's just like I.",
                    "label": 0
                },
                {
                    "sent": "Just recently got aware of this, and yeah, it's pretty pretty similar actually, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Relations involving more than two.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We have this can just handle Dyadic relations.",
                    "label": 0
                },
                {
                    "sent": "So what you would have to do is you would have to introduce a new set of entities and like have relations to this set of entities.",
                    "label": 0
                },
                {
                    "sent": "So here is obviously out of product of our product.",
                    "label": 0
                },
                {
                    "sent": "Small matrices gives you the big matrix right?",
                    "label": 0
                },
                {
                    "sent": "Yeah so right now either.",
                    "label": 0
                },
                {
                    "sent": "Three cases we get 3 dimensional matrix.",
                    "label": 0
                },
                {
                    "sent": "Yeah, no, no I would I would I would means like if you have like like for example complexes like many proteins that somehow interact.",
                    "label": 0
                },
                {
                    "sent": "And we use this information also, but we just.",
                    "label": 0
                },
                {
                    "sent": "Produce like a new entity class which was a complex and then all the complexes at the proteins that interacted in this complex had relation to this complex, not to each other.",
                    "label": 0
                },
                {
                    "sent": "So in this case, this becomes a tensor factorization problem, right?",
                    "label": 0
                },
                {
                    "sent": "Essentially about singing Gordon not working.",
                    "label": 0
                },
                {
                    "sent": "It's a lot harder.",
                    "label": 0
                },
                {
                    "sent": "So right now we cannot take a tenses with this, yeah?",
                    "label": 0
                },
                {
                    "sent": "No more questions.",
                    "label": 0
                },
                {
                    "sent": "Have you involved with speaker?",
                    "label": 0
                }
            ]
        }
    }
}