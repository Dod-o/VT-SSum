{
    "id": "wlqtp6fcqc4kteub6oa6v4juvjmlv4j5",
    "title": "Differentiable Sparse Coding",
    "info": {
        "author": [
            "David Bradley, Robotics Institute, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Jan. 15, 2009",
        "recorded": "November 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/cmulls08_bradley_dsc/",
    "segmentation": [
        [
            "Alright, I guess I'll start now.",
            "My name is David Bradley.",
            "I'm going to be talking about some work that I've been doing on differential sparse coding with my advisor, Drew Bagnell."
        ],
        [
            "So I'd like to start off with just a broad overview of why I'm interested in this topic.",
            "In my research, I'm interested in learning complex systems.",
            "For instance, this is a robot that I work with that is.",
            "Assigned the task of driving through an outdoor offroad environment.",
            "So I work on the perception system of this robot and to give like a broad overview of the sort of system that goes into making this happen.",
            "It consists of a bunch of modules that process the sensor input sensor data to produce an output path to the goal.",
            "What I'd like to do is find ways to initialize these intermediate modules with cheap data, for instance unlabeled data.",
            "And then I'd also like to be able to do joint optimization of this entire system and all these modules through backpropagation."
        ],
        [
            "So to focus in on this work.",
            "I'll be talking about sparse coding, which is a type of generative model, and what it does is it takes unlabeled input vectors and performs optimization on them to infer some latent variables.",
            "In particular, one of the latent variables, this W vector that I'll be talking about has been shown to be good as input to a classifier to predict things about X.",
            "So that makes it an appealing semi supervised learning method.",
            "And.",
            "What I would like to do then to perform the giant optimization by backpropagation is then take the loss gradient from the output classifier and use it to improve this transformation that we're getting out of the optimization.",
            "However, the previous work on using sparse coding for semi supervised learning wasn't able to do that for reasons that I'll discuss, and So what I'm going to talk about is ways to make this differentiable by switching to alternative smoother priors such as Cal regularization, and then using an implicit differentiation technique to differentiate that optimization with respect to its other."
        ],
        [
            "Parameters.",
            "So sparse coding tries to interpret a high dimensional input input, such as, say, this image of a puppet show."
        ],
        [
            "In terms of two things.",
            "A set of basis vectors.",
            "So for instance, each basis vector might be a puppet and then a set of coefficients that act on each of the basis vectors to produce the input that we actually see."
        ],
        [
            "Now more mathematically, what we're going to do is we're going to use optimization.",
            "To find a vector W that when multiplied by our basis matrix B will approximate an input vector X.",
            "We want to then use W to classify that input vector.",
            "And so this approximation is defined by a reconstruction loss function that will put over those.",
            "Now this is in contrast to more typical method of generating another feature space for an input which is projection where you just take the input vector and multiply it by the basis vector and then apply your output transfer function and there's going to be several advantages from this."
        ],
        [
            "Source coding I'll talk about.",
            "And what I mean by sparse is that we want only a few elements in the vector to be significant or non 0.",
            "And this will have beneficial effects for using those vectors then to classify."
        ],
        [
            "So for an example.",
            "More concrete example of this process.",
            "They are input vector is a 28 by 28 pixel image of a handwritten digit such as those up top.",
            "Now one of the basis learned bases learn from this algorithm is pictured on the right there and each of those small images are the same size as the handwritten digit images and their individual basis vectors and hear each basis vector consists of basically a stroke that could be used in.",
            "Creating a digit and then they are colored based on how much they are used in this particular rate.",
            "Reconstruction of the two shown on the right there and we want to make sure that our reconstruction of the two is approximately equal to the input."
        ],
        [
            "Two that we started with.",
            "And because we're doing optimization instead of projection, we can make RW sparser.",
            "So if we took this 6 dimensional input on the top, there where the middle four elements are on.",
            "And project it on a basis.",
            "We we on the right there we see that there's a lot of activation, all the basis vectors that are completely contained within the input, and then some partial activation on the other two basis vectors instead.",
            "If we take an optimization approach, we get a much sparser output because only the two basis vectors that are really needed to explain the input are actually used."
        ],
        [
            "So.",
            "To take a more in depth, look at the generative model that we're assuming here.",
            "We're going to assume that we're going to start with an input matrix X, where each column of X is.",
            "Say, for instance a digit.",
            "Now to see we want a generative model.",
            "Then for the probability of the matrix X and we're going to put that in terms of two independent variables of matrix W and a matrix B.",
            "Now we can pull out this matrix be outside of the inner integration, and then we make the additional assumption that each column of X is independent.",
            "The vector for each column of X is independent of the other W vectors.",
            "So that gives us two terms within this integration, a likelihood term and a prior term over the W vector.",
            "So then.",
            "Because the integration is computation."
        ],
        [
            "Intractable, we look at the maximum or maximum of Pastore.",
            "Our estimate of the parameters.",
            "So just basically the W vector that maximizes this probability.",
            "An so too."
        ],
        [
            "To find the one that maximizes that probability, just take the negative log of the probability and.",
            "That turns the product into some, and we get basically a loss function and a regularization function over on the right.",
            "For each vector.",
            "So different choices of those two loss and regularization functions will give us different algorithms, for instance."
        ],
        [
            "Common choice is using regular squared error loss for the loss function between the reconstruction and the input, and then also L1 regularization as the prior.",
            "And this turns out to have some nice properties.",
            "It's convex and it also produces sparse W vectors.",
            "And it's been used in a lot of engineering applications.",
            "Sparse coding can also be used to solve for B as well as this W that we've been showing here, but for now, that process is nonconvex.",
            "If we only have a certain number of basis vectors.",
            "But it's also is, but it's showing that this product that this process, even though it is nonconvex, it generates useful features on diverse problems.",
            "There was some work and I see."
        ],
        [
            "Smell of 2007.",
            "So yeah, so to solve for the basis, we just put an L2 constraint on the basis vector and then we can do an alternating optimization of solving for the map estimate of W and then the map estimate of the basis.",
            "So how this fits into the work that I want to do then is we have the sparse coding module over here that we can use that uses this reconstruction loss function to find a good basis that can well represent a pool of unlabeled data.",
            "So it's extracting information from the unlabeled data that can then help us for classification."
        ],
        [
            "And it was shown in the rain at all work for my CL 2007 that the W vectors output by this actually do help in a lot of circumstances for classification problems.",
            "But"
        ],
        [
            "As we said, the L1 prior that they used is not does not produce differential outputs.",
            "So how do we fix?"
        ],
        [
            "Well, it's a little bit unsatisfying to be in this situation 'cause we have this system that can give us extract information from unlabeled data.",
            "But then we're limited to whatever it's pulled out of the unlabeled data, and we can't buy sit for the specific problem that we want to use, so it would be much more powerful tool if we could then buys it for the."
        ],
        [
            "That we're interested in.",
            "And the first thing that prevents us from doing that is instability.",
            "If we use an L1 prior, then that means that the map estimates for W that we get out of it are going to be discontinuous.",
            "And the amount of discontinuity depends actually on the correlation of the basis.",
            "If you have two basis vectors that are exactly the same or just reversed versions of each other, then you actually you don't have just one maximum for the map estimate, but you have a whole continuous section of solutions.",
            "So because those are discontinuous, we can't differentiate the map estimate with respect to the basis in order to improve it.",
            "So instead in our work we used KL divergent regularization which we picked because it's been proven to compete with L1 regularization in the online learning setting."
        ],
        [
            "Then the second problem with applying backpropagation is that there's no closed form equation here.",
            "We just have an optimization problem.",
            "So instead we use the fact that once we found the map estimate, the derivative of this loss function plus the derivative the prior is going to be equal to 0."
        ],
        [
            "So then we can imply implicit differentiation and differentiate that whole equation.",
            "And because our map estimate is a function of B, we get this nice gradient of W with respect to be in our implicit differentiation.",
            "And so we can just solve for that term."
        ],
        [
            "And for instance using the KL divergent that gives us an expression like this."
        ],
        [
            "We've applied this to several standard benchmark datasets so far.",
            "For instance, this is the MNIST handwriting handwritten digit recognition data set, and."
        ],
        [
            "I applied our method by first using the unsupervised sparse coding process to learn a basis for the 50,000 digits in the training set."
        ],
        [
            "And then we put a classifier.",
            "In this case we use the Maxent classifier.",
            "Outputs of that to use those W vectors and to predict which digit it is.",
            "Then we back propagate the gradients to improve the base."
        ],
        [
            "And see if we could improve performance."
        ],
        [
            "Further, and what we found was that the KL divergent regularization maintained the important sparsity characteristics of L1 that we wanted, and so in particular what this is showing, that is that the weight is concentrated in a few elements.",
            "Here, KL divergences a black line, and on this log scale of the basis vectors, it's really most of the.",
            "Activation of each example is contained within the first 16 out of 256 basis vectors.",
            "So because its slope is steeper, KL is sparser than the other.",
            "Possibilities there."
        ],
        [
            "Then also we found that the KL divergent added stability to the map estimates what this is showing is it's showing the results applying applying multiple discriminant analysis on.",
            "Four different feature spaces, so in the top left hand corner we have the input feature space, which is just.",
            "You know, the raw pixel representation of the digits, and there we see that if we plot each of the different digits as a different color, there's a lot of overlap between the different classes.",
            "But then the basis learned by the L1 regularization shown in the top right hand corner helps that situation and separates out the different digit classes.",
            "A bit and then.",
            "By applying KL we get a much nicer separation of the two classes because the different digits tend to be clustered better and then applying backpropagation further separates those."
        ],
        [
            "So what's the axis?",
            "Oh, so this is used?",
            "Multiple discriminants analysis."
        ],
        [
            "To find the most discriminative 2 dimensional subspace 'cause these are actually like like this 180 dimensional and this is 256 dimensional.",
            "So I just found the best previous slide.",
            "So you said Ki have this sparsity but none of them is actually there alright oh oh L1 does drop to 0 here so.",
            "Oh yes, yes.",
            "Scale regulation does not go to zero, but that is actually not the important component of sparsity for this classification and semi supervised learning.",
            "To kill.",
            "Question.",
            "Yes, yeah.",
            "So I just include positive and negative copies of each basis vector and so that way I just have positive weights over expanded basis set.",
            "1.",
            "Yeah, so I used the unnormalized KL divergences exactly."
        ],
        [
            "So to take a little more quantitative, look at the results, the misclassification rate goes down after just switching to KL divergent regularization from L1.",
            "That's quite a significant jump at, especially when there's a lot of training examples, and then applying backpropagation also helps a lot.",
            "In the case where there's a lot of training examples, and so there's a lot of residual error in the training set to use for this back propagation."
        ],
        [
            "And then this effect is not just limited to the Maxent classifier, but it's true across a lot of different classifiers."
        ],
        [
            "And it produces state of the art results for permutation invariant classifiers on the emnace data set."
        ],
        [
            "Next we tried this.",
            "Yes, so that is I think you're referring to the."
        ],
        [
            "The.",
            "A convolutional neural networks.",
            "Yeah, so that's a little bit different 'cause they have a very strong prior on how pixels are connected to other pixels.",
            "So.",
            "Right?",
            "Yeah to other permutation invariant learning algorithms."
        ],
        [
            "And so.",
            "We also tried it on character recognition."
        ],
        [
            "Because this experiment was done in the rain at all work where they took the basis that they learned from digits and showed that it was also helpful for recognizing handwritten characters.",
            "And so in to follow their method.",
            "We first did sparse approximation, learned our Maxent classifier and then took the additional step."
        ],
        [
            "Back propagation on the basis."
        ],
        [
            "And what we found again was that switching from L1 to KL improved performance and then doing the backpropagation allowed us to further improve performance, especially when we had a lot of examples to work with."
        ],
        [
            "And for our third application we tried the text classification data set that takes movie reviews and tries to predict what the reviewer thought of the movie on a 10 point scale from loved it too hated it.",
            "And so the feature vectors for this is you take a movie review, create a histogram of how many of which terms are present, normalize that histogram, and then we sparse coded those normalized histograms."
        ],
        [
            "And from that sparse coding, then we use just linear regression to predict.",
            "Want to predict what the sentiment should?"
        ],
        [
            "T. And we found that this was."
        ],
        [
            "Actually competitive an show better results than state of the art.",
            "Graphical model methods such as Leighton Dursley allocation and the supervised version of it that came out in last year's NIPS, a supervised Latent Dursley allocation.",
            "So it showed a moderate but significant performance boost over those."
        ],
        [
            "And what I'd like to use this for is to get away from the benchmark datasets and go back to the robots that are more interested in and use this sparse coding to develop better features for classifying areas of the outdoor environment to improve the navigation of the crusher robot.",
            "And I'm also working."
        ],
        [
            "On making the sparse coding process convex, currently it's not.",
            "Sparse approximation is convex, but the sparse coding process is not convex because it includes this.",
            "This non convex rank constraint on the number of bases.",
            "So we can relax that by using instead a convex but sparse priors such as L1 over the.",
            "The number of basis, and then theoretically that should allow us to use a boosting process to handle an infinitely large basis effectively."
        ],
        [
            "So any questions?",
            "So do you have any feeling or initiation?",
            "Which part of the killed averages actually helps you?",
            "Is it it's behavior at zero, but it's kind of or its behavior.",
            "Yes.",
            "Normalized, it doesn't mean.",
            "Yeah so.",
            "I. Yeah, so if you look at the derivative of the KL divergent."
        ],
        [
            "If you do that, mirroring the basis trick, it actually looks like arcsine OK, so.",
            "So it's this red line here and L1.",
            "The derivative of that prior is this blue line here.",
            "So the thing that makes KL more desirable is that it still has this high slope near 0, which produces sparse or produces a lot of coefficients near 0.",
            "But it has a small slow growing slope up here, so you have less instability.",
            "I mean, So what would happen if you just kind of replace that?",
            "I guess the sharp point at 0 bye bye little parabola and then at some point you just like turning gold.",
            "Yeah, um, So what would happen is so.",
            "That's a regularization that's also been used a lot.",
            "It's like L1 plus epsilon.",
            "But that the problem with that is that it gives you the same instability problem that we had earlier, where instead you've got 10 H function.",
            "So it's deep here.",
            "But then it goes in asymptotes at some constant value.",
            "Ability at zero.",
            "Anyone like behavior at the right?",
            "So just one L2 what?",
            "So something like if you think it's something L2L1 and then some like elastic Nets.",
            "The origin Oh yeah.",
            "Yeah, I mean something like that definitely helps, but the problem with L2 is that it gets so big as you go out to Infinity that you want something that's very slow growing.",
            "Like for instance the log.",
            "And.",
            "Oh, just that most of the magnitude of the vector is contained in a few elements.",
            "So for the.",
            "Classifiers that I was using, that's really more the relevant measure.",
            "I'm trying to think of other ways to define that, like, is it some ratio between L1 norm in the L2 norm that help sparsity, but haven't quite nailed that down yet.",
            "Alright well thanks for your attention and I'll turn."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, I guess I'll start now.",
                    "label": 0
                },
                {
                    "sent": "My name is David Bradley.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be talking about some work that I've been doing on differential sparse coding with my advisor, Drew Bagnell.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'd like to start off with just a broad overview of why I'm interested in this topic.",
                    "label": 0
                },
                {
                    "sent": "In my research, I'm interested in learning complex systems.",
                    "label": 0
                },
                {
                    "sent": "For instance, this is a robot that I work with that is.",
                    "label": 0
                },
                {
                    "sent": "Assigned the task of driving through an outdoor offroad environment.",
                    "label": 0
                },
                {
                    "sent": "So I work on the perception system of this robot and to give like a broad overview of the sort of system that goes into making this happen.",
                    "label": 0
                },
                {
                    "sent": "It consists of a bunch of modules that process the sensor input sensor data to produce an output path to the goal.",
                    "label": 0
                },
                {
                    "sent": "What I'd like to do is find ways to initialize these intermediate modules with cheap data, for instance unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And then I'd also like to be able to do joint optimization of this entire system and all these modules through backpropagation.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to focus in on this work.",
                    "label": 0
                },
                {
                    "sent": "I'll be talking about sparse coding, which is a type of generative model, and what it does is it takes unlabeled input vectors and performs optimization on them to infer some latent variables.",
                    "label": 0
                },
                {
                    "sent": "In particular, one of the latent variables, this W vector that I'll be talking about has been shown to be good as input to a classifier to predict things about X.",
                    "label": 0
                },
                {
                    "sent": "So that makes it an appealing semi supervised learning method.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What I would like to do then to perform the giant optimization by backpropagation is then take the loss gradient from the output classifier and use it to improve this transformation that we're getting out of the optimization.",
                    "label": 0
                },
                {
                    "sent": "However, the previous work on using sparse coding for semi supervised learning wasn't able to do that for reasons that I'll discuss, and So what I'm going to talk about is ways to make this differentiable by switching to alternative smoother priors such as Cal regularization, and then using an implicit differentiation technique to differentiate that optimization with respect to its other.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameters.",
                    "label": 0
                },
                {
                    "sent": "So sparse coding tries to interpret a high dimensional input input, such as, say, this image of a puppet show.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of two things.",
                    "label": 0
                },
                {
                    "sent": "A set of basis vectors.",
                    "label": 0
                },
                {
                    "sent": "So for instance, each basis vector might be a puppet and then a set of coefficients that act on each of the basis vectors to produce the input that we actually see.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now more mathematically, what we're going to do is we're going to use optimization.",
                    "label": 0
                },
                {
                    "sent": "To find a vector W that when multiplied by our basis matrix B will approximate an input vector X.",
                    "label": 0
                },
                {
                    "sent": "We want to then use W to classify that input vector.",
                    "label": 0
                },
                {
                    "sent": "And so this approximation is defined by a reconstruction loss function that will put over those.",
                    "label": 0
                },
                {
                    "sent": "Now this is in contrast to more typical method of generating another feature space for an input which is projection where you just take the input vector and multiply it by the basis vector and then apply your output transfer function and there's going to be several advantages from this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Source coding I'll talk about.",
                    "label": 0
                },
                {
                    "sent": "And what I mean by sparse is that we want only a few elements in the vector to be significant or non 0.",
                    "label": 0
                },
                {
                    "sent": "And this will have beneficial effects for using those vectors then to classify.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for an example.",
                    "label": 0
                },
                {
                    "sent": "More concrete example of this process.",
                    "label": 0
                },
                {
                    "sent": "They are input vector is a 28 by 28 pixel image of a handwritten digit such as those up top.",
                    "label": 0
                },
                {
                    "sent": "Now one of the basis learned bases learn from this algorithm is pictured on the right there and each of those small images are the same size as the handwritten digit images and their individual basis vectors and hear each basis vector consists of basically a stroke that could be used in.",
                    "label": 0
                },
                {
                    "sent": "Creating a digit and then they are colored based on how much they are used in this particular rate.",
                    "label": 0
                },
                {
                    "sent": "Reconstruction of the two shown on the right there and we want to make sure that our reconstruction of the two is approximately equal to the input.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two that we started with.",
                    "label": 0
                },
                {
                    "sent": "And because we're doing optimization instead of projection, we can make RW sparser.",
                    "label": 0
                },
                {
                    "sent": "So if we took this 6 dimensional input on the top, there where the middle four elements are on.",
                    "label": 0
                },
                {
                    "sent": "And project it on a basis.",
                    "label": 0
                },
                {
                    "sent": "We we on the right there we see that there's a lot of activation, all the basis vectors that are completely contained within the input, and then some partial activation on the other two basis vectors instead.",
                    "label": 0
                },
                {
                    "sent": "If we take an optimization approach, we get a much sparser output because only the two basis vectors that are really needed to explain the input are actually used.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To take a more in depth, look at the generative model that we're assuming here.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that we're going to start with an input matrix X, where each column of X is.",
                    "label": 0
                },
                {
                    "sent": "Say, for instance a digit.",
                    "label": 0
                },
                {
                    "sent": "Now to see we want a generative model.",
                    "label": 0
                },
                {
                    "sent": "Then for the probability of the matrix X and we're going to put that in terms of two independent variables of matrix W and a matrix B.",
                    "label": 0
                },
                {
                    "sent": "Now we can pull out this matrix be outside of the inner integration, and then we make the additional assumption that each column of X is independent.",
                    "label": 0
                },
                {
                    "sent": "The vector for each column of X is independent of the other W vectors.",
                    "label": 0
                },
                {
                    "sent": "So that gives us two terms within this integration, a likelihood term and a prior term over the W vector.",
                    "label": 0
                },
                {
                    "sent": "So then.",
                    "label": 0
                },
                {
                    "sent": "Because the integration is computation.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Intractable, we look at the maximum or maximum of Pastore.",
                    "label": 0
                },
                {
                    "sent": "Our estimate of the parameters.",
                    "label": 0
                },
                {
                    "sent": "So just basically the W vector that maximizes this probability.",
                    "label": 0
                },
                {
                    "sent": "An so too.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To find the one that maximizes that probability, just take the negative log of the probability and.",
                    "label": 0
                },
                {
                    "sent": "That turns the product into some, and we get basically a loss function and a regularization function over on the right.",
                    "label": 0
                },
                {
                    "sent": "For each vector.",
                    "label": 0
                },
                {
                    "sent": "So different choices of those two loss and regularization functions will give us different algorithms, for instance.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Common choice is using regular squared error loss for the loss function between the reconstruction and the input, and then also L1 regularization as the prior.",
                    "label": 0
                },
                {
                    "sent": "And this turns out to have some nice properties.",
                    "label": 0
                },
                {
                    "sent": "It's convex and it also produces sparse W vectors.",
                    "label": 0
                },
                {
                    "sent": "And it's been used in a lot of engineering applications.",
                    "label": 0
                },
                {
                    "sent": "Sparse coding can also be used to solve for B as well as this W that we've been showing here, but for now, that process is nonconvex.",
                    "label": 0
                },
                {
                    "sent": "If we only have a certain number of basis vectors.",
                    "label": 0
                },
                {
                    "sent": "But it's also is, but it's showing that this product that this process, even though it is nonconvex, it generates useful features on diverse problems.",
                    "label": 0
                },
                {
                    "sent": "There was some work and I see.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Smell of 2007.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so to solve for the basis, we just put an L2 constraint on the basis vector and then we can do an alternating optimization of solving for the map estimate of W and then the map estimate of the basis.",
                    "label": 0
                },
                {
                    "sent": "So how this fits into the work that I want to do then is we have the sparse coding module over here that we can use that uses this reconstruction loss function to find a good basis that can well represent a pool of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So it's extracting information from the unlabeled data that can then help us for classification.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it was shown in the rain at all work for my CL 2007 that the W vectors output by this actually do help in a lot of circumstances for classification problems.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we said, the L1 prior that they used is not does not produce differential outputs.",
                    "label": 0
                },
                {
                    "sent": "So how do we fix?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, it's a little bit unsatisfying to be in this situation 'cause we have this system that can give us extract information from unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "But then we're limited to whatever it's pulled out of the unlabeled data, and we can't buy sit for the specific problem that we want to use, so it would be much more powerful tool if we could then buys it for the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we're interested in.",
                    "label": 0
                },
                {
                    "sent": "And the first thing that prevents us from doing that is instability.",
                    "label": 0
                },
                {
                    "sent": "If we use an L1 prior, then that means that the map estimates for W that we get out of it are going to be discontinuous.",
                    "label": 0
                },
                {
                    "sent": "And the amount of discontinuity depends actually on the correlation of the basis.",
                    "label": 0
                },
                {
                    "sent": "If you have two basis vectors that are exactly the same or just reversed versions of each other, then you actually you don't have just one maximum for the map estimate, but you have a whole continuous section of solutions.",
                    "label": 0
                },
                {
                    "sent": "So because those are discontinuous, we can't differentiate the map estimate with respect to the basis in order to improve it.",
                    "label": 0
                },
                {
                    "sent": "So instead in our work we used KL divergent regularization which we picked because it's been proven to compete with L1 regularization in the online learning setting.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the second problem with applying backpropagation is that there's no closed form equation here.",
                    "label": 0
                },
                {
                    "sent": "We just have an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So instead we use the fact that once we found the map estimate, the derivative of this loss function plus the derivative the prior is going to be equal to 0.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then we can imply implicit differentiation and differentiate that whole equation.",
                    "label": 0
                },
                {
                    "sent": "And because our map estimate is a function of B, we get this nice gradient of W with respect to be in our implicit differentiation.",
                    "label": 0
                },
                {
                    "sent": "And so we can just solve for that term.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for instance using the KL divergent that gives us an expression like this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've applied this to several standard benchmark datasets so far.",
                    "label": 0
                },
                {
                    "sent": "For instance, this is the MNIST handwriting handwritten digit recognition data set, and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I applied our method by first using the unsupervised sparse coding process to learn a basis for the 50,000 digits in the training set.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we put a classifier.",
                    "label": 0
                },
                {
                    "sent": "In this case we use the Maxent classifier.",
                    "label": 1
                },
                {
                    "sent": "Outputs of that to use those W vectors and to predict which digit it is.",
                    "label": 0
                },
                {
                    "sent": "Then we back propagate the gradients to improve the base.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And see if we could improve performance.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Further, and what we found was that the KL divergent regularization maintained the important sparsity characteristics of L1 that we wanted, and so in particular what this is showing, that is that the weight is concentrated in a few elements.",
                    "label": 0
                },
                {
                    "sent": "Here, KL divergences a black line, and on this log scale of the basis vectors, it's really most of the.",
                    "label": 0
                },
                {
                    "sent": "Activation of each example is contained within the first 16 out of 256 basis vectors.",
                    "label": 0
                },
                {
                    "sent": "So because its slope is steeper, KL is sparser than the other.",
                    "label": 0
                },
                {
                    "sent": "Possibilities there.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then also we found that the KL divergent added stability to the map estimates what this is showing is it's showing the results applying applying multiple discriminant analysis on.",
                    "label": 0
                },
                {
                    "sent": "Four different feature spaces, so in the top left hand corner we have the input feature space, which is just.",
                    "label": 0
                },
                {
                    "sent": "You know, the raw pixel representation of the digits, and there we see that if we plot each of the different digits as a different color, there's a lot of overlap between the different classes.",
                    "label": 0
                },
                {
                    "sent": "But then the basis learned by the L1 regularization shown in the top right hand corner helps that situation and separates out the different digit classes.",
                    "label": 0
                },
                {
                    "sent": "A bit and then.",
                    "label": 0
                },
                {
                    "sent": "By applying KL we get a much nicer separation of the two classes because the different digits tend to be clustered better and then applying backpropagation further separates those.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's the axis?",
                    "label": 0
                },
                {
                    "sent": "Oh, so this is used?",
                    "label": 0
                },
                {
                    "sent": "Multiple discriminants analysis.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To find the most discriminative 2 dimensional subspace 'cause these are actually like like this 180 dimensional and this is 256 dimensional.",
                    "label": 0
                },
                {
                    "sent": "So I just found the best previous slide.",
                    "label": 0
                },
                {
                    "sent": "So you said Ki have this sparsity but none of them is actually there alright oh oh L1 does drop to 0 here so.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Scale regulation does not go to zero, but that is actually not the important component of sparsity for this classification and semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "To kill.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah.",
                    "label": 0
                },
                {
                    "sent": "So I just include positive and negative copies of each basis vector and so that way I just have positive weights over expanded basis set.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I used the unnormalized KL divergences exactly.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to take a little more quantitative, look at the results, the misclassification rate goes down after just switching to KL divergent regularization from L1.",
                    "label": 0
                },
                {
                    "sent": "That's quite a significant jump at, especially when there's a lot of training examples, and then applying backpropagation also helps a lot.",
                    "label": 0
                },
                {
                    "sent": "In the case where there's a lot of training examples, and so there's a lot of residual error in the training set to use for this back propagation.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this effect is not just limited to the Maxent classifier, but it's true across a lot of different classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it produces state of the art results for permutation invariant classifiers on the emnace data set.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next we tried this.",
                    "label": 0
                },
                {
                    "sent": "Yes, so that is I think you're referring to the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "A convolutional neural networks.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's a little bit different 'cause they have a very strong prior on how pixels are connected to other pixels.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yeah to other permutation invariant learning algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "We also tried it on character recognition.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because this experiment was done in the rain at all work where they took the basis that they learned from digits and showed that it was also helpful for recognizing handwritten characters.",
                    "label": 0
                },
                {
                    "sent": "And so in to follow their method.",
                    "label": 0
                },
                {
                    "sent": "We first did sparse approximation, learned our Maxent classifier and then took the additional step.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back propagation on the basis.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we found again was that switching from L1 to KL improved performance and then doing the backpropagation allowed us to further improve performance, especially when we had a lot of examples to work with.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for our third application we tried the text classification data set that takes movie reviews and tries to predict what the reviewer thought of the movie on a 10 point scale from loved it too hated it.",
                    "label": 0
                },
                {
                    "sent": "And so the feature vectors for this is you take a movie review, create a histogram of how many of which terms are present, normalize that histogram, and then we sparse coded those normalized histograms.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And from that sparse coding, then we use just linear regression to predict.",
                    "label": 0
                },
                {
                    "sent": "Want to predict what the sentiment should?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "T. And we found that this was.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually competitive an show better results than state of the art.",
                    "label": 0
                },
                {
                    "sent": "Graphical model methods such as Leighton Dursley allocation and the supervised version of it that came out in last year's NIPS, a supervised Latent Dursley allocation.",
                    "label": 0
                },
                {
                    "sent": "So it showed a moderate but significant performance boost over those.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what I'd like to use this for is to get away from the benchmark datasets and go back to the robots that are more interested in and use this sparse coding to develop better features for classifying areas of the outdoor environment to improve the navigation of the crusher robot.",
                    "label": 0
                },
                {
                    "sent": "And I'm also working.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On making the sparse coding process convex, currently it's not.",
                    "label": 0
                },
                {
                    "sent": "Sparse approximation is convex, but the sparse coding process is not convex because it includes this.",
                    "label": 0
                },
                {
                    "sent": "This non convex rank constraint on the number of bases.",
                    "label": 0
                },
                {
                    "sent": "So we can relax that by using instead a convex but sparse priors such as L1 over the.",
                    "label": 0
                },
                {
                    "sent": "The number of basis, and then theoretically that should allow us to use a boosting process to handle an infinitely large basis effectively.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So any questions?",
                    "label": 0
                },
                {
                    "sent": "So do you have any feeling or initiation?",
                    "label": 0
                },
                {
                    "sent": "Which part of the killed averages actually helps you?",
                    "label": 0
                },
                {
                    "sent": "Is it it's behavior at zero, but it's kind of or its behavior.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Normalized, it doesn't mean.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "I. Yeah, so if you look at the derivative of the KL divergent.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you do that, mirroring the basis trick, it actually looks like arcsine OK, so.",
                    "label": 0
                },
                {
                    "sent": "So it's this red line here and L1.",
                    "label": 0
                },
                {
                    "sent": "The derivative of that prior is this blue line here.",
                    "label": 0
                },
                {
                    "sent": "So the thing that makes KL more desirable is that it still has this high slope near 0, which produces sparse or produces a lot of coefficients near 0.",
                    "label": 0
                },
                {
                    "sent": "But it has a small slow growing slope up here, so you have less instability.",
                    "label": 0
                },
                {
                    "sent": "I mean, So what would happen if you just kind of replace that?",
                    "label": 0
                },
                {
                    "sent": "I guess the sharp point at 0 bye bye little parabola and then at some point you just like turning gold.",
                    "label": 0
                },
                {
                    "sent": "Yeah, um, So what would happen is so.",
                    "label": 0
                },
                {
                    "sent": "That's a regularization that's also been used a lot.",
                    "label": 0
                },
                {
                    "sent": "It's like L1 plus epsilon.",
                    "label": 0
                },
                {
                    "sent": "But that the problem with that is that it gives you the same instability problem that we had earlier, where instead you've got 10 H function.",
                    "label": 0
                },
                {
                    "sent": "So it's deep here.",
                    "label": 0
                },
                {
                    "sent": "But then it goes in asymptotes at some constant value.",
                    "label": 0
                },
                {
                    "sent": "Ability at zero.",
                    "label": 0
                },
                {
                    "sent": "Anyone like behavior at the right?",
                    "label": 0
                },
                {
                    "sent": "So just one L2 what?",
                    "label": 0
                },
                {
                    "sent": "So something like if you think it's something L2L1 and then some like elastic Nets.",
                    "label": 0
                },
                {
                    "sent": "The origin Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean something like that definitely helps, but the problem with L2 is that it gets so big as you go out to Infinity that you want something that's very slow growing.",
                    "label": 0
                },
                {
                    "sent": "Like for instance the log.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Oh, just that most of the magnitude of the vector is contained in a few elements.",
                    "label": 0
                },
                {
                    "sent": "So for the.",
                    "label": 0
                },
                {
                    "sent": "Classifiers that I was using, that's really more the relevant measure.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to think of other ways to define that, like, is it some ratio between L1 norm in the L2 norm that help sparsity, but haven't quite nailed that down yet.",
                    "label": 0
                },
                {
                    "sent": "Alright well thanks for your attention and I'll turn.",
                    "label": 0
                }
            ]
        }
    }
}