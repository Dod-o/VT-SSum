{
    "id": "kyas5sucws7w6w2zuxpajjsvymkzpvy3",
    "title": "A Gaussian Process View on MKL",
    "info": {
        "author": [
            "Raquel Urtasun, Department of Computer Science, University of Toronto"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_urtasun_gpv/",
    "segmentation": [
        [
            "OK so I have a very simple go today which is like over the less amount of people are sleeping at the end of my talk.",
            "Can you hear me on the back?",
            "Yes great.",
            "OK so I will try to show as many pictures as I can since I claimed to do computer vision from time to time.",
            "So today I will talk about the Gaussian process view of MCL."
        ],
        [
            "OK Anne, first I will introduce with Gaussian processes are this will be very brief an I will stay simple formulation of MCL in the Gaussian process framework and then I will go over different formulations which go beyond convex combinations of multiple kernel learning.",
            "In particular I would try to focus in the scenario where you have noise.",
            "And show a little bit about latent variable models and I will conclude the talk."
        ],
        [
            "OK so I got some process is a collection of random variables.",
            "Anything in number of which have a Gaussian have a joint Gaussian distribution.",
            "So basically they are probability distributions over functions and functions are actually infinite dimensional.",
            "So what we're really having here is a prior distribution over instantiation's of the functions which are finite dimensional object.",
            "So let me."
        ],
        [
            "Simply show you some examples of the prior, which is a Gaussian process of basically, as the name says, I got some processes.",
            "Gaussian Ann is nonparametric, therefore everything we need to know is the data and the covariance matrix, which is usually defined in terms of a kernel.",
            "So in this case here what I have is samples from the Gaussian process.",
            "When I use a linear kernel.",
            "So I suspected the type of functions that we have are linear functions.",
            "Now if I change the kernel for example I use."
        ],
        [
            "RV S with particular set of hyperparameters.",
            "Then I'm going to have functions that are actually non linear and they look smooth as we would expect from an RBF.",
            "Now if I change the hyperparameters, I will change the type of functions that I can generate with my Gaussian process."
        ],
        [
            "For example, here I increase the length scale.",
            "Therefore I expect things that are even more smooth than tensor linear.",
            "And then there are those of."
        ],
        [
            "So I can use the same as with any kernel method.",
            "For example, here I can just use a bias and I can combine all."
        ],
        [
            "This kernels in order to get complicated functions to civilly."
        ],
        [
            "Now Gaussian processes are actually used typically for regression, so they're going to use in conjunction with the data, so we're going to have some inputs X as some target values Y.",
            "And we're going to some assume a prior distribution over functions by selecting a kernel.",
            "And basically we will combine the prior with the data together posterior distribution OK."
        ],
        [
            "And this is an example where we have a 1 dimensional input, 11 dimensional output.",
            "I have two data points here.",
            "The line represented here is a mean prediction.",
            "OK, so the posterior is actually got every point with different meaning covariance.",
            "So this is the main function and I'm using here in RBF and I also have here.",
            "They were standard deviation which represents my variance at every point.",
            "OK, So what we see is that close to the training points.",
            "The variance is very small and as I go far from the training points in my various increases as I will expect him.",
            "Less confident in my."
        ],
        [
            "Action now."
        ],
        [
            "I say add more."
        ],
        [
            "More data then I get more."
        ],
        [
            "I'm more confident."
        ],
        [
            "This prediction here."
        ],
        [
            "OK, so learning in Gaussian processes how is typically done is by doing maximum likelihood or maximum posteriori and the basic idea is that the only thing that we need to learn is actually hyperparameters, which I do not hear theater of my kernel OK, and once I have a fixed hyperparameters, where once I estimate them, then estimating the mean and the variance at every point is actually closed form, which is very very simple to estimate right?",
            "And let me just introduce now how?"
        ],
        [
            "We can do MCL in this formulation.",
            "So basically as is typically then we have a linear combination of kernels, so I have a set of kernels here which I'm going to denote it with uppercase I and I have yes a linear set, and here I'm not constraining to be a convex combination, so I don't have the typical constraint that is have to sum to one OK and the idea is that so this is this.",
            "Kernels can be either different kernels on the same input data or what is typically done in computer vision is that we use different features.",
            "For each one of these kernels, and again now I can learn the hyperparameters of American and a little.",
            "The weights, but yes, using maximum a posterior maximum likelihood.",
            "OK Ann, with the use different regularizers and in particular so different priors if you want to talk in terms of valuation terms and we use is ADA Laplacian prior which is the other one on the weights which is what you will usually have in MCL right you have you want sparse combination of kernels.",
            "We use Gaussian priors which is the L2 or we use combinations of both with you."
        ],
        [
            "I think that these are instances of elastic Nets.",
            "OK, so some results of this simple MCL, so this is the call to one or one data set and these are I have to save results at the end of 2008.",
            "So I'm going to show more updated versions of this curve as we go on time an at the time.",
            "So the fact that we were able to combine all these kernels give us very very good results compared to yes using a single type of feature an.",
            "So this was good news.",
            "The bad news here was that actually if you just average the kernels you get more or less the same results as if you actually.",
            "Try to learn the kernel weights and I think this is very well known in a lot of problems that this is this is happening, which is, I guess not not very nice and some colleagues at the University of Osho also state of the art with this type of approach in activity recognition in computer vision applications."
        ],
        [
            "OK, So what are the advantages of using simple MKO?",
            "Is that the optimization using the same process is really really simple animal ticket with classes scenarios.",
            "Actually if we share the parameters then for each of the classifiers we can learn everything together using the same processes.",
            "That's great.",
            "Now the problem is as was mentioned, is the fact that is very disappointed.",
            "Yes average in the kernel, so you have you noticed emissions on the hyperparameters?",
            "Actually that's more or less the same.",
            "An one thing that we.",
            "So that was one of the reasons is that the fact that we have a fixed combination across all this space limits the ability to capture.",
            "You know more rich extractors here.",
            "So in the case for example when you have noise or incomplete data or where you expect the properties of the space to worry locali simple linear combination is."
        ],
        [
            "It is not enough.",
            "An in the case of Gaussian processes, you're going to extend this linear combination to be, for example, predator kernels, another different combinations, or some some products.",
            "You can combine all these different things, and again you can learn this using maximum likelihood and the idea of you combine producer Colonel says that you're going to have a kernel that is much sharper than what you had before."
        ],
        [
            "OK, so but the still this measures are global, so we still feel that too simplistic so we really wanted to go into local versions of this an I have to point out that we weren't at all the first one, so to try to use this locality and there has been multiple approaches in the SVM framework, but I'm now going to yes presented with with."
        ],
        [
            "Our case.",
            "OK, so Mario Christie's will sitting on the back of the room did this for his PhD.",
            "Basically the idea is that we are going to consider that were Colonel again by Smosh some kernels.",
            "But now these kernels are going to be more complicated than the current that we had before an for each one of our kernels.",
            "What we're going to do is that we are going to compute the kernel by the Hadamard product.",
            "So Elementwise product of a parametric kernel, which is what you typically do, right?",
            "For example, the RBF anano parametric kernel, which hopefully captures this locality of the weights in this space."
        ],
        [
            "OK, so this is now what were some of corners look like.",
            "So we have this nonparametric kernel.",
            "We have this parametric kernel and the idea is that we have we want to learn the hyper parameters of this guy here.",
            "Plus all these metrics here which is N * N. OK so you can assume that things are symmetric for example.",
            "But still you have tons of parameters to estimate, so it's going to be really really simple to over fit to the data right?",
            "So we decided was."
        ],
        [
            "To try to look into simplification on this nonparametric thing so that we have less chance to overfit.",
            "So the first idea is well, if we have a matrix that is, you know 2 high dimensional, then let's try to do some low rank approximation right?",
            "So in this case here we have yes a local representation and we assume symmetric form.",
            "OK.",
            "So now the number of parameters is going to be the number of kernels times the dimension of this low rank times the, which is the number of training points that we have.",
            "OK."
        ],
        [
            "Now we will have also our parametric control over there, so we went even further than that.",
            "I'd say, well, we really care about in our applications is to capture this notion of noise per data point OK. And then instead of having a P dimensional space where we're going to have is dimensional space, is that we're going to have you as a single one dimensional vector here.",
            "And you can think of this again as really every here actually pointing out how much I trust that data points in order to estimate my kernel combination.",
            "OK."
        ],
        [
            "An if you actually have structure in your noise as we had in our case, you can think of modeling this as a clustering problem, which further simplifies the things.",
            "We're basically, I say I have a set of indicator variables here which represents which cluster every point is, and then if I assume that they only have P clusters and I have only P values to estimate, which basically are the different types of noise that I expect my data to be OK, and then this.",
            "Actually you compute these metrics and then you.",
            "Multiply that they had more product with the number with the parametric form and the.",
            "I'm sorry.",
            "Yeah, so the idea of Department application that we were looking at.",
            "We were looking at this video segment application which I will just show in a minute and in that case we will expect the actual type of noise that the data has to have some structure in the sense that is not yet ready but we have a set of clusters which actually can represent all the possible noises that we have an basically.",
            "Then you can simply further simplifies to only have 3 parameters times the number of clusters.",
            "Sorry the number of views here.",
            "OK."
        ],
        [
            "And yes, in yes to contrast it with the traditional color combination.",
            "So basically what regional kernel combination is that this guy here is just a constant, which is the weight that you give for that particular kernel.",
            "OK, and here we have much richer structures and then."
        ],
        [
            "OK, so how will you learn this thing again?",
            "You do maximum likelihood and we yes use a relatively non external priors in this case which seems to be working relatively well and in order to do inference we are going to do multiclass classification and the only thing that we did yes to maximize over the class and then pick up the class which is the maximum response."
        ],
        [
            "OK, so let me go over the different examples that Mario was working on.",
            "So here is Mario, a few years ago an so we were looking into this big disagreement data set, which will disagreement in this case.",
            "Let me just explain what it is so we have this scenario which we have.",
            "We ask questions to a set of subjects an we record the video as well as the audio an they be.",
            "This agreement is the fact that you can actually.",
            "Respond visually, saying this and moving your head right so the two views which are going to be the other video actually give information about this.",
            "The fact that he said yes, right?",
            "You can yes say yes, but not move your head.",
            "In that case the video is actually corrupted innocence, right?",
            "So that's and give you information that actually this is saying yes.",
            "And the same thing can actually happen for the early on the video, right?",
            "So in order to have a control setting, we simulate this be disagreement by adding background head motion.",
            "In the case of, in the case of the gesture and bubble noise for the case of the audio."
        ],
        [
            "OK, so these are yes results as a function of the view disagreement.",
            "So how much of this noisy data is there?",
            "So in the case that there is no big disagreement, then in this case one of the classifiers is really good, so you actually don't even need the audio in order to classify and you know it doesn't marriage to MCL.",
            "You do this complicated combination.",
            "You actually do fairly well, and in the case of you have a lot of disagreement.",
            "This means that 70% of the samples have at least have one via corrupted.",
            "Either the audio or video, and in this case you see that there is a boost performance if you actually try to capture the fact that some of these samples are corrupted, which is basically what this clustering is doing.",
            "OK, and here is again the the audio and video which are the two modalities.",
            "So."
        ],
        [
            "So we try also this in a more standard of recognition data set again, is Caltech 101, so it's 100 and one categories, so it's the same example as I showed before, and here we are going to compute multiple kernels by using different features that people have built in the vision community."
        ],
        [
            "OK, and so the good news is that actually we perform more you we match the state of the art of the time.",
            "So this is one year later.",
            "So you see that there is much much more people actually trying to combine different sources of information.",
            "Now the bad news again is that we get more or less the same results as by doing the kernel combination OK and the main reason here is that actually having this richer structure thus help when you have noisy data or when you have.",
            "This this locality very nothing, but actually we believe that in this data set without we don't have enough data to actually capture this this variation."
        ],
        [
            "Exactly the same thing.",
            "So the reported booster.",
            "Yeah, it was in performance and we did not get that Bruce, so I've seen a version of that paper which they did not have that much boost, so I don't know what was the way they did to make it to work.",
            "But here.",
            "So I guess we couldn't really make it to work.",
            "And actually here you see as an example as a function of the number of clusters, we really have flat.",
            "Performance an now we simulate missing data and in that case you actually have a boosted performance with respect to your simple MCL."
        ],
        [
            "So no so here where they showed before was actually Biclustering, and Kevin simplified version of this an Now the.",
            "So what we try to do is so very preliminary results into trying to estimate the full nonparametric covariance.",
            "Another question, if you want to do that is worse, actually.",
            "The regularizer that I avoid this massive overfitting that you would expect to have.",
            "So we thought of two possible regularizers which are very typical.",
            "Rather use the passion regularizer based on some measure of distance and similarity that you have.",
            "Or if you have some estimate of maybe what you know, a good estimation of this.",
            "Then maybe you want to have a Frobenius norm that minimize the distance to that estimate that you think is is possible.",
            "For example, this can be yes.",
            "The fact that you know the constant weights or yes, if it's you know average kernels all once an."
        ],
        [
            "We need a simple experiment where in this audio video, original data set and actually we got a boost performance.",
            "If we learn the complicated metrics, very interesting technical violation codes."
        ],
        [
            "Raining, which was developed by you'll nips a couple of years ago, and the main idea is this is going to be retrospective setting, and the main idea is that you want to impose an agreement prior in order to regularize your solution.",
            "So you're going to have different views here.",
            "You're going to have Gaussian process, so little function for a review, and then you have Gaussian process priors on each of these guys.",
            "And then you have no Gaussian noise on this.",
            "The good thing about using this is that you can actually marginalized this in closed form.",
            "An you marginalized this.",
            "Then you end up with a Gaussian process.",
            "At this note here we check variance, which is a combination of kernels in a very non essential form.",
            "So what you get is the inverse of the sum of the inverses of the kernels.",
            "Now the good thing about this is that.",
            "In the inverse you actually can use this in a semi supervised setting, so it's going to influence your individual covariance.",
            "Therefore the covariance at the end and the bad thing about this is that inverses of inverses are, you know, are going to be pretty bad condition, so you have to be very careful when you."
        ],
        [
            "Do this and so so Mario, since he was interested in noise.",
            "Extend this to actually have.",
            "More complicated in those functions, so is the case of heteroscedastic noise, where you expect again the noise to vary across data points, and the idea here is."
        ],
        [
            "Basically, key again so this summer scenarios before where you have a subset of clusters and in this case he assumed that he knew for the training points the actual cluster that it belongs to, and then for the test point you need to obtain the clustering.",
            "And for the training you need to actually.",
            "You need to actually learn the parameters of the noise, which in this case we assume very simple.",
            "Yes, diagonal matrix, so there is no correlation between the noise that is just different variances for every point data point."
        ],
        [
            "OK, and then so this I already mentioned that for learning we learn this noise weights and for testing we only need to estimate the Twitch cluster every test point belongs to.",
            "OK, and I just want to mention one more thing and then I will finally finish the presentation and the thing that is related to this is how to learn latent variable models and this is in the case where is unsupervised and you can think of in this observer scenario.",
            "Basically what you do is not only learning the hyperparameters, but you also learn the input space which is the latent variables.",
            "So you have this complex optimization.",
            "Where X here?",
            "So the kernel is a function of X is an ordinary functions, so this is with message to learn, but there are different techniques in order to do that, so we're not going to the details.",
            "But the interesting thing here is that this.",
            "Very simply relates to metric learning with brand divergences, so maybe Killian you are talking a little bit about this.",
            "Maybe not a little bit.",
            "OK, so the basic idea here is the metric learning.",
            "You can identify this simply by yes they logged brand variance.",
            "OK where basically this guy is symbols and then you get the negative sign that usually have."
        ],
        [
            "OK, and what you can interpret here is that you are learning the kernel, but you're learning this kernel to be restricted to be generated by this low dimensional representation, and there has been extensions lately into not only having a single source, but having also multiple views and having shared and private information in order to not only capture element space which represents each one of the views, but also have some written information would represent things that particular.",
            "For for every every view.",
            "OK."
        ],
        [
            "So today I hope that at least I convince some of you that the Gaussian processes can provide a very simple and effective way to framework to do MCL an they allow much more complex combinations that yes, a simple linear mixing.",
            "And so I show a little bit hard to do locality an I show very briefly have additional training can impose this disagreement in a transactive setting an let him very very well.",
            "All those and most of these work again has been done by Mario on the on the back of their and there is much more to do, and in particular there is going to be later in this workshop, so I think it's at 10:00 o'clock by Cedric Anne Francis on also Gaussian process for MCL.",
            "So thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I have a very simple go today which is like over the less amount of people are sleeping at the end of my talk.",
                    "label": 0
                },
                {
                    "sent": "Can you hear me on the back?",
                    "label": 0
                },
                {
                    "sent": "Yes great.",
                    "label": 0
                },
                {
                    "sent": "OK so I will try to show as many pictures as I can since I claimed to do computer vision from time to time.",
                    "label": 0
                },
                {
                    "sent": "So today I will talk about the Gaussian process view of MCL.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK Anne, first I will introduce with Gaussian processes are this will be very brief an I will stay simple formulation of MCL in the Gaussian process framework and then I will go over different formulations which go beyond convex combinations of multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "In particular I would try to focus in the scenario where you have noise.",
                    "label": 0
                },
                {
                    "sent": "And show a little bit about latent variable models and I will conclude the talk.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I got some process is a collection of random variables.",
                    "label": 1
                },
                {
                    "sent": "Anything in number of which have a Gaussian have a joint Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "So basically they are probability distributions over functions and functions are actually infinite dimensional.",
                    "label": 0
                },
                {
                    "sent": "So what we're really having here is a prior distribution over instantiation's of the functions which are finite dimensional object.",
                    "label": 0
                },
                {
                    "sent": "So let me.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simply show you some examples of the prior, which is a Gaussian process of basically, as the name says, I got some processes.",
                    "label": 1
                },
                {
                    "sent": "Gaussian Ann is nonparametric, therefore everything we need to know is the data and the covariance matrix, which is usually defined in terms of a kernel.",
                    "label": 1
                },
                {
                    "sent": "So in this case here what I have is samples from the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "When I use a linear kernel.",
                    "label": 0
                },
                {
                    "sent": "So I suspected the type of functions that we have are linear functions.",
                    "label": 0
                },
                {
                    "sent": "Now if I change the kernel for example I use.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "RV S with particular set of hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to have functions that are actually non linear and they look smooth as we would expect from an RBF.",
                    "label": 0
                },
                {
                    "sent": "Now if I change the hyperparameters, I will change the type of functions that I can generate with my Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, here I increase the length scale.",
                    "label": 0
                },
                {
                    "sent": "Therefore I expect things that are even more smooth than tensor linear.",
                    "label": 0
                },
                {
                    "sent": "And then there are those of.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I can use the same as with any kernel method.",
                    "label": 0
                },
                {
                    "sent": "For example, here I can just use a bias and I can combine all.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kernels in order to get complicated functions to civilly.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now Gaussian processes are actually used typically for regression, so they're going to use in conjunction with the data, so we're going to have some inputs X as some target values Y.",
                    "label": 0
                },
                {
                    "sent": "And we're going to some assume a prior distribution over functions by selecting a kernel.",
                    "label": 1
                },
                {
                    "sent": "And basically we will combine the prior with the data together posterior distribution OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is an example where we have a 1 dimensional input, 11 dimensional output.",
                    "label": 0
                },
                {
                    "sent": "I have two data points here.",
                    "label": 0
                },
                {
                    "sent": "The line represented here is a mean prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, so the posterior is actually got every point with different meaning covariance.",
                    "label": 0
                },
                {
                    "sent": "So this is the main function and I'm using here in RBF and I also have here.",
                    "label": 0
                },
                {
                    "sent": "They were standard deviation which represents my variance at every point.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we see is that close to the training points.",
                    "label": 0
                },
                {
                    "sent": "The variance is very small and as I go far from the training points in my various increases as I will expect him.",
                    "label": 0
                },
                {
                    "sent": "Less confident in my.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action now.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I say add more.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More data then I get more.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm more confident.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This prediction here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so learning in Gaussian processes how is typically done is by doing maximum likelihood or maximum posteriori and the basic idea is that the only thing that we need to learn is actually hyperparameters, which I do not hear theater of my kernel OK, and once I have a fixed hyperparameters, where once I estimate them, then estimating the mean and the variance at every point is actually closed form, which is very very simple to estimate right?",
                    "label": 0
                },
                {
                    "sent": "And let me just introduce now how?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can do MCL in this formulation.",
                    "label": 0
                },
                {
                    "sent": "So basically as is typically then we have a linear combination of kernels, so I have a set of kernels here which I'm going to denote it with uppercase I and I have yes a linear set, and here I'm not constraining to be a convex combination, so I don't have the typical constraint that is have to sum to one OK and the idea is that so this is this.",
                    "label": 0
                },
                {
                    "sent": "Kernels can be either different kernels on the same input data or what is typically done in computer vision is that we use different features.",
                    "label": 1
                },
                {
                    "sent": "For each one of these kernels, and again now I can learn the hyperparameters of American and a little.",
                    "label": 1
                },
                {
                    "sent": "The weights, but yes, using maximum a posterior maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "OK Ann, with the use different regularizers and in particular so different priors if you want to talk in terms of valuation terms and we use is ADA Laplacian prior which is the other one on the weights which is what you will usually have in MCL right you have you want sparse combination of kernels.",
                    "label": 0
                },
                {
                    "sent": "We use Gaussian priors which is the L2 or we use combinations of both with you.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think that these are instances of elastic Nets.",
                    "label": 0
                },
                {
                    "sent": "OK, so some results of this simple MCL, so this is the call to one or one data set and these are I have to save results at the end of 2008.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to show more updated versions of this curve as we go on time an at the time.",
                    "label": 0
                },
                {
                    "sent": "So the fact that we were able to combine all these kernels give us very very good results compared to yes using a single type of feature an.",
                    "label": 0
                },
                {
                    "sent": "So this was good news.",
                    "label": 0
                },
                {
                    "sent": "The bad news here was that actually if you just average the kernels you get more or less the same results as if you actually.",
                    "label": 0
                },
                {
                    "sent": "Try to learn the kernel weights and I think this is very well known in a lot of problems that this is this is happening, which is, I guess not not very nice and some colleagues at the University of Osho also state of the art with this type of approach in activity recognition in computer vision applications.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what are the advantages of using simple MKO?",
                    "label": 0
                },
                {
                    "sent": "Is that the optimization using the same process is really really simple animal ticket with classes scenarios.",
                    "label": 0
                },
                {
                    "sent": "Actually if we share the parameters then for each of the classifiers we can learn everything together using the same processes.",
                    "label": 0
                },
                {
                    "sent": "That's great.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is as was mentioned, is the fact that is very disappointed.",
                    "label": 0
                },
                {
                    "sent": "Yes average in the kernel, so you have you noticed emissions on the hyperparameters?",
                    "label": 0
                },
                {
                    "sent": "Actually that's more or less the same.",
                    "label": 0
                },
                {
                    "sent": "An one thing that we.",
                    "label": 0
                },
                {
                    "sent": "So that was one of the reasons is that the fact that we have a fixed combination across all this space limits the ability to capture.",
                    "label": 0
                },
                {
                    "sent": "You know more rich extractors here.",
                    "label": 0
                },
                {
                    "sent": "So in the case for example when you have noise or incomplete data or where you expect the properties of the space to worry locali simple linear combination is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is not enough.",
                    "label": 0
                },
                {
                    "sent": "An in the case of Gaussian processes, you're going to extend this linear combination to be, for example, predator kernels, another different combinations, or some some products.",
                    "label": 0
                },
                {
                    "sent": "You can combine all these different things, and again you can learn this using maximum likelihood and the idea of you combine producer Colonel says that you're going to have a kernel that is much sharper than what you had before.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so but the still this measures are global, so we still feel that too simplistic so we really wanted to go into local versions of this an I have to point out that we weren't at all the first one, so to try to use this locality and there has been multiple approaches in the SVM framework, but I'm now going to yes presented with with.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our case.",
                    "label": 0
                },
                {
                    "sent": "OK, so Mario Christie's will sitting on the back of the room did this for his PhD.",
                    "label": 0
                },
                {
                    "sent": "Basically the idea is that we are going to consider that were Colonel again by Smosh some kernels.",
                    "label": 0
                },
                {
                    "sent": "But now these kernels are going to be more complicated than the current that we had before an for each one of our kernels.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is that we are going to compute the kernel by the Hadamard product.",
                    "label": 0
                },
                {
                    "sent": "So Elementwise product of a parametric kernel, which is what you typically do, right?",
                    "label": 0
                },
                {
                    "sent": "For example, the RBF anano parametric kernel, which hopefully captures this locality of the weights in this space.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is now what were some of corners look like.",
                    "label": 0
                },
                {
                    "sent": "So we have this nonparametric kernel.",
                    "label": 0
                },
                {
                    "sent": "We have this parametric kernel and the idea is that we have we want to learn the hyper parameters of this guy here.",
                    "label": 0
                },
                {
                    "sent": "Plus all these metrics here which is N * N. OK so you can assume that things are symmetric for example.",
                    "label": 0
                },
                {
                    "sent": "But still you have tons of parameters to estimate, so it's going to be really really simple to over fit to the data right?",
                    "label": 0
                },
                {
                    "sent": "So we decided was.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To try to look into simplification on this nonparametric thing so that we have less chance to overfit.",
                    "label": 0
                },
                {
                    "sent": "So the first idea is well, if we have a matrix that is, you know 2 high dimensional, then let's try to do some low rank approximation right?",
                    "label": 0
                },
                {
                    "sent": "So in this case here we have yes a local representation and we assume symmetric form.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now the number of parameters is going to be the number of kernels times the dimension of this low rank times the, which is the number of training points that we have.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we will have also our parametric control over there, so we went even further than that.",
                    "label": 0
                },
                {
                    "sent": "I'd say, well, we really care about in our applications is to capture this notion of noise per data point OK. And then instead of having a P dimensional space where we're going to have is dimensional space, is that we're going to have you as a single one dimensional vector here.",
                    "label": 0
                },
                {
                    "sent": "And you can think of this again as really every here actually pointing out how much I trust that data points in order to estimate my kernel combination.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An if you actually have structure in your noise as we had in our case, you can think of modeling this as a clustering problem, which further simplifies the things.",
                    "label": 0
                },
                {
                    "sent": "We're basically, I say I have a set of indicator variables here which represents which cluster every point is, and then if I assume that they only have P clusters and I have only P values to estimate, which basically are the different types of noise that I expect my data to be OK, and then this.",
                    "label": 0
                },
                {
                    "sent": "Actually you compute these metrics and then you.",
                    "label": 0
                },
                {
                    "sent": "Multiply that they had more product with the number with the parametric form and the.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the idea of Department application that we were looking at.",
                    "label": 0
                },
                {
                    "sent": "We were looking at this video segment application which I will just show in a minute and in that case we will expect the actual type of noise that the data has to have some structure in the sense that is not yet ready but we have a set of clusters which actually can represent all the possible noises that we have an basically.",
                    "label": 0
                },
                {
                    "sent": "Then you can simply further simplifies to only have 3 parameters times the number of clusters.",
                    "label": 0
                },
                {
                    "sent": "Sorry the number of views here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And yes, in yes to contrast it with the traditional color combination.",
                    "label": 0
                },
                {
                    "sent": "So basically what regional kernel combination is that this guy here is just a constant, which is the weight that you give for that particular kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, and here we have much richer structures and then.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how will you learn this thing again?",
                    "label": 0
                },
                {
                    "sent": "You do maximum likelihood and we yes use a relatively non external priors in this case which seems to be working relatively well and in order to do inference we are going to do multiclass classification and the only thing that we did yes to maximize over the class and then pick up the class which is the maximum response.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me go over the different examples that Mario was working on.",
                    "label": 1
                },
                {
                    "sent": "So here is Mario, a few years ago an so we were looking into this big disagreement data set, which will disagreement in this case.",
                    "label": 0
                },
                {
                    "sent": "Let me just explain what it is so we have this scenario which we have.",
                    "label": 0
                },
                {
                    "sent": "We ask questions to a set of subjects an we record the video as well as the audio an they be.",
                    "label": 0
                },
                {
                    "sent": "This agreement is the fact that you can actually.",
                    "label": 1
                },
                {
                    "sent": "Respond visually, saying this and moving your head right so the two views which are going to be the other video actually give information about this.",
                    "label": 0
                },
                {
                    "sent": "The fact that he said yes, right?",
                    "label": 0
                },
                {
                    "sent": "You can yes say yes, but not move your head.",
                    "label": 0
                },
                {
                    "sent": "In that case the video is actually corrupted innocence, right?",
                    "label": 0
                },
                {
                    "sent": "So that's and give you information that actually this is saying yes.",
                    "label": 0
                },
                {
                    "sent": "And the same thing can actually happen for the early on the video, right?",
                    "label": 0
                },
                {
                    "sent": "So in order to have a control setting, we simulate this be disagreement by adding background head motion.",
                    "label": 0
                },
                {
                    "sent": "In the case of, in the case of the gesture and bubble noise for the case of the audio.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so these are yes results as a function of the view disagreement.",
                    "label": 0
                },
                {
                    "sent": "So how much of this noisy data is there?",
                    "label": 0
                },
                {
                    "sent": "So in the case that there is no big disagreement, then in this case one of the classifiers is really good, so you actually don't even need the audio in order to classify and you know it doesn't marriage to MCL.",
                    "label": 0
                },
                {
                    "sent": "You do this complicated combination.",
                    "label": 0
                },
                {
                    "sent": "You actually do fairly well, and in the case of you have a lot of disagreement.",
                    "label": 0
                },
                {
                    "sent": "This means that 70% of the samples have at least have one via corrupted.",
                    "label": 0
                },
                {
                    "sent": "Either the audio or video, and in this case you see that there is a boost performance if you actually try to capture the fact that some of these samples are corrupted, which is basically what this clustering is doing.",
                    "label": 0
                },
                {
                    "sent": "OK, and here is again the the audio and video which are the two modalities.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we try also this in a more standard of recognition data set again, is Caltech 101, so it's 100 and one categories, so it's the same example as I showed before, and here we are going to compute multiple kernels by using different features that people have built in the vision community.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so the good news is that actually we perform more you we match the state of the art of the time.",
                    "label": 0
                },
                {
                    "sent": "So this is one year later.",
                    "label": 0
                },
                {
                    "sent": "So you see that there is much much more people actually trying to combine different sources of information.",
                    "label": 0
                },
                {
                    "sent": "Now the bad news again is that we get more or less the same results as by doing the kernel combination OK and the main reason here is that actually having this richer structure thus help when you have noisy data or when you have.",
                    "label": 0
                },
                {
                    "sent": "This this locality very nothing, but actually we believe that in this data set without we don't have enough data to actually capture this this variation.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "So the reported booster.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it was in performance and we did not get that Bruce, so I've seen a version of that paper which they did not have that much boost, so I don't know what was the way they did to make it to work.",
                    "label": 0
                },
                {
                    "sent": "But here.",
                    "label": 0
                },
                {
                    "sent": "So I guess we couldn't really make it to work.",
                    "label": 0
                },
                {
                    "sent": "And actually here you see as an example as a function of the number of clusters, we really have flat.",
                    "label": 0
                },
                {
                    "sent": "Performance an now we simulate missing data and in that case you actually have a boosted performance with respect to your simple MCL.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So no so here where they showed before was actually Biclustering, and Kevin simplified version of this an Now the.",
                    "label": 0
                },
                {
                    "sent": "So what we try to do is so very preliminary results into trying to estimate the full nonparametric covariance.",
                    "label": 0
                },
                {
                    "sent": "Another question, if you want to do that is worse, actually.",
                    "label": 0
                },
                {
                    "sent": "The regularizer that I avoid this massive overfitting that you would expect to have.",
                    "label": 0
                },
                {
                    "sent": "So we thought of two possible regularizers which are very typical.",
                    "label": 0
                },
                {
                    "sent": "Rather use the passion regularizer based on some measure of distance and similarity that you have.",
                    "label": 0
                },
                {
                    "sent": "Or if you have some estimate of maybe what you know, a good estimation of this.",
                    "label": 0
                },
                {
                    "sent": "Then maybe you want to have a Frobenius norm that minimize the distance to that estimate that you think is is possible.",
                    "label": 0
                },
                {
                    "sent": "For example, this can be yes.",
                    "label": 0
                },
                {
                    "sent": "The fact that you know the constant weights or yes, if it's you know average kernels all once an.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We need a simple experiment where in this audio video, original data set and actually we got a boost performance.",
                    "label": 0
                },
                {
                    "sent": "If we learn the complicated metrics, very interesting technical violation codes.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Raining, which was developed by you'll nips a couple of years ago, and the main idea is this is going to be retrospective setting, and the main idea is that you want to impose an agreement prior in order to regularize your solution.",
                    "label": 0
                },
                {
                    "sent": "So you're going to have different views here.",
                    "label": 0
                },
                {
                    "sent": "You're going to have Gaussian process, so little function for a review, and then you have Gaussian process priors on each of these guys.",
                    "label": 0
                },
                {
                    "sent": "And then you have no Gaussian noise on this.",
                    "label": 0
                },
                {
                    "sent": "The good thing about using this is that you can actually marginalized this in closed form.",
                    "label": 0
                },
                {
                    "sent": "An you marginalized this.",
                    "label": 0
                },
                {
                    "sent": "Then you end up with a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "At this note here we check variance, which is a combination of kernels in a very non essential form.",
                    "label": 0
                },
                {
                    "sent": "So what you get is the inverse of the sum of the inverses of the kernels.",
                    "label": 0
                },
                {
                    "sent": "Now the good thing about this is that.",
                    "label": 0
                },
                {
                    "sent": "In the inverse you actually can use this in a semi supervised setting, so it's going to influence your individual covariance.",
                    "label": 0
                },
                {
                    "sent": "Therefore the covariance at the end and the bad thing about this is that inverses of inverses are, you know, are going to be pretty bad condition, so you have to be very careful when you.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do this and so so Mario, since he was interested in noise.",
                    "label": 0
                },
                {
                    "sent": "Extend this to actually have.",
                    "label": 0
                },
                {
                    "sent": "More complicated in those functions, so is the case of heteroscedastic noise, where you expect again the noise to vary across data points, and the idea here is.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, key again so this summer scenarios before where you have a subset of clusters and in this case he assumed that he knew for the training points the actual cluster that it belongs to, and then for the test point you need to obtain the clustering.",
                    "label": 0
                },
                {
                    "sent": "And for the training you need to actually.",
                    "label": 0
                },
                {
                    "sent": "You need to actually learn the parameters of the noise, which in this case we assume very simple.",
                    "label": 0
                },
                {
                    "sent": "Yes, diagonal matrix, so there is no correlation between the noise that is just different variances for every point data point.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and then so this I already mentioned that for learning we learn this noise weights and for testing we only need to estimate the Twitch cluster every test point belongs to.",
                    "label": 0
                },
                {
                    "sent": "OK, and I just want to mention one more thing and then I will finally finish the presentation and the thing that is related to this is how to learn latent variable models and this is in the case where is unsupervised and you can think of in this observer scenario.",
                    "label": 0
                },
                {
                    "sent": "Basically what you do is not only learning the hyperparameters, but you also learn the input space which is the latent variables.",
                    "label": 0
                },
                {
                    "sent": "So you have this complex optimization.",
                    "label": 0
                },
                {
                    "sent": "Where X here?",
                    "label": 0
                },
                {
                    "sent": "So the kernel is a function of X is an ordinary functions, so this is with message to learn, but there are different techniques in order to do that, so we're not going to the details.",
                    "label": 0
                },
                {
                    "sent": "But the interesting thing here is that this.",
                    "label": 0
                },
                {
                    "sent": "Very simply relates to metric learning with brand divergences, so maybe Killian you are talking a little bit about this.",
                    "label": 0
                },
                {
                    "sent": "Maybe not a little bit.",
                    "label": 0
                },
                {
                    "sent": "OK, so the basic idea here is the metric learning.",
                    "label": 0
                },
                {
                    "sent": "You can identify this simply by yes they logged brand variance.",
                    "label": 0
                },
                {
                    "sent": "OK where basically this guy is symbols and then you get the negative sign that usually have.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and what you can interpret here is that you are learning the kernel, but you're learning this kernel to be restricted to be generated by this low dimensional representation, and there has been extensions lately into not only having a single source, but having also multiple views and having shared and private information in order to not only capture element space which represents each one of the views, but also have some written information would represent things that particular.",
                    "label": 0
                },
                {
                    "sent": "For for every every view.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today I hope that at least I convince some of you that the Gaussian processes can provide a very simple and effective way to framework to do MCL an they allow much more complex combinations that yes, a simple linear mixing.",
                    "label": 0
                },
                {
                    "sent": "And so I show a little bit hard to do locality an I show very briefly have additional training can impose this disagreement in a transactive setting an let him very very well.",
                    "label": 0
                },
                {
                    "sent": "All those and most of these work again has been done by Mario on the on the back of their and there is much more to do, and in particular there is going to be later in this workshop, so I think it's at 10:00 o'clock by Cedric Anne Francis on also Gaussian process for MCL.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}