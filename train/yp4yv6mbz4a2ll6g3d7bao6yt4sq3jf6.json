{
    "id": "yp4yv6mbz4a2ll6g3d7bao6yt4sq3jf6",
    "title": "Batch Discovery of Recurring Rare Classes toward Identifying Anomalous Samples",
    "info": {
        "presenter": [
            "M. Murat Dundar, Department of Computer and Information Science, Indiana University-Purdue University Indianapolis"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_dundar_anomalous_samples/",
    "segmentation": [
        [
            "Good afternoon everyone.",
            "My name is brought in Dashain.",
            "I will be presenting our paper and based discovery of recurring rate classes toward identifying anomalous samples.",
            "So this is a joint work with Holly Timbertech."
        ],
        [
            "So here's the problem that we are trying to solve.",
            "Given a batch of samples, we want to identify global clusters and their corresponding local realizations across samples under random effects, and there are certain challenges that makes this problem interesting from a research perspective, and the first one will be the samples are subject to random effects and the number of local and global clusters are not known, so we have some clusters that are extremely rare.",
            "And those are the most interesting ones that certainly we don't wanna miss them.",
            "Clusters can be recurring, but there are also isolated clusters which only appear once and local cluster distributions are not necessarily Gaussian or are not necessarily unimodal."
        ],
        [
            "So here's an illustrative example that also demonstrates the problem.",
            "So we have 5 samples.",
            "So in this case there are two global clusters that generate this data.",
            "We have the red cluster and the blue Cluster an.",
            "As you can see, the cluster centers may deviate from 1 sample to others, so we have blue clusters here and then.",
            "In the other sample it's here, and in the 5th sample it's here.",
            "So we all.",
            "We also have this problem of.",
            "Some clusters may appear and disappear across samples, so we don't have any blue clusters contributing to sample two and four.",
            "We also have this problem of multimodal data or skewed data, so some of the clusters could have been generated by by a mixture of Gaussians, or they can be secured or multimode."
        ],
        [
            "So the main motivation behind our researches is some problems from flow cytometry data analysis.",
            "So flow cytometry is a single cell single cell screening technology and it plays a crucial role in immunology, hematology and oncology.",
            "And here you see the typical architecture of a flow cytometer, so we have a few lytic system that contains all the cells and this political system also contains several biomarkers and these biomarkers, depending on the actually the DNA characteristics of the cells, each cell respond to these biomarkers in a different way so they get different colors.",
            "So when we send these two laser lights, the laser light is deflected through the lens and then the scattered light from each cell is captured by a number of detectors.",
            "And these detectors are designed to capture light at a certain wavelength, so this will be the number of detectors will be the number of attributes that we have in our data set.",
            "So this is a flow cytometry is an important tool actually for diagnosing a lot of diseases, because in each biological sample we will see a lot of different functionally distinct cell populations, and depending on this, the characteristics of this cell populations, we can learn more about somebody's immune system.",
            "We can diagnose HIV aids.",
            "We can also diagnose certain blood diseases, cancer, an other other things so."
        ],
        [
            "The related work so we can broadly categorize the related work into tree.",
            "First, given that we have a batch of samples, we can simply pull all of the data together and then just use a standard clustering algorithm to cluster this data.",
            "But the problem here is that when we have random effects involved and then you merge all of the samples together, some of the clusters will overlap with some other clusters and you will lose the original clustering formation.",
            "So the second approach will be the sequential processing.",
            "You could just first.",
            "Cluster each data on an individual basis and then try to match clusters across centers using graph matching algorithms or some other tools.",
            "Now this approach.",
            "First of all it is it has limitation the main limitation of being suboptimal because you have a single inference task and you are splitting this task.",
            "We are decoupling this test into two different subtasks and you are trying to solve 1 by 1 which is suboptimal and the other thing is when you have rail classes involved and then you try to do.",
            "Process the samples on an independent basis.",
            "Then chances are that you will not see lot of their classes, meaning that if you have very few points, chances are that you will miss those rare classes.",
            "When you process these samples on a per sample basis.",
            "Now the third approach that we will mainly consider is when we perform clustering and cluster matching jointly.",
            "So we call this joint processing and I will be mainly refering to nonparametric Bayesian techniques in this area and including HTM and HTML here and I will also describe why these models are somewhat."
        ],
        [
            "Strict if so, the algorithm that we propose is aspire aspire stands for animal sample, phenotype identification with random effects.",
            "It can identify recurring classes in a completely unsupervised fashion.",
            "It can deal with data perturbed by random effects, and it can characterize normal and anomalous states under very big assumptions about sample."
        ],
        [
            "Actress ticks so and these are the technical details of our algorithm in a nutshell, so we are using an infinite mixture of dishnet process Gaussian mixture models for characterizing a single sample, and then we have one DPM for each local distribution of a class and this offers the flexibility to model complex local distributions and we have a sharing mechanism established across multiple samples to create dependencies and our model also take into account the.",
            "Potential random effects and we use a collapsed Gibbs sampler."
        ],
        [
            "Model inference and the generative model we have here is I will describe the generative model in four stages.",
            "First we will be.",
            "We will use independent modeling of samples by the PM and then we will introduce dependencies.",
            "Then we will introduce random effect into the model and then finally I will propose our model which takes into account multiple individual samples."
        ],
        [
            "With multiple DPMS.",
            "So here is independent modeling of samples by DPM, so we have a point, a data point that is associated by a Gaussian distribution.",
            "And this parameters of the Gaussian distribution come from a random probability measure measure which is sampled from a dish that process.",
            "So this G sub J is actually a discrete distribution with infinitely many mixture components and we have two parameters of the dish that process which is first base distribution and the other one is the concentration parameter.",
            "So the base distribution helps us incorporate prior knowledge about cluster parameters and AA or concentration parameter controls the number of mixture components and their sizes."
        ],
        [
            "So this is a two step generative process in the PM, so we first drove an Atom from the sub J and then we generated data point from the distribution defined by the drone Atom.",
            "So in this case it just subject is defined by a different set of atoms in the."
        ],
        [
            "In the DPM now we can introduce dependencies across multiple samples by making this slight change, which takes us to the HTP.",
            "More hierarchical process model.",
            "So in the deep model we have our base distribution as a fixed bivariate distribution, and now instead of using a fixed bivariate distribution, we are defining a hyper prior over this base distribution.",
            "And now by doing this change, now we ensure that each sub J is now defined by the same set of atoms.",
            "But their proportions are different, so this allows us for shading parameters across multiple samples.",
            "So it introduces the shading mechanism across samples."
        ],
        [
            "Now the third extension to the HTP model will be HTP Emory.",
            "So in HD PM there is this restriction that all of the atoms exact copies of all of the atoms are shared across all samples, so you have multiple clusters that appears in multiple samples.",
            "They all share the same parameter, but from a realistic for from a real world perspective, this is somewhat restrictive because when you have random effects involved, you don't expect the same cluster to appear across all samples, so the clusters will be perturbed to some extent.",
            "So and this takes us to the HTP Emory modeled by Kim Answer might.",
            "And it's an extension of HTP that generates local data from noisy versions of."
        ],
        [
            "Mobile parameters and this is the generative model for HTP Marina.",
            "Sorry this is the generative model for HTP Emory.",
            "Now, instead of having this sub zero here we have the sub zero J which means we have a customized based distribution for each sample and the outcomes of this based distribution are noisy versions of some global parameters."
        ],
        [
            "And this takes us to the proposed generative model.",
            "So the main restriction by HTPM and HTP Emery is that they use a single Gaussian for each local cluster.",
            "But this is not very ideal, especially when you have cluster distributions that are secured or multi malt.",
            "So the proposed algorithm uses instead of using a single DPM for each sample.",
            "We are using a potentially infinitely many dish that process mixture of Gaussians for each sample so that when there are multiple clusters involved in a sample and each cluster can appear as a.",
            "As secured or multiple distributions, we will be able to able to model these distributions in a more effective manner."
        ],
        [
            "So, and this is the generative model that we proposed, and in this case we have 5 sub K which are the global parameters distributed according to G sub zero and then we have this distribution that is centered at suffice up K and this is the noise model that we consider.",
            "So each Atom in this for each sample.",
            "Sorry for each local realization of a class, it is distributed according to this normal distribution which is parameterized by the corresponding global parameters."
        ],
        [
            "So the model inference is performed by a collapsed Gibbs sampler.",
            "So in the first step we are just sampling the cluster indicator variables for all data points and in the second step, so each scan of the Gibbs sampler involves two stages.",
            "The first stage is sampling the local cluster indicator variables for all data points and then we are sampling the global cluster indicator variables for all mixture components.",
            "And here we have this posterior predictive distribution for local distributions and posterior predictive distribution for global clusters.",
            "So these are both.",
            "We can obtain them in a closed closed form solution because our model is conjugate.",
            "So and the noise model also preserves the conjugates of the model.",
            "So which means we can completely eliminate all of the cluster parameters and just sample the cluster indicator variables and data point indicator variables."
        ],
        [
            "So as an illustration, we have generated 20 samples, each with 5000 data points and then after all points are generated, so we use this generative model to generate the data and then we get three clusters.",
            "Three classes actually one of the classes is the dominant class and the other two classes are extremely rare.",
            "So these are the percentages or ratios of data points from this rail class."
        ],
        [
            "And then we pull data points from all 20 samples together and this is the image that we see.",
            "So we have this dominant class and we have these two rare clusters.",
            "And as you can see there are serious deviation across clusters and the goal here is whether or not our algorithm can recover the distributions of the global clusters and."
        ],
        [
            "Then we run inspire the proposed algorithm on this data and we actually does almost a perfect job in recovering the global cluster distributions here and here.",
            "You see, the global cluster distribution, the true ones as the solid lines, solid color lines, and then we."
        ],
        [
            "Compare this for example with the PM that uses pooled data and this is the type of clusters that we will get.",
            "Sorry, so this is the type of cluster state we will get an this is HTPM and this is HTML DHTML, little slightly better than the other two techniques, whereas the other two techniques as you can see generates a lot of extraneous clusters, and the main reason being that this cluster distributions are not necessarily Gaussian.",
            "So when you combine when you consider this generative model, they are not necessarily Gaussian and as a result HD PM and DPM.",
            "And HTML, they all of the three generates extra."
        ],
        [
            "Clusters, so we also apply our technique on a real world application problem.",
            "So we have this a couple data set, so a couple actually monitors labs involved in HIV and AIDS research and Max in trial."
        ],
        [
            "Around the world.",
            "So the data set we have contains actually three biological samples exposed to three different stimulation levels and collected by 15 different labs.",
            "So we have three sources of variation in our data.",
            "First we have the sample heterogeneity.",
            "Then we have the different stimulation levels and then we have 15 different labs collecting these measurements.",
            "So there are 202 samples and 1.1 million data points in our data set.",
            "Each data point is a cell, so the goal is to recognize cells belonging to two rare classes.",
            "Ann says not belonging to one of the two rare classes are considered normal and this is the histogram or the ratio of the samples data points we have here.",
            "98% of the sales come from normal cells which are not very interesting.",
            "0.5% comes from rail class one and one point.",
            "3% comes from rail class too.",
            "So these are the two most interesting clusters that we want."
        ],
        [
            "Point, so these are the benchmarks that we use and we are using.",
            "F1 score is a powerful."
        ],
        [
            "This measure and these are the results that we have, so we can see that methods modeling random effects in general do better.",
            "So we have HTP emerian aspire doing better than the PM and 8:00 PM.",
            "And we see that all three techniques modeling each sample by a single DPM.",
            "Generate excessively large number of clusters.",
            "As you can see from here, the last column and HT PM almost completely Miss Israel classes, whereas the proposed technique actually does a decent job of recovering this rare cluster distributions."
        ],
        [
            "So as a conclusion, we propose a new method for batch clustering, sample subject to random effects.",
            "Our algorithm is implemented in C++ and the source code is publicly available on my website.",
            "So an on a data set involving 1.9 million points.",
            "Our algorithm runs in five hours on an 8 core PC, an part of the algorithm is parallelizable.",
            "The first part is Paralyzable and the second part is not.",
            "So we also as a future research direction, for example.",
            "We can also incorporate labeled information into this process through a process called restricted Gibbs sampler, and we can also use this kind of models.",
            "For example, for in a problem involving group anomaly detection, which is recently an emerging field in this area.",
            "So, and this concludes my talk and I will be happy to answer any questions that you might have."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon everyone.",
                    "label": 0
                },
                {
                    "sent": "My name is brought in Dashain.",
                    "label": 0
                },
                {
                    "sent": "I will be presenting our paper and based discovery of recurring rate classes toward identifying anomalous samples.",
                    "label": 1
                },
                {
                    "sent": "So this is a joint work with Holly Timbertech.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the problem that we are trying to solve.",
                    "label": 0
                },
                {
                    "sent": "Given a batch of samples, we want to identify global clusters and their corresponding local realizations across samples under random effects, and there are certain challenges that makes this problem interesting from a research perspective, and the first one will be the samples are subject to random effects and the number of local and global clusters are not known, so we have some clusters that are extremely rare.",
                    "label": 1
                },
                {
                    "sent": "And those are the most interesting ones that certainly we don't wanna miss them.",
                    "label": 1
                },
                {
                    "sent": "Clusters can be recurring, but there are also isolated clusters which only appear once and local cluster distributions are not necessarily Gaussian or are not necessarily unimodal.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an illustrative example that also demonstrates the problem.",
                    "label": 0
                },
                {
                    "sent": "So we have 5 samples.",
                    "label": 0
                },
                {
                    "sent": "So in this case there are two global clusters that generate this data.",
                    "label": 0
                },
                {
                    "sent": "We have the red cluster and the blue Cluster an.",
                    "label": 0
                },
                {
                    "sent": "As you can see, the cluster centers may deviate from 1 sample to others, so we have blue clusters here and then.",
                    "label": 1
                },
                {
                    "sent": "In the other sample it's here, and in the 5th sample it's here.",
                    "label": 0
                },
                {
                    "sent": "So we all.",
                    "label": 0
                },
                {
                    "sent": "We also have this problem of.",
                    "label": 0
                },
                {
                    "sent": "Some clusters may appear and disappear across samples, so we don't have any blue clusters contributing to sample two and four.",
                    "label": 1
                },
                {
                    "sent": "We also have this problem of multimodal data or skewed data, so some of the clusters could have been generated by by a mixture of Gaussians, or they can be secured or multimode.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main motivation behind our researches is some problems from flow cytometry data analysis.",
                    "label": 0
                },
                {
                    "sent": "So flow cytometry is a single cell single cell screening technology and it plays a crucial role in immunology, hematology and oncology.",
                    "label": 1
                },
                {
                    "sent": "And here you see the typical architecture of a flow cytometer, so we have a few lytic system that contains all the cells and this political system also contains several biomarkers and these biomarkers, depending on the actually the DNA characteristics of the cells, each cell respond to these biomarkers in a different way so they get different colors.",
                    "label": 0
                },
                {
                    "sent": "So when we send these two laser lights, the laser light is deflected through the lens and then the scattered light from each cell is captured by a number of detectors.",
                    "label": 0
                },
                {
                    "sent": "And these detectors are designed to capture light at a certain wavelength, so this will be the number of detectors will be the number of attributes that we have in our data set.",
                    "label": 0
                },
                {
                    "sent": "So this is a flow cytometry is an important tool actually for diagnosing a lot of diseases, because in each biological sample we will see a lot of different functionally distinct cell populations, and depending on this, the characteristics of this cell populations, we can learn more about somebody's immune system.",
                    "label": 0
                },
                {
                    "sent": "We can diagnose HIV aids.",
                    "label": 0
                },
                {
                    "sent": "We can also diagnose certain blood diseases, cancer, an other other things so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The related work so we can broadly categorize the related work into tree.",
                    "label": 0
                },
                {
                    "sent": "First, given that we have a batch of samples, we can simply pull all of the data together and then just use a standard clustering algorithm to cluster this data.",
                    "label": 0
                },
                {
                    "sent": "But the problem here is that when we have random effects involved and then you merge all of the samples together, some of the clusters will overlap with some other clusters and you will lose the original clustering formation.",
                    "label": 0
                },
                {
                    "sent": "So the second approach will be the sequential processing.",
                    "label": 1
                },
                {
                    "sent": "You could just first.",
                    "label": 0
                },
                {
                    "sent": "Cluster each data on an individual basis and then try to match clusters across centers using graph matching algorithms or some other tools.",
                    "label": 0
                },
                {
                    "sent": "Now this approach.",
                    "label": 0
                },
                {
                    "sent": "First of all it is it has limitation the main limitation of being suboptimal because you have a single inference task and you are splitting this task.",
                    "label": 0
                },
                {
                    "sent": "We are decoupling this test into two different subtasks and you are trying to solve 1 by 1 which is suboptimal and the other thing is when you have rail classes involved and then you try to do.",
                    "label": 0
                },
                {
                    "sent": "Process the samples on an independent basis.",
                    "label": 0
                },
                {
                    "sent": "Then chances are that you will not see lot of their classes, meaning that if you have very few points, chances are that you will miss those rare classes.",
                    "label": 0
                },
                {
                    "sent": "When you process these samples on a per sample basis.",
                    "label": 1
                },
                {
                    "sent": "Now the third approach that we will mainly consider is when we perform clustering and cluster matching jointly.",
                    "label": 0
                },
                {
                    "sent": "So we call this joint processing and I will be mainly refering to nonparametric Bayesian techniques in this area and including HTM and HTML here and I will also describe why these models are somewhat.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Strict if so, the algorithm that we propose is aspire aspire stands for animal sample, phenotype identification with random effects.",
                    "label": 0
                },
                {
                    "sent": "It can identify recurring classes in a completely unsupervised fashion.",
                    "label": 1
                },
                {
                    "sent": "It can deal with data perturbed by random effects, and it can characterize normal and anomalous states under very big assumptions about sample.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actress ticks so and these are the technical details of our algorithm in a nutshell, so we are using an infinite mixture of dishnet process Gaussian mixture models for characterizing a single sample, and then we have one DPM for each local distribution of a class and this offers the flexibility to model complex local distributions and we have a sharing mechanism established across multiple samples to create dependencies and our model also take into account the.",
                    "label": 0
                },
                {
                    "sent": "Potential random effects and we use a collapsed Gibbs sampler.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model inference and the generative model we have here is I will describe the generative model in four stages.",
                    "label": 0
                },
                {
                    "sent": "First we will be.",
                    "label": 0
                },
                {
                    "sent": "We will use independent modeling of samples by the PM and then we will introduce dependencies.",
                    "label": 1
                },
                {
                    "sent": "Then we will introduce random effect into the model and then finally I will propose our model which takes into account multiple individual samples.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With multiple DPMS.",
                    "label": 0
                },
                {
                    "sent": "So here is independent modeling of samples by DPM, so we have a point, a data point that is associated by a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "And this parameters of the Gaussian distribution come from a random probability measure measure which is sampled from a dish that process.",
                    "label": 0
                },
                {
                    "sent": "So this G sub J is actually a discrete distribution with infinitely many mixture components and we have two parameters of the dish that process which is first base distribution and the other one is the concentration parameter.",
                    "label": 0
                },
                {
                    "sent": "So the base distribution helps us incorporate prior knowledge about cluster parameters and AA or concentration parameter controls the number of mixture components and their sizes.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a two step generative process in the PM, so we first drove an Atom from the sub J and then we generated data point from the distribution defined by the drone Atom.",
                    "label": 0
                },
                {
                    "sent": "So in this case it just subject is defined by a different set of atoms in the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the DPM now we can introduce dependencies across multiple samples by making this slight change, which takes us to the HTP.",
                    "label": 0
                },
                {
                    "sent": "More hierarchical process model.",
                    "label": 0
                },
                {
                    "sent": "So in the deep model we have our base distribution as a fixed bivariate distribution, and now instead of using a fixed bivariate distribution, we are defining a hyper prior over this base distribution.",
                    "label": 0
                },
                {
                    "sent": "And now by doing this change, now we ensure that each sub J is now defined by the same set of atoms.",
                    "label": 1
                },
                {
                    "sent": "But their proportions are different, so this allows us for shading parameters across multiple samples.",
                    "label": 1
                },
                {
                    "sent": "So it introduces the shading mechanism across samples.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the third extension to the HTP model will be HTP Emory.",
                    "label": 0
                },
                {
                    "sent": "So in HD PM there is this restriction that all of the atoms exact copies of all of the atoms are shared across all samples, so you have multiple clusters that appears in multiple samples.",
                    "label": 0
                },
                {
                    "sent": "They all share the same parameter, but from a realistic for from a real world perspective, this is somewhat restrictive because when you have random effects involved, you don't expect the same cluster to appear across all samples, so the clusters will be perturbed to some extent.",
                    "label": 0
                },
                {
                    "sent": "So and this takes us to the HTP Emory modeled by Kim Answer might.",
                    "label": 0
                },
                {
                    "sent": "And it's an extension of HTP that generates local data from noisy versions of.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mobile parameters and this is the generative model for HTP Marina.",
                    "label": 0
                },
                {
                    "sent": "Sorry this is the generative model for HTP Emory.",
                    "label": 0
                },
                {
                    "sent": "Now, instead of having this sub zero here we have the sub zero J which means we have a customized based distribution for each sample and the outcomes of this based distribution are noisy versions of some global parameters.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this takes us to the proposed generative model.",
                    "label": 0
                },
                {
                    "sent": "So the main restriction by HTPM and HTP Emery is that they use a single Gaussian for each local cluster.",
                    "label": 1
                },
                {
                    "sent": "But this is not very ideal, especially when you have cluster distributions that are secured or multi malt.",
                    "label": 0
                },
                {
                    "sent": "So the proposed algorithm uses instead of using a single DPM for each sample.",
                    "label": 0
                },
                {
                    "sent": "We are using a potentially infinitely many dish that process mixture of Gaussians for each sample so that when there are multiple clusters involved in a sample and each cluster can appear as a.",
                    "label": 0
                },
                {
                    "sent": "As secured or multiple distributions, we will be able to able to model these distributions in a more effective manner.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, and this is the generative model that we proposed, and in this case we have 5 sub K which are the global parameters distributed according to G sub zero and then we have this distribution that is centered at suffice up K and this is the noise model that we consider.",
                    "label": 0
                },
                {
                    "sent": "So each Atom in this for each sample.",
                    "label": 0
                },
                {
                    "sent": "Sorry for each local realization of a class, it is distributed according to this normal distribution which is parameterized by the corresponding global parameters.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the model inference is performed by a collapsed Gibbs sampler.",
                    "label": 1
                },
                {
                    "sent": "So in the first step we are just sampling the cluster indicator variables for all data points and in the second step, so each scan of the Gibbs sampler involves two stages.",
                    "label": 0
                },
                {
                    "sent": "The first stage is sampling the local cluster indicator variables for all data points and then we are sampling the global cluster indicator variables for all mixture components.",
                    "label": 1
                },
                {
                    "sent": "And here we have this posterior predictive distribution for local distributions and posterior predictive distribution for global clusters.",
                    "label": 0
                },
                {
                    "sent": "So these are both.",
                    "label": 0
                },
                {
                    "sent": "We can obtain them in a closed closed form solution because our model is conjugate.",
                    "label": 0
                },
                {
                    "sent": "So and the noise model also preserves the conjugates of the model.",
                    "label": 0
                },
                {
                    "sent": "So which means we can completely eliminate all of the cluster parameters and just sample the cluster indicator variables and data point indicator variables.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as an illustration, we have generated 20 samples, each with 5000 data points and then after all points are generated, so we use this generative model to generate the data and then we get three clusters.",
                    "label": 1
                },
                {
                    "sent": "Three classes actually one of the classes is the dominant class and the other two classes are extremely rare.",
                    "label": 0
                },
                {
                    "sent": "So these are the percentages or ratios of data points from this rail class.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we pull data points from all 20 samples together and this is the image that we see.",
                    "label": 0
                },
                {
                    "sent": "So we have this dominant class and we have these two rare clusters.",
                    "label": 0
                },
                {
                    "sent": "And as you can see there are serious deviation across clusters and the goal here is whether or not our algorithm can recover the distributions of the global clusters and.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we run inspire the proposed algorithm on this data and we actually does almost a perfect job in recovering the global cluster distributions here and here.",
                    "label": 0
                },
                {
                    "sent": "You see, the global cluster distribution, the true ones as the solid lines, solid color lines, and then we.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compare this for example with the PM that uses pooled data and this is the type of clusters that we will get.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so this is the type of cluster state we will get an this is HTPM and this is HTML DHTML, little slightly better than the other two techniques, whereas the other two techniques as you can see generates a lot of extraneous clusters, and the main reason being that this cluster distributions are not necessarily Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So when you combine when you consider this generative model, they are not necessarily Gaussian and as a result HD PM and DPM.",
                    "label": 0
                },
                {
                    "sent": "And HTML, they all of the three generates extra.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clusters, so we also apply our technique on a real world application problem.",
                    "label": 0
                },
                {
                    "sent": "So we have this a couple data set, so a couple actually monitors labs involved in HIV and AIDS research and Max in trial.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Around the world.",
                    "label": 0
                },
                {
                    "sent": "So the data set we have contains actually three biological samples exposed to three different stimulation levels and collected by 15 different labs.",
                    "label": 1
                },
                {
                    "sent": "So we have three sources of variation in our data.",
                    "label": 0
                },
                {
                    "sent": "First we have the sample heterogeneity.",
                    "label": 0
                },
                {
                    "sent": "Then we have the different stimulation levels and then we have 15 different labs collecting these measurements.",
                    "label": 1
                },
                {
                    "sent": "So there are 202 samples and 1.1 million data points in our data set.",
                    "label": 1
                },
                {
                    "sent": "Each data point is a cell, so the goal is to recognize cells belonging to two rare classes.",
                    "label": 0
                },
                {
                    "sent": "Ann says not belonging to one of the two rare classes are considered normal and this is the histogram or the ratio of the samples data points we have here.",
                    "label": 0
                },
                {
                    "sent": "98% of the sales come from normal cells which are not very interesting.",
                    "label": 0
                },
                {
                    "sent": "0.5% comes from rail class one and one point.",
                    "label": 0
                },
                {
                    "sent": "3% comes from rail class too.",
                    "label": 0
                },
                {
                    "sent": "So these are the two most interesting clusters that we want.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point, so these are the benchmarks that we use and we are using.",
                    "label": 0
                },
                {
                    "sent": "F1 score is a powerful.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This measure and these are the results that we have, so we can see that methods modeling random effects in general do better.",
                    "label": 0
                },
                {
                    "sent": "So we have HTP emerian aspire doing better than the PM and 8:00 PM.",
                    "label": 0
                },
                {
                    "sent": "And we see that all three techniques modeling each sample by a single DPM.",
                    "label": 1
                },
                {
                    "sent": "Generate excessively large number of clusters.",
                    "label": 0
                },
                {
                    "sent": "As you can see from here, the last column and HT PM almost completely Miss Israel classes, whereas the proposed technique actually does a decent job of recovering this rare cluster distributions.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as a conclusion, we propose a new method for batch clustering, sample subject to random effects.",
                    "label": 1
                },
                {
                    "sent": "Our algorithm is implemented in C++ and the source code is publicly available on my website.",
                    "label": 0
                },
                {
                    "sent": "So an on a data set involving 1.9 million points.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm runs in five hours on an 8 core PC, an part of the algorithm is parallelizable.",
                    "label": 0
                },
                {
                    "sent": "The first part is Paralyzable and the second part is not.",
                    "label": 0
                },
                {
                    "sent": "So we also as a future research direction, for example.",
                    "label": 0
                },
                {
                    "sent": "We can also incorporate labeled information into this process through a process called restricted Gibbs sampler, and we can also use this kind of models.",
                    "label": 0
                },
                {
                    "sent": "For example, for in a problem involving group anomaly detection, which is recently an emerging field in this area.",
                    "label": 0
                },
                {
                    "sent": "So, and this concludes my talk and I will be happy to answer any questions that you might have.",
                    "label": 0
                }
            ]
        }
    }
}