{
    "id": "jrb6oeqqod4rgzcocdjr3brt36b4pkr3",
    "title": "Neural Networks for State Evaluation in General Game Playing",
    "info": {
        "author": [
            "Daniel Michulke, Dresden University of Technology"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Neural Networks"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_michulke_nnse/",
    "segmentation": [
        [
            "Mr Teacher, who is my supervisor at the dress and University of Technology.",
            "Um?"
        ],
        [
            "I will first give an introduction on what General Gameplaying itself is and what the game, description languages, and then I show how we can construct evaluation functions within the context of general game thing.",
            "And finally I will evaluate my approach.",
            "I'm.",
            "The main problem."
        ],
        [
            "In game playing is that is the following and in May 11th in 90."
        ],
        [
            "27 The blue one against Casper, of which was quite a successful artificial intelligence.",
            "But we have to."
        ],
        [
            "Cover and it was achieved primarily because they used massive, massively parallel processing.",
            "The evaluation functions used where handcrafted for just situations.",
            "The opening books were designed by grandmas and the program was even modified between the game.",
            "So what we can."
        ],
        [
            "Learn from this is this was rather programmer intelligence and not artificial intelligence.",
            "OK, it's a hard claim, but just to show the thing and the problem."
        ],
        [
            "In general, not in this case is that the agent was designed like that.",
            "We see that the rules were adapted more or less by a programmer in order to for him to create an agent who finally can play matches.",
            "The idea of."
        ],
        [
            "General Game Playing is the following.",
            "We simply.",
            "Losing the program out of this process and the programmer first develops the agent and then the agent is supposed to handle the rules in a sophisticated way in order to play a match.",
            "To have a good insight on how this spell."
        ],
        [
            "Only works we have to show how the game, what the game description, languages and this is the format in which the rules of the game are written down.",
            "GDL itself is a wide variety of Datalog.",
            "Its rules are written in prefix notation and its variables are preceded by a question like the latter two things we only need 4 for understanding, some other examples which I will use throughout this presentation."
        ],
        [
            "GDL has some keywords as we can see here.",
            "Again is is in this case or in general when using GL game is encoded as a finite state machine and the only things they have in the game is the 1st first rolls, which means for example ingest.",
            "This would be white or black.",
            "Then we have the initial state which would be in chess.",
            "The board set up and then we have legal moves and if we do this, if the players do these legal moves then there is a next statement which states how the next state can be generated.",
            "And finally, if one of the sites is terminal.",
            "Then there is a goal function defined on it, and the goal function generally gives the feedback which would be like in reinforcement learning.",
            "For example the reward.",
            "If it is in GLC find that it's an in an interval between one 100 and zero.",
            "If it's zero, the agent obviously lost, and if it's 100 then the agent one."
        ],
        [
            "Here we have an example of simple game Tic Tac toe, and I suppose everybody knows the game.",
            "We have an initial board state which would be that in this case that they sell with the coordinates 1, one is blank.",
            "We have two rolls actually, I just gave one role here, which is the X player.",
            "The player who makes who makes the Max with the access.",
            "The other one would be the one who makes the hose within Tic Tac toe.",
            "We have another rule which states that if a cell with the coordinate sandwich or variables in this case is blank.",
            "And explore Max this very cell.",
            "Then the cell in the next state has an X in it, which is quite logical.",
            "It's just a formalization of the concepts everybody knows, and the terminal state would be for example, again that there is a line of access and the ex player would be with, for example, only achieve 0 points, which means he lost.",
            "If there is a line of hours within the game.",
            "I will show you how in general this works to construct a game tree, or how the game can then be.",
            "Can can be represented what we see here is."
        ],
        [
            "Board of Tic Tac toe and we see the state represented in GDL.",
            "We see that all the cells are blank except this cell with the coordinates to one which has an O in it and the cell with the coordinates 2 two with which has an X in it.",
            "We"
        ],
        [
            "The legality relation, which is part of the rules and we can see here that if we have to read it like it's prefix notation, so the upper line describes what is the conclusion and the lines below describe what are the premises for that.",
            "So if this rule tells that if a cell is blank.",
            "Then it is legal.",
            "It is a legal move for the player to mark this very cell.",
            "So what we can do?"
        ],
        [
            "We can derive from the state that these are the possible moves."
        ],
        [
            "Mark 112 Mark 3 three without.",
            "With the exception of the two cells that are already marked.",
            "We have those moves and we have the state and we can now use the."
        ],
        [
            "Next relation, which is again part of the rules in order to.",
            "In order to derive the successor states.",
            "This means, for example, if a cell is blank and explain marks that sell, then the next cell then this very cell has an X in it.",
            "Of course there are other next next relations.",
            "For example, the frame axioms that mean, which means for example in this case that if you know is in the cell, then it remains in the cell in the next state.",
            "So what we can do by using these relations step by step is we can create a game tree.",
            "Here we see a game tree which of Tic tac toe with just the example I use and as soon as we have the game tree we can simply use guided search."
        ],
        [
            "Which is a general algorithm for a game playing?",
            "For search we can use minimax or Alpha, better search or something like that.",
            "Or we can use the Max and search and we have to evaluate the States in case it is not possible to reach until terminal State an for this state evaluation we can use Monte Carlo or a self constructed evaluation function which only depends on the rules and this is what we do in our approach."
        ],
        [
            "Constructing evaluation functions.",
            "There are already 2 existing or there are more, but there are two successful systems.",
            "One is called clone player, the others call."
        ],
        [
            "Black player tycoon player.",
            "Simply uses linear combination of weights and features.",
            "It determines some predefined features, some other features it derives from the rules and it determines just some weights, combines them and uses them for state evaluation.",
            "While Flex Player uses a fussy evaluation of the goal function, the goal function would be line X and he's simply pacifies the logical relation and the logical.",
            "Things concepts which are behind this thing the formulas and combines them in a fancy way.",
            "The problems of both, however, are that they are they have a predefined mathematical form.",
            "So there are for sure some games, some state value functions for the for a game which cannot be represented by them and they simply don't use learning, so they ignore experience, which is a bad thing so."
        ],
        [
            "The idea of us is that we position eyes, proposition, lies the goal function which we just go function can be seen here by the goal statement on the left side, we transformed into a neural networks using these CIL to P algorithm and the neural network outputs.",
            "We simply aggregate in one state value."
        ],
        [
            "For propositional is for the first step, the propositional azatian of goals grinding sensation.",
            "We simply have to eliminate all variables, which is actually not that simple.",
            "But one thing is unified unification of deliverables with with the."
        ],
        [
            "The rules as we as we can see here in general, on the upper level there is the goal.",
            "Which describes that ex player wins 100 points if it achieved a line of X is then the level below we have, uh, align is described as either a row or a column, or a diagonal, and I just took what the example of the row that a row is.",
            "That there is a row of some some content, C. In this case, if there are cells with the coordinates M as X coordinate and 123 as Y coordinate.",
            "Which have.",
            "So this is how the rows are defined in this case and we're simply unify line X.",
            "We can use the X, substitute the variable, see with the access and so on.",
            "So the variables he disappears and what else we can do is."
        ],
        [
            "We can simply ground instantiate the Rose, order the idea on the variable M by simply inserting every.",
            "Every.",
            "Every Atom which is possible for the variable, which in this case as it is, coordinates.",
            "Would be 1, two or three.",
            "So in the end we would have something like that.",
            "Which."
        ],
        [
            "Without variables, we can.",
            "However, we can use this if we take this structure, we can as well represent this structure as a proof tree, which would look like that we have the goal we have the lines we have the."
        ],
        [
            "And we have finally the cell statements at the bottom and this structure.",
            "We will now transform to a neural network."
        ],
        [
            "For this we use the CIL to P algorithm, which means connectionist inductive learning and logic programming system and it translates the proof tree.",
            "2A bipolar network with the neuron output in minus one and one and two.",
            "A bipolar activation function and the idea is just to use the effects to translate them to propositional variables and they represent in the end input neurons.",
            "While conjunctions and disjunctions in the proof tree which we saw represented by the ends and yours would be.",
            "Would be a non leaf nodes, so it produces basically a network which is isomorphic to the proof stream.",
            "The idea idea of the CIL tapir algorithm itself is that the neuron output can be can be.",
            "Divided."
        ],
        [
            "Such that a neuron output smaller than a smaller than a Max would revenue with a new one with that output would represent false and the other way round.",
            "If the neuron output exceeds some specified value than the\nRepresents to the value in between, which can be seen here by the white thing.",
            "The white area is more or less forbidden as it would represent a value between true and false, which does not really exist in this scenario, at least not in logic, and it sets at.",
            "The other idea is simply that it sets the weights such the weights W for each of the connections between neurons such that the output is always either true or false always represent either true or false, but never is a value in between.",
            "And these are the final rules for representing a conjunction in this."
        ],
        [
            "Function, The ideas are basically the same.",
            "The only difference is that the disjunction has the user threshold unit for exemplifying disjunctions and."
        ],
        [
            "Junctions.",
            "So what we do then is finally to represent the our goal function as a neural network.",
            "The neural network has in the end an output of minus one and one or in between in the interval of minus one and one this output we normalize finally and aggregated to state value.",
            "However, we have a problem with."
        ],
        [
            "Rhythm, imagine there is a conjunction set consisting of four antecedents and we simply use the CIL to Pegram.",
            "So we set the parameters such that the constraints are satisfied within the algorithm and we even set them in a way which is, which is optimal more or less.",
            "Then we have the following problem.",
            "If none of the conjunction antecedents is true then the output of the neuron representing this very conjunction is minus one.",
            "If one is true, however, it gives the same output value, but we need for example, in connect four we would need a search guidance or a state evaluation which would represent which would prefer a state where at least one of its disks in connect four, for example is is there, then none of its disks are human.",
            "Would at least prefer this situation.",
            "So what we did?"
        ],
        [
            "Is we modified a little desire to be algorithm.",
            "We set the parameter Amen to some empirically or we yeah we with some empirically optimized.",
            "Value and we set the weights to one which is actually a hard set because we ignore the wait condition of sale to be.",
            "In this case, the algorithm loses its correctness.",
            "But we get a higher resolution."
        ],
        [
            "The other problem or what we achieved right now is that we have an evaluation function for a game playing the problem or the other thing which we achieved is that we are now able to train these networks and for this reason we need the training signal generation.",
            "We simply have to put something at the output in order to train the generation we.",
            "For this reason we use simply some matches and the last match for the last match.",
            "We have the reward which is known by the rules and we have the state itself and we can use this pair.",
            "State and output value to to learn the networks.",
            "However we have.",
            "We have several networks, one for each combination of role and the goal value.",
            "So in this case, which we see here where the X player one 100 points, we have to put the network which represents X player 100.",
            "We have to put training signal of 1 and the network player zero.",
            "We have to put the training signal of 1 as well and for the rest minus one."
        ],
        [
            "However, we can enhance this thing because we can use simply the TD Lambda algorithm."
        ],
        [
            "More or less we can use predecessor states of the terminal States and we simply we simply discount the training signal by a parameter Lambda.",
            "And yeah, this is basically the TD Lambda algorithm.",
            "And there is even another trick we can which we can use.",
            "We can prove for example in this case, if this case occurs."
        ],
        [
            "Optimal player for the Explorer leads to us to win 4X player.",
            "In this case we don't even need to discount the."
        ],
        [
            "A state the predecessor state, but we can simply train it as if it was a terminal state, where X player 1.",
            "Because it is provable in this case, that optimal play would lead to a win for X plane.",
            "So we showed that it's possible to train the networks."
        ],
        [
            "Finally, we want to evaluate our approach.",
            "For this we use."
        ],
        [
            "Two games.",
            "The first is pentacle which is a two player game with alternative moves and the player first you have to play with five disks and align wins the game.",
            "It's like Tic tac toe more or less and the the each player first moves disk first.",
            "It puts a disk on the board and then you rotate one of the four quadrants and the quadrants can be distinguished by their background color in this image.",
            "The other thing."
        ],
        [
            "Is 3D tic tac toe?",
            "Which is a generalization of Tic Tac toe, and you simply need 4 instead of three marks in a cell in order to win."
        ],
        [
            "And our tests where we wanted to see whether how good the initial evaluation quality of our neural networks are, how good the improvement through learning is, and how good this algorithm performs in real time.",
            "So we use two test dimensions.",
            "We limited the search depth to the one to see the evaluation quality, and we did not limit the search depth in the other case in order to see how it performs in real time.",
            "And we from sometimes we initialize the network freshly.",
            "So much so, such that the evaluation quality of the initialized network was possible to be measured, while if we use a series of matches and only initialized the network in the beginning of the in the first match, then we could see the improvement through learning."
        ],
        [
            "And we compared our system in the two games against Flex Player, which is 1.",
            "Which one 2006 the GP competition World competition and which was second and best Knowledge base player in 2009?",
            "And yeah, and generally performed very well."
        ],
        [
            "So first the initial evaluation quality.",
            "We simply played 300 matches and at each match at each match we initialize the neural network freshly and we saw that Flex play."
        ],
        [
            "Hello.",
            "One in the beginning in the game of Pentico.",
            "While in Tic Tac toe, it wasn't that easy because always a beginning player one.",
            "So the guy who starts he wins in 3D Tic tac toe, however."
        ],
        [
            "The real time performance was much better.",
            "Or yeah, actually neural could achieve a slide win over those 300 matches, which we.",
            "Which we use for experiments.",
            "The other idea was a douche."
        ],
        [
            "5 * 500 matches and we're simply initialized at the first of the 500 matches.",
            "We initialize the neural networks and we run this this test five times in each of the games, and we used one place."
        ],
        [
            "Once.",
            "And here we could see that one play learning leads.",
            "Yeah, to a significant improvement in the game of Pentago.",
            "And the real time learning is more or less ambivalent as we can see there is some."
        ],
        [
            "How an inconsistency?",
            "And we're not sure.",
            "The learning behaved a little strangely.",
            "Let's say like that.",
            "However, as this is the first paper related to this article, we had some time series about the things that happened while real time.",
            "Yeah, well real time learning."
        ],
        [
            "And what we see here is the moving average of the last 20 matches of the system.",
            "Neuro of the neural network system, and we see the Fed line is the moving the average of the moving average of each of the test series, while at the upper line is the maximum and the lower line is the minimum and we can see that in all in each single test run our system at least achieved 70% win rate over at least 20 matches.",
            "So our problem was we didn't stop learning actually and I think.",
            "In this case, these two cases for the games learning actually should be stopped at some point of time, as it is indicated because this 70% win rate is much more promising than the 55% which we achieved over in the overall experiment."
        ],
        [
            "So what have we done?",
            "We achieved.",
            "We showed that neural networks in general are applicable for state evaluation.",
            "In general game playing.",
            "Of course we used, which indicates that we can use a universal function approximator.",
            "We showed that learning is in generally possible, and we again some flexibility.",
            "For example, we could simply add a neuron to our new to our neural networks and we could connect it to other neurons and then simply see whether this neuron somehow.",
            "Would represent another feature after learning and then we showed that learning might improve performance.",
            "Actually, in our case in the whole test runs, it wasn't the case, but at some point of time this was always the case.",
            "And we showed that it is possible to to combine to some to some extent.",
            "Logic and learning in such a complex domain as general gameplaying, how?"
        ],
        [
            "However, we have some problems.",
            "The one is that we need to ground instantiate the goal condition of the game rules and this is not always feasible.",
            "Becausw, yeah, actually it's free.",
            "Propositional eyes such a goal condition in the worst case the the size increases exponentially, then the initialized networks are still are incorrect because we remove the weight constraint simply, and the CIL 2P approach is generally possible.",
            "But it has a very small resolution.",
            "If we stayed too.",
            "If we stick to correctness then we have a very small resolution.",
            "And."
        ],
        [
            "Future work regarding this is that we use a correct neural network with higher resolution and this work is almost done and I think it will be submitted like in the next two months.",
            "We will optimize the parameters as all the parameters were not really optimized for it was more like a check whether it is generally possible then to achieve really high win rates or something like that and we will enhance the system to a computer general game player."
        ],
        [
            "And I thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mr Teacher, who is my supervisor at the dress and University of Technology.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will first give an introduction on what General Gameplaying itself is and what the game, description languages, and then I show how we can construct evaluation functions within the context of general game thing.",
                    "label": 1
                },
                {
                    "sent": "And finally I will evaluate my approach.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "The main problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In game playing is that is the following and in May 11th in 90.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "27 The blue one against Casper, of which was quite a successful artificial intelligence.",
                    "label": 0
                },
                {
                    "sent": "But we have to.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cover and it was achieved primarily because they used massive, massively parallel processing.",
                    "label": 1
                },
                {
                    "sent": "The evaluation functions used where handcrafted for just situations.",
                    "label": 0
                },
                {
                    "sent": "The opening books were designed by grandmas and the program was even modified between the game.",
                    "label": 0
                },
                {
                    "sent": "So what we can.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learn from this is this was rather programmer intelligence and not artificial intelligence.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a hard claim, but just to show the thing and the problem.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In general, not in this case is that the agent was designed like that.",
                    "label": 0
                },
                {
                    "sent": "We see that the rules were adapted more or less by a programmer in order to for him to create an agent who finally can play matches.",
                    "label": 0
                },
                {
                    "sent": "The idea of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "General Game Playing is the following.",
                    "label": 1
                },
                {
                    "sent": "We simply.",
                    "label": 0
                },
                {
                    "sent": "Losing the program out of this process and the programmer first develops the agent and then the agent is supposed to handle the rules in a sophisticated way in order to play a match.",
                    "label": 0
                },
                {
                    "sent": "To have a good insight on how this spell.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Only works we have to show how the game, what the game description, languages and this is the format in which the rules of the game are written down.",
                    "label": 0
                },
                {
                    "sent": "GDL itself is a wide variety of Datalog.",
                    "label": 1
                },
                {
                    "sent": "Its rules are written in prefix notation and its variables are preceded by a question like the latter two things we only need 4 for understanding, some other examples which I will use throughout this presentation.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "GDL has some keywords as we can see here.",
                    "label": 0
                },
                {
                    "sent": "Again is is in this case or in general when using GL game is encoded as a finite state machine and the only things they have in the game is the 1st first rolls, which means for example ingest.",
                    "label": 0
                },
                {
                    "sent": "This would be white or black.",
                    "label": 0
                },
                {
                    "sent": "Then we have the initial state which would be in chess.",
                    "label": 0
                },
                {
                    "sent": "The board set up and then we have legal moves and if we do this, if the players do these legal moves then there is a next statement which states how the next state can be generated.",
                    "label": 0
                },
                {
                    "sent": "And finally, if one of the sites is terminal.",
                    "label": 0
                },
                {
                    "sent": "Then there is a goal function defined on it, and the goal function generally gives the feedback which would be like in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "For example the reward.",
                    "label": 0
                },
                {
                    "sent": "If it is in GLC find that it's an in an interval between one 100 and zero.",
                    "label": 0
                },
                {
                    "sent": "If it's zero, the agent obviously lost, and if it's 100 then the agent one.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we have an example of simple game Tic Tac toe, and I suppose everybody knows the game.",
                    "label": 1
                },
                {
                    "sent": "We have an initial board state which would be that in this case that they sell with the coordinates 1, one is blank.",
                    "label": 0
                },
                {
                    "sent": "We have two rolls actually, I just gave one role here, which is the X player.",
                    "label": 0
                },
                {
                    "sent": "The player who makes who makes the Max with the access.",
                    "label": 0
                },
                {
                    "sent": "The other one would be the one who makes the hose within Tic Tac toe.",
                    "label": 0
                },
                {
                    "sent": "We have another rule which states that if a cell with the coordinate sandwich or variables in this case is blank.",
                    "label": 0
                },
                {
                    "sent": "And explore Max this very cell.",
                    "label": 0
                },
                {
                    "sent": "Then the cell in the next state has an X in it, which is quite logical.",
                    "label": 0
                },
                {
                    "sent": "It's just a formalization of the concepts everybody knows, and the terminal state would be for example, again that there is a line of access and the ex player would be with, for example, only achieve 0 points, which means he lost.",
                    "label": 0
                },
                {
                    "sent": "If there is a line of hours within the game.",
                    "label": 0
                },
                {
                    "sent": "I will show you how in general this works to construct a game tree, or how the game can then be.",
                    "label": 0
                },
                {
                    "sent": "Can can be represented what we see here is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Board of Tic Tac toe and we see the state represented in GDL.",
                    "label": 0
                },
                {
                    "sent": "We see that all the cells are blank except this cell with the coordinates to one which has an O in it and the cell with the coordinates 2 two with which has an X in it.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The legality relation, which is part of the rules and we can see here that if we have to read it like it's prefix notation, so the upper line describes what is the conclusion and the lines below describe what are the premises for that.",
                    "label": 0
                },
                {
                    "sent": "So if this rule tells that if a cell is blank.",
                    "label": 0
                },
                {
                    "sent": "Then it is legal.",
                    "label": 0
                },
                {
                    "sent": "It is a legal move for the player to mark this very cell.",
                    "label": 0
                },
                {
                    "sent": "So what we can do?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can derive from the state that these are the possible moves.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mark 112 Mark 3 three without.",
                    "label": 0
                },
                {
                    "sent": "With the exception of the two cells that are already marked.",
                    "label": 0
                },
                {
                    "sent": "We have those moves and we have the state and we can now use the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next relation, which is again part of the rules in order to.",
                    "label": 0
                },
                {
                    "sent": "In order to derive the successor states.",
                    "label": 0
                },
                {
                    "sent": "This means, for example, if a cell is blank and explain marks that sell, then the next cell then this very cell has an X in it.",
                    "label": 0
                },
                {
                    "sent": "Of course there are other next next relations.",
                    "label": 0
                },
                {
                    "sent": "For example, the frame axioms that mean, which means for example in this case that if you know is in the cell, then it remains in the cell in the next state.",
                    "label": 0
                },
                {
                    "sent": "So what we can do by using these relations step by step is we can create a game tree.",
                    "label": 0
                },
                {
                    "sent": "Here we see a game tree which of Tic tac toe with just the example I use and as soon as we have the game tree we can simply use guided search.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is a general algorithm for a game playing?",
                    "label": 1
                },
                {
                    "sent": "For search we can use minimax or Alpha, better search or something like that.",
                    "label": 0
                },
                {
                    "sent": "Or we can use the Max and search and we have to evaluate the States in case it is not possible to reach until terminal State an for this state evaluation we can use Monte Carlo or a self constructed evaluation function which only depends on the rules and this is what we do in our approach.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Constructing evaluation functions.",
                    "label": 0
                },
                {
                    "sent": "There are already 2 existing or there are more, but there are two successful systems.",
                    "label": 0
                },
                {
                    "sent": "One is called clone player, the others call.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Black player tycoon player.",
                    "label": 0
                },
                {
                    "sent": "Simply uses linear combination of weights and features.",
                    "label": 1
                },
                {
                    "sent": "It determines some predefined features, some other features it derives from the rules and it determines just some weights, combines them and uses them for state evaluation.",
                    "label": 0
                },
                {
                    "sent": "While Flex Player uses a fussy evaluation of the goal function, the goal function would be line X and he's simply pacifies the logical relation and the logical.",
                    "label": 0
                },
                {
                    "sent": "Things concepts which are behind this thing the formulas and combines them in a fancy way.",
                    "label": 1
                },
                {
                    "sent": "The problems of both, however, are that they are they have a predefined mathematical form.",
                    "label": 0
                },
                {
                    "sent": "So there are for sure some games, some state value functions for the for a game which cannot be represented by them and they simply don't use learning, so they ignore experience, which is a bad thing so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea of us is that we position eyes, proposition, lies the goal function which we just go function can be seen here by the goal statement on the left side, we transformed into a neural networks using these CIL to P algorithm and the neural network outputs.",
                    "label": 0
                },
                {
                    "sent": "We simply aggregate in one state value.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For propositional is for the first step, the propositional azatian of goals grinding sensation.",
                    "label": 0
                },
                {
                    "sent": "We simply have to eliminate all variables, which is actually not that simple.",
                    "label": 0
                },
                {
                    "sent": "But one thing is unified unification of deliverables with with the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The rules as we as we can see here in general, on the upper level there is the goal.",
                    "label": 0
                },
                {
                    "sent": "Which describes that ex player wins 100 points if it achieved a line of X is then the level below we have, uh, align is described as either a row or a column, or a diagonal, and I just took what the example of the row that a row is.",
                    "label": 0
                },
                {
                    "sent": "That there is a row of some some content, C. In this case, if there are cells with the coordinates M as X coordinate and 123 as Y coordinate.",
                    "label": 0
                },
                {
                    "sent": "Which have.",
                    "label": 0
                },
                {
                    "sent": "So this is how the rows are defined in this case and we're simply unify line X.",
                    "label": 0
                },
                {
                    "sent": "We can use the X, substitute the variable, see with the access and so on.",
                    "label": 0
                },
                {
                    "sent": "So the variables he disappears and what else we can do is.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can simply ground instantiate the Rose, order the idea on the variable M by simply inserting every.",
                    "label": 0
                },
                {
                    "sent": "Every.",
                    "label": 0
                },
                {
                    "sent": "Every Atom which is possible for the variable, which in this case as it is, coordinates.",
                    "label": 0
                },
                {
                    "sent": "Would be 1, two or three.",
                    "label": 0
                },
                {
                    "sent": "So in the end we would have something like that.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Without variables, we can.",
                    "label": 0
                },
                {
                    "sent": "However, we can use this if we take this structure, we can as well represent this structure as a proof tree, which would look like that we have the goal we have the lines we have the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have finally the cell statements at the bottom and this structure.",
                    "label": 0
                },
                {
                    "sent": "We will now transform to a neural network.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For this we use the CIL to P algorithm, which means connectionist inductive learning and logic programming system and it translates the proof tree.",
                    "label": 1
                },
                {
                    "sent": "2A bipolar network with the neuron output in minus one and one and two.",
                    "label": 0
                },
                {
                    "sent": "A bipolar activation function and the idea is just to use the effects to translate them to propositional variables and they represent in the end input neurons.",
                    "label": 0
                },
                {
                    "sent": "While conjunctions and disjunctions in the proof tree which we saw represented by the ends and yours would be.",
                    "label": 0
                },
                {
                    "sent": "Would be a non leaf nodes, so it produces basically a network which is isomorphic to the proof stream.",
                    "label": 0
                },
                {
                    "sent": "The idea idea of the CIL tapir algorithm itself is that the neuron output can be can be.",
                    "label": 0
                },
                {
                    "sent": "Divided.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such that a neuron output smaller than a smaller than a Max would revenue with a new one with that output would represent false and the other way round.",
                    "label": 0
                },
                {
                    "sent": "If the neuron output exceeds some specified value than the\nRepresents to the value in between, which can be seen here by the white thing.",
                    "label": 0
                },
                {
                    "sent": "The white area is more or less forbidden as it would represent a value between true and false, which does not really exist in this scenario, at least not in logic, and it sets at.",
                    "label": 0
                },
                {
                    "sent": "The other idea is simply that it sets the weights such the weights W for each of the connections between neurons such that the output is always either true or false always represent either true or false, but never is a value in between.",
                    "label": 0
                },
                {
                    "sent": "And these are the final rules for representing a conjunction in this.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function, The ideas are basically the same.",
                    "label": 0
                },
                {
                    "sent": "The only difference is that the disjunction has the user threshold unit for exemplifying disjunctions and.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Junctions.",
                    "label": 0
                },
                {
                    "sent": "So what we do then is finally to represent the our goal function as a neural network.",
                    "label": 0
                },
                {
                    "sent": "The neural network has in the end an output of minus one and one or in between in the interval of minus one and one this output we normalize finally and aggregated to state value.",
                    "label": 0
                },
                {
                    "sent": "However, we have a problem with.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rhythm, imagine there is a conjunction set consisting of four antecedents and we simply use the CIL to Pegram.",
                    "label": 0
                },
                {
                    "sent": "So we set the parameters such that the constraints are satisfied within the algorithm and we even set them in a way which is, which is optimal more or less.",
                    "label": 0
                },
                {
                    "sent": "Then we have the following problem.",
                    "label": 0
                },
                {
                    "sent": "If none of the conjunction antecedents is true then the output of the neuron representing this very conjunction is minus one.",
                    "label": 0
                },
                {
                    "sent": "If one is true, however, it gives the same output value, but we need for example, in connect four we would need a search guidance or a state evaluation which would represent which would prefer a state where at least one of its disks in connect four, for example is is there, then none of its disks are human.",
                    "label": 0
                },
                {
                    "sent": "Would at least prefer this situation.",
                    "label": 0
                },
                {
                    "sent": "So what we did?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is we modified a little desire to be algorithm.",
                    "label": 0
                },
                {
                    "sent": "We set the parameter Amen to some empirically or we yeah we with some empirically optimized.",
                    "label": 0
                },
                {
                    "sent": "Value and we set the weights to one which is actually a hard set because we ignore the wait condition of sale to be.",
                    "label": 0
                },
                {
                    "sent": "In this case, the algorithm loses its correctness.",
                    "label": 0
                },
                {
                    "sent": "But we get a higher resolution.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other problem or what we achieved right now is that we have an evaluation function for a game playing the problem or the other thing which we achieved is that we are now able to train these networks and for this reason we need the training signal generation.",
                    "label": 1
                },
                {
                    "sent": "We simply have to put something at the output in order to train the generation we.",
                    "label": 0
                },
                {
                    "sent": "For this reason we use simply some matches and the last match for the last match.",
                    "label": 0
                },
                {
                    "sent": "We have the reward which is known by the rules and we have the state itself and we can use this pair.",
                    "label": 0
                },
                {
                    "sent": "State and output value to to learn the networks.",
                    "label": 0
                },
                {
                    "sent": "However we have.",
                    "label": 0
                },
                {
                    "sent": "We have several networks, one for each combination of role and the goal value.",
                    "label": 0
                },
                {
                    "sent": "So in this case, which we see here where the X player one 100 points, we have to put the network which represents X player 100.",
                    "label": 1
                },
                {
                    "sent": "We have to put training signal of 1 and the network player zero.",
                    "label": 0
                },
                {
                    "sent": "We have to put the training signal of 1 as well and for the rest minus one.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, we can enhance this thing because we can use simply the TD Lambda algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More or less we can use predecessor states of the terminal States and we simply we simply discount the training signal by a parameter Lambda.",
                    "label": 0
                },
                {
                    "sent": "And yeah, this is basically the TD Lambda algorithm.",
                    "label": 0
                },
                {
                    "sent": "And there is even another trick we can which we can use.",
                    "label": 0
                },
                {
                    "sent": "We can prove for example in this case, if this case occurs.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimal player for the Explorer leads to us to win 4X player.",
                    "label": 0
                },
                {
                    "sent": "In this case we don't even need to discount the.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A state the predecessor state, but we can simply train it as if it was a terminal state, where X player 1.",
                    "label": 0
                },
                {
                    "sent": "Because it is provable in this case, that optimal play would lead to a win for X plane.",
                    "label": 0
                },
                {
                    "sent": "So we showed that it's possible to train the networks.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, we want to evaluate our approach.",
                    "label": 0
                },
                {
                    "sent": "For this we use.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two games.",
                    "label": 0
                },
                {
                    "sent": "The first is pentacle which is a two player game with alternative moves and the player first you have to play with five disks and align wins the game.",
                    "label": 0
                },
                {
                    "sent": "It's like Tic tac toe more or less and the the each player first moves disk first.",
                    "label": 0
                },
                {
                    "sent": "It puts a disk on the board and then you rotate one of the four quadrants and the quadrants can be distinguished by their background color in this image.",
                    "label": 1
                },
                {
                    "sent": "The other thing.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is 3D tic tac toe?",
                    "label": 0
                },
                {
                    "sent": "Which is a generalization of Tic Tac toe, and you simply need 4 instead of three marks in a cell in order to win.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And our tests where we wanted to see whether how good the initial evaluation quality of our neural networks are, how good the improvement through learning is, and how good this algorithm performs in real time.",
                    "label": 1
                },
                {
                    "sent": "So we use two test dimensions.",
                    "label": 0
                },
                {
                    "sent": "We limited the search depth to the one to see the evaluation quality, and we did not limit the search depth in the other case in order to see how it performs in real time.",
                    "label": 0
                },
                {
                    "sent": "And we from sometimes we initialize the network freshly.",
                    "label": 1
                },
                {
                    "sent": "So much so, such that the evaluation quality of the initialized network was possible to be measured, while if we use a series of matches and only initialized the network in the beginning of the in the first match, then we could see the improvement through learning.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we compared our system in the two games against Flex Player, which is 1.",
                    "label": 0
                },
                {
                    "sent": "Which one 2006 the GP competition World competition and which was second and best Knowledge base player in 2009?",
                    "label": 0
                },
                {
                    "sent": "And yeah, and generally performed very well.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first the initial evaluation quality.",
                    "label": 0
                },
                {
                    "sent": "We simply played 300 matches and at each match at each match we initialize the neural network freshly and we saw that Flex play.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "One in the beginning in the game of Pentico.",
                    "label": 0
                },
                {
                    "sent": "While in Tic Tac toe, it wasn't that easy because always a beginning player one.",
                    "label": 0
                },
                {
                    "sent": "So the guy who starts he wins in 3D Tic tac toe, however.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The real time performance was much better.",
                    "label": 0
                },
                {
                    "sent": "Or yeah, actually neural could achieve a slide win over those 300 matches, which we.",
                    "label": 0
                },
                {
                    "sent": "Which we use for experiments.",
                    "label": 0
                },
                {
                    "sent": "The other idea was a douche.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "5 * 500 matches and we're simply initialized at the first of the 500 matches.",
                    "label": 0
                },
                {
                    "sent": "We initialize the neural networks and we run this this test five times in each of the games, and we used one place.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once.",
                    "label": 0
                },
                {
                    "sent": "And here we could see that one play learning leads.",
                    "label": 1
                },
                {
                    "sent": "Yeah, to a significant improvement in the game of Pentago.",
                    "label": 1
                },
                {
                    "sent": "And the real time learning is more or less ambivalent as we can see there is some.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How an inconsistency?",
                    "label": 0
                },
                {
                    "sent": "And we're not sure.",
                    "label": 0
                },
                {
                    "sent": "The learning behaved a little strangely.",
                    "label": 0
                },
                {
                    "sent": "Let's say like that.",
                    "label": 0
                },
                {
                    "sent": "However, as this is the first paper related to this article, we had some time series about the things that happened while real time.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well real time learning.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we see here is the moving average of the last 20 matches of the system.",
                    "label": 1
                },
                {
                    "sent": "Neuro of the neural network system, and we see the Fed line is the moving the average of the moving average of each of the test series, while at the upper line is the maximum and the lower line is the minimum and we can see that in all in each single test run our system at least achieved 70% win rate over at least 20 matches.",
                    "label": 0
                },
                {
                    "sent": "So our problem was we didn't stop learning actually and I think.",
                    "label": 0
                },
                {
                    "sent": "In this case, these two cases for the games learning actually should be stopped at some point of time, as it is indicated because this 70% win rate is much more promising than the 55% which we achieved over in the overall experiment.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what have we done?",
                    "label": 0
                },
                {
                    "sent": "We achieved.",
                    "label": 0
                },
                {
                    "sent": "We showed that neural networks in general are applicable for state evaluation.",
                    "label": 1
                },
                {
                    "sent": "In general game playing.",
                    "label": 1
                },
                {
                    "sent": "Of course we used, which indicates that we can use a universal function approximator.",
                    "label": 0
                },
                {
                    "sent": "We showed that learning is in generally possible, and we again some flexibility.",
                    "label": 0
                },
                {
                    "sent": "For example, we could simply add a neuron to our new to our neural networks and we could connect it to other neurons and then simply see whether this neuron somehow.",
                    "label": 1
                },
                {
                    "sent": "Would represent another feature after learning and then we showed that learning might improve performance.",
                    "label": 0
                },
                {
                    "sent": "Actually, in our case in the whole test runs, it wasn't the case, but at some point of time this was always the case.",
                    "label": 0
                },
                {
                    "sent": "And we showed that it is possible to to combine to some to some extent.",
                    "label": 0
                },
                {
                    "sent": "Logic and learning in such a complex domain as general gameplaying, how?",
                    "label": 1
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, we have some problems.",
                    "label": 0
                },
                {
                    "sent": "The one is that we need to ground instantiate the goal condition of the game rules and this is not always feasible.",
                    "label": 1
                },
                {
                    "sent": "Becausw, yeah, actually it's free.",
                    "label": 0
                },
                {
                    "sent": "Propositional eyes such a goal condition in the worst case the the size increases exponentially, then the initialized networks are still are incorrect because we remove the weight constraint simply, and the CIL 2P approach is generally possible.",
                    "label": 1
                },
                {
                    "sent": "But it has a very small resolution.",
                    "label": 0
                },
                {
                    "sent": "If we stayed too.",
                    "label": 0
                },
                {
                    "sent": "If we stick to correctness then we have a very small resolution.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Future work regarding this is that we use a correct neural network with higher resolution and this work is almost done and I think it will be submitted like in the next two months.",
                    "label": 0
                },
                {
                    "sent": "We will optimize the parameters as all the parameters were not really optimized for it was more like a check whether it is generally possible then to achieve really high win rates or something like that and we will enhance the system to a computer general game player.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}