{
    "id": "7do4avawqw5rkwygtetinv4qrsai7v63",
    "title": "On Max-Margin Markov Networks in Hierarchical Document Classification",
    "info": {
        "author": [
            "Juho Rousu, Department of Computer Science, University of Helsinki"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/iiia06_rousu_mmmnh/",
    "segmentation": [
        [
            "First pictures we have.",
            "Is already interesting topic practical document classification.",
            "OK, so my name is you are also and I'm talking about much Martin Marco Networks in hierarchical document classification and this is joint work with Greg Sanders under Jet, Marc Antonio Taylor and this mostly based on JML article that is about to come out pretty soon.",
            "OK."
        ],
        [
            "So, so this is the specific problem we talk about, so we have some some classification hierarchy.",
            "Some topics and.",
            "And then some news articles or some other documents that need to be classified according to this hierarchy.",
            "We assumed that the classifications are kind of multi label nature so that we have several categories in this hierarchy that are relevant to this each document.",
            "So we have this kind of union of partial parts in this whole hierarchy that is part of the document classification.",
            "So this is the.",
            "This is the setup.",
            "Yes.",
            "To pick out which labels so that within your, why is the plus 1 -- 1 if you have binary variables representing which labels yes, yes.",
            "Plus means plus equals green.",
            "Yes, and this these are minus they are not part of the label OK?"
        ],
        [
            "Right OK so.",
            "Obviously this is a question has been studied by many people over the years and there are many strategies for learning this kind of in this kind of settings, and I guess the most obvious.",
            "Ways to just flatten the hierarchy, so you just consider it snowed into hierarchy.",
            "Separately, try to build a classifier for the node and then somehow combined compound predictions.",
            "Then to get this multi level label prediction.",
            "OK so this is of course competition are relatively inexpensive, but it doesn't make any use of the dependencies between between the labels that are quite obvious.",
            "If your hierarchy represent.",
            "Kind of world in any meaningful way.",
            "I do a little bit better if you train the models.",
            "Hierarchical is so that you start with root node of the hierarchy and train a classifier there and then kind of split the data based on the membership.",
            "Into subtrees and continue this kind of.",
            "Dividing today time and training the training models.",
            "OK, so this kind of approach of obviously picks up some of the dependencies between the micro labels, but the training data fragments towards the leaves, so you will get less and less data data in the leaves.",
            "So this estimation becomes reliable, less reliable, and also it's hard to say what is the loss function that is more or less trained in terms with so, so it's not not.",
            "Kind of till tomorrow.",
            "Where to do?",
            "And so we want to do something better."
        ],
        [
            "OK, so so we use use this kind of conditional random field model that you have seen a couple of times already.",
            "Here during this workshop, so we have.",
            "So we took the edges of the hierarchy and assign some potential function for each edge that is an exponential where we have the weight vector for the edge.",
            "An then joint fits a map for this document X and this labeling of that edge.",
            "So basically.",
            "We have kind of context dependent feature feature Maps there there and we learn the weights for these features.",
            "OK, so then we have this this partition function there to worry about.",
            "As well.",
            "Right, so this is this is the model."
        ],
        [
            "Let's take a look at the feature vectors.",
            "Obviously you can contract them in many ways, but this is turns out to be quite useful way to do it.",
            "You construct this feature vector of blocks that are specific to.",
            "Edge and labeling of an edge so you have heaven.",
            "Some feature vector for your documents that can be back of words substring spectrum or something else.",
            "And you prefix that with an indicator.",
            "So whether it whether your document belongs to certain.",
            "Edge or a certain node in a hierarchy, or certain pair of nodes in your hierarchy.",
            "OK, So what it does is it follows us to learn.",
            "Import consist of this these features in different context, so you can pick up different features in different different nodes of the hierarchy or edges of the hierarchy.",
            "This special structure, attached to repeat this this this feature vector for this document that can be utilized in computation.",
            "So we can we can do things in less memory than if you have some kind of huge.",
            "Huge feature vector folder, whole structure.",
            "So this is illustration how this this feature vectors are composed of the whole.",
            "This is the total.",
            "Feature vector folder for the whole hierarchy.",
            "And it's composed of edge blocks.",
            "Like this for each edge and each of the edge blocks further decomposes into.",
            "Blocks based on edge labeling.",
            "What this indicator does it says.",
            "Turns on the feature vector in the correct, correct context, so it's quieter.",
            "Kind of interesting.",
            "Structure."
        ],
        [
            "K so then of course, another important thing to consider is the loss function.",
            "But So what kind of loss function used be used when you're training this kind of model?",
            "So you have this multi label?",
            "That's true multi label for your documentation and some predict that one and you should think about what is the what is the law that you assigned to it.",
            "So of course you can use Cheryl one loss where you just say that if you're multi label vectors differ, you incur a penalty of 1.",
            "So that's the kind of loss that is frequently used in this color classification.",
            "Tasks of Alias is not very good because you don't make any difference between.",
            "Slight mistakes and very severe mistakes in your prediction, so that's not not not optimal.",
            "The other end of the spectrum is hamming loss, where you look at your multi label vectors as a whole and just count how many were incorrect.",
            "So that obviously takes into account little bit of severity of the mistakes in your predictions, but but it doesn't do it still doesn't know anything about the hierarchy, so it doesn't know that actually some nodes have some specific positions.",
            "Some notice the root of the hierarchy, and some some are leaves.",
            "So maybe we want to.",
            "Take that into account in our loss function and there are few suggestions for this kind of losses.",
            "One is last week called Path Loss, then suggested by Niccolo Kitsap, Jahnke and Co authors.",
            "Where we penalized first mistake along a path in the hierarchy.",
            "So the idea is that.",
            "OK, so.",
            "If there are mistake in some prediction path path, so maybe.",
            "We shouldn't penalize any mistake that we still make after, so if you get the parent incorrect, maybe we shouldn't penalize the prediction in the child because they already apparent was incorrect.",
            "So this kind of thinking leads to leads to this kind of loss that you only penalize the first mistake so you look how far you're correct and when you make a mistake, you incur a penalty.",
            "After that, you don't incur any more penalties.",
            "And you can have some kind of waiting too.",
            "Rebate to penalties based on where into hierarchy you are making the mistake.",
            "This is edge losses.",
            "Some simplified worsen it.",
            "You just look at one edge at a time and you say that, OK, you penalize the child if the.",
            "If the parent was the correct but OK, so it doesn't take account the whole path.",
            "The good thing about this is that leads to.",
            "More tractable models in optimization of obviously is kind of an approximation of the part loss, so it's not.",
            "Ask hinova elegant.",
            "Right so this guy."
        ],
        [
            "Of components we can use an so this is the learning framework, and Fernando Pereira already told you about this kind of much marching approaches.",
            "Pro structures trucks are outputs.",
            "Night system, kind of.",
            "Rake up it here so the idea is that we take the correct multi label for of the document and we try to separate this correct multi label from the incorrect ones by a large margin.",
            "So so kind of the chair metrical idea is looks like this.",
            "We have a way to vector and we have to feed sore joint fits or Maps.",
            "Feature vectors of documents and multi level of pears and we say that OK, this is the correct one and we want to have some margin to this incorrect ones and the margin should be the larger to worst prediction is so the more losses labels have the further away they should be from the correct one.",
            "And then we need to allow some slack cause because otherwise we won't be able to find find the model, so it's not not really.",
            "Possible or West separate things with hard margin?"
        ],
        [
            "OK, so this is the.",
            "Model and this is.",
            "This is how this equations look, so we have this kind of.",
            "Normalization regularization term.",
            "Constraints about the margins we need to satisfy this constraint, and there is some slack that we don't allow there and do our dual looks like this.",
            "So we have a linear term where there is this loss loss.",
            "A serviceable quadratic term where we have a joint kernel for for this document.",
            "Multi label pairs.",
            "A simple box constraints for for this.",
            "And all variables.",
            "But the problem is this is hopelessly intractable, tractable.",
            "Model because you have exponential number of primal constraints.",
            "And also do all variables so.",
            "You cannot do any any realistic sized datasets with this kind of quadratic programs, so.",
            "Conflict out of order question, but there are many ways how you can make these models tractable so."
        ],
        [
            "Right and we have followed this.",
            "This idea proposed by pen task or that they marginalized the problem.",
            "The idea is that instead of looking at this.",
            "Original dual variables which we have exponential number of.",
            "We look at the marginals edge marginals of this this this.",
            "Variables.",
            "And we have a polynomial number of those.",
            "What we need?",
            "We need this loss function to be composable by the edges, so we need to have a loss function that that decomposes.",
            "Like this so.",
            "Of the loss functions I showed you Hamming loss on edge loss or or.",
            "Among the ones who can use.",
            "OK, and we also need the kernels that compost by the edges and this directly falls out from the fitzer fitzer representations I showed you so because we have blocks in the feature vectors based on edges.",
            "The Colonel obviously decomposes as well, so this kind of."
        ],
        [
            "Ingredients give us marginalized problem where we have the form is quite similar to the original develop, but we have this sums over the edges and.",
            "Otherwise, The thing is more or less the same.",
            "We still have box constraints.",
            "I wrote them in matrix form.",
            "And then there is additional set of constraints that need to be set to.",
            "Make sure that our marginalized problem corresponds to the original dual and these are kind of constraints that say that the edges need to somehow agree, agree about the labelings.",
            "So OK, so this is this problem has a polynomial size, so it's.",
            "This kind of complexity is you need to have.",
            "But this still is way too much for any kind of QP solvers to be used on this problem.",
            "So for example, we have a data set with little over 1000 examples and little less than 200.",
            "Michael labels and if you really wrote this problem, it would consume something like 10 gigabytes of memory.",
            "So even though it's polynomial size and actually low panel polynomial size, it's too big.",
            "So we need to do something for the problem."
        ],
        [
            "Obviously you want to decompose tomorrow somehow, so.",
            "So, but the problem is that it's not trivial to decompose this objective decomposes by the edges, but constraints don't.",
            "So the this consistency constraints, there are tight together, so you cannot decompose fully by the edges.",
            "Similarly, you cannot decompose fully paid examples because the kernel obviously ties examples together.",
            "What you can do you can you can look at the gradient of the of the objective.",
            "So because here you don't have example interactions, so and this is what we have actually done.",
            "So so we have used them creating test method.",
            "Polo training this in decomposed form."
        ],
        [
            "And.",
            "Specific algorithm we use this conditional gradient method by persicus.",
            "So it conducts iterative gradient search, indefeasible set and.",
            "Update directions are kind of highest fees with this feasible points, assuming the current trading and this is a linear program that you that you need to solve.",
            "OK, so this allows us this decomposed."
        ],
        [
            "Program and very quickly I saw you address about what this algorithm is about, so.",
            "Look at look at the current gradient."
        ],
        [
            "Book assuming this gradient is constant, so we make linear approximation of this quadratic object.",
            "If we look what is the best point.",
            "So this is the this corner is the best point.",
            "The computer."
        ],
        [
            "Saddlepoint how long this line.",
            "Continue from that point and iterate."
        ],
        [
            "This kind of."
        ],
        [
            "Search"
        ],
        [
            "To Dakota water.",
            "Credit line service.",
            "Yes yes yes.",
            "OK, so this is the kind of approach, but is what is being done."
        ],
        [
            "OK, to solve this this this up to help direction so you can use a linear programming solver to solve this problem, But that actually will not be the most efficient way you can do.",
            "Because we can actually utilize this hierarchical structure to solve this problem much, much faster.",
            "And the idea is that this vertices of this visible set actually.",
            "Correspond to multi labels.",
            "What we need to find this best corner is to find ways to save us to find the best multi label.",
            "So we can use inference over this hierarchy, so dynamic programming over the hierarchy in linear time we can find this best best corner, so it's very much faster than using linear programming solver."
        ],
        [
            "Run.",
            "OK, very quickly I go to so you some experiments we have right service on Wiper Alpha or datasets so.",
            "Why qualify Subpattern data set set rate.",
            "With little over 1000 examples, 200 MW destroyed.",
            "There's this secret family, so there's 34 Michael labels, and it trained with.",
            "200 two 1500 documents.",
            "The compared our article returns that we called HM Cube, so hierarchal Max margin Markov compared to flat SVM hierarchical trends, SVM and and hierarchical regularised least squares algorithm by Niccolo Casas Bianchi.",
            "Everything was implemented it in Matlab, well except Flat SV and that was SVM, light and test user Paci."
        ],
        [
            "Basic computer.",
            "Optimize things.",
            "OK, so this is something about optimization efficiency, so this kind of data.",
            "Can we compare this?",
            "What happens when we use linear programming versus finding this to use this up the directions versus finding them by dynamic programming inference then so you can see that by dynamic programming it's like.",
            "In 15 minutes, while they are pretty close to the optimum and and this linear program is much much lower, so it takes hours to reach.",
            "Kind of reasonable.",
            "Region, but anyway we can tractably learn this kind of this kind of data set."
        ],
        [
            "OK, something about prediction accuracy.",
            "So this is level wise F1.",
            "Statistiques OB day.",
            "Look at each level of the hierarchy separately and look at micro label predictions.",
            "Microwave paper prices on microarray Apple recall and compute F1 from it and so so the depth of the hierarchical scores this way.",
            "OK, so I guess one thing you can notice from this.",
            "Maybe this is clear this why for data obviously that one goes down.",
            "So Reuters is not not that clear because it's very degenerate hierarchy.",
            "So this effect is not clear but your F1 goes down when we go below to go down to levels.",
            "But in the higher levels, all of the methods are pretty even, but when you go deeper, you can see that OK, flat SVM starts to lose out, and that's because the recall drops very dramatically, so flat SVM is not able to recall deep nodes.",
            "Position is good for flat SVM all the time, and all this hierarchical variance they do.",
            "Much better on its ends at this much margin markup is the.",
            "Seems to do pretty well in the leaves compared to the other hierarchal training medicine.",
            "We don't actually know why, but but obviously they're kind of take the results as they as they.",
            "So to us so.",
            "Right, so this is so we do reasonably reasonably well it."
        ],
        [
            "Wait a minute.",
            "Something about the scalability.",
            "OK, so we can do something like 10,005 thousand make microwave dental 1000 examples 5000 microfibers.",
            "But so if you kind of multiply both of those figures by 10 and compute how much you would use memory, so we could take up the whole hard disk with this kind of.",
            "Formalize, and so it might be that we need to think of something else to scale up to really large large datasets and hierarchies.",
            "Mostly this is because of the kernel that takes quadratic size studio in the number of examples.",
            "There's some some possibilities to improve things so obvious you can use some kind of chunking to take only a part of the data set into memory at the time, and also these algorithms are very straightforward to paralyze.",
            "You can allot each chunk to a different processor, and so optimizing parallel."
        ],
        [
            "But OK, so this is my last slide, so this we have kind of present at kernel based approach for hierarchical text classification where documents can belong to more than one category at a time.",
            "So we can so that the prediction accuracy.",
            "In the pirate cases, improved by this this method.",
            "And we constructively optimize the models.",
            "By decomposing into single example subproblems.",
            "What is conditional gradient such an?",
            "Efficient inference algorithm to find update direction.",
            "So on the scale that we can tackle is around 1000, thousands of examples, hundreds of micro levels at the moment, and so the further scale up is of course interesting.",
            "Future computer cost question.",
            "OK, thank you for your attention.",
            "So we have time for just one question fairly.",
            "There's any.",
            "Do you have any idea about performance goes up for the order status is so it's?",
            "Yeah Reuters.",
            "It's quite funny hierarchies this secret hierarchy.",
            "It's like you have a root node and the second level is like something like almost 20 nodes on the second level and only really one or two deeper nodes, so deeper part.",
            "So it's very degenerate hierarchy, so that it's my opinion that might be iteration that so the second level is hard to predict 'cause it's like.",
            "Most of the data is there and this long parts one or two are somehow so specific that these are easier.",
            "That's my yes.",
            "David Process opinionated process which can be resolved to have data even further down is this one possibility.",
            "Explanation I'm not sure I understand, yeah.",
            "I'm not really in the dimension, is that in many repos?",
            "Is there something goes wrong with this higher level off?",
            "Further down then maybe it could be quickly resolved if they haven't got the labels right.",
            "Ah OK, that might be yes, OK?",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First pictures we have.",
                    "label": 0
                },
                {
                    "sent": "Is already interesting topic practical document classification.",
                    "label": 0
                },
                {
                    "sent": "OK, so my name is you are also and I'm talking about much Martin Marco Networks in hierarchical document classification and this is joint work with Greg Sanders under Jet, Marc Antonio Taylor and this mostly based on JML article that is about to come out pretty soon.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so this is the specific problem we talk about, so we have some some classification hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Some topics and.",
                    "label": 0
                },
                {
                    "sent": "And then some news articles or some other documents that need to be classified according to this hierarchy.",
                    "label": 0
                },
                {
                    "sent": "We assumed that the classifications are kind of multi label nature so that we have several categories in this hierarchy that are relevant to this each document.",
                    "label": 0
                },
                {
                    "sent": "So we have this kind of union of partial parts in this whole hierarchy that is part of the document classification.",
                    "label": 1
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the setup.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "To pick out which labels so that within your, why is the plus 1 -- 1 if you have binary variables representing which labels yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Plus means plus equals green.",
                    "label": 0
                },
                {
                    "sent": "Yes, and this these are minus they are not part of the label OK?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right OK so.",
                    "label": 0
                },
                {
                    "sent": "Obviously this is a question has been studied by many people over the years and there are many strategies for learning this kind of in this kind of settings, and I guess the most obvious.",
                    "label": 0
                },
                {
                    "sent": "Ways to just flatten the hierarchy, so you just consider it snowed into hierarchy.",
                    "label": 1
                },
                {
                    "sent": "Separately, try to build a classifier for the node and then somehow combined compound predictions.",
                    "label": 0
                },
                {
                    "sent": "Then to get this multi level label prediction.",
                    "label": 1
                },
                {
                    "sent": "OK so this is of course competition are relatively inexpensive, but it doesn't make any use of the dependencies between between the labels that are quite obvious.",
                    "label": 0
                },
                {
                    "sent": "If your hierarchy represent.",
                    "label": 0
                },
                {
                    "sent": "Kind of world in any meaningful way.",
                    "label": 0
                },
                {
                    "sent": "I do a little bit better if you train the models.",
                    "label": 0
                },
                {
                    "sent": "Hierarchical is so that you start with root node of the hierarchy and train a classifier there and then kind of split the data based on the membership.",
                    "label": 0
                },
                {
                    "sent": "Into subtrees and continue this kind of.",
                    "label": 0
                },
                {
                    "sent": "Dividing today time and training the training models.",
                    "label": 0
                },
                {
                    "sent": "OK, so this kind of approach of obviously picks up some of the dependencies between the micro labels, but the training data fragments towards the leaves, so you will get less and less data data in the leaves.",
                    "label": 1
                },
                {
                    "sent": "So this estimation becomes reliable, less reliable, and also it's hard to say what is the loss function that is more or less trained in terms with so, so it's not not.",
                    "label": 0
                },
                {
                    "sent": "Kind of till tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Where to do?",
                    "label": 0
                },
                {
                    "sent": "And so we want to do something better.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so we use use this kind of conditional random field model that you have seen a couple of times already.",
                    "label": 1
                },
                {
                    "sent": "Here during this workshop, so we have.",
                    "label": 0
                },
                {
                    "sent": "So we took the edges of the hierarchy and assign some potential function for each edge that is an exponential where we have the weight vector for the edge.",
                    "label": 1
                },
                {
                    "sent": "An then joint fits a map for this document X and this labeling of that edge.",
                    "label": 0
                },
                {
                    "sent": "So basically.",
                    "label": 1
                },
                {
                    "sent": "We have kind of context dependent feature feature Maps there there and we learn the weights for these features.",
                    "label": 0
                },
                {
                    "sent": "OK, so then we have this this partition function there to worry about.",
                    "label": 1
                },
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is this is the model.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's take a look at the feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Obviously you can contract them in many ways, but this is turns out to be quite useful way to do it.",
                    "label": 0
                },
                {
                    "sent": "You construct this feature vector of blocks that are specific to.",
                    "label": 1
                },
                {
                    "sent": "Edge and labeling of an edge so you have heaven.",
                    "label": 0
                },
                {
                    "sent": "Some feature vector for your documents that can be back of words substring spectrum or something else.",
                    "label": 1
                },
                {
                    "sent": "And you prefix that with an indicator.",
                    "label": 0
                },
                {
                    "sent": "So whether it whether your document belongs to certain.",
                    "label": 0
                },
                {
                    "sent": "Edge or a certain node in a hierarchy, or certain pair of nodes in your hierarchy.",
                    "label": 1
                },
                {
                    "sent": "OK, So what it does is it follows us to learn.",
                    "label": 1
                },
                {
                    "sent": "Import consist of this these features in different context, so you can pick up different features in different different nodes of the hierarchy or edges of the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "This special structure, attached to repeat this this this feature vector for this document that can be utilized in computation.",
                    "label": 0
                },
                {
                    "sent": "So we can we can do things in less memory than if you have some kind of huge.",
                    "label": 1
                },
                {
                    "sent": "Huge feature vector folder, whole structure.",
                    "label": 0
                },
                {
                    "sent": "So this is illustration how this this feature vectors are composed of the whole.",
                    "label": 0
                },
                {
                    "sent": "This is the total.",
                    "label": 0
                },
                {
                    "sent": "Feature vector folder for the whole hierarchy.",
                    "label": 0
                },
                {
                    "sent": "And it's composed of edge blocks.",
                    "label": 0
                },
                {
                    "sent": "Like this for each edge and each of the edge blocks further decomposes into.",
                    "label": 0
                },
                {
                    "sent": "Blocks based on edge labeling.",
                    "label": 0
                },
                {
                    "sent": "What this indicator does it says.",
                    "label": 0
                },
                {
                    "sent": "Turns on the feature vector in the correct, correct context, so it's quieter.",
                    "label": 0
                },
                {
                    "sent": "Kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "Structure.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K so then of course, another important thing to consider is the loss function.",
                    "label": 0
                },
                {
                    "sent": "But So what kind of loss function used be used when you're training this kind of model?",
                    "label": 0
                },
                {
                    "sent": "So you have this multi label?",
                    "label": 0
                },
                {
                    "sent": "That's true multi label for your documentation and some predict that one and you should think about what is the what is the law that you assigned to it.",
                    "label": 0
                },
                {
                    "sent": "So of course you can use Cheryl one loss where you just say that if you're multi label vectors differ, you incur a penalty of 1.",
                    "label": 0
                },
                {
                    "sent": "So that's the kind of loss that is frequently used in this color classification.",
                    "label": 0
                },
                {
                    "sent": "Tasks of Alias is not very good because you don't make any difference between.",
                    "label": 0
                },
                {
                    "sent": "Slight mistakes and very severe mistakes in your prediction, so that's not not not optimal.",
                    "label": 0
                },
                {
                    "sent": "The other end of the spectrum is hamming loss, where you look at your multi label vectors as a whole and just count how many were incorrect.",
                    "label": 0
                },
                {
                    "sent": "So that obviously takes into account little bit of severity of the mistakes in your predictions, but but it doesn't do it still doesn't know anything about the hierarchy, so it doesn't know that actually some nodes have some specific positions.",
                    "label": 0
                },
                {
                    "sent": "Some notice the root of the hierarchy, and some some are leaves.",
                    "label": 0
                },
                {
                    "sent": "So maybe we want to.",
                    "label": 0
                },
                {
                    "sent": "Take that into account in our loss function and there are few suggestions for this kind of losses.",
                    "label": 0
                },
                {
                    "sent": "One is last week called Path Loss, then suggested by Niccolo Kitsap, Jahnke and Co authors.",
                    "label": 0
                },
                {
                    "sent": "Where we penalized first mistake along a path in the hierarchy.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "If there are mistake in some prediction path path, so maybe.",
                    "label": 0
                },
                {
                    "sent": "We shouldn't penalize any mistake that we still make after, so if you get the parent incorrect, maybe we shouldn't penalize the prediction in the child because they already apparent was incorrect.",
                    "label": 0
                },
                {
                    "sent": "So this kind of thinking leads to leads to this kind of loss that you only penalize the first mistake so you look how far you're correct and when you make a mistake, you incur a penalty.",
                    "label": 0
                },
                {
                    "sent": "After that, you don't incur any more penalties.",
                    "label": 0
                },
                {
                    "sent": "And you can have some kind of waiting too.",
                    "label": 0
                },
                {
                    "sent": "Rebate to penalties based on where into hierarchy you are making the mistake.",
                    "label": 0
                },
                {
                    "sent": "This is edge losses.",
                    "label": 0
                },
                {
                    "sent": "Some simplified worsen it.",
                    "label": 0
                },
                {
                    "sent": "You just look at one edge at a time and you say that, OK, you penalize the child if the.",
                    "label": 1
                },
                {
                    "sent": "If the parent was the correct but OK, so it doesn't take account the whole path.",
                    "label": 0
                },
                {
                    "sent": "The good thing about this is that leads to.",
                    "label": 0
                },
                {
                    "sent": "More tractable models in optimization of obviously is kind of an approximation of the part loss, so it's not.",
                    "label": 0
                },
                {
                    "sent": "Ask hinova elegant.",
                    "label": 0
                },
                {
                    "sent": "Right so this guy.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of components we can use an so this is the learning framework, and Fernando Pereira already told you about this kind of much marching approaches.",
                    "label": 0
                },
                {
                    "sent": "Pro structures trucks are outputs.",
                    "label": 0
                },
                {
                    "sent": "Night system, kind of.",
                    "label": 0
                },
                {
                    "sent": "Rake up it here so the idea is that we take the correct multi label for of the document and we try to separate this correct multi label from the incorrect ones by a large margin.",
                    "label": 1
                },
                {
                    "sent": "So so kind of the chair metrical idea is looks like this.",
                    "label": 0
                },
                {
                    "sent": "We have a way to vector and we have to feed sore joint fits or Maps.",
                    "label": 0
                },
                {
                    "sent": "Feature vectors of documents and multi level of pears and we say that OK, this is the correct one and we want to have some margin to this incorrect ones and the margin should be the larger to worst prediction is so the more losses labels have the further away they should be from the correct one.",
                    "label": 0
                },
                {
                    "sent": "And then we need to allow some slack cause because otherwise we won't be able to find find the model, so it's not not really.",
                    "label": 0
                },
                {
                    "sent": "Possible or West separate things with hard margin?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the.",
                    "label": 0
                },
                {
                    "sent": "Model and this is.",
                    "label": 0
                },
                {
                    "sent": "This is how this equations look, so we have this kind of.",
                    "label": 0
                },
                {
                    "sent": "Normalization regularization term.",
                    "label": 0
                },
                {
                    "sent": "Constraints about the margins we need to satisfy this constraint, and there is some slack that we don't allow there and do our dual looks like this.",
                    "label": 0
                },
                {
                    "sent": "So we have a linear term where there is this loss loss.",
                    "label": 0
                },
                {
                    "sent": "A serviceable quadratic term where we have a joint kernel for for this document.",
                    "label": 0
                },
                {
                    "sent": "Multi label pairs.",
                    "label": 0
                },
                {
                    "sent": "A simple box constraints for for this.",
                    "label": 0
                },
                {
                    "sent": "And all variables.",
                    "label": 0
                },
                {
                    "sent": "But the problem is this is hopelessly intractable, tractable.",
                    "label": 0
                },
                {
                    "sent": "Model because you have exponential number of primal constraints.",
                    "label": 1
                },
                {
                    "sent": "And also do all variables so.",
                    "label": 0
                },
                {
                    "sent": "You cannot do any any realistic sized datasets with this kind of quadratic programs, so.",
                    "label": 0
                },
                {
                    "sent": "Conflict out of order question, but there are many ways how you can make these models tractable so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right and we have followed this.",
                    "label": 0
                },
                {
                    "sent": "This idea proposed by pen task or that they marginalized the problem.",
                    "label": 0
                },
                {
                    "sent": "The idea is that instead of looking at this.",
                    "label": 0
                },
                {
                    "sent": "Original dual variables which we have exponential number of.",
                    "label": 0
                },
                {
                    "sent": "We look at the marginals edge marginals of this this this.",
                    "label": 0
                },
                {
                    "sent": "Variables.",
                    "label": 0
                },
                {
                    "sent": "And we have a polynomial number of those.",
                    "label": 0
                },
                {
                    "sent": "What we need?",
                    "label": 0
                },
                {
                    "sent": "We need this loss function to be composable by the edges, so we need to have a loss function that that decomposes.",
                    "label": 1
                },
                {
                    "sent": "Like this so.",
                    "label": 1
                },
                {
                    "sent": "Of the loss functions I showed you Hamming loss on edge loss or or.",
                    "label": 0
                },
                {
                    "sent": "Among the ones who can use.",
                    "label": 0
                },
                {
                    "sent": "OK, and we also need the kernels that compost by the edges and this directly falls out from the fitzer fitzer representations I showed you so because we have blocks in the feature vectors based on edges.",
                    "label": 0
                },
                {
                    "sent": "The Colonel obviously decomposes as well, so this kind of.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ingredients give us marginalized problem where we have the form is quite similar to the original develop, but we have this sums over the edges and.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, The thing is more or less the same.",
                    "label": 0
                },
                {
                    "sent": "We still have box constraints.",
                    "label": 1
                },
                {
                    "sent": "I wrote them in matrix form.",
                    "label": 0
                },
                {
                    "sent": "And then there is additional set of constraints that need to be set to.",
                    "label": 0
                },
                {
                    "sent": "Make sure that our marginalized problem corresponds to the original dual and these are kind of constraints that say that the edges need to somehow agree, agree about the labelings.",
                    "label": 1
                },
                {
                    "sent": "So OK, so this is this problem has a polynomial size, so it's.",
                    "label": 0
                },
                {
                    "sent": "This kind of complexity is you need to have.",
                    "label": 1
                },
                {
                    "sent": "But this still is way too much for any kind of QP solvers to be used on this problem.",
                    "label": 0
                },
                {
                    "sent": "So for example, we have a data set with little over 1000 examples and little less than 200.",
                    "label": 1
                },
                {
                    "sent": "Michael labels and if you really wrote this problem, it would consume something like 10 gigabytes of memory.",
                    "label": 0
                },
                {
                    "sent": "So even though it's polynomial size and actually low panel polynomial size, it's too big.",
                    "label": 0
                },
                {
                    "sent": "So we need to do something for the problem.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Obviously you want to decompose tomorrow somehow, so.",
                    "label": 0
                },
                {
                    "sent": "So, but the problem is that it's not trivial to decompose this objective decomposes by the edges, but constraints don't.",
                    "label": 0
                },
                {
                    "sent": "So the this consistency constraints, there are tight together, so you cannot decompose fully by the edges.",
                    "label": 1
                },
                {
                    "sent": "Similarly, you cannot decompose fully paid examples because the kernel obviously ties examples together.",
                    "label": 0
                },
                {
                    "sent": "What you can do you can you can look at the gradient of the of the objective.",
                    "label": 1
                },
                {
                    "sent": "So because here you don't have example interactions, so and this is what we have actually done.",
                    "label": 0
                },
                {
                    "sent": "So so we have used them creating test method.",
                    "label": 0
                },
                {
                    "sent": "Polo training this in decomposed form.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Specific algorithm we use this conditional gradient method by persicus.",
                    "label": 1
                },
                {
                    "sent": "So it conducts iterative gradient search, indefeasible set and.",
                    "label": 0
                },
                {
                    "sent": "Update directions are kind of highest fees with this feasible points, assuming the current trading and this is a linear program that you that you need to solve.",
                    "label": 0
                },
                {
                    "sent": "OK, so this allows us this decomposed.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Program and very quickly I saw you address about what this algorithm is about, so.",
                    "label": 0
                },
                {
                    "sent": "Look at look at the current gradient.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Book assuming this gradient is constant, so we make linear approximation of this quadratic object.",
                    "label": 0
                },
                {
                    "sent": "If we look what is the best point.",
                    "label": 0
                },
                {
                    "sent": "So this is the this corner is the best point.",
                    "label": 0
                },
                {
                    "sent": "The computer.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Saddlepoint how long this line.",
                    "label": 0
                },
                {
                    "sent": "Continue from that point and iterate.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Search",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To Dakota water.",
                    "label": 0
                },
                {
                    "sent": "Credit line service.",
                    "label": 0
                },
                {
                    "sent": "Yes yes yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the kind of approach, but is what is being done.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, to solve this this this up to help direction so you can use a linear programming solver to solve this problem, But that actually will not be the most efficient way you can do.",
                    "label": 0
                },
                {
                    "sent": "Because we can actually utilize this hierarchical structure to solve this problem much, much faster.",
                    "label": 1
                },
                {
                    "sent": "And the idea is that this vertices of this visible set actually.",
                    "label": 0
                },
                {
                    "sent": "Correspond to multi labels.",
                    "label": 0
                },
                {
                    "sent": "What we need to find this best corner is to find ways to save us to find the best multi label.",
                    "label": 0
                },
                {
                    "sent": "So we can use inference over this hierarchy, so dynamic programming over the hierarchy in linear time we can find this best best corner, so it's very much faster than using linear programming solver.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Run.",
                    "label": 0
                },
                {
                    "sent": "OK, very quickly I go to so you some experiments we have right service on Wiper Alpha or datasets so.",
                    "label": 0
                },
                {
                    "sent": "Why qualify Subpattern data set set rate.",
                    "label": 0
                },
                {
                    "sent": "With little over 1000 examples, 200 MW destroyed.",
                    "label": 0
                },
                {
                    "sent": "There's this secret family, so there's 34 Michael labels, and it trained with.",
                    "label": 0
                },
                {
                    "sent": "200 two 1500 documents.",
                    "label": 0
                },
                {
                    "sent": "The compared our article returns that we called HM Cube, so hierarchal Max margin Markov compared to flat SVM hierarchical trends, SVM and and hierarchical regularised least squares algorithm by Niccolo Casas Bianchi.",
                    "label": 1
                },
                {
                    "sent": "Everything was implemented it in Matlab, well except Flat SV and that was SVM, light and test user Paci.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basic computer.",
                    "label": 0
                },
                {
                    "sent": "Optimize things.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is something about optimization efficiency, so this kind of data.",
                    "label": 1
                },
                {
                    "sent": "Can we compare this?",
                    "label": 0
                },
                {
                    "sent": "What happens when we use linear programming versus finding this to use this up the directions versus finding them by dynamic programming inference then so you can see that by dynamic programming it's like.",
                    "label": 1
                },
                {
                    "sent": "In 15 minutes, while they are pretty close to the optimum and and this linear program is much much lower, so it takes hours to reach.",
                    "label": 0
                },
                {
                    "sent": "Kind of reasonable.",
                    "label": 0
                },
                {
                    "sent": "Region, but anyway we can tractably learn this kind of this kind of data set.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, something about prediction accuracy.",
                    "label": 0
                },
                {
                    "sent": "So this is level wise F1.",
                    "label": 0
                },
                {
                    "sent": "Statistiques OB day.",
                    "label": 0
                },
                {
                    "sent": "Look at each level of the hierarchy separately and look at micro label predictions.",
                    "label": 0
                },
                {
                    "sent": "Microwave paper prices on microarray Apple recall and compute F1 from it and so so the depth of the hierarchical scores this way.",
                    "label": 0
                },
                {
                    "sent": "OK, so I guess one thing you can notice from this.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is clear this why for data obviously that one goes down.",
                    "label": 0
                },
                {
                    "sent": "So Reuters is not not that clear because it's very degenerate hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So this effect is not clear but your F1 goes down when we go below to go down to levels.",
                    "label": 0
                },
                {
                    "sent": "But in the higher levels, all of the methods are pretty even, but when you go deeper, you can see that OK, flat SVM starts to lose out, and that's because the recall drops very dramatically, so flat SVM is not able to recall deep nodes.",
                    "label": 1
                },
                {
                    "sent": "Position is good for flat SVM all the time, and all this hierarchical variance they do.",
                    "label": 0
                },
                {
                    "sent": "Much better on its ends at this much margin markup is the.",
                    "label": 0
                },
                {
                    "sent": "Seems to do pretty well in the leaves compared to the other hierarchal training medicine.",
                    "label": 0
                },
                {
                    "sent": "We don't actually know why, but but obviously they're kind of take the results as they as they.",
                    "label": 0
                },
                {
                    "sent": "So to us so.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is so we do reasonably reasonably well it.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wait a minute.",
                    "label": 0
                },
                {
                    "sent": "Something about the scalability.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can do something like 10,005 thousand make microwave dental 1000 examples 5000 microfibers.",
                    "label": 0
                },
                {
                    "sent": "But so if you kind of multiply both of those figures by 10 and compute how much you would use memory, so we could take up the whole hard disk with this kind of.",
                    "label": 1
                },
                {
                    "sent": "Formalize, and so it might be that we need to think of something else to scale up to really large large datasets and hierarchies.",
                    "label": 0
                },
                {
                    "sent": "Mostly this is because of the kernel that takes quadratic size studio in the number of examples.",
                    "label": 0
                },
                {
                    "sent": "There's some some possibilities to improve things so obvious you can use some kind of chunking to take only a part of the data set into memory at the time, and also these algorithms are very straightforward to paralyze.",
                    "label": 1
                },
                {
                    "sent": "You can allot each chunk to a different processor, and so optimizing parallel.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But OK, so this is my last slide, so this we have kind of present at kernel based approach for hierarchical text classification where documents can belong to more than one category at a time.",
                    "label": 1
                },
                {
                    "sent": "So we can so that the prediction accuracy.",
                    "label": 0
                },
                {
                    "sent": "In the pirate cases, improved by this this method.",
                    "label": 0
                },
                {
                    "sent": "And we constructively optimize the models.",
                    "label": 0
                },
                {
                    "sent": "By decomposing into single example subproblems.",
                    "label": 0
                },
                {
                    "sent": "What is conditional gradient such an?",
                    "label": 1
                },
                {
                    "sent": "Efficient inference algorithm to find update direction.",
                    "label": 0
                },
                {
                    "sent": "So on the scale that we can tackle is around 1000, thousands of examples, hundreds of micro levels at the moment, and so the further scale up is of course interesting.",
                    "label": 0
                },
                {
                    "sent": "Future computer cost question.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "So we have time for just one question fairly.",
                    "label": 0
                },
                {
                    "sent": "There's any.",
                    "label": 0
                },
                {
                    "sent": "Do you have any idea about performance goes up for the order status is so it's?",
                    "label": 0
                },
                {
                    "sent": "Yeah Reuters.",
                    "label": 0
                },
                {
                    "sent": "It's quite funny hierarchies this secret hierarchy.",
                    "label": 0
                },
                {
                    "sent": "It's like you have a root node and the second level is like something like almost 20 nodes on the second level and only really one or two deeper nodes, so deeper part.",
                    "label": 0
                },
                {
                    "sent": "So it's very degenerate hierarchy, so that it's my opinion that might be iteration that so the second level is hard to predict 'cause it's like.",
                    "label": 0
                },
                {
                    "sent": "Most of the data is there and this long parts one or two are somehow so specific that these are easier.",
                    "label": 0
                },
                {
                    "sent": "That's my yes.",
                    "label": 0
                },
                {
                    "sent": "David Process opinionated process which can be resolved to have data even further down is this one possibility.",
                    "label": 0
                },
                {
                    "sent": "Explanation I'm not sure I understand, yeah.",
                    "label": 0
                },
                {
                    "sent": "I'm not really in the dimension, is that in many repos?",
                    "label": 0
                },
                {
                    "sent": "Is there something goes wrong with this higher level off?",
                    "label": 0
                },
                {
                    "sent": "Further down then maybe it could be quickly resolved if they haven't got the labels right.",
                    "label": 0
                },
                {
                    "sent": "Ah OK, that might be yes, OK?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}