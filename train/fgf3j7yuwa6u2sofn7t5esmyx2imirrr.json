{
    "id": "fgf3j7yuwa6u2sofn7t5esmyx2imirrr",
    "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs",
    "info": {
        "author": [
            "Vicente Ordonez, Stony Brook University"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Image Analysis",
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/nips2011_ordonez_captioned/",
    "segmentation": [
        [
            "Hi or paper is about generating image descriptions.",
            "Most computer vision methods would usually output individual pieces of information as opposed to what human would use to visually explain an image.",
            "For instance, for this picture, a good computer vision system would desirably would output a something like Sky, trees, water buildings, perhaps even the bridge, but human would rather explain this image as something like an old bridge.",
            "Over a peaceful River near my home in Long Island, so or that's the goal of our paper generate this kind of output out of images and."
        ],
        [
            "We approach this problem in a data driven manner.",
            "So in order to do so, we have built 1 million image data set where each of the images has associated visually descriptive captions and the way we collect this data set is by collect Ace by gathering an enormous amount of captions from the web where users have already assigned image descriptions.",
            "The key here is to filter down these captions in such a way that we end up with the ones that are the most visually descriptive.",
            "And so we've approached this problem in a very simple manner, just for a given query image.",
            "We retrieve the visually similar images from our data set an using standard global image feature representations like just and color, and we directly transfer captions, for instance, from these pictures we have transferred a caption that says for this image the water is clear enough to see fish swimming around in it."
        ],
        [
            "We Additionally rerank the images that we retrieved in the previous step by running high level content estimators like object detectors seem classifiers, stuff detectors, people in action detectors and also text the statistics.",
            "Given that we have lots of captions, for instance in this picture we have detected in our query image a bridge and water and we have a run object detectors on or matching images and we have.",
            "Re rank them based on how the individual components of the image are visually similar to or query image an.",
            "We Additionally take advantage of the fact that we have descriptions because we only run object detectors and or matching images.",
            "If a relevant keyword is found on the text.",
            "Additionally we use the text to computer statistics.",
            "For instance if in this example if a lot of the images that are retrieved contain the word abridged, then those images get rewarded in all final ranking.",
            "And again we get a set of images that are most visually similar to a query image with respect to high level image content, and we can transfer a caption like the bridge over the Lake on Suzhou River on Social St."
        ],
        [
            "Finally, these are some of the results we got.",
            "For instance, you can see that the captions are really good because they were actually written by humans.",
            "The first picture on the top says amazing colors in the Sky at sunset, with the orange of the cloud and the blue of the Sky behind.",
            "And of course, our method also has, you know, some flaws.",
            "Of course, we cannot really represent all possible observable images even with 1,000,000 images with captions, so.",
            "And also, or estimators of content are noisy and you can see how this gave fail.",
            "But we have some quantitative evaluation that you can check out our poster session poster 21 an.",
            "Also you can ask us about, you know for their questions and also how to have access to our data set.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi or paper is about generating image descriptions.",
                    "label": 0
                },
                {
                    "sent": "Most computer vision methods would usually output individual pieces of information as opposed to what human would use to visually explain an image.",
                    "label": 0
                },
                {
                    "sent": "For instance, for this picture, a good computer vision system would desirably would output a something like Sky, trees, water buildings, perhaps even the bridge, but human would rather explain this image as something like an old bridge.",
                    "label": 1
                },
                {
                    "sent": "Over a peaceful River near my home in Long Island, so or that's the goal of our paper generate this kind of output out of images and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We approach this problem in a data driven manner.",
                    "label": 0
                },
                {
                    "sent": "So in order to do so, we have built 1 million image data set where each of the images has associated visually descriptive captions and the way we collect this data set is by collect Ace by gathering an enormous amount of captions from the web where users have already assigned image descriptions.",
                    "label": 0
                },
                {
                    "sent": "The key here is to filter down these captions in such a way that we end up with the ones that are the most visually descriptive.",
                    "label": 0
                },
                {
                    "sent": "And so we've approached this problem in a very simple manner, just for a given query image.",
                    "label": 0
                },
                {
                    "sent": "We retrieve the visually similar images from our data set an using standard global image feature representations like just and color, and we directly transfer captions, for instance, from these pictures we have transferred a caption that says for this image the water is clear enough to see fish swimming around in it.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We Additionally rerank the images that we retrieved in the previous step by running high level content estimators like object detectors seem classifiers, stuff detectors, people in action detectors and also text the statistics.",
                    "label": 0
                },
                {
                    "sent": "Given that we have lots of captions, for instance in this picture we have detected in our query image a bridge and water and we have a run object detectors on or matching images and we have.",
                    "label": 0
                },
                {
                    "sent": "Re rank them based on how the individual components of the image are visually similar to or query image an.",
                    "label": 0
                },
                {
                    "sent": "We Additionally take advantage of the fact that we have descriptions because we only run object detectors and or matching images.",
                    "label": 0
                },
                {
                    "sent": "If a relevant keyword is found on the text.",
                    "label": 0
                },
                {
                    "sent": "Additionally we use the text to computer statistics.",
                    "label": 0
                },
                {
                    "sent": "For instance if in this example if a lot of the images that are retrieved contain the word abridged, then those images get rewarded in all final ranking.",
                    "label": 0
                },
                {
                    "sent": "And again we get a set of images that are most visually similar to a query image with respect to high level image content, and we can transfer a caption like the bridge over the Lake on Suzhou River on Social St.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, these are some of the results we got.",
                    "label": 0
                },
                {
                    "sent": "For instance, you can see that the captions are really good because they were actually written by humans.",
                    "label": 0
                },
                {
                    "sent": "The first picture on the top says amazing colors in the Sky at sunset, with the orange of the cloud and the blue of the Sky behind.",
                    "label": 1
                },
                {
                    "sent": "And of course, our method also has, you know, some flaws.",
                    "label": 0
                },
                {
                    "sent": "Of course, we cannot really represent all possible observable images even with 1,000,000 images with captions, so.",
                    "label": 0
                },
                {
                    "sent": "And also, or estimators of content are noisy and you can see how this gave fail.",
                    "label": 0
                },
                {
                    "sent": "But we have some quantitative evaluation that you can check out our poster session poster 21 an.",
                    "label": 0
                },
                {
                    "sent": "Also you can ask us about, you know for their questions and also how to have access to our data set.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}