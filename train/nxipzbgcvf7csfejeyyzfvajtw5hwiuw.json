{
    "id": "nxipzbgcvf7csfejeyyzfvajtw5hwiuw",
    "title": "Feature Shaping for Linear SVM Classifiers",
    "info": {
        "author": [
            "Martin Scholz, HP Labs, Palo Alto, HP Labs"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/kdd09_scholz_fslsvmc/",
    "segmentation": [
        [
            "Yeah hello everybody, thanks for your patience.",
            "Presenting joint work with George Foreman and Jim Rajaram.",
            "We're all from HP Labs based in Palo Alto.",
            "So this talk is about new preprocessing technique which we refer to as feature shaping and as you can tell from the title, we were mostly interested in and are focusing on the case of linear support vector machines."
        ],
        [
            "So very quick reminder.",
            "I'm sure you've all seen plenty of pictures like this one.",
            "What are linear SVM is all about you have your point clouds positive and negative examples, and the goal of a linear SVM is to separate those.",
            "So you have your hyperplane an in the ideal case you have your positives on one side, the negatives on the other side with the nice margin where you don't have any examples.",
            "Clearly this doesn't always work so.",
            "In that case, you are using the soft margin approach.",
            "There's a penalty for examples on the wrong side, and for examples inside the margin.",
            "OK, so obviously this is a toy example.",
            "So how do things look in reality?",
            "Clearly the two dimensional case is very exciting.",
            "In practice, we often have thousands or even millions of features, some of which are highly predictive.",
            "Others are more or less useless and might even confuse the linear SVM.",
            "So we have quite a large range there.",
            "As soon as we leave domains like text classification, we should also be prepared for the case in which we have very different kinds of attributes, so we would have a nice mix of nominal and continuous features, for example.",
            "So what we're proposing is to look at one feature at a time, and to help the SVM by doing some smart transformations here.",
            "So we're actually looking at projections of the data.",
            "And this might look a bit myopic at first, but this is actually quite common in feels like feature selection.",
            "So in feature selection, if you're following the filtering approach, we look at one feature at a time, and this already gives us a good good idea of how predictive each feature is.",
            "An we will just keep the most productive ones.",
            "So we were wondering, is it possible to use more powerful transformations here at the feature Level 1 feature at a time in a way that helps our linear SVM's?",
            "So clearly we want to want them to become more."
        ],
        [
            "Predictive and from motivations here is a simple example of a feature which linear SVM may not fully appreciate, although it's quite predictive, so you might think of this as something like blood pressure.",
            "So you would have different values of blood pressure along the X axis, and then we can.",
            "We can just draw a histogram here.",
            "So for each feature value you would have different observations, certain number of positives, a certain number of negatives.",
            "For the purpose of orientation we also plotted here the in blue the fraction of positives.",
            "So what happens here is we have a nice interval in the middle where we have a higher likelihood of seeing a positive example, but since this is an interval it's not easy for the SVM to capture this kind of pattern.",
            "So maybe we can apply some kind of transformation here, which would help the."
        ],
        [
            "I'm.",
            "OK, let's first look at the formal optimization problem and figure out which kinds of transformations do make a change at all.",
            "So I assume you're familiar a little bit with linear SVM and the first line we have our objective function.",
            "We want to minimize regularization term over weight vector and then we have our constant to see which determines the degree of penalization.",
            "If points on the wrong side of the hyperplane.",
            "And the lines two and three are basically just these constraints.",
            "So let's start simple.",
            "What happens if we apply an affine transformation here?",
            "Well, nothing, and you can easily see that from the formal optimization problem, because the offset is not part of the regularization.",
            "So we can just follow around or.",
            "Refine transformations without changing the objective function at all.",
            "So let's look into linear transformations.",
            "Without the affine transformations, that means we would just scale each feature individually.",
            "Well, since we have this regularization term, we kind of change the price tags on our different features.",
            "And clearly once we're doing this.",
            "There might be cheaper, better solutions, so yes, it has an effect, but it's interesting to note that if we were to scale all of our features using the same constant, in that case we would just look at a bigger or smaller version of the same optimization problem with one exception.",
            "We would implicitly change the value of C or that's a different story.",
            "So this means that we're scaling just matters relative to each other, so the absolute values don't matter.",
            "It matters how you scale features relative to each other.",
            "And there are other things we could come up with.",
            "We could change the distances between examples.",
            "Clearly this has an effect as soon as support vectors are involved, and even more so if we're looking into non monotonic transformations.",
            "OK, so much about motivation and or different observations.",
            "So let's make."
        ],
        [
            "Wish list So what could we potentially get out of this?",
            "First of all, clearly in raw datasets we expect to find many irrelevant features or attributes, so we would like to detect those.",
            "Then the scaling might be inappropriate.",
            "So if you think of the scales as price text, then we would like the scale of these features to reflect the importance for an SVM.",
            "For example, blood pressure and body mass index.",
            "These two features live at a different scale, and that doesn't necessarily reflect the importance for a particular learning task.",
            "3rd one well, sometimes features do not have any kind of linear dependency which would help or might help an SVM.",
            "So for example, if you look at something like death rate when you're speeding that we have an exponential curve and we might prefer linear curve or to give a linear curve to the SVM.",
            "And finally, we've already seen the blood pressure example.",
            "Sometimes we have an interval so it's non monotonic relationship and we might also want to fix that."
        ],
        [
            "So what do people usually do about?",
            "Inappropriate representations we can look at something like a landscape of different techniques.",
            "Here there are more expensive ones that look at multiple features, and there are cheaper ones that look at individual features from the more expensive ones.",
            "I just want to talk about nonlinear kernels because that's an obvious thing to do here.",
            "The thing about it is it's either on or off, so you wouldn't if a linear SVM does a good job, but there are just a handful of features that misbehave a bit.",
            "You won't want to change the kernel because it's very hard to tune in kernel for different kinds of features that you would like to be used with a different kernel.",
            "It could handcrafted kernel, but that is again very expensive, so I don't want to talk about the other message here.",
            "If you look at individual features, then we have an input feature XI want to transform it somehow?",
            "And there are.",
            "Different methods here that might help us.",
            "The most obvious one is feature selection, and in formal terms, what we're basically going to do is either we include a feature or we zero it out.",
            "Another method here is feature scaling and there is a relaxation of that.",
            "We might use continuous wait for each feature.",
            "If you're scaling is at the end of this list, but let's first look at select."
        ],
        [
            "And scaling.",
            "So for selection.",
            "Filter approach the many metrics out in the market.",
            "So here I just want to refer to prior work done by forming presented at Sycamore 8.",
            "So what you're forming found was that by normal separation is the getting the best results on a large text benchmark.",
            "So we just focusing on this.",
            "The learner here was a linear SVM two, so it's quite compatible to our results.",
            "So I think I'm going to skip."
        ],
        [
            "This slide in the interest of time.",
            "So this is basically the formula and the interesting thing is you can use the same formula, which is the difference between the inverse normal of the true positive rate and the false positive rate can use that same thing for scale."
        ],
        [
            "Bing.",
            "In that case.",
            "You get.",
            "Quite an improvement in terms of F measure.",
            "So here we are comparing a binary representation of terms and a representation where we use the values zero and the BNS score of each feature.",
            "So."
        ],
        [
            "Now the interesting result is that on top of this, if you're using feature selection, it doesn't really help, so you're getting the best values here.",
            "If you just apply plain being a scaling, that's quite an interesting result, because this already proves the point that by transforming your features one by one you can get improvement if you're running a linear linear SVM."
        ],
        [
            "Back to our example.",
            "Um?",
            "So here we have two problems on monotonicity and it's also not very linear.",
            "So what if we come up with the transformation that allows us to end up with something that is monotone in the fraction of positives?",
            "So how would we do that?"
        ],
        [
            "The first step would be clearly to come up with the blue curve, so this is a conditional probability estimation problem.",
            "And we haven't introduced any restrictions here, so we are dealing with very different kinds of attributes.",
            "Nominal, ordinal, continuous, or our experiments.",
            "We use very simple method at this point.",
            "So the output here is a function P that gives us the conditional probability estimates.",
            "The next step is."
        ],
        [
            "To do the actual reshaping, so in terms of our example, that would be the mapping from the left hand side to the right hand side.",
            "We can use as an input or estimator function Pi.",
            "The goal would be something like making the feature more linearly dependent or even just monotonic.",
            "So we tried to different things at this point.",
            "Surprisingly, very simple method gave the most consistent results here, which this method we call the local probability shaper.",
            "What we did here is we just mapped the feature value of XI to the conditional probability estimate.",
            "So if you're.",
            "OK. Skipping a bit, so we also tried other transformation.",
            "I wanna talk transformations like using the rank of features or things derived from RC plots, but that didn't really fly, so we are."
        ],
        [
            "Checking with the Ehllapi Shaper, there's three more steps here.",
            "We organized this preprocessing in terms of the pipeline.",
            "So we already covered the first 2.",
            "Then we applied feature scaling normalization and then a technical step preserving sparsity."
        ],
        [
            "So we already talked about scaling.",
            "All we had to do here was to adapt it to the case where we."
        ],
        [
            "The binary features normalization.",
            "We just tried the standard stuff to normalization at one normalization normalization.",
            "The first one of these worked the best."
        ],
        [
            "We messed with sparsity with this method, but since affine transformation don't have any effect, we can just add a constant here and make sure that we met zero to zero.",
            "That helped a lot for our text datasets."
        ],
        [
            "So a few experimental results.",
            "We run this on text benchmark and on the UCI benchmark we used, we broke down multiclass problems into binary classification problems.",
            "Of course we used an SVM and we use cross validation to pick our value of C."
        ],
        [
            "So this is the interesting result on text.",
            "We got an improvement, although our estimators all very simple and we used a very simple transformation method.",
            "So here we show full learning curves for the baseline where we do nothing.",
            "Scaling helps.",
            "Shaping helps even more."
        ],
        [
            "The interesting thing is that on the UCI data set where we really have a nice mix of different attributes when we brutally applied this method to all the features, no questions asked, we got quite a large improvement when we looked at the ranking.",
            "So this AOC on the UCI data and we were wondering how does that happen?"
        ],
        [
            "And how does it look at the level of individual learning tasks?",
            "So we sorted our binary classification problems.",
            "So along the X axis, from very hard to very simple.",
            "And the dots show you how we did after shaping and what we can see is that sometimes if shaping help it helped a lot.",
            "Sometimes it hurt, but you could easily detect that in practice, so this isn't really a problem.",
            "And clearly when you're the problem is very easy, then you shouldn't use shaping because you can only lose."
        ],
        [
            "Finally, we conducted a lesion study, so if you look at the.",
            "Pipeline, then we deactivated few individual steps here.",
            "So what this tells you at the very right when we're using everything, we're getting these points for the different metrics and the second point is shaping.",
            "If you deactivate that, it takes really beta deep it."
        ],
        [
            "OK, so I think I'll just show you the conclusions and won't have time for questions.",
            "All good questions by the speakers are next week for Switch to implement.",
            "Yeah.",
            "How many features will probabilities?",
            "After the shaping, features really not important to the problem, not important.",
            "Probability, yeah, so yeah those would be features with end up at a very low scale.",
            "Interesting questions so.",
            "I don't know right away.",
            "I just can say that if you're looking at the scaling results then you would basically try to include all the features."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah hello everybody, thanks for your patience.",
                    "label": 0
                },
                {
                    "sent": "Presenting joint work with George Foreman and Jim Rajaram.",
                    "label": 0
                },
                {
                    "sent": "We're all from HP Labs based in Palo Alto.",
                    "label": 1
                },
                {
                    "sent": "So this talk is about new preprocessing technique which we refer to as feature shaping and as you can tell from the title, we were mostly interested in and are focusing on the case of linear support vector machines.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So very quick reminder.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you've all seen plenty of pictures like this one.",
                    "label": 0
                },
                {
                    "sent": "What are linear SVM is all about you have your point clouds positive and negative examples, and the goal of a linear SVM is to separate those.",
                    "label": 0
                },
                {
                    "sent": "So you have your hyperplane an in the ideal case you have your positives on one side, the negatives on the other side with the nice margin where you don't have any examples.",
                    "label": 0
                },
                {
                    "sent": "Clearly this doesn't always work so.",
                    "label": 0
                },
                {
                    "sent": "In that case, you are using the soft margin approach.",
                    "label": 0
                },
                {
                    "sent": "There's a penalty for examples on the wrong side, and for examples inside the margin.",
                    "label": 0
                },
                {
                    "sent": "OK, so obviously this is a toy example.",
                    "label": 0
                },
                {
                    "sent": "So how do things look in reality?",
                    "label": 1
                },
                {
                    "sent": "Clearly the two dimensional case is very exciting.",
                    "label": 0
                },
                {
                    "sent": "In practice, we often have thousands or even millions of features, some of which are highly predictive.",
                    "label": 0
                },
                {
                    "sent": "Others are more or less useless and might even confuse the linear SVM.",
                    "label": 0
                },
                {
                    "sent": "So we have quite a large range there.",
                    "label": 0
                },
                {
                    "sent": "As soon as we leave domains like text classification, we should also be prepared for the case in which we have very different kinds of attributes, so we would have a nice mix of nominal and continuous features, for example.",
                    "label": 0
                },
                {
                    "sent": "So what we're proposing is to look at one feature at a time, and to help the SVM by doing some smart transformations here.",
                    "label": 0
                },
                {
                    "sent": "So we're actually looking at projections of the data.",
                    "label": 0
                },
                {
                    "sent": "And this might look a bit myopic at first, but this is actually quite common in feels like feature selection.",
                    "label": 0
                },
                {
                    "sent": "So in feature selection, if you're following the filtering approach, we look at one feature at a time, and this already gives us a good good idea of how predictive each feature is.",
                    "label": 0
                },
                {
                    "sent": "An we will just keep the most productive ones.",
                    "label": 0
                },
                {
                    "sent": "So we were wondering, is it possible to use more powerful transformations here at the feature Level 1 feature at a time in a way that helps our linear SVM's?",
                    "label": 0
                },
                {
                    "sent": "So clearly we want to want them to become more.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Predictive and from motivations here is a simple example of a feature which linear SVM may not fully appreciate, although it's quite predictive, so you might think of this as something like blood pressure.",
                    "label": 0
                },
                {
                    "sent": "So you would have different values of blood pressure along the X axis, and then we can.",
                    "label": 0
                },
                {
                    "sent": "We can just draw a histogram here.",
                    "label": 0
                },
                {
                    "sent": "So for each feature value you would have different observations, certain number of positives, a certain number of negatives.",
                    "label": 0
                },
                {
                    "sent": "For the purpose of orientation we also plotted here the in blue the fraction of positives.",
                    "label": 0
                },
                {
                    "sent": "So what happens here is we have a nice interval in the middle where we have a higher likelihood of seeing a positive example, but since this is an interval it's not easy for the SVM to capture this kind of pattern.",
                    "label": 0
                },
                {
                    "sent": "So maybe we can apply some kind of transformation here, which would help the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "OK, let's first look at the formal optimization problem and figure out which kinds of transformations do make a change at all.",
                    "label": 0
                },
                {
                    "sent": "So I assume you're familiar a little bit with linear SVM and the first line we have our objective function.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize regularization term over weight vector and then we have our constant to see which determines the degree of penalization.",
                    "label": 0
                },
                {
                    "sent": "If points on the wrong side of the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "And the lines two and three are basically just these constraints.",
                    "label": 0
                },
                {
                    "sent": "So let's start simple.",
                    "label": 0
                },
                {
                    "sent": "What happens if we apply an affine transformation here?",
                    "label": 0
                },
                {
                    "sent": "Well, nothing, and you can easily see that from the formal optimization problem, because the offset is not part of the regularization.",
                    "label": 0
                },
                {
                    "sent": "So we can just follow around or.",
                    "label": 0
                },
                {
                    "sent": "Refine transformations without changing the objective function at all.",
                    "label": 0
                },
                {
                    "sent": "So let's look into linear transformations.",
                    "label": 0
                },
                {
                    "sent": "Without the affine transformations, that means we would just scale each feature individually.",
                    "label": 1
                },
                {
                    "sent": "Well, since we have this regularization term, we kind of change the price tags on our different features.",
                    "label": 0
                },
                {
                    "sent": "And clearly once we're doing this.",
                    "label": 0
                },
                {
                    "sent": "There might be cheaper, better solutions, so yes, it has an effect, but it's interesting to note that if we were to scale all of our features using the same constant, in that case we would just look at a bigger or smaller version of the same optimization problem with one exception.",
                    "label": 0
                },
                {
                    "sent": "We would implicitly change the value of C or that's a different story.",
                    "label": 0
                },
                {
                    "sent": "So this means that we're scaling just matters relative to each other, so the absolute values don't matter.",
                    "label": 0
                },
                {
                    "sent": "It matters how you scale features relative to each other.",
                    "label": 0
                },
                {
                    "sent": "And there are other things we could come up with.",
                    "label": 0
                },
                {
                    "sent": "We could change the distances between examples.",
                    "label": 1
                },
                {
                    "sent": "Clearly this has an effect as soon as support vectors are involved, and even more so if we're looking into non monotonic transformations.",
                    "label": 0
                },
                {
                    "sent": "OK, so much about motivation and or different observations.",
                    "label": 0
                },
                {
                    "sent": "So let's make.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wish list So what could we potentially get out of this?",
                    "label": 0
                },
                {
                    "sent": "First of all, clearly in raw datasets we expect to find many irrelevant features or attributes, so we would like to detect those.",
                    "label": 1
                },
                {
                    "sent": "Then the scaling might be inappropriate.",
                    "label": 0
                },
                {
                    "sent": "So if you think of the scales as price text, then we would like the scale of these features to reflect the importance for an SVM.",
                    "label": 0
                },
                {
                    "sent": "For example, blood pressure and body mass index.",
                    "label": 1
                },
                {
                    "sent": "These two features live at a different scale, and that doesn't necessarily reflect the importance for a particular learning task.",
                    "label": 0
                },
                {
                    "sent": "3rd one well, sometimes features do not have any kind of linear dependency which would help or might help an SVM.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you look at something like death rate when you're speeding that we have an exponential curve and we might prefer linear curve or to give a linear curve to the SVM.",
                    "label": 1
                },
                {
                    "sent": "And finally, we've already seen the blood pressure example.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we have an interval so it's non monotonic relationship and we might also want to fix that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do people usually do about?",
                    "label": 0
                },
                {
                    "sent": "Inappropriate representations we can look at something like a landscape of different techniques.",
                    "label": 0
                },
                {
                    "sent": "Here there are more expensive ones that look at multiple features, and there are cheaper ones that look at individual features from the more expensive ones.",
                    "label": 0
                },
                {
                    "sent": "I just want to talk about nonlinear kernels because that's an obvious thing to do here.",
                    "label": 0
                },
                {
                    "sent": "The thing about it is it's either on or off, so you wouldn't if a linear SVM does a good job, but there are just a handful of features that misbehave a bit.",
                    "label": 0
                },
                {
                    "sent": "You won't want to change the kernel because it's very hard to tune in kernel for different kinds of features that you would like to be used with a different kernel.",
                    "label": 0
                },
                {
                    "sent": "It could handcrafted kernel, but that is again very expensive, so I don't want to talk about the other message here.",
                    "label": 0
                },
                {
                    "sent": "If you look at individual features, then we have an input feature XI want to transform it somehow?",
                    "label": 1
                },
                {
                    "sent": "And there are.",
                    "label": 0
                },
                {
                    "sent": "Different methods here that might help us.",
                    "label": 0
                },
                {
                    "sent": "The most obvious one is feature selection, and in formal terms, what we're basically going to do is either we include a feature or we zero it out.",
                    "label": 0
                },
                {
                    "sent": "Another method here is feature scaling and there is a relaxation of that.",
                    "label": 0
                },
                {
                    "sent": "We might use continuous wait for each feature.",
                    "label": 0
                },
                {
                    "sent": "If you're scaling is at the end of this list, but let's first look at select.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And scaling.",
                    "label": 0
                },
                {
                    "sent": "So for selection.",
                    "label": 0
                },
                {
                    "sent": "Filter approach the many metrics out in the market.",
                    "label": 0
                },
                {
                    "sent": "So here I just want to refer to prior work done by forming presented at Sycamore 8.",
                    "label": 0
                },
                {
                    "sent": "So what you're forming found was that by normal separation is the getting the best results on a large text benchmark.",
                    "label": 0
                },
                {
                    "sent": "So we just focusing on this.",
                    "label": 0
                },
                {
                    "sent": "The learner here was a linear SVM two, so it's quite compatible to our results.",
                    "label": 0
                },
                {
                    "sent": "So I think I'm going to skip.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This slide in the interest of time.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the formula and the interesting thing is you can use the same formula, which is the difference between the inverse normal of the true positive rate and the false positive rate can use that same thing for scale.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bing.",
                    "label": 0
                },
                {
                    "sent": "In that case.",
                    "label": 0
                },
                {
                    "sent": "You get.",
                    "label": 0
                },
                {
                    "sent": "Quite an improvement in terms of F measure.",
                    "label": 0
                },
                {
                    "sent": "So here we are comparing a binary representation of terms and a representation where we use the values zero and the BNS score of each feature.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the interesting result is that on top of this, if you're using feature selection, it doesn't really help, so you're getting the best values here.",
                    "label": 0
                },
                {
                    "sent": "If you just apply plain being a scaling, that's quite an interesting result, because this already proves the point that by transforming your features one by one you can get improvement if you're running a linear linear SVM.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back to our example.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So here we have two problems on monotonicity and it's also not very linear.",
                    "label": 0
                },
                {
                    "sent": "So what if we come up with the transformation that allows us to end up with something that is monotone in the fraction of positives?",
                    "label": 0
                },
                {
                    "sent": "So how would we do that?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first step would be clearly to come up with the blue curve, so this is a conditional probability estimation problem.",
                    "label": 0
                },
                {
                    "sent": "And we haven't introduced any restrictions here, so we are dealing with very different kinds of attributes.",
                    "label": 0
                },
                {
                    "sent": "Nominal, ordinal, continuous, or our experiments.",
                    "label": 0
                },
                {
                    "sent": "We use very simple method at this point.",
                    "label": 0
                },
                {
                    "sent": "So the output here is a function P that gives us the conditional probability estimates.",
                    "label": 0
                },
                {
                    "sent": "The next step is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To do the actual reshaping, so in terms of our example, that would be the mapping from the left hand side to the right hand side.",
                    "label": 0
                },
                {
                    "sent": "We can use as an input or estimator function Pi.",
                    "label": 0
                },
                {
                    "sent": "The goal would be something like making the feature more linearly dependent or even just monotonic.",
                    "label": 1
                },
                {
                    "sent": "So we tried to different things at this point.",
                    "label": 0
                },
                {
                    "sent": "Surprisingly, very simple method gave the most consistent results here, which this method we call the local probability shaper.",
                    "label": 1
                },
                {
                    "sent": "What we did here is we just mapped the feature value of XI to the conditional probability estimate.",
                    "label": 0
                },
                {
                    "sent": "So if you're.",
                    "label": 0
                },
                {
                    "sent": "OK. Skipping a bit, so we also tried other transformation.",
                    "label": 0
                },
                {
                    "sent": "I wanna talk transformations like using the rank of features or things derived from RC plots, but that didn't really fly, so we are.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Checking with the Ehllapi Shaper, there's three more steps here.",
                    "label": 0
                },
                {
                    "sent": "We organized this preprocessing in terms of the pipeline.",
                    "label": 0
                },
                {
                    "sent": "So we already covered the first 2.",
                    "label": 0
                },
                {
                    "sent": "Then we applied feature scaling normalization and then a technical step preserving sparsity.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we already talked about scaling.",
                    "label": 0
                },
                {
                    "sent": "All we had to do here was to adapt it to the case where we.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The binary features normalization.",
                    "label": 0
                },
                {
                    "sent": "We just tried the standard stuff to normalization at one normalization normalization.",
                    "label": 0
                },
                {
                    "sent": "The first one of these worked the best.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We messed with sparsity with this method, but since affine transformation don't have any effect, we can just add a constant here and make sure that we met zero to zero.",
                    "label": 0
                },
                {
                    "sent": "That helped a lot for our text datasets.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a few experimental results.",
                    "label": 0
                },
                {
                    "sent": "We run this on text benchmark and on the UCI benchmark we used, we broke down multiclass problems into binary classification problems.",
                    "label": 1
                },
                {
                    "sent": "Of course we used an SVM and we use cross validation to pick our value of C.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the interesting result on text.",
                    "label": 0
                },
                {
                    "sent": "We got an improvement, although our estimators all very simple and we used a very simple transformation method.",
                    "label": 0
                },
                {
                    "sent": "So here we show full learning curves for the baseline where we do nothing.",
                    "label": 0
                },
                {
                    "sent": "Scaling helps.",
                    "label": 0
                },
                {
                    "sent": "Shaping helps even more.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The interesting thing is that on the UCI data set where we really have a nice mix of different attributes when we brutally applied this method to all the features, no questions asked, we got quite a large improvement when we looked at the ranking.",
                    "label": 0
                },
                {
                    "sent": "So this AOC on the UCI data and we were wondering how does that happen?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And how does it look at the level of individual learning tasks?",
                    "label": 0
                },
                {
                    "sent": "So we sorted our binary classification problems.",
                    "label": 0
                },
                {
                    "sent": "So along the X axis, from very hard to very simple.",
                    "label": 0
                },
                {
                    "sent": "And the dots show you how we did after shaping and what we can see is that sometimes if shaping help it helped a lot.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it hurt, but you could easily detect that in practice, so this isn't really a problem.",
                    "label": 0
                },
                {
                    "sent": "And clearly when you're the problem is very easy, then you shouldn't use shaping because you can only lose.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, we conducted a lesion study, so if you look at the.",
                    "label": 1
                },
                {
                    "sent": "Pipeline, then we deactivated few individual steps here.",
                    "label": 0
                },
                {
                    "sent": "So what this tells you at the very right when we're using everything, we're getting these points for the different metrics and the second point is shaping.",
                    "label": 0
                },
                {
                    "sent": "If you deactivate that, it takes really beta deep it.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I think I'll just show you the conclusions and won't have time for questions.",
                    "label": 0
                },
                {
                    "sent": "All good questions by the speakers are next week for Switch to implement.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "How many features will probabilities?",
                    "label": 0
                },
                {
                    "sent": "After the shaping, features really not important to the problem, not important.",
                    "label": 0
                },
                {
                    "sent": "Probability, yeah, so yeah those would be features with end up at a very low scale.",
                    "label": 0
                },
                {
                    "sent": "Interesting questions so.",
                    "label": 0
                },
                {
                    "sent": "I don't know right away.",
                    "label": 0
                },
                {
                    "sent": "I just can say that if you're looking at the scaling results then you would basically try to include all the features.",
                    "label": 0
                }
            ]
        }
    }
}