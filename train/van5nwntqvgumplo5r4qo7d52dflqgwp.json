{
    "id": "van5nwntqvgumplo5r4qo7d52dflqgwp",
    "title": "Statistical Classification and Cluster Processes",
    "info": {
        "author": [
            "Peter McCullagh, Department of Statistics, University of Chicago"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/mlss09us_mccullagh_sccp/",
    "segmentation": [
        [
            "Vacation models.",
            "Thank you Misha and.",
            "I want to thank the organizers for the invitation to speak here.",
            "Parser mission, and Steve.",
            "It's a great pleasure, not far from me to come.",
            "Kelly and I are both locals here, so it's very convenient.",
            "One of the things that I wanted to learn when I came here was what machine learning is all about.",
            "Or computational learning?"
        ],
        [
            "I had initially thought that machine learning was about classification.",
            "At least that's I'm usually five years behind the times, and so that's what it used to mean, and so I thought that that's what it still meant, and so I decided to focus my talk around that theme.",
            "But I've learned since that it's about other things as well.",
            "Estimation somebody told me it's about statistical estimation.",
            "Banyu tells me it's.",
            "Statistics was computation, which begs the question, what is statistics without computation?",
            "I don't know, and for others it's about algorithms perhaps.",
            "But whatever it is about, I'm going to be talking about classification."
        ],
        [
            "I want to distinguish between classification in a deterministic sense and classification in a statistical sense.",
            "So the the way that I'm going to formulate the problem is here that we have what we call training data on N specimens or units, whatever they may represent, and we on every unit there is observed feature values which live in some feature space, which in all of the examples here will just be Euclidean space might have to be transformed to be in that space as we saw in last talk.",
            "And then there is this issue that there comes along a new unit.",
            "So there has to be in the set up, not just the training data, not just a training unit.",
            "The sample of training units, but whole our whole flow sequence of units.",
            "So given a new unit with whatever feature values it has.",
            "You want to compute the probability that they knew unit belongs to whatever classes are possible.",
            "So I distinguish between classification in the deterministic sense where you declare this unit belongs to class X.",
            "With statistical classification, where you say the probability distribution for this unit is 30% for this class, 25% for Class 2 and so on.",
            "I'm going to be looking at this in the sense of.",
            "Statistical classification are probabilistic classification and in all cases the classes will be either a fixed set, finite set of labels such as zero to 9 or the Letters A-Z.",
            "Or some more or less open set, which will be the blocks of the partition B that's observed.",
            "So we get to observe that I'm talking here about.",
            "Cases where the classes are in the training sample are observed.",
            "So the the classes to which the new unit could be long will be the classes that are observed in the training sample.",
            "And there is this possibility of the classes being open that you might want to assign.",
            "You might want to say, well, yes, certainly this is 1.",
            "This is something this is a new digit, but if it happens to look like an X you wouldn't want necessarily to assign it to one of the classes zero through 9 and you'd want to end up with putting a very high probability.",
            "That is an image that you haven't seen.",
            "A class that you haven't seen before.",
            "So I'm going to be interested.",
            "I'm interested in situations where there is appreciable variation and where the sample is not so huge.",
            "That you can learn anything with extremely high probability at all.",
            "So the variation is going to be appreciable.",
            "So I'm not talking about extremely large samples.",
            "And we for this purpose the only way that I know how to model this is to use a stochastic a generative model stochastic process, which will either be IID or exchangeable, or something else, as we will see towards the end.",
            "I'm going to describe two distinct classes.",
            "Of classification models, one being an exchangeable model, which I call the Gaussians model, and another one which is not an exchangeable model, but it's of a different type based on what point processes."
        ],
        [
            "OK, so both of these models are going to involve the notion of a partition.",
            "So by a partition I don't mean a partition of the feature space, I mean a partition of the specimens.",
            "So a partition just go through it in the abstract.",
            "Here if we have got a finite set of N. Just integers one up to NA partition of N, which I will denote always by B is one of three things, or it's all of three things.",
            "It's a, it's a set of disjoint nonempty subsets whose union is the whole set.",
            "So I will typically write it as a as a set of sets with vertical bars separating the blocks, but you have to bear in mind that the order within the blocks is to be disregarded and the order of the blocks is also to be disregarded.",
            "So 246 partition, 1 three partition, 5 up here on the right top right is the same as reordering if the blocks are reordered and the within the blocks is reordered as well.",
            "So a partition is also an equivalence relation.",
            "I will use the same notation regardless of how I think about this thing, so it's an equivalence relation on the set N Boolean relation is reflexive, symmetric and transitive.",
            "And it's also of course, abulia this matrix, so these three.",
            "Representations of be set of sets.",
            "Set of subsets that should say an equivalence relation in this Boolean matrix.",
            "We're all going to be represented in the same way, and so when the symbol B comes up, you just have to think of it in whichever way is the most convenient, so the number B refers to the number of elements, which is the number of blocks.",
            "Or if you want to think of it in terms of the matrix, it's the rank, I suppose.",
            "And then for each element in B, that's a subset, the number of the little B is the number of elements.",
            "So when I talk about partitions, I will be talking about set partitions.",
            "I will usually not be talking about integer partitions, but associated with every set partition.",
            "There is an integer partition, was just disregards the elements, but just counts the block sizes.",
            "So the integer partition associated with this be that I've shown here.",
            "1 block of size 1, one of size 2 and one of size 3 is the integer partition 1 + 2 + 3."
        ],
        [
            "And I'm going to be talking about probability distributions on partitions and not just probability distribution of partitions but but but stochastic processes on partition so.",
            "So by the set script again I mean the set of all partitions of the set and subscripti 2 contains 2 elements, either the two elements belong to one block or they do not belong to separate blocks, so they're just two elements in E2E3, the set of all partitions of the set 123 contains 5 elements, either all three elements are in the same block or we have a block of size one in a block of size 2.",
            "Three different possibilities.",
            "There are all elements are in separate blocks, and likewise E, 4E Four contains 15 elements.",
            "There wasn't enough space here to write all of them down, so I have grouped them into the natural grouping.",
            "Either they're all four in one block, or there are three and 1 four types of that.",
            "Or there are two and two.",
            "Then there are three types of that.",
            "Remember the order of the blocks doesn't count.",
            "So 1 two partition 3 four is the same as 3, four partition 1 two.",
            "And then there could be 211 partitions and there are six types for that and so on.",
            "OK, so these sets get are all finite of course, but they get Big Brother quickly in there, rather awkward to deal with.",
            "So for example E5 contains 52 elements and E 6 contains 203 and so on.",
            "These are the Bell numbers.",
            "OK, so I want to be talking about probability distributions on these objects.",
            "And I want to be and I'm going to be thinking of them as a stochastic process.",
            "What does that mean?",
            "First of all, is going to be an exchangeable stochastic process.",
            "I'm going to have to define a probability distribution on every each script EN and they're going to have to.",
            "They're going to be compatible with each other under the deletion map and under permutation, so they're going to be what permuting permutations acts in the obvious way.",
            "In terms of the matrix representation, it just permutes the rows and the same permutation gets applied to the columns by conjugation essentially.",
            "And then for deletion, that's easy to describe if you have a partition of seven things, you can represent that as a 7 by 7 matrix that induces a partition of 6 things.",
            "You just remove the last row in the column.",
            "That gives you a 6 by 6 matrix, so that determines a map from E7 into E 6.",
            "The deletion map, and likewise there's a deletion map from all the way down.",
            "So these sets form was called a projective system.",
            "And to construct a stochastic process, all you have to do is construct a probability distribution on each of them, all of them being compatible with each other, and that automatically defines stochastic process and."
        ],
        [
            "I'm only going to illustrate one of these.",
            "What it means, so here is E3.",
            "I've listed the five elements of E3 Scripty three up the top here, and there's a probability distribution P3, and there's a probability distribution P3 prime, which is different and they're both invariant under permuting the elements 123.",
            "They're both finitely exchangeable.",
            "Here is another distribution on E4.",
            "I haven't listed separately.",
            "There are four elements of this type and and three of the next type and so on, so.",
            "You can extrapolate there where necessary, and if you add those up they will add up to one for P4 and will add up to one for P4 prime.",
            "And the compatibility, the Kolmogorov compatibility condition for a stochastic process is that the deletion map carries P4 to P3, which it does in fact, and so P3 is the marginal distribution of P4 and P3 prime is also the marginal distribution of P4 prime.",
            "So when you delete element number 4, you will get one of those.",
            "Partitions up at the top and if you get 123 you could have deleted 4 from here.",
            "Or you could have deleted 4 from here so you have to have the probability P3 of the single block partition being the sum of the probabilities that P4 assigned to those two elements and so on for all of the other possibilities.",
            "So that's what I mean by an exchangeable partition."
        ],
        [
            "Process.",
            "Just let's write out the full definition here.",
            "You have a whole sequence of these probability distributions.",
            "They all satisfy this compatibility condition under deletion.",
            "And once that is done, the reason for insisting on that is that you can calculate conditional distributions.",
            "If you if P4 and P5 were not compatible with each other, there would be no way of connecting the fifth element with the four element.",
            "There has to be a.",
            "There has to be a suitable sequence progressing to Infinity in order for inference to proceed.",
            "And in this and so the Commodore of compatibility condition is the criterion that allows you to do that.",
            "So you can calculate the conditional distribution.",
            "This is the conditional distribution that the next element belongs to whatever block given.",
            "The configuration of the end things that you have already observed.",
            "OK."
        ],
        [
            "No, there are obviously, I guess it's not obvious, but there are lots and lots of exchangeable partition processes, but there's one of them that plays the role of the normal distribution in for continuous observations, and that's the UN's distribution.",
            "It seems to arise everywhere you look if you know to look for it.",
            "And here it is.",
            "It's a very simple expression.",
            "This just says the probability of B, the partition B of end things is proportional to some parameter Lambda raised to the number of blocks times the product over the blocks of the gamma function applied to the number in each block.",
            "While it arises, I will explain in a moment why it arises everywhere you look.",
            "But the conditional distributions, well, you have to show that these are compatible.",
            "I've only just shown you here PN but PN plus one and PN are compatible with each other, which is by no means obvious.",
            "And if you were to try to write down.",
            "Anne.",
            "A sequence of probability distributions that are compatible.",
            "It's quite difficult thing to do.",
            "It's not something you unless you know exactly what you're doing is not something you could do in a week.",
            "The probability distributions the conditional distribution is just the ratio P N + 1 / P N evaluated at compatible partitions and the conditional distribution for the humans process is just got by this ratio.",
            "And this is just the Chinese restaurant process.",
            "This is exactly the Chinese restaurant processes says that the probability that the next element U N + 1 the next specimen, the next individual, the next whatever you want to call it.",
            "Gets assigned to Block B, is equal to the number of elements that are already in Block B.",
            "Proportional to that, if B is already a block, but if B is not already a block.",
            "Let's say if you want a new table in the Chinese restaurant description, the probability is proportional to Lambda, so there's this intensity associated with new tables, and then the existing tables they intensity is proportional to table size, so that's pitmans Chinese restaurant description of the humans process makes it very easy to construct to work with.",
            "It's a very natural.",
            "Phone type operation.",
            "It's called the humans.",
            "A process because it arises in genetics under mutation model.",
            "A neutral mutation model and humans originally introduced this in 1972, but in the context of it was described not in terms of set partitions, but in terms of integer partitions, but of course associated with any set partition.",
            "There is an integer partition, so it's just a projection downwards and you just get this awkward ugly component tutorial factor over on the right hand side.",
            "Generally speaking, set partitions are much easier to work with, although they are much more numerous.",
            "They're much easier to work with than integer partitions, so I will not deal with this sort of thing.",
            "There is also a fundamental distinction between.",
            "That partitions an integer partitions in this deletion operator that I'm describing applies to set partitions.",
            "It does not apply to integer partition, so you cannot have an exchangeable process an integer partitions, whereas you can have an exchangeable process on."
        ],
        [
            "Set partitions.",
            "OK, so why does this arise?",
            "Where else does it arise?",
            "The earliest example that I can see where it has arisen.",
            "In Fishers work in 1940, having to do with the estimating the number of unspe seen species, the idea there being that the block that you're observing butterflies in the wild.",
            "You want to know how many distinct species there are while you go out and capture a whole lot specimens and you will have the specimens will naturally fall in blocks.",
            "So many of this species so many species and so on and you and in Fisher didn't actually use.",
            "The talk in terms of partitions as such, but he did effectively the same thing using a mixture of Poisson gamma mixture model, in which which is equivalent in certain sense to the UN's process.",
            "Actually, there's a geometric decrease.",
            "Reciprocal decrease.",
            "The axes are conditionally independent.",
            "Poisson random variables, either the multiplicities.",
            "And then there's conditional distribution given the number.",
            "So that was Fisher's work in 1940.",
            "It was followed up by a good in 1953 and then local ER on David Wallace and Fred Mosteller in 1964 used essentially the same Fisher model in discrimination work on The Federalist Papers, and Efron insisted used essentially the same sort of model to estimate how many words Shakespeare knew using a stylistic.",
            "Model for Shakespearean usage is sort of stylistic model that only a statistician could appreciate, where you just count the frequency of the words and ignore the order.",
            "This model also arises in Kendall's work on family size distribution, so he Kendall was looking at the number of families in Yorkshire having a different name, so they would be sort of blocking.",
            "There was done by name and that's described in Kelly's book.",
            "The Hurons process also arises in the prime factorization of large integers.",
            "A paper by Donnelly and Grimmett.",
            "It's also a partition in just by.",
            "Additionally, process so nearly no matter what you do, you will end up with the unions process some some sort of force of convergence to the humans process.",
            "Now there are lots of exchangeable partition processes, and I could use any of them, but I'm going to use the."
        ],
        [
            "Particular one because it's it's so simple.",
            "There's also another characterization due to all this, which has to do with a lack of memory property.",
            "Which is similar in some sense to self similarity property, but some similar to the lack of memory property of the exponential distribution, which says that given that the partition lies below some.",
            "Fixed partition what is the conditional distribution and the unions property?",
            "Has the unions distribution has the property that is just a product form of the same type?",
            "And let's see this uniquely characterizes the.",
            "The Hurons distribution is not properties two and three.",
            "There look like they're the same thing, but they are in fact different."
        ],
        [
            "OK, So what I'm going to use that now in a way not very dissimilar to the way that Michael Jordan uses the same.",
            "These originally processes to construct a very simple cluster process by a cluster process.",
            "I mean the following three parts to it.",
            "First of all, they have to be an index set.",
            "You start off with that and get your foundation set.",
            "That's just going to be 123.",
            "For up to Infinity, then there's going to be a random sequence.",
            "These are the feature values taking values in Rd.",
            "And then there is a random partition of that infinite set, and that's going to be an exchangeable random partition.",
            "Note that it's not a partition of the feature space.",
            "I'm not talking about partitioning the feature space at all.",
            "I'm talking about partitioning the units, partitioning the specimens.",
            "And a sample is just a finite subset of the specimens and that will have probability distribution.",
            "The partition will have probability distribution PN.",
            "So the guys humans process says.",
            "What is it?",
            "Distribution of the partition restricted to the sample that B square brackets N?",
            "What is the probability distribution of the partition restricted to the sample?",
            "Well, that's the unions distribution.",
            "That's the first line.",
            "And then what is the probability distribution or density of the feature values on the sampled units given the infinite partition?",
            "That's this normal distribution.",
            "Which is well, zero main would be fine.",
            "All the means are the same.",
            "And then there's there's two bits.",
            "There's a non IID part, and then there's a bit that's constant on blocks.",
            "And there's a covariance matrix between blocks, which is.",
            "Sigma Prime and a covariance matrix within blocks, which is Sigma.",
            "Those are parameters that will have to be estimated, of course, as will mu and a couple of other things.",
            "So this defines an exchangeable partition process in which for any finite set in, you just get to observe the feature values for that sample and the partition on that sample, and then for classification you get the feature value for a new individual, and you want to say what the.",
            "The classes are."
        ],
        [
            "OK. Militz"
        ],
        [
            "So you could construct the Gaussian process, just define it in a different way in terms of random variables I have so far just."
        ],
        [
            "But here this is the definition in terms of probability distribution.",
            "No, no real random variables."
        ],
        [
            "Such here, but you could construct define it, describe the same thing in terms of random variables you have these eaters which are going to define the cluster centers.",
            "These are random variables.",
            "And then the epsilons, which are, if you like the residuals, which are an infinite sequence, Gaussian sequence, independent aveta.",
            "Then you have the Chinese restaurant process.",
            "That's this, says the table to which individual I plus one is assigned, is just choose one of the.",
            "The tables to which the previous I individuals have been assigned with equal probability or else you choose a new table.",
            "That's just a sequential description of the Chinese restaurant process, and then the observation consists of the block factor here and the feature values, why?",
            "The parameters I said are they Lambda, mu and the two.",
            "Variance components Now this is the simplest way to combine these.",
            "There are more esoteric structures that can be constructed this way and I will illustrate a couple of them in a moment, but their got by putting more information into the eater and having a nonlinear function G there, so it's not just a dish."
        ],
        [
            "And.",
            "So this is what the realization of the Gaussian process looks like in R2.",
            "So everything got turned upside down here for complicated reasons, but it doesn't affect the distribution.",
            "The distribution is still the same, so in the top row what I have shown here I've got.",
            "I think it's about 500 a sample of size 500 from this process and I have put the first 250 of them on the left hand side and then the next 250 in the same trajectory of the same process on the right.",
            "So from the left to the right you can see that the pattern is essentially the same.",
            "I don't know if you can see the colors there or not, but there should be five colors on the left and the same five colors should appear on the right.",
            "In roughly the same configuration.",
            "The bottom panel is done in exactly the same way.",
            "If it's just a bigger value of Lambda, so the bigger the value of Lambda, the more blocks there will be enough in a number of blocks is roughly Lambda log in an is about I think it's 4 or 500 here.",
            "So here we have.",
            "There should be 10 blocks here in this 250 observations and then in the next 250 observations.",
            "There are in fact 11 blocks, so probably 1 new block has appeared.",
            "And that's the sort of thing we should expect.",
            "All of these have mean zero.",
            "These are every every point.",
            "Here is a Gaussian variable of mean zero.",
            "It's just that they are correlated, and it's the correlations that are inducing these clusters.",
            "The clusters are central cluster centers are random variables, so there are in fact only in a sense.",
            "There are only 10 random variables here.",
            "If you look at think of these as cluster centers.",
            "And so on."
        ],
        [
            "Just a slight variation on this.",
            "Make it a little bit more interesting.",
            "You can introduce clusters and sub clusters, so let's just go back to."
        ],
        [
            "What I mean by that?"
        ],
        [
            "Just describe it in terms of distributions.",
            "Once you've got the partition B capital N, you can subdivide every block by another.",
            "You in this process and introduce a third variance component so that each cluster gets split up into sub clusters and then and then instead of having two variance components here we will have a B and and a subpartition of be multiplied by another covariance matrix so that."
        ],
        [
            "What I mean by clusters and sub clusters and the process will look like this.",
            "So if you look here, the coloring here indicates the major clusters.",
            "So if you look down at the bottom here, I don't know what color this is, but looks like there are three subclusters here.",
            "I don't know for sure, but it looks like there are three and then the same three in the same trajectory in the trajectory of this process, the next 250 points the same three clusters persist.",
            "And likewise, you can probably see the same pattern up in the top.",
            "In various places.",
            "OK, and you can make that even more complicated, but I will.",
            "2 levels is usually enough.",
            "Faster become empty.",
            "Master, by definition is nonempty.",
            "So there are six clusters here and then in the next 250 points, not all of those clusters need to occur, so I guess there is upside down.",
            "This looks like a 9, but in fact the six I think, and so five over there telling me that only five of them and you might want to looks like this is a one element cluster.",
            "It doesn't seem to occur over there.",
            "I guess that.",
            "So you don't lose them as such, but you don't necessarily bear in mind that the population here has infinitely many clusters that if you ask how many clusters are there, the answer is easy infinite.",
            "How many are there in the sample?",
            "That's the hard problem.",
            "Well, you get to observe.",
            "That's a finite number."
        ],
        [
            "And if you want to make it a little bit more esoteric, you can do things.",
            "Construct Gaussian processes that have end up with configurations like this with a nonlinear G so that it can make all these squiggles, and so on."
        ],
        [
            "OK, so now what about classification?",
            "There's no set of classes in this.",
            "Bear in mind that the B is just.",
            "A boolean matrix that tells you whether two individuals belong to the same class or not.",
            "There's no labeled set of classes, so when you talk about classification, you have to be a little bit careful what you mean by it, but you can calculate the conditional distributions, and this is the conditional distribution.",
            "Given the training data, that's that's the Y&B for the units in the sample, plus the features for the new individual.",
            "YN plus one or Y primer has there that will be just calculate that conditional distribution and that will be defined.",
            "There will be a conditional distribution of that sort for be in one of the observed blocks or not.",
            "So in some sense, the lack of a set of classes is an advantage here, because it allows you possibility of assigning a new individual to a class that you haven't."
        ],
        [
            "Observed in this training sample.",
            "So here is the explicit calculation.",
            "It's not a difficult calculation to do.",
            "Computationally, it's easy, but you can actually do it explicitly with a little bit of a simplification up there and it just says that the probability that the new element is assigned to block B.",
            "Of the existing partition is proportional to the first of all is proportional to two things, is proportional to the block size.",
            "That's the number of be multiplied by the Gaussian density evaluated under at this point.",
            "With this particular covariance matrix, which are all written down there, and so that's part of the answer and the other bit is for B being a block that's not previously observed in the training sample.",
            "The probability is proportional to that, and so you calculate all of these and add them up and divide.",
            "That gives you the probability distribution for the new individual.",
            "It's a rule.",
            "Let's essentialness of the same structure as the Chinese restaurant process, but a little bit more difficult, little bit more difficult to compute.",
            "And of course to to use this, you have to estimate a few of the parameters that the covariance matrices and the Lambda and so on, but those are not.",
            "Those are very easy things to do.",
            "This becomes a little bit more interesting and I don't have an example to show you, but if you have the notion of classes and subclasses, there are more empty sets around, so you can you can end up with the conclusion that this new individual looks absolutely nothing like anything we have seen in the training sample.",
            "That could be one, but you could end up with a conclusion.",
            "That this individual looks like one of these big blocks that I saw in the training sample, but it doesn't look like any of the subblocks.",
            "That's another sort of empty set, so there are more, so it's it's of this major species, but not one of the observed sub species.",
            "And then that will emerge naturally in in the conditional calculation."
        ],
        [
            "OK, so here's the sort of just getting back to the simple version of the Gaussian process.",
            "So the observed data, or over here on the right, the training data and all I can illustrate here is just the block that has maximum conditional probability.",
            "So that's saying that let's look down here if the new individual falls over here.",
            "Then the probability that he is assigned to block the blue block.",
            "Is bigger than the probability that he is assigned to any of the other blocks?",
            "Doesn't have to be bigger than 1/2, of course, but if you just look at the block that has the maximum conditional probability, that will of course generate a partition of the feature space.",
            "But you also have all of this empty space here, in which they knew class has higher probability than any of the existing classes, and that's a very natural sort of thing.",
            "If you observe the new individual over here, well, it seems natural to say it doesn't look like anything we've observed.",
            "And you would want that to be."
        ],
        [
            "The other case, OK, that is the that is what I regard as the classical model for classification.",
            "It's not fundamentally different from the Fishers model.",
            "By going back to the 1940s.",
            "That it does have this issue of being able to assign the new individual to a previously unseen class.",
            "Which Fishers model does not.",
            "But apart from that the answers are pretty much the same as a fisherian model.",
            "I want to.",
            "Now turn to a different sort of completely different sort of model.",
            "It's based on a completely different structure involving point processes.",
            "OK, so begin in the same way we have got a finite set of classes of feature space and now to generate.",
            "To construct a generative model, I'm going to proceed as follows.",
            "I make a make myself an intensity function is going to be a random intensity function on the product space C * S for classes and features.",
            "And then given lambda's at is a point process with parameter Lambda that intensity.",
            "So then each, then we observe there has to be a description of how things get observed and that description has to be such that the number of observed points is finite.",
            "So there is a finiteness issue.",
            "So you have to make sure that things are finite, either by embedding this in time or by looking at a window in which things are finite.",
            "OK, so then you get to observe this bold set, which is a subset of consisting of classes and features, and then every event consists of a Class A pair of class and a feature X&Y.",
            "We get to observe bold set on some region finite time.",
            "Then there has to be a notion of a next event, otherwise the whole thing doesn't make any sense.",
            "There's no issue of classification at all and we ask what is the conditional distribution of the next event given the data.",
            "And for a variety of reasons this looks like it's going to be just an exchangeable process, but it's not.",
            "There's no index set.",
            "And the rate of occurrence of events is random and."
        ],
        [
            "So on so let's just go through this and go through the details of this.",
            "The probability distributions for these are Cox processes, conditionally Poisson processes.",
            "So the observation is a finite point, processes conditionally Poisson.",
            "It's I will call it a configuration configuration just means a finite is the set of all the configuration space is the set of all finite subsets of this general space Script X, which will be the product space features, times, classes.",
            "Fan for it to be a probability distribution.",
            "The sample size, the number of events is going to be random.",
            "And the integral over the integral over the feature space.",
            "Someone want to end the integral is equal to 1.",
            "That's just says there's a probability distribution X.",
            "Here should be thought of as a set.",
            "It's not an ordered set, it's just a set.",
            "There's no first element, second element, and so on.",
            "And the actual formula for PN is that thing up there.",
            "It's saying if you look at it carefully, it's saying the probability of observing points X one up to XN.",
            "Well, it's just the intensity at the product of the intensities.",
            "But not only do you get to observe points at X1 up to XN, there are no points anywhere else.",
            "So that's where this E to the minus, Lambda S comes from.",
            "The empty space is part of the observation as well.",
            "And what is it then?",
            "The plays the role of predictive distribution.",
            "It's it's this pop and Gallo conditional intensity.",
            "It's also the Bayes estimate of Lambda, so that ratio it looks like a conditional probability distribution, but it isn't because this doesn't integrate to one.",
            "This is a conditional intensity, so it has to be normalized."
        ],
        [
            "I'm going to describe one such example where this calculation can be done explicitly, it involves.",
            "What's called the permanent point process.",
            "So let me introduce some terminology about permanence.",
            "The Alpha permanent of a matrix is just the sum over permutations Alpha raised to the number of cycles in the permutation times a product like a determinant.",
            "So it's just like a determinant, except that the plus minus signs are replaced by this Alpha to the power.",
            "So if Alpha is minus one, this is essentially a determinant.",
            "If Alpha is plus one, that's essentially the ordinary permanent.",
            "And another expression that arises here is.",
            "That's the limit of the previous one as Alpha goes to zero and I just called up this sum of cyclic products, which is the sum over cyclic permutations only.",
            "So N -- 1 factorial terms.",
            "So these two terms come up in the distribution probability distribution that I'll describe in a moment.",
            "It's rather difficult in general to compute these except for the very simplest of matrices.",
            "And here's an example, the permanent of the matrix N by N matrix of all ones is just this gamma function ratio of gamma functions, which is exactly the normalizing constant that occurs in the in the humans process, and there's a close relationship here with the humans process, these permanents have this interesting convolution property, so they are connected with binomial and multinomial.",
            "Probability distributions and with partition distributions.",
            "So all those are difficult to compute.",
            "They are quite interesting probabilistically."
        ],
        [
            "So the bozone point process is constructed by follows.",
            "You get yourself a positive definite function.",
            "Construct a Gaussian process covariance function K / 2 you do all of this and you generate the process with that random intensity which is sum of squared Gaussians.",
            "Now you could say why don't you just exponentiate rather than take a sum of squares?",
            "Well, that's another Gaussian.",
            "That's another Cox process, the log gamma.",
            "Cox process, but it's very difficult to do anything with it now.",
            "This one's not easy either, but this one at least you can make some progress with.",
            "You can actually write down the cumulant density, the moment density, and the probability density, all in principle computable, although perhaps not in the polynomial time.",
            "But that's another matter."
        ],
        [
            "The multi class model that I will use just has independent intensities for each class.",
            "So the probability distribution will just be a product like this over the various classes will be an A1 for class one up to Alpha, K for class K, and then the superposition process will also be a bowsan process.",
            "And this same constant arises here, and the parameter is just the sum of the alphas.",
            "So now why is feet?"
        ],
        [
            "Vendice the classification distribution now looks like this given the training data consisting of classes X features Y and a new feature, the probability that the next individual gets assigned to Class R is proportional to that permanent ratio.",
            "No possibility here of assigning things to an unseen class because for the moment and unseen there's no such thing.",
            "But if you consider the limit as what's the limit, no limit that's described here, then most of the classes will not be represented in the sample and you can do the same thing.",
            "And now this possibility of an unseen class does feature there is that Lambda K~ of why what?",
            "But in any case, that looks pretty much the same, except that here you have got these permanence.",
            "And here you have got cyclic products only."
        ],
        [
            "Jay so I thought I would illustrate this with the help of my colleague Jiyong.",
            "So we constructed a data set looking like like so looks a lot darker here on the screen than it does on my computer, but these are red and blue.",
            "And, well, you can't actually see the Reds, but you know they're looking at the edges here.",
            "But there are.",
            "There are this checkerboard pattern three by three checkerboard pattern.",
            "There are 10 points in each little square uniform independent, so they're not generated according to the model.",
            "Some parameters need to be estimated.",
            "We do that by cross validation.",
            "The covariance function is the one up in the top player.",
            "They told parameter the range parameter is about point 7.47.",
            "I know we want to see what is.",
            "How does this work for classification?",
            "Well, first of all, if you were to plug this data set into the Gaussians, processes would say like there's nothing I can do here because the Gaussian process would not just not work with this sort of thing, because all the clusters in the God students process have to be compact and not mixed up like this."
        ],
        [
            "OK, so this is what it looks like.",
            "This is the probability distribution is the probability of red given the data for all of the various points in this unit square with the parameters estimated by cross validation and the probability of red ranges from about somewhere about 5% to 95% from the darkest red to the darkest blue.",
            "I should say these this permanent ratio in this was computed analytically.",
            "By an approximation or I should say approximated, not computed.",
            "There are other ways of doing it, more or less approximately more or less exactly by Markov chain methods and Monte Carlo methods of other sorts.",
            "But the approximation seems to work reasonably well, at least for this purpose, involving the matrices that are involved here are going to be of order.",
            "Well, let's see how many Reds are 50 Reds there, so 50 by 50 and 40 by 40 for the Blues.",
            "OK, so that's how it looks.",
            "You can see here that there are some misclassifications in the sense that this triangle, which is really a blue point.",
            "Yeah, if something falls in that neighborhood, something falls close to here.",
            "It would be assigned with probability .55 roughly to the red, where in fact that's part of the blue region button on the whole for 10 points in each small square.",
            "This is not too doesn't do it bad job of reconstructing the."
        ],
        [
            "Checkerboard pattern.",
            "OK, I'll finish up now by just saying a little bit about algorithms for approximation of permanents.",
            "Computer scientists are all well aware that these permanents are sensibly uncomputable in the traditional sense.",
            "For anything bigger than about 20 or 30.",
            "The top two algorithms, which are exact or just not good enough to for the sorts of purposes that we need here, there are stochastic approximation algorithms, one of them by Sam Cohen myself, which seems to work well, I should say, incidentally, that we don't actually need to calculate permanent in this.",
            "In this we only need to calculate permanent ratios, and that's a lot easier than calculating the permanent itself.",
            "So generally speaking, these are probably approximation.",
            "This is not as bad as it seems.",
            "And as I said, we used a truncated cycle expansion.",
            "Naive but not too bad."
        ],
        [
            "And I will just finish up with reference."
        ],
        [
            "This here is a pile of references."
        ],
        [
            "And I will leave it at that.",
            "Thank you.",
            "Yes, Charlie.",
            "Mention of the features or it doesn't affect your conversation.",
            "Doesn't affect the computation at all.",
            "Is that the key metrics?",
            "Once you have, once you've got your distance matrix in your K matrix, then you can forget about the feature space."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vacation models.",
                    "label": 0
                },
                {
                    "sent": "Thank you Misha and.",
                    "label": 0
                },
                {
                    "sent": "I want to thank the organizers for the invitation to speak here.",
                    "label": 0
                },
                {
                    "sent": "Parser mission, and Steve.",
                    "label": 0
                },
                {
                    "sent": "It's a great pleasure, not far from me to come.",
                    "label": 0
                },
                {
                    "sent": "Kelly and I are both locals here, so it's very convenient.",
                    "label": 0
                },
                {
                    "sent": "One of the things that I wanted to learn when I came here was what machine learning is all about.",
                    "label": 0
                },
                {
                    "sent": "Or computational learning?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I had initially thought that machine learning was about classification.",
                    "label": 0
                },
                {
                    "sent": "At least that's I'm usually five years behind the times, and so that's what it used to mean, and so I thought that that's what it still meant, and so I decided to focus my talk around that theme.",
                    "label": 0
                },
                {
                    "sent": "But I've learned since that it's about other things as well.",
                    "label": 0
                },
                {
                    "sent": "Estimation somebody told me it's about statistical estimation.",
                    "label": 0
                },
                {
                    "sent": "Banyu tells me it's.",
                    "label": 0
                },
                {
                    "sent": "Statistics was computation, which begs the question, what is statistics without computation?",
                    "label": 0
                },
                {
                    "sent": "I don't know, and for others it's about algorithms perhaps.",
                    "label": 0
                },
                {
                    "sent": "But whatever it is about, I'm going to be talking about classification.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to distinguish between classification in a deterministic sense and classification in a statistical sense.",
                    "label": 0
                },
                {
                    "sent": "So the the way that I'm going to formulate the problem is here that we have what we call training data on N specimens or units, whatever they may represent, and we on every unit there is observed feature values which live in some feature space, which in all of the examples here will just be Euclidean space might have to be transformed to be in that space as we saw in last talk.",
                    "label": 0
                },
                {
                    "sent": "And then there is this issue that there comes along a new unit.",
                    "label": 0
                },
                {
                    "sent": "So there has to be in the set up, not just the training data, not just a training unit.",
                    "label": 0
                },
                {
                    "sent": "The sample of training units, but whole our whole flow sequence of units.",
                    "label": 0
                },
                {
                    "sent": "So given a new unit with whatever feature values it has.",
                    "label": 1
                },
                {
                    "sent": "You want to compute the probability that they knew unit belongs to whatever classes are possible.",
                    "label": 0
                },
                {
                    "sent": "So I distinguish between classification in the deterministic sense where you declare this unit belongs to class X.",
                    "label": 0
                },
                {
                    "sent": "With statistical classification, where you say the probability distribution for this unit is 30% for this class, 25% for Class 2 and so on.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be looking at this in the sense of.",
                    "label": 1
                },
                {
                    "sent": "Statistical classification are probabilistic classification and in all cases the classes will be either a fixed set, finite set of labels such as zero to 9 or the Letters A-Z.",
                    "label": 1
                },
                {
                    "sent": "Or some more or less open set, which will be the blocks of the partition B that's observed.",
                    "label": 0
                },
                {
                    "sent": "So we get to observe that I'm talking here about.",
                    "label": 0
                },
                {
                    "sent": "Cases where the classes are in the training sample are observed.",
                    "label": 0
                },
                {
                    "sent": "So the the classes to which the new unit could be long will be the classes that are observed in the training sample.",
                    "label": 0
                },
                {
                    "sent": "And there is this possibility of the classes being open that you might want to assign.",
                    "label": 0
                },
                {
                    "sent": "You might want to say, well, yes, certainly this is 1.",
                    "label": 0
                },
                {
                    "sent": "This is something this is a new digit, but if it happens to look like an X you wouldn't want necessarily to assign it to one of the classes zero through 9 and you'd want to end up with putting a very high probability.",
                    "label": 0
                },
                {
                    "sent": "That is an image that you haven't seen.",
                    "label": 0
                },
                {
                    "sent": "A class that you haven't seen before.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to be interested.",
                    "label": 0
                },
                {
                    "sent": "I'm interested in situations where there is appreciable variation and where the sample is not so huge.",
                    "label": 0
                },
                {
                    "sent": "That you can learn anything with extremely high probability at all.",
                    "label": 0
                },
                {
                    "sent": "So the variation is going to be appreciable.",
                    "label": 0
                },
                {
                    "sent": "So I'm not talking about extremely large samples.",
                    "label": 0
                },
                {
                    "sent": "And we for this purpose the only way that I know how to model this is to use a stochastic a generative model stochastic process, which will either be IID or exchangeable, or something else, as we will see towards the end.",
                    "label": 0
                },
                {
                    "sent": "I'm going to describe two distinct classes.",
                    "label": 0
                },
                {
                    "sent": "Of classification models, one being an exchangeable model, which I call the Gaussians model, and another one which is not an exchangeable model, but it's of a different type based on what point processes.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so both of these models are going to involve the notion of a partition.",
                    "label": 0
                },
                {
                    "sent": "So by a partition I don't mean a partition of the feature space, I mean a partition of the specimens.",
                    "label": 0
                },
                {
                    "sent": "So a partition just go through it in the abstract.",
                    "label": 0
                },
                {
                    "sent": "Here if we have got a finite set of N. Just integers one up to NA partition of N, which I will denote always by B is one of three things, or it's all of three things.",
                    "label": 0
                },
                {
                    "sent": "It's a, it's a set of disjoint nonempty subsets whose union is the whole set.",
                    "label": 1
                },
                {
                    "sent": "So I will typically write it as a as a set of sets with vertical bars separating the blocks, but you have to bear in mind that the order within the blocks is to be disregarded and the order of the blocks is also to be disregarded.",
                    "label": 0
                },
                {
                    "sent": "So 246 partition, 1 three partition, 5 up here on the right top right is the same as reordering if the blocks are reordered and the within the blocks is reordered as well.",
                    "label": 0
                },
                {
                    "sent": "So a partition is also an equivalence relation.",
                    "label": 0
                },
                {
                    "sent": "I will use the same notation regardless of how I think about this thing, so it's an equivalence relation on the set N Boolean relation is reflexive, symmetric and transitive.",
                    "label": 0
                },
                {
                    "sent": "And it's also of course, abulia this matrix, so these three.",
                    "label": 0
                },
                {
                    "sent": "Representations of be set of sets.",
                    "label": 1
                },
                {
                    "sent": "Set of subsets that should say an equivalence relation in this Boolean matrix.",
                    "label": 0
                },
                {
                    "sent": "We're all going to be represented in the same way, and so when the symbol B comes up, you just have to think of it in whichever way is the most convenient, so the number B refers to the number of elements, which is the number of blocks.",
                    "label": 0
                },
                {
                    "sent": "Or if you want to think of it in terms of the matrix, it's the rank, I suppose.",
                    "label": 0
                },
                {
                    "sent": "And then for each element in B, that's a subset, the number of the little B is the number of elements.",
                    "label": 1
                },
                {
                    "sent": "So when I talk about partitions, I will be talking about set partitions.",
                    "label": 0
                },
                {
                    "sent": "I will usually not be talking about integer partitions, but associated with every set partition.",
                    "label": 0
                },
                {
                    "sent": "There is an integer partition, was just disregards the elements, but just counts the block sizes.",
                    "label": 0
                },
                {
                    "sent": "So the integer partition associated with this be that I've shown here.",
                    "label": 0
                },
                {
                    "sent": "1 block of size 1, one of size 2 and one of size 3 is the integer partition 1 + 2 + 3.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'm going to be talking about probability distributions on partitions and not just probability distribution of partitions but but but stochastic processes on partition so.",
                    "label": 1
                },
                {
                    "sent": "So by the set script again I mean the set of all partitions of the set and subscripti 2 contains 2 elements, either the two elements belong to one block or they do not belong to separate blocks, so they're just two elements in E2E3, the set of all partitions of the set 123 contains 5 elements, either all three elements are in the same block or we have a block of size one in a block of size 2.",
                    "label": 1
                },
                {
                    "sent": "Three different possibilities.",
                    "label": 0
                },
                {
                    "sent": "There are all elements are in separate blocks, and likewise E, 4E Four contains 15 elements.",
                    "label": 0
                },
                {
                    "sent": "There wasn't enough space here to write all of them down, so I have grouped them into the natural grouping.",
                    "label": 0
                },
                {
                    "sent": "Either they're all four in one block, or there are three and 1 four types of that.",
                    "label": 0
                },
                {
                    "sent": "Or there are two and two.",
                    "label": 1
                },
                {
                    "sent": "Then there are three types of that.",
                    "label": 1
                },
                {
                    "sent": "Remember the order of the blocks doesn't count.",
                    "label": 0
                },
                {
                    "sent": "So 1 two partition 3 four is the same as 3, four partition 1 two.",
                    "label": 0
                },
                {
                    "sent": "And then there could be 211 partitions and there are six types for that and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so these sets get are all finite of course, but they get Big Brother quickly in there, rather awkward to deal with.",
                    "label": 0
                },
                {
                    "sent": "So for example E5 contains 52 elements and E 6 contains 203 and so on.",
                    "label": 0
                },
                {
                    "sent": "These are the Bell numbers.",
                    "label": 0
                },
                {
                    "sent": "OK, so I want to be talking about probability distributions on these objects.",
                    "label": 0
                },
                {
                    "sent": "And I want to be and I'm going to be thinking of them as a stochastic process.",
                    "label": 0
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "First of all, is going to be an exchangeable stochastic process.",
                    "label": 0
                },
                {
                    "sent": "I'm going to have to define a probability distribution on every each script EN and they're going to have to.",
                    "label": 0
                },
                {
                    "sent": "They're going to be compatible with each other under the deletion map and under permutation, so they're going to be what permuting permutations acts in the obvious way.",
                    "label": 0
                },
                {
                    "sent": "In terms of the matrix representation, it just permutes the rows and the same permutation gets applied to the columns by conjugation essentially.",
                    "label": 1
                },
                {
                    "sent": "And then for deletion, that's easy to describe if you have a partition of seven things, you can represent that as a 7 by 7 matrix that induces a partition of 6 things.",
                    "label": 1
                },
                {
                    "sent": "You just remove the last row in the column.",
                    "label": 0
                },
                {
                    "sent": "That gives you a 6 by 6 matrix, so that determines a map from E7 into E 6.",
                    "label": 0
                },
                {
                    "sent": "The deletion map, and likewise there's a deletion map from all the way down.",
                    "label": 0
                },
                {
                    "sent": "So these sets form was called a projective system.",
                    "label": 0
                },
                {
                    "sent": "And to construct a stochastic process, all you have to do is construct a probability distribution on each of them, all of them being compatible with each other, and that automatically defines stochastic process and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm only going to illustrate one of these.",
                    "label": 0
                },
                {
                    "sent": "What it means, so here is E3.",
                    "label": 0
                },
                {
                    "sent": "I've listed the five elements of E3 Scripty three up the top here, and there's a probability distribution P3, and there's a probability distribution P3 prime, which is different and they're both invariant under permuting the elements 123.",
                    "label": 0
                },
                {
                    "sent": "They're both finitely exchangeable.",
                    "label": 0
                },
                {
                    "sent": "Here is another distribution on E4.",
                    "label": 0
                },
                {
                    "sent": "I haven't listed separately.",
                    "label": 0
                },
                {
                    "sent": "There are four elements of this type and and three of the next type and so on, so.",
                    "label": 0
                },
                {
                    "sent": "You can extrapolate there where necessary, and if you add those up they will add up to one for P4 and will add up to one for P4 prime.",
                    "label": 0
                },
                {
                    "sent": "And the compatibility, the Kolmogorov compatibility condition for a stochastic process is that the deletion map carries P4 to P3, which it does in fact, and so P3 is the marginal distribution of P4 and P3 prime is also the marginal distribution of P4 prime.",
                    "label": 1
                },
                {
                    "sent": "So when you delete element number 4, you will get one of those.",
                    "label": 0
                },
                {
                    "sent": "Partitions up at the top and if you get 123 you could have deleted 4 from here.",
                    "label": 0
                },
                {
                    "sent": "Or you could have deleted 4 from here so you have to have the probability P3 of the single block partition being the sum of the probabilities that P4 assigned to those two elements and so on for all of the other possibilities.",
                    "label": 0
                },
                {
                    "sent": "So that's what I mean by an exchangeable partition.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Process.",
                    "label": 0
                },
                {
                    "sent": "Just let's write out the full definition here.",
                    "label": 0
                },
                {
                    "sent": "You have a whole sequence of these probability distributions.",
                    "label": 0
                },
                {
                    "sent": "They all satisfy this compatibility condition under deletion.",
                    "label": 1
                },
                {
                    "sent": "And once that is done, the reason for insisting on that is that you can calculate conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "If you if P4 and P5 were not compatible with each other, there would be no way of connecting the fifth element with the four element.",
                    "label": 0
                },
                {
                    "sent": "There has to be a.",
                    "label": 0
                },
                {
                    "sent": "There has to be a suitable sequence progressing to Infinity in order for inference to proceed.",
                    "label": 0
                },
                {
                    "sent": "And in this and so the Commodore of compatibility condition is the criterion that allows you to do that.",
                    "label": 0
                },
                {
                    "sent": "So you can calculate the conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "This is the conditional distribution that the next element belongs to whatever block given.",
                    "label": 1
                },
                {
                    "sent": "The configuration of the end things that you have already observed.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, there are obviously, I guess it's not obvious, but there are lots and lots of exchangeable partition processes, but there's one of them that plays the role of the normal distribution in for continuous observations, and that's the UN's distribution.",
                    "label": 0
                },
                {
                    "sent": "It seems to arise everywhere you look if you know to look for it.",
                    "label": 0
                },
                {
                    "sent": "And here it is.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple expression.",
                    "label": 0
                },
                {
                    "sent": "This just says the probability of B, the partition B of end things is proportional to some parameter Lambda raised to the number of blocks times the product over the blocks of the gamma function applied to the number in each block.",
                    "label": 0
                },
                {
                    "sent": "While it arises, I will explain in a moment why it arises everywhere you look.",
                    "label": 0
                },
                {
                    "sent": "But the conditional distributions, well, you have to show that these are compatible.",
                    "label": 1
                },
                {
                    "sent": "I've only just shown you here PN but PN plus one and PN are compatible with each other, which is by no means obvious.",
                    "label": 0
                },
                {
                    "sent": "And if you were to try to write down.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "A sequence of probability distributions that are compatible.",
                    "label": 0
                },
                {
                    "sent": "It's quite difficult thing to do.",
                    "label": 0
                },
                {
                    "sent": "It's not something you unless you know exactly what you're doing is not something you could do in a week.",
                    "label": 0
                },
                {
                    "sent": "The probability distributions the conditional distribution is just the ratio P N + 1 / P N evaluated at compatible partitions and the conditional distribution for the humans process is just got by this ratio.",
                    "label": 0
                },
                {
                    "sent": "And this is just the Chinese restaurant process.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the Chinese restaurant processes says that the probability that the next element U N + 1 the next specimen, the next individual, the next whatever you want to call it.",
                    "label": 0
                },
                {
                    "sent": "Gets assigned to Block B, is equal to the number of elements that are already in Block B.",
                    "label": 0
                },
                {
                    "sent": "Proportional to that, if B is already a block, but if B is not already a block.",
                    "label": 0
                },
                {
                    "sent": "Let's say if you want a new table in the Chinese restaurant description, the probability is proportional to Lambda, so there's this intensity associated with new tables, and then the existing tables they intensity is proportional to table size, so that's pitmans Chinese restaurant description of the humans process makes it very easy to construct to work with.",
                    "label": 0
                },
                {
                    "sent": "It's a very natural.",
                    "label": 0
                },
                {
                    "sent": "Phone type operation.",
                    "label": 0
                },
                {
                    "sent": "It's called the humans.",
                    "label": 0
                },
                {
                    "sent": "A process because it arises in genetics under mutation model.",
                    "label": 0
                },
                {
                    "sent": "A neutral mutation model and humans originally introduced this in 1972, but in the context of it was described not in terms of set partitions, but in terms of integer partitions, but of course associated with any set partition.",
                    "label": 0
                },
                {
                    "sent": "There is an integer partition, so it's just a projection downwards and you just get this awkward ugly component tutorial factor over on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "Generally speaking, set partitions are much easier to work with, although they are much more numerous.",
                    "label": 1
                },
                {
                    "sent": "They're much easier to work with than integer partitions, so I will not deal with this sort of thing.",
                    "label": 0
                },
                {
                    "sent": "There is also a fundamental distinction between.",
                    "label": 1
                },
                {
                    "sent": "That partitions an integer partitions in this deletion operator that I'm describing applies to set partitions.",
                    "label": 0
                },
                {
                    "sent": "It does not apply to integer partition, so you cannot have an exchangeable process an integer partitions, whereas you can have an exchangeable process on.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Set partitions.",
                    "label": 0
                },
                {
                    "sent": "OK, so why does this arise?",
                    "label": 0
                },
                {
                    "sent": "Where else does it arise?",
                    "label": 0
                },
                {
                    "sent": "The earliest example that I can see where it has arisen.",
                    "label": 0
                },
                {
                    "sent": "In Fishers work in 1940, having to do with the estimating the number of unspe seen species, the idea there being that the block that you're observing butterflies in the wild.",
                    "label": 0
                },
                {
                    "sent": "You want to know how many distinct species there are while you go out and capture a whole lot specimens and you will have the specimens will naturally fall in blocks.",
                    "label": 0
                },
                {
                    "sent": "So many of this species so many species and so on and you and in Fisher didn't actually use.",
                    "label": 0
                },
                {
                    "sent": "The talk in terms of partitions as such, but he did effectively the same thing using a mixture of Poisson gamma mixture model, in which which is equivalent in certain sense to the UN's process.",
                    "label": 0
                },
                {
                    "sent": "Actually, there's a geometric decrease.",
                    "label": 0
                },
                {
                    "sent": "Reciprocal decrease.",
                    "label": 0
                },
                {
                    "sent": "The axes are conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "Poisson random variables, either the multiplicities.",
                    "label": 0
                },
                {
                    "sent": "And then there's conditional distribution given the number.",
                    "label": 1
                },
                {
                    "sent": "So that was Fisher's work in 1940.",
                    "label": 0
                },
                {
                    "sent": "It was followed up by a good in 1953 and then local ER on David Wallace and Fred Mosteller in 1964 used essentially the same Fisher model in discrimination work on The Federalist Papers, and Efron insisted used essentially the same sort of model to estimate how many words Shakespeare knew using a stylistic.",
                    "label": 1
                },
                {
                    "sent": "Model for Shakespearean usage is sort of stylistic model that only a statistician could appreciate, where you just count the frequency of the words and ignore the order.",
                    "label": 0
                },
                {
                    "sent": "This model also arises in Kendall's work on family size distribution, so he Kendall was looking at the number of families in Yorkshire having a different name, so they would be sort of blocking.",
                    "label": 0
                },
                {
                    "sent": "There was done by name and that's described in Kelly's book.",
                    "label": 0
                },
                {
                    "sent": "The Hurons process also arises in the prime factorization of large integers.",
                    "label": 0
                },
                {
                    "sent": "A paper by Donnelly and Grimmett.",
                    "label": 0
                },
                {
                    "sent": "It's also a partition in just by.",
                    "label": 0
                },
                {
                    "sent": "Additionally, process so nearly no matter what you do, you will end up with the unions process some some sort of force of convergence to the humans process.",
                    "label": 0
                },
                {
                    "sent": "Now there are lots of exchangeable partition processes, and I could use any of them, but I'm going to use the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Particular one because it's it's so simple.",
                    "label": 0
                },
                {
                    "sent": "There's also another characterization due to all this, which has to do with a lack of memory property.",
                    "label": 0
                },
                {
                    "sent": "Which is similar in some sense to self similarity property, but some similar to the lack of memory property of the exponential distribution, which says that given that the partition lies below some.",
                    "label": 1
                },
                {
                    "sent": "Fixed partition what is the conditional distribution and the unions property?",
                    "label": 0
                },
                {
                    "sent": "Has the unions distribution has the property that is just a product form of the same type?",
                    "label": 0
                },
                {
                    "sent": "And let's see this uniquely characterizes the.",
                    "label": 0
                },
                {
                    "sent": "The Hurons distribution is not properties two and three.",
                    "label": 0
                },
                {
                    "sent": "There look like they're the same thing, but they are in fact different.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what I'm going to use that now in a way not very dissimilar to the way that Michael Jordan uses the same.",
                    "label": 0
                },
                {
                    "sent": "These originally processes to construct a very simple cluster process by a cluster process.",
                    "label": 1
                },
                {
                    "sent": "I mean the following three parts to it.",
                    "label": 1
                },
                {
                    "sent": "First of all, they have to be an index set.",
                    "label": 0
                },
                {
                    "sent": "You start off with that and get your foundation set.",
                    "label": 1
                },
                {
                    "sent": "That's just going to be 123.",
                    "label": 0
                },
                {
                    "sent": "For up to Infinity, then there's going to be a random sequence.",
                    "label": 0
                },
                {
                    "sent": "These are the feature values taking values in Rd.",
                    "label": 0
                },
                {
                    "sent": "And then there is a random partition of that infinite set, and that's going to be an exchangeable random partition.",
                    "label": 0
                },
                {
                    "sent": "Note that it's not a partition of the feature space.",
                    "label": 1
                },
                {
                    "sent": "I'm not talking about partitioning the feature space at all.",
                    "label": 0
                },
                {
                    "sent": "I'm talking about partitioning the units, partitioning the specimens.",
                    "label": 0
                },
                {
                    "sent": "And a sample is just a finite subset of the specimens and that will have probability distribution.",
                    "label": 0
                },
                {
                    "sent": "The partition will have probability distribution PN.",
                    "label": 0
                },
                {
                    "sent": "So the guys humans process says.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "Distribution of the partition restricted to the sample that B square brackets N?",
                    "label": 0
                },
                {
                    "sent": "What is the probability distribution of the partition restricted to the sample?",
                    "label": 0
                },
                {
                    "sent": "Well, that's the unions distribution.",
                    "label": 0
                },
                {
                    "sent": "That's the first line.",
                    "label": 0
                },
                {
                    "sent": "And then what is the probability distribution or density of the feature values on the sampled units given the infinite partition?",
                    "label": 0
                },
                {
                    "sent": "That's this normal distribution.",
                    "label": 0
                },
                {
                    "sent": "Which is well, zero main would be fine.",
                    "label": 0
                },
                {
                    "sent": "All the means are the same.",
                    "label": 0
                },
                {
                    "sent": "And then there's there's two bits.",
                    "label": 0
                },
                {
                    "sent": "There's a non IID part, and then there's a bit that's constant on blocks.",
                    "label": 0
                },
                {
                    "sent": "And there's a covariance matrix between blocks, which is.",
                    "label": 0
                },
                {
                    "sent": "Sigma Prime and a covariance matrix within blocks, which is Sigma.",
                    "label": 0
                },
                {
                    "sent": "Those are parameters that will have to be estimated, of course, as will mu and a couple of other things.",
                    "label": 0
                },
                {
                    "sent": "So this defines an exchangeable partition process in which for any finite set in, you just get to observe the feature values for that sample and the partition on that sample, and then for classification you get the feature value for a new individual, and you want to say what the.",
                    "label": 0
                },
                {
                    "sent": "The classes are.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Militz",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you could construct the Gaussian process, just define it in a different way in terms of random variables I have so far just.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But here this is the definition in terms of probability distribution.",
                    "label": 0
                },
                {
                    "sent": "No, no real random variables.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Such here, but you could construct define it, describe the same thing in terms of random variables you have these eaters which are going to define the cluster centers.",
                    "label": 0
                },
                {
                    "sent": "These are random variables.",
                    "label": 0
                },
                {
                    "sent": "And then the epsilons, which are, if you like the residuals, which are an infinite sequence, Gaussian sequence, independent aveta.",
                    "label": 0
                },
                {
                    "sent": "Then you have the Chinese restaurant process.",
                    "label": 0
                },
                {
                    "sent": "That's this, says the table to which individual I plus one is assigned, is just choose one of the.",
                    "label": 1
                },
                {
                    "sent": "The tables to which the previous I individuals have been assigned with equal probability or else you choose a new table.",
                    "label": 0
                },
                {
                    "sent": "That's just a sequential description of the Chinese restaurant process, and then the observation consists of the block factor here and the feature values, why?",
                    "label": 0
                },
                {
                    "sent": "The parameters I said are they Lambda, mu and the two.",
                    "label": 0
                },
                {
                    "sent": "Variance components Now this is the simplest way to combine these.",
                    "label": 0
                },
                {
                    "sent": "There are more esoteric structures that can be constructed this way and I will illustrate a couple of them in a moment, but their got by putting more information into the eater and having a nonlinear function G there, so it's not just a dish.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So this is what the realization of the Gaussian process looks like in R2.",
                    "label": 1
                },
                {
                    "sent": "So everything got turned upside down here for complicated reasons, but it doesn't affect the distribution.",
                    "label": 0
                },
                {
                    "sent": "The distribution is still the same, so in the top row what I have shown here I've got.",
                    "label": 0
                },
                {
                    "sent": "I think it's about 500 a sample of size 500 from this process and I have put the first 250 of them on the left hand side and then the next 250 in the same trajectory of the same process on the right.",
                    "label": 0
                },
                {
                    "sent": "So from the left to the right you can see that the pattern is essentially the same.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you can see the colors there or not, but there should be five colors on the left and the same five colors should appear on the right.",
                    "label": 0
                },
                {
                    "sent": "In roughly the same configuration.",
                    "label": 0
                },
                {
                    "sent": "The bottom panel is done in exactly the same way.",
                    "label": 0
                },
                {
                    "sent": "If it's just a bigger value of Lambda, so the bigger the value of Lambda, the more blocks there will be enough in a number of blocks is roughly Lambda log in an is about I think it's 4 or 500 here.",
                    "label": 0
                },
                {
                    "sent": "So here we have.",
                    "label": 0
                },
                {
                    "sent": "There should be 10 blocks here in this 250 observations and then in the next 250 observations.",
                    "label": 0
                },
                {
                    "sent": "There are in fact 11 blocks, so probably 1 new block has appeared.",
                    "label": 0
                },
                {
                    "sent": "And that's the sort of thing we should expect.",
                    "label": 0
                },
                {
                    "sent": "All of these have mean zero.",
                    "label": 0
                },
                {
                    "sent": "These are every every point.",
                    "label": 0
                },
                {
                    "sent": "Here is a Gaussian variable of mean zero.",
                    "label": 0
                },
                {
                    "sent": "It's just that they are correlated, and it's the correlations that are inducing these clusters.",
                    "label": 0
                },
                {
                    "sent": "The clusters are central cluster centers are random variables, so there are in fact only in a sense.",
                    "label": 0
                },
                {
                    "sent": "There are only 10 random variables here.",
                    "label": 0
                },
                {
                    "sent": "If you look at think of these as cluster centers.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a slight variation on this.",
                    "label": 0
                },
                {
                    "sent": "Make it a little bit more interesting.",
                    "label": 0
                },
                {
                    "sent": "You can introduce clusters and sub clusters, so let's just go back to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I mean by that?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just describe it in terms of distributions.",
                    "label": 0
                },
                {
                    "sent": "Once you've got the partition B capital N, you can subdivide every block by another.",
                    "label": 0
                },
                {
                    "sent": "You in this process and introduce a third variance component so that each cluster gets split up into sub clusters and then and then instead of having two variance components here we will have a B and and a subpartition of be multiplied by another covariance matrix so that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I mean by clusters and sub clusters and the process will look like this.",
                    "label": 0
                },
                {
                    "sent": "So if you look here, the coloring here indicates the major clusters.",
                    "label": 0
                },
                {
                    "sent": "So if you look down at the bottom here, I don't know what color this is, but looks like there are three subclusters here.",
                    "label": 0
                },
                {
                    "sent": "I don't know for sure, but it looks like there are three and then the same three in the same trajectory in the trajectory of this process, the next 250 points the same three clusters persist.",
                    "label": 0
                },
                {
                    "sent": "And likewise, you can probably see the same pattern up in the top.",
                    "label": 0
                },
                {
                    "sent": "In various places.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can make that even more complicated, but I will.",
                    "label": 0
                },
                {
                    "sent": "2 levels is usually enough.",
                    "label": 0
                },
                {
                    "sent": "Faster become empty.",
                    "label": 0
                },
                {
                    "sent": "Master, by definition is nonempty.",
                    "label": 0
                },
                {
                    "sent": "So there are six clusters here and then in the next 250 points, not all of those clusters need to occur, so I guess there is upside down.",
                    "label": 0
                },
                {
                    "sent": "This looks like a 9, but in fact the six I think, and so five over there telling me that only five of them and you might want to looks like this is a one element cluster.",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem to occur over there.",
                    "label": 0
                },
                {
                    "sent": "I guess that.",
                    "label": 0
                },
                {
                    "sent": "So you don't lose them as such, but you don't necessarily bear in mind that the population here has infinitely many clusters that if you ask how many clusters are there, the answer is easy infinite.",
                    "label": 0
                },
                {
                    "sent": "How many are there in the sample?",
                    "label": 0
                },
                {
                    "sent": "That's the hard problem.",
                    "label": 0
                },
                {
                    "sent": "Well, you get to observe.",
                    "label": 0
                },
                {
                    "sent": "That's a finite number.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you want to make it a little bit more esoteric, you can do things.",
                    "label": 0
                },
                {
                    "sent": "Construct Gaussian processes that have end up with configurations like this with a nonlinear G so that it can make all these squiggles, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now what about classification?",
                    "label": 0
                },
                {
                    "sent": "There's no set of classes in this.",
                    "label": 1
                },
                {
                    "sent": "Bear in mind that the B is just.",
                    "label": 0
                },
                {
                    "sent": "A boolean matrix that tells you whether two individuals belong to the same class or not.",
                    "label": 0
                },
                {
                    "sent": "There's no labeled set of classes, so when you talk about classification, you have to be a little bit careful what you mean by it, but you can calculate the conditional distributions, and this is the conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "Given the training data, that's that's the Y&B for the units in the sample, plus the features for the new individual.",
                    "label": 0
                },
                {
                    "sent": "YN plus one or Y primer has there that will be just calculate that conditional distribution and that will be defined.",
                    "label": 0
                },
                {
                    "sent": "There will be a conditional distribution of that sort for be in one of the observed blocks or not.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, the lack of a set of classes is an advantage here, because it allows you possibility of assigning a new individual to a class that you haven't.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Observed in this training sample.",
                    "label": 0
                },
                {
                    "sent": "So here is the explicit calculation.",
                    "label": 1
                },
                {
                    "sent": "It's not a difficult calculation to do.",
                    "label": 0
                },
                {
                    "sent": "Computationally, it's easy, but you can actually do it explicitly with a little bit of a simplification up there and it just says that the probability that the new element is assigned to block B.",
                    "label": 1
                },
                {
                    "sent": "Of the existing partition is proportional to the first of all is proportional to two things, is proportional to the block size.",
                    "label": 0
                },
                {
                    "sent": "That's the number of be multiplied by the Gaussian density evaluated under at this point.",
                    "label": 0
                },
                {
                    "sent": "With this particular covariance matrix, which are all written down there, and so that's part of the answer and the other bit is for B being a block that's not previously observed in the training sample.",
                    "label": 0
                },
                {
                    "sent": "The probability is proportional to that, and so you calculate all of these and add them up and divide.",
                    "label": 0
                },
                {
                    "sent": "That gives you the probability distribution for the new individual.",
                    "label": 0
                },
                {
                    "sent": "It's a rule.",
                    "label": 0
                },
                {
                    "sent": "Let's essentialness of the same structure as the Chinese restaurant process, but a little bit more difficult, little bit more difficult to compute.",
                    "label": 0
                },
                {
                    "sent": "And of course to to use this, you have to estimate a few of the parameters that the covariance matrices and the Lambda and so on, but those are not.",
                    "label": 0
                },
                {
                    "sent": "Those are very easy things to do.",
                    "label": 0
                },
                {
                    "sent": "This becomes a little bit more interesting and I don't have an example to show you, but if you have the notion of classes and subclasses, there are more empty sets around, so you can you can end up with the conclusion that this new individual looks absolutely nothing like anything we have seen in the training sample.",
                    "label": 1
                },
                {
                    "sent": "That could be one, but you could end up with a conclusion.",
                    "label": 0
                },
                {
                    "sent": "That this individual looks like one of these big blocks that I saw in the training sample, but it doesn't look like any of the subblocks.",
                    "label": 0
                },
                {
                    "sent": "That's another sort of empty set, so there are more, so it's it's of this major species, but not one of the observed sub species.",
                    "label": 0
                },
                {
                    "sent": "And then that will emerge naturally in in the conditional calculation.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's the sort of just getting back to the simple version of the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So the observed data, or over here on the right, the training data and all I can illustrate here is just the block that has maximum conditional probability.",
                    "label": 1
                },
                {
                    "sent": "So that's saying that let's look down here if the new individual falls over here.",
                    "label": 0
                },
                {
                    "sent": "Then the probability that he is assigned to block the blue block.",
                    "label": 0
                },
                {
                    "sent": "Is bigger than the probability that he is assigned to any of the other blocks?",
                    "label": 0
                },
                {
                    "sent": "Doesn't have to be bigger than 1/2, of course, but if you just look at the block that has the maximum conditional probability, that will of course generate a partition of the feature space.",
                    "label": 0
                },
                {
                    "sent": "But you also have all of this empty space here, in which they knew class has higher probability than any of the existing classes, and that's a very natural sort of thing.",
                    "label": 0
                },
                {
                    "sent": "If you observe the new individual over here, well, it seems natural to say it doesn't look like anything we've observed.",
                    "label": 0
                },
                {
                    "sent": "And you would want that to be.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other case, OK, that is the that is what I regard as the classical model for classification.",
                    "label": 0
                },
                {
                    "sent": "It's not fundamentally different from the Fishers model.",
                    "label": 0
                },
                {
                    "sent": "By going back to the 1940s.",
                    "label": 0
                },
                {
                    "sent": "That it does have this issue of being able to assign the new individual to a previously unseen class.",
                    "label": 0
                },
                {
                    "sent": "Which Fishers model does not.",
                    "label": 0
                },
                {
                    "sent": "But apart from that the answers are pretty much the same as a fisherian model.",
                    "label": 0
                },
                {
                    "sent": "I want to.",
                    "label": 0
                },
                {
                    "sent": "Now turn to a different sort of completely different sort of model.",
                    "label": 0
                },
                {
                    "sent": "It's based on a completely different structure involving point processes.",
                    "label": 0
                },
                {
                    "sent": "OK, so begin in the same way we have got a finite set of classes of feature space and now to generate.",
                    "label": 1
                },
                {
                    "sent": "To construct a generative model, I'm going to proceed as follows.",
                    "label": 1
                },
                {
                    "sent": "I make a make myself an intensity function is going to be a random intensity function on the product space C * S for classes and features.",
                    "label": 1
                },
                {
                    "sent": "And then given lambda's at is a point process with parameter Lambda that intensity.",
                    "label": 0
                },
                {
                    "sent": "So then each, then we observe there has to be a description of how things get observed and that description has to be such that the number of observed points is finite.",
                    "label": 0
                },
                {
                    "sent": "So there is a finiteness issue.",
                    "label": 0
                },
                {
                    "sent": "So you have to make sure that things are finite, either by embedding this in time or by looking at a window in which things are finite.",
                    "label": 0
                },
                {
                    "sent": "OK, so then you get to observe this bold set, which is a subset of consisting of classes and features, and then every event consists of a Class A pair of class and a feature X&Y.",
                    "label": 0
                },
                {
                    "sent": "We get to observe bold set on some region finite time.",
                    "label": 0
                },
                {
                    "sent": "Then there has to be a notion of a next event, otherwise the whole thing doesn't make any sense.",
                    "label": 0
                },
                {
                    "sent": "There's no issue of classification at all and we ask what is the conditional distribution of the next event given the data.",
                    "label": 1
                },
                {
                    "sent": "And for a variety of reasons this looks like it's going to be just an exchangeable process, but it's not.",
                    "label": 0
                },
                {
                    "sent": "There's no index set.",
                    "label": 1
                },
                {
                    "sent": "And the rate of occurrence of events is random and.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So on so let's just go through this and go through the details of this.",
                    "label": 0
                },
                {
                    "sent": "The probability distributions for these are Cox processes, conditionally Poisson processes.",
                    "label": 0
                },
                {
                    "sent": "So the observation is a finite point, processes conditionally Poisson.",
                    "label": 0
                },
                {
                    "sent": "It's I will call it a configuration configuration just means a finite is the set of all the configuration space is the set of all finite subsets of this general space Script X, which will be the product space features, times, classes.",
                    "label": 0
                },
                {
                    "sent": "Fan for it to be a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "The sample size, the number of events is going to be random.",
                    "label": 0
                },
                {
                    "sent": "And the integral over the integral over the feature space.",
                    "label": 0
                },
                {
                    "sent": "Someone want to end the integral is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "That's just says there's a probability distribution X.",
                    "label": 0
                },
                {
                    "sent": "Here should be thought of as a set.",
                    "label": 0
                },
                {
                    "sent": "It's not an ordered set, it's just a set.",
                    "label": 0
                },
                {
                    "sent": "There's no first element, second element, and so on.",
                    "label": 0
                },
                {
                    "sent": "And the actual formula for PN is that thing up there.",
                    "label": 0
                },
                {
                    "sent": "It's saying if you look at it carefully, it's saying the probability of observing points X one up to XN.",
                    "label": 0
                },
                {
                    "sent": "Well, it's just the intensity at the product of the intensities.",
                    "label": 1
                },
                {
                    "sent": "But not only do you get to observe points at X1 up to XN, there are no points anywhere else.",
                    "label": 0
                },
                {
                    "sent": "So that's where this E to the minus, Lambda S comes from.",
                    "label": 0
                },
                {
                    "sent": "The empty space is part of the observation as well.",
                    "label": 0
                },
                {
                    "sent": "And what is it then?",
                    "label": 0
                },
                {
                    "sent": "The plays the role of predictive distribution.",
                    "label": 1
                },
                {
                    "sent": "It's it's this pop and Gallo conditional intensity.",
                    "label": 0
                },
                {
                    "sent": "It's also the Bayes estimate of Lambda, so that ratio it looks like a conditional probability distribution, but it isn't because this doesn't integrate to one.",
                    "label": 1
                },
                {
                    "sent": "This is a conditional intensity, so it has to be normalized.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to describe one such example where this calculation can be done explicitly, it involves.",
                    "label": 0
                },
                {
                    "sent": "What's called the permanent point process.",
                    "label": 0
                },
                {
                    "sent": "So let me introduce some terminology about permanence.",
                    "label": 0
                },
                {
                    "sent": "The Alpha permanent of a matrix is just the sum over permutations Alpha raised to the number of cycles in the permutation times a product like a determinant.",
                    "label": 1
                },
                {
                    "sent": "So it's just like a determinant, except that the plus minus signs are replaced by this Alpha to the power.",
                    "label": 0
                },
                {
                    "sent": "So if Alpha is minus one, this is essentially a determinant.",
                    "label": 0
                },
                {
                    "sent": "If Alpha is plus one, that's essentially the ordinary permanent.",
                    "label": 0
                },
                {
                    "sent": "And another expression that arises here is.",
                    "label": 0
                },
                {
                    "sent": "That's the limit of the previous one as Alpha goes to zero and I just called up this sum of cyclic products, which is the sum over cyclic permutations only.",
                    "label": 0
                },
                {
                    "sent": "So N -- 1 factorial terms.",
                    "label": 0
                },
                {
                    "sent": "So these two terms come up in the distribution probability distribution that I'll describe in a moment.",
                    "label": 0
                },
                {
                    "sent": "It's rather difficult in general to compute these except for the very simplest of matrices.",
                    "label": 0
                },
                {
                    "sent": "And here's an example, the permanent of the matrix N by N matrix of all ones is just this gamma function ratio of gamma functions, which is exactly the normalizing constant that occurs in the in the humans process, and there's a close relationship here with the humans process, these permanents have this interesting convolution property, so they are connected with binomial and multinomial.",
                    "label": 0
                },
                {
                    "sent": "Probability distributions and with partition distributions.",
                    "label": 0
                },
                {
                    "sent": "So all those are difficult to compute.",
                    "label": 0
                },
                {
                    "sent": "They are quite interesting probabilistically.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the bozone point process is constructed by follows.",
                    "label": 1
                },
                {
                    "sent": "You get yourself a positive definite function.",
                    "label": 1
                },
                {
                    "sent": "Construct a Gaussian process covariance function K / 2 you do all of this and you generate the process with that random intensity which is sum of squared Gaussians.",
                    "label": 1
                },
                {
                    "sent": "Now you could say why don't you just exponentiate rather than take a sum of squares?",
                    "label": 0
                },
                {
                    "sent": "Well, that's another Gaussian.",
                    "label": 0
                },
                {
                    "sent": "That's another Cox process, the log gamma.",
                    "label": 0
                },
                {
                    "sent": "Cox process, but it's very difficult to do anything with it now.",
                    "label": 1
                },
                {
                    "sent": "This one's not easy either, but this one at least you can make some progress with.",
                    "label": 0
                },
                {
                    "sent": "You can actually write down the cumulant density, the moment density, and the probability density, all in principle computable, although perhaps not in the polynomial time.",
                    "label": 0
                },
                {
                    "sent": "But that's another matter.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The multi class model that I will use just has independent intensities for each class.",
                    "label": 1
                },
                {
                    "sent": "So the probability distribution will just be a product like this over the various classes will be an A1 for class one up to Alpha, K for class K, and then the superposition process will also be a bowsan process.",
                    "label": 1
                },
                {
                    "sent": "And this same constant arises here, and the parameter is just the sum of the alphas.",
                    "label": 0
                },
                {
                    "sent": "So now why is feet?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vendice the classification distribution now looks like this given the training data consisting of classes X features Y and a new feature, the probability that the next individual gets assigned to Class R is proportional to that permanent ratio.",
                    "label": 0
                },
                {
                    "sent": "No possibility here of assigning things to an unseen class because for the moment and unseen there's no such thing.",
                    "label": 0
                },
                {
                    "sent": "But if you consider the limit as what's the limit, no limit that's described here, then most of the classes will not be represented in the sample and you can do the same thing.",
                    "label": 0
                },
                {
                    "sent": "And now this possibility of an unseen class does feature there is that Lambda K~ of why what?",
                    "label": 0
                },
                {
                    "sent": "But in any case, that looks pretty much the same, except that here you have got these permanence.",
                    "label": 0
                },
                {
                    "sent": "And here you have got cyclic products only.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jay so I thought I would illustrate this with the help of my colleague Jiyong.",
                    "label": 0
                },
                {
                    "sent": "So we constructed a data set looking like like so looks a lot darker here on the screen than it does on my computer, but these are red and blue.",
                    "label": 0
                },
                {
                    "sent": "And, well, you can't actually see the Reds, but you know they're looking at the edges here.",
                    "label": 0
                },
                {
                    "sent": "But there are.",
                    "label": 0
                },
                {
                    "sent": "There are this checkerboard pattern three by three checkerboard pattern.",
                    "label": 0
                },
                {
                    "sent": "There are 10 points in each little square uniform independent, so they're not generated according to the model.",
                    "label": 0
                },
                {
                    "sent": "Some parameters need to be estimated.",
                    "label": 0
                },
                {
                    "sent": "We do that by cross validation.",
                    "label": 0
                },
                {
                    "sent": "The covariance function is the one up in the top player.",
                    "label": 0
                },
                {
                    "sent": "They told parameter the range parameter is about point 7.47.",
                    "label": 0
                },
                {
                    "sent": "I know we want to see what is.",
                    "label": 0
                },
                {
                    "sent": "How does this work for classification?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, if you were to plug this data set into the Gaussians, processes would say like there's nothing I can do here because the Gaussian process would not just not work with this sort of thing, because all the clusters in the God students process have to be compact and not mixed up like this.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is what it looks like.",
                    "label": 0
                },
                {
                    "sent": "This is the probability distribution is the probability of red given the data for all of the various points in this unit square with the parameters estimated by cross validation and the probability of red ranges from about somewhere about 5% to 95% from the darkest red to the darkest blue.",
                    "label": 0
                },
                {
                    "sent": "I should say these this permanent ratio in this was computed analytically.",
                    "label": 0
                },
                {
                    "sent": "By an approximation or I should say approximated, not computed.",
                    "label": 0
                },
                {
                    "sent": "There are other ways of doing it, more or less approximately more or less exactly by Markov chain methods and Monte Carlo methods of other sorts.",
                    "label": 0
                },
                {
                    "sent": "But the approximation seems to work reasonably well, at least for this purpose, involving the matrices that are involved here are going to be of order.",
                    "label": 0
                },
                {
                    "sent": "Well, let's see how many Reds are 50 Reds there, so 50 by 50 and 40 by 40 for the Blues.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's how it looks.",
                    "label": 0
                },
                {
                    "sent": "You can see here that there are some misclassifications in the sense that this triangle, which is really a blue point.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if something falls in that neighborhood, something falls close to here.",
                    "label": 0
                },
                {
                    "sent": "It would be assigned with probability .55 roughly to the red, where in fact that's part of the blue region button on the whole for 10 points in each small square.",
                    "label": 0
                },
                {
                    "sent": "This is not too doesn't do it bad job of reconstructing the.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Checkerboard pattern.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll finish up now by just saying a little bit about algorithms for approximation of permanents.",
                    "label": 1
                },
                {
                    "sent": "Computer scientists are all well aware that these permanents are sensibly uncomputable in the traditional sense.",
                    "label": 0
                },
                {
                    "sent": "For anything bigger than about 20 or 30.",
                    "label": 0
                },
                {
                    "sent": "The top two algorithms, which are exact or just not good enough to for the sorts of purposes that we need here, there are stochastic approximation algorithms, one of them by Sam Cohen myself, which seems to work well, I should say, incidentally, that we don't actually need to calculate permanent in this.",
                    "label": 1
                },
                {
                    "sent": "In this we only need to calculate permanent ratios, and that's a lot easier than calculating the permanent itself.",
                    "label": 0
                },
                {
                    "sent": "So generally speaking, these are probably approximation.",
                    "label": 1
                },
                {
                    "sent": "This is not as bad as it seems.",
                    "label": 1
                },
                {
                    "sent": "And as I said, we used a truncated cycle expansion.",
                    "label": 0
                },
                {
                    "sent": "Naive but not too bad.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I will just finish up with reference.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This here is a pile of references.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I will leave it at that.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes, Charlie.",
                    "label": 0
                },
                {
                    "sent": "Mention of the features or it doesn't affect your conversation.",
                    "label": 0
                },
                {
                    "sent": "Doesn't affect the computation at all.",
                    "label": 0
                },
                {
                    "sent": "Is that the key metrics?",
                    "label": 0
                },
                {
                    "sent": "Once you have, once you've got your distance matrix in your K matrix, then you can forget about the feature space.",
                    "label": 0
                }
            ]
        }
    }
}