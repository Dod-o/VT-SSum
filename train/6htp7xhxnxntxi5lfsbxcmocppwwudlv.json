{
    "id": "6htp7xhxnxntxi5lfsbxcmocppwwudlv",
    "title": "An Almost Optimal PAC Algorithm",
    "info": {
        "author": [
            "Hans U. Simon, Fakult\u00e4t f\u00fcr Mathematik, Ruhr University Bochum"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_simon_pac_algorithm/",
    "segmentation": [
        [
            "Thanks for the invitation to this conference.",
            "Yeah, the older ones of us may well remember the year 1984 when Leslie Valiant wrote his influential article in the communications of CM.",
            "The article was titled The Theory of the Learnable.",
            "And while in the eastern countries there was already some theory in progress in the Western countries was a kind of Big Bang for modern mathematical theory of machine learning.",
            "And amazingly, even in this most elementary model, the PEC model.",
            "There was an open problem remaining as far as the size of the sample is concerned, and this talk will be mainly about this problem from the very old times of learning theory."
        ],
        [
            "So let's see how this works.",
            "OK, that's fine, so here you see how the talk will be structured.",
            "And I stop."
        ],
        [
            "Act without telling you the main problem and the main result."
        ],
        [
            "So here are the bounds that came up quite soon after the pack learning model was established.",
            "There is this lower bound.",
            "Was proven by even voiced hustler currents and billions, and there is also this upper bound which was proven to proof by Blumer, Blumer, Hustler and.",
            "I think young fish does well in Barmouth.",
            "And as you see, these bounds almost match each other, but there is a log arhythmic Depp, right, and while this logarithmic gap may not be of such a big practical importance for a theoretician, it's quite annoying that even in the basic model, you do not have to complete understanding and know exactly what the what is the true bound.",
            "So this the gap has survived for two D 426 years after it was there, and in this talk we will considerably narrow this skip, although not completely removing it."
        ],
        [
            "So in the year 2004, Varn Woods was putting this problem explicitly.",
            "In the on the list of open problems on the open in the open problem session of the year 2004, along with the conjecture, then the conjecture was that the one inclusion graph algorithm by how's the little stonean himself, might actually an optimal one whose consumption of examples matches with the lower bound.",
            "While this problem is in its generality, still completely and widely open, it has been confirmed for subclasses.",
            "So for instance, in lost in the last year, multi dance that was able to show that the conjecture can be confirmed for all intersection closed classes."
        ],
        [
            "So now my contribution is the following.",
            "I will prove a new upper bounds an actually you see that it coincides with the classical upper bound with one little but striking exception.",
            "The logarithmic factor is replaced by a new factor which is named LK&LK.",
            "Is the K fold or a truncated version of the K fold iterated logarithm right?",
            "And this of course.",
            "If you choose K sufficiently large or even for moderate values of K, this is an extremely slowly growing function, so it's very small.",
            "Get by now and throughout the talk I will consider the parameter K as a constant.",
            "But in the proceedings you will find some considerations for looking at K at a function that may depend on the sample size M."
        ],
        [
            "Yeah, there is an alternative statements which is more or less equivalent to the statement that we have seen on the preceding slide in the alternative statement I just solved the inequality for epsilon instead of M, so I upper bound the accuracy parameter epsilon in terms of the sample size to BC dimension of the concept class and the confidence parameter Delta.",
            "And yeah, that's in the classical computation.",
            "To calculate these inequalities back and forth.",
            "So we can maybe may prove this inequality.",
            "This that's enough.",
            "And if you compare this to the inequality that has been known before, you see again that the only difference is that we deal with the K fold iterated logarithm at the place of the logarithm in the classical bound.",
            "So in this alternative statement, will be the one that we use in the proof."
        ],
        [
            "Of course we need a new policy for constructing the hypothesis.",
            "The old policy was basically just choosing any consistency practices.",
            "And it was shown by our and ought not, that this is not sufficient, because there are consistent algorithms where this additional luck factor in the upper bound is actually forced.",
            "So you cannot just choose any consistency purposes.",
            "You have to do something else."
        ],
        [
            "So here you see the comparison between what was done before and what we will do on the left hand side you see just the classical policy.",
            "So there's the version space of all hypothesis that are consistent with the labeled examples and you just pick any hypothesis.",
            "And of course TC pollicis look empirically equally good.",
            "So it's hard to say which one is better than the other one.",
            "But as I said, this is not sufficient.",
            "So what will what we will do?",
            "Is we we decompose the sample into subsamples and then we compute just any consistency policies for the subsamples and afterwards we take the majority vote over these hypothesis.",
            "So that's a very simple modification.",
            "I think of the classical algorithm.",
            "And so yeah, it's except for the majority vote, it's almost the same.",
            "So for the subsamples you applied classical strategy."
        ],
        [
            "Yeah, here is again the same in a more formal manner.",
            "So we decompose the sample into subsamples of the same size.",
            "Then for every subsample we just pick, you can apply a classical pick learner.",
            "Denoted Capital L on this slide.",
            "So we pick any hypothesis for the subsample SK.",
            "And finally we return the majority votes over the policies as the final hypothesis, so that's the that's the whole procedure."
        ],
        [
            "So yeah, there there will be some time to give you some information about the proof.",
            "I cannot give the proof in full technical detail here, but I plan to give you the global construction of the proof so that you will get them the main idea."
        ],
        [
            "So first of all, everything here proceeds in stages.",
            "Of course, the algorithm proceeds in stages as we have discussed before, but also the analysis proceeds in stages.",
            "And in particular, when it comes to discuss the case hypothesis, HK.",
            "Then we do not analyze this hypothesis with respect to the.",
            "Underlying probability distribution P. But we will analyze it against the conditional distribution.",
            "So P conditioned to some event.",
            "And this event is chosen.",
            "Independence of the preceding hypothesis.",
            "Right, so this is an important point.",
            "And see how the pack model allows this modification.",
            "Of course, the only thing that we have to keep in mind is that for the analysis of HHK, we cannot employ the full subsample SK, but only the part of the sum subsample SK that hits the event E. That that's the only thing that we have to keep in mind.",
            "But once we have done that, then we can consider SK intersection the event E. As the sequence of independent Lee, drawn labeled examples for the in the case stage.",
            "Yeah, this basic idea has been used before, I should say so.",
            "People, for instance who are familiar with the PhD thesis of Steve Chanukah, will know, perhaps know that he used the same technique for result that I will will briefly describe at the end of the talk.",
            "But Steve did not.",
            "Proceed in stages.",
            "In the algorithm he just in the analysis and we do it in the algorithm as well."
        ],
        [
            "OK, so.",
            "See.",
            "OK, so now I will show you some major steps in the proof.",
            "So the first step is that we consider the error sets of the majority votes and we decompose it into other error sets.",
            "And of course we can for each possible majority, which is represented by a set capital J subset of the sets of 2K McManus only pollicis and this set capital J due to have cardinality K, so that it forms a majority.",
            "Right, and there are exponentially many in K such sets, capital J that represent majorities, and for each fixed sets capital J there is this common error sets of all the hypothesis in that fall into this set J. OK, so we have to.",
            "Basically we have to discuss these common error sets.",
            "So given any set capital J or presenting a majority.",
            "And we have to discuss that there is a common point in the common error sets, right?",
            "Because only the points and the common error sets will be done wrong by the majority vote.",
            "Yeah, so the error set E can be decomposed into these common error sets.",
            "EJ, right?",
            "So that's the main.",
            "Thing in the first step."
        ],
        [
            "And if you consider JAK as a constant, of course we have reduced the problem in bounding the error of EJ for some fixed choice of J.",
            "So let's now consider a fixed choice of capital J, representing one of the conceivable majorities that may go wrong.",
            "And some really number the indices.",
            "Just buy one 2K, so that's similar notation.",
            "And now we discuss these common error probabilities epsilon K. So epsilon K is the probability that the first K hypothesis in J all make the same mistake.",
            "And then we can decompose this by the law of conditional probabilities.",
            "We can decompose this probability as it is shown on this slide.",
            "And this holds in particular for the event EJ that all the hypothesis addressed by J have an error.",
            "The same error point.",
            "Answer Yeah, you see the decomposition here and now.",
            "Remember that we plan to analyze the case hypothesis with respect to conditional probability distribution and here you see the conditional probability distribution that comes up by the decomposition of the probability of EJ.",
            "So we analyzed the error set Eka of The Cave hypothesis conditioned to the common error sets of the first K -- 1 hypothesis.",
            "That's basically what we do.",
            "And as you see, the center sample that we can use for that is just the subsample SK intersected by the common error region of the first K -- 1 hypothesis.",
            "And now there is 1 nice thing which helps in the analysis.",
            "So either the error of the common error of the first K -- 1 hypothesis is already very small, but then we are happy of course.",
            "Or it is large, but then there is a kind of self correction because this means that the sample that we can use in stage K is comparably large, right?",
            "So there is a kind of self correction that is going on here."
        ],
        [
            "Yeah, this this step three is just that we use the classical, the classical pick generalization error bounds.",
            "Four stage K but with respect to this conditional distribution, as I said and then do some calculations and then it comes out that with high probability these common error probabilities this error probabilities epsilon K for the common error regions."
        ],
        [
            "They evolve according to this recursion right so?",
            "The first parameter, epsilon one, is just bounded by the classical error bounds and the next one is bounded as you see in this recursive.",
            "Indiscretion right, so you are left with the problem of solving this recursion.",
            "And this again is quite technical.",
            "I show don't bother you with the technicalities, but of course you can solve this recursion an it happens that the solution has this particular form.",
            "Yeah, and this is the form that we needed in the alternative statement of our main result.",
            "So this."
        ],
        [
            "Is that basically the main?",
            "This gives you the main theorem actually.",
            "Therefore, the probability of EJ.",
            "But as I said becausw the total error region of the majority votes can be decomposed in.",
            "In these problems in these events EJ we now have also an upper bound on the probability of the error of the majority vote, right?",
            "So yeah, that was basically the main structure of the proof.",
            "The technical details you will see in the proceedings.",
            "Yeah, of course the gap has not vanished, just it still there, right?",
            "So still this iterated logarithm, and because of this factor which is exponential in K in this beginning you cannot make this dip arbitrarily small.",
            "Yeah, let me go."
        ],
        [
            "This talk by some final remarks."
        ],
        [
            "First of all, I would like to compare our analysis with the analysis undertaken by Steve Honaker in his PhD thesis.",
            "So Steve Hanukkah has shown that the lag rhythmic gap in the classical upper bound on the generalization error.",
            "Can be replaced by the logarithm of what he calls the disagreement coefficient.",
            "So you can replace the logarithm of 1 over epsilon by the logarithm of the disagreement coefficient.",
            "And this is good news for all concept classes that have a constant disagreement coefficient, because for these classes that the gap will be bridged.",
            "And actually this was used by a multi darnstaedt with he finds the definition of the disagreement coefficient a little bit.",
            "And this was used by Multi Township for showing that the one inclusion graph algorithm is optimal for intersection closed classes.",
            "But as I said the there also, although there's some similarity in the analysis, Konica also broke the analysis in two stages an analyzed The Cave stage with respect to a conditional probability measure and the event on which it is conditioned depends on the 1st K minus one stages.",
            "So this looks similar.",
            "But there are also some differences.",
            "So in C phonics work this.",
            "Decomposition in two stages happens in the analysis only and has no influence on what the algorithm does.",
            "He still has the policy of picking just any consistency.",
            "Part is is and we used this decomposition in the algorithm already.",
            "And.",
            "And second, he conditions against hitting a disagreement region, whereas we condition on hitting a common error region of some of the preceding hypothesis.",
            "Yeah, let me briefly discuss some efficiency issues so our algorithm will be efficient whenever the consistency problem can be solved efficiently, and this is pretty good though by inclusion graph algorithm for example, what is has an exponential dependence on the public share links dimension.",
            "Yeah, and then of course there is this open question whether yeah, there's even the open question whether our algorithm LK might be an optimal pick algorithm for some choice of K. In the proceedings you will see a proof that the algorithm L2 treats optimally the concept classes that were used by our end ordner for showing that you cannot take any consistency policies.",
            "So we can take this probably sub optimal algorithm of choosing any apodosis and then switch to the.",
            "L2 version of it and then it becomes optimal.",
            "So there is the question whether LK might perhaps already been optimal algorithms, so we have no matching lower bound.",
            "And of course, the main challenge is to solve the to close the gap.",
            "Yeah, thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks for the invitation to this conference.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the older ones of us may well remember the year 1984 when Leslie Valiant wrote his influential article in the communications of CM.",
                    "label": 0
                },
                {
                    "sent": "The article was titled The Theory of the Learnable.",
                    "label": 0
                },
                {
                    "sent": "And while in the eastern countries there was already some theory in progress in the Western countries was a kind of Big Bang for modern mathematical theory of machine learning.",
                    "label": 0
                },
                {
                    "sent": "And amazingly, even in this most elementary model, the PEC model.",
                    "label": 0
                },
                {
                    "sent": "There was an open problem remaining as far as the size of the sample is concerned, and this talk will be mainly about this problem from the very old times of learning theory.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see how this works.",
                    "label": 0
                },
                {
                    "sent": "OK, that's fine, so here you see how the talk will be structured.",
                    "label": 0
                },
                {
                    "sent": "And I stop.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Act without telling you the main problem and the main result.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the bounds that came up quite soon after the pack learning model was established.",
                    "label": 0
                },
                {
                    "sent": "There is this lower bound.",
                    "label": 0
                },
                {
                    "sent": "Was proven by even voiced hustler currents and billions, and there is also this upper bound which was proven to proof by Blumer, Blumer, Hustler and.",
                    "label": 0
                },
                {
                    "sent": "I think young fish does well in Barmouth.",
                    "label": 0
                },
                {
                    "sent": "And as you see, these bounds almost match each other, but there is a log arhythmic Depp, right, and while this logarithmic gap may not be of such a big practical importance for a theoretician, it's quite annoying that even in the basic model, you do not have to complete understanding and know exactly what the what is the true bound.",
                    "label": 0
                },
                {
                    "sent": "So this the gap has survived for two D 426 years after it was there, and in this talk we will considerably narrow this skip, although not completely removing it.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the year 2004, Varn Woods was putting this problem explicitly.",
                    "label": 0
                },
                {
                    "sent": "In the on the list of open problems on the open in the open problem session of the year 2004, along with the conjecture, then the conjecture was that the one inclusion graph algorithm by how's the little stonean himself, might actually an optimal one whose consumption of examples matches with the lower bound.",
                    "label": 1
                },
                {
                    "sent": "While this problem is in its generality, still completely and widely open, it has been confirmed for subclasses.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in lost in the last year, multi dance that was able to show that the conjecture can be confirmed for all intersection closed classes.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now my contribution is the following.",
                    "label": 0
                },
                {
                    "sent": "I will prove a new upper bounds an actually you see that it coincides with the classical upper bound with one little but striking exception.",
                    "label": 0
                },
                {
                    "sent": "The logarithmic factor is replaced by a new factor which is named LK&LK.",
                    "label": 0
                },
                {
                    "sent": "Is the K fold or a truncated version of the K fold iterated logarithm right?",
                    "label": 0
                },
                {
                    "sent": "And this of course.",
                    "label": 0
                },
                {
                    "sent": "If you choose K sufficiently large or even for moderate values of K, this is an extremely slowly growing function, so it's very small.",
                    "label": 0
                },
                {
                    "sent": "Get by now and throughout the talk I will consider the parameter K as a constant.",
                    "label": 0
                },
                {
                    "sent": "But in the proceedings you will find some considerations for looking at K at a function that may depend on the sample size M.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, there is an alternative statements which is more or less equivalent to the statement that we have seen on the preceding slide in the alternative statement I just solved the inequality for epsilon instead of M, so I upper bound the accuracy parameter epsilon in terms of the sample size to BC dimension of the concept class and the confidence parameter Delta.",
                    "label": 1
                },
                {
                    "sent": "And yeah, that's in the classical computation.",
                    "label": 0
                },
                {
                    "sent": "To calculate these inequalities back and forth.",
                    "label": 0
                },
                {
                    "sent": "So we can maybe may prove this inequality.",
                    "label": 0
                },
                {
                    "sent": "This that's enough.",
                    "label": 0
                },
                {
                    "sent": "And if you compare this to the inequality that has been known before, you see again that the only difference is that we deal with the K fold iterated logarithm at the place of the logarithm in the classical bound.",
                    "label": 0
                },
                {
                    "sent": "So in this alternative statement, will be the one that we use in the proof.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course we need a new policy for constructing the hypothesis.",
                    "label": 1
                },
                {
                    "sent": "The old policy was basically just choosing any consistency practices.",
                    "label": 0
                },
                {
                    "sent": "And it was shown by our and ought not, that this is not sufficient, because there are consistent algorithms where this additional luck factor in the upper bound is actually forced.",
                    "label": 0
                },
                {
                    "sent": "So you cannot just choose any consistency purposes.",
                    "label": 0
                },
                {
                    "sent": "You have to do something else.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here you see the comparison between what was done before and what we will do on the left hand side you see just the classical policy.",
                    "label": 0
                },
                {
                    "sent": "So there's the version space of all hypothesis that are consistent with the labeled examples and you just pick any hypothesis.",
                    "label": 1
                },
                {
                    "sent": "And of course TC pollicis look empirically equally good.",
                    "label": 0
                },
                {
                    "sent": "So it's hard to say which one is better than the other one.",
                    "label": 0
                },
                {
                    "sent": "But as I said, this is not sufficient.",
                    "label": 0
                },
                {
                    "sent": "So what will what we will do?",
                    "label": 0
                },
                {
                    "sent": "Is we we decompose the sample into subsamples and then we compute just any consistency policies for the subsamples and afterwards we take the majority vote over these hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So that's a very simple modification.",
                    "label": 1
                },
                {
                    "sent": "I think of the classical algorithm.",
                    "label": 0
                },
                {
                    "sent": "And so yeah, it's except for the majority vote, it's almost the same.",
                    "label": 0
                },
                {
                    "sent": "So for the subsamples you applied classical strategy.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, here is again the same in a more formal manner.",
                    "label": 0
                },
                {
                    "sent": "So we decompose the sample into subsamples of the same size.",
                    "label": 0
                },
                {
                    "sent": "Then for every subsample we just pick, you can apply a classical pick learner.",
                    "label": 0
                },
                {
                    "sent": "Denoted Capital L on this slide.",
                    "label": 0
                },
                {
                    "sent": "So we pick any hypothesis for the subsample SK.",
                    "label": 0
                },
                {
                    "sent": "And finally we return the majority votes over the policies as the final hypothesis, so that's the that's the whole procedure.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, there there will be some time to give you some information about the proof.",
                    "label": 0
                },
                {
                    "sent": "I cannot give the proof in full technical detail here, but I plan to give you the global construction of the proof so that you will get them the main idea.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, everything here proceeds in stages.",
                    "label": 0
                },
                {
                    "sent": "Of course, the algorithm proceeds in stages as we have discussed before, but also the analysis proceeds in stages.",
                    "label": 0
                },
                {
                    "sent": "And in particular, when it comes to discuss the case hypothesis, HK.",
                    "label": 0
                },
                {
                    "sent": "Then we do not analyze this hypothesis with respect to the.",
                    "label": 0
                },
                {
                    "sent": "Underlying probability distribution P. But we will analyze it against the conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "So P conditioned to some event.",
                    "label": 0
                },
                {
                    "sent": "And this event is chosen.",
                    "label": 0
                },
                {
                    "sent": "Independence of the preceding hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is an important point.",
                    "label": 0
                },
                {
                    "sent": "And see how the pack model allows this modification.",
                    "label": 0
                },
                {
                    "sent": "Of course, the only thing that we have to keep in mind is that for the analysis of HHK, we cannot employ the full subsample SK, but only the part of the sum subsample SK that hits the event E. That that's the only thing that we have to keep in mind.",
                    "label": 0
                },
                {
                    "sent": "But once we have done that, then we can consider SK intersection the event E. As the sequence of independent Lee, drawn labeled examples for the in the case stage.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this basic idea has been used before, I should say so.",
                    "label": 0
                },
                {
                    "sent": "People, for instance who are familiar with the PhD thesis of Steve Chanukah, will know, perhaps know that he used the same technique for result that I will will briefly describe at the end of the talk.",
                    "label": 0
                },
                {
                    "sent": "But Steve did not.",
                    "label": 0
                },
                {
                    "sent": "Proceed in stages.",
                    "label": 0
                },
                {
                    "sent": "In the algorithm he just in the analysis and we do it in the algorithm as well.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I will show you some major steps in the proof.",
                    "label": 0
                },
                {
                    "sent": "So the first step is that we consider the error sets of the majority votes and we decompose it into other error sets.",
                    "label": 0
                },
                {
                    "sent": "And of course we can for each possible majority, which is represented by a set capital J subset of the sets of 2K McManus only pollicis and this set capital J due to have cardinality K, so that it forms a majority.",
                    "label": 1
                },
                {
                    "sent": "Right, and there are exponentially many in K such sets, capital J that represent majorities, and for each fixed sets capital J there is this common error sets of all the hypothesis in that fall into this set J. OK, so we have to.",
                    "label": 0
                },
                {
                    "sent": "Basically we have to discuss these common error sets.",
                    "label": 0
                },
                {
                    "sent": "So given any set capital J or presenting a majority.",
                    "label": 0
                },
                {
                    "sent": "And we have to discuss that there is a common point in the common error sets, right?",
                    "label": 0
                },
                {
                    "sent": "Because only the points and the common error sets will be done wrong by the majority vote.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so the error set E can be decomposed into these common error sets.",
                    "label": 0
                },
                {
                    "sent": "EJ, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the main.",
                    "label": 0
                },
                {
                    "sent": "Thing in the first step.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you consider JAK as a constant, of course we have reduced the problem in bounding the error of EJ for some fixed choice of J.",
                    "label": 0
                },
                {
                    "sent": "So let's now consider a fixed choice of capital J, representing one of the conceivable majorities that may go wrong.",
                    "label": 0
                },
                {
                    "sent": "And some really number the indices.",
                    "label": 0
                },
                {
                    "sent": "Just buy one 2K, so that's similar notation.",
                    "label": 0
                },
                {
                    "sent": "And now we discuss these common error probabilities epsilon K. So epsilon K is the probability that the first K hypothesis in J all make the same mistake.",
                    "label": 0
                },
                {
                    "sent": "And then we can decompose this by the law of conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "We can decompose this probability as it is shown on this slide.",
                    "label": 0
                },
                {
                    "sent": "And this holds in particular for the event EJ that all the hypothesis addressed by J have an error.",
                    "label": 0
                },
                {
                    "sent": "The same error point.",
                    "label": 0
                },
                {
                    "sent": "Answer Yeah, you see the decomposition here and now.",
                    "label": 0
                },
                {
                    "sent": "Remember that we plan to analyze the case hypothesis with respect to conditional probability distribution and here you see the conditional probability distribution that comes up by the decomposition of the probability of EJ.",
                    "label": 0
                },
                {
                    "sent": "So we analyzed the error set Eka of The Cave hypothesis conditioned to the common error sets of the first K -- 1 hypothesis.",
                    "label": 0
                },
                {
                    "sent": "That's basically what we do.",
                    "label": 0
                },
                {
                    "sent": "And as you see, the center sample that we can use for that is just the subsample SK intersected by the common error region of the first K -- 1 hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And now there is 1 nice thing which helps in the analysis.",
                    "label": 0
                },
                {
                    "sent": "So either the error of the common error of the first K -- 1 hypothesis is already very small, but then we are happy of course.",
                    "label": 0
                },
                {
                    "sent": "Or it is large, but then there is a kind of self correction because this means that the sample that we can use in stage K is comparably large, right?",
                    "label": 0
                },
                {
                    "sent": "So there is a kind of self correction that is going on here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, this this step three is just that we use the classical, the classical pick generalization error bounds.",
                    "label": 0
                },
                {
                    "sent": "Four stage K but with respect to this conditional distribution, as I said and then do some calculations and then it comes out that with high probability these common error probabilities this error probabilities epsilon K for the common error regions.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They evolve according to this recursion right so?",
                    "label": 0
                },
                {
                    "sent": "The first parameter, epsilon one, is just bounded by the classical error bounds and the next one is bounded as you see in this recursive.",
                    "label": 1
                },
                {
                    "sent": "Indiscretion right, so you are left with the problem of solving this recursion.",
                    "label": 1
                },
                {
                    "sent": "And this again is quite technical.",
                    "label": 0
                },
                {
                    "sent": "I show don't bother you with the technicalities, but of course you can solve this recursion an it happens that the solution has this particular form.",
                    "label": 1
                },
                {
                    "sent": "Yeah, and this is the form that we needed in the alternative statement of our main result.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that basically the main?",
                    "label": 1
                },
                {
                    "sent": "This gives you the main theorem actually.",
                    "label": 0
                },
                {
                    "sent": "Therefore, the probability of EJ.",
                    "label": 0
                },
                {
                    "sent": "But as I said becausw the total error region of the majority votes can be decomposed in.",
                    "label": 1
                },
                {
                    "sent": "In these problems in these events EJ we now have also an upper bound on the probability of the error of the majority vote, right?",
                    "label": 0
                },
                {
                    "sent": "So yeah, that was basically the main structure of the proof.",
                    "label": 0
                },
                {
                    "sent": "The technical details you will see in the proceedings.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course the gap has not vanished, just it still there, right?",
                    "label": 0
                },
                {
                    "sent": "So still this iterated logarithm, and because of this factor which is exponential in K in this beginning you cannot make this dip arbitrarily small.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let me go.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This talk by some final remarks.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, I would like to compare our analysis with the analysis undertaken by Steve Honaker in his PhD thesis.",
                    "label": 0
                },
                {
                    "sent": "So Steve Hanukkah has shown that the lag rhythmic gap in the classical upper bound on the generalization error.",
                    "label": 0
                },
                {
                    "sent": "Can be replaced by the logarithm of what he calls the disagreement coefficient.",
                    "label": 0
                },
                {
                    "sent": "So you can replace the logarithm of 1 over epsilon by the logarithm of the disagreement coefficient.",
                    "label": 0
                },
                {
                    "sent": "And this is good news for all concept classes that have a constant disagreement coefficient, because for these classes that the gap will be bridged.",
                    "label": 0
                },
                {
                    "sent": "And actually this was used by a multi darnstaedt with he finds the definition of the disagreement coefficient a little bit.",
                    "label": 0
                },
                {
                    "sent": "And this was used by Multi Township for showing that the one inclusion graph algorithm is optimal for intersection closed classes.",
                    "label": 0
                },
                {
                    "sent": "But as I said the there also, although there's some similarity in the analysis, Konica also broke the analysis in two stages an analyzed The Cave stage with respect to a conditional probability measure and the event on which it is conditioned depends on the 1st K minus one stages.",
                    "label": 0
                },
                {
                    "sent": "So this looks similar.",
                    "label": 0
                },
                {
                    "sent": "But there are also some differences.",
                    "label": 0
                },
                {
                    "sent": "So in C phonics work this.",
                    "label": 0
                },
                {
                    "sent": "Decomposition in two stages happens in the analysis only and has no influence on what the algorithm does.",
                    "label": 0
                },
                {
                    "sent": "He still has the policy of picking just any consistency.",
                    "label": 0
                },
                {
                    "sent": "Part is is and we used this decomposition in the algorithm already.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And second, he conditions against hitting a disagreement region, whereas we condition on hitting a common error region of some of the preceding hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let me briefly discuss some efficiency issues so our algorithm will be efficient whenever the consistency problem can be solved efficiently, and this is pretty good though by inclusion graph algorithm for example, what is has an exponential dependence on the public share links dimension.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then of course there is this open question whether yeah, there's even the open question whether our algorithm LK might be an optimal pick algorithm for some choice of K. In the proceedings you will see a proof that the algorithm L2 treats optimally the concept classes that were used by our end ordner for showing that you cannot take any consistency policies.",
                    "label": 1
                },
                {
                    "sent": "So we can take this probably sub optimal algorithm of choosing any apodosis and then switch to the.",
                    "label": 0
                },
                {
                    "sent": "L2 version of it and then it becomes optimal.",
                    "label": 0
                },
                {
                    "sent": "So there is the question whether LK might perhaps already been optimal algorithms, so we have no matching lower bound.",
                    "label": 0
                },
                {
                    "sent": "And of course, the main challenge is to solve the to close the gap.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}