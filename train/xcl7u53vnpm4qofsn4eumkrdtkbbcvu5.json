{
    "id": "xcl7u53vnpm4qofsn4eumkrdtkbbcvu5",
    "title": "Parameter Learning in Probabilistic Databases: A Least Squares Approach",
    "info": {
        "author": [
            "Bernd Gutmann, Department of Computer Science, KU Leuven",
            "Angelika Kimmig, Faculty of Applied Sciences, University of Freiburg",
            "Luc De Raedt, Department of Computer Science, KU Leuven",
            "Kristian Kersting, Fraunhofer IAIS"
        ],
        "published": "Oct. 10, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd08_gutmann_plip/",
    "segmentation": [
        [
            "OK, so welcome to my talk.",
            "This is joint work with Angela communicate and looked at Ann Kristin casting who is not here today.",
            "So to give you an overview of my talk, I will start with this summary."
        ],
        [
            "Actually.",
            "So we are trying to do learn parameters for Pristiq databases.",
            "First thing is we deal with multi relational data and with structured data.",
            "So there is a multitude of opportunities within this SRL world you can choose your model from and probabilistic databases are amongst the most simple one.",
            "OK.",
            "Microphone.",
            "OK. Ha.",
            "Ha, you're better.",
            "OK, so probabilistic databases are amongst the most simplistic one you can use for this task and we want to do perimetre learning for this.",
            "The problem is normally they are non generative framework so it means you cannot draw samples and use this sampling distribution to learn back probabilities.",
            "This gives you a problem and one way to overcome this is you're using some optimization criterion and then you.",
            "Use gradient method to to optimize this.",
            "Anne.",
            "OK, so the examples you can use for this are queries in our case together with probabilities.",
            "OK, so that's."
        ],
        [
            "The main part and then the rest of the talk.",
            "I will briefly introduce probabilistic databases.",
            "I will say what I mean by learning para meters for them.",
            "Then I will introduce prob log and I will show how you can do permanent there.",
            "And finally I will give you some experimental results."
        ],
        [
            "OK, um so probabilistic data comes from a lot of sources like you're integrating big databases where you have ambiguities and uncertainty of data arising from that and other sources.",
            "You're doing experiments in the wet lab and you accumulate.",
            "Your results are a lot of experiments.",
            "This gives you something like dependencies between genes and diseases, and you're not sure about this dependencies, or you can run text extraction algorithms.",
            "And you want to store the results somehow."
        ],
        [
            "One option to store and to deal with this data is you're using probabilistic databases.",
            "I guess everybody who is familiar with database, so probabilistic databases.",
            "The most simple extension to that you simply add one more comment every table which depicts the probability of the entries there, and then you get basically a distribution over possible worlds, which means you get a distribution over complete databases and then you can ask queries like is a particular element contained in a database.",
            "And the answers are."
        ],
        [
            "Probability OK, so we want to do parimeter learning for this.",
            "This means you already know the entries in your database and you want to adjust the probabilities attached to each entry such that your training data is fit well.",
            "So somehow you can learn this."
        ],
        [
            "Probabilities and you get there these numbers.",
            "OK, that's what we want to do in the remaining of the talk.",
            "I will show you one example for probabilistic database namely Prob.",
            "Log which was introduced by the Hut and Kimmich last year.",
            "And then I will show you how you can do permanent learning there.",
            "OK."
        ],
        [
            "Here's the same example written in problem.",
            "What you can see is there is a probabilistic graph.",
            "Each edge on this graph has a probability, which means when you generate or sample subgraphs from this, every edge has a certain probability to be contained in the sample sub graph on the right hand side you see the same graph model and prob log.",
            "Each edge is modeled as a probabilistic facts and you have some additional background knowledge.",
            "What tells you?",
            "So you can have a path in this graph.",
            "OK, the probabilities attached.",
            "This effects mean that when you sample subprograms from this, you basically roll a biased coin flip."
        ],
        [
            "Coin for effect and you then get the subprogram."
        ],
        [
            "You can do this one time you can do."
        ],
        [
            "Smart time and every time you sample you get a different subprogram or subjects from this."
        ],
        [
            "And every time you get another probability, OK, so you can use this sampling distribution to answer queries.",
            "For instance, you might be interested in going from this gene on the lower left side to the disease, two node on the upper parts, and how you can do this is the following way you go overall."
        ],
        [
            "All subgraphs which have."
        ],
        [
            "Such a path."
        ],
        [
            "There are five."
        ],
        [
            "Six in this graph, and then you."
        ],
        [
            "Calculate probability for this."
        ],
        [
            "Grams and you sum them up."
        ],
        [
            "Obviously it works in this very small example, but when you go up when you go to bigger and larger programs, this will not work because you have exponentially many subprograms, right?",
            "So it's not a very clever idea to do this.",
            "In this kind of paper last year, that and.",
            "Communication and casting, they introduced a way to do this in more clever way, and they're doing this by first generating all possible proofs.",
            "Work free using normal Prolog SLD resolution and then generating a Boolean formula from this and in the next step they build a BDD."
        ],
        [
            "So, um.",
            "Yes, and graph the same query and you can see the resulting BDD.",
            "Obviously you have two possibilities to reach this top node from the clean here first."
        ],
        [
            "Direct pump which you can see in the speedy here and you."
        ],
        [
            "After path over the disease to not, which is here if you're not familiar with PDZ, it means binary decision diagrams and it's if you want so compact way to represent a binary decision tree.",
            "So you start with the full decision tree and then you merge similar subtrees, which then gives you this PD here.",
            "OK, so if you have to speedy then you can use it to calculate."
        ],
        [
            "The probability is in the bottom of where you would start at 1 zero nodes and then you then go up to the root and."
        ],
        [
            "At every node you calculate probabilities up to that node."
        ],
        [
            "And once you reach the root node of this PD, you finally have calculated the probability.",
            "Um?"
        ],
        [
            "So we wanted to perimetre learning and for problem."
        ],
        [
            "It would mean less.",
            "You have your program with."
        ],
        [
            "Or probabilities.",
            "You have some train."
        ],
        [
            "Data and I just showed you how you can answer queries so you can."
        ],
        [
            "Calculate probabilities so you would know how you could calculate this guys over there and.",
            "One way to do parallel learning here is you need some optimization criterion or some error function."
        ],
        [
            "And the most simple choice you can make is that you just want these two distributions to be equal, so."
        ],
        [
            "Captured by this means quitero, it just goes over all training examples and sums up the error and then average, is it?",
            "OK, the reason why we are choosing this here is because we have a non generative model in Bayesian networks for instance."
        ],
        [
            "You want to pair with learning.",
            "It's quite straightforward, so.",
            "You have the sampling distribution which."
        ],
        [
            "I'll show you how you can draw samples from your model, right?",
            "And once you have the samples you can imprint."
        ],
        [
            "Relearned para meters from the samples, and hopefully would end up with the same parameters, OK sampling in our case would mean you draw complete interpretations as a sample, and obviously this is not doable if you have a large program with.",
            "Several thousands and hundreds of hundreds of thousands of facts.",
            "So training that we are using, it's it's not interpretations.",
            "It's curious, so we."
        ],
        [
            "Did permit learning from securities.",
            "But there's a problem because our probabilistic database doesn't tell us how we can draw securities from it.",
            "It just tells us how we can draw interpretations so we don't have a sampling distribution for this.",
            "Anne."
        ],
        [
            "That's why we cannot estimate the probability from our various directly.",
            "That's why we're using the mean squared error function.",
            "OK, we have this optimization criterion and then we just do pretty straightforward approach.",
            "We just calculate the gradient and then we do a gradient descent search.",
            "So we randomly initialize our probabilities.",
            "We start somewhere."
        ],
        [
            "Then we look for the steepest decent we calculate the gradient.",
            "We make one step in that direction and we iterate OK.",
            "So that's basically what we were doing.",
            "The only question is how you can calculate this gradient efficiently.",
            "I showed you already this PDD algorithm to calculate the probability for query and it turns out that you can use the same algorithm with some small modifications to calculate gradient."
        ],
        [
            "OK, here's again.",
            "The same example of the same BDD.",
            "And let's say you want to calculate the gradient with respect to some edge, so then you again start at your 01 terminal nodes.",
            "You run some."
        ],
        [
            "Op algorithm which looks ready."
        ],
        [
            "The same."
        ],
        [
            "Only difference is that you have to do some additional bookkeeping 'cause it might be the case that notes which are irrelevant for the outcome of the of the PD deleted once when ability is generated.",
            "And that's why I need this additional bookkeeping to make sure that you don't miss this cases.",
            "OK, so that's basically what we're doing and how we're doing it.",
            "Just want to point out one last interesting feature of this."
        ],
        [
            "Namely, you can use proofs, so I was telling you that we learned probabilities from queries.",
            "The difference between the query and the proof is quite significant because whenever query in this case you know that there is a path between two nodes, But you don't know which path it is right.",
            "So in this case you have, you might have to direct path.",
            "You might have to path over the disease, one node, on the other hand, if you have a proof, then the proof tells you exactly.",
            "Which way you would take in the in this case, and it turns out that you can represent a proof in our case, just as a concatenation of defects used to make this proof right, so it's pretty straightforward to learn from proofs, because proofs are just curious.",
            "In our case, if you're not so familiar with this, maybe you know.",
            "PFG is provided contact grammars proof.",
            "There might be a complete pass tree 1st, and so and if you have very in that case would be, you know that you can parse a particular sentence and a proof would be that you know the complete parse tree for the sentence.",
            "OK, so."
        ],
        [
            "Let's look at some experimental results on the right hand side you see a big graph.",
            "So we were doing permit learning in this graph.",
            "I showed you already how you can represent graphs as a problem program so.",
            "That's how I did it look like.",
            "OK, the way we got our data is we use this big buy online database which is basically a huge biological graph of several millions of entries.",
            "Every entry in this graph can either be a paper or a gene or disease and dependencies between these reflect.",
            "Some are reflected by edges, right?",
            "And every edge there carries a probability to indicate how strongly dependences.",
            "So this graph is obviously very huge and we are not able at the moment to deal with this complete graph at once.",
            "So what we did is we generated the smaller subgraphs from this, then we had some graphics look like this.",
            "In this graph we randomly sampled some pairs of nodes and we generated the probability for path between these two nodes.",
            "So then we had a set of training examples which is.",
            "Always a tuple of probability and experience.",
            "So next step we dropped the probabilities on original graph and we tried to learn back to probabilities from our training data.",
            "Here you can see the result for this."
        ],
        [
            "On the X axis is see the number of iterations and Y axis depicts error and as you can see the error goes down.",
            "So it basically means our algorithm works.",
            "In this case.",
            "You can also see the more training data you give, the smaller the error of the probabilities is, well.",
            "It's quite obvious because if you give more training data you give more constraints on how your original probability distribution look like.",
            "The error fraternity minimized was this mean squared error?",
            "So when you look at the mean?"
        ],
        [
            "Squared error, you would expect that it goes down, and indeed that's what's happening, and you can also see it goes down quite fast.",
            "I was pointing out that you can use proofs and I was saying proofs gives you more information so."
        ],
        [
            "It happens when you learn from proofs, that's.",
            "The result for this on the X axis to see the fraction of the training data which was presented as a proof.",
            "So when you're on the left hand side, you only have queries and you have this path predicates and when you're on the right hand side you have only proof.",
            "So for every query you know exactly the path which was taken there, and what you can see is the more proofs you give to your algorithm, the better it gets, so both the error on defects, the probability goes down.",
            "Also, the error at the mean squared error goes down.",
            "OK, so to wrap up."
        ],
        [
            "I will show the same sliders beginning.",
            "I was telling you that probabilistic databases are very simple approach to deal with structured and multi relational data.",
            "I was arguing that it's not that straightforward parameter learning there because you don't have a generative model, but once you define this optimization criterion you can use standard gradient descent, optimize it and it turns out for this kind of data we were using it.",
            "It works quite well OK.",
            "Thanks for your attention."
        ],
        [
            "And please come to our poster in the evening.",
            "OK thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so welcome to my talk.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Angela communicate and looked at Ann Kristin casting who is not here today.",
                    "label": 1
                },
                {
                    "sent": "So to give you an overview of my talk, I will start with this summary.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "So we are trying to do learn parameters for Pristiq databases.",
                    "label": 0
                },
                {
                    "sent": "First thing is we deal with multi relational data and with structured data.",
                    "label": 0
                },
                {
                    "sent": "So there is a multitude of opportunities within this SRL world you can choose your model from and probabilistic databases are amongst the most simple one.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Microphone.",
                    "label": 0
                },
                {
                    "sent": "OK. Ha.",
                    "label": 0
                },
                {
                    "sent": "Ha, you're better.",
                    "label": 0
                },
                {
                    "sent": "OK, so probabilistic databases are amongst the most simplistic one you can use for this task and we want to do perimetre learning for this.",
                    "label": 0
                },
                {
                    "sent": "The problem is normally they are non generative framework so it means you cannot draw samples and use this sampling distribution to learn back probabilities.",
                    "label": 0
                },
                {
                    "sent": "This gives you a problem and one way to overcome this is you're using some optimization criterion and then you.",
                    "label": 0
                },
                {
                    "sent": "Use gradient method to to optimize this.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "OK, so the examples you can use for this are queries in our case together with probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main part and then the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "I will briefly introduce probabilistic databases.",
                    "label": 0
                },
                {
                    "sent": "I will say what I mean by learning para meters for them.",
                    "label": 0
                },
                {
                    "sent": "Then I will introduce prob log and I will show how you can do permanent there.",
                    "label": 0
                },
                {
                    "sent": "And finally I will give you some experimental results.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, um so probabilistic data comes from a lot of sources like you're integrating big databases where you have ambiguities and uncertainty of data arising from that and other sources.",
                    "label": 0
                },
                {
                    "sent": "You're doing experiments in the wet lab and you accumulate.",
                    "label": 0
                },
                {
                    "sent": "Your results are a lot of experiments.",
                    "label": 0
                },
                {
                    "sent": "This gives you something like dependencies between genes and diseases, and you're not sure about this dependencies, or you can run text extraction algorithms.",
                    "label": 0
                },
                {
                    "sent": "And you want to store the results somehow.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One option to store and to deal with this data is you're using probabilistic databases.",
                    "label": 1
                },
                {
                    "sent": "I guess everybody who is familiar with database, so probabilistic databases.",
                    "label": 1
                },
                {
                    "sent": "The most simple extension to that you simply add one more comment every table which depicts the probability of the entries there, and then you get basically a distribution over possible worlds, which means you get a distribution over complete databases and then you can ask queries like is a particular element contained in a database.",
                    "label": 0
                },
                {
                    "sent": "And the answers are.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probability OK, so we want to do parimeter learning for this.",
                    "label": 0
                },
                {
                    "sent": "This means you already know the entries in your database and you want to adjust the probabilities attached to each entry such that your training data is fit well.",
                    "label": 0
                },
                {
                    "sent": "So somehow you can learn this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probabilities and you get there these numbers.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what we want to do in the remaining of the talk.",
                    "label": 0
                },
                {
                    "sent": "I will show you one example for probabilistic database namely Prob.",
                    "label": 0
                },
                {
                    "sent": "Log which was introduced by the Hut and Kimmich last year.",
                    "label": 0
                },
                {
                    "sent": "And then I will show you how you can do permanent learning there.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's the same example written in problem.",
                    "label": 0
                },
                {
                    "sent": "What you can see is there is a probabilistic graph.",
                    "label": 0
                },
                {
                    "sent": "Each edge on this graph has a probability, which means when you generate or sample subgraphs from this, every edge has a certain probability to be contained in the sample sub graph on the right hand side you see the same graph model and prob log.",
                    "label": 0
                },
                {
                    "sent": "Each edge is modeled as a probabilistic facts and you have some additional background knowledge.",
                    "label": 0
                },
                {
                    "sent": "What tells you?",
                    "label": 0
                },
                {
                    "sent": "So you can have a path in this graph.",
                    "label": 0
                },
                {
                    "sent": "OK, the probabilities attached.",
                    "label": 0
                },
                {
                    "sent": "This effects mean that when you sample subprograms from this, you basically roll a biased coin flip.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coin for effect and you then get the subprogram.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do this one time you can do.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Smart time and every time you sample you get a different subprogram or subjects from this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And every time you get another probability, OK, so you can use this sampling distribution to answer queries.",
                    "label": 0
                },
                {
                    "sent": "For instance, you might be interested in going from this gene on the lower left side to the disease, two node on the upper parts, and how you can do this is the following way you go overall.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All subgraphs which have.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such a path.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are five.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Six in this graph, and then you.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Calculate probability for this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Grams and you sum them up.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Obviously it works in this very small example, but when you go up when you go to bigger and larger programs, this will not work because you have exponentially many subprograms, right?",
                    "label": 0
                },
                {
                    "sent": "So it's not a very clever idea to do this.",
                    "label": 0
                },
                {
                    "sent": "In this kind of paper last year, that and.",
                    "label": 0
                },
                {
                    "sent": "Communication and casting, they introduced a way to do this in more clever way, and they're doing this by first generating all possible proofs.",
                    "label": 0
                },
                {
                    "sent": "Work free using normal Prolog SLD resolution and then generating a Boolean formula from this and in the next step they build a BDD.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Yes, and graph the same query and you can see the resulting BDD.",
                    "label": 0
                },
                {
                    "sent": "Obviously you have two possibilities to reach this top node from the clean here first.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Direct pump which you can see in the speedy here and you.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After path over the disease to not, which is here if you're not familiar with PDZ, it means binary decision diagrams and it's if you want so compact way to represent a binary decision tree.",
                    "label": 0
                },
                {
                    "sent": "So you start with the full decision tree and then you merge similar subtrees, which then gives you this PD here.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have to speedy then you can use it to calculate.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The probability is in the bottom of where you would start at 1 zero nodes and then you then go up to the root and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At every node you calculate probabilities up to that node.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And once you reach the root node of this PD, you finally have calculated the probability.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we wanted to perimetre learning and for problem.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It would mean less.",
                    "label": 0
                },
                {
                    "sent": "You have your program with.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or probabilities.",
                    "label": 0
                },
                {
                    "sent": "You have some train.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data and I just showed you how you can answer queries so you can.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Calculate probabilities so you would know how you could calculate this guys over there and.",
                    "label": 0
                },
                {
                    "sent": "One way to do parallel learning here is you need some optimization criterion or some error function.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the most simple choice you can make is that you just want these two distributions to be equal, so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Captured by this means quitero, it just goes over all training examples and sums up the error and then average, is it?",
                    "label": 0
                },
                {
                    "sent": "OK, the reason why we are choosing this here is because we have a non generative model in Bayesian networks for instance.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You want to pair with learning.",
                    "label": 0
                },
                {
                    "sent": "It's quite straightforward, so.",
                    "label": 0
                },
                {
                    "sent": "You have the sampling distribution which.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll show you how you can draw samples from your model, right?",
                    "label": 0
                },
                {
                    "sent": "And once you have the samples you can imprint.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relearned para meters from the samples, and hopefully would end up with the same parameters, OK sampling in our case would mean you draw complete interpretations as a sample, and obviously this is not doable if you have a large program with.",
                    "label": 0
                },
                {
                    "sent": "Several thousands and hundreds of hundreds of thousands of facts.",
                    "label": 0
                },
                {
                    "sent": "So training that we are using, it's it's not interpretations.",
                    "label": 0
                },
                {
                    "sent": "It's curious, so we.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did permit learning from securities.",
                    "label": 0
                },
                {
                    "sent": "But there's a problem because our probabilistic database doesn't tell us how we can draw securities from it.",
                    "label": 0
                },
                {
                    "sent": "It just tells us how we can draw interpretations so we don't have a sampling distribution for this.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's why we cannot estimate the probability from our various directly.",
                    "label": 0
                },
                {
                    "sent": "That's why we're using the mean squared error function.",
                    "label": 0
                },
                {
                    "sent": "OK, we have this optimization criterion and then we just do pretty straightforward approach.",
                    "label": 0
                },
                {
                    "sent": "We just calculate the gradient and then we do a gradient descent search.",
                    "label": 0
                },
                {
                    "sent": "So we randomly initialize our probabilities.",
                    "label": 0
                },
                {
                    "sent": "We start somewhere.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we look for the steepest decent we calculate the gradient.",
                    "label": 0
                },
                {
                    "sent": "We make one step in that direction and we iterate OK.",
                    "label": 0
                },
                {
                    "sent": "So that's basically what we were doing.",
                    "label": 0
                },
                {
                    "sent": "The only question is how you can calculate this gradient efficiently.",
                    "label": 0
                },
                {
                    "sent": "I showed you already this PDD algorithm to calculate the probability for query and it turns out that you can use the same algorithm with some small modifications to calculate gradient.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here's again.",
                    "label": 0
                },
                {
                    "sent": "The same example of the same BDD.",
                    "label": 0
                },
                {
                    "sent": "And let's say you want to calculate the gradient with respect to some edge, so then you again start at your 01 terminal nodes.",
                    "label": 0
                },
                {
                    "sent": "You run some.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Op algorithm which looks ready.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only difference is that you have to do some additional bookkeeping 'cause it might be the case that notes which are irrelevant for the outcome of the of the PD deleted once when ability is generated.",
                    "label": 0
                },
                {
                    "sent": "And that's why I need this additional bookkeeping to make sure that you don't miss this cases.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's basically what we're doing and how we're doing it.",
                    "label": 0
                },
                {
                    "sent": "Just want to point out one last interesting feature of this.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Namely, you can use proofs, so I was telling you that we learned probabilities from queries.",
                    "label": 0
                },
                {
                    "sent": "The difference between the query and the proof is quite significant because whenever query in this case you know that there is a path between two nodes, But you don't know which path it is right.",
                    "label": 0
                },
                {
                    "sent": "So in this case you have, you might have to direct path.",
                    "label": 0
                },
                {
                    "sent": "You might have to path over the disease, one node, on the other hand, if you have a proof, then the proof tells you exactly.",
                    "label": 0
                },
                {
                    "sent": "Which way you would take in the in this case, and it turns out that you can represent a proof in our case, just as a concatenation of defects used to make this proof right, so it's pretty straightforward to learn from proofs, because proofs are just curious.",
                    "label": 0
                },
                {
                    "sent": "In our case, if you're not so familiar with this, maybe you know.",
                    "label": 0
                },
                {
                    "sent": "PFG is provided contact grammars proof.",
                    "label": 0
                },
                {
                    "sent": "There might be a complete pass tree 1st, and so and if you have very in that case would be, you know that you can parse a particular sentence and a proof would be that you know the complete parse tree for the sentence.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at some experimental results on the right hand side you see a big graph.",
                    "label": 0
                },
                {
                    "sent": "So we were doing permit learning in this graph.",
                    "label": 0
                },
                {
                    "sent": "I showed you already how you can represent graphs as a problem program so.",
                    "label": 0
                },
                {
                    "sent": "That's how I did it look like.",
                    "label": 0
                },
                {
                    "sent": "OK, the way we got our data is we use this big buy online database which is basically a huge biological graph of several millions of entries.",
                    "label": 0
                },
                {
                    "sent": "Every entry in this graph can either be a paper or a gene or disease and dependencies between these reflect.",
                    "label": 0
                },
                {
                    "sent": "Some are reflected by edges, right?",
                    "label": 0
                },
                {
                    "sent": "And every edge there carries a probability to indicate how strongly dependences.",
                    "label": 0
                },
                {
                    "sent": "So this graph is obviously very huge and we are not able at the moment to deal with this complete graph at once.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we generated the smaller subgraphs from this, then we had some graphics look like this.",
                    "label": 0
                },
                {
                    "sent": "In this graph we randomly sampled some pairs of nodes and we generated the probability for path between these two nodes.",
                    "label": 0
                },
                {
                    "sent": "So then we had a set of training examples which is.",
                    "label": 0
                },
                {
                    "sent": "Always a tuple of probability and experience.",
                    "label": 0
                },
                {
                    "sent": "So next step we dropped the probabilities on original graph and we tried to learn back to probabilities from our training data.",
                    "label": 0
                },
                {
                    "sent": "Here you can see the result for this.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the X axis is see the number of iterations and Y axis depicts error and as you can see the error goes down.",
                    "label": 0
                },
                {
                    "sent": "So it basically means our algorithm works.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "You can also see the more training data you give, the smaller the error of the probabilities is, well.",
                    "label": 0
                },
                {
                    "sent": "It's quite obvious because if you give more training data you give more constraints on how your original probability distribution look like.",
                    "label": 0
                },
                {
                    "sent": "The error fraternity minimized was this mean squared error?",
                    "label": 0
                },
                {
                    "sent": "So when you look at the mean?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Squared error, you would expect that it goes down, and indeed that's what's happening, and you can also see it goes down quite fast.",
                    "label": 0
                },
                {
                    "sent": "I was pointing out that you can use proofs and I was saying proofs gives you more information so.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It happens when you learn from proofs, that's.",
                    "label": 0
                },
                {
                    "sent": "The result for this on the X axis to see the fraction of the training data which was presented as a proof.",
                    "label": 1
                },
                {
                    "sent": "So when you're on the left hand side, you only have queries and you have this path predicates and when you're on the right hand side you have only proof.",
                    "label": 0
                },
                {
                    "sent": "So for every query you know exactly the path which was taken there, and what you can see is the more proofs you give to your algorithm, the better it gets, so both the error on defects, the probability goes down.",
                    "label": 0
                },
                {
                    "sent": "Also, the error at the mean squared error goes down.",
                    "label": 0
                },
                {
                    "sent": "OK, so to wrap up.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will show the same sliders beginning.",
                    "label": 0
                },
                {
                    "sent": "I was telling you that probabilistic databases are very simple approach to deal with structured and multi relational data.",
                    "label": 1
                },
                {
                    "sent": "I was arguing that it's not that straightforward parameter learning there because you don't have a generative model, but once you define this optimization criterion you can use standard gradient descent, optimize it and it turns out for this kind of data we were using it.",
                    "label": 0
                },
                {
                    "sent": "It works quite well OK.",
                    "label": 0
                },
                {
                    "sent": "Thanks for your attention.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And please come to our poster in the evening.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                }
            ]
        }
    }
}