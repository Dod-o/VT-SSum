{
    "id": "tikhanj76ec633zbaqbpwqgmjnyqsl3l",
    "title": "Cross-View Action Recognition via a Transferable Dictionary Pair",
    "info": {
        "author": [
            "Jingjing Zheng, University of Maryland"
        ],
        "published": "Oct. 9, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/bmvc2012_zheng_transferable_dictionary/",
    "segmentation": [
        [
            "Hello everyone.",
            "For account to my presentation or Crossville action recognition, while transferable division pair this work is a joint works with Doctor John, Doctor Phillips, and Chilapa.",
            "The problem."
        ],
        [
            "Of course we will actually recognition or rallies when the viewpoint of the training data is different from the viewpoint of test test videos.",
            "So here from this slide you can say two videos for action waving, saying from 2 viewpoints.",
            "The first point is called the Southfield, while the second viewpoint is called the Target view.",
            "With the training, videos are absorbed from the source field where the cat videos are observed from Target View.",
            "According to the General framework of classification with first extractor, discriminative features X1 and X2 for the training and test videos, respectively, and then using the training features X one and corresponding label.",
            "Why we train our classifier F1 for the action women thinks the test videos are from a different view, we have ask.",
            "Can I directly use the Casper F-12 in the from Southfield to classify the unlabeled test videos from the target view?",
            "The answer is of course now.",
            "Very fun that approaches that are effective for action recognition tend to perform very poorly when applied on datasets with large view variations.",
            "This is because the one action unit looks very different across different views and the such that the action models then from the South will become less discriminative for recognizing actions from a different view."
        ],
        [
            "So in order to directly use the classifier, everyone trained from the Southfield to classify the unlabeled test video from their target view.",
            "We had to use a view invariant feature representation and the view invent feature representation.",
            "We would like videos that record the same action across different views to outline the in a common View Inventor feature space.",
            "This means that they don't need to differentiate the viewpoint or training videos from the viewpoint of test videos.",
            "So in this way, general approaches for action recognition can also be directly used here.",
            "So the problem of Crossville action recognition becomes how to obtain a view invented feature representation.",
            "The hope is that we have pairs of videos that record the same action across the two views.",
            "Those actions are.",
            "Although the in both views we call the shared actions.",
            "Note that the shared actions are exclusive from the test actions.",
            "The reason that we point out is that the lender will invent feature.",
            "Presentation is not affected by the test actions.",
            "So finally the problem of Crossville.",
            "Action recognition becomes how to obtain a view invent feature representation by making."
        ],
        [
            "Also shared actions.",
            "Here we present to our framework for Crossville action recognition based on sparse representations using a transferable diction pair.",
            "This transferable diction pair consists of two dictionaries that correspond to the source and tag levels, respectively.",
            "The two dictionaries are learned simultaneously from two sets of videos that recall the same action across the two views.",
            "At the same time, the two dictionaries are learned by encouraging encouraging each video in a pair to have the same spouse repetitions.",
            "In other words, we transfer this pass codes of the share action videos from the source field to the shared action videos from the target view.",
            "So the test for prediction pair things features between the two views that are useful for action recognition.",
            "So let's talk about more how to learn are transferable."
        ],
        [
            "Asian pear here wait?",
            "We learn the transferable addition pair by encouraging each video in apparent ahead seems sparse.",
            "Representations, then, given a lender dickering pair DS and DT will obtain the sparse feature representations V1 and V2 for the training and test videos, Exelon next two respectively, and then using the Special Code League One and correspond label why we can train our classifier.",
            "F1 for the.",
            "Election waving here.",
            "The cashier trainer based on this passcode can be directly used to classify the unable test videos from the target view.",
            "This it cause the DS and DT are learned by forcing two sets of videos that recall the same action or cross too weird to have the same space representations.",
            "It means that if X1 and X2 come from the same action class, then they go on and they too will be similar.",
            "Uh, so.",
            "So the problem of course few action recognition becomes how to learn or transportation paired."
        ],
        [
            "Here we consider 2 settings for learning or Transferable Edition pair.",
            "The unsupervised setting on the supervised setting for unsupervised setting.",
            "Videos of shared actions are unlabeled where first supervised setting videos or sugarcanes are labeled."
        ],
        [
            "In this slide, let me give a brief review of the sparse coding and digital learning.",
            "SVD algorithm is well known for officially learning an Overcomplete dictionary deal from a set of input signals.",
            "So here let X be a set of N dimensional input signals.",
            "The dictionary D and sparse code.",
            "They can be learned by solving the minimalization of the reconstruction error and the sparsity constraint.",
            "When the dictionary is given, the sparse coding space repetition, they can be learned by the mean machine or the construction construction error and sparsity constraint using the circularizing personal algorithm."
        ],
        [
            "For the unsupervised setting, our goal is to find the discriminative feature representations that are the same for different views on the same action.",
            "We can.",
            "We achieve this goal by making use of shared videos that require the same action across the two views.",
            "The object function for the unsupervised setting is given by the minimalization of the reconstruction error from both views and the sparsity constraint.",
            "So let the DS and DT can be learned by using the SVD algorithm."
        ],
        [
            "For a supervised setting or action categories, or should action videos are available in both views, we can leverage this information to learn a word discriminative disrepair in what?",
            "Until this go away in coverage label.",
            "Consistent regularization term into the object function.",
            "So here the auto function is given by the manipulation or reconstruction error and the distributor tools best code error.",
            "Here the metrics A is a linear transformation metrics and the metric, you are the ideal discriminative passcode for pairs of videos.",
            "Here they this order function not only regularize the spark powder of pairs of videos to be the same, but also regularize the sparse code there to be more like the ideal discriminative sparkles Q after linear transformation A.",
            "So let me talk about what are the metrics.",
            "Q is another related motivation behind this.",
            "First we partition the additional terms into disjoint subsets and each subset is responsible for representing videos from one action class.",
            "So from this slide you can see that the dictionary terms DS and DT are divided into 3 subsets and class one.",
            "Somehow someone explain to an extra 3.",
            "Can be represented by the 1st subset of dictionary terms.",
            "They will need development given to an device ring.",
            "Similarly, a cultural samples can be well represented by the 2nd subset of dictionary items.",
            "In this way, the metrics Q is block diagonal.",
            "And each block corresponds to one subset of dictionary atoms, and one has samples.",
            "So this resulting or correspondence between class samples and.",
            "So this loss in performance between the dictionary terms and action labels.",
            "The intuition behind this idea is that videos of the same from the same class tend to have similar or same features, so so each video can be well represented by by the same subset of dictionary terms.",
            "On the contrary, videos from different classes to have different features and they should be well represented by strong subsets or dictionary items."
        ],
        [
            "In this slide, we show how to extend our approach to multiple action recognition.",
            "Assume that their piece of use an one target view of the corresponding organ function can be given by the minimal issue or reconstruct error from Wall South Bill and the target views.",
            "You can see that the.",
            "Videos that report the same action across all wheels are aligned in a carbon.",
            "Will invent us pass feature space so any action models land using training videos from the also feels can be directly used to classify and label test videos from the target view."
        ],
        [
            "They evaluate our purge using the XMR smartview datasets.",
            "This data set consists of a set of users and one top view of our love and actions performed 3 times by 10 actors.",
            "From this slide, because they'll figure where each column corresponds to 1 camera view and each additional correspond to what action class seeing from 5 different camera views.",
            "Each action video is represented using global and local features.",
            "Way for following the follow the standard experiment, setting your and use the new action class.",
            "Our strategy for choosing the test action.",
            "This means that each time we only consider one action class for selecting the Test Action report, the classification accuracy by averaging over wall possible combinations of.",
            "Directing their test action."
        ],
        [
            "Way first we evaluate our approach for transferring action models or cross Paris views.",
            "So here we learn two different pairs.",
            "The first is independent dictionary pair and the other is transferable.",
            "To compare for the independent edition pair, they learn the dictionaries separately for each view without any knowledge transfer.",
            "Wherefore transferable diction pair.",
            "They then the dictionaries or according to our unsupervised.",
            "And the Super breast approaches respectively.",
            "So here we come octane, two different space repetitions baseline based on each sparse representation.",
            "We use the key nearest neighbor classifier to classify unlabeled test videos from the target view.",
            "The table one shows the recognition accuracy for 20 combinations of the source and target views.",
            "You can see that the key nearest neighbor classifier without any large transfer performs very poorly and.",
            "And therefore most of the combinations the recognition accuracy is less than 50%.",
            "On the contrary, both of us approaches achieves very high accuracy and.",
            "This demonstrates demonstrates the transferability of the simultaneously.",
            "Then the dictionary pair.",
            "In addition, the higher recognition accuracy obtained by our supervised approach demonstrates that the diction pair land using labeled shared action videos is worth discriminative."
        ],
        [
            "Compared to other unsupervised approaches, our unsupervised approach yields a much better performance and.",
            "Choose more than 90% recognition accuracy for half of water combinations of Paris views."
        ],
        [
            "Compare that to one of the state of art approach.",
            "A supervised approach achieve, achieve.",
            "Leonie perfect performance for all the cases except for the case where the camera for is the source of the target view.",
            "It is interesting to note that for the case where camera for is the source or target view, the recognition accuracy is a little lower than other combinations.",
            "They say because the camera for is set above the actions and the different actions look the same from the top view."
        ],
        [
            "Finally, we evaluate our approach for multiple action recognition.",
            "You can see that both of our purchase achieve nearly perfect performance for world cases.",
            "Is that for a case where the camera for is the target view?",
            "This again shows shows that it is harder to transfer action models across views that involves the camera form."
        ],
        [
            "Here we also choose the recognition accuracy for each action category in each target revealed.",
            "The the top bar figure corresponds to our unsupervised approach and the bottom bar figure correspond to a supervisor approach.",
            "The film bars in a group correspond to the recognition accuracy for each for one action category.",
            "Uh.",
            "You can see that except for the case where the camera for is the target, if you both are super, both our purchase achieves more than 90% recognition accuracy.",
            "This again shows that it's harder to transfer action models or from across views that involves the top view."
        ],
        [
            "So in summary, we print method for Crossville action recognition.",
            "At least approach can directly exploit the video level correspondence, and it was so bridges the gap or special presentations or pairs of videos that record the same action or across different views.",
            "This approach can also be applied to much view action recognition.",
            "It also achieves the state of the art performance."
        ],
        [
            "Oh, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everyone.",
                    "label": 0
                },
                {
                    "sent": "For account to my presentation or Crossville action recognition, while transferable division pair this work is a joint works with Doctor John, Doctor Phillips, and Chilapa.",
                    "label": 0
                },
                {
                    "sent": "The problem.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course we will actually recognition or rallies when the viewpoint of the training data is different from the viewpoint of test test videos.",
                    "label": 0
                },
                {
                    "sent": "So here from this slide you can say two videos for action waving, saying from 2 viewpoints.",
                    "label": 0
                },
                {
                    "sent": "The first point is called the Southfield, while the second viewpoint is called the Target view.",
                    "label": 0
                },
                {
                    "sent": "With the training, videos are absorbed from the source field where the cat videos are observed from Target View.",
                    "label": 0
                },
                {
                    "sent": "According to the General framework of classification with first extractor, discriminative features X1 and X2 for the training and test videos, respectively, and then using the training features X one and corresponding label.",
                    "label": 0
                },
                {
                    "sent": "Why we train our classifier F1 for the action women thinks the test videos are from a different view, we have ask.",
                    "label": 0
                },
                {
                    "sent": "Can I directly use the Casper F-12 in the from Southfield to classify the unlabeled test videos from the target view?",
                    "label": 0
                },
                {
                    "sent": "The answer is of course now.",
                    "label": 0
                },
                {
                    "sent": "Very fun that approaches that are effective for action recognition tend to perform very poorly when applied on datasets with large view variations.",
                    "label": 0
                },
                {
                    "sent": "This is because the one action unit looks very different across different views and the such that the action models then from the South will become less discriminative for recognizing actions from a different view.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to directly use the classifier, everyone trained from the Southfield to classify the unlabeled test video from their target view.",
                    "label": 0
                },
                {
                    "sent": "We had to use a view invariant feature representation and the view invent feature representation.",
                    "label": 0
                },
                {
                    "sent": "We would like videos that record the same action across different views to outline the in a common View Inventor feature space.",
                    "label": 0
                },
                {
                    "sent": "This means that they don't need to differentiate the viewpoint or training videos from the viewpoint of test videos.",
                    "label": 0
                },
                {
                    "sent": "So in this way, general approaches for action recognition can also be directly used here.",
                    "label": 0
                },
                {
                    "sent": "So the problem of Crossville action recognition becomes how to obtain a view invented feature representation.",
                    "label": 0
                },
                {
                    "sent": "The hope is that we have pairs of videos that record the same action across the two views.",
                    "label": 0
                },
                {
                    "sent": "Those actions are.",
                    "label": 0
                },
                {
                    "sent": "Although the in both views we call the shared actions.",
                    "label": 0
                },
                {
                    "sent": "Note that the shared actions are exclusive from the test actions.",
                    "label": 1
                },
                {
                    "sent": "The reason that we point out is that the lender will invent feature.",
                    "label": 0
                },
                {
                    "sent": "Presentation is not affected by the test actions.",
                    "label": 0
                },
                {
                    "sent": "So finally the problem of Crossville.",
                    "label": 0
                },
                {
                    "sent": "Action recognition becomes how to obtain a view invent feature representation by making.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also shared actions.",
                    "label": 0
                },
                {
                    "sent": "Here we present to our framework for Crossville action recognition based on sparse representations using a transferable diction pair.",
                    "label": 0
                },
                {
                    "sent": "This transferable diction pair consists of two dictionaries that correspond to the source and tag levels, respectively.",
                    "label": 0
                },
                {
                    "sent": "The two dictionaries are learned simultaneously from two sets of videos that recall the same action across the two views.",
                    "label": 0
                },
                {
                    "sent": "At the same time, the two dictionaries are learned by encouraging encouraging each video in a pair to have the same spouse repetitions.",
                    "label": 1
                },
                {
                    "sent": "In other words, we transfer this pass codes of the share action videos from the source field to the shared action videos from the target view.",
                    "label": 0
                },
                {
                    "sent": "So the test for prediction pair things features between the two views that are useful for action recognition.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about more how to learn are transferable.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Asian pear here wait?",
                    "label": 0
                },
                {
                    "sent": "We learn the transferable addition pair by encouraging each video in apparent ahead seems sparse.",
                    "label": 1
                },
                {
                    "sent": "Representations, then, given a lender dickering pair DS and DT will obtain the sparse feature representations V1 and V2 for the training and test videos, Exelon next two respectively, and then using the Special Code League One and correspond label why we can train our classifier.",
                    "label": 0
                },
                {
                    "sent": "F1 for the.",
                    "label": 0
                },
                {
                    "sent": "Election waving here.",
                    "label": 0
                },
                {
                    "sent": "The cashier trainer based on this passcode can be directly used to classify the unable test videos from the target view.",
                    "label": 0
                },
                {
                    "sent": "This it cause the DS and DT are learned by forcing two sets of videos that recall the same action or cross too weird to have the same space representations.",
                    "label": 1
                },
                {
                    "sent": "It means that if X1 and X2 come from the same action class, then they go on and they too will be similar.",
                    "label": 0
                },
                {
                    "sent": "Uh, so.",
                    "label": 0
                },
                {
                    "sent": "So the problem of course few action recognition becomes how to learn or transportation paired.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we consider 2 settings for learning or Transferable Edition pair.",
                    "label": 0
                },
                {
                    "sent": "The unsupervised setting on the supervised setting for unsupervised setting.",
                    "label": 0
                },
                {
                    "sent": "Videos of shared actions are unlabeled where first supervised setting videos or sugarcanes are labeled.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this slide, let me give a brief review of the sparse coding and digital learning.",
                    "label": 0
                },
                {
                    "sent": "SVD algorithm is well known for officially learning an Overcomplete dictionary deal from a set of input signals.",
                    "label": 0
                },
                {
                    "sent": "So here let X be a set of N dimensional input signals.",
                    "label": 1
                },
                {
                    "sent": "The dictionary D and sparse code.",
                    "label": 0
                },
                {
                    "sent": "They can be learned by solving the minimalization of the reconstruction error and the sparsity constraint.",
                    "label": 1
                },
                {
                    "sent": "When the dictionary is given, the sparse coding space repetition, they can be learned by the mean machine or the construction construction error and sparsity constraint using the circularizing personal algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the unsupervised setting, our goal is to find the discriminative feature representations that are the same for different views on the same action.",
                    "label": 1
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "We achieve this goal by making use of shared videos that require the same action across the two views.",
                    "label": 0
                },
                {
                    "sent": "The object function for the unsupervised setting is given by the minimalization of the reconstruction error from both views and the sparsity constraint.",
                    "label": 0
                },
                {
                    "sent": "So let the DS and DT can be learned by using the SVD algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For a supervised setting or action categories, or should action videos are available in both views, we can leverage this information to learn a word discriminative disrepair in what?",
                    "label": 0
                },
                {
                    "sent": "Until this go away in coverage label.",
                    "label": 0
                },
                {
                    "sent": "Consistent regularization term into the object function.",
                    "label": 0
                },
                {
                    "sent": "So here the auto function is given by the manipulation or reconstruction error and the distributor tools best code error.",
                    "label": 0
                },
                {
                    "sent": "Here the metrics A is a linear transformation metrics and the metric, you are the ideal discriminative passcode for pairs of videos.",
                    "label": 0
                },
                {
                    "sent": "Here they this order function not only regularize the spark powder of pairs of videos to be the same, but also regularize the sparse code there to be more like the ideal discriminative sparkles Q after linear transformation A.",
                    "label": 0
                },
                {
                    "sent": "So let me talk about what are the metrics.",
                    "label": 0
                },
                {
                    "sent": "Q is another related motivation behind this.",
                    "label": 0
                },
                {
                    "sent": "First we partition the additional terms into disjoint subsets and each subset is responsible for representing videos from one action class.",
                    "label": 0
                },
                {
                    "sent": "So from this slide you can see that the dictionary terms DS and DT are divided into 3 subsets and class one.",
                    "label": 0
                },
                {
                    "sent": "Somehow someone explain to an extra 3.",
                    "label": 0
                },
                {
                    "sent": "Can be represented by the 1st subset of dictionary terms.",
                    "label": 0
                },
                {
                    "sent": "They will need development given to an device ring.",
                    "label": 0
                },
                {
                    "sent": "Similarly, a cultural samples can be well represented by the 2nd subset of dictionary items.",
                    "label": 0
                },
                {
                    "sent": "In this way, the metrics Q is block diagonal.",
                    "label": 0
                },
                {
                    "sent": "And each block corresponds to one subset of dictionary atoms, and one has samples.",
                    "label": 0
                },
                {
                    "sent": "So this resulting or correspondence between class samples and.",
                    "label": 0
                },
                {
                    "sent": "So this loss in performance between the dictionary terms and action labels.",
                    "label": 0
                },
                {
                    "sent": "The intuition behind this idea is that videos of the same from the same class tend to have similar or same features, so so each video can be well represented by by the same subset of dictionary terms.",
                    "label": 0
                },
                {
                    "sent": "On the contrary, videos from different classes to have different features and they should be well represented by strong subsets or dictionary items.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this slide, we show how to extend our approach to multiple action recognition.",
                    "label": 1
                },
                {
                    "sent": "Assume that their piece of use an one target view of the corresponding organ function can be given by the minimal issue or reconstruct error from Wall South Bill and the target views.",
                    "label": 1
                },
                {
                    "sent": "You can see that the.",
                    "label": 1
                },
                {
                    "sent": "Videos that report the same action across all wheels are aligned in a carbon.",
                    "label": 0
                },
                {
                    "sent": "Will invent us pass feature space so any action models land using training videos from the also feels can be directly used to classify and label test videos from the target view.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They evaluate our purge using the XMR smartview datasets.",
                    "label": 0
                },
                {
                    "sent": "This data set consists of a set of users and one top view of our love and actions performed 3 times by 10 actors.",
                    "label": 0
                },
                {
                    "sent": "From this slide, because they'll figure where each column corresponds to 1 camera view and each additional correspond to what action class seeing from 5 different camera views.",
                    "label": 0
                },
                {
                    "sent": "Each action video is represented using global and local features.",
                    "label": 0
                },
                {
                    "sent": "Way for following the follow the standard experiment, setting your and use the new action class.",
                    "label": 0
                },
                {
                    "sent": "Our strategy for choosing the test action.",
                    "label": 1
                },
                {
                    "sent": "This means that each time we only consider one action class for selecting the Test Action report, the classification accuracy by averaging over wall possible combinations of.",
                    "label": 0
                },
                {
                    "sent": "Directing their test action.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Way first we evaluate our approach for transferring action models or cross Paris views.",
                    "label": 0
                },
                {
                    "sent": "So here we learn two different pairs.",
                    "label": 0
                },
                {
                    "sent": "The first is independent dictionary pair and the other is transferable.",
                    "label": 0
                },
                {
                    "sent": "To compare for the independent edition pair, they learn the dictionaries separately for each view without any knowledge transfer.",
                    "label": 0
                },
                {
                    "sent": "Wherefore transferable diction pair.",
                    "label": 0
                },
                {
                    "sent": "They then the dictionaries or according to our unsupervised.",
                    "label": 0
                },
                {
                    "sent": "And the Super breast approaches respectively.",
                    "label": 0
                },
                {
                    "sent": "So here we come octane, two different space repetitions baseline based on each sparse representation.",
                    "label": 0
                },
                {
                    "sent": "We use the key nearest neighbor classifier to classify unlabeled test videos from the target view.",
                    "label": 0
                },
                {
                    "sent": "The table one shows the recognition accuracy for 20 combinations of the source and target views.",
                    "label": 0
                },
                {
                    "sent": "You can see that the key nearest neighbor classifier without any large transfer performs very poorly and.",
                    "label": 0
                },
                {
                    "sent": "And therefore most of the combinations the recognition accuracy is less than 50%.",
                    "label": 0
                },
                {
                    "sent": "On the contrary, both of us approaches achieves very high accuracy and.",
                    "label": 0
                },
                {
                    "sent": "This demonstrates demonstrates the transferability of the simultaneously.",
                    "label": 0
                },
                {
                    "sent": "Then the dictionary pair.",
                    "label": 0
                },
                {
                    "sent": "In addition, the higher recognition accuracy obtained by our supervised approach demonstrates that the diction pair land using labeled shared action videos is worth discriminative.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compared to other unsupervised approaches, our unsupervised approach yields a much better performance and.",
                    "label": 0
                },
                {
                    "sent": "Choose more than 90% recognition accuracy for half of water combinations of Paris views.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compare that to one of the state of art approach.",
                    "label": 0
                },
                {
                    "sent": "A supervised approach achieve, achieve.",
                    "label": 0
                },
                {
                    "sent": "Leonie perfect performance for all the cases except for the case where the camera for is the source of the target view.",
                    "label": 0
                },
                {
                    "sent": "It is interesting to note that for the case where camera for is the source or target view, the recognition accuracy is a little lower than other combinations.",
                    "label": 0
                },
                {
                    "sent": "They say because the camera for is set above the actions and the different actions look the same from the top view.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, we evaluate our approach for multiple action recognition.",
                    "label": 0
                },
                {
                    "sent": "You can see that both of our purchase achieve nearly perfect performance for world cases.",
                    "label": 0
                },
                {
                    "sent": "Is that for a case where the camera for is the target view?",
                    "label": 0
                },
                {
                    "sent": "This again shows shows that it is harder to transfer action models across views that involves the camera form.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we also choose the recognition accuracy for each action category in each target revealed.",
                    "label": 0
                },
                {
                    "sent": "The the top bar figure corresponds to our unsupervised approach and the bottom bar figure correspond to a supervisor approach.",
                    "label": 0
                },
                {
                    "sent": "The film bars in a group correspond to the recognition accuracy for each for one action category.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "You can see that except for the case where the camera for is the target, if you both are super, both our purchase achieves more than 90% recognition accuracy.",
                    "label": 0
                },
                {
                    "sent": "This again shows that it's harder to transfer action models or from across views that involves the top view.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary, we print method for Crossville action recognition.",
                    "label": 0
                },
                {
                    "sent": "At least approach can directly exploit the video level correspondence, and it was so bridges the gap or special presentations or pairs of videos that record the same action or across different views.",
                    "label": 1
                },
                {
                    "sent": "This approach can also be applied to much view action recognition.",
                    "label": 0
                },
                {
                    "sent": "It also achieves the state of the art performance.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, thank you.",
                    "label": 0
                }
            ]
        }
    }
}