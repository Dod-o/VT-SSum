{
    "id": "rhdv6tlpft6nfdqauogw7x3hxwzmqs7q",
    "title": "Inexact Search Directions in Interior Point Methods for Large Scale Optimization",
    "info": {
        "author": [
            "Jacek Gondzio, School of Mathematics, University of Edinburgh"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science",
            "Top->Mathematics"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_gondzio_optimization/",
    "segmentation": [
        [
            "Thank you very much, John, Philip and Mike in prison in the UK for giving me this opportunity of conveying some of the ideas about interior point methods to you.",
            "I have to confess that when guys approached me to come here, I was a little bit embarrassed because my research does not really touch probabilistic aspects so you wouldn't see much of the word probabilistic in it.",
            "But I tried to make a connection to what you might be interested in, and in particular one of these connections is that maybe in exact directions, which I will show you they can work in interior Point method.",
            "Maybe this inexact directions can somehow be constructed by probabilistic methods.",
            "Let's have a look."
        ],
        [
            "Edit.",
            "Next there is certain enthusiasm around machine learning community for the 1st order methods, and obviously they deliver good quality solutions to many of your problems, but it's very easy to throw out interior Point methods saying oh they require 2nd order information.",
            "This will be way way to difficult to estimate.",
            "I'd like to convince you that it does not necessarily have to be so that I actually interior point methods can compete on many of.",
            "Machine learning problems with first order methods.",
            "The way to achieve it is to replace exact search directions within exact ones, compute them much, much faster than the standard interior point method would do.",
            "But obviously this needs using an exact mutant direction, so some iterative methods will have to be involved and they do not work unless properly preconditioned.",
            "And so on.",
            "An 2 problems on which I'd like to show how it works are compressed sensing.",
            "Or more generally, sparse approximations and Google problem."
        ],
        [
            "Right?",
            "What is good about first order methods?",
            "They are usually applied to solve these problems where the functions F&C are convex and they may have some enjoyable features like smoothness, separability.",
            "Sometimes they are strongly convex, but what is?",
            "At disadvantage of 1st order methods is that they fight.",
            "They struggle if there were a complicated feasible set.",
            "So unless the feasible set is something really easy or in a box a hyperplane, you cannot really successfully use first order methods.",
            "And there is a broad class of optimization problems where you have to respect constraints or at least get close to satisfying them.",
            "And then I think interior point methods come.",
            "At the rescue.",
            "They are typically used for this setting you have.",
            "To minimize the function subject to inequality and equality constraints and the most well known classes of problems are linear and quadratic optimization.",
            "Here we only."
        ],
        [
            "Have inequality's on X and linear constraints in quadratic optimization this would be quadratic nonlinear optimization.",
            "Any general functions G&H 2nd order cone?",
            "Now X belongs to a Kronecker product of counts or semidefinite optimization, where X is positive semidefinite matrix.",
            "These are successful areas for for interior Point."
        ],
        [
            "Well, here's the observation.",
            "1st Order methods we can prove.",
            "Well, not we.",
            "You can prove complexity in oh of one over epsilon or one over epsilon squared.",
            "And indeed these methods go for the first few iterations very quickly and get you a rough approximation of the solution.",
            "But should you like to get to high accuracy that will take eternity on many real problems.",
            "The thing is, machine learning community does not necessarily need this high accuracy frequently.",
            "On the other hand, Interpoint.",
            "Use a second order information because use Newton has this fantastic complexity when you compare it with this, because this is the log of one over epsilon produces accurate solutions, never does many iterations, but one iteration may last forever.",
            "So you either have many iterations to wait infinite time, or you have one iteration that takes one hour of heavy sweating of your."
        ],
        [
            "Computer.",
            "That's the choice.",
            "And obviously there is a big difference between one over epsilon and log of 1 / 1 over epsilon.",
            "This is a small number compared to to that, but that's an obvious thing.",
            "Anyway, I do not think I will manage to change your appreciation for 1st order methods, but let me just start a little bit.",
            "This Hornets nest and say give Ipms eye consideration.",
            "Sometimes they might do."
        ],
        [
            "Good job for you.",
            "So now a few slides."
        ],
        [
            "About interior point methods.",
            "I will focus on linear and quadratic optimization problems and my assumptions will be that a has full rank and Q is positive semidefinite, so it's a convex optimization problem.",
            "Dimensions may be large and I would also like to make this assumption that matrices A&QI would never like to store them.",
            "I would like to use them only as the operator because if the problem is large, storing these matrices would be out of question.",
            "Doing factorizations this is just Daft idea, we would never go into this.",
            "So let's assume we will only use these matrices to do matrix vector products.",
            "Just like the 1st order methods.",
            "And in many applications actually you can do these in the low complexity with low complexity algorithms should a be matrix that you can use fast Fourier transforms rather than storing a dense matrix and doing a heavy multiplication with it by fast Fourier transform you will get quickly the result."
        ],
        [
            "Right, there are three things which are responsible for.",
            "Interior point methods and they are accumulated on this slide.",
            "The first is this beauty.",
            "The logarithmic barrier function, which has the feature that rather than writing explicitly inequality X greater than zero, you will.",
            "Minimize this function.",
            "Minimizing this function throws things away from this axis, so it's a barrier on X approaching zero.",
            "Very easy concept.",
            "Then we need duality theory and this for quadratic optimization would lead to such a system of equations and the rest is solving the system of nonlinear equations and Newton method is obviously a natural candidate to deliver good technique to do so.",
            "These are the three things."
        ],
        [
            "An interior point.",
            "For first order conditions, which take this form, if we assume that the first 2 equations are satisfied, we can construct.",
            "Initial solutions to guarantee this and have feasible solutions.",
            "Then we will only have to deal with this equation.",
            "By the way, Capital X Capital S are diagonal matrices where vectors are sitting on the diagonal.",
            "So this is a diagonal matrix times the vector of ones is equal to me times the vector of ones, meaning every complementary product componentwise SJSJ should all be the same or close to each other realistically.",
            "Matrix spoke with vectors on the diagonal.",
            "Yes there is.",
            "There is a vector and we artificially put the vector on the diagonal of the matrix to be able to make diagonal by diagonal.",
            "Well, any mathematician would write that simply XJSJ is equal to me for all JS, but somehow interior Point Community has this obsession of writing it that way.",
            "And I follow this obsession.",
            "Sorry about that.",
            "No, this is the system of nonlinear equations.",
            "Only one of them is nonlinear.",
            "Really.",
            "We will apply Newton methods to solve it, and that's how interior point methods work."
        ],
        [
            "There is an interesting feature that this system of equations can be solved for any value of parameter me any positive value and it actually guides the solution from some far away in space to the optimal solution.",
            "The optimal solution, by the way, for quadratic problem does not have to be a vertex that maybe somewhere here it might even be in the middle of the set.",
            "Interior point methods work well if you can keep all the iterates XY and South somewhere in the neighborhood of the central path of the red curve.",
            "So the neighborhood is this green."
        ],
        [
            "Area here.",
            "We can differently define the neighborhood, but typically we just control what the error in this third nonlinear equation is.",
            "If we use Euclidean norm, this will be the definition of the neighborhood.",
            "If we use Infinity norm, then we would like to penalize products and keep them not too small relative with the barrier me not too large relative with the barrier mean.",
            "That's sort of geometric.",
            "Average.",
            "Kept small."
        ],
        [
            "There is a standard result which is very nice, which says that take an arbitrary accuracy you would like to achieve after not more than that, many iterations.",
            "Interior Point method will deliver it.",
            "It's a, it's a staggering result when you, when you think of complexity of the algorithm that you solve the problem of dimension N, then merely square root of N iterations.",
            "But to be perfectly honest, this is a worst case result and it is really worst case because in practice."
        ],
        [
            "It's much, much better.",
            "In practice you do just 1012 or 15 iterations, whatever the size of the problem and the job is done, so it's fantastic.",
            "It's fantastic, but.",
            "How much does the single iteration cost may go?",
            "Pretty expensive.",
            "In terms of dimensions.",
            "Now you will agree with me that it will be natural to say let's try to preserve this feature.",
            "Let's do something about this one."
        ],
        [
            "So let us try to accelerate interior point methods.",
            "Ideally we would like to reduce the cost of a single iteration to just something that depends linearly on the problem dimension.",
            "And this can be done if we do not keep obsessed about using the Newton method.",
            "If we relax a little bit requirements and use an inexact Newton method, then.",
            "Practical algorithm can be designed that would have better features."
        ],
        [
            "In terms of solving equations, this would mean that rather than computing Newton direction exactly as the solution of the system.",
            "We will solve it in exactly and we will admit certain error here."
        ],
        [
            "The theoretical question arises.",
            "Pretty obvious how flexible can one be in allowing errors in this direction.",
            "In other words, I'd like the error to be a fraction of the right hand side.",
            "How large this Delta might be, and ideally I'd like Delta to be as large as possible without affecting the complexity result."
        ],
        [
            "So here's the answer.",
            "It is possible to design the algorithm that uses inexact search directions and exact Newton method.",
            "That would admit quite large error Delta here.",
            "The algorithm would try to get that reduction of duality gap in each iteration.",
            "This will not be possible in one iteration.",
            "We reduce the duality gap by certain fraction noticeably smaller than one, and this is enough to.",
            "Do."
        ],
        [
            "Reduce that.",
            "The complexity is the same as before.",
            "There may be worse constant here, but in terms of worst case, complexity is the same result.",
            "So here's the summary.",
            "We can use an inexact Newton method.",
            "We can allow the error up to 30% in the computation of directions, and we retain the same complexity result.",
            "No.",
            "I deliberately didn't think of.",
            "Giving a proof here because this would be boring for most of you, but should you be interested in details, just go ahead.",
            "Find this paper on my webpage.",
            "Yes.",
            "Um?",
            "I'm not sure if I'm just being thick, but I didn't get an intuition for how long would implement in exactly the message from your.",
            "That's a very.",
            "That's a very different thing, which is in the second part of the talk, because we somehow will have to solve equations and guarantee that the error in this equation does not exceed 30% of the right hand side.",
            "That's a different story I'm getting into."
        ],
        [
            "Right, the proof just a sketch, focuses on controlling the error in Newton method.",
            "Newton Method is a second order method.",
            "We did the first differentiation to get first order optimality conditions and then the second differentiation to get the Newton direction.",
            "That's why we made the error and it's important to know what the error is if we manage to bind to bound this error as something proportional to the barrier will be able to make full Newtons inexact Newton step.",
            "And get the necessary get the required complexity of the algorithm."
        ],
        [
            "Next, one good news.",
            "From the first part of the talk we can replace Exact Newton method with an inexact Newton method.",
            "We can keep the residual under control the residual of the error.",
            "An inexact Newton method and the worst case complexity will not be affected.",
            "And this brings me.",
            "To the following thing."
        ],
        [
            "We have not made any assumptions regarding the source of and exactness.",
            "Well, the obvious things would be use approximate Hessian, use approximate Jacobian.",
            "So use something much cheaper than the true matrices there.",
            "Use some iterative method to compute Newton direction.",
            "But who knows, maybe I don't have competence is to do that, but you might have them.",
            "Maybe there is a way of finding these directions through some probabilistic method and get a good result."
        ],
        [
            "Now I'd like to show you 2 examples how this work in practice, and both of them have a little bit of probabilistic word.",
            "One is compressed sensing, another one is Google."
        ],
        [
            "OK, the sparse approximations.",
            "A joint work with my two collaborators came on us and Pavel.",
            "This appears everywhere and you are much more aware of that than me.",
            "Just just three examples.",
            "Statistics wavelet based signal image processing, compressed sensing.",
            "We want to solve this optimization problem and so many areas.",
            "And obviously the the email community likes this very much, I would say."
        ],
        [
            "Invasion statistics this is just estimating X from the observations.",
            "It's a work of tipsy, Ronnie."
        ],
        [
            "Maybe I'll skip this one.",
            "This is this is once again the thing from image.",
            "Construction where you well, you can.",
            "You can see the paper.",
            "It's actually a very influential work done by Chanda, Hahn, Saunders and well known in signal processing commune."
        ],
        [
            "Let's have a look at compressed sensing thing in compressed sensing.",
            "People argue that if you have the signal that is sparse, that has only a few that has a representation in certain basis, which would have only a few nonzero entries, then you could project this signal using a cleverly chosen basis, and then it's easy to retrieve the information from this projected signal easily and once again this this leads to the same.",
            "Optimization problem, that's the work of Candace Romberg and Terence Tao."
        ],
        [
            "Let's go back to optimization.",
            "In terms of optimization, we want to do this as a matter of fact, we frequently would like to deal with the true norm we are.",
            "We are after, which would be the zero norm.",
            "The zero norm means we would like to have the vector that has as few non zero entries as possible, but zero norm is a disaster because this leads to combinatorial optimization.",
            "Luckily the one norm provides a very reasonable.",
            "Substitute 20 N and we could tackle the problem in this way, but we could also vary it like for example paying attention to the second term, keeping the first under control or vice versa.",
            "Minimize the first term.",
            "Keep the second under control there.",
            "There's a variety of reformulations.",
            "I sometimes see methods which prefer particular reformulation to the other because it benefits the method simply."
        ],
        [
            "Problems in compressed sensing have alot of orthogonality and in my opinion this is one of the reasons why 1st order methods are able to solve them now.",
            "It's not just the orthogonality we are used to in numerical analysis, it's I call it a 2A orthogonality.",
            "We take the matrix which is a unitary.",
            "And also an orthonormal matrix, you slice it, you take only a subset of rows of it, so all rows are orthogonal to each other.",
            "And this is the first feature.",
            "But it's not all.",
            "Then you look at columns of this matrix A and the columns.",
            "If you choose a small subset of them, the columns are almost orthogonal to each other.",
            "So you have orthogonality.",
            "Perfect orthogonality this way, and you have almost orthogonality.",
            "This way, for a small subset of columns, this is the famous restricted isometry property, which means that the projections behave well, not the projections.",
            "The transformations on a subset of columns behave almost like a, like an isometry.",
            "They almost preserve the distance."
        ],
        [
            "Well, that's the illustration of this restricted isometry property.",
            "You have the matrix A.",
            "You pick up five columns from it.",
            "Here's the Matrix, a bar, and this matrix.",
            "A bar has such a feature that it's trans transposed times the matrix produces something that can very well be approximated by the scaled identity.",
            "No, what would you expect from a second order method that works on the Matrix, which is almost an identity?",
            "Well, it's probably page 2 or Page 3 in the book of nonlinear optimization.",
            "Newton method is almost the same as the steepest descent.",
            "You do not have to invert the matrix, which is an inverse, which is an identity.",
            "That's not a great discovery, and that's why you can use the gradients because they are rich in information that will never be zigzagging at typical disaster for ill conditioned problems we have this double orthogonality here and."
        ],
        [
            "And we can really take advantage of it.",
            "But before I will be able to.",
            "Tackle it with interior point methods.",
            "A little bit you have to.",
            "You have to respect this diagonal.",
            "It's on my next page in terms of finding the precondition.",
            "Yeah.",
            "The interior point introduces some very nasty diagonal scaling matrix which has to be respected, because if we don't respect that, we somehow go against the logic of interior point, I'll show it.",
            "The problem.",
            "To use interior point method here is related to the fact that norm one.",
            "Is a nondifferentiable function, so we cannot use it, but there is a way of replacing every element in this vector with the positive and negative part, because then we will replace the absolute value.",
            "Of every component with the sum of them and at the cost of doing this, expanding the space from dimension N, two dimension 2 N, we can replace norm one with such a vector and get rid of non differentiability.",
            "There's a little price to pay.",
            "We blow everything up by a factor of two.",
            "We now have to solve this quadratic optimization problem in dimension 2 by two, but on the other hand, there's a lot of structure.",
            "The matrices are replicated in it, so there will be no real increase of cost of computations because we will do only multiplications with matrix A."
        ],
        [
            "Here we come to do something that Matthias mentioned.",
            "Interior Point method always introduces very nasty matrices which misbehave which are a headache for numerical analysts because some interests here go to Infinity, others go to zero.",
            "It's a it's a condition number explodes to Infinity and that's why they have to be preserved in the preconditioner.",
            "But this very difficult matrix, potentially of large dimension, can be replaced by something extremely easy, which is the identity is replicated a few times scaled identities."
        ],
        [
            "It takes about 2 three pages of mathematics to analyze.",
            "Eigenvalues of the preconditioned matrix.",
            "Why do we do this?",
            "If we manage to show that eigenvalues of the preconditioned matrix are close to one, then they are well clustered and iterative method, Krylov subspace iterative method will provide us with the fast convergence, and that's indeed the case.",
            "We have a theorem saying that N eigenvalues are exactly equal to 1.",
            "The remaining you can values are close to 1.",
            "The distance the difference is bounded by restricted isometry constant.",
            "This constant is always something like 0.4 zero point 5, so noticeably smaller than one and the rest we can control.",
            "We can control by choosing this L. This is a user defined threshold to determine what is a large entry into the inverse.",
            "So it's a nice result that guarantees that all eigenvalues will, as we approach optimality, will get closer and closer to 1.",
            "Or within an interval around 1:00."
        ],
        [
            "It's written down in the paper.",
            "Now.",
            "That's how it works in practice on a problem that I could steal.",
            "Do these plots in Matlab, so this was the problem of dimension 4000 by 4000.",
            "You see here the number that equation system that has been solved.",
            "So for example equation #18 by the conjugate gradient was solved in about 1000 iterations.",
            "But if we use the preconditioner, this is the red curve.",
            "It was solved in about 20 iterations, which is good.",
            "While this happened, look here what happens at iteration.",
            "Sorry, not iteration for equation #18.",
            "The blue line indicates the spread of eigenvalues in the original matrix and it goes from something like 10 power minus one up to 10 power three or four.",
            "The preconditioned matrix is the red block is very all the eigenvalues are getting very close to one, so it's a good thing.",
            "We have good clustering of eigenvalues and therefore the conjugate gradient.",
            "Preconditioned conjugate gradient convergence quick."
        ],
        [
            "Now the comparison with the 1st order method.",
            "This is Nestor.",
            "Implemented by Becker Bobingen Countess.",
            "But we measure it in matrix vector products because the point is that gradient methods do only matrix vector product.",
            "This interior point method does nothing else but only matrix vector product.",
            "All the other things are hidden.",
            "They do not cost anything, and as you can see, when the problem dimension goes up to 1,000,000, this is a very comperable behavior.",
            "So it is actually possible to solve these problems with interior Point method.",
            "But you have to admit in exact directions.",
            "By the way, there is a anybody who wishes to use it.",
            "It's a Matlab implementation you can get."
        ],
        [
            "OK. How many of you Google at think once per day?",
            "Quite a few 100 times per day.",
            "Yeah, we all do it.",
            "We all benefit from the web page ranking perfectly well done by Google.",
            "What's the problem?",
            "The problem is that web pages that are visited visited frequently are more important and they will be ranked on top of the ranking.",
            "I abuse a little bit the the details offered, but that's that's the thing and I picked up this picture from Wikipedia.",
            "The web pages that are visited frequently have a big smile on it."
        ],
        [
            "Now the mathematics of it is the following.",
            "You have an adjacency matrix G of web page links.",
            "It can be made column stochastic if we if we normalize the rows and columns appropriately.",
            "Then the trouble that with the analysis of this matrix would be that there may be dead ends.",
            "The places which are not connected to anything else or a subset of the network that is not connected to anything else.",
            "To avoid such a situation, we will add something called the teleportation.",
            "This is not science fiction.",
            "This teleportation here has the meaning of going from any node in the network to any other node with certain probability, 1 minus Lambda, where the typical Lambda used in Google problem is zero 85, giving here zero 15.",
            "We are after the dominant eigenvector of this matrix M, So we'd like to solve this system of equations.",
            "You do realize that this system can be multiplied with any scalar giving us another solution, so we need to normalize somehow X, otherwise there will be a family of solutions.",
            "Let's say YTX is equal to 1.",
            "And once we find this dominant right eigenvector, this will define the ranking of web pages."
        ],
        [
            "So the optimizers could do a nasty thing about it, meaning rather than solving a system of linear equations, minimize the error in this equation.",
            "That's written here.",
            "We take M X -- X and we minimize the square of the Euclidean norm.",
            "In it it gives us a nice quadratic function.",
            "It's obviously positive definite by construction.",
            "The Matrix Q is positive definite.",
            "It's nice and context.",
            "And it's not very easy problem, although the size of it."
        ],
        [
            "Maybe large, it's easy, and in particular we can approximate it with this preconditioner.",
            "I mean, we can approximate the matrix in the Newton system with such a preconditioner in which the whole matrix Q is crudely replaced by its diagonal.",
            "The thing is that again thought will be taken into account.",
            "So what is good in interior Point method?",
            "What brings this information about the activity of variables in interior point will be taken into account, but we will only take diagonal of the matrix Q here.",
            "It's a it's a work in preparation.",
            "We have not finished."
        ],
        [
            "We run it on a number of problems that go from 4000 to 1,000,000.",
            "By the way, the problems have been taken directly from the paper of Nesterov about the smoothing gradient method, and we replicated his generation of the problem and run interior point on it, and I have to say I was very surprised with it because I tend to say interior point method converges in 10 iterations, sometimes 12, sometimes 15, but on these problems it converges in.",
            "6, five or three iterations and the number of matrix vector products drops to almost one 1 digit.",
            "This is way, way better than the smoothing gradient method on this specific problem, But then we tried how it works on more complicated problems and it's not so good, so I actually think that what was given in this paper that we followed were rather easy problems."
        ],
        [
            "Right time to conclude.",
            "I'd like to.",
            "So I'd like to say that there is an advantage.",
            "In relaxing certain feature of interior point methods not insist on using direct on using direct methods.",
            "Direct linear algebra methods and computing Newton directions exactly because this is not always feasible for large problems.",
            "There is no way of doing it, so one could replace.",
            "It with an inexact IPM using inexact using some approximate Newton directions.",
            "The good thing is that it still preserves the worst case complexity result, but on top of it adds significant improvement in computational speed.",
            "The 2nd order information can and I would say if you can cheaply estimate it always should be used because it brings an important piece of information about about the functions we are trying to optimize.",
            "So whenever you whenever you can, you should use it in my opinion.",
            "To summarize this inexact Newton directions and Ipms do not increase visibly the number of the number of iterations to reach optimality, but they lead to significant reduction of the cost of a single iteration, so they are advantageous from a computational point of view.",
            "No, I should once again make this pledge to you.",
            "There may be other methods that I'm unaware of.",
            "I did it from the perspective of numerical analysis.",
            "How do you replace exact Newton method with an inexact method?",
            "You replace direct Cholesky with some sort of iterative method, that's it, but maybe there are other ways of doing it based on some probabilistic estimates."
        ],
        [
            "That's all, thank you very much.",
            "So in the first part of the talk.",
            "Hearing that basically things workout, as long as you're in exact YouTube does not make an error more than zero point, yes.",
            "In the sense of that seems magic to me.",
            "Where is that number zero point?",
            "We can expect it sometime song Delta thing or something.",
            "Alright, this number.",
            "I need to backtrack quite a few slides, but let me do it.",
            "Sorry to keep your patience."
        ],
        [
            "Here this is the the.",
            "These are the 1st order optimality conditions for a specific value of the parameter mean.",
            "We will reduce this parameter because this is the force to go to optimality.",
            "So if the iteratees somewhere here in the next step, we would like not to simply go to the central path, but we would like to go to the point on the central path with a slightly smaller me.",
            "We will do Sigma times me and now think about it.",
            "Why should we solve this problem exactly in the first place?",
            "Why there is no point?",
            "It's only an approximation.",
            "It's only an intermediate step on our way to optimality, so it's not such a surprising thing when I say that rather than solving this exactly because anyway in the next iteration I will reduce the barrier next time I would solve it in exactly and and I think it's not.",
            "It's not a surprise that the level of an exactness can be quite large, as long as I managed to keep everything in this region.",
            "Where the method behaves, which is the region guaranteeing the good behavior of interior point.",
            "That's my explanation why why we can be so permissive about the size of the error that it can go to up to 30%.",
            "Reason why 30% and not 31%?",
            "Oh well, it's.",
            "For the constants which I picked in the proof, 0.3 was a reasonably elegant number.",
            "You probably could have 0.3123789 and this would work.",
            "Some someone would, you know go for other constants and maybe this would allow 40% error in the Newton direction.",
            "Don't don't get.",
            "Too much from this 30%.",
            "It's a.",
            "It's an example.",
            "It's more to say that we can achieve the same complexity result.",
            "Admitting quite quite a large error in Newton direction.",
            "There's another possible connection between what you talked about in probability.",
            "It's a sort of problem for people who like problems at the sort I'm going to say you talked about, Morgan writes result which had root in log one over epsilon, and then you said in practice it's much, much better well.",
            "Is that, is that true?",
            "But OK?",
            "Can one make math out of that?",
            "Your experience, your communities experiences and one could try to look at some average case collection of problems and trying to prove something like?",
            "Yes, the average sense, which would be a reasonable math problem to make out of your observation.",
            "And then you have to think about.",
            "What are reasonable problems and you know might lead you to think interesting thoughts about the type of problems that that is truthful?",
            "And because it's probably worst case is not true.",
            "That is longer.",
            "You are absolutely correct.",
            "It would be very interesting to try to capture in the analysis how the average case differs from the worst case.",
            "An attempt like this was was made by you, yet another guy from Stanford.",
            "Yes, a lot of you.",
            "Well, you have seen this talk on this chain.",
            "Donahoe Sanders.",
            "Yeah, it's Stanford obsession.",
            "The well in reality many good things were done to be serious, but the average case analysis interesting, Lee was unable to deliver a better result by you.",
            "Yeah, and he's I think he's explanation of it was that these central path, which I gave you here, looks nice, very easy to predict.",
            "You would say, oh, if we are here, we can jump to this point.",
            "That's not true.",
            "There are.",
            "Artificially created examples in which the central path gets very close to the boundary, then switches goes to another is pretty nasty, has a lot of turns if it has a lot of turns.",
            "It's also much more difficult to follow it, and it's much more difficult to capture it in the in the average case analysis, so it's a little bit disappointing to me because I'm a practitioner, I can see them converging so quickly and.",
            "I would like to bite it, but it's difficult questions.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you very much, John, Philip and Mike in prison in the UK for giving me this opportunity of conveying some of the ideas about interior point methods to you.",
                    "label": 1
                },
                {
                    "sent": "I have to confess that when guys approached me to come here, I was a little bit embarrassed because my research does not really touch probabilistic aspects so you wouldn't see much of the word probabilistic in it.",
                    "label": 1
                },
                {
                    "sent": "But I tried to make a connection to what you might be interested in, and in particular one of these connections is that maybe in exact directions, which I will show you they can work in interior Point method.",
                    "label": 0
                },
                {
                    "sent": "Maybe this inexact directions can somehow be constructed by probabilistic methods.",
                    "label": 0
                },
                {
                    "sent": "Let's have a look.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Edit.",
                    "label": 0
                },
                {
                    "sent": "Next there is certain enthusiasm around machine learning community for the 1st order methods, and obviously they deliver good quality solutions to many of your problems, but it's very easy to throw out interior Point methods saying oh they require 2nd order information.",
                    "label": 0
                },
                {
                    "sent": "This will be way way to difficult to estimate.",
                    "label": 0
                },
                {
                    "sent": "I'd like to convince you that it does not necessarily have to be so that I actually interior point methods can compete on many of.",
                    "label": 1
                },
                {
                    "sent": "Machine learning problems with first order methods.",
                    "label": 0
                },
                {
                    "sent": "The way to achieve it is to replace exact search directions within exact ones, compute them much, much faster than the standard interior point method would do.",
                    "label": 0
                },
                {
                    "sent": "But obviously this needs using an exact mutant direction, so some iterative methods will have to be involved and they do not work unless properly preconditioned.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "An 2 problems on which I'd like to show how it works are compressed sensing.",
                    "label": 1
                },
                {
                    "sent": "Or more generally, sparse approximations and Google problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "What is good about first order methods?",
                    "label": 0
                },
                {
                    "sent": "They are usually applied to solve these problems where the functions F&C are convex and they may have some enjoyable features like smoothness, separability.",
                    "label": 1
                },
                {
                    "sent": "Sometimes they are strongly convex, but what is?",
                    "label": 0
                },
                {
                    "sent": "At disadvantage of 1st order methods is that they fight.",
                    "label": 0
                },
                {
                    "sent": "They struggle if there were a complicated feasible set.",
                    "label": 0
                },
                {
                    "sent": "So unless the feasible set is something really easy or in a box a hyperplane, you cannot really successfully use first order methods.",
                    "label": 0
                },
                {
                    "sent": "And there is a broad class of optimization problems where you have to respect constraints or at least get close to satisfying them.",
                    "label": 0
                },
                {
                    "sent": "And then I think interior point methods come.",
                    "label": 0
                },
                {
                    "sent": "At the rescue.",
                    "label": 0
                },
                {
                    "sent": "They are typically used for this setting you have.",
                    "label": 0
                },
                {
                    "sent": "To minimize the function subject to inequality and equality constraints and the most well known classes of problems are linear and quadratic optimization.",
                    "label": 0
                },
                {
                    "sent": "Here we only.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have inequality's on X and linear constraints in quadratic optimization this would be quadratic nonlinear optimization.",
                    "label": 0
                },
                {
                    "sent": "Any general functions G&H 2nd order cone?",
                    "label": 0
                },
                {
                    "sent": "Now X belongs to a Kronecker product of counts or semidefinite optimization, where X is positive semidefinite matrix.",
                    "label": 0
                },
                {
                    "sent": "These are successful areas for for interior Point.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, here's the observation.",
                    "label": 0
                },
                {
                    "sent": "1st Order methods we can prove.",
                    "label": 0
                },
                {
                    "sent": "Well, not we.",
                    "label": 0
                },
                {
                    "sent": "You can prove complexity in oh of one over epsilon or one over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "And indeed these methods go for the first few iterations very quickly and get you a rough approximation of the solution.",
                    "label": 0
                },
                {
                    "sent": "But should you like to get to high accuracy that will take eternity on many real problems.",
                    "label": 0
                },
                {
                    "sent": "The thing is, machine learning community does not necessarily need this high accuracy frequently.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, Interpoint.",
                    "label": 0
                },
                {
                    "sent": "Use a second order information because use Newton has this fantastic complexity when you compare it with this, because this is the log of one over epsilon produces accurate solutions, never does many iterations, but one iteration may last forever.",
                    "label": 0
                },
                {
                    "sent": "So you either have many iterations to wait infinite time, or you have one iteration that takes one hour of heavy sweating of your.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Computer.",
                    "label": 0
                },
                {
                    "sent": "That's the choice.",
                    "label": 0
                },
                {
                    "sent": "And obviously there is a big difference between one over epsilon and log of 1 / 1 over epsilon.",
                    "label": 0
                },
                {
                    "sent": "This is a small number compared to to that, but that's an obvious thing.",
                    "label": 0
                },
                {
                    "sent": "Anyway, I do not think I will manage to change your appreciation for 1st order methods, but let me just start a little bit.",
                    "label": 0
                },
                {
                    "sent": "This Hornets nest and say give Ipms eye consideration.",
                    "label": 1
                },
                {
                    "sent": "Sometimes they might do.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good job for you.",
                    "label": 0
                },
                {
                    "sent": "So now a few slides.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About interior point methods.",
                    "label": 0
                },
                {
                    "sent": "I will focus on linear and quadratic optimization problems and my assumptions will be that a has full rank and Q is positive semidefinite, so it's a convex optimization problem.",
                    "label": 1
                },
                {
                    "sent": "Dimensions may be large and I would also like to make this assumption that matrices A&QI would never like to store them.",
                    "label": 0
                },
                {
                    "sent": "I would like to use them only as the operator because if the problem is large, storing these matrices would be out of question.",
                    "label": 0
                },
                {
                    "sent": "Doing factorizations this is just Daft idea, we would never go into this.",
                    "label": 0
                },
                {
                    "sent": "So let's assume we will only use these matrices to do matrix vector products.",
                    "label": 0
                },
                {
                    "sent": "Just like the 1st order methods.",
                    "label": 0
                },
                {
                    "sent": "And in many applications actually you can do these in the low complexity with low complexity algorithms should a be matrix that you can use fast Fourier transforms rather than storing a dense matrix and doing a heavy multiplication with it by fast Fourier transform you will get quickly the result.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, there are three things which are responsible for.",
                    "label": 0
                },
                {
                    "sent": "Interior point methods and they are accumulated on this slide.",
                    "label": 0
                },
                {
                    "sent": "The first is this beauty.",
                    "label": 0
                },
                {
                    "sent": "The logarithmic barrier function, which has the feature that rather than writing explicitly inequality X greater than zero, you will.",
                    "label": 0
                },
                {
                    "sent": "Minimize this function.",
                    "label": 0
                },
                {
                    "sent": "Minimizing this function throws things away from this axis, so it's a barrier on X approaching zero.",
                    "label": 0
                },
                {
                    "sent": "Very easy concept.",
                    "label": 0
                },
                {
                    "sent": "Then we need duality theory and this for quadratic optimization would lead to such a system of equations and the rest is solving the system of nonlinear equations and Newton method is obviously a natural candidate to deliver good technique to do so.",
                    "label": 1
                },
                {
                    "sent": "These are the three things.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An interior point.",
                    "label": 0
                },
                {
                    "sent": "For first order conditions, which take this form, if we assume that the first 2 equations are satisfied, we can construct.",
                    "label": 1
                },
                {
                    "sent": "Initial solutions to guarantee this and have feasible solutions.",
                    "label": 0
                },
                {
                    "sent": "Then we will only have to deal with this equation.",
                    "label": 0
                },
                {
                    "sent": "By the way, Capital X Capital S are diagonal matrices where vectors are sitting on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "So this is a diagonal matrix times the vector of ones is equal to me times the vector of ones, meaning every complementary product componentwise SJSJ should all be the same or close to each other realistically.",
                    "label": 0
                },
                {
                    "sent": "Matrix spoke with vectors on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "Yes there is.",
                    "label": 0
                },
                {
                    "sent": "There is a vector and we artificially put the vector on the diagonal of the matrix to be able to make diagonal by diagonal.",
                    "label": 0
                },
                {
                    "sent": "Well, any mathematician would write that simply XJSJ is equal to me for all JS, but somehow interior Point Community has this obsession of writing it that way.",
                    "label": 0
                },
                {
                    "sent": "And I follow this obsession.",
                    "label": 0
                },
                {
                    "sent": "Sorry about that.",
                    "label": 0
                },
                {
                    "sent": "No, this is the system of nonlinear equations.",
                    "label": 0
                },
                {
                    "sent": "Only one of them is nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 1
                },
                {
                    "sent": "We will apply Newton methods to solve it, and that's how interior point methods work.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is an interesting feature that this system of equations can be solved for any value of parameter me any positive value and it actually guides the solution from some far away in space to the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "The optimal solution, by the way, for quadratic problem does not have to be a vertex that maybe somewhere here it might even be in the middle of the set.",
                    "label": 0
                },
                {
                    "sent": "Interior point methods work well if you can keep all the iterates XY and South somewhere in the neighborhood of the central path of the red curve.",
                    "label": 1
                },
                {
                    "sent": "So the neighborhood is this green.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Area here.",
                    "label": 0
                },
                {
                    "sent": "We can differently define the neighborhood, but typically we just control what the error in this third nonlinear equation is.",
                    "label": 0
                },
                {
                    "sent": "If we use Euclidean norm, this will be the definition of the neighborhood.",
                    "label": 0
                },
                {
                    "sent": "If we use Infinity norm, then we would like to penalize products and keep them not too small relative with the barrier me not too large relative with the barrier mean.",
                    "label": 0
                },
                {
                    "sent": "That's sort of geometric.",
                    "label": 0
                },
                {
                    "sent": "Average.",
                    "label": 0
                },
                {
                    "sent": "Kept small.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is a standard result which is very nice, which says that take an arbitrary accuracy you would like to achieve after not more than that, many iterations.",
                    "label": 0
                },
                {
                    "sent": "Interior Point method will deliver it.",
                    "label": 1
                },
                {
                    "sent": "It's a, it's a staggering result when you, when you think of complexity of the algorithm that you solve the problem of dimension N, then merely square root of N iterations.",
                    "label": 0
                },
                {
                    "sent": "But to be perfectly honest, this is a worst case result and it is really worst case because in practice.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's much, much better.",
                    "label": 0
                },
                {
                    "sent": "In practice you do just 1012 or 15 iterations, whatever the size of the problem and the job is done, so it's fantastic.",
                    "label": 0
                },
                {
                    "sent": "It's fantastic, but.",
                    "label": 0
                },
                {
                    "sent": "How much does the single iteration cost may go?",
                    "label": 0
                },
                {
                    "sent": "Pretty expensive.",
                    "label": 0
                },
                {
                    "sent": "In terms of dimensions.",
                    "label": 0
                },
                {
                    "sent": "Now you will agree with me that it will be natural to say let's try to preserve this feature.",
                    "label": 0
                },
                {
                    "sent": "Let's do something about this one.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let us try to accelerate interior point methods.",
                    "label": 0
                },
                {
                    "sent": "Ideally we would like to reduce the cost of a single iteration to just something that depends linearly on the problem dimension.",
                    "label": 1
                },
                {
                    "sent": "And this can be done if we do not keep obsessed about using the Newton method.",
                    "label": 1
                },
                {
                    "sent": "If we relax a little bit requirements and use an inexact Newton method, then.",
                    "label": 0
                },
                {
                    "sent": "Practical algorithm can be designed that would have better features.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of solving equations, this would mean that rather than computing Newton direction exactly as the solution of the system.",
                    "label": 0
                },
                {
                    "sent": "We will solve it in exactly and we will admit certain error here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The theoretical question arises.",
                    "label": 0
                },
                {
                    "sent": "Pretty obvious how flexible can one be in allowing errors in this direction.",
                    "label": 0
                },
                {
                    "sent": "In other words, I'd like the error to be a fraction of the right hand side.",
                    "label": 0
                },
                {
                    "sent": "How large this Delta might be, and ideally I'd like Delta to be as large as possible without affecting the complexity result.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the answer.",
                    "label": 0
                },
                {
                    "sent": "It is possible to design the algorithm that uses inexact search directions and exact Newton method.",
                    "label": 0
                },
                {
                    "sent": "That would admit quite large error Delta here.",
                    "label": 0
                },
                {
                    "sent": "The algorithm would try to get that reduction of duality gap in each iteration.",
                    "label": 0
                },
                {
                    "sent": "This will not be possible in one iteration.",
                    "label": 0
                },
                {
                    "sent": "We reduce the duality gap by certain fraction noticeably smaller than one, and this is enough to.",
                    "label": 0
                },
                {
                    "sent": "Do.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reduce that.",
                    "label": 0
                },
                {
                    "sent": "The complexity is the same as before.",
                    "label": 0
                },
                {
                    "sent": "There may be worse constant here, but in terms of worst case, complexity is the same result.",
                    "label": 0
                },
                {
                    "sent": "So here's the summary.",
                    "label": 0
                },
                {
                    "sent": "We can use an inexact Newton method.",
                    "label": 1
                },
                {
                    "sent": "We can allow the error up to 30% in the computation of directions, and we retain the same complexity result.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "I deliberately didn't think of.",
                    "label": 0
                },
                {
                    "sent": "Giving a proof here because this would be boring for most of you, but should you be interested in details, just go ahead.",
                    "label": 0
                },
                {
                    "sent": "Find this paper on my webpage.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if I'm just being thick, but I didn't get an intuition for how long would implement in exactly the message from your.",
                    "label": 0
                },
                {
                    "sent": "That's a very.",
                    "label": 0
                },
                {
                    "sent": "That's a very different thing, which is in the second part of the talk, because we somehow will have to solve equations and guarantee that the error in this equation does not exceed 30% of the right hand side.",
                    "label": 0
                },
                {
                    "sent": "That's a different story I'm getting into.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, the proof just a sketch, focuses on controlling the error in Newton method.",
                    "label": 1
                },
                {
                    "sent": "Newton Method is a second order method.",
                    "label": 0
                },
                {
                    "sent": "We did the first differentiation to get first order optimality conditions and then the second differentiation to get the Newton direction.",
                    "label": 0
                },
                {
                    "sent": "That's why we made the error and it's important to know what the error is if we manage to bind to bound this error as something proportional to the barrier will be able to make full Newtons inexact Newton step.",
                    "label": 0
                },
                {
                    "sent": "And get the necessary get the required complexity of the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next, one good news.",
                    "label": 0
                },
                {
                    "sent": "From the first part of the talk we can replace Exact Newton method with an inexact Newton method.",
                    "label": 1
                },
                {
                    "sent": "We can keep the residual under control the residual of the error.",
                    "label": 0
                },
                {
                    "sent": "An inexact Newton method and the worst case complexity will not be affected.",
                    "label": 0
                },
                {
                    "sent": "And this brings me.",
                    "label": 0
                },
                {
                    "sent": "To the following thing.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have not made any assumptions regarding the source of and exactness.",
                    "label": 1
                },
                {
                    "sent": "Well, the obvious things would be use approximate Hessian, use approximate Jacobian.",
                    "label": 0
                },
                {
                    "sent": "So use something much cheaper than the true matrices there.",
                    "label": 1
                },
                {
                    "sent": "Use some iterative method to compute Newton direction.",
                    "label": 0
                },
                {
                    "sent": "But who knows, maybe I don't have competence is to do that, but you might have them.",
                    "label": 0
                },
                {
                    "sent": "Maybe there is a way of finding these directions through some probabilistic method and get a good result.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'd like to show you 2 examples how this work in practice, and both of them have a little bit of probabilistic word.",
                    "label": 0
                },
                {
                    "sent": "One is compressed sensing, another one is Google.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the sparse approximations.",
                    "label": 0
                },
                {
                    "sent": "A joint work with my two collaborators came on us and Pavel.",
                    "label": 1
                },
                {
                    "sent": "This appears everywhere and you are much more aware of that than me.",
                    "label": 0
                },
                {
                    "sent": "Just just three examples.",
                    "label": 1
                },
                {
                    "sent": "Statistics wavelet based signal image processing, compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "We want to solve this optimization problem and so many areas.",
                    "label": 1
                },
                {
                    "sent": "And obviously the the email community likes this very much, I would say.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Invasion statistics this is just estimating X from the observations.",
                    "label": 0
                },
                {
                    "sent": "It's a work of tipsy, Ronnie.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe I'll skip this one.",
                    "label": 0
                },
                {
                    "sent": "This is this is once again the thing from image.",
                    "label": 0
                },
                {
                    "sent": "Construction where you well, you can.",
                    "label": 0
                },
                {
                    "sent": "You can see the paper.",
                    "label": 0
                },
                {
                    "sent": "It's actually a very influential work done by Chanda, Hahn, Saunders and well known in signal processing commune.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's have a look at compressed sensing thing in compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "People argue that if you have the signal that is sparse, that has only a few that has a representation in certain basis, which would have only a few nonzero entries, then you could project this signal using a cleverly chosen basis, and then it's easy to retrieve the information from this projected signal easily and once again this this leads to the same.",
                    "label": 0
                },
                {
                    "sent": "Optimization problem, that's the work of Candace Romberg and Terence Tao.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's go back to optimization.",
                    "label": 0
                },
                {
                    "sent": "In terms of optimization, we want to do this as a matter of fact, we frequently would like to deal with the true norm we are.",
                    "label": 0
                },
                {
                    "sent": "We are after, which would be the zero norm.",
                    "label": 0
                },
                {
                    "sent": "The zero norm means we would like to have the vector that has as few non zero entries as possible, but zero norm is a disaster because this leads to combinatorial optimization.",
                    "label": 0
                },
                {
                    "sent": "Luckily the one norm provides a very reasonable.",
                    "label": 0
                },
                {
                    "sent": "Substitute 20 N and we could tackle the problem in this way, but we could also vary it like for example paying attention to the second term, keeping the first under control or vice versa.",
                    "label": 0
                },
                {
                    "sent": "Minimize the first term.",
                    "label": 0
                },
                {
                    "sent": "Keep the second under control there.",
                    "label": 0
                },
                {
                    "sent": "There's a variety of reformulations.",
                    "label": 0
                },
                {
                    "sent": "I sometimes see methods which prefer particular reformulation to the other because it benefits the method simply.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problems in compressed sensing have alot of orthogonality and in my opinion this is one of the reasons why 1st order methods are able to solve them now.",
                    "label": 0
                },
                {
                    "sent": "It's not just the orthogonality we are used to in numerical analysis, it's I call it a 2A orthogonality.",
                    "label": 0
                },
                {
                    "sent": "We take the matrix which is a unitary.",
                    "label": 0
                },
                {
                    "sent": "And also an orthonormal matrix, you slice it, you take only a subset of rows of it, so all rows are orthogonal to each other.",
                    "label": 1
                },
                {
                    "sent": "And this is the first feature.",
                    "label": 0
                },
                {
                    "sent": "But it's not all.",
                    "label": 0
                },
                {
                    "sent": "Then you look at columns of this matrix A and the columns.",
                    "label": 1
                },
                {
                    "sent": "If you choose a small subset of them, the columns are almost orthogonal to each other.",
                    "label": 0
                },
                {
                    "sent": "So you have orthogonality.",
                    "label": 0
                },
                {
                    "sent": "Perfect orthogonality this way, and you have almost orthogonality.",
                    "label": 1
                },
                {
                    "sent": "This way, for a small subset of columns, this is the famous restricted isometry property, which means that the projections behave well, not the projections.",
                    "label": 0
                },
                {
                    "sent": "The transformations on a subset of columns behave almost like a, like an isometry.",
                    "label": 0
                },
                {
                    "sent": "They almost preserve the distance.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, that's the illustration of this restricted isometry property.",
                    "label": 1
                },
                {
                    "sent": "You have the matrix A.",
                    "label": 0
                },
                {
                    "sent": "You pick up five columns from it.",
                    "label": 0
                },
                {
                    "sent": "Here's the Matrix, a bar, and this matrix.",
                    "label": 0
                },
                {
                    "sent": "A bar has such a feature that it's trans transposed times the matrix produces something that can very well be approximated by the scaled identity.",
                    "label": 0
                },
                {
                    "sent": "No, what would you expect from a second order method that works on the Matrix, which is almost an identity?",
                    "label": 0
                },
                {
                    "sent": "Well, it's probably page 2 or Page 3 in the book of nonlinear optimization.",
                    "label": 0
                },
                {
                    "sent": "Newton method is almost the same as the steepest descent.",
                    "label": 0
                },
                {
                    "sent": "You do not have to invert the matrix, which is an inverse, which is an identity.",
                    "label": 0
                },
                {
                    "sent": "That's not a great discovery, and that's why you can use the gradients because they are rich in information that will never be zigzagging at typical disaster for ill conditioned problems we have this double orthogonality here and.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can really take advantage of it.",
                    "label": 0
                },
                {
                    "sent": "But before I will be able to.",
                    "label": 1
                },
                {
                    "sent": "Tackle it with interior point methods.",
                    "label": 0
                },
                {
                    "sent": "A little bit you have to.",
                    "label": 0
                },
                {
                    "sent": "You have to respect this diagonal.",
                    "label": 0
                },
                {
                    "sent": "It's on my next page in terms of finding the precondition.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The interior point introduces some very nasty diagonal scaling matrix which has to be respected, because if we don't respect that, we somehow go against the logic of interior point, I'll show it.",
                    "label": 0
                },
                {
                    "sent": "The problem.",
                    "label": 1
                },
                {
                    "sent": "To use interior point method here is related to the fact that norm one.",
                    "label": 0
                },
                {
                    "sent": "Is a nondifferentiable function, so we cannot use it, but there is a way of replacing every element in this vector with the positive and negative part, because then we will replace the absolute value.",
                    "label": 0
                },
                {
                    "sent": "Of every component with the sum of them and at the cost of doing this, expanding the space from dimension N, two dimension 2 N, we can replace norm one with such a vector and get rid of non differentiability.",
                    "label": 0
                },
                {
                    "sent": "There's a little price to pay.",
                    "label": 0
                },
                {
                    "sent": "We blow everything up by a factor of two.",
                    "label": 0
                },
                {
                    "sent": "We now have to solve this quadratic optimization problem in dimension 2 by two, but on the other hand, there's a lot of structure.",
                    "label": 0
                },
                {
                    "sent": "The matrices are replicated in it, so there will be no real increase of cost of computations because we will do only multiplications with matrix A.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we come to do something that Matthias mentioned.",
                    "label": 0
                },
                {
                    "sent": "Interior Point method always introduces very nasty matrices which misbehave which are a headache for numerical analysts because some interests here go to Infinity, others go to zero.",
                    "label": 0
                },
                {
                    "sent": "It's a it's a condition number explodes to Infinity and that's why they have to be preserved in the preconditioner.",
                    "label": 0
                },
                {
                    "sent": "But this very difficult matrix, potentially of large dimension, can be replaced by something extremely easy, which is the identity is replicated a few times scaled identities.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It takes about 2 three pages of mathematics to analyze.",
                    "label": 0
                },
                {
                    "sent": "Eigenvalues of the preconditioned matrix.",
                    "label": 0
                },
                {
                    "sent": "Why do we do this?",
                    "label": 0
                },
                {
                    "sent": "If we manage to show that eigenvalues of the preconditioned matrix are close to one, then they are well clustered and iterative method, Krylov subspace iterative method will provide us with the fast convergence, and that's indeed the case.",
                    "label": 0
                },
                {
                    "sent": "We have a theorem saying that N eigenvalues are exactly equal to 1.",
                    "label": 0
                },
                {
                    "sent": "The remaining you can values are close to 1.",
                    "label": 0
                },
                {
                    "sent": "The distance the difference is bounded by restricted isometry constant.",
                    "label": 0
                },
                {
                    "sent": "This constant is always something like 0.4 zero point 5, so noticeably smaller than one and the rest we can control.",
                    "label": 0
                },
                {
                    "sent": "We can control by choosing this L. This is a user defined threshold to determine what is a large entry into the inverse.",
                    "label": 0
                },
                {
                    "sent": "So it's a nice result that guarantees that all eigenvalues will, as we approach optimality, will get closer and closer to 1.",
                    "label": 0
                },
                {
                    "sent": "Or within an interval around 1:00.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's written down in the paper.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "That's how it works in practice on a problem that I could steal.",
                    "label": 0
                },
                {
                    "sent": "Do these plots in Matlab, so this was the problem of dimension 4000 by 4000.",
                    "label": 0
                },
                {
                    "sent": "You see here the number that equation system that has been solved.",
                    "label": 0
                },
                {
                    "sent": "So for example equation #18 by the conjugate gradient was solved in about 1000 iterations.",
                    "label": 0
                },
                {
                    "sent": "But if we use the preconditioner, this is the red curve.",
                    "label": 0
                },
                {
                    "sent": "It was solved in about 20 iterations, which is good.",
                    "label": 0
                },
                {
                    "sent": "While this happened, look here what happens at iteration.",
                    "label": 0
                },
                {
                    "sent": "Sorry, not iteration for equation #18.",
                    "label": 0
                },
                {
                    "sent": "The blue line indicates the spread of eigenvalues in the original matrix and it goes from something like 10 power minus one up to 10 power three or four.",
                    "label": 1
                },
                {
                    "sent": "The preconditioned matrix is the red block is very all the eigenvalues are getting very close to one, so it's a good thing.",
                    "label": 0
                },
                {
                    "sent": "We have good clustering of eigenvalues and therefore the conjugate gradient.",
                    "label": 1
                },
                {
                    "sent": "Preconditioned conjugate gradient convergence quick.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the comparison with the 1st order method.",
                    "label": 0
                },
                {
                    "sent": "This is Nestor.",
                    "label": 0
                },
                {
                    "sent": "Implemented by Becker Bobingen Countess.",
                    "label": 0
                },
                {
                    "sent": "But we measure it in matrix vector products because the point is that gradient methods do only matrix vector product.",
                    "label": 0
                },
                {
                    "sent": "This interior point method does nothing else but only matrix vector product.",
                    "label": 0
                },
                {
                    "sent": "All the other things are hidden.",
                    "label": 0
                },
                {
                    "sent": "They do not cost anything, and as you can see, when the problem dimension goes up to 1,000,000, this is a very comperable behavior.",
                    "label": 0
                },
                {
                    "sent": "So it is actually possible to solve these problems with interior Point method.",
                    "label": 0
                },
                {
                    "sent": "But you have to admit in exact directions.",
                    "label": 0
                },
                {
                    "sent": "By the way, there is a anybody who wishes to use it.",
                    "label": 0
                },
                {
                    "sent": "It's a Matlab implementation you can get.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. How many of you Google at think once per day?",
                    "label": 0
                },
                {
                    "sent": "Quite a few 100 times per day.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we all do it.",
                    "label": 0
                },
                {
                    "sent": "We all benefit from the web page ranking perfectly well done by Google.",
                    "label": 0
                },
                {
                    "sent": "What's the problem?",
                    "label": 0
                },
                {
                    "sent": "The problem is that web pages that are visited visited frequently are more important and they will be ranked on top of the ranking.",
                    "label": 0
                },
                {
                    "sent": "I abuse a little bit the the details offered, but that's that's the thing and I picked up this picture from Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "The web pages that are visited frequently have a big smile on it.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the mathematics of it is the following.",
                    "label": 0
                },
                {
                    "sent": "You have an adjacency matrix G of web page links.",
                    "label": 1
                },
                {
                    "sent": "It can be made column stochastic if we if we normalize the rows and columns appropriately.",
                    "label": 0
                },
                {
                    "sent": "Then the trouble that with the analysis of this matrix would be that there may be dead ends.",
                    "label": 0
                },
                {
                    "sent": "The places which are not connected to anything else or a subset of the network that is not connected to anything else.",
                    "label": 0
                },
                {
                    "sent": "To avoid such a situation, we will add something called the teleportation.",
                    "label": 0
                },
                {
                    "sent": "This is not science fiction.",
                    "label": 0
                },
                {
                    "sent": "This teleportation here has the meaning of going from any node in the network to any other node with certain probability, 1 minus Lambda, where the typical Lambda used in Google problem is zero 85, giving here zero 15.",
                    "label": 0
                },
                {
                    "sent": "We are after the dominant eigenvector of this matrix M, So we'd like to solve this system of equations.",
                    "label": 0
                },
                {
                    "sent": "You do realize that this system can be multiplied with any scalar giving us another solution, so we need to normalize somehow X, otherwise there will be a family of solutions.",
                    "label": 1
                },
                {
                    "sent": "Let's say YTX is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "And once we find this dominant right eigenvector, this will define the ranking of web pages.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the optimizers could do a nasty thing about it, meaning rather than solving a system of linear equations, minimize the error in this equation.",
                    "label": 0
                },
                {
                    "sent": "That's written here.",
                    "label": 0
                },
                {
                    "sent": "We take M X -- X and we minimize the square of the Euclidean norm.",
                    "label": 0
                },
                {
                    "sent": "In it it gives us a nice quadratic function.",
                    "label": 0
                },
                {
                    "sent": "It's obviously positive definite by construction.",
                    "label": 0
                },
                {
                    "sent": "The Matrix Q is positive definite.",
                    "label": 0
                },
                {
                    "sent": "It's nice and context.",
                    "label": 0
                },
                {
                    "sent": "And it's not very easy problem, although the size of it.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe large, it's easy, and in particular we can approximate it with this preconditioner.",
                    "label": 0
                },
                {
                    "sent": "I mean, we can approximate the matrix in the Newton system with such a preconditioner in which the whole matrix Q is crudely replaced by its diagonal.",
                    "label": 0
                },
                {
                    "sent": "The thing is that again thought will be taken into account.",
                    "label": 0
                },
                {
                    "sent": "So what is good in interior Point method?",
                    "label": 0
                },
                {
                    "sent": "What brings this information about the activity of variables in interior point will be taken into account, but we will only take diagonal of the matrix Q here.",
                    "label": 0
                },
                {
                    "sent": "It's a it's a work in preparation.",
                    "label": 0
                },
                {
                    "sent": "We have not finished.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We run it on a number of problems that go from 4000 to 1,000,000.",
                    "label": 0
                },
                {
                    "sent": "By the way, the problems have been taken directly from the paper of Nesterov about the smoothing gradient method, and we replicated his generation of the problem and run interior point on it, and I have to say I was very surprised with it because I tend to say interior point method converges in 10 iterations, sometimes 12, sometimes 15, but on these problems it converges in.",
                    "label": 0
                },
                {
                    "sent": "6, five or three iterations and the number of matrix vector products drops to almost one 1 digit.",
                    "label": 0
                },
                {
                    "sent": "This is way, way better than the smoothing gradient method on this specific problem, But then we tried how it works on more complicated problems and it's not so good, so I actually think that what was given in this paper that we followed were rather easy problems.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right time to conclude.",
                    "label": 0
                },
                {
                    "sent": "I'd like to.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to say that there is an advantage.",
                    "label": 0
                },
                {
                    "sent": "In relaxing certain feature of interior point methods not insist on using direct on using direct methods.",
                    "label": 0
                },
                {
                    "sent": "Direct linear algebra methods and computing Newton directions exactly because this is not always feasible for large problems.",
                    "label": 0
                },
                {
                    "sent": "There is no way of doing it, so one could replace.",
                    "label": 0
                },
                {
                    "sent": "It with an inexact IPM using inexact using some approximate Newton directions.",
                    "label": 1
                },
                {
                    "sent": "The good thing is that it still preserves the worst case complexity result, but on top of it adds significant improvement in computational speed.",
                    "label": 0
                },
                {
                    "sent": "The 2nd order information can and I would say if you can cheaply estimate it always should be used because it brings an important piece of information about about the functions we are trying to optimize.",
                    "label": 1
                },
                {
                    "sent": "So whenever you whenever you can, you should use it in my opinion.",
                    "label": 0
                },
                {
                    "sent": "To summarize this inexact Newton directions and Ipms do not increase visibly the number of the number of iterations to reach optimality, but they lead to significant reduction of the cost of a single iteration, so they are advantageous from a computational point of view.",
                    "label": 0
                },
                {
                    "sent": "No, I should once again make this pledge to you.",
                    "label": 0
                },
                {
                    "sent": "There may be other methods that I'm unaware of.",
                    "label": 0
                },
                {
                    "sent": "I did it from the perspective of numerical analysis.",
                    "label": 0
                },
                {
                    "sent": "How do you replace exact Newton method with an inexact method?",
                    "label": 0
                },
                {
                    "sent": "You replace direct Cholesky with some sort of iterative method, that's it, but maybe there are other ways of doing it based on some probabilistic estimates.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's all, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So in the first part of the talk.",
                    "label": 0
                },
                {
                    "sent": "Hearing that basically things workout, as long as you're in exact YouTube does not make an error more than zero point, yes.",
                    "label": 0
                },
                {
                    "sent": "In the sense of that seems magic to me.",
                    "label": 0
                },
                {
                    "sent": "Where is that number zero point?",
                    "label": 0
                },
                {
                    "sent": "We can expect it sometime song Delta thing or something.",
                    "label": 0
                },
                {
                    "sent": "Alright, this number.",
                    "label": 0
                },
                {
                    "sent": "I need to backtrack quite a few slides, but let me do it.",
                    "label": 0
                },
                {
                    "sent": "Sorry to keep your patience.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here this is the the.",
                    "label": 0
                },
                {
                    "sent": "These are the 1st order optimality conditions for a specific value of the parameter mean.",
                    "label": 0
                },
                {
                    "sent": "We will reduce this parameter because this is the force to go to optimality.",
                    "label": 0
                },
                {
                    "sent": "So if the iteratees somewhere here in the next step, we would like not to simply go to the central path, but we would like to go to the point on the central path with a slightly smaller me.",
                    "label": 1
                },
                {
                    "sent": "We will do Sigma times me and now think about it.",
                    "label": 0
                },
                {
                    "sent": "Why should we solve this problem exactly in the first place?",
                    "label": 0
                },
                {
                    "sent": "Why there is no point?",
                    "label": 0
                },
                {
                    "sent": "It's only an approximation.",
                    "label": 0
                },
                {
                    "sent": "It's only an intermediate step on our way to optimality, so it's not such a surprising thing when I say that rather than solving this exactly because anyway in the next iteration I will reduce the barrier next time I would solve it in exactly and and I think it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not a surprise that the level of an exactness can be quite large, as long as I managed to keep everything in this region.",
                    "label": 0
                },
                {
                    "sent": "Where the method behaves, which is the region guaranteeing the good behavior of interior point.",
                    "label": 0
                },
                {
                    "sent": "That's my explanation why why we can be so permissive about the size of the error that it can go to up to 30%.",
                    "label": 0
                },
                {
                    "sent": "Reason why 30% and not 31%?",
                    "label": 0
                },
                {
                    "sent": "Oh well, it's.",
                    "label": 0
                },
                {
                    "sent": "For the constants which I picked in the proof, 0.3 was a reasonably elegant number.",
                    "label": 0
                },
                {
                    "sent": "You probably could have 0.3123789 and this would work.",
                    "label": 0
                },
                {
                    "sent": "Some someone would, you know go for other constants and maybe this would allow 40% error in the Newton direction.",
                    "label": 0
                },
                {
                    "sent": "Don't don't get.",
                    "label": 0
                },
                {
                    "sent": "Too much from this 30%.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's an example.",
                    "label": 0
                },
                {
                    "sent": "It's more to say that we can achieve the same complexity result.",
                    "label": 0
                },
                {
                    "sent": "Admitting quite quite a large error in Newton direction.",
                    "label": 0
                },
                {
                    "sent": "There's another possible connection between what you talked about in probability.",
                    "label": 0
                },
                {
                    "sent": "It's a sort of problem for people who like problems at the sort I'm going to say you talked about, Morgan writes result which had root in log one over epsilon, and then you said in practice it's much, much better well.",
                    "label": 0
                },
                {
                    "sent": "Is that, is that true?",
                    "label": 0
                },
                {
                    "sent": "But OK?",
                    "label": 0
                },
                {
                    "sent": "Can one make math out of that?",
                    "label": 0
                },
                {
                    "sent": "Your experience, your communities experiences and one could try to look at some average case collection of problems and trying to prove something like?",
                    "label": 0
                },
                {
                    "sent": "Yes, the average sense, which would be a reasonable math problem to make out of your observation.",
                    "label": 0
                },
                {
                    "sent": "And then you have to think about.",
                    "label": 0
                },
                {
                    "sent": "What are reasonable problems and you know might lead you to think interesting thoughts about the type of problems that that is truthful?",
                    "label": 0
                },
                {
                    "sent": "And because it's probably worst case is not true.",
                    "label": 0
                },
                {
                    "sent": "That is longer.",
                    "label": 0
                },
                {
                    "sent": "You are absolutely correct.",
                    "label": 0
                },
                {
                    "sent": "It would be very interesting to try to capture in the analysis how the average case differs from the worst case.",
                    "label": 0
                },
                {
                    "sent": "An attempt like this was was made by you, yet another guy from Stanford.",
                    "label": 0
                },
                {
                    "sent": "Yes, a lot of you.",
                    "label": 0
                },
                {
                    "sent": "Well, you have seen this talk on this chain.",
                    "label": 0
                },
                {
                    "sent": "Donahoe Sanders.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's Stanford obsession.",
                    "label": 0
                },
                {
                    "sent": "The well in reality many good things were done to be serious, but the average case analysis interesting, Lee was unable to deliver a better result by you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and he's I think he's explanation of it was that these central path, which I gave you here, looks nice, very easy to predict.",
                    "label": 0
                },
                {
                    "sent": "You would say, oh, if we are here, we can jump to this point.",
                    "label": 0
                },
                {
                    "sent": "That's not true.",
                    "label": 0
                },
                {
                    "sent": "There are.",
                    "label": 0
                },
                {
                    "sent": "Artificially created examples in which the central path gets very close to the boundary, then switches goes to another is pretty nasty, has a lot of turns if it has a lot of turns.",
                    "label": 0
                },
                {
                    "sent": "It's also much more difficult to follow it, and it's much more difficult to capture it in the in the average case analysis, so it's a little bit disappointing to me because I'm a practitioner, I can see them converging so quickly and.",
                    "label": 0
                },
                {
                    "sent": "I would like to bite it, but it's difficult questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}