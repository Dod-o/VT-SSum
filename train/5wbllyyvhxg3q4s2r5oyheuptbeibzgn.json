{
    "id": "5wbllyyvhxg3q4s2r5oyheuptbeibzgn",
    "title": "Designing an Electronic Reverse Dictionary Based on Two Word Association Norms of English Language",
    "info": {
        "author": [
            "Jorge Reyes-Magana, Universidad Autonoma de Yucatan"
        ],
        "published": "Dec. 2, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Humanities->Languages",
            "Top->Humanities->Linguistics->Lexicography"
        ]
    },
    "url": "http://videolectures.net/elexisconference2019_reyes_magana_language/",
    "segmentation": [
        [
            "Hi everyone, my name is Jorge Reyes magania.",
            "I'm coming from the National University of Thomas of Mexico, so let's get started."
        ],
        [
            "Here we have all the content that we're going to be talking about."
        ],
        [
            "First of all, we have two types of dictionary.",
            "The first one is the most common one.",
            "That is there some astrological that provides meanings.",
            "When you have a word, the user obtain the meaning of such word and the other one is a onomatopoeia logical.",
            "When that works in the opposite way, given the description of a word, the user obtained a related concept.",
            "So I try to do an electronic onomatopoeia, logical dictionary.",
            "I kind of data reverse dictionary that are based on some word Association norms that I'm going to be talking about this kind of resources."
        ],
        [
            "So nowadays we can think that the most logical search is made through different electronic ways.",
            "For example Google, Yahoo or another searchers.",
            "When you can put your definition of what you're trying to looking at an all the word that you're trying to to look is going to appear here.",
            "But in sometimes this word is not good enough on it gives more information that we are looking at, so there is no good sometimes."
        ],
        [
            "So in this layer I'm going to.",
            "Percent the main idea of my work.",
            "First of all, I'm going to have some definition made by humans.",
            "For example, a person is say what is a small road and living in trees.",
            "With a long, bushy tail, so with this definition and this description, I only kept some specific words in a lame away.",
            "For example, I have.",
            "Only in prod and leave tail bushy three small and with these words I have.",
            "Applied an algorithm into a graph.",
            "This graph it's a mathematical object that has some properties, so you can run algorithms into this graph and using the words that people say to me I have to in fear.",
            "That what the person is talking about is a squirrel, for example.",
            "So this is the basic idea of my work so."
        ],
        [
            "First of all, I'm going to talk about the word Association norms.",
            "These are some psychological resources cycle linguistical resources.",
            "When an this work in first you they give you a word.",
            "For example B and you have to say the first word that comes to your mind when you hear being on.",
            "When somebody says be an in the first column, we have different words that represent the response that people give.",
            "For example, hive honey Steam bus was batura of all of these words were made by people when they hear being when they hear be so, and the first one they they would be is a stimulus word and the other ones are called the response words in the second column.",
            "Sorry in the second column we have a mathematical values that represent the frequency and in other way it represents the associative strength.",
            "That is only a percentage that is calculated from the frequency.",
            "So with this into values mathematical values I construct."
        ],
        [
            "A graph.",
            "The word Association norms are the notes.",
            "The words are the notes of my graph and for example in this layer we have.",
            "The word being in the center that represent the stimulus word, and all the responses are around.",
            "It seems to be some edges that connect each one of the words that are present in the notes.",
            "And you can put the value into that edge.",
            "That is called a weighted graph, OK?",
            "And there is another example here that is of whether whether it's in the center and all the other words were the responses that people give to water.",
            "I only present two of these stimulus word with this response is because all of the graph that I used is too big to present in one single layer.",
            "So this is the main idea of the original resource that I used to run my algorithms."
        ],
        [
            "So I used the word Association norms that are available on Internet.",
            "The first one is the edenburg associated, the sorrows that it has 8211 stimulus words with 20,445 different words, including the stimuli and responses.",
            "The other one is the collection of the University of South Florida, that it has more than 6000 participants than produced nearly 3/4 of a million responses to 5018 stimulus word.",
            "I use in a separate wave.",
            "It means that I have one graph for AT and another graph for Florida.",
            "Collection"
        ],
        [
            "So in the general idea of the graph, is the graph representing the word Association norms have been elaborated with lemmas?",
            "I use only lemmas of each one of the words is undirected so that every stimulus is connected to every associated word without any presidents order an for the weight of the ages.",
            "I have two functions, the frequency and the Association strength.",
            "In this algorithm that I use, works on shortest paths, so I have to make an inversion of that frequency and an inversion of the associative strength."
        ],
        [
            "First of all, when I finish my graph I it's time to produce or to Rhonda between centrally algorithm that it makes the search.",
            "So this algorithm is a math thing and it was developed in 1977.",
            "And the main idea of this algorithm is that a traditional between this algorithm assumes an important nodes connect to other nodes."
        ],
        [
            "So this is the formula that calculates this between centrality from one.",
            "Note, it's a mathematical stuff, and it says that for a given node V in a graph, GD between centrality is calculated as a relation between the number of shortest paths between nodes.",
            "A&J that pass through Nelvie and the number of service parts between nodes I enjoy so.",
            "In this next layer."
        ],
        [
            "I give you a little explanation of how this is calculated.",
            "This is a little very small graph that only contains for notes and we are trying to calculate.",
            "In this case there between the centrality of node V. So let's see we have to look of every path in the graph.",
            "For example, if we look.",
            "The path that are from A to B.",
            "It's only one path.",
            "And how many of those parts pass through be only one?",
            "Because he's in the other side, so.",
            "When we calculated the value is wind divided into 1 divided into 1.",
            "Is everything is almost the same, but in the next in the last element that we have into right, we see that we have a 0 because we're calculating there the path between C&D, we have only one path between Sandy, but no, no one.",
            "None of those paths pass through being.",
            "Remember that we're calculating day between centrality of node B. OK, some mad that I have to do a little brief explanation.",
            "Because I use these things to run my algorithm instead of having notes like ABC or DI have forts, remember that."
        ],
        [
            "A little variation of this algorithm between centrality was made, and instead of having the shortest path in all the graph, they use only one subset that works like the origin and one other subset that works like in the destination of the shortest path or the target and between centrality are the notes that are in the middle of those subsets, so.",
            "For example, this is a general idea of how the algorithm works, but.",
            "My subset is going to be the the words that I present in their first layers rather than bushy animals, cetera an these same subset is going to be the target one, so it is supposed, or it is expected that between centrality nodes.",
            "In the middle of those is the word squirrel.",
            "That's how my work."
        ],
        [
            "Is done another algorithm that I tried is called page rank, that is the one used originally by Google that was used to rank the the pages that we have an Internet.",
            "Which one is the most important and taking into account all the incoming links that has into graph.",
            "So for example we have this graph and node B has a highest value because it has a lot of.",
            "Incoming links.",
            "Instead of having."
        ],
        [
            "Web pages they have words.",
            "The purchases remain here.",
            "Is that the target word tested with a definition to be searched correspond to the higher scores returned by the patron algorithm."
        ],
        [
            "This is some of briefly several codes that represent the algorithm complete.",
            "We have first above to pre process all the one data sets, having only the lemmas we have to preprocess the definition to search.",
            "In the same way we kept only the lemmas, we remove the stop words.",
            "Then we build the graph.",
            "With the word Association norms.",
            "Second, I have to print graph because in a very big graph all the paths were going wrong and we are not getting the correct words.",
            "So I have to cut some notes into the note that give me a more reduced graph and this graph work in a better way.",
            "And for each definition.",
            "I only kept the words that were available in the World Association arms.",
            "I build a sub graph that was used like source and target and I run the algorithm the between centrality and Pagerank that you can see in the last year part of this server code that is only the end the call to the methods, the mathematical methods.",
            "So."
        ],
        [
            "Well, I have a evaluation corpus.",
            "Consisting of seven concepts.",
            "Then the finisher were proved of each one of those concepts they were given by human native speakers.",
            "In most cases that officially the definition are very different.",
            "From the ones found in dictionaries, they lack specialized terms and include cultural references and connotation, and the words that I work that were tested were whether Squirrel bench Hurricane, lemon bucket, and clothes."
        ],
        [
            "Here is a brief example of how the definition what were the definition that were tested in my program.",
            "If these are the definition of squirrel.",
            "And to give you an idea, how were the definition that were used?"
        ],
        [
            "For my evaluation also I use a metric that is called precision at K, that precision at one stands that the concept associated to a definition was Frank in the 1st place position at rest at three.",
            "Sorry, the concept was in the first 3 results, precision and the same applies to precision at 5:00 or position at 10 that whatever was looking for was into that range."
        ],
        [
            "My results are the following.",
            "Remember that I didn't combine the two word Association arms.",
            "I worked separately.",
            "I have the 18 results in the upper.",
            "But I have to.",
            "The weight of the graph.",
            "The first one is a emberas frequency and the inverse Association strength applied to the two algorithms.",
            "Remember that I work with between the centrality and the other one was page rank.",
            "In the first case, the higher value.",
            "The highest value was obtained using between centrality and the weight of inverse Association strength.",
            "Having those value in bold letter.",
            "In the next result we have.",
            "That is also consistent.",
            "I mean the highest value was using the between centrality algorithm and using also the inverse inverse Association strength.",
            "As you can see, the page rank algorithm, it's not good for this kind of task.",
            "The the best that I've tried is between a central algorithm, OK?"
        ],
        [
            "After that I have to compare myself to other approaches.",
            "The first one is the one look to sellers that always to describe concept and returns a list of words and phrases related to that concept and the other one is an algorithm that is color copy BM 25 that is based in properly probabilistic models with a bag of words implementation."
        ],
        [
            "Here are the results that are compared to mine and in some cases and they want me in position at three and position at 5, but in position at 10 I still won.",
            "By really close, but the most it close to me was the BM 25 algorithm that showed better performance than a one look reverse dictionary.",
            "The higher results are consistent with the one seen in the reverse dictionary using the norms of Florida that showed the best performance.",
            "That's another result that we get the Florida are the best norms that works for my task."
        ],
        [
            "So here we have the conclusions.",
            "This paper introduces a model for an onomatopoeia logical search that has some novelties.",
            "It's really simple and we are using graph techniques and algorithms that run into mathematical things that works in a good way here.",
            "We observed that the graph that was built with all the notes and the age is contained in the data set tends to be not so good.",
            "Due to the number of parts that are common, wrong results."
        ],
        [
            "On we have shown how description of concepts that are made by common people with nonscientific specification can retrieve accurate results using our method and the success of our system with no scientific input can drive new lines of applied search and implementation of different assistant reading systems, especially oriented to people with a range of a fascia's like dysnomia on Alzheimer's disease."
        ],
        [
            "Here are my references and."
        ],
        [
            "Thank you very much and you have any question."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everyone, my name is Jorge Reyes magania.",
                    "label": 0
                },
                {
                    "sent": "I'm coming from the National University of Thomas of Mexico, so let's get started.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have all the content that we're going to be talking about.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, we have two types of dictionary.",
                    "label": 0
                },
                {
                    "sent": "The first one is the most common one.",
                    "label": 0
                },
                {
                    "sent": "That is there some astrological that provides meanings.",
                    "label": 0
                },
                {
                    "sent": "When you have a word, the user obtain the meaning of such word and the other one is a onomatopoeia logical.",
                    "label": 1
                },
                {
                    "sent": "When that works in the opposite way, given the description of a word, the user obtained a related concept.",
                    "label": 1
                },
                {
                    "sent": "So I try to do an electronic onomatopoeia, logical dictionary.",
                    "label": 0
                },
                {
                    "sent": "I kind of data reverse dictionary that are based on some word Association norms that I'm going to be talking about this kind of resources.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So nowadays we can think that the most logical search is made through different electronic ways.",
                    "label": 0
                },
                {
                    "sent": "For example Google, Yahoo or another searchers.",
                    "label": 0
                },
                {
                    "sent": "When you can put your definition of what you're trying to looking at an all the word that you're trying to to look is going to appear here.",
                    "label": 0
                },
                {
                    "sent": "But in sometimes this word is not good enough on it gives more information that we are looking at, so there is no good sometimes.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this layer I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Percent the main idea of my work.",
                    "label": 0
                },
                {
                    "sent": "First of all, I'm going to have some definition made by humans.",
                    "label": 0
                },
                {
                    "sent": "For example, a person is say what is a small road and living in trees.",
                    "label": 1
                },
                {
                    "sent": "With a long, bushy tail, so with this definition and this description, I only kept some specific words in a lame away.",
                    "label": 1
                },
                {
                    "sent": "For example, I have.",
                    "label": 0
                },
                {
                    "sent": "Only in prod and leave tail bushy three small and with these words I have.",
                    "label": 0
                },
                {
                    "sent": "Applied an algorithm into a graph.",
                    "label": 0
                },
                {
                    "sent": "This graph it's a mathematical object that has some properties, so you can run algorithms into this graph and using the words that people say to me I have to in fear.",
                    "label": 0
                },
                {
                    "sent": "That what the person is talking about is a squirrel, for example.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic idea of my work so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, I'm going to talk about the word Association norms.",
                    "label": 1
                },
                {
                    "sent": "These are some psychological resources cycle linguistical resources.",
                    "label": 0
                },
                {
                    "sent": "When an this work in first you they give you a word.",
                    "label": 0
                },
                {
                    "sent": "For example B and you have to say the first word that comes to your mind when you hear being on.",
                    "label": 1
                },
                {
                    "sent": "When somebody says be an in the first column, we have different words that represent the response that people give.",
                    "label": 0
                },
                {
                    "sent": "For example, hive honey Steam bus was batura of all of these words were made by people when they hear being when they hear be so, and the first one they they would be is a stimulus word and the other ones are called the response words in the second column.",
                    "label": 0
                },
                {
                    "sent": "Sorry in the second column we have a mathematical values that represent the frequency and in other way it represents the associative strength.",
                    "label": 0
                },
                {
                    "sent": "That is only a percentage that is calculated from the frequency.",
                    "label": 0
                },
                {
                    "sent": "So with this into values mathematical values I construct.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A graph.",
                    "label": 0
                },
                {
                    "sent": "The word Association norms are the notes.",
                    "label": 0
                },
                {
                    "sent": "The words are the notes of my graph and for example in this layer we have.",
                    "label": 0
                },
                {
                    "sent": "The word being in the center that represent the stimulus word, and all the responses are around.",
                    "label": 0
                },
                {
                    "sent": "It seems to be some edges that connect each one of the words that are present in the notes.",
                    "label": 0
                },
                {
                    "sent": "And you can put the value into that edge.",
                    "label": 0
                },
                {
                    "sent": "That is called a weighted graph, OK?",
                    "label": 0
                },
                {
                    "sent": "And there is another example here that is of whether whether it's in the center and all the other words were the responses that people give to water.",
                    "label": 0
                },
                {
                    "sent": "I only present two of these stimulus word with this response is because all of the graph that I used is too big to present in one single layer.",
                    "label": 0
                },
                {
                    "sent": "So this is the main idea of the original resource that I used to run my algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I used the word Association norms that are available on Internet.",
                    "label": 0
                },
                {
                    "sent": "The first one is the edenburg associated, the sorrows that it has 8211 stimulus words with 20,445 different words, including the stimuli and responses.",
                    "label": 0
                },
                {
                    "sent": "The other one is the collection of the University of South Florida, that it has more than 6000 participants than produced nearly 3/4 of a million responses to 5018 stimulus word.",
                    "label": 1
                },
                {
                    "sent": "I use in a separate wave.",
                    "label": 0
                },
                {
                    "sent": "It means that I have one graph for AT and another graph for Florida.",
                    "label": 0
                },
                {
                    "sent": "Collection",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the general idea of the graph, is the graph representing the word Association norms have been elaborated with lemmas?",
                    "label": 1
                },
                {
                    "sent": "I use only lemmas of each one of the words is undirected so that every stimulus is connected to every associated word without any presidents order an for the weight of the ages.",
                    "label": 1
                },
                {
                    "sent": "I have two functions, the frequency and the Association strength.",
                    "label": 0
                },
                {
                    "sent": "In this algorithm that I use, works on shortest paths, so I have to make an inversion of that frequency and an inversion of the associative strength.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, when I finish my graph I it's time to produce or to Rhonda between centrally algorithm that it makes the search.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm is a math thing and it was developed in 1977.",
                    "label": 0
                },
                {
                    "sent": "And the main idea of this algorithm is that a traditional between this algorithm assumes an important nodes connect to other nodes.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the formula that calculates this between centrality from one.",
                    "label": 0
                },
                {
                    "sent": "Note, it's a mathematical stuff, and it says that for a given node V in a graph, GD between centrality is calculated as a relation between the number of shortest paths between nodes.",
                    "label": 1
                },
                {
                    "sent": "A&J that pass through Nelvie and the number of service parts between nodes I enjoy so.",
                    "label": 0
                },
                {
                    "sent": "In this next layer.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I give you a little explanation of how this is calculated.",
                    "label": 0
                },
                {
                    "sent": "This is a little very small graph that only contains for notes and we are trying to calculate.",
                    "label": 0
                },
                {
                    "sent": "In this case there between the centrality of node V. So let's see we have to look of every path in the graph.",
                    "label": 0
                },
                {
                    "sent": "For example, if we look.",
                    "label": 0
                },
                {
                    "sent": "The path that are from A to B.",
                    "label": 0
                },
                {
                    "sent": "It's only one path.",
                    "label": 0
                },
                {
                    "sent": "And how many of those parts pass through be only one?",
                    "label": 0
                },
                {
                    "sent": "Because he's in the other side, so.",
                    "label": 0
                },
                {
                    "sent": "When we calculated the value is wind divided into 1 divided into 1.",
                    "label": 0
                },
                {
                    "sent": "Is everything is almost the same, but in the next in the last element that we have into right, we see that we have a 0 because we're calculating there the path between C&D, we have only one path between Sandy, but no, no one.",
                    "label": 0
                },
                {
                    "sent": "None of those paths pass through being.",
                    "label": 0
                },
                {
                    "sent": "Remember that we're calculating day between centrality of node B. OK, some mad that I have to do a little brief explanation.",
                    "label": 0
                },
                {
                    "sent": "Because I use these things to run my algorithm instead of having notes like ABC or DI have forts, remember that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little variation of this algorithm between centrality was made, and instead of having the shortest path in all the graph, they use only one subset that works like the origin and one other subset that works like in the destination of the shortest path or the target and between centrality are the notes that are in the middle of those subsets, so.",
                    "label": 0
                },
                {
                    "sent": "For example, this is a general idea of how the algorithm works, but.",
                    "label": 0
                },
                {
                    "sent": "My subset is going to be the the words that I present in their first layers rather than bushy animals, cetera an these same subset is going to be the target one, so it is supposed, or it is expected that between centrality nodes.",
                    "label": 1
                },
                {
                    "sent": "In the middle of those is the word squirrel.",
                    "label": 0
                },
                {
                    "sent": "That's how my work.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is done another algorithm that I tried is called page rank, that is the one used originally by Google that was used to rank the the pages that we have an Internet.",
                    "label": 0
                },
                {
                    "sent": "Which one is the most important and taking into account all the incoming links that has into graph.",
                    "label": 1
                },
                {
                    "sent": "So for example we have this graph and node B has a highest value because it has a lot of.",
                    "label": 0
                },
                {
                    "sent": "Incoming links.",
                    "label": 0
                },
                {
                    "sent": "Instead of having.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Web pages they have words.",
                    "label": 0
                },
                {
                    "sent": "The purchases remain here.",
                    "label": 0
                },
                {
                    "sent": "Is that the target word tested with a definition to be searched correspond to the higher scores returned by the patron algorithm.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is some of briefly several codes that represent the algorithm complete.",
                    "label": 0
                },
                {
                    "sent": "We have first above to pre process all the one data sets, having only the lemmas we have to preprocess the definition to search.",
                    "label": 0
                },
                {
                    "sent": "In the same way we kept only the lemmas, we remove the stop words.",
                    "label": 0
                },
                {
                    "sent": "Then we build the graph.",
                    "label": 0
                },
                {
                    "sent": "With the word Association norms.",
                    "label": 0
                },
                {
                    "sent": "Second, I have to print graph because in a very big graph all the paths were going wrong and we are not getting the correct words.",
                    "label": 0
                },
                {
                    "sent": "So I have to cut some notes into the note that give me a more reduced graph and this graph work in a better way.",
                    "label": 0
                },
                {
                    "sent": "And for each definition.",
                    "label": 0
                },
                {
                    "sent": "I only kept the words that were available in the World Association arms.",
                    "label": 0
                },
                {
                    "sent": "I build a sub graph that was used like source and target and I run the algorithm the between centrality and Pagerank that you can see in the last year part of this server code that is only the end the call to the methods, the mathematical methods.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, I have a evaluation corpus.",
                    "label": 0
                },
                {
                    "sent": "Consisting of seven concepts.",
                    "label": 0
                },
                {
                    "sent": "Then the finisher were proved of each one of those concepts they were given by human native speakers.",
                    "label": 0
                },
                {
                    "sent": "In most cases that officially the definition are very different.",
                    "label": 0
                },
                {
                    "sent": "From the ones found in dictionaries, they lack specialized terms and include cultural references and connotation, and the words that I work that were tested were whether Squirrel bench Hurricane, lemon bucket, and clothes.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is a brief example of how the definition what were the definition that were tested in my program.",
                    "label": 0
                },
                {
                    "sent": "If these are the definition of squirrel.",
                    "label": 1
                },
                {
                    "sent": "And to give you an idea, how were the definition that were used?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For my evaluation also I use a metric that is called precision at K, that precision at one stands that the concept associated to a definition was Frank in the 1st place position at rest at three.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the concept was in the first 3 results, precision and the same applies to precision at 5:00 or position at 10 that whatever was looking for was into that range.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My results are the following.",
                    "label": 0
                },
                {
                    "sent": "Remember that I didn't combine the two word Association arms.",
                    "label": 0
                },
                {
                    "sent": "I worked separately.",
                    "label": 0
                },
                {
                    "sent": "I have the 18 results in the upper.",
                    "label": 1
                },
                {
                    "sent": "But I have to.",
                    "label": 0
                },
                {
                    "sent": "The weight of the graph.",
                    "label": 0
                },
                {
                    "sent": "The first one is a emberas frequency and the inverse Association strength applied to the two algorithms.",
                    "label": 0
                },
                {
                    "sent": "Remember that I work with between the centrality and the other one was page rank.",
                    "label": 0
                },
                {
                    "sent": "In the first case, the higher value.",
                    "label": 0
                },
                {
                    "sent": "The highest value was obtained using between centrality and the weight of inverse Association strength.",
                    "label": 0
                },
                {
                    "sent": "Having those value in bold letter.",
                    "label": 0
                },
                {
                    "sent": "In the next result we have.",
                    "label": 0
                },
                {
                    "sent": "That is also consistent.",
                    "label": 0
                },
                {
                    "sent": "I mean the highest value was using the between centrality algorithm and using also the inverse inverse Association strength.",
                    "label": 0
                },
                {
                    "sent": "As you can see, the page rank algorithm, it's not good for this kind of task.",
                    "label": 0
                },
                {
                    "sent": "The the best that I've tried is between a central algorithm, OK?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After that I have to compare myself to other approaches.",
                    "label": 0
                },
                {
                    "sent": "The first one is the one look to sellers that always to describe concept and returns a list of words and phrases related to that concept and the other one is an algorithm that is color copy BM 25 that is based in properly probabilistic models with a bag of words implementation.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are the results that are compared to mine and in some cases and they want me in position at three and position at 5, but in position at 10 I still won.",
                    "label": 0
                },
                {
                    "sent": "By really close, but the most it close to me was the BM 25 algorithm that showed better performance than a one look reverse dictionary.",
                    "label": 0
                },
                {
                    "sent": "The higher results are consistent with the one seen in the reverse dictionary using the norms of Florida that showed the best performance.",
                    "label": 1
                },
                {
                    "sent": "That's another result that we get the Florida are the best norms that works for my task.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we have the conclusions.",
                    "label": 0
                },
                {
                    "sent": "This paper introduces a model for an onomatopoeia logical search that has some novelties.",
                    "label": 1
                },
                {
                    "sent": "It's really simple and we are using graph techniques and algorithms that run into mathematical things that works in a good way here.",
                    "label": 1
                },
                {
                    "sent": "We observed that the graph that was built with all the notes and the age is contained in the data set tends to be not so good.",
                    "label": 1
                },
                {
                    "sent": "Due to the number of parts that are common, wrong results.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On we have shown how description of concepts that are made by common people with nonscientific specification can retrieve accurate results using our method and the success of our system with no scientific input can drive new lines of applied search and implementation of different assistant reading systems, especially oriented to people with a range of a fascia's like dysnomia on Alzheimer's disease.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are my references and.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much and you have any question.",
                    "label": 0
                }
            ]
        }
    }
}