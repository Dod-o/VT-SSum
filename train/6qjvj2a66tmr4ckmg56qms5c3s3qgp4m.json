{
    "id": "6qjvj2a66tmr4ckmg56qms5c3s3qgp4m",
    "title": "Grounding of Textual Phrases in Images by Reconstruction",
    "info": {
        "author": [
            "Anna Rohrbach, Max Planck Institute for Informatics, Max Planck Institute"
        ],
        "published": "Oct. 24, 2016",
        "recorded": "October 2016",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/eccv2016_rohrbach_textual_phrases/",
    "segmentation": [
        [
            "Hi, welcome to my talk about grounding of textual phrases and images by reconstruction.",
            "Imagine."
        ],
        [
            "I showed you this image and I tell you to find this 2 girls in hats in the middle."
        ],
        [
            "As humans, we can easily understand which people I'm referring to and locate them in the scene.",
            "Even though there are other people.",
            "So we refer to this task as visual grounding and this is what I'm going to talk about today.",
            "This task is."
        ],
        [
            "Different from the more familiar classical problem of object detection, which is aiming to localize all the instances of a certain class, like a person."
        ],
        [
            "Or hat and back to our setting."
        ],
        [
            "We're actually in."
        ],
        [
            "Trusted to localize the potentially multi word complex natural language expressions.",
            "So this task."
        ],
        [
            "Can many interesting applications like the human robot interaction or it can be supporting other language vision tasks like image or visual captioning or visual question answering as well as it can provide support for image to text coreference resolution.",
            "So this."
        ],
        [
            "Maria, is that a test?",
            "I'm given an image and a certain expression or a query.",
            "We are trying to predict the bounding box for this expression.",
            "Now what can be the settings at training time?"
        ],
        [
            "So we can have a fully annotated data set which provides images, associated expressions, and the corresponding localizations.",
            "For example, bounding boxes.",
            "However, not all the datasets are like that.",
            "So this setting I refer to as supervised."
        ],
        [
            "Unlike the other searching, when we have images, we know the associated phrases, but we have no localization information.",
            "This is the unsupervised scenario."
        ],
        [
            "And the intermediate scenario is semi supervised when for part of the phrases we have on Earth the localization and for others we don't.",
            "So we are particularly interested in reducing this amount of the localizations of provision in this work.",
            "Um?"
        ],
        [
            "Following I will be depicting this respective regimes with this pictograms, so prior."
        ],
        [
            "Work has mainly focused on the fully supervised scenario and there has been some work on the unsupervised scenario by looking how to latently infer correspondence between fragments of sentences and images this work."
        ],
        [
            "Allans itself in all three supervision regimes, and the main result that we obtained."
        ],
        [
            "This work is that we are able to substantially improve in all these settings across the board, so let me now introduce you to our approach."
        ],
        [
            "Once again, the goal is."
        ],
        [
            "Given an image and the sentence or phrase predicts this bounding box grounding, so how do we achieve?"
        ],
        [
            "This is our approach.",
            "We call it Grounder, which stands for grounding by reconstruction that works as follows.",
            "We start with Jenner."
        ],
        [
            "Getting bounding box proposals.",
            "Now for every proposal region and the query phrase, we predict."
        ],
        [
            "Attention mechanism which can give us the attention weights Alpha for each of the regions, and we will simply take the box with the highest attention weight as the grounding prediction.",
            "This reflects what happens at Test time and a training time.",
            "Our goal is to learn such an attention mechanism, which would actually provide us the correct grounding.",
            "So let's look at different supervision scenarios.",
            "Again, in the fully supervised case we have the ground truth information about the bounding box for every query phrase, which means that we can directly optimize our attention mechanism for the correct attention loss.",
            "However, what should we do if you're not given any localization information at training time?",
            "We suggest to do their reconstruction.",
            "That's where the reconstruction comes from in our approach.",
            "So we focus on this visual features which have been selected by the attention mechanism selected or attended tool, and we are trying to generate the phrase again and then make sure that we have actually reconstructed our input query phrase, which then guarantees that we have attended to the right region.",
            "And that allows us to basically latently learn the attention mechanism that we care about.",
            "So and then the same as supervised setting respectively for the instances where the supervision is available, we can rely on the attention loss and for everything else, we can use the reconstruction loss.",
            "So this is the high level overview of the approach and now let me walk you through the details of the implementation that we took.",
            "So the phrases are."
        ],
        [
            "Presented with the word embedding and then encoded completely with an illustration."
        ],
        [
            "Each of the regions is represented with the CNN feature and the attention mechan."
        ],
        [
            "ISM is combining both inputs together and then predicts the softmax values Alpha."
        ],
        [
            "As I said, I used to obtain the grounding.",
            "Now there."
        ],
        [
            "Mission loss."
        ],
        [
            "Is the cross entropy loss which estimates the log probability of predicting the correct attack?"
        ],
        [
            "For example, with certain index J granters.",
            "Now the reconstruction."
        ],
        [
            "Is achieved by computing the weighted sum over the visual features with the predicted weights Alpha and then providing this as input into another STM, which then generates the phrase again."
        ],
        [
            "And finally, the reconstruction loss is computed as a cross entropy loss to estimate the log probability of predicting the imp."
        ],
        [
            "Phrase Q.",
            "And to sum it up, for all the instances where the supervision is available, we have this attention, loss and overall for all instances we compute the reconstruction loss.",
            "So let us look at how this performs in practice.",
            "We evaluate our approach on 2."
        ],
        [
            "Datasets we start with the Flickr surgically entities which has been proposed last year, which augmented the captions of the images by providing bounding boxes for all the individual noun phrases.",
            "So we generate 100 object proposals for every image and we extract the faster CNN visual features.",
            "And now in the following the accuracy is the percentage of the phrases where the intersection over Union between the predicted box and the ground truth box is at least 0.5.",
            "Now let's look at the result."
        ],
        [
            "So he replied to the accuracy with respect to the amount of the available annotations.",
            "So OK, responds to the unsupervised setting.",
            "An 100 corresponds to supervised setting.",
            "Now here we compare our ground are unsupervised, which only has the reconstruction loss and one of the prior works which we have adopted by performing the best case kind of evaluation to this evaluation protocol.",
            "On the right hand side, we compared to all the recent state of the art approaches which are fully supervised.",
            "And now in the middle you plot the semi supervised performance in different regimes of supervision.",
            "So what do we notice in this plot?"
        ],
        [
            "First of all, without any localization information, we are able to achieve reasonable performance which is on par with some of the fully supervised methods on the right hand side.",
            "Now."
        ],
        [
            "Is the little annotations.",
            "We significantly improving their performance and achieving close to state of the art result?",
            "And another."
        ],
        [
            "Duration is that our semi supervised Grounder, which relies on two losses, is consistently better than the supervised only method which only relies on the attention loss.",
            "And finally we improve over the best available result by 4.5% on this data set.",
            "We also evaluate on the refer it game data set."
        ],
        [
            "Which provides for different regions in the image referring expressions which are slightly more complex than in the Flickr data set.",
            "So again, we have 100 proposal regions and the VGG and some spatial features as used in some prior work.",
            "And the same evaluation protocol.",
            "Now let's look."
        ],
        [
            "This data set we observe similar tendencies.",
            "This data set is harder, however we see."
        ],
        [
            "That again with little annotated data we are able to substantially improve.",
            "Along the axis and."
        ],
        [
            "Again, the same as supervised always shows a better result than the fully supervised by.",
            "Again, supporting that the reconstruction loss provides our method more information than just looking at the one correct predicted bounding box.",
            "And here the gap between our approach and the best prior work is 10.6 points.",
            "So let us look at some qualitative results here I compare."
        ],
        [
            "Unsupervised in the supervised version of Rounder to show you that.",
            "The unsupervised method is able to localize many things correctly, like a woman about."
        ],
        [
            "Icicle?"
        ],
        [
            "However, for the pavement, it doesn't predict the entirely correct result.",
            "And, uh."
        ],
        [
            "Comparison on the left side there is the supervised method and on the right side rounder with only three percent of annotated data used.",
            "And we see a man or."
        ],
        [
            "Which pants?"
        ],
        [
            "Brown West adult."
        ],
        [
            "Some predictions are similar or even better than of the supervised method on the left."
        ],
        [
            "Some more results, so the preferred game data set has this referring expressions and I hear show the predictions in red and the ground rules region.",
            "In this light blue.",
            "So picture to the left on the wall is correctly recognized, which means that we can understand the spatial information in the query."
        ],
        [
            "Another example person in blue.",
            "Here we collectively understand the color."
        ],
        [
            "And here is 1 failure where we incorrectly localized the White Horse right of Brown horse in the middle.",
            "Basically our approach instead localizes this Brown horse in the middle, which can be due to multiple reasons.",
            "This is the rather complex natural language expression to understand and then our approach does not explicitly introduce the pairwise relations between different objects in the scene, which would be necessary to actually correctly predict the bounding box in this case.",
            "So."
        ],
        [
            "To conclude, we have shown that the unsupervised grounding works in our ground are with the reconstruction objective.",
            "We have also shown that the semi supervised setting actually is best.",
            "It improves over the purely supervised setting.",
            "It works very efficiently with very little annotations, and it improves significantly over the state of the art methods and some possible extensions naturally are to integrate the knowledge of multiple phrases at training time and then also model the spatial relations between them at training time.",
            "So I'd like to invite you to attend our poster.",
            "It's up."
        ],
        [
            "The errors in the first floor.",
            "If you haven't been there yet, please check out our follow-up work, which just recently won the Victory Challenge and I will have this hopefully.",
            "Playing video demo here while I take your questions, thank you.",
            "So I have a question, so it seems that you can use your approach to even do like learning on cocoa like you have list of objects in cocoa and the images without having the bounding boxes.",
            "So if you use a standard data set like this I mean do you have any idea?",
            "We tried it to do like weakly supervised learning in cocoa for example.",
            "With this approach it seems it should work right?",
            "That would be interesting to try.",
            "We've we've sort of that and we never got to it, so also in Pascal there are some works which will be supervised detection that would be interesting to try and it and have you compared to like attention based approaches.",
            "So there are these caption attention based approaches in captioning where the attention keeps jumping here and there.",
            "And that also seems kind of related to this idea.",
            "Have you tried comparing against them?",
            "So now we haven't so short and Intel would be predicting the attention for every word while generating the caption.",
            "One could look into the quality of that.",
            "I believe nobody so far evaluated how well that really works.",
            "And yeah, we haven't.",
            "We haven't looked at how that that would perform, but it's an interesting thing to try.",
            "Is it also learned blatantly?",
            "Basically this.",
            "Mechanism.",
            "OK, thank you, thank you.",
            "Let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, welcome to my talk about grounding of textual phrases and images by reconstruction.",
                    "label": 0
                },
                {
                    "sent": "Imagine.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I showed you this image and I tell you to find this 2 girls in hats in the middle.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As humans, we can easily understand which people I'm referring to and locate them in the scene.",
                    "label": 1
                },
                {
                    "sent": "Even though there are other people.",
                    "label": 1
                },
                {
                    "sent": "So we refer to this task as visual grounding and this is what I'm going to talk about today.",
                    "label": 0
                },
                {
                    "sent": "This task is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different from the more familiar classical problem of object detection, which is aiming to localize all the instances of a certain class, like a person.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or hat and back to our setting.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're actually in.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trusted to localize the potentially multi word complex natural language expressions.",
                    "label": 0
                },
                {
                    "sent": "So this task.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can many interesting applications like the human robot interaction or it can be supporting other language vision tasks like image or visual captioning or visual question answering as well as it can provide support for image to text coreference resolution.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maria, is that a test?",
                    "label": 0
                },
                {
                    "sent": "I'm given an image and a certain expression or a query.",
                    "label": 0
                },
                {
                    "sent": "We are trying to predict the bounding box for this expression.",
                    "label": 0
                },
                {
                    "sent": "Now what can be the settings at training time?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can have a fully annotated data set which provides images, associated expressions, and the corresponding localizations.",
                    "label": 0
                },
                {
                    "sent": "For example, bounding boxes.",
                    "label": 0
                },
                {
                    "sent": "However, not all the datasets are like that.",
                    "label": 0
                },
                {
                    "sent": "So this setting I refer to as supervised.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unlike the other searching, when we have images, we know the associated phrases, but we have no localization information.",
                    "label": 0
                },
                {
                    "sent": "This is the unsupervised scenario.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the intermediate scenario is semi supervised when for part of the phrases we have on Earth the localization and for others we don't.",
                    "label": 0
                },
                {
                    "sent": "So we are particularly interested in reducing this amount of the localizations of provision in this work.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Following I will be depicting this respective regimes with this pictograms, so prior.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work has mainly focused on the fully supervised scenario and there has been some work on the unsupervised scenario by looking how to latently infer correspondence between fragments of sentences and images this work.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Allans itself in all three supervision regimes, and the main result that we obtained.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This work is that we are able to substantially improve in all these settings across the board, so let me now introduce you to our approach.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once again, the goal is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Given an image and the sentence or phrase predicts this bounding box grounding, so how do we achieve?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is our approach.",
                    "label": 0
                },
                {
                    "sent": "We call it Grounder, which stands for grounding by reconstruction that works as follows.",
                    "label": 1
                },
                {
                    "sent": "We start with Jenner.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Getting bounding box proposals.",
                    "label": 0
                },
                {
                    "sent": "Now for every proposal region and the query phrase, we predict.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Attention mechanism which can give us the attention weights Alpha for each of the regions, and we will simply take the box with the highest attention weight as the grounding prediction.",
                    "label": 0
                },
                {
                    "sent": "This reflects what happens at Test time and a training time.",
                    "label": 0
                },
                {
                    "sent": "Our goal is to learn such an attention mechanism, which would actually provide us the correct grounding.",
                    "label": 0
                },
                {
                    "sent": "So let's look at different supervision scenarios.",
                    "label": 0
                },
                {
                    "sent": "Again, in the fully supervised case we have the ground truth information about the bounding box for every query phrase, which means that we can directly optimize our attention mechanism for the correct attention loss.",
                    "label": 1
                },
                {
                    "sent": "However, what should we do if you're not given any localization information at training time?",
                    "label": 0
                },
                {
                    "sent": "We suggest to do their reconstruction.",
                    "label": 0
                },
                {
                    "sent": "That's where the reconstruction comes from in our approach.",
                    "label": 0
                },
                {
                    "sent": "So we focus on this visual features which have been selected by the attention mechanism selected or attended tool, and we are trying to generate the phrase again and then make sure that we have actually reconstructed our input query phrase, which then guarantees that we have attended to the right region.",
                    "label": 0
                },
                {
                    "sent": "And that allows us to basically latently learn the attention mechanism that we care about.",
                    "label": 0
                },
                {
                    "sent": "So and then the same as supervised setting respectively for the instances where the supervision is available, we can rely on the attention loss and for everything else, we can use the reconstruction loss.",
                    "label": 1
                },
                {
                    "sent": "So this is the high level overview of the approach and now let me walk you through the details of the implementation that we took.",
                    "label": 0
                },
                {
                    "sent": "So the phrases are.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Presented with the word embedding and then encoded completely with an illustration.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each of the regions is represented with the CNN feature and the attention mechan.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "ISM is combining both inputs together and then predicts the softmax values Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I said, I used to obtain the grounding.",
                    "label": 0
                },
                {
                    "sent": "Now there.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mission loss.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the cross entropy loss which estimates the log probability of predicting the correct attack?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, with certain index J granters.",
                    "label": 0
                },
                {
                    "sent": "Now the reconstruction.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is achieved by computing the weighted sum over the visual features with the predicted weights Alpha and then providing this as input into another STM, which then generates the phrase again.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, the reconstruction loss is computed as a cross entropy loss to estimate the log probability of predicting the imp.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Phrase Q.",
                    "label": 0
                },
                {
                    "sent": "And to sum it up, for all the instances where the supervision is available, we have this attention, loss and overall for all instances we compute the reconstruction loss.",
                    "label": 0
                },
                {
                    "sent": "So let us look at how this performs in practice.",
                    "label": 0
                },
                {
                    "sent": "We evaluate our approach on 2.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Datasets we start with the Flickr surgically entities which has been proposed last year, which augmented the captions of the images by providing bounding boxes for all the individual noun phrases.",
                    "label": 1
                },
                {
                    "sent": "So we generate 100 object proposals for every image and we extract the faster CNN visual features.",
                    "label": 1
                },
                {
                    "sent": "And now in the following the accuracy is the percentage of the phrases where the intersection over Union between the predicted box and the ground truth box is at least 0.5.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at the result.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So he replied to the accuracy with respect to the amount of the available annotations.",
                    "label": 0
                },
                {
                    "sent": "So OK, responds to the unsupervised setting.",
                    "label": 0
                },
                {
                    "sent": "An 100 corresponds to supervised setting.",
                    "label": 0
                },
                {
                    "sent": "Now here we compare our ground are unsupervised, which only has the reconstruction loss and one of the prior works which we have adopted by performing the best case kind of evaluation to this evaluation protocol.",
                    "label": 0
                },
                {
                    "sent": "On the right hand side, we compared to all the recent state of the art approaches which are fully supervised.",
                    "label": 0
                },
                {
                    "sent": "And now in the middle you plot the semi supervised performance in different regimes of supervision.",
                    "label": 0
                },
                {
                    "sent": "So what do we notice in this plot?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First of all, without any localization information, we are able to achieve reasonable performance which is on par with some of the fully supervised methods on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the little annotations.",
                    "label": 0
                },
                {
                    "sent": "We significantly improving their performance and achieving close to state of the art result?",
                    "label": 0
                },
                {
                    "sent": "And another.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Duration is that our semi supervised Grounder, which relies on two losses, is consistently better than the supervised only method which only relies on the attention loss.",
                    "label": 0
                },
                {
                    "sent": "And finally we improve over the best available result by 4.5% on this data set.",
                    "label": 0
                },
                {
                    "sent": "We also evaluate on the refer it game data set.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which provides for different regions in the image referring expressions which are slightly more complex than in the Flickr data set.",
                    "label": 1
                },
                {
                    "sent": "So again, we have 100 proposal regions and the VGG and some spatial features as used in some prior work.",
                    "label": 0
                },
                {
                    "sent": "And the same evaluation protocol.",
                    "label": 0
                },
                {
                    "sent": "Now let's look.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This data set we observe similar tendencies.",
                    "label": 0
                },
                {
                    "sent": "This data set is harder, however we see.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That again with little annotated data we are able to substantially improve.",
                    "label": 0
                },
                {
                    "sent": "Along the axis and.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, the same as supervised always shows a better result than the fully supervised by.",
                    "label": 0
                },
                {
                    "sent": "Again, supporting that the reconstruction loss provides our method more information than just looking at the one correct predicted bounding box.",
                    "label": 0
                },
                {
                    "sent": "And here the gap between our approach and the best prior work is 10.6 points.",
                    "label": 0
                },
                {
                    "sent": "So let us look at some qualitative results here I compare.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unsupervised in the supervised version of Rounder to show you that.",
                    "label": 0
                },
                {
                    "sent": "The unsupervised method is able to localize many things correctly, like a woman about.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Icicle?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, for the pavement, it doesn't predict the entirely correct result.",
                    "label": 0
                },
                {
                    "sent": "And, uh.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comparison on the left side there is the supervised method and on the right side rounder with only three percent of annotated data used.",
                    "label": 0
                },
                {
                    "sent": "And we see a man or.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which pants?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Brown West adult.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some predictions are similar or even better than of the supervised method on the left.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some more results, so the preferred game data set has this referring expressions and I hear show the predictions in red and the ground rules region.",
                    "label": 0
                },
                {
                    "sent": "In this light blue.",
                    "label": 0
                },
                {
                    "sent": "So picture to the left on the wall is correctly recognized, which means that we can understand the spatial information in the query.",
                    "label": 1
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another example person in blue.",
                    "label": 0
                },
                {
                    "sent": "Here we collectively understand the color.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is 1 failure where we incorrectly localized the White Horse right of Brown horse in the middle.",
                    "label": 1
                },
                {
                    "sent": "Basically our approach instead localizes this Brown horse in the middle, which can be due to multiple reasons.",
                    "label": 0
                },
                {
                    "sent": "This is the rather complex natural language expression to understand and then our approach does not explicitly introduce the pairwise relations between different objects in the scene, which would be necessary to actually correctly predict the bounding box in this case.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude, we have shown that the unsupervised grounding works in our ground are with the reconstruction objective.",
                    "label": 1
                },
                {
                    "sent": "We have also shown that the semi supervised setting actually is best.",
                    "label": 0
                },
                {
                    "sent": "It improves over the purely supervised setting.",
                    "label": 0
                },
                {
                    "sent": "It works very efficiently with very little annotations, and it improves significantly over the state of the art methods and some possible extensions naturally are to integrate the knowledge of multiple phrases at training time and then also model the spatial relations between them at training time.",
                    "label": 1
                },
                {
                    "sent": "So I'd like to invite you to attend our poster.",
                    "label": 0
                },
                {
                    "sent": "It's up.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The errors in the first floor.",
                    "label": 0
                },
                {
                    "sent": "If you haven't been there yet, please check out our follow-up work, which just recently won the Victory Challenge and I will have this hopefully.",
                    "label": 0
                },
                {
                    "sent": "Playing video demo here while I take your questions, thank you.",
                    "label": 0
                },
                {
                    "sent": "So I have a question, so it seems that you can use your approach to even do like learning on cocoa like you have list of objects in cocoa and the images without having the bounding boxes.",
                    "label": 0
                },
                {
                    "sent": "So if you use a standard data set like this I mean do you have any idea?",
                    "label": 0
                },
                {
                    "sent": "We tried it to do like weakly supervised learning in cocoa for example.",
                    "label": 0
                },
                {
                    "sent": "With this approach it seems it should work right?",
                    "label": 0
                },
                {
                    "sent": "That would be interesting to try.",
                    "label": 0
                },
                {
                    "sent": "We've we've sort of that and we never got to it, so also in Pascal there are some works which will be supervised detection that would be interesting to try and it and have you compared to like attention based approaches.",
                    "label": 0
                },
                {
                    "sent": "So there are these caption attention based approaches in captioning where the attention keeps jumping here and there.",
                    "label": 0
                },
                {
                    "sent": "And that also seems kind of related to this idea.",
                    "label": 0
                },
                {
                    "sent": "Have you tried comparing against them?",
                    "label": 0
                },
                {
                    "sent": "So now we haven't so short and Intel would be predicting the attention for every word while generating the caption.",
                    "label": 0
                },
                {
                    "sent": "One could look into the quality of that.",
                    "label": 0
                },
                {
                    "sent": "I believe nobody so far evaluated how well that really works.",
                    "label": 0
                },
                {
                    "sent": "And yeah, we haven't.",
                    "label": 0
                },
                {
                    "sent": "We haven't looked at how that that would perform, but it's an interesting thing to try.",
                    "label": 0
                },
                {
                    "sent": "Is it also learned blatantly?",
                    "label": 0
                },
                {
                    "sent": "Basically this.",
                    "label": 0
                },
                {
                    "sent": "Mechanism.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you, thank you.",
                    "label": 0
                },
                {
                    "sent": "Let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}