{
    "id": "jy5aososdwz7jysktm3vbtf4iaaqt3mf",
    "title": "Various Formulations for Learning the Kernel and Structured Sparsity",
    "info": {
        "author": [
            "Massimiliano Pontil, Department of Computer Science, University College London"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Multi-Task Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_pontil_vfl/",
    "segmentation": [
        [
            "So this is the plan of the."
        ],
        [
            "Talk.",
            "Is divided into roughly three part plus an introduction to the problem.",
            "So first define what I mean by learning the kernel some related problems.",
            "Then we'll discuss how do we learn?",
            "Convex combination of continuous parameterized set of kernels, which it's an extension of the standard multiple kernel learning setup.",
            "Then I will move on on a second problem which is.",
            "Learn the kernel in multi task learning and there I think this is a nice application of.",
            "Multiple kernel learning or learning the kernel.",
            "I think it's a problem where you see that this stuff really works so you can see learner kernel multitask learning as a way to learn how the tasks are related.",
            "And the last part representation, maybe it will be little bit short is.",
            "A follow up on a paper that we had just now at NIPS an it is adding constraints in multiple kernel learning so status parcel.",
            "So the."
        ],
        [
            "This is the setup.",
            "We so K is a positive semi definite kernel.",
            "X is the input space.",
            "Associated the current that is reproducing kernel Hilbert space, and this is the norm, so standard way of supervised learning is to learn from input output data for a function which minimizes these.",
            "There is a functional of FK, the minimizations taken into the processing Kerbal space.",
            "So this is a method which has been studied for several years now and everybody knows and includes several techniques actually support vector machines, regularize the squares, and so on many more.",
            "So here I consider input output data, But this could also be used for.",
            "Unsupervised learning, for example anomaly detection or other problems.",
            "So the loss function here is not really important.",
            "There is this output, but.",
            "I guess we're more familiar with this, so I use this notation.",
            "So the question is, how do we choose the kernel for a given task OK?",
            "So one.",
            "Approach is to look at this function of the kernel, so the result of this minimization problem as a criterion for selecting the kernel.",
            "OK, so so the idea is very simple and.",
            "We try to further minimize this quantity over.",
            "The Colonel came.",
            "We prescribe a set of candidate kernels, Script K and we started this minimization problem OK.",
            "So by now there have also been a series of papers which have considered this problem.",
            "And so here is just a very partial list, but many more.",
            "So the first observation."
        ],
        [
            "So let me review few properties of this problem and then we move to the main part of the talk.",
            "So the first observation is that our.",
            "Function of the kernel is convex, but this is it.",
            "Something that we like.",
            "To see that.",
            "So there is a simple proof which is given here.",
            "So you first use the representative theorem and which states that the solution of the previous, let's say abstract optimization problem where there was dysfunction introducing Kerbal Space.",
            "So the solution is a linear combination of the kernel center at the input data points.",
            "Very old and simple and very useful result.",
            "So when you substitute this formula into the objective function which was in the previous slide, you obtain a finite dimension optimization problem, in which this kernel.",
            "This is the kernel matrix here K the North or the kernel function, but the kernel matrix or abuse of notation, but I think it's clear to everybody.",
            "So when you substitute this formula in the objective function, you obtain a free dimension optimization problem.",
            "Then if you make this change of variable, so you called said the matrix K times the vector of coefficients E and let's assume for simplicity that K is invertible, then you obtain these equivalent problem and here you see immediately that this problem is jointly convex.",
            "In these new variables at an in the kernel K. OK, so as a result when we minimize this overhead function remains convex.",
            "And so this is.",
            "A quick way to see that our criterion that we're going to use is is a convex file.",
            "There isn't."
        ],
        [
            "Observation that.",
            "It is useful in gaining some intuition, some understanding.",
            "About what's going on here.",
            "Which I discuss in the case in which I have a safe night set of kind of kernels, and I want to.",
            "Minimize my criterion or the convex combination.",
            "Of these corners OK.",
            "So in this case my problem.",
            "So in this case basically here.",
            "So there is a minimization over at the musician over K bye.",
            "I changed it to minimization and you carry out the minimization over the kernel.",
            "Then you obtain these equivalent problem.",
            "Where song?",
            "The problem of learning curve, which is at the scribe is equivalent to this problem, which has a more compact form where you minimize over as many functions as your your reproducing kernel Hilbert spaces.",
            "And here so you have the sum of these functions and here you have the sum of the norms of each of these function in the corresponding Hilbert space.",
            "And if have heart is a solution of this problem.",
            "Then there.",
            "The convex combination of the kernels where the parameters are defined by this equation is the optimal is an optimal kernel."
        ],
        [
            "On my previous problem, OK.",
            "So these are observation alpis gaining some intuition about what's going on here.",
            "So we have we're trying to find the sparse combination of colors so sparsely play an important role here."
        ],
        [
            "The result follows essentially follows from an important property, which is in the paper of our own sign and then buy this property and then buy this variational form.",
            "For this stuff I think is well known.",
            "I will not discuss it in detail, and this can also be extended to also LP norms, and there have been some recent work about that, so we're here.",
            "You have an LP norm as opposed to this is an L1 norm."
        ],
        [
            "Another special case which help us gaining some intuition about the program describing is the parametric case.",
            "So the case in which these functions are linear and so here for simplicity, are just.",
            "1 dimensional function.",
            "So I take one component of XI multiplied by some weight vector beta J.",
            "So in this case the problem the kernel is essentially the last, so the kernels are 1 dimensional and bit more general.",
            "If the function FJ is our linear function on a subset of the variables.",
            "So this sum is over asset GL and the collection of this set is a partition.",
            "I have the group lasso problem.",
            "OK so.",
            "I have here this 21 norm, right?",
            "So this is a parametric.",
            "Analog."
        ],
        [
            "Of this of this phone."
        ],
        [
            "So we see that there is a close relation between the problem of learning the kernel which describe, and these sparsity methods."
        ],
        [
            "What time do they start?",
            "Forgot 40.",
            "So this is so the first part is a word that we did actually along time ago."
        ],
        [
            "We have Andrea, Sergio and Charles Micheli.",
            "So we.",
            "Again, we so want to goal is to now to study this problem OK?",
            "So we choose as our class or iPod is a class of corners.",
            "A very general set which is a set of kernels which are obtained by taking.",
            "These say.",
            "With a closed convex Hall.",
            "Of a set of basic kernels which are parameterized by this parameter, Omega.",
            "OK, so Omega is in this set capital Omega, so this close convex solar is nothing else that is interactive presentation.",
            "So I take a measure P. And I take the integral.",
            "Of the basic respect to this measure, OK?",
            "So what is an example of this so we could choose a G of Omega to be a Gaussian kernel, OK?",
            "And then and then.",
            "If so, so this is a class of radial kernels, and if the set capital Omega is the old positive line, by famous theorem by shamburg.",
            "The kernel, which would include in this class are.",
            "All radial corners, so their kernels which are.",
            "Um?",
            "Forget it.",
            "So Curtis of the norms X -- T, which occurs in every dimension.",
            "OK, so this is a very large, very large set OK?",
            "I however not difficult to show that if if the class or basic kernels is uniformly bounded.",
            "OK, according to this equation.",
            "Still, it makes sense to study our problem.",
            "It makes sense to minimize the function of K over this class.",
            "Becaused you can show that it is.",
            "It is a lower bound on the value of this objective function can take and cannot be equal to 0, which means that you cannot fit exactly the data.",
            "OK, so.",
            "So I will discuss some some thoughts about that.",
            "You could choose many other cases of of basic kernels.",
            "In particular, you could choose.",
            "Set of curvature.",
            "I mentioned parameterization OK. And of course, if you take this set to be finite set, you have standard multiple kernel learning OK.",
            "The connection to this so there is also this rated work which does.",
            "However, these people and and also this connection to this mix normalization also applies in this case, but it's somewhat more complicated, so it is describing this paper will not will not described here.",
            "So now I will."
        ],
        [
            "To.",
            "Understand it more in detail.",
            "What is the problem we have to solve?",
            "So our problem again is to minimize this function over K generic element of the class which I defined above.",
            "Now by using function duality you can rewrite the problem minimizing this function over K here as this minimax problem OK. Where so you maximize over K. So now is the Max because there is this minus maximize over K, the minimum over C of this with this function.",
            "And then.",
            "So as you can see, this function is convex in C for fixed, K is concave in K for fixing, and you can show that you can interchange the minimum and maximum, so there's a saddle point for this problem and these are the conditions for optimality so."
        ],
        [
            "See how the cat is a solution of this problem.",
            "So in case that I write as.",
            "They integrate with respect to the associated measure we had OK.",
            "So she had the cat is a solution of the problem if it only if these two conditions are verified.",
            "So when I look at this function of Omega.",
            "Uh.",
            "And I take Omega to be in the support of the measure P at this function attains its maximum OK. And also see that is a solution of the population problem.",
            "Fixed gun.",
            "OK, these are necessary sufficient condition for optimality and also what is interesting is that the solution cat is not unique in general, but you can always show that there is a solution which involves at most N plus one kernels.",
            "So in other words this measure is a finite measure with at most N plus one atoms.",
            "It is a useful result cause it motivates now an algorithm for.",
            "For finding Saddle point problem but other point solution.",
            "But here is the algorithm."
        ],
        [
            "So you start from this corner in the class.",
            "For example, you take Gaussian kernel.",
            "If you would take on this combination of Gaussians.",
            "You solve the regularization problem, so this could be a little square of an SVN.",
            "And then you try to find.",
            "A new kernel which maximizes our this function right?",
            "And then you take.",
            "By line search you'll find the convex combination of the current Colonel.",
            "So Katie and Geo, Maga hat, and then you repeat this OK.",
            "So every time you add 1 Colonel in the common combination.",
            "And so there is a result by Andreas, which shows that there exists a limit point of this algorithm and end limit point is a solution of of the optimization problem.",
            "OK, typically what happens is that two things.",
            "The algorithm in a different number of steps becausw, so something which I'm hiding here is that when you maximize this function, you must ensure that this is maximum is greater than the value of this function instead of G Omega, you put the current kernel OK.",
            "If that is not possible, you can stop.",
            "You have an optimal solution.",
            "And 2nd in this step, when you take a convex combination of the current kernel with the.",
            "You can get one off and you select the new one.",
            "So typically have a very small common combination colors.",
            "Um?"
        ],
        [
            "So we haven't done massive experiments with algate, and so in fact the work which I described in here is quite not very recent, so this is what we published 40 years ago.",
            "So here, just.",
            "A fairly simple experiment on the M list data where we try to compare this algorithm with multiple kernel learning.",
            "Where are we consider Gaussian kernels?",
            "We took parameters, so we divided our image in the upper part and the lower part and we consider a Gaussian for each of the two parts.",
            "So we have only two parameters and with our method we explore continuously the range of these parameters and we compare with multiple kernel learning where we consider a grid of parameters.",
            "So five for the.",
            "Sorry it was left and right.",
            "Have parts of the image not happen right?",
            "So we took five Gaussian for the left for the right, so we have 25 possible kernels, right?",
            "Cause?",
            "Each of the left you can multiply by one in the right.",
            "And these are the results that we got.",
            "So the main conclusion was that.",
            "So our method is not is robust to the range of the parameters, whereas the multiple kernel learning.",
            "So if you start some bad corners it.",
            "It doesn't find a good solution, so that was the main conclusion.",
            "Another conclusion was that although the method.",
            "Improves over multiple kernel learning and this robust over the parameter range.",
            "This comes with the price and the price is that."
        ],
        [
            "This second step in the method is a is a possibly hard.",
            "Optimization problem, but it's not convex.",
            "And so we solved this programming.",
            "You can write down this difference of convex functions when the parameter space is high dimensional.",
            "So in this case what I mentioned was OK, but when it is high dimensional we it is difficult to solve this problem basically OK.",
            "So."
        ],
        [
            "So.",
            "How do you have any questions so far?",
            "So I moved to the next."
        ],
        [
            "The nice little part of the talk.",
            "So here are some ideas about learning multitask learning.",
            "So this is different topic."
        ],
        [
            "So I apply exactly the same recipe that I've described before to the case in which I have multiple functions, so multiple tasks.",
            "So I think of my input space or my inputs X as a concatenation of two variables.",
            "So zed and T, where T is an index which the notes which of the task I'm considering.",
            "OK, so this functions F of ZNT is the value of the T task at the inputs that I impose that.",
            "OK.",
            "This is an occasion to everybody.",
            "OK, so this is just a way to say that I have many tasks which are indexed by this.",
            "At tea, sometimes you can put it here, but I just.",
            "Like this one, right?",
            "Like this when farsighted it is exactly the same problem that this guy before by this choice of the input.",
            "So in this case.",
            "Are you?",
            "If you choose a kernel which for this for the function on this space, so the kernel modest relationship between the task as I will now describe by some examples.",
            "So in this case the problem of choosing this kernel can be seen as as the problem of learning all the tasks are related within a model of course.",
            "And the example which I want to describe, which is a refined quite stacked if it's the case, is the simplest case in which these functions are linear.",
            "So for each T the function is linear instead.",
            "So in this case, if you think about it, the most general linear function that we can have is a function which has this parameterization.",
            "So V is a vector of parameters.",
            "And this is your feature vector so that there is a matrix B which depends on the task T and it is applied to the input set OK.",
            "So in this case the kernel becomes this function.",
            "OK.",
            "Please buy linear instead and that prime and then there are these methods which are task task specific.",
            "So the problem is to minimize my criterion over a kernel K which is of this type.",
            "For this method is better in some subset, OK?",
            "So let's see an example very simple exam."
        ],
        [
            "So here I take on this combination of only two.",
            "Um?",
            "Kernels to multitasker.",
            "So these are also called multitask earners, because there is a certainty.",
            "So the first one is for the linear instead and is linear Currin variables at and.",
            "Then there is this correct?",
            "Are Delta here interpretation?",
            "This is a kernel which is a specific, so the tasks are independent.",
            "If you learn independent if if you were just using this kernel II.",
            "Current doesn't contain TNT prime, so if you just use this kernel, you treat all the task as the same task.",
            "These matrices BT which appear."
        ],
        [
            "Before OK. For this, in this case they are orthogonal to each other.",
            "In this case they are all the same matrix.",
            "So in fact, if you use this kernel, you can show that the problem realization problem with the minimize over function in the space of this kernel is it is the same as this problem.",
            "But here you introduce a parameter for each task is WT is a parameter for each of the task.",
            "And you have this regularizer which says that you have this parameter Lambda in the convex combination and the regularizer says that.",
            "You try to make the task all close to an average vector OK. And then there.",
            "The closer Lambda is to zero, the closer is the task will be to the average.",
            "OK, so in the limited Lambda going to 0, all the task would be the same as something which is confirm.",
            "If you look at this kernel and is equal to 0, this kernel is not there, so you all have the same task OK.",
            "So of course, in this case doesn't make much sense to.",
            "You can try to minimize over Lambda, but in this case we can just do cross validation with just one parameter.",
            "But you can imagine if you have many different multitask earners than you could try to do to combine the different kernels by multiple kernel learning.",
            "And there was some real extension, of course.",
            "But"
        ],
        [
            "Let me move to another example, which is more interesting because here we see that we don't take on this combination of orphan Atom any kernel, so we get back to the previous example that I described.",
            "We take this class of corners.",
            "So there is a linear kernel which depends on the matrix D, which is in some set.",
            "So here I take the to be positive semi definite abounded trace.",
            "But I could take a smaller set.",
            "And then I have again the connector.",
            "Colonel on the task indexes.",
            "So here the interpretation is that by minimizing my criterion over kernels in this class, I try to find a common kernels across the task.",
            "OK, so once the is found, the tasks are learned one by one independently, but they all use the same kernel.",
            "OK, some linear kernel.",
            "And you can also have a different application of learning the kernel within this class.",
            "The first one is that you are trying to find few common orthogonal features shared by the task, and so this matrix you will just be the the message from the Reagan vectors of the Majesty.",
            "Or equivalently, you can show that this is nothing else.",
            "The trace norm regularization OK, in which task vectors are regularised by using the trace norm which encourage low rank matrices?",
            "And you could think of other examples in which you put further constraints on this matrix.",
            "Or you also try to model some.",
            "Cluster of task.",
            "So all these all these example which I described here could be implemented by choosing a class of kernels and going.",
            "Minimizing errors within this class OK."
        ],
        [
            "So I think multitask learning is an example where learn the Colonel was very well and just to mention one simple example which is also.",
            "A few years ago so if you try.",
            "Some datasets.",
            "Give us more than we."
        ],
        [
            "I described it works very well, so this mode.",
            "So for example."
        ],
        [
            "Here in these computers are conservative.",
            "I said we find the Matrix deal which has one leading a convector.",
            "So it means that the task are all approximated by 1 features and as you can see.",
            "By putting the task together you really improve performance, so I think so the main message from this discussion is that there are some.",
            "Simple classes of kernels which work very well multitask learning.",
            "However, there are other classes which could be tried which I haven't tried myself, but I think there are some talks later on which maybe speak about this, and I think this could be a promising direction for further work on learning kernel multitask learning.",
            "OK."
        ],
        [
            "So in the last part of the talk, and I think I have.",
            "5 minutes maybe or.",
            "OK.",
            "So the last part is."
        ],
        [
            "He just might say at recycle from the nips paper visia so.",
            "It will be extremely fast.",
            "In fact, we don't OK.",
            "So the idea here is so I discuss this review briefly what the kernel is.",
            "Then I discuss general proposal so we minimize our criteria within.",
            "At hand.",
            "Say close Contacts all over of the continuous class of kernels.",
            "Then I discuss learning the current multitask learning.",
            "The kernel class is also fairly large, so here I take the opposite direction.",
            "I taken a very much constrained class, so I choose all your flight number of kernels and I take on this combination.",
            "But I add additional constraints on the parameters in the convex combination.",
            "So this set capital Lambda is a convex set.",
            "Which.",
            "So when when it is a positive part and this is essentially multiple kernel learning, but when it is a subset of the positive orthant, we have restricted class of chorus.",
            "So again, in this case, the problem of this problem is equivalent to another regularization problem with some.",
            "Regularizer which depends on each of the functions in the kernel class and for the regularizer is defined by this formula OK.",
            "So Omega over vector, which here I called beta.",
            "Given Lambda, so you forgot that I had given, Lambda is the is.",
            "The is the feeling with this family or quadratics over this parameter Lambda?",
            "And then she was taken over Lambda in the sector land, OK?",
            "So."
        ],
        [
            "What is the penalty function?",
            "When does that capital Lambda is the?",
            "Is the positive order and this is just the L1 norm.",
            "So in that case I have standard.",
            "Multiple kernel learning.",
            "Goal is to consider the distinctive sets and to see that the choice of this set as a means to enforce some kind of structure sparsity.",
            "Across the coefficients in this combination OK?",
            "So here I'm just going to describe some examples in the in the parametric case for the case in which.",
            "When Lambda is a positive part and this is the L1 normal, this vector parameters, I'm just doing the last method lesu LA, so always forget.",
            "But for example, if I choose ordering constraints of this type Lambda, one is ready for the Lambda 2.",
            "My penalty function is not the L1 norm.",
            "But it is a combination of the L1 norm of the L2 norm.",
            "Which is given here and here is the level set.",
            "So in here the intuition is that."
        ],
        [
            "These constraints, favors favors solutions which are in the set.",
            "OK, so because of this property, so the penalty function is always greater than the L1 norm of the coefficients.",
            "OK, and the quality holds if it only if the vector form by the absolute values of this coefficient is in the set OK.",
            "So going back to the previous example, if."
        ],
        [
            "If absolute value of beta one is, get the absolute value of beta two.",
            "I have that one or otherwise I have something which is it's bigger than that one.",
            "So in this case I penalize more."
        ],
        [
            "So with this recipe you can generate different penalty functions which are convex, by the way, for the same reason.",
            "As the beginning of the talk, because this function is jointly convex, if I minimize over Lambda remains convincing beta.",
            "And you can also show that this is a norm when the set capital."
        ],
        [
            "Lasercorn um?",
            "So you can choose different sets of Lambda and understand that these favors some kind of structure sparsity, for example."
        ],
        [
            "Well, let's keep this one, but for example, you could have a director secret graph and you could say that.",
            "When a note points from when there is a link from another node, the variable in the first node is greater than the value of the second note."
        ],
        [
            "So this favors some kind of ordering relationship between.",
            "At the variables, so you try to favors vector beta, which in absolute value satisfied is ordering relationship.",
            "So bitter one absolute value should be encouraged to be greater or equal than both beta two and between absolute value.",
            "Without"
        ],
        [
            "You can have other examples.",
            "Offset the capital lambdas which favors higher order structure sparsity.",
            "Which I described here, but I don't have time to.",
            "To go into details, but this is the case for the difference operator, and yet the intuition is that if you have a vector, we satisfy this constraint.",
            "These vectors will be sparse only within contiguous.",
            "If you continue this region, so the sparsity pattern will be will consist of contiguous regions.",
            "So we have done some."
        ],
        [
            "Palamari simulations in which we try to use this penalty in a linear model with.",
            "Squared loss function regularize the squares and we have obtained."
        ],
        [
            "Some encourageing results so.",
            "One conclusion is that our method is is more general in the group Lasso course group lasso, and it is more robust, although in certain cases does not work as well as a group lasso.",
            "This is an example in which the group lasso works better.",
            "This is our model.",
            "And this is a group lost with rocky overlapping groups along the line."
        ],
        [
            "But in this case, if we have this bump here, so this is this, the vector of absolute value of the progression of the absolute value of the regression coefficient.",
            "Our method works better than the group lasso, which is this one.",
            "Here our method is this one right?",
            "And if we use these higher order constraints, we do even even even."
        ],
        [
            "So we have another number of results in which."
        ],
        [
            "We show that.",
            "Some other kind of target vectors.",
            "Be sure."
        ],
        [
            "That we will.",
            "We improve over the last method and then.",
            "And that.",
            "The panel through the right with the with the.",
            "Correct number of contiguous region is the one which works the best."
        ],
        [
            "So these are discussed more in detail in the in the newspaper, so I think I don't have time to say more.",
            "Let me just conclude the presentation.",
            "So I discussed the main observations, so considering continuous kernel classes may prove advantageous over.",
            "Finite combination kurnass weather.",
            "Men drawback is that this requires solving adaptation problem.",
            "Then I discussed learn the current multitask learning and as a means to learn it, ask relatedness.",
            "I discussed some simple class of multitaskers.",
            "But so here are the main messages that are richer class of kernel which will which I think could provide improved results.",
            "And finally I I comment on a new family of penalty function for structured sparsity, which I described only in the case of linear models.",
            "But this could be.",
            "Apply the same straightforward manner to the linear case, which will give a constrained version.",
            "Multiple kernel learning Zen in the linear case, and this appears to be.",
            "To work better than the previous methods, so thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the plan of the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk.",
                    "label": 0
                },
                {
                    "sent": "Is divided into roughly three part plus an introduction to the problem.",
                    "label": 0
                },
                {
                    "sent": "So first define what I mean by learning the kernel some related problems.",
                    "label": 0
                },
                {
                    "sent": "Then we'll discuss how do we learn?",
                    "label": 0
                },
                {
                    "sent": "Convex combination of continuous parameterized set of kernels, which it's an extension of the standard multiple kernel learning setup.",
                    "label": 0
                },
                {
                    "sent": "Then I will move on on a second problem which is.",
                    "label": 0
                },
                {
                    "sent": "Learn the kernel in multi task learning and there I think this is a nice application of.",
                    "label": 0
                },
                {
                    "sent": "Multiple kernel learning or learning the kernel.",
                    "label": 1
                },
                {
                    "sent": "I think it's a problem where you see that this stuff really works so you can see learner kernel multitask learning as a way to learn how the tasks are related.",
                    "label": 0
                },
                {
                    "sent": "And the last part representation, maybe it will be little bit short is.",
                    "label": 0
                },
                {
                    "sent": "A follow up on a paper that we had just now at NIPS an it is adding constraints in multiple kernel learning so status parcel.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the setup.",
                    "label": 0
                },
                {
                    "sent": "We so K is a positive semi definite kernel.",
                    "label": 1
                },
                {
                    "sent": "X is the input space.",
                    "label": 0
                },
                {
                    "sent": "Associated the current that is reproducing kernel Hilbert space, and this is the norm, so standard way of supervised learning is to learn from input output data for a function which minimizes these.",
                    "label": 0
                },
                {
                    "sent": "There is a functional of FK, the minimizations taken into the processing Kerbal space.",
                    "label": 0
                },
                {
                    "sent": "So this is a method which has been studied for several years now and everybody knows and includes several techniques actually support vector machines, regularize the squares, and so on many more.",
                    "label": 0
                },
                {
                    "sent": "So here I consider input output data, But this could also be used for.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised learning, for example anomaly detection or other problems.",
                    "label": 0
                },
                {
                    "sent": "So the loss function here is not really important.",
                    "label": 0
                },
                {
                    "sent": "There is this output, but.",
                    "label": 0
                },
                {
                    "sent": "I guess we're more familiar with this, so I use this notation.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how do we choose the kernel for a given task OK?",
                    "label": 1
                },
                {
                    "sent": "So one.",
                    "label": 0
                },
                {
                    "sent": "Approach is to look at this function of the kernel, so the result of this minimization problem as a criterion for selecting the kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so so the idea is very simple and.",
                    "label": 0
                },
                {
                    "sent": "We try to further minimize this quantity over.",
                    "label": 0
                },
                {
                    "sent": "The Colonel came.",
                    "label": 0
                },
                {
                    "sent": "We prescribe a set of candidate kernels, Script K and we started this minimization problem OK.",
                    "label": 1
                },
                {
                    "sent": "So by now there have also been a series of papers which have considered this problem.",
                    "label": 0
                },
                {
                    "sent": "And so here is just a very partial list, but many more.",
                    "label": 0
                },
                {
                    "sent": "So the first observation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me review few properties of this problem and then we move to the main part of the talk.",
                    "label": 0
                },
                {
                    "sent": "So the first observation is that our.",
                    "label": 0
                },
                {
                    "sent": "Function of the kernel is convex, but this is it.",
                    "label": 1
                },
                {
                    "sent": "Something that we like.",
                    "label": 0
                },
                {
                    "sent": "To see that.",
                    "label": 0
                },
                {
                    "sent": "So there is a simple proof which is given here.",
                    "label": 0
                },
                {
                    "sent": "So you first use the representative theorem and which states that the solution of the previous, let's say abstract optimization problem where there was dysfunction introducing Kerbal Space.",
                    "label": 1
                },
                {
                    "sent": "So the solution is a linear combination of the kernel center at the input data points.",
                    "label": 0
                },
                {
                    "sent": "Very old and simple and very useful result.",
                    "label": 0
                },
                {
                    "sent": "So when you substitute this formula into the objective function which was in the previous slide, you obtain a finite dimension optimization problem, in which this kernel.",
                    "label": 0
                },
                {
                    "sent": "This is the kernel matrix here K the North or the kernel function, but the kernel matrix or abuse of notation, but I think it's clear to everybody.",
                    "label": 0
                },
                {
                    "sent": "So when you substitute this formula in the objective function, you obtain a free dimension optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Then if you make this change of variable, so you called said the matrix K times the vector of coefficients E and let's assume for simplicity that K is invertible, then you obtain these equivalent problem and here you see immediately that this problem is jointly convex.",
                    "label": 1
                },
                {
                    "sent": "In these new variables at an in the kernel K. OK, so as a result when we minimize this overhead function remains convex.",
                    "label": 0
                },
                {
                    "sent": "And so this is.",
                    "label": 0
                },
                {
                    "sent": "A quick way to see that our criterion that we're going to use is is a convex file.",
                    "label": 0
                },
                {
                    "sent": "There isn't.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Observation that.",
                    "label": 0
                },
                {
                    "sent": "It is useful in gaining some intuition, some understanding.",
                    "label": 0
                },
                {
                    "sent": "About what's going on here.",
                    "label": 0
                },
                {
                    "sent": "Which I discuss in the case in which I have a safe night set of kind of kernels, and I want to.",
                    "label": 0
                },
                {
                    "sent": "Minimize my criterion or the convex combination.",
                    "label": 0
                },
                {
                    "sent": "Of these corners OK.",
                    "label": 0
                },
                {
                    "sent": "So in this case my problem.",
                    "label": 0
                },
                {
                    "sent": "So in this case basically here.",
                    "label": 0
                },
                {
                    "sent": "So there is a minimization over at the musician over K bye.",
                    "label": 1
                },
                {
                    "sent": "I changed it to minimization and you carry out the minimization over the kernel.",
                    "label": 0
                },
                {
                    "sent": "Then you obtain these equivalent problem.",
                    "label": 0
                },
                {
                    "sent": "Where song?",
                    "label": 0
                },
                {
                    "sent": "The problem of learning curve, which is at the scribe is equivalent to this problem, which has a more compact form where you minimize over as many functions as your your reproducing kernel Hilbert spaces.",
                    "label": 1
                },
                {
                    "sent": "And here so you have the sum of these functions and here you have the sum of the norms of each of these function in the corresponding Hilbert space.",
                    "label": 1
                },
                {
                    "sent": "And if have heart is a solution of this problem.",
                    "label": 0
                },
                {
                    "sent": "Then there.",
                    "label": 0
                },
                {
                    "sent": "The convex combination of the kernels where the parameters are defined by this equation is the optimal is an optimal kernel.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On my previous problem, OK.",
                    "label": 0
                },
                {
                    "sent": "So these are observation alpis gaining some intuition about what's going on here.",
                    "label": 0
                },
                {
                    "sent": "So we have we're trying to find the sparse combination of colors so sparsely play an important role here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The result follows essentially follows from an important property, which is in the paper of our own sign and then buy this property and then buy this variational form.",
                    "label": 1
                },
                {
                    "sent": "For this stuff I think is well known.",
                    "label": 0
                },
                {
                    "sent": "I will not discuss it in detail, and this can also be extended to also LP norms, and there have been some recent work about that, so we're here.",
                    "label": 1
                },
                {
                    "sent": "You have an LP norm as opposed to this is an L1 norm.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another special case which help us gaining some intuition about the program describing is the parametric case.",
                    "label": 0
                },
                {
                    "sent": "So the case in which these functions are linear and so here for simplicity, are just.",
                    "label": 0
                },
                {
                    "sent": "1 dimensional function.",
                    "label": 0
                },
                {
                    "sent": "So I take one component of XI multiplied by some weight vector beta J.",
                    "label": 0
                },
                {
                    "sent": "So in this case the problem the kernel is essentially the last, so the kernels are 1 dimensional and bit more general.",
                    "label": 0
                },
                {
                    "sent": "If the function FJ is our linear function on a subset of the variables.",
                    "label": 0
                },
                {
                    "sent": "So this sum is over asset GL and the collection of this set is a partition.",
                    "label": 1
                },
                {
                    "sent": "I have the group lasso problem.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "I have here this 21 norm, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a parametric.",
                    "label": 0
                },
                {
                    "sent": "Analog.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this of this phone.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we see that there is a close relation between the problem of learning the kernel which describe, and these sparsity methods.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What time do they start?",
                    "label": 0
                },
                {
                    "sent": "Forgot 40.",
                    "label": 0
                },
                {
                    "sent": "So this is so the first part is a word that we did actually along time ago.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have Andrea, Sergio and Charles Micheli.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Again, we so want to goal is to now to study this problem OK?",
                    "label": 0
                },
                {
                    "sent": "So we choose as our class or iPod is a class of corners.",
                    "label": 0
                },
                {
                    "sent": "A very general set which is a set of kernels which are obtained by taking.",
                    "label": 1
                },
                {
                    "sent": "These say.",
                    "label": 0
                },
                {
                    "sent": "With a closed convex Hall.",
                    "label": 1
                },
                {
                    "sent": "Of a set of basic kernels which are parameterized by this parameter, Omega.",
                    "label": 0
                },
                {
                    "sent": "OK, so Omega is in this set capital Omega, so this close convex solar is nothing else that is interactive presentation.",
                    "label": 0
                },
                {
                    "sent": "So I take a measure P. And I take the integral.",
                    "label": 0
                },
                {
                    "sent": "Of the basic respect to this measure, OK?",
                    "label": 0
                },
                {
                    "sent": "So what is an example of this so we could choose a G of Omega to be a Gaussian kernel, OK?",
                    "label": 0
                },
                {
                    "sent": "And then and then.",
                    "label": 0
                },
                {
                    "sent": "If so, so this is a class of radial kernels, and if the set capital Omega is the old positive line, by famous theorem by shamburg.",
                    "label": 1
                },
                {
                    "sent": "The kernel, which would include in this class are.",
                    "label": 0
                },
                {
                    "sent": "All radial corners, so their kernels which are.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Forget it.",
                    "label": 0
                },
                {
                    "sent": "So Curtis of the norms X -- T, which occurs in every dimension.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a very large, very large set OK?",
                    "label": 0
                },
                {
                    "sent": "I however not difficult to show that if if the class or basic kernels is uniformly bounded.",
                    "label": 0
                },
                {
                    "sent": "OK, according to this equation.",
                    "label": 0
                },
                {
                    "sent": "Still, it makes sense to study our problem.",
                    "label": 0
                },
                {
                    "sent": "It makes sense to minimize the function of K over this class.",
                    "label": 0
                },
                {
                    "sent": "Becaused you can show that it is.",
                    "label": 0
                },
                {
                    "sent": "It is a lower bound on the value of this objective function can take and cannot be equal to 0, which means that you cannot fit exactly the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So I will discuss some some thoughts about that.",
                    "label": 0
                },
                {
                    "sent": "You could choose many other cases of of basic kernels.",
                    "label": 0
                },
                {
                    "sent": "In particular, you could choose.",
                    "label": 1
                },
                {
                    "sent": "Set of curvature.",
                    "label": 0
                },
                {
                    "sent": "I mentioned parameterization OK. And of course, if you take this set to be finite set, you have standard multiple kernel learning OK.",
                    "label": 0
                },
                {
                    "sent": "The connection to this so there is also this rated work which does.",
                    "label": 0
                },
                {
                    "sent": "However, these people and and also this connection to this mix normalization also applies in this case, but it's somewhat more complicated, so it is describing this paper will not will not described here.",
                    "label": 0
                },
                {
                    "sent": "So now I will.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To.",
                    "label": 0
                },
                {
                    "sent": "Understand it more in detail.",
                    "label": 0
                },
                {
                    "sent": "What is the problem we have to solve?",
                    "label": 0
                },
                {
                    "sent": "So our problem again is to minimize this function over K generic element of the class which I defined above.",
                    "label": 0
                },
                {
                    "sent": "Now by using function duality you can rewrite the problem minimizing this function over K here as this minimax problem OK. Where so you maximize over K. So now is the Max because there is this minus maximize over K, the minimum over C of this with this function.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, this function is convex in C for fixed, K is concave in K for fixing, and you can show that you can interchange the minimum and maximum, so there's a saddle point for this problem and these are the conditions for optimality so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See how the cat is a solution of this problem.",
                    "label": 1
                },
                {
                    "sent": "So in case that I write as.",
                    "label": 0
                },
                {
                    "sent": "They integrate with respect to the associated measure we had OK.",
                    "label": 1
                },
                {
                    "sent": "So she had the cat is a solution of the problem if it only if these two conditions are verified.",
                    "label": 0
                },
                {
                    "sent": "So when I look at this function of Omega.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "And I take Omega to be in the support of the measure P at this function attains its maximum OK. And also see that is a solution of the population problem.",
                    "label": 0
                },
                {
                    "sent": "Fixed gun.",
                    "label": 0
                },
                {
                    "sent": "OK, these are necessary sufficient condition for optimality and also what is interesting is that the solution cat is not unique in general, but you can always show that there is a solution which involves at most N plus one kernels.",
                    "label": 1
                },
                {
                    "sent": "So in other words this measure is a finite measure with at most N plus one atoms.",
                    "label": 0
                },
                {
                    "sent": "It is a useful result cause it motivates now an algorithm for.",
                    "label": 0
                },
                {
                    "sent": "For finding Saddle point problem but other point solution.",
                    "label": 0
                },
                {
                    "sent": "But here is the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you start from this corner in the class.",
                    "label": 0
                },
                {
                    "sent": "For example, you take Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "If you would take on this combination of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "You solve the regularization problem, so this could be a little square of an SVN.",
                    "label": 0
                },
                {
                    "sent": "And then you try to find.",
                    "label": 0
                },
                {
                    "sent": "A new kernel which maximizes our this function right?",
                    "label": 0
                },
                {
                    "sent": "And then you take.",
                    "label": 0
                },
                {
                    "sent": "By line search you'll find the convex combination of the current Colonel.",
                    "label": 0
                },
                {
                    "sent": "So Katie and Geo, Maga hat, and then you repeat this OK.",
                    "label": 0
                },
                {
                    "sent": "So every time you add 1 Colonel in the common combination.",
                    "label": 0
                },
                {
                    "sent": "And so there is a result by Andreas, which shows that there exists a limit point of this algorithm and end limit point is a solution of of the optimization problem.",
                    "label": 1
                },
                {
                    "sent": "OK, typically what happens is that two things.",
                    "label": 0
                },
                {
                    "sent": "The algorithm in a different number of steps becausw, so something which I'm hiding here is that when you maximize this function, you must ensure that this is maximum is greater than the value of this function instead of G Omega, you put the current kernel OK.",
                    "label": 0
                },
                {
                    "sent": "If that is not possible, you can stop.",
                    "label": 0
                },
                {
                    "sent": "You have an optimal solution.",
                    "label": 0
                },
                {
                    "sent": "And 2nd in this step, when you take a convex combination of the current kernel with the.",
                    "label": 0
                },
                {
                    "sent": "You can get one off and you select the new one.",
                    "label": 0
                },
                {
                    "sent": "So typically have a very small common combination colors.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we haven't done massive experiments with algate, and so in fact the work which I described in here is quite not very recent, so this is what we published 40 years ago.",
                    "label": 0
                },
                {
                    "sent": "So here, just.",
                    "label": 0
                },
                {
                    "sent": "A fairly simple experiment on the M list data where we try to compare this algorithm with multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "Where are we consider Gaussian kernels?",
                    "label": 0
                },
                {
                    "sent": "We took parameters, so we divided our image in the upper part and the lower part and we consider a Gaussian for each of the two parts.",
                    "label": 0
                },
                {
                    "sent": "So we have only two parameters and with our method we explore continuously the range of these parameters and we compare with multiple kernel learning where we consider a grid of parameters.",
                    "label": 1
                },
                {
                    "sent": "So five for the.",
                    "label": 0
                },
                {
                    "sent": "Sorry it was left and right.",
                    "label": 1
                },
                {
                    "sent": "Have parts of the image not happen right?",
                    "label": 0
                },
                {
                    "sent": "So we took five Gaussian for the left for the right, so we have 25 possible kernels, right?",
                    "label": 0
                },
                {
                    "sent": "Cause?",
                    "label": 0
                },
                {
                    "sent": "Each of the left you can multiply by one in the right.",
                    "label": 0
                },
                {
                    "sent": "And these are the results that we got.",
                    "label": 0
                },
                {
                    "sent": "So the main conclusion was that.",
                    "label": 0
                },
                {
                    "sent": "So our method is not is robust to the range of the parameters, whereas the multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "So if you start some bad corners it.",
                    "label": 0
                },
                {
                    "sent": "It doesn't find a good solution, so that was the main conclusion.",
                    "label": 0
                },
                {
                    "sent": "Another conclusion was that although the method.",
                    "label": 1
                },
                {
                    "sent": "Improves over multiple kernel learning and this robust over the parameter range.",
                    "label": 0
                },
                {
                    "sent": "This comes with the price and the price is that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This second step in the method is a is a possibly hard.",
                    "label": 0
                },
                {
                    "sent": "Optimization problem, but it's not convex.",
                    "label": 0
                },
                {
                    "sent": "And so we solved this programming.",
                    "label": 0
                },
                {
                    "sent": "You can write down this difference of convex functions when the parameter space is high dimensional.",
                    "label": 0
                },
                {
                    "sent": "So in this case what I mentioned was OK, but when it is high dimensional we it is difficult to solve this problem basically OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How do you have any questions so far?",
                    "label": 0
                },
                {
                    "sent": "So I moved to the next.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The nice little part of the talk.",
                    "label": 1
                },
                {
                    "sent": "So here are some ideas about learning multitask learning.",
                    "label": 0
                },
                {
                    "sent": "So this is different topic.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I apply exactly the same recipe that I've described before to the case in which I have multiple functions, so multiple tasks.",
                    "label": 0
                },
                {
                    "sent": "So I think of my input space or my inputs X as a concatenation of two variables.",
                    "label": 0
                },
                {
                    "sent": "So zed and T, where T is an index which the notes which of the task I'm considering.",
                    "label": 0
                },
                {
                    "sent": "OK, so this functions F of ZNT is the value of the T task at the inputs that I impose that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This is an occasion to everybody.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just a way to say that I have many tasks which are indexed by this.",
                    "label": 0
                },
                {
                    "sent": "At tea, sometimes you can put it here, but I just.",
                    "label": 0
                },
                {
                    "sent": "Like this one, right?",
                    "label": 0
                },
                {
                    "sent": "Like this when farsighted it is exactly the same problem that this guy before by this choice of the input.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "Are you?",
                    "label": 0
                },
                {
                    "sent": "If you choose a kernel which for this for the function on this space, so the kernel modest relationship between the task as I will now describe by some examples.",
                    "label": 0
                },
                {
                    "sent": "So in this case the problem of choosing this kernel can be seen as as the problem of learning all the tasks are related within a model of course.",
                    "label": 0
                },
                {
                    "sent": "And the example which I want to describe, which is a refined quite stacked if it's the case, is the simplest case in which these functions are linear.",
                    "label": 0
                },
                {
                    "sent": "So for each T the function is linear instead.",
                    "label": 0
                },
                {
                    "sent": "So in this case, if you think about it, the most general linear function that we can have is a function which has this parameterization.",
                    "label": 0
                },
                {
                    "sent": "So V is a vector of parameters.",
                    "label": 0
                },
                {
                    "sent": "And this is your feature vector so that there is a matrix B which depends on the task T and it is applied to the input set OK.",
                    "label": 0
                },
                {
                    "sent": "So in this case the kernel becomes this function.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Please buy linear instead and that prime and then there are these methods which are task task specific.",
                    "label": 0
                },
                {
                    "sent": "So the problem is to minimize my criterion over a kernel K which is of this type.",
                    "label": 0
                },
                {
                    "sent": "For this method is better in some subset, OK?",
                    "label": 0
                },
                {
                    "sent": "So let's see an example very simple exam.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I take on this combination of only two.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Kernels to multitasker.",
                    "label": 0
                },
                {
                    "sent": "So these are also called multitask earners, because there is a certainty.",
                    "label": 0
                },
                {
                    "sent": "So the first one is for the linear instead and is linear Currin variables at and.",
                    "label": 0
                },
                {
                    "sent": "Then there is this correct?",
                    "label": 0
                },
                {
                    "sent": "Are Delta here interpretation?",
                    "label": 0
                },
                {
                    "sent": "This is a kernel which is a specific, so the tasks are independent.",
                    "label": 0
                },
                {
                    "sent": "If you learn independent if if you were just using this kernel II.",
                    "label": 0
                },
                {
                    "sent": "Current doesn't contain TNT prime, so if you just use this kernel, you treat all the task as the same task.",
                    "label": 0
                },
                {
                    "sent": "These matrices BT which appear.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before OK. For this, in this case they are orthogonal to each other.",
                    "label": 0
                },
                {
                    "sent": "In this case they are all the same matrix.",
                    "label": 0
                },
                {
                    "sent": "So in fact, if you use this kernel, you can show that the problem realization problem with the minimize over function in the space of this kernel is it is the same as this problem.",
                    "label": 0
                },
                {
                    "sent": "But here you introduce a parameter for each task is WT is a parameter for each of the task.",
                    "label": 0
                },
                {
                    "sent": "And you have this regularizer which says that you have this parameter Lambda in the convex combination and the regularizer says that.",
                    "label": 0
                },
                {
                    "sent": "You try to make the task all close to an average vector OK. And then there.",
                    "label": 0
                },
                {
                    "sent": "The closer Lambda is to zero, the closer is the task will be to the average.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the limited Lambda going to 0, all the task would be the same as something which is confirm.",
                    "label": 0
                },
                {
                    "sent": "If you look at this kernel and is equal to 0, this kernel is not there, so you all have the same task OK.",
                    "label": 0
                },
                {
                    "sent": "So of course, in this case doesn't make much sense to.",
                    "label": 0
                },
                {
                    "sent": "You can try to minimize over Lambda, but in this case we can just do cross validation with just one parameter.",
                    "label": 0
                },
                {
                    "sent": "But you can imagine if you have many different multitask earners than you could try to do to combine the different kernels by multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "And there was some real extension, of course.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me move to another example, which is more interesting because here we see that we don't take on this combination of orphan Atom any kernel, so we get back to the previous example that I described.",
                    "label": 0
                },
                {
                    "sent": "We take this class of corners.",
                    "label": 0
                },
                {
                    "sent": "So there is a linear kernel which depends on the matrix D, which is in some set.",
                    "label": 1
                },
                {
                    "sent": "So here I take the to be positive semi definite abounded trace.",
                    "label": 0
                },
                {
                    "sent": "But I could take a smaller set.",
                    "label": 0
                },
                {
                    "sent": "And then I have again the connector.",
                    "label": 0
                },
                {
                    "sent": "Colonel on the task indexes.",
                    "label": 0
                },
                {
                    "sent": "So here the interpretation is that by minimizing my criterion over kernels in this class, I try to find a common kernels across the task.",
                    "label": 0
                },
                {
                    "sent": "OK, so once the is found, the tasks are learned one by one independently, but they all use the same kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, some linear kernel.",
                    "label": 0
                },
                {
                    "sent": "And you can also have a different application of learning the kernel within this class.",
                    "label": 0
                },
                {
                    "sent": "The first one is that you are trying to find few common orthogonal features shared by the task, and so this matrix you will just be the the message from the Reagan vectors of the Majesty.",
                    "label": 1
                },
                {
                    "sent": "Or equivalently, you can show that this is nothing else.",
                    "label": 0
                },
                {
                    "sent": "The trace norm regularization OK, in which task vectors are regularised by using the trace norm which encourage low rank matrices?",
                    "label": 0
                },
                {
                    "sent": "And you could think of other examples in which you put further constraints on this matrix.",
                    "label": 0
                },
                {
                    "sent": "Or you also try to model some.",
                    "label": 0
                },
                {
                    "sent": "Cluster of task.",
                    "label": 0
                },
                {
                    "sent": "So all these all these example which I described here could be implemented by choosing a class of kernels and going.",
                    "label": 0
                },
                {
                    "sent": "Minimizing errors within this class OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think multitask learning is an example where learn the Colonel was very well and just to mention one simple example which is also.",
                    "label": 0
                },
                {
                    "sent": "A few years ago so if you try.",
                    "label": 0
                },
                {
                    "sent": "Some datasets.",
                    "label": 0
                },
                {
                    "sent": "Give us more than we.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I described it works very well, so this mode.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here in these computers are conservative.",
                    "label": 0
                },
                {
                    "sent": "I said we find the Matrix deal which has one leading a convector.",
                    "label": 0
                },
                {
                    "sent": "So it means that the task are all approximated by 1 features and as you can see.",
                    "label": 0
                },
                {
                    "sent": "By putting the task together you really improve performance, so I think so the main message from this discussion is that there are some.",
                    "label": 0
                },
                {
                    "sent": "Simple classes of kernels which work very well multitask learning.",
                    "label": 0
                },
                {
                    "sent": "However, there are other classes which could be tried which I haven't tried myself, but I think there are some talks later on which maybe speak about this, and I think this could be a promising direction for further work on learning kernel multitask learning.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the last part of the talk, and I think I have.",
                    "label": 1
                },
                {
                    "sent": "5 minutes maybe or.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the last part is.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He just might say at recycle from the nips paper visia so.",
                    "label": 0
                },
                {
                    "sent": "It will be extremely fast.",
                    "label": 0
                },
                {
                    "sent": "In fact, we don't OK.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is so I discuss this review briefly what the kernel is.",
                    "label": 0
                },
                {
                    "sent": "Then I discuss general proposal so we minimize our criteria within.",
                    "label": 0
                },
                {
                    "sent": "At hand.",
                    "label": 0
                },
                {
                    "sent": "Say close Contacts all over of the continuous class of kernels.",
                    "label": 0
                },
                {
                    "sent": "Then I discuss learning the current multitask learning.",
                    "label": 0
                },
                {
                    "sent": "The kernel class is also fairly large, so here I take the opposite direction.",
                    "label": 0
                },
                {
                    "sent": "I taken a very much constrained class, so I choose all your flight number of kernels and I take on this combination.",
                    "label": 0
                },
                {
                    "sent": "But I add additional constraints on the parameters in the convex combination.",
                    "label": 0
                },
                {
                    "sent": "So this set capital Lambda is a convex set.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "So when when it is a positive part and this is essentially multiple kernel learning, but when it is a subset of the positive orthant, we have restricted class of chorus.",
                    "label": 0
                },
                {
                    "sent": "So again, in this case, the problem of this problem is equivalent to another regularization problem with some.",
                    "label": 0
                },
                {
                    "sent": "Regularizer which depends on each of the functions in the kernel class and for the regularizer is defined by this formula OK.",
                    "label": 0
                },
                {
                    "sent": "So Omega over vector, which here I called beta.",
                    "label": 0
                },
                {
                    "sent": "Given Lambda, so you forgot that I had given, Lambda is the is.",
                    "label": 0
                },
                {
                    "sent": "The is the feeling with this family or quadratics over this parameter Lambda?",
                    "label": 0
                },
                {
                    "sent": "And then she was taken over Lambda in the sector land, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is the penalty function?",
                    "label": 0
                },
                {
                    "sent": "When does that capital Lambda is the?",
                    "label": 0
                },
                {
                    "sent": "Is the positive order and this is just the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "So in that case I have standard.",
                    "label": 0
                },
                {
                    "sent": "Multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "Goal is to consider the distinctive sets and to see that the choice of this set as a means to enforce some kind of structure sparsity.",
                    "label": 0
                },
                {
                    "sent": "Across the coefficients in this combination OK?",
                    "label": 0
                },
                {
                    "sent": "So here I'm just going to describe some examples in the in the parametric case for the case in which.",
                    "label": 0
                },
                {
                    "sent": "When Lambda is a positive part and this is the L1 normal, this vector parameters, I'm just doing the last method lesu LA, so always forget.",
                    "label": 0
                },
                {
                    "sent": "But for example, if I choose ordering constraints of this type Lambda, one is ready for the Lambda 2.",
                    "label": 0
                },
                {
                    "sent": "My penalty function is not the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "But it is a combination of the L1 norm of the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "Which is given here and here is the level set.",
                    "label": 0
                },
                {
                    "sent": "So in here the intuition is that.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These constraints, favors favors solutions which are in the set.",
                    "label": 0
                },
                {
                    "sent": "OK, so because of this property, so the penalty function is always greater than the L1 norm of the coefficients.",
                    "label": 0
                },
                {
                    "sent": "OK, and the quality holds if it only if the vector form by the absolute values of this coefficient is in the set OK.",
                    "label": 0
                },
                {
                    "sent": "So going back to the previous example, if.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If absolute value of beta one is, get the absolute value of beta two.",
                    "label": 0
                },
                {
                    "sent": "I have that one or otherwise I have something which is it's bigger than that one.",
                    "label": 0
                },
                {
                    "sent": "So in this case I penalize more.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with this recipe you can generate different penalty functions which are convex, by the way, for the same reason.",
                    "label": 0
                },
                {
                    "sent": "As the beginning of the talk, because this function is jointly convex, if I minimize over Lambda remains convincing beta.",
                    "label": 0
                },
                {
                    "sent": "And you can also show that this is a norm when the set capital.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lasercorn um?",
                    "label": 0
                },
                {
                    "sent": "So you can choose different sets of Lambda and understand that these favors some kind of structure sparsity, for example.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, let's keep this one, but for example, you could have a director secret graph and you could say that.",
                    "label": 0
                },
                {
                    "sent": "When a note points from when there is a link from another node, the variable in the first node is greater than the value of the second note.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this favors some kind of ordering relationship between.",
                    "label": 0
                },
                {
                    "sent": "At the variables, so you try to favors vector beta, which in absolute value satisfied is ordering relationship.",
                    "label": 0
                },
                {
                    "sent": "So bitter one absolute value should be encouraged to be greater or equal than both beta two and between absolute value.",
                    "label": 0
                },
                {
                    "sent": "Without",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can have other examples.",
                    "label": 0
                },
                {
                    "sent": "Offset the capital lambdas which favors higher order structure sparsity.",
                    "label": 0
                },
                {
                    "sent": "Which I described here, but I don't have time to.",
                    "label": 0
                },
                {
                    "sent": "To go into details, but this is the case for the difference operator, and yet the intuition is that if you have a vector, we satisfy this constraint.",
                    "label": 1
                },
                {
                    "sent": "These vectors will be sparse only within contiguous.",
                    "label": 1
                },
                {
                    "sent": "If you continue this region, so the sparsity pattern will be will consist of contiguous regions.",
                    "label": 0
                },
                {
                    "sent": "So we have done some.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Palamari simulations in which we try to use this penalty in a linear model with.",
                    "label": 0
                },
                {
                    "sent": "Squared loss function regularize the squares and we have obtained.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some encourageing results so.",
                    "label": 0
                },
                {
                    "sent": "One conclusion is that our method is is more general in the group Lasso course group lasso, and it is more robust, although in certain cases does not work as well as a group lasso.",
                    "label": 0
                },
                {
                    "sent": "This is an example in which the group lasso works better.",
                    "label": 0
                },
                {
                    "sent": "This is our model.",
                    "label": 0
                },
                {
                    "sent": "And this is a group lost with rocky overlapping groups along the line.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in this case, if we have this bump here, so this is this, the vector of absolute value of the progression of the absolute value of the regression coefficient.",
                    "label": 0
                },
                {
                    "sent": "Our method works better than the group lasso, which is this one.",
                    "label": 0
                },
                {
                    "sent": "Here our method is this one right?",
                    "label": 0
                },
                {
                    "sent": "And if we use these higher order constraints, we do even even even.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have another number of results in which.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We show that.",
                    "label": 0
                },
                {
                    "sent": "Some other kind of target vectors.",
                    "label": 0
                },
                {
                    "sent": "Be sure.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we will.",
                    "label": 0
                },
                {
                    "sent": "We improve over the last method and then.",
                    "label": 0
                },
                {
                    "sent": "And that.",
                    "label": 0
                },
                {
                    "sent": "The panel through the right with the with the.",
                    "label": 0
                },
                {
                    "sent": "Correct number of contiguous region is the one which works the best.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are discussed more in detail in the in the newspaper, so I think I don't have time to say more.",
                    "label": 0
                },
                {
                    "sent": "Let me just conclude the presentation.",
                    "label": 0
                },
                {
                    "sent": "So I discussed the main observations, so considering continuous kernel classes may prove advantageous over.",
                    "label": 1
                },
                {
                    "sent": "Finite combination kurnass weather.",
                    "label": 0
                },
                {
                    "sent": "Men drawback is that this requires solving adaptation problem.",
                    "label": 1
                },
                {
                    "sent": "Then I discussed learn the current multitask learning and as a means to learn it, ask relatedness.",
                    "label": 0
                },
                {
                    "sent": "I discussed some simple class of multitaskers.",
                    "label": 1
                },
                {
                    "sent": "But so here are the main messages that are richer class of kernel which will which I think could provide improved results.",
                    "label": 0
                },
                {
                    "sent": "And finally I I comment on a new family of penalty function for structured sparsity, which I described only in the case of linear models.",
                    "label": 0
                },
                {
                    "sent": "But this could be.",
                    "label": 0
                },
                {
                    "sent": "Apply the same straightforward manner to the linear case, which will give a constrained version.",
                    "label": 0
                },
                {
                    "sent": "Multiple kernel learning Zen in the linear case, and this appears to be.",
                    "label": 0
                },
                {
                    "sent": "To work better than the previous methods, so thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}