{
    "id": "pba5p2qa6zcwyyoczyibx7wfrblkyn4v",
    "title": "Drifting Games, Boosting and Online Learning",
    "info": {
        "author": [
            "Yoav Freund, Department of Computer Science and Engineering, UC San Diego"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_freund_dgb/",
    "segmentation": [
        [
            "Thank you for this very nice introduction.",
            "My wife would really like to learn that I'm efficient.",
            "Dude is there?",
            "Audio there good.",
            "OK so yes.",
            "So the talk.",
            "I change it on Friday or I change it last week because I realized I'm just trying to talk about too many things at once.",
            "So I reduced the part that is on online learning.",
            "I remove that essentially and I'll only give you some pointers at the end."
        ],
        [
            "OK, So what is this talk about?",
            "This talk about is essentially about a new improvement to Adaboost, so Adaboost has become very popular, but even as its popular, many people have pointed out its problems and the main problem.",
            "I want to focus on in this talk is that it is very sensitive to label noise.",
            "OK, so many people notice it before.",
            "Here is an example.",
            "We take the letter database from.",
            "The Irvine repository and we make the problem binary just so that we can focus on one problem by looking at FIJ versus the other letters.",
            "And then if we run other boost.",
            "On decision trees, I think then we get very good performance if we run logic boost we get.",
            "Almost the same performance, however, both algorithms fail miserably when we add 20% label noise.",
            "OK, so Tom Dietrich has pointed it out long time ago that Adaboost is very sensitive to that.",
            "An logic boost is somewhat less sensitive.",
            "You see that it's a little bit better, but not that much better.",
            "OK, So what is the?"
        ],
        [
            "The problem is that boosting puts way too much weight on outliers.",
            "OK, so if examples you get on them the incorrect prediction because they are simply incorrect examples, their label is incorrect, then you tend to with time put on it way too much weight and really what you'd want to do is."
        ],
        [
            "Want to give up on them, right?",
            "So what are outliers?",
            "Outliers are exactly those examples that we want to say.",
            "Don't show me those examples that are just confusing me, OK?",
            "So here is an algorithm."
        ],
        [
            "And that here is just a summary of the algorithms performance.",
            "This is algorithm called robust boost and what you see is that it performs slightly worse.",
            "On the noiseless case, and that's essentially because of some numerical issues.",
            "But then when you add 20% noise, it performs much, much better than the previous algorithms.",
            "You can clearly see it here, but you can see it even more if you.",
            "In this case we added the 20% noise ourselves so we could look what is the performance according to the original."
        ],
        [
            "Before we add the noise."
        ],
        [
            "And that performance is very good.",
            "So if you look at other booster logic boost, the error is essentially the noise level, while if you look at robust boost, it essentially gets mistakes that are much much lower than the noise level.",
            "So it essentially you can say correct most of the mistaken labels.",
            "Which statistically doesn't seem like a problem, but if you work on these kind of problems, you know that computationally it's a very hard thing to identify those examples in which you have the incorrect label.",
            "OK."
        ],
        [
            "So here is the plan of the talk, so I'm going to basically show you how how I do it.",
            "Here's the plan.",
            "First of all, I'm going to tell you that it's been pretty much identified.",
            "What's the problem with label noise and it's associated with the convexity of loss functions?",
            "Then I'm going to tell you about an old algorithm boost by majority and show you how it's analyzed using a concept of drifting games.",
            "And then I'm going to tell you, and that is actually a very old algorithm.",
            "It's before Adaboost, it's 95, while Adaboost is 97.",
            "And then I'm going to show you what I've been working on in the last 10 years, which is essentially moving this algorithm into the continuous time.",
            "And why do I do that?",
            "And then finally, I'm going to go back to the experiments and show you how when you look at things the right way, the results actually are not that surprising.",
            "OK, so starting with label."
        ],
        [
            "Noise and convex loss functions.",
            "So using loss convex loss function is a very very common paradigm."
        ],
        [
            "In all of machine learning, if you look at Perceptron, Adaboost, logic, Boustan soft margin SVM, they all look use some kind of convex loss function.",
            "OK, so that that."
        ],
        [
            "Much is clear.",
            "And they work.",
            "All of these algorithms work very well when the data is linearly separable, OK, and when the data is linearly separable.",
            "We know really that computationally there's no big issue."
        ],
        [
            "However, all of them can get into bad trouble when the data is not linearly separable."
        ],
        [
            "And so really, what is the problem?",
            "The problem is that convex loss functions are a very poor approximation for the classification error.",
            "That is what we really want to minimize when, especially when you're on the incorrect side.",
            "So what I mean by that?"
        ],
        [
            "Um?",
            "I'll show you in a minute, but the other side of it, so everybody knows that you'd really want to minimize the.",
            "The classification error, but the classification error is not a convex loss functions and once you get into minimizing something that is not convex, basically our tendency is to just throw up our arms and say we can do it.",
            "So I'm going to show you that in some sense we can do it."
        ],
        [
            "OK so here is what I mean.",
            "Kind of more specifically this is the margin.",
            "So what I mean by the margin is simply what is the?",
            "What is the sum of the base classifiers before we threshold it?",
            "OK, so times the correct label, so if you're on if the score is on this side, then based if the margin is on this side, you're correct.",
            "And if you're on this side, you're incorrect, so therefore the classification loss here is one OK, and this is really the function that we want to minimize.",
            "However, because it's not convex, what we minimizes some upper bound on it, so this is the.",
            "Upper bound this linear function is upper bounds that are used by SVM.",
            "This is not quite an upper bound, but this is used by perceptron and other boost uses this exponential wave function and logic boost use the function that is exponential on this side, but becomes linear on this side.",
            "OK, so they're all convex and because of that they have a global minimum and we know how to minimize them.",
            "But that's also the problem.",
            "The problem is if we have 20% label noise, then these examples necessarily need to be here and we don't want to really pay attention to them.",
            "OK, so that's that's our problem.",
            "The microphone seems to me unstable.",
            "Everybody can hear me well.",
            "OK. Alright, so here."
        ],
        [
            "Is an example that basically not an example the last year in acnl, Phil Long and raucous or video showed a very nice result.",
            "They basically showed that the problem that I kind of intuitively says with convex loss functions is actually an inherent problem.",
            "You can't get rid of it as long as you're using convex loss functions.",
            "OK, so here we have data.",
            "This is a particular distribution that they suggested that is linearly separable.",
            "OK, so this line separates the blue points from the red point.",
            "And we have a little bit more points here, and so this this is not a problem.",
            "Now, what happens if we add label noise to it?",
            "Let's say 10% label noise."
        ],
        [
            "So we now have in each one of these points we have the majority, as it was labeled before.",
            "But there is a 10% minority that is labeled the opposite way.",
            "OK, so in fact, in terms of what is the optimal thing, it's still the same thing.",
            "This classifier is still the best classifier you can have for this data.",
            "However, if you use convex loss function, something really strange will happen.",
            "What will happen is that essentially these large margin examples, the mistakes, the noise.",
            "There is very, very penalized very highly for it.",
            "Why are you penalized very highly for it?",
            "It's very far from the boundary, right?",
            "The further you are from the boundary, the bigger the loss function, right?",
            "So you're penalized very highly for those, and so the hyperplane doesn't want to be so far it will try to get close to those.",
            "OK, there are many ways for it to get close.",
            "So what we need is somehow to break the symmetry, and that's what the pullers are doing.",
            "The pullers will pull the hyperplane in a particular direction.",
            "And what will happen is that the penalizes these examples that are right here in the middle.",
            "Those will get classified incorrectly.",
            "OK, why would they get classified incorrectly?",
            "Because they are so close to the boundary that however you turn the hyperplane, their loss on them is not going to be very large.",
            "OK, So what will happen is that you'll get a classifier that is like this, or maybe going through them and then your error rate in terms of a classifier is very high.",
            "And what they proved in a theorem is that for any convex loss function there exists a Lin."
        ],
        [
            "Really separable distribution."
        ],
        [
            "Such that when independent label noise is added, the linear classifier that minimizes the loss function has very high classification error, so it doesn't really matter which convex function you give me.",
            "Then I can always generate the distribution on which the thing that finds the global minimum seems to be very good is actually going to perform terribly.",
            "So when real problem we're in real trouble.",
            "OK, So what I'm going to basically tell you is how we can essentially address this problem and we can minimize in this particular case, under some assumptions, whose characterization is a little bit of a problem yet, but we can actually minimize the non convex loss function.",
            "OK, so this is this old algorithm called Boost by majority."
        ],
        [
            "And."
        ],
        [
            "The way that I think about this algorithm, I thought about it when I designed it is that you think about it as a game between a booster and a weak learner.",
            "OK, so it's an interactive game and the."
        ],
        [
            "Rule that you're generating at the end is a very simple rule.",
            "It's going to be a majority vote, just like in other boots, but it's going to be an unweighted majority vote, so all of the rules are going to get weight 1.",
            "OK. And."
        ],
        [
            "The most important thing is that the number of iterations of boosting is going to be set up ahead of time.",
            "So I'm going to decide I'm going to run this algorithm for 100 iterations, and that's it.",
            "Then I'm going to stop and take the majority rule, and so that when you have a finite horizon that starts to give you a chance to talk about giving up, right?",
            "If you don't.",
            "If you're never going to stop, there's no reason to ever give up, but if you're getting close to the end and some examples, you're doing very badly, then, well, maybe you should give up on them, and that comes out."
        ],
        [
            "OK so here is how officially how formally the game goes on."
        ],
        [
            "Each iteration, the booster assigns weights to the training examples, just like in regular boosting algorithms."
        ],
        [
            "And then the learner chooses a rule whose error with respect to the chosen weights is smaller than half minus gamma.",
            "So I'm giving you these weights.",
            "Now you need to give me a rule, right?",
            "That's the usual way that you think about it.",
            "Important thing is gamma is also fixed ahead of time."
        ],
        [
            "OK, and the rule then is added to the majority vote and you repeat that for tea."
        ],
        [
            "Times.",
            "OK, now the goal of the booster is to minimize the number of errors of the final majority rule on the training set.",
            "OK, so we're not talking here about how well it will perform on a test set, just on the training set."
        ],
        [
            "OK, so this is the first case of what I call a drifting game.",
            "And here is kind of how you think about it as such.",
            "You think about each."
        ],
        [
            "Sample is being a chip.",
            "OK, so each example is like one of these chips and each bin hold."
        ],
        [
            "All of those examples on which the difference between the number of correct and incorrect base rules is at that point.",
            "OK, so it's essentially S is the margin of the rule.",
            "OK, now here's a small detail that initially seems very insignificant but becomes important in the analysis."
        ],
        [
            "Yes.",
            "It's it's useful to think about having huge number of chips.",
            "OK, so one way of thinking about it."
        ],
        [
            "Is to just let the number of training examples go to Infinity?",
            "That's one way of thinking about it.",
            "A more useful way is actually to think about."
        ],
        [
            "The examples is having continuous measure.",
            "OK, so you just have a mass of examples of course.",
            "Doesn't really quite make sense, but let's just go with it for now.",
            "And that's the way that I'm kind of denoting it.",
            "So instead of chips, I just have this mass and the thing that the mass gives me is basically it's very easy in this case to characterize what is the worst case.",
            "What is the worst thing that the adversary that is the weak learner can do to me?",
            "OK, in the other cases, it's not quite tight.",
            "OK, So what we have is this."
        ],
        [
            "Game and this is what I call the lattice on which the game is played.",
            "OK, so all of the examples start at zero and after the first rule is added.",
            "Some examples are correct, some are incorrect.",
            "OK, then after the second rule you have a vote of two or zero or minus two and so on."
        ],
        [
            "And what I'll decide is I'm going to have T this number of iterations be an odd number.",
            "OK, because if it's even then we have this special case where at the end we have the same number of votes going correctly or one and 4 -- 1.",
            "And then we have to break that.",
            "I somehow so odd is much more convenient."
        ],
        [
            "OK, and here is how the game goes.",
            "OK, so this is really kind of the meat of it.",
            "What you have here is all of the examples initially are at zero because we haven't had any rules and now we have the boot."
        ],
        [
            "After assigning weight to the examples, essentially a density, now all of the examples are kind of the same, so really doesn't matter on this iteration.",
            "How you assign density?",
            "Let's just say that it assigns assigns weight of 1 to all of these examples."
        ],
        [
            "Now the weak learner has to choose which examples it's going to classify correctly, in which examples not correctly, and we set gamma to 0.1, so it has to choose essentially at least zero point.",
            "6 of these examples and predicting them correctly.",
            "OK, so that's the dark blue set and what's going to happen then?",
            "Is that these examples are going to move OK, so these are the correct ones.",
            "They move up the in."
        ],
        [
            "Correct, they moved out.",
            "OK, now the situation is more interesting because now the booster has something to play."
        ],
        [
            "It has two types of examples, so it's going to give essentially these examples, some weight and these examples some weight.",
            "Intuitively, these are the examples that the first one got incorrect, so it wants to give them more weight.",
            "OK, and indeed it will turn out that this is the optimal weights for this particular three step game.",
            "OK, so now we have more weights on these examples, less weights on these, and so now the."
        ],
        [
            "Weak learner again has some freedom right?",
            "It it it can put more mistakes here and less mistakes here because the mistakes here don't cost it that much.",
            "Right they cost less in terms of the way to there.",
            "OK, so let's say divides it like this and now the."
        ],
        [
            "Apples move another step, like here and now.",
            "We're just before the last rule, so that's an interesting step to look at, because now."
        ],
        [
            "If we look at the booster, these are obviously the weights that it's going to assign.",
            "So because these examples that you have two votes correct, they're going to stay correct at the end no matter what, how their predicted.",
            "So it's going to give them weight 0 because it doesn't really care, but also these examples on which both are incorrect.",
            "It's going to assign them wait zero because it gave up on them, right?",
            "They can't become correct all of a sudden.",
            "So it's kind of very interesting.",
            "It's the first time we see something that is very different from Adaboost.",
            "OK, and this is the optimal weight.",
            "You put all of the weights on of those examples that make a difference at the end.",
            "OK, so you do."
        ],
        [
            "That now the weak learner of course will make all of these incorrect.",
            "All of these incorrect, which you don't care, and then it will make 0.6 of those correct.",
            "It has been."
        ],
        [
            "OK, and then you get this final configuration and."
        ],
        [
            "What you get in this final configuration is that on these examples, the majority rule is correct, and on these examples the majority."
        ],
        [
            "All is incorrect.",
            "OK, that's the end of the game.",
            "Alright.",
            "So."
        ],
        [
            "Here is the interesting thing I'm just going to jump all the way to the end and tell you what is the thing that the weak learner, the one that splits the bins, should do in order to make the job as hard as possible for the booster.",
            "OK, it turns out that it's very, very simple to characterize."
        ],
        [
            "It chooses half plus gamma from each bin to be correct.",
            "At every iteration, that's the min, Max, optimal it can do.",
            "OK."
        ],
        [
            "OK, and another way to think about it is that the prediction of each base rule on each example is chosen independently at random to be correct with probability half plus gamma.",
            "OK, that's basically the same statement just here.",
            "We just talk about volume and here we talk about probability.",
            "But in terms of really probability theory, it's the same statement.",
            "OK, and this is very surprising, right?",
            "Basically this is kind of a brain dead weak learner.",
            "It's just giving us the correct label each time with probability half plus gamma and and that's the best it can do.",
            "It cannot make things worse than us now, WHI?",
            "Why is it?",
            "It's clear that this is something that that is will always work because no matter how much weight we put weights on the different bins, this will be correct with respect to our weights, half plus gamma of the time.",
            "So the real question which remains is.",
            "How can we basically force this algorithm not to be any better?",
            "So let's say that it basically at some point decide so I actually am going to do something different, and so I'm going to actually give you at the end of majority rule that is less correct.",
            "How can we basically force it not to be able to do that?",
            "So in order to discuss this, I'm going to define something."
        ],
        [
            "All the potential intuitively let me go back intuitively, the way that we're going to do.",
            "That is, we're going to say, OK, here is a weak learner, and it's going to do something worse than just have half plus gamma, half minus gamma.",
            "Let me think that later on it will just do the right thing.",
            "OK, just do the half plus gamma half minus gamma.",
            "How can I make sure that on this step it can't gain anything OK and that comes back to the potential."
        ],
        [
            "OK, so the potential is the probability mu probability is the probability according to this measure that I defined on the chips of the examples on which the final majority vote is incorrect.",
            "Given the configuration after iteration T, is this configuration so it's a parameter and the remaining steps the learner is going to play add opt OK.",
            "So so if it's going to play this way?",
            "From a given configuration, then basically it doesn't really matter what I'll do.",
            "You can calculate how many will be correct at the end OK, and that's this quantity that I defined here potential."
        ],
        [
            "OK, so interesting to look at.",
            "What is the total potential?",
            "Initially the total potential initially is, well, you have iteration zero and all of the examples are at the origin.",
            "OK, so that's the initial potential."
        ],
        [
            "The final potential is simply the training error of the final majority rule.",
            "OK, that's simply because at this point there is no more iterations and this is just the definition.",
            "OK, so So what?"
        ],
        [
            "I'm going to show you is that the boosting algorithm can choose weights in such a way that the total potential will not increase.",
            "OK, so from iteration to iteration it will basically use the weights that it has control over so that no matter what the weak learner does, it cannot cause the potential to increase, right?",
            "So if the initial potential is something that's basically going to be the final training error.",
            "OK.",
            "So that's what I said.",
            "OK, so now I'm going to define."
        ],
        [
            "More break this up into pieces, so I'm going to talk about the potential of a particular bin.",
            "OK, so we have these bins indexed by S, so that's the fraction of examples in bin South after iteration T on which the final majority rule will be incorrect at the end.",
            "OK, so for each bin."
        ],
        [
            "Something different, and if we define FTS to be the amount of chips that we have in business."
        ],
        [
            "At iteration T, then the total potential is simply this sum.",
            "Oh, it should be S down here.",
            "OK, so it's the sum of FTS times the potential TS.",
            "That's all it is, right?"
        ],
        [
            "Now important thing is that the TTS does not depend on the configuration, is just a number that is per bin and depends only on TNS, and we can write it in closed form.",
            "OK, So what it is, it's simply the binomial distribution.",
            "OK, so that's deep."
        ],
        [
            "TS minus half plus gamma right?",
            "So if it plays optimally, all of the DTS is are going to be exactly 0, but it might not want to play what we think is optimally so, so.",
            "So that's how we measure how we characterize what the weak learner does."
        ],
        [
            "WTS I'm just going to define it at this point to be the difference between the two potentials at the next step.",
            "OK, this is going to be actually the weights that we used to give to the examples, But at this point is just."
        ],
        [
            "It's just this difference.",
            "OK, so here is the theorem.",
            "If DTS WTS is larger equal to 0.",
            "So basically this is exactly characterizing that the weak learner is giving us a rule that has error smaller than half minus gamma or at most half minus gamma, then the potential does not increase.",
            "OK, it can only decrease.",
            "And the corollary to that if we basically."
        ],
        [
            "Caffeinated from Step 0 all the way to the end is that for all T if for all T the weighted error of HD is smaller than half minus gamma, then the initial potential is larger equal to the final training error.",
            "OK, that basically will give us the theorem.",
            "OK."
        ],
        [
            "Here is the proof.",
            "So the proof is like this.",
            "It's pretty easy to calculate FT plus one at the next time for bin S as a function of FT and S -- 1 and FS plus one in the DTS.",
            "Right, because basically that's just calculating how many go up, what fraction goes up, what fraction goes down and that just is the formula.",
            "OK, so if we want to calc."
        ],
        [
            "Wait, the potential at the next step.",
            "We can just use the equation that I told you before and plug in this equation for FT. Oak."
        ],
        [
            "Now we can just do some algebra and it turns out that we can rewrite this sum as a slightly different somewhere.",
            "FTS appears only once, and ETS appears only once and the fees are now added and subtracted.",
            "And."
        ],
        [
            "What we see is that this term that is here is simply 50 at S and this term here is simply double."
        ],
        [
            "Atheist.",
            "OK, so when we write it out, we get that."
        ],
        [
            "50 + 1 The total potential is exactly the previous potential minus DTZ WTS."
        ],
        [
            "And so if DTS sum of DTS WTS larger or equal to 0, then 50 + 1 is smaller equal to 50.",
            "That's that's the proof.",
            "So it's just algebra."
        ],
        [
            "Once you have the right, the right quantity."
        ],
        [
            "OK, so this is kind of a different way of saying it's setting the boosting weights at iteration T to be this horrible expression that is just the difference between two binomial tails.",
            "So it's a binomial term then."
        ],
        [
            "When we get the final error, is the binomial for TT over two half plus gamma, which this is a function that goes down very quickly with T. OK, so that's basically the guarantee.",
            "OK, so we know that it's a boosting algorithm, but in what way does it look so different than other boosts?"
        ],
        [
            "So here is the way.",
            "So what I plot here is the weight and the potential.",
            "So remember, the weight is essentially the derivative of the potential, so this is the weight and this is the potential initially when you're at T1.",
            "Remember, you're only around here, so you just see an increasing potential and is increasing weight function.",
            "But as you go to T ill."
        ],
        [
            "Seven and the total number of iterations here is 100 as you go."
        ],
        [
            "Get closer, you see that on some example, so that the weight function goes up and up and up and up reaches a maximum, then goes down.",
            "Right, so you start to basically give up on examples that you got too many times incorrectly."
        ],
        [
            "And then you go on and on until it iteration 101."
        ],
        [
            "Which is the last iteration.",
            "You basically are concentrating only on those examples in which exactly half got it correct in half got it incorrect, which is what I started with.",
            "OK.",
            "So that's all good and well and we're indeed minimizing a non convex loss function."
        ],
        [
            "And if we compare it to the previous boosting algorithm, you see that it's very different.",
            "So here I plot actually for all of these algorithms, it's useful to look at both the weights and the potential for other boost.",
            "It's kind of not very informative because the potential and the weight RE to the minus X, so they're kind of interchangeable.",
            "However, for logic boost whose potential function is this log of 1 + E to the minus by logic boost is the same as Tibshirani and Friedman.",
            "Gentle, gentle adaboost.",
            "Then the weight function is this, and this weight function is a SIG model, so it basically it increases, but then it stops increasing.",
            "OK, because the potential function becomes linear.",
            "Our function is even more aggressive.",
            "It increases and then it reaches a maximum, then it decreases.",
            "OK, so it's very different.",
            "Alright, so the high level some."
        ],
        [
            "Three is that the worst case adversary splits each bin into half, minus gamma, incorrect and half plus gamma, correct?",
            "And you can think."
        ],
        [
            "Now this is random walk with IID steps.",
            "OK, so this is just a random walk with probability half plus gamma of going up."
        ],
        [
            "Minus game of going down.",
            "And the algorithm is derived as the optimal response to the simple worst case adversary.",
            "So this seems like brain dead adversary.",
            "But it turns out that it's the worst adversary.",
            "This is something that is kind of surprising but true."
        ],
        [
            "OK, so now I'm going to talk about boosting in continuous time.",
            "So the question is."
        ],
        [
            "Why why continuous time?",
            "What's what's the big deal with that?",
            "So the deal is that boost by majority was invented before Adaboost.",
            "So how come we're not using boost by majority rather than other boosts?",
            "Well, the simplest answer is that it's much more."
        ],
        [
            "Complex.",
            "OK, that's a very good answer by itself.",
            "However, you know we can overcome complexity if we really."
        ],
        [
            "Have a good reason.",
            "So the more the deeper reason is that boost by majority needs to know epsilon and gamma before starting right.",
            "So Gamma needs to be known and epsilon what we're shooting for needs to be known and those are never known ahead of time.",
            "Worse than that, the dependence, the number of iterations we need to run depends on these parameters like this in this way.",
            "OK, so especially problematic is the dependence on the advantage over random guessing.",
            "So if that is just let's say 1%, then we're committing ahead of time before anything starts to run this algorithm for at least 10,000 iterations.",
            "So in practice this is just not a reasonable thing.",
            "OK, so why did Adaboost catch the stage?",
            "Because other boost who stands for adaptive boosting adapts to the sequence of these advantages."
        ],
        [
            "OK, So what that gives you is there is no need to set parameters in advance.",
            "That's a very very big deal.",
            "You don't know it until you until you publish it.",
            "You know basically you publish an algorithm with no parameters.",
            "People are actually going to try it.",
            "When you publish an algorithm with parameters, somebody else is going to try.",
            "And it generates a weight."
        ],
        [
            "Majority rule OK, so these are the big advantages that we want to do and you don't need to decide ahead of time how many iterations to run.",
            "You just stop using cross validation and in in boost majority we somehow decided ahead of time when to stop, which is kind of strange thing.",
            "OK, so the real question is how can we make?"
        ],
        [
            "It's by majority adaptive and the idea is that."
        ],
        [
            "For that, we're going to continue this time.",
            "OK, so how much time at 47 minutes?",
            "OK so.",
            "OK, so we let the time."
        ],
        [
            "Step go to 0.",
            "So let me read it."
        ],
        [
            "Rate the number of iterations we require by boost majority is one over gamma squared log of one over epsilon."
        ],
        [
            "OK, so the idea is that we're going to keep epsilon fixed.",
            "That's the error that we're shooting for, and then we're going to let gamma this gamma, which is the advantage go to 0, right?",
            "Because somehow if we have a weak learner that can give us some gamma, let's say 1%, of course it can give us a smaller gamma.",
            "Well, let's say a 10th of a percent.",
            "OK, so we're going to let gamma go to zero, but that would require us to make the number of iterations go to Infinity.",
            "OK, so that's a bother something but will get over it.",
            "And and what is going to happen when we do that?",
            "Right now we're doing it.",
            "Just a thought experiment.",
            "So think about it, the gamma is very, very small and we're getting let's say, one."
        ],
        [
            "And we're getting a rule whose error is, let's say, 4040%.",
            "OK, so it's much better than the 49% that is required.",
            "So we add this rule.",
            "Almost nothing changes, right?",
            "The distribution changes a tiny bit, and so on.",
            "So we go, we check the same rule against this distribution.",
            "Well, it still has maybe 41% error, so we had it again and we add it again and we add it again.",
            "And when we gamma is very, very small, will add it a huge number of times until basically it becomes something that is not good enough.",
            "OK, so.",
            "So."
        ],
        [
            "That yields essentially an adaptive boosting and awaited majority rule, right?",
            "Because what is it?",
            "We added the same rule 55 times.",
            "We ran it totally for 10,000 times, so that's whatever 5.",
            "5 1/2%.",
            "And so that's the waiting.",
            "And it's adaptive because really, we didn't.",
            "We asked for so little that we.",
            "That's like asking for nothing at all, right?",
            "We're asking for such a tiny gamma that any algorithm that you give me that it's like any rule that you give me that is slightly better than random guessing I can use and I'll just give it weight according to this heuristic.",
            "OK so but the problem is this is not realistic to actually do right because because you have to run for so many iterations."
        ],
        [
            "What we want to do is we want to somehow calculate exactly how many times we're going to add a rule, given that you give it to me.",
            "So in order to calculate that, then we need to somehow characterize what's going on.",
            "Now remember what's the adversary doing?",
            "Is just doing a random walk right?",
            "So how do you characterize a random walk when you make the number of steps go to Infinity?",
            "Well, that is something called Brownian motion.",
            "OK, and I'll tell you why it's Brownian motion in a minute, 'cause there's something very non intuitive about Brownian motion that if you don't really look into it, is easy to miss.",
            "OK."
        ],
        [
            "Just technically, instead of T going from one to T going to Infinity, we want to keep this control.",
            "We don't want to go to Infinity, so we're going just going from T0 to T1 and we're going to go in little steps 1 / T."
        ],
        [
            "OK, so here is the original game lattice as I drove it before, so in this case we have T = 3 and T is one or two or three.",
            "Now we just."
        ],
        [
            "Going to go instead T Go third 2 third one.",
            "OK, now the real question is how do we make the steps of the score?",
            "How do we make the steps as we go along?",
            "So the natural thing is to make."
        ],
        [
            "Delta S How much we changes also go like 1 / T. Right, so that seems reasonable.",
            "So if we do this is for one step, then we go up and down like this."
        ],
        [
            "For two steps.",
            "We take."
        ],
        [
            "Three steps and we also end up at plus one or minus one.",
            "49 steps.",
            "We go like this and we end up either somewhere between plus one and minus one."
        ],
        [
            "And all of that looks very natural and good.",
            "However, it's a real problem when you think about the random walk without going over that."
        ],
        [
            "Because the variance of where you end up it goes is T steps times the variance of one step which is 1 / T squared and that gives you 1 / T. So as T goes to Infinity, now you have that the variance goes to 0.",
            "So in fact, if you use this kind of steps you get nonsense.",
            "You get basically that at the limit of continuous time the everything stays at zero all the time.",
            "Very strange, but again, that's the way it is.",
            "So in order to overcome this you need to do something that is very very non intuitive.",
            "But people in stochastic processes do all the time."
        ],
        [
            "Which is to say that Delta S goes like 1 / sqrt T. OK, that's the only way that it would work, and so here is 1."
        ],
        [
            "Step again, just like before, but for three steps, so the very."
        ],
        [
            "This here is 1.",
            "4th"
        ],
        [
            "The steps we take, Delta S is equal to 1 / sqrt 3, so we go not between plus one and minus one like before.",
            "Now we go between plus sqrt 3 and minus sqrt 3.",
            "OK. And the nice thing is that."
        ],
        [
            "The variance is still 3 * 1 / 3 it's one, so the variance stayed the same.",
            "If we go another step."
        ],
        [
            "We get."
        ],
        [
            "This weird thing that that we go in.",
            "In each step we have one we have 1/3 which is the square root of a 9.",
            "And 1 / sqrt T and so we go between 3:00 and minus minus three."
        ],
        [
            "Now the variance is still one.",
            "So this is how we're going to do things."
        ],
        [
            "Take the limit in this kind of strange way in order to make things work and the variance will stay as we want, but on the other hand, the range will go to Infinity, right?",
            "So in epsilon time the this chip can go to minus arbitrarily big or arbitrarily small or plus arbitrarily big.",
            "OK, and that is very unintuitive.",
            "But again, that's exactly how Brownian motion is.",
            "OK, so so this is I don't think that I can go now through the detailed analysis of it, but I'll just tell you on the high level.",
            "How does it?",
            "How does the math now look?"
        ],
        [
            "And you go to this."
        ],
        [
            "Brownian motion.",
            "So before in discrete time we had equations that relate time T to time T + 1 based on random walks.",
            "OK, so that's how it works when we."
        ],
        [
            "To discrete time.",
            "When you do continuous time, you have to do use differential equations that describe the density evolution for Brownian motion with drift.",
            "OK, it's just that's the that's what you get in the limit, and these are very well known equations.",
            "They're called the Commodore forward and backward equations for people that know stochastic processes.",
            "OK."
        ],
        [
            "So how do these, for instance look like so in?"
        ],
        [
            "By majority that was the iteration like that.",
            "That's how the recursion that relates potential at time T and two time T -- 1."
        ],
        [
            "And these are the boundary condition in continuous.",
            "That's an algorithm called Brown Boost.",
            "You get that this is the recursion.",
            "OK, so the recursion now is a differential equation of 2nd order.",
            "Anne."
        ],
        [
            "And the boundary conditions are actually exactly the same as before."
        ],
        [
            "OK, so basically we can do the whole thing in continuous time, but now instead of what we have to do in the single step of boost by majority of Brown boost is to solve 2 nonlinear equations with two unknowns.",
            "Which is something that actually computationally I have no idea how computationally hard it is, but practically you can do it with Newton method."
        ],
        [
            "So.",
            "OK, so now we're going to talk about robust boots, so this Brown boots that something I published maybe three or four years ago.",
            "It still wasn't working.",
            "You know it was very nice theory, but it wasn't working."
        ],
        [
            "So so robust boost is the one that finally works."
        ],
        [
            "So instead of minimizing the training error, what we want to minimize in this case is the examples whose margins smaller equal to Theta number.",
            "There was something in the background going on that boosting.",
            "If you know tends not to overfit and there is an explanation based on margin, and here I basically put my chips on this explanation and I'm saying no.",
            "What we really want to minimize is not just the number of mistakes we want to minimize the number of examples whose margin is smaller than something it's essentially like.",
            "Support vector machines, but it's support vector machines where you give up on examples who have who fall below the margin."
        ],
        [
            "And what you have to do when you do this is you have to control the weights.",
            "You have to control the norm the way that it's done in support vector machines you control the L2 norm of your examples.",
            "Here we're doing the control of the weights in a completely different way.",
            "We're controlling instead of controlling the range with controlling the variance of the scores.",
            "And it was one last thing is we want to allow confidence rated."
        ],
        [
            "Weak learners, so we want the weak learner not to be able to just say plus one or minus one, but any number in between.",
            "OK, so experimental results.",
            "Just what today?",
            "Yeah so so just just to kind of overwhelm you.",
            "This is what you get."
        ],
        [
            "When you do all of that, it takes.",
            "It takes awhile to to understand it.",
            "It takes even longer to encode it, but to program this.",
            "But we did it."
        ],
        [
            "OK, so."
        ],
        [
            "So now the back to the experimental result.",
            "OK, so here is first the experiment results on long Sir video.",
            "OK, so remember the data that I showed you that they proved that nothing that is convex can actually minimize it.",
            "Can actually find the good classifier.",
            "So here we are."
        ],
        [
            "Movies this is a movie about using Adaboost and."
        ],
        [
            "You see here is that this iteration.",
            "You see the number up here is still going on, but boosting is not really.",
            "It's stuck in this point, right?",
            "These are all the penalising examples, it's slightly more complex than what I showed you before.",
            "It's actually a binary set up, but I'm not going to go to details essentially the same as what I showed you and what you see is that basically after a small number of iterations it separates pretty well the large margin and the.",
            "Colors but the penal izers are stuck in the middle and it cannot differentiate them.",
            "And it doesn't get much better when you use logic boost a little bit better because it puts a little bit less weight on these bad examples, right?",
            "These are the examples that hurt you.",
            "These are the 10% error that is added to the data and these are the examples in which you put a huge amount of weight which misleads you.",
            "OK, and here is what happens with robust Boo, so it's interesting what you see is this is the potential function you see that at the beginning it's very much the same, but getting now to the end it completely ignores these examples out here and it concentrates more and more on the examples in the middle and eventually it gets them all correct.",
            "So in fact in the rocks are video long problem, it finds exactly the correct underlying classifier.",
            "OK, so.",
            "This this might seem like something that nobody can reproduce, but you can reproduce it."
        ],
        [
            "So we put a lot of effort into making this into open source.",
            "It's called J Boost and I would really like you to download it and try try it out.",
            "This is the website you can just Google on J Boost."
        ],
        [
            "OK, so this is a result using this implementation.",
            "And these are the results I showed you before and now let me show you how what happens."
        ],
        [
            "What happens when you that was a sudden end to my talk so?",
            "So this was, this is what happens when you run logic boost.",
            "OK, so logic boost is a little bit better than other boosts and here is what happens."
        ],
        [
            "Essentially, it's very similar to what you saw before."
        ],
        [
            "You don't see the red here, almost not OK you see."
        ],
        [
            "Little bit of the red here, but what happens is that that it gets to some point, but beyond that point it doesn't really improve."
        ],
        [
            "So this is iteration 100, iteration 2000, so oh sorry, I'm completely mistaken.",
            "This is the case where you have zero noise.",
            "OK, so in this case everything works nicely.",
            "In this."
        ],
        [
            "Everything works nicely."
        ],
        [
            "But here is logic both with 20% noise and."
        ],
        [
            "What you see here is that Nomad."
        ],
        [
            "Or how much you run."
        ],
        [
            "Basically, there is no Sir."
        ],
        [
            "Operation, it doesn't manage to even in 2000 iterations it doesn't really manage to separate the positive from the negative, which is what you saw at the end.",
            "And what's the reason?",
            "The reason is that 20% of these examples that are positive actually have negative label, and you can try to get them in the correct side because they are inherently incorrect.",
            "Here is what robust boost does on that so."
        ],
        [
            "So initially it does very similar stuff."
        ],
        [
            "But then."
        ],
        [
            "When you get to this step, it basically gives up on all of the examples that are out here or out here.",
            "OK, so this is the potential function whose gradient is the weight function.",
            "OK, so only examples from here."
        ],
        [
            "To hear it, and now it's even only examples from here to here.",
            "So all of these examples out here whether there are labeled plus or blue or red, they're ignored.",
            "They're completely new."
        ],
        [
            "Look, then and now we go even further and we push those."
        ],
        [
            "Examples further."
        ],
        [
            "And then finally, ah."
        ],
        [
            "After 430 iterations, it's."
        ],
        [
            "One and you see that it found a large margin.",
            "Classification.",
            "And but it gave up.",
            "So you see, all of these blue ones here, and there's a lot of red ones here.",
            "Hopefully, yeah, red ones here and those those are the mislabeled examples that it gave up on so that it could find the correct majority rule.",
            "So this is, I think, justifying the explanation, the long and tedious explanation that I gave you before.",
            "OK, so pretty much I'm done with that."
        ],
        [
            "Lock"
        ],
        [
            "Just wanted to say there is much more details in the papers.",
            "This is essentially the 1st paper that is about this.",
            "Everything that I talked about before today."
        ],
        [
            "Then unfortunately it got rejected from ICM L 09 so I had to give a talk here.",
            "But that's not so something to say, particularly in ICML 09.",
            "This is another paper that we did using drifting games for online learning, and that was rejected from."
        ],
        [
            "Called and this is a paper based on this paper and that was."
        ],
        [
            "Ejected from UI.",
            "So.",
            "Equally rejected from everybody.",
            "This one was not rigid."
        ],
        [
            "Maybe because I didn't submit it yet.",
            "You can get all of these papers from my website.",
            "So to summer."
        ],
        [
            "Guys don't get too upset."
        ],
        [
            "If your paper is rejected.",
            "Because you might present it."
        ],
        [
            "Invited speaker."
        ],
        [
            "Also tried a boost.",
            "J Boost is pretty cool.",
            "I think you should give it a spin.",
            "And please ask me questions.",
            "There is no time for questions as there is Oh my God.",
            "OK, so ask me questions now, but if you don't get to ask me Now, please write me email.",
            "I'm very eager and specially I'm going to stay here till Thursday.",
            "So the best thing is you write an email say I'd like to meet and talk with you to understand what you're talking about and and we'll just find the time and talk.",
            "Anne."
        ],
        [
            "And that's it.",
            "Thank you.",
            "We do actually have time for it."
        ],
        [
            "OK, back there.",
            "Yeah.",
            "An algorithm that does tagging like random forest compactor boosting up.",
            "So that's a very good question.",
            "We're trying it right now, so bagging definitely does better than boosting.",
            "However, it doesn't do well when your base classifiers are very restricted, right?",
            "So so essentially, in the problem of of.",
            "Long answer video.",
            "It's not going to do anything good because each time it's going to give you the same bad weak hypothesis because you have enough examples.",
            "In other cases we actually found that random forest works better in this particular one that I showed you here.",
            "We tried various and it turned out that the best one was Booth was bagging C 4.5.",
            "So yes, so there are other algorithms and there's definitely something more going on with bagging that I still don't completely understand.",
            "And that I would like to incorporate so, but what it's doing is very, very different right?",
            "Because because if you basically just talk about this video and the long answer video data set, I don't know of any algorithm that would be able to get that right.",
            "However, if you try to learn that same thing with decision trees, it turns out you can, but that's kind of like going going around different way so.",
            "So yeah, so that's kind of still open.",
            "What's there more things that we don't understand, yeah.",
            "With the back set of plastic box, you have an option to select from that set using a test and you can actually collect rebuilding ensemble Diane.",
            "Yeah, that's that's true, but that still doesn't tell you how to get rid of noisy examples.",
            "Right so.",
            "It's not quite the same thing, yes.",
            "And explicitly have variables to late.",
            "Oh oh so so mixture models mixture models when you have a large number of dimensions and a small amount of data overfit terribly.",
            "So so basically if you really try to learn the underlying true distribution, then then you might be successful.",
            "If the distribution is really like what you hoped it is, but if the distribution is different then there's no guarantee whatsoever.",
            "Yeah.",
            "Goes down again.",
            "You don't want to keep realizing things would be more and more on the wrong side of the frontier.",
            "Another one side?",
            "Well, maybe intuitively, but the fact is that OK, so that's exactly what we do when we remove outliers, right?",
            "We basically say this example.",
            "We suffer on it.",
            "Let's say in regression case so much we just wanted to pretend it's not there.",
            "So no, in regression in particular because it's so sensitive to outliers, that's exactly what we do now.",
            "The fact that we don't know how to analyze it is separate, but intuitively it makes sense, and it works.",
            "So that's what people do.",
            "Yeah.",
            "Sonny concerns me, which is seems like in some sense you're you're learning to ignore a part of the problem space, right?",
            "This algorithm into filtering situation where we have an Oracle with an infinite amount of data.",
            "And so maybe maybe things are changing slowly.",
            "Would you be able to track an on line situation?",
            "Well, I don't have anything theoretical to say about it at this point, but actually there's there's a string of algorithms called online boosting which people have used and and quite successfully, and I believe that this can be used in online boosting just as well.",
            "Now, of course, if you basically are starting to give up on things that are that are that eventually become become different then, then you need to somehow overcome that.",
            "So what you would essentially need to do.",
            "Is you'd need to rerun the algorithm from from scratch from time to time.",
            "Yes, yes.",
            "More questions, yes.",
            "Big problem here.",
            "Sometimes look better, so it's going to fail sometimes, right?",
            "I don't know yet.",
            "I don't know yet.",
            "It's obviously going to fail on some on some distribution.",
            "My intuition is the following.",
            "There is, there is a kind of assumption that we usually make in learning which is never has been made explicit, which is that if we learn a good rule using, let's say, the best rule using a small amount of training data.",
            "It's not very far from the good rule, using a lot of training data now none of our setups actually capture that, but I believe that under that assumption that essentially if you look at your empirical error surface and you add more and more examples, it's not like initially you see a minimum and then you see that the minimum is in a completely different place.",
            "You just see that the minimum becomes finer and finer, and you know exactly where to put things.",
            "So essentially, after a small amount of training data you can eliminate.",
            "A lot of your version space if you will.",
            "Even though even though you know you might kind of potentially be illuminating apart that when you had a lot of data, would be would be correct.",
            "So I think that that's the underlying assumption.",
            "I don't know yet how to formalize it, so at this point it's just kind of an intuition.",
            "Yes.",
            "Very good question.",
            "I think that the reason is that the that the program committee didn't understand it so.",
            "So, so that is that is actually, I think the true reason.",
            "I don't know if you understood it any better.",
            "However, the advantage of the situation that urine is that you can actually download the software and try it right.",
            "So I think that the big advantage of Adaboost was not so much that it had great theory, but it was a one page algorithm.",
            "So people just said, oh, I can code that and they coded it in a few minutes.",
            "This is too complicated for people to just code in a few minutes.",
            "But you can download it so.",
            "So why it was rejected?",
            "I don't know why I see it as a badge of honor.",
            "Any other questions?",
            "Yes.",
            "Do you still do after?",
            "Oh yes, active learning is one of the directions I really want to pursue with this and we've actually started.",
            "So here's the interesting thing that happens when you get close to the end.",
            "Is that basically any example that you get very correct or very incorrect?",
            "Get 0 weight.",
            "So that's interesting, right?",
            "Because basically that means that for those examples we don't need to know the label 'cause we're not going to look at the label anyway, so in fact we're starting to use that, and it seems impractical problems that we're working on right now envision that it actually works pretty well now.",
            "I don't have any theorems to show about it yet.",
            "But yeah, I think that it's going to be very useful thing for active learning.",
            "Anymore.",
            "Yes.",
            "Papers about.",
            "Oh, OK, thank you for the question.",
            "I'll pay you later.",
            "So those were about using this for online learning, so this drifting game idea is actually something that is very close to an old algorithm that I worked with.",
            "Manfred and Nicholas is a Bianchi and other people on binomial weights online learning, which is not exponential weight and and it uses very very similar kind of chip game in its underlying thing, and basically by making that chip game continuous time.",
            "We managed to get a new performance.",
            "Now that's paper.",
            "That's the quote paper you AI paper is using that to come up with a better tracking algorithm.",
            "So we have a tracking algorithm that is much less sensitive to assumptions then.",
            "Then the Kalman filter.",
            "So so those are the papers.",
            "Thank you.",
            "Yes.",
            "I wonder about regression.",
            "Does this apply to regression was close to classification?",
            "Oh so OK. Yeah, I think it does apply to regression, but you need to be willing to rid yourself of the square error right?",
            "Because the problem with regression is really that it minimizes square error, and that's not usually what we really want to minimize.",
            "What am I saying that?",
            "Why do we have outliers removal in regression?",
            "Because outliers hurt us.",
            "So much.",
            "What we really want to do in regression, I believe is very close to what Vapnik suggests, which is we want.",
            "Most of the examples to be very close to our function and then on the rest we don't care like 10% of the examples can be arbitrarily far for that kind of robust regression.",
            "Sometimes it's called yes, we can use these ideas and we started to work on that.",
            "But so so you need to basically say.",
            "When am I going to give up on an example, right?",
            "That's the underlying intuition.",
            "Sometimes you want examples in which you give up in regression.",
            "You never give up on examples as long as you're using squared error.",
            "But if you clip the square, which is basically like Huber robust statistics then then you start to do something more interesting and then you can use ideas like like Boost.",
            "Majority.",
            "Other questions.",
            "Alright, thank you and Bon Appetit."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for this very nice introduction.",
                    "label": 0
                },
                {
                    "sent": "My wife would really like to learn that I'm efficient.",
                    "label": 0
                },
                {
                    "sent": "Dude is there?",
                    "label": 0
                },
                {
                    "sent": "Audio there good.",
                    "label": 0
                },
                {
                    "sent": "OK so yes.",
                    "label": 0
                },
                {
                    "sent": "So the talk.",
                    "label": 0
                },
                {
                    "sent": "I change it on Friday or I change it last week because I realized I'm just trying to talk about too many things at once.",
                    "label": 0
                },
                {
                    "sent": "So I reduced the part that is on online learning.",
                    "label": 0
                },
                {
                    "sent": "I remove that essentially and I'll only give you some pointers at the end.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is this talk about?",
                    "label": 0
                },
                {
                    "sent": "This talk about is essentially about a new improvement to Adaboost, so Adaboost has become very popular, but even as its popular, many people have pointed out its problems and the main problem.",
                    "label": 0
                },
                {
                    "sent": "I want to focus on in this talk is that it is very sensitive to label noise.",
                    "label": 1
                },
                {
                    "sent": "OK, so many people notice it before.",
                    "label": 0
                },
                {
                    "sent": "Here is an example.",
                    "label": 0
                },
                {
                    "sent": "We take the letter database from.",
                    "label": 0
                },
                {
                    "sent": "The Irvine repository and we make the problem binary just so that we can focus on one problem by looking at FIJ versus the other letters.",
                    "label": 0
                },
                {
                    "sent": "And then if we run other boost.",
                    "label": 0
                },
                {
                    "sent": "On decision trees, I think then we get very good performance if we run logic boost we get.",
                    "label": 0
                },
                {
                    "sent": "Almost the same performance, however, both algorithms fail miserably when we add 20% label noise.",
                    "label": 0
                },
                {
                    "sent": "OK, so Tom Dietrich has pointed it out long time ago that Adaboost is very sensitive to that.",
                    "label": 0
                },
                {
                    "sent": "An logic boost is somewhat less sensitive.",
                    "label": 0
                },
                {
                    "sent": "You see that it's a little bit better, but not that much better.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem is that boosting puts way too much weight on outliers.",
                    "label": 0
                },
                {
                    "sent": "OK, so if examples you get on them the incorrect prediction because they are simply incorrect examples, their label is incorrect, then you tend to with time put on it way too much weight and really what you'd want to do is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Want to give up on them, right?",
                    "label": 1
                },
                {
                    "sent": "So what are outliers?",
                    "label": 0
                },
                {
                    "sent": "Outliers are exactly those examples that we want to say.",
                    "label": 0
                },
                {
                    "sent": "Don't show me those examples that are just confusing me, OK?",
                    "label": 0
                },
                {
                    "sent": "So here is an algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that here is just a summary of the algorithms performance.",
                    "label": 0
                },
                {
                    "sent": "This is algorithm called robust boost and what you see is that it performs slightly worse.",
                    "label": 0
                },
                {
                    "sent": "On the noiseless case, and that's essentially because of some numerical issues.",
                    "label": 0
                },
                {
                    "sent": "But then when you add 20% noise, it performs much, much better than the previous algorithms.",
                    "label": 0
                },
                {
                    "sent": "You can clearly see it here, but you can see it even more if you.",
                    "label": 0
                },
                {
                    "sent": "In this case we added the 20% noise ourselves so we could look what is the performance according to the original.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before we add the noise.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that performance is very good.",
                    "label": 0
                },
                {
                    "sent": "So if you look at other booster logic boost, the error is essentially the noise level, while if you look at robust boost, it essentially gets mistakes that are much much lower than the noise level.",
                    "label": 0
                },
                {
                    "sent": "So it essentially you can say correct most of the mistaken labels.",
                    "label": 0
                },
                {
                    "sent": "Which statistically doesn't seem like a problem, but if you work on these kind of problems, you know that computationally it's a very hard thing to identify those examples in which you have the incorrect label.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the plan of the talk, so I'm going to basically show you how how I do it.",
                    "label": 0
                },
                {
                    "sent": "Here's the plan.",
                    "label": 0
                },
                {
                    "sent": "First of all, I'm going to tell you that it's been pretty much identified.",
                    "label": 0
                },
                {
                    "sent": "What's the problem with label noise and it's associated with the convexity of loss functions?",
                    "label": 1
                },
                {
                    "sent": "Then I'm going to tell you about an old algorithm boost by majority and show you how it's analyzed using a concept of drifting games.",
                    "label": 1
                },
                {
                    "sent": "And then I'm going to tell you, and that is actually a very old algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's before Adaboost, it's 95, while Adaboost is 97.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to show you what I've been working on in the last 10 years, which is essentially moving this algorithm into the continuous time.",
                    "label": 0
                },
                {
                    "sent": "And why do I do that?",
                    "label": 0
                },
                {
                    "sent": "And then finally, I'm going to go back to the experiments and show you how when you look at things the right way, the results actually are not that surprising.",
                    "label": 0
                },
                {
                    "sent": "OK, so starting with label.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Noise and convex loss functions.",
                    "label": 0
                },
                {
                    "sent": "So using loss convex loss function is a very very common paradigm.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In all of machine learning, if you look at Perceptron, Adaboost, logic, Boustan soft margin SVM, they all look use some kind of convex loss function.",
                    "label": 0
                },
                {
                    "sent": "OK, so that that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Much is clear.",
                    "label": 0
                },
                {
                    "sent": "And they work.",
                    "label": 0
                },
                {
                    "sent": "All of these algorithms work very well when the data is linearly separable, OK, and when the data is linearly separable.",
                    "label": 1
                },
                {
                    "sent": "We know really that computationally there's no big issue.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, all of them can get into bad trouble when the data is not linearly separable.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so really, what is the problem?",
                    "label": 0
                },
                {
                    "sent": "The problem is that convex loss functions are a very poor approximation for the classification error.",
                    "label": 1
                },
                {
                    "sent": "That is what we really want to minimize when, especially when you're on the incorrect side.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by that?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I'll show you in a minute, but the other side of it, so everybody knows that you'd really want to minimize the.",
                    "label": 0
                },
                {
                    "sent": "The classification error, but the classification error is not a convex loss functions and once you get into minimizing something that is not convex, basically our tendency is to just throw up our arms and say we can do it.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to show you that in some sense we can do it.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so here is what I mean.",
                    "label": 0
                },
                {
                    "sent": "Kind of more specifically this is the margin.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by the margin is simply what is the?",
                    "label": 0
                },
                {
                    "sent": "What is the sum of the base classifiers before we threshold it?",
                    "label": 0
                },
                {
                    "sent": "OK, so times the correct label, so if you're on if the score is on this side, then based if the margin is on this side, you're correct.",
                    "label": 0
                },
                {
                    "sent": "And if you're on this side, you're incorrect, so therefore the classification loss here is one OK, and this is really the function that we want to minimize.",
                    "label": 0
                },
                {
                    "sent": "However, because it's not convex, what we minimizes some upper bound on it, so this is the.",
                    "label": 0
                },
                {
                    "sent": "Upper bound this linear function is upper bounds that are used by SVM.",
                    "label": 0
                },
                {
                    "sent": "This is not quite an upper bound, but this is used by perceptron and other boost uses this exponential wave function and logic boost use the function that is exponential on this side, but becomes linear on this side.",
                    "label": 0
                },
                {
                    "sent": "OK, so they're all convex and because of that they have a global minimum and we know how to minimize them.",
                    "label": 0
                },
                {
                    "sent": "But that's also the problem.",
                    "label": 0
                },
                {
                    "sent": "The problem is if we have 20% label noise, then these examples necessarily need to be here and we don't want to really pay attention to them.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's our problem.",
                    "label": 0
                },
                {
                    "sent": "The microphone seems to me unstable.",
                    "label": 0
                },
                {
                    "sent": "Everybody can hear me well.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, so here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is an example that basically not an example the last year in acnl, Phil Long and raucous or video showed a very nice result.",
                    "label": 0
                },
                {
                    "sent": "They basically showed that the problem that I kind of intuitively says with convex loss functions is actually an inherent problem.",
                    "label": 0
                },
                {
                    "sent": "You can't get rid of it as long as you're using convex loss functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we have data.",
                    "label": 0
                },
                {
                    "sent": "This is a particular distribution that they suggested that is linearly separable.",
                    "label": 0
                },
                {
                    "sent": "OK, so this line separates the blue points from the red point.",
                    "label": 0
                },
                {
                    "sent": "And we have a little bit more points here, and so this this is not a problem.",
                    "label": 0
                },
                {
                    "sent": "Now, what happens if we add label noise to it?",
                    "label": 0
                },
                {
                    "sent": "Let's say 10% label noise.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we now have in each one of these points we have the majority, as it was labeled before.",
                    "label": 0
                },
                {
                    "sent": "But there is a 10% minority that is labeled the opposite way.",
                    "label": 0
                },
                {
                    "sent": "OK, so in fact, in terms of what is the optimal thing, it's still the same thing.",
                    "label": 0
                },
                {
                    "sent": "This classifier is still the best classifier you can have for this data.",
                    "label": 0
                },
                {
                    "sent": "However, if you use convex loss function, something really strange will happen.",
                    "label": 0
                },
                {
                    "sent": "What will happen is that essentially these large margin examples, the mistakes, the noise.",
                    "label": 1
                },
                {
                    "sent": "There is very, very penalized very highly for it.",
                    "label": 0
                },
                {
                    "sent": "Why are you penalized very highly for it?",
                    "label": 0
                },
                {
                    "sent": "It's very far from the boundary, right?",
                    "label": 0
                },
                {
                    "sent": "The further you are from the boundary, the bigger the loss function, right?",
                    "label": 0
                },
                {
                    "sent": "So you're penalized very highly for those, and so the hyperplane doesn't want to be so far it will try to get close to those.",
                    "label": 0
                },
                {
                    "sent": "OK, there are many ways for it to get close.",
                    "label": 0
                },
                {
                    "sent": "So what we need is somehow to break the symmetry, and that's what the pullers are doing.",
                    "label": 0
                },
                {
                    "sent": "The pullers will pull the hyperplane in a particular direction.",
                    "label": 0
                },
                {
                    "sent": "And what will happen is that the penalizes these examples that are right here in the middle.",
                    "label": 0
                },
                {
                    "sent": "Those will get classified incorrectly.",
                    "label": 0
                },
                {
                    "sent": "OK, why would they get classified incorrectly?",
                    "label": 0
                },
                {
                    "sent": "Because they are so close to the boundary that however you turn the hyperplane, their loss on them is not going to be very large.",
                    "label": 0
                },
                {
                    "sent": "OK, So what will happen is that you'll get a classifier that is like this, or maybe going through them and then your error rate in terms of a classifier is very high.",
                    "label": 0
                },
                {
                    "sent": "And what they proved in a theorem is that for any convex loss function there exists a Lin.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really separable distribution.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Such that when independent label noise is added, the linear classifier that minimizes the loss function has very high classification error, so it doesn't really matter which convex function you give me.",
                    "label": 1
                },
                {
                    "sent": "Then I can always generate the distribution on which the thing that finds the global minimum seems to be very good is actually going to perform terribly.",
                    "label": 0
                },
                {
                    "sent": "So when real problem we're in real trouble.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm going to basically tell you is how we can essentially address this problem and we can minimize in this particular case, under some assumptions, whose characterization is a little bit of a problem yet, but we can actually minimize the non convex loss function.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this old algorithm called Boost by majority.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way that I think about this algorithm, I thought about it when I designed it is that you think about it as a game between a booster and a weak learner.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's an interactive game and the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rule that you're generating at the end is a very simple rule.",
                    "label": 0
                },
                {
                    "sent": "It's going to be a majority vote, just like in other boots, but it's going to be an unweighted majority vote, so all of the rules are going to get weight 1.",
                    "label": 0
                },
                {
                    "sent": "OK. And.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The most important thing is that the number of iterations of boosting is going to be set up ahead of time.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to decide I'm going to run this algorithm for 100 iterations, and that's it.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to stop and take the majority rule, and so that when you have a finite horizon that starts to give you a chance to talk about giving up, right?",
                    "label": 0
                },
                {
                    "sent": "If you don't.",
                    "label": 0
                },
                {
                    "sent": "If you're never going to stop, there's no reason to ever give up, but if you're getting close to the end and some examples, you're doing very badly, then, well, maybe you should give up on them, and that comes out.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so here is how officially how formally the game goes on.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each iteration, the booster assigns weights to the training examples, just like in regular boosting algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then the learner chooses a rule whose error with respect to the chosen weights is smaller than half minus gamma.",
                    "label": 1
                },
                {
                    "sent": "So I'm giving you these weights.",
                    "label": 0
                },
                {
                    "sent": "Now you need to give me a rule, right?",
                    "label": 0
                },
                {
                    "sent": "That's the usual way that you think about it.",
                    "label": 0
                },
                {
                    "sent": "Important thing is gamma is also fixed ahead of time.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and the rule then is added to the majority vote and you repeat that for tea.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Times.",
                    "label": 0
                },
                {
                    "sent": "OK, now the goal of the booster is to minimize the number of errors of the final majority rule on the training set.",
                    "label": 1
                },
                {
                    "sent": "OK, so we're not talking here about how well it will perform on a test set, just on the training set.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the first case of what I call a drifting game.",
                    "label": 1
                },
                {
                    "sent": "And here is kind of how you think about it as such.",
                    "label": 0
                },
                {
                    "sent": "You think about each.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample is being a chip.",
                    "label": 0
                },
                {
                    "sent": "OK, so each example is like one of these chips and each bin hold.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All of those examples on which the difference between the number of correct and incorrect base rules is at that point.",
                    "label": 1
                },
                {
                    "sent": "OK, so it's essentially S is the margin of the rule.",
                    "label": 0
                },
                {
                    "sent": "OK, now here's a small detail that initially seems very insignificant but becomes important in the analysis.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "It's it's useful to think about having huge number of chips.",
                    "label": 0
                },
                {
                    "sent": "OK, so one way of thinking about it.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is to just let the number of training examples go to Infinity?",
                    "label": 1
                },
                {
                    "sent": "That's one way of thinking about it.",
                    "label": 0
                },
                {
                    "sent": "A more useful way is actually to think about.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The examples is having continuous measure.",
                    "label": 0
                },
                {
                    "sent": "OK, so you just have a mass of examples of course.",
                    "label": 0
                },
                {
                    "sent": "Doesn't really quite make sense, but let's just go with it for now.",
                    "label": 0
                },
                {
                    "sent": "And that's the way that I'm kind of denoting it.",
                    "label": 0
                },
                {
                    "sent": "So instead of chips, I just have this mass and the thing that the mass gives me is basically it's very easy in this case to characterize what is the worst case.",
                    "label": 0
                },
                {
                    "sent": "What is the worst thing that the adversary that is the weak learner can do to me?",
                    "label": 0
                },
                {
                    "sent": "OK, in the other cases, it's not quite tight.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we have is this.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Game and this is what I call the lattice on which the game is played.",
                    "label": 0
                },
                {
                    "sent": "OK, so all of the examples start at zero and after the first rule is added.",
                    "label": 0
                },
                {
                    "sent": "Some examples are correct, some are incorrect.",
                    "label": 0
                },
                {
                    "sent": "OK, then after the second rule you have a vote of two or zero or minus two and so on.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what I'll decide is I'm going to have T this number of iterations be an odd number.",
                    "label": 0
                },
                {
                    "sent": "OK, because if it's even then we have this special case where at the end we have the same number of votes going correctly or one and 4 -- 1.",
                    "label": 0
                },
                {
                    "sent": "And then we have to break that.",
                    "label": 0
                },
                {
                    "sent": "I somehow so odd is much more convenient.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and here is how the game goes.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is really kind of the meat of it.",
                    "label": 0
                },
                {
                    "sent": "What you have here is all of the examples initially are at zero because we haven't had any rules and now we have the boot.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After assigning weight to the examples, essentially a density, now all of the examples are kind of the same, so really doesn't matter on this iteration.",
                    "label": 0
                },
                {
                    "sent": "How you assign density?",
                    "label": 0
                },
                {
                    "sent": "Let's just say that it assigns assigns weight of 1 to all of these examples.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the weak learner has to choose which examples it's going to classify correctly, in which examples not correctly, and we set gamma to 0.1, so it has to choose essentially at least zero point.",
                    "label": 0
                },
                {
                    "sent": "6 of these examples and predicting them correctly.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the dark blue set and what's going to happen then?",
                    "label": 0
                },
                {
                    "sent": "Is that these examples are going to move OK, so these are the correct ones.",
                    "label": 0
                },
                {
                    "sent": "They move up the in.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Correct, they moved out.",
                    "label": 0
                },
                {
                    "sent": "OK, now the situation is more interesting because now the booster has something to play.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It has two types of examples, so it's going to give essentially these examples, some weight and these examples some weight.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, these are the examples that the first one got incorrect, so it wants to give them more weight.",
                    "label": 0
                },
                {
                    "sent": "OK, and indeed it will turn out that this is the optimal weights for this particular three step game.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have more weights on these examples, less weights on these, and so now the.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Weak learner again has some freedom right?",
                    "label": 1
                },
                {
                    "sent": "It it it can put more mistakes here and less mistakes here because the mistakes here don't cost it that much.",
                    "label": 0
                },
                {
                    "sent": "Right they cost less in terms of the way to there.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say divides it like this and now the.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apples move another step, like here and now.",
                    "label": 0
                },
                {
                    "sent": "We're just before the last rule, so that's an interesting step to look at, because now.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at the booster, these are obviously the weights that it's going to assign.",
                    "label": 0
                },
                {
                    "sent": "So because these examples that you have two votes correct, they're going to stay correct at the end no matter what, how their predicted.",
                    "label": 0
                },
                {
                    "sent": "So it's going to give them weight 0 because it doesn't really care, but also these examples on which both are incorrect.",
                    "label": 0
                },
                {
                    "sent": "It's going to assign them wait zero because it gave up on them, right?",
                    "label": 0
                },
                {
                    "sent": "They can't become correct all of a sudden.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of very interesting.",
                    "label": 0
                },
                {
                    "sent": "It's the first time we see something that is very different from Adaboost.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is the optimal weight.",
                    "label": 0
                },
                {
                    "sent": "You put all of the weights on of those examples that make a difference at the end.",
                    "label": 0
                },
                {
                    "sent": "OK, so you do.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That now the weak learner of course will make all of these incorrect.",
                    "label": 0
                },
                {
                    "sent": "All of these incorrect, which you don't care, and then it will make 0.6 of those correct.",
                    "label": 0
                },
                {
                    "sent": "It has been.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and then you get this final configuration and.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you get in this final configuration is that on these examples, the majority rule is correct, and on these examples the majority.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All is incorrect.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the end of the game.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the interesting thing I'm just going to jump all the way to the end and tell you what is the thing that the weak learner, the one that splits the bins, should do in order to make the job as hard as possible for the booster.",
                    "label": 0
                },
                {
                    "sent": "OK, it turns out that it's very, very simple to characterize.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It chooses half plus gamma from each bin to be correct.",
                    "label": 1
                },
                {
                    "sent": "At every iteration, that's the min, Max, optimal it can do.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and another way to think about it is that the prediction of each base rule on each example is chosen independently at random to be correct with probability half plus gamma.",
                    "label": 1
                },
                {
                    "sent": "OK, that's basically the same statement just here.",
                    "label": 0
                },
                {
                    "sent": "We just talk about volume and here we talk about probability.",
                    "label": 0
                },
                {
                    "sent": "But in terms of really probability theory, it's the same statement.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is very surprising, right?",
                    "label": 0
                },
                {
                    "sent": "Basically this is kind of a brain dead weak learner.",
                    "label": 0
                },
                {
                    "sent": "It's just giving us the correct label each time with probability half plus gamma and and that's the best it can do.",
                    "label": 0
                },
                {
                    "sent": "It cannot make things worse than us now, WHI?",
                    "label": 0
                },
                {
                    "sent": "Why is it?",
                    "label": 0
                },
                {
                    "sent": "It's clear that this is something that that is will always work because no matter how much weight we put weights on the different bins, this will be correct with respect to our weights, half plus gamma of the time.",
                    "label": 0
                },
                {
                    "sent": "So the real question which remains is.",
                    "label": 0
                },
                {
                    "sent": "How can we basically force this algorithm not to be any better?",
                    "label": 0
                },
                {
                    "sent": "So let's say that it basically at some point decide so I actually am going to do something different, and so I'm going to actually give you at the end of majority rule that is less correct.",
                    "label": 0
                },
                {
                    "sent": "How can we basically force it not to be able to do that?",
                    "label": 0
                },
                {
                    "sent": "So in order to discuss this, I'm going to define something.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the potential intuitively let me go back intuitively, the way that we're going to do.",
                    "label": 0
                },
                {
                    "sent": "That is, we're going to say, OK, here is a weak learner, and it's going to do something worse than just have half plus gamma, half minus gamma.",
                    "label": 0
                },
                {
                    "sent": "Let me think that later on it will just do the right thing.",
                    "label": 0
                },
                {
                    "sent": "OK, just do the half plus gamma half minus gamma.",
                    "label": 0
                },
                {
                    "sent": "How can I make sure that on this step it can't gain anything OK and that comes back to the potential.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the potential is the probability mu probability is the probability according to this measure that I defined on the chips of the examples on which the final majority vote is incorrect.",
                    "label": 1
                },
                {
                    "sent": "Given the configuration after iteration T, is this configuration so it's a parameter and the remaining steps the learner is going to play add opt OK.",
                    "label": 1
                },
                {
                    "sent": "So so if it's going to play this way?",
                    "label": 0
                },
                {
                    "sent": "From a given configuration, then basically it doesn't really matter what I'll do.",
                    "label": 0
                },
                {
                    "sent": "You can calculate how many will be correct at the end OK, and that's this quantity that I defined here potential.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so interesting to look at.",
                    "label": 0
                },
                {
                    "sent": "What is the total potential?",
                    "label": 0
                },
                {
                    "sent": "Initially the total potential initially is, well, you have iteration zero and all of the examples are at the origin.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's the initial potential.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The final potential is simply the training error of the final majority rule.",
                    "label": 1
                },
                {
                    "sent": "OK, that's simply because at this point there is no more iterations and this is just the definition.",
                    "label": 0
                },
                {
                    "sent": "OK, so So what?",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to show you is that the boosting algorithm can choose weights in such a way that the total potential will not increase.",
                    "label": 1
                },
                {
                    "sent": "OK, so from iteration to iteration it will basically use the weights that it has control over so that no matter what the weak learner does, it cannot cause the potential to increase, right?",
                    "label": 1
                },
                {
                    "sent": "So if the initial potential is something that's basically going to be the final training error.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's what I said.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm going to define.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More break this up into pieces, so I'm going to talk about the potential of a particular bin.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have these bins indexed by S, so that's the fraction of examples in bin South after iteration T on which the final majority rule will be incorrect at the end.",
                    "label": 1
                },
                {
                    "sent": "OK, so for each bin.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something different, and if we define FTS to be the amount of chips that we have in business.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At iteration T, then the total potential is simply this sum.",
                    "label": 0
                },
                {
                    "sent": "Oh, it should be S down here.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's the sum of FTS times the potential TS.",
                    "label": 0
                },
                {
                    "sent": "That's all it is, right?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now important thing is that the TTS does not depend on the configuration, is just a number that is per bin and depends only on TNS, and we can write it in closed form.",
                    "label": 1
                },
                {
                    "sent": "OK, So what it is, it's simply the binomial distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's deep.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "TS minus half plus gamma right?",
                    "label": 0
                },
                {
                    "sent": "So if it plays optimally, all of the DTS is are going to be exactly 0, but it might not want to play what we think is optimally so, so.",
                    "label": 0
                },
                {
                    "sent": "So that's how we measure how we characterize what the weak learner does.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "WTS I'm just going to define it at this point to be the difference between the two potentials at the next step.",
                    "label": 0
                },
                {
                    "sent": "OK, this is going to be actually the weights that we used to give to the examples, But at this point is just.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's just this difference.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the theorem.",
                    "label": 0
                },
                {
                    "sent": "If DTS WTS is larger equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So basically this is exactly characterizing that the weak learner is giving us a rule that has error smaller than half minus gamma or at most half minus gamma, then the potential does not increase.",
                    "label": 0
                },
                {
                    "sent": "OK, it can only decrease.",
                    "label": 0
                },
                {
                    "sent": "And the corollary to that if we basically.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Caffeinated from Step 0 all the way to the end is that for all T if for all T the weighted error of HD is smaller than half minus gamma, then the initial potential is larger equal to the final training error.",
                    "label": 1
                },
                {
                    "sent": "OK, that basically will give us the theorem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the proof.",
                    "label": 0
                },
                {
                    "sent": "So the proof is like this.",
                    "label": 0
                },
                {
                    "sent": "It's pretty easy to calculate FT plus one at the next time for bin S as a function of FT and S -- 1 and FS plus one in the DTS.",
                    "label": 0
                },
                {
                    "sent": "Right, because basically that's just calculating how many go up, what fraction goes up, what fraction goes down and that just is the formula.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we want to calc.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait, the potential at the next step.",
                    "label": 0
                },
                {
                    "sent": "We can just use the equation that I told you before and plug in this equation for FT. Oak.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can just do some algebra and it turns out that we can rewrite this sum as a slightly different somewhere.",
                    "label": 0
                },
                {
                    "sent": "FTS appears only once, and ETS appears only once and the fees are now added and subtracted.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we see is that this term that is here is simply 50 at S and this term here is simply double.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Atheist.",
                    "label": 0
                },
                {
                    "sent": "OK, so when we write it out, we get that.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "50 + 1 The total potential is exactly the previous potential minus DTZ WTS.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so if DTS sum of DTS WTS larger or equal to 0, then 50 + 1 is smaller equal to 50.",
                    "label": 0
                },
                {
                    "sent": "That's that's the proof.",
                    "label": 0
                },
                {
                    "sent": "So it's just algebra.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once you have the right, the right quantity.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is kind of a different way of saying it's setting the boosting weights at iteration T to be this horrible expression that is just the difference between two binomial tails.",
                    "label": 0
                },
                {
                    "sent": "So it's a binomial term then.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we get the final error, is the binomial for TT over two half plus gamma, which this is a function that goes down very quickly with T. OK, so that's basically the guarantee.",
                    "label": 0
                },
                {
                    "sent": "OK, so we know that it's a boosting algorithm, but in what way does it look so different than other boosts?",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the way.",
                    "label": 0
                },
                {
                    "sent": "So what I plot here is the weight and the potential.",
                    "label": 0
                },
                {
                    "sent": "So remember, the weight is essentially the derivative of the potential, so this is the weight and this is the potential initially when you're at T1.",
                    "label": 0
                },
                {
                    "sent": "Remember, you're only around here, so you just see an increasing potential and is increasing weight function.",
                    "label": 0
                },
                {
                    "sent": "But as you go to T ill.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seven and the total number of iterations here is 100 as you go.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get closer, you see that on some example, so that the weight function goes up and up and up and up reaches a maximum, then goes down.",
                    "label": 0
                },
                {
                    "sent": "Right, so you start to basically give up on examples that you got too many times incorrectly.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you go on and on until it iteration 101.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is the last iteration.",
                    "label": 0
                },
                {
                    "sent": "You basically are concentrating only on those examples in which exactly half got it correct in half got it incorrect, which is what I started with.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's all good and well and we're indeed minimizing a non convex loss function.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we compare it to the previous boosting algorithm, you see that it's very different.",
                    "label": 0
                },
                {
                    "sent": "So here I plot actually for all of these algorithms, it's useful to look at both the weights and the potential for other boost.",
                    "label": 0
                },
                {
                    "sent": "It's kind of not very informative because the potential and the weight RE to the minus X, so they're kind of interchangeable.",
                    "label": 0
                },
                {
                    "sent": "However, for logic boost whose potential function is this log of 1 + E to the minus by logic boost is the same as Tibshirani and Friedman.",
                    "label": 0
                },
                {
                    "sent": "Gentle, gentle adaboost.",
                    "label": 0
                },
                {
                    "sent": "Then the weight function is this, and this weight function is a SIG model, so it basically it increases, but then it stops increasing.",
                    "label": 0
                },
                {
                    "sent": "OK, because the potential function becomes linear.",
                    "label": 0
                },
                {
                    "sent": "Our function is even more aggressive.",
                    "label": 0
                },
                {
                    "sent": "It increases and then it reaches a maximum, then it decreases.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's very different.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the high level some.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three is that the worst case adversary splits each bin into half, minus gamma, incorrect and half plus gamma, correct?",
                    "label": 0
                },
                {
                    "sent": "And you can think.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this is random walk with IID steps.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just a random walk with probability half plus gamma of going up.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Minus game of going down.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm is derived as the optimal response to the simple worst case adversary.",
                    "label": 1
                },
                {
                    "sent": "So this seems like brain dead adversary.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that it's the worst adversary.",
                    "label": 0
                },
                {
                    "sent": "This is something that is kind of surprising but true.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I'm going to talk about boosting in continuous time.",
                    "label": 0
                },
                {
                    "sent": "So the question is.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why why continuous time?",
                    "label": 0
                },
                {
                    "sent": "What's what's the big deal with that?",
                    "label": 0
                },
                {
                    "sent": "So the deal is that boost by majority was invented before Adaboost.",
                    "label": 0
                },
                {
                    "sent": "So how come we're not using boost by majority rather than other boosts?",
                    "label": 0
                },
                {
                    "sent": "Well, the simplest answer is that it's much more.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complex.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a very good answer by itself.",
                    "label": 0
                },
                {
                    "sent": "However, you know we can overcome complexity if we really.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have a good reason.",
                    "label": 0
                },
                {
                    "sent": "So the more the deeper reason is that boost by majority needs to know epsilon and gamma before starting right.",
                    "label": 1
                },
                {
                    "sent": "So Gamma needs to be known and epsilon what we're shooting for needs to be known and those are never known ahead of time.",
                    "label": 0
                },
                {
                    "sent": "Worse than that, the dependence, the number of iterations we need to run depends on these parameters like this in this way.",
                    "label": 0
                },
                {
                    "sent": "OK, so especially problematic is the dependence on the advantage over random guessing.",
                    "label": 0
                },
                {
                    "sent": "So if that is just let's say 1%, then we're committing ahead of time before anything starts to run this algorithm for at least 10,000 iterations.",
                    "label": 0
                },
                {
                    "sent": "So in practice this is just not a reasonable thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so why did Adaboost catch the stage?",
                    "label": 0
                },
                {
                    "sent": "Because other boost who stands for adaptive boosting adapts to the sequence of these advantages.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what that gives you is there is no need to set parameters in advance.",
                    "label": 1
                },
                {
                    "sent": "That's a very very big deal.",
                    "label": 0
                },
                {
                    "sent": "You don't know it until you until you publish it.",
                    "label": 0
                },
                {
                    "sent": "You know basically you publish an algorithm with no parameters.",
                    "label": 0
                },
                {
                    "sent": "People are actually going to try it.",
                    "label": 0
                },
                {
                    "sent": "When you publish an algorithm with parameters, somebody else is going to try.",
                    "label": 0
                },
                {
                    "sent": "And it generates a weight.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Majority rule OK, so these are the big advantages that we want to do and you don't need to decide ahead of time how many iterations to run.",
                    "label": 0
                },
                {
                    "sent": "You just stop using cross validation and in in boost majority we somehow decided ahead of time when to stop, which is kind of strange thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so the real question is how can we make?",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's by majority adaptive and the idea is that.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For that, we're going to continue this time.",
                    "label": 0
                },
                {
                    "sent": "OK, so how much time at 47 minutes?",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "OK, so we let the time.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Step go to 0.",
                    "label": 0
                },
                {
                    "sent": "So let me read it.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rate the number of iterations we require by boost majority is one over gamma squared log of one over epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the idea is that we're going to keep epsilon fixed.",
                    "label": 0
                },
                {
                    "sent": "That's the error that we're shooting for, and then we're going to let gamma this gamma, which is the advantage go to 0, right?",
                    "label": 0
                },
                {
                    "sent": "Because somehow if we have a weak learner that can give us some gamma, let's say 1%, of course it can give us a smaller gamma.",
                    "label": 0
                },
                {
                    "sent": "Well, let's say a 10th of a percent.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to let gamma go to zero, but that would require us to make the number of iterations go to Infinity.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's a bother something but will get over it.",
                    "label": 0
                },
                {
                    "sent": "And and what is going to happen when we do that?",
                    "label": 0
                },
                {
                    "sent": "Right now we're doing it.",
                    "label": 0
                },
                {
                    "sent": "Just a thought experiment.",
                    "label": 0
                },
                {
                    "sent": "So think about it, the gamma is very, very small and we're getting let's say, one.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we're getting a rule whose error is, let's say, 4040%.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's much better than the 49% that is required.",
                    "label": 0
                },
                {
                    "sent": "So we add this rule.",
                    "label": 0
                },
                {
                    "sent": "Almost nothing changes, right?",
                    "label": 0
                },
                {
                    "sent": "The distribution changes a tiny bit, and so on.",
                    "label": 0
                },
                {
                    "sent": "So we go, we check the same rule against this distribution.",
                    "label": 1
                },
                {
                    "sent": "Well, it still has maybe 41% error, so we had it again and we add it again and we add it again.",
                    "label": 0
                },
                {
                    "sent": "And when we gamma is very, very small, will add it a huge number of times until basically it becomes something that is not good enough.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That yields essentially an adaptive boosting and awaited majority rule, right?",
                    "label": 0
                },
                {
                    "sent": "Because what is it?",
                    "label": 0
                },
                {
                    "sent": "We added the same rule 55 times.",
                    "label": 0
                },
                {
                    "sent": "We ran it totally for 10,000 times, so that's whatever 5.",
                    "label": 0
                },
                {
                    "sent": "5 1/2%.",
                    "label": 0
                },
                {
                    "sent": "And so that's the waiting.",
                    "label": 0
                },
                {
                    "sent": "And it's adaptive because really, we didn't.",
                    "label": 0
                },
                {
                    "sent": "We asked for so little that we.",
                    "label": 0
                },
                {
                    "sent": "That's like asking for nothing at all, right?",
                    "label": 0
                },
                {
                    "sent": "We're asking for such a tiny gamma that any algorithm that you give me that it's like any rule that you give me that is slightly better than random guessing I can use and I'll just give it weight according to this heuristic.",
                    "label": 0
                },
                {
                    "sent": "OK so but the problem is this is not realistic to actually do right because because you have to run for so many iterations.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we want to do is we want to somehow calculate exactly how many times we're going to add a rule, given that you give it to me.",
                    "label": 0
                },
                {
                    "sent": "So in order to calculate that, then we need to somehow characterize what's going on.",
                    "label": 0
                },
                {
                    "sent": "Now remember what's the adversary doing?",
                    "label": 0
                },
                {
                    "sent": "Is just doing a random walk right?",
                    "label": 0
                },
                {
                    "sent": "So how do you characterize a random walk when you make the number of steps go to Infinity?",
                    "label": 0
                },
                {
                    "sent": "Well, that is something called Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "OK, and I'll tell you why it's Brownian motion in a minute, 'cause there's something very non intuitive about Brownian motion that if you don't really look into it, is easy to miss.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just technically, instead of T going from one to T going to Infinity, we want to keep this control.",
                    "label": 0
                },
                {
                    "sent": "We don't want to go to Infinity, so we're going just going from T0 to T1 and we're going to go in little steps 1 / T.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here is the original game lattice as I drove it before, so in this case we have T = 3 and T is one or two or three.",
                    "label": 0
                },
                {
                    "sent": "Now we just.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to go instead T Go third 2 third one.",
                    "label": 0
                },
                {
                    "sent": "OK, now the real question is how do we make the steps of the score?",
                    "label": 0
                },
                {
                    "sent": "How do we make the steps as we go along?",
                    "label": 0
                },
                {
                    "sent": "So the natural thing is to make.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Delta S How much we changes also go like 1 / T. Right, so that seems reasonable.",
                    "label": 0
                },
                {
                    "sent": "So if we do this is for one step, then we go up and down like this.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For two steps.",
                    "label": 0
                },
                {
                    "sent": "We take.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three steps and we also end up at plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "49 steps.",
                    "label": 0
                },
                {
                    "sent": "We go like this and we end up either somewhere between plus one and minus one.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And all of that looks very natural and good.",
                    "label": 0
                },
                {
                    "sent": "However, it's a real problem when you think about the random walk without going over that.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because the variance of where you end up it goes is T steps times the variance of one step which is 1 / T squared and that gives you 1 / T. So as T goes to Infinity, now you have that the variance goes to 0.",
                    "label": 0
                },
                {
                    "sent": "So in fact, if you use this kind of steps you get nonsense.",
                    "label": 0
                },
                {
                    "sent": "You get basically that at the limit of continuous time the everything stays at zero all the time.",
                    "label": 0
                },
                {
                    "sent": "Very strange, but again, that's the way it is.",
                    "label": 0
                },
                {
                    "sent": "So in order to overcome this you need to do something that is very very non intuitive.",
                    "label": 0
                },
                {
                    "sent": "But people in stochastic processes do all the time.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is to say that Delta S goes like 1 / sqrt T. OK, that's the only way that it would work, and so here is 1.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Step again, just like before, but for three steps, so the very.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This here is 1.",
                    "label": 0
                },
                {
                    "sent": "4th",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The steps we take, Delta S is equal to 1 / sqrt 3, so we go not between plus one and minus one like before.",
                    "label": 0
                },
                {
                    "sent": "Now we go between plus sqrt 3 and minus sqrt 3.",
                    "label": 0
                },
                {
                    "sent": "OK. And the nice thing is that.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The variance is still 3 * 1 / 3 it's one, so the variance stayed the same.",
                    "label": 0
                },
                {
                    "sent": "If we go another step.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We get.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This weird thing that that we go in.",
                    "label": 0
                },
                {
                    "sent": "In each step we have one we have 1/3 which is the square root of a 9.",
                    "label": 0
                },
                {
                    "sent": "And 1 / sqrt T and so we go between 3:00 and minus minus three.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the variance is still one.",
                    "label": 0
                },
                {
                    "sent": "So this is how we're going to do things.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take the limit in this kind of strange way in order to make things work and the variance will stay as we want, but on the other hand, the range will go to Infinity, right?",
                    "label": 0
                },
                {
                    "sent": "So in epsilon time the this chip can go to minus arbitrarily big or arbitrarily small or plus arbitrarily big.",
                    "label": 0
                },
                {
                    "sent": "OK, and that is very unintuitive.",
                    "label": 0
                },
                {
                    "sent": "But again, that's exactly how Brownian motion is.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this is I don't think that I can go now through the detailed analysis of it, but I'll just tell you on the high level.",
                    "label": 0
                },
                {
                    "sent": "How does it?",
                    "label": 0
                },
                {
                    "sent": "How does the math now look?",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you go to this.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "So before in discrete time we had equations that relate time T to time T + 1 based on random walks.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's how it works when we.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To discrete time.",
                    "label": 0
                },
                {
                    "sent": "When you do continuous time, you have to do use differential equations that describe the density evolution for Brownian motion with drift.",
                    "label": 1
                },
                {
                    "sent": "OK, it's just that's the that's what you get in the limit, and these are very well known equations.",
                    "label": 0
                },
                {
                    "sent": "They're called the Commodore forward and backward equations for people that know stochastic processes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do these, for instance look like so in?",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By majority that was the iteration like that.",
                    "label": 0
                },
                {
                    "sent": "That's how the recursion that relates potential at time T and two time T -- 1.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are the boundary condition in continuous.",
                    "label": 0
                },
                {
                    "sent": "That's an algorithm called Brown Boost.",
                    "label": 0
                },
                {
                    "sent": "You get that this is the recursion.",
                    "label": 0
                },
                {
                    "sent": "OK, so the recursion now is a differential equation of 2nd order.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the boundary conditions are actually exactly the same as before.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so basically we can do the whole thing in continuous time, but now instead of what we have to do in the single step of boost by majority of Brown boost is to solve 2 nonlinear equations with two unknowns.",
                    "label": 0
                },
                {
                    "sent": "Which is something that actually computationally I have no idea how computationally hard it is, but practically you can do it with Newton method.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we're going to talk about robust boots, so this Brown boots that something I published maybe three or four years ago.",
                    "label": 0
                },
                {
                    "sent": "It still wasn't working.",
                    "label": 0
                },
                {
                    "sent": "You know it was very nice theory, but it wasn't working.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so robust boost is the one that finally works.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So instead of minimizing the training error, what we want to minimize in this case is the examples whose margins smaller equal to Theta number.",
                    "label": 1
                },
                {
                    "sent": "There was something in the background going on that boosting.",
                    "label": 0
                },
                {
                    "sent": "If you know tends not to overfit and there is an explanation based on margin, and here I basically put my chips on this explanation and I'm saying no.",
                    "label": 1
                },
                {
                    "sent": "What we really want to minimize is not just the number of mistakes we want to minimize the number of examples whose margin is smaller than something it's essentially like.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines, but it's support vector machines where you give up on examples who have who fall below the margin.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what you have to do when you do this is you have to control the weights.",
                    "label": 0
                },
                {
                    "sent": "You have to control the norm the way that it's done in support vector machines you control the L2 norm of your examples.",
                    "label": 0
                },
                {
                    "sent": "Here we're doing the control of the weights in a completely different way.",
                    "label": 0
                },
                {
                    "sent": "We're controlling instead of controlling the range with controlling the variance of the scores.",
                    "label": 1
                },
                {
                    "sent": "And it was one last thing is we want to allow confidence rated.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weak learners, so we want the weak learner not to be able to just say plus one or minus one, but any number in between.",
                    "label": 0
                },
                {
                    "sent": "OK, so experimental results.",
                    "label": 0
                },
                {
                    "sent": "Just what today?",
                    "label": 0
                },
                {
                    "sent": "Yeah so so just just to kind of overwhelm you.",
                    "label": 0
                },
                {
                    "sent": "This is what you get.",
                    "label": 0
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When you do all of that, it takes.",
                    "label": 0
                },
                {
                    "sent": "It takes awhile to to understand it.",
                    "label": 0
                },
                {
                    "sent": "It takes even longer to encode it, but to program this.",
                    "label": 0
                },
                {
                    "sent": "But we did it.",
                    "label": 0
                }
            ]
        },
        "clip_129": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_130": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the back to the experimental result.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is first the experiment results on long Sir video.",
                    "label": 1
                },
                {
                    "sent": "OK, so remember the data that I showed you that they proved that nothing that is convex can actually minimize it.",
                    "label": 0
                },
                {
                    "sent": "Can actually find the good classifier.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                }
            ]
        },
        "clip_131": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Movies this is a movie about using Adaboost and.",
                    "label": 0
                }
            ]
        },
        "clip_132": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You see here is that this iteration.",
                    "label": 0
                },
                {
                    "sent": "You see the number up here is still going on, but boosting is not really.",
                    "label": 0
                },
                {
                    "sent": "It's stuck in this point, right?",
                    "label": 0
                },
                {
                    "sent": "These are all the penalising examples, it's slightly more complex than what I showed you before.",
                    "label": 0
                },
                {
                    "sent": "It's actually a binary set up, but I'm not going to go to details essentially the same as what I showed you and what you see is that basically after a small number of iterations it separates pretty well the large margin and the.",
                    "label": 0
                },
                {
                    "sent": "Colors but the penal izers are stuck in the middle and it cannot differentiate them.",
                    "label": 0
                },
                {
                    "sent": "And it doesn't get much better when you use logic boost a little bit better because it puts a little bit less weight on these bad examples, right?",
                    "label": 0
                },
                {
                    "sent": "These are the examples that hurt you.",
                    "label": 0
                },
                {
                    "sent": "These are the 10% error that is added to the data and these are the examples in which you put a huge amount of weight which misleads you.",
                    "label": 0
                },
                {
                    "sent": "OK, and here is what happens with robust Boo, so it's interesting what you see is this is the potential function you see that at the beginning it's very much the same, but getting now to the end it completely ignores these examples out here and it concentrates more and more on the examples in the middle and eventually it gets them all correct.",
                    "label": 0
                },
                {
                    "sent": "So in fact in the rocks are video long problem, it finds exactly the correct underlying classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This this might seem like something that nobody can reproduce, but you can reproduce it.",
                    "label": 0
                }
            ]
        },
        "clip_133": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we put a lot of effort into making this into open source.",
                    "label": 0
                },
                {
                    "sent": "It's called J Boost and I would really like you to download it and try try it out.",
                    "label": 0
                },
                {
                    "sent": "This is the website you can just Google on J Boost.",
                    "label": 0
                }
            ]
        },
        "clip_134": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is a result using this implementation.",
                    "label": 0
                },
                {
                    "sent": "And these are the results I showed you before and now let me show you how what happens.",
                    "label": 0
                }
            ]
        },
        "clip_135": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What happens when you that was a sudden end to my talk so?",
                    "label": 0
                },
                {
                    "sent": "So this was, this is what happens when you run logic boost.",
                    "label": 0
                },
                {
                    "sent": "OK, so logic boost is a little bit better than other boosts and here is what happens.",
                    "label": 0
                }
            ]
        },
        "clip_136": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Essentially, it's very similar to what you saw before.",
                    "label": 0
                }
            ]
        },
        "clip_137": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You don't see the red here, almost not OK you see.",
                    "label": 0
                }
            ]
        },
        "clip_138": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little bit of the red here, but what happens is that that it gets to some point, but beyond that point it doesn't really improve.",
                    "label": 0
                }
            ]
        },
        "clip_139": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is iteration 100, iteration 2000, so oh sorry, I'm completely mistaken.",
                    "label": 0
                },
                {
                    "sent": "This is the case where you have zero noise.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case everything works nicely.",
                    "label": 0
                },
                {
                    "sent": "In this.",
                    "label": 0
                }
            ]
        },
        "clip_140": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everything works nicely.",
                    "label": 0
                }
            ]
        },
        "clip_141": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But here is logic both with 20% noise and.",
                    "label": 0
                }
            ]
        },
        "clip_142": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you see here is that Nomad.",
                    "label": 0
                }
            ]
        },
        "clip_143": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or how much you run.",
                    "label": 0
                }
            ]
        },
        "clip_144": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, there is no Sir.",
                    "label": 0
                }
            ]
        },
        "clip_145": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Operation, it doesn't manage to even in 2000 iterations it doesn't really manage to separate the positive from the negative, which is what you saw at the end.",
                    "label": 0
                },
                {
                    "sent": "And what's the reason?",
                    "label": 0
                },
                {
                    "sent": "The reason is that 20% of these examples that are positive actually have negative label, and you can try to get them in the correct side because they are inherently incorrect.",
                    "label": 0
                },
                {
                    "sent": "Here is what robust boost does on that so.",
                    "label": 0
                }
            ]
        },
        "clip_146": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So initially it does very similar stuff.",
                    "label": 0
                }
            ]
        },
        "clip_147": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But then.",
                    "label": 0
                }
            ]
        },
        "clip_148": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When you get to this step, it basically gives up on all of the examples that are out here or out here.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the potential function whose gradient is the weight function.",
                    "label": 0
                },
                {
                    "sent": "OK, so only examples from here.",
                    "label": 0
                }
            ]
        },
        "clip_149": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To hear it, and now it's even only examples from here to here.",
                    "label": 0
                },
                {
                    "sent": "So all of these examples out here whether there are labeled plus or blue or red, they're ignored.",
                    "label": 0
                },
                {
                    "sent": "They're completely new.",
                    "label": 0
                }
            ]
        },
        "clip_150": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look, then and now we go even further and we push those.",
                    "label": 0
                }
            ]
        },
        "clip_151": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples further.",
                    "label": 0
                }
            ]
        },
        "clip_152": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then finally, ah.",
                    "label": 0
                }
            ]
        },
        "clip_153": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After 430 iterations, it's.",
                    "label": 0
                }
            ]
        },
        "clip_154": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One and you see that it found a large margin.",
                    "label": 0
                },
                {
                    "sent": "Classification.",
                    "label": 0
                },
                {
                    "sent": "And but it gave up.",
                    "label": 0
                },
                {
                    "sent": "So you see, all of these blue ones here, and there's a lot of red ones here.",
                    "label": 0
                },
                {
                    "sent": "Hopefully, yeah, red ones here and those those are the mislabeled examples that it gave up on so that it could find the correct majority rule.",
                    "label": 0
                },
                {
                    "sent": "So this is, I think, justifying the explanation, the long and tedious explanation that I gave you before.",
                    "label": 0
                },
                {
                    "sent": "OK, so pretty much I'm done with that.",
                    "label": 0
                }
            ]
        },
        "clip_155": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lock",
                    "label": 0
                }
            ]
        },
        "clip_156": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just wanted to say there is much more details in the papers.",
                    "label": 1
                },
                {
                    "sent": "This is essentially the 1st paper that is about this.",
                    "label": 0
                },
                {
                    "sent": "Everything that I talked about before today.",
                    "label": 0
                }
            ]
        },
        "clip_157": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then unfortunately it got rejected from ICM L 09 so I had to give a talk here.",
                    "label": 0
                },
                {
                    "sent": "But that's not so something to say, particularly in ICML 09.",
                    "label": 0
                },
                {
                    "sent": "This is another paper that we did using drifting games for online learning, and that was rejected from.",
                    "label": 0
                }
            ]
        },
        "clip_158": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Called and this is a paper based on this paper and that was.",
                    "label": 0
                }
            ]
        },
        "clip_159": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ejected from UI.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Equally rejected from everybody.",
                    "label": 0
                },
                {
                    "sent": "This one was not rigid.",
                    "label": 0
                }
            ]
        },
        "clip_160": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe because I didn't submit it yet.",
                    "label": 0
                },
                {
                    "sent": "You can get all of these papers from my website.",
                    "label": 0
                },
                {
                    "sent": "So to summer.",
                    "label": 0
                }
            ]
        },
        "clip_161": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guys don't get too upset.",
                    "label": 0
                }
            ]
        },
        "clip_162": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If your paper is rejected.",
                    "label": 0
                },
                {
                    "sent": "Because you might present it.",
                    "label": 0
                }
            ]
        },
        "clip_163": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Invited speaker.",
                    "label": 0
                }
            ]
        },
        "clip_164": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_165": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also tried a boost.",
                    "label": 0
                },
                {
                    "sent": "J Boost is pretty cool.",
                    "label": 0
                },
                {
                    "sent": "I think you should give it a spin.",
                    "label": 0
                },
                {
                    "sent": "And please ask me questions.",
                    "label": 0
                },
                {
                    "sent": "There is no time for questions as there is Oh my God.",
                    "label": 0
                },
                {
                    "sent": "OK, so ask me questions now, but if you don't get to ask me Now, please write me email.",
                    "label": 0
                },
                {
                    "sent": "I'm very eager and specially I'm going to stay here till Thursday.",
                    "label": 0
                },
                {
                    "sent": "So the best thing is you write an email say I'd like to meet and talk with you to understand what you're talking about and and we'll just find the time and talk.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_166": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "We do actually have time for it.",
                    "label": 0
                }
            ]
        },
        "clip_167": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, back there.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "An algorithm that does tagging like random forest compactor boosting up.",
                    "label": 0
                },
                {
                    "sent": "So that's a very good question.",
                    "label": 0
                },
                {
                    "sent": "We're trying it right now, so bagging definitely does better than boosting.",
                    "label": 0
                },
                {
                    "sent": "However, it doesn't do well when your base classifiers are very restricted, right?",
                    "label": 0
                },
                {
                    "sent": "So so essentially, in the problem of of.",
                    "label": 0
                },
                {
                    "sent": "Long answer video.",
                    "label": 0
                },
                {
                    "sent": "It's not going to do anything good because each time it's going to give you the same bad weak hypothesis because you have enough examples.",
                    "label": 0
                },
                {
                    "sent": "In other cases we actually found that random forest works better in this particular one that I showed you here.",
                    "label": 0
                },
                {
                    "sent": "We tried various and it turned out that the best one was Booth was bagging C 4.5.",
                    "label": 0
                },
                {
                    "sent": "So yes, so there are other algorithms and there's definitely something more going on with bagging that I still don't completely understand.",
                    "label": 0
                },
                {
                    "sent": "And that I would like to incorporate so, but what it's doing is very, very different right?",
                    "label": 0
                },
                {
                    "sent": "Because because if you basically just talk about this video and the long answer video data set, I don't know of any algorithm that would be able to get that right.",
                    "label": 0
                },
                {
                    "sent": "However, if you try to learn that same thing with decision trees, it turns out you can, but that's kind of like going going around different way so.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so that's kind of still open.",
                    "label": 0
                },
                {
                    "sent": "What's there more things that we don't understand, yeah.",
                    "label": 0
                },
                {
                    "sent": "With the back set of plastic box, you have an option to select from that set using a test and you can actually collect rebuilding ensemble Diane.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's true, but that still doesn't tell you how to get rid of noisy examples.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "It's not quite the same thing, yes.",
                    "label": 0
                },
                {
                    "sent": "And explicitly have variables to late.",
                    "label": 0
                },
                {
                    "sent": "Oh oh so so mixture models mixture models when you have a large number of dimensions and a small amount of data overfit terribly.",
                    "label": 0
                },
                {
                    "sent": "So so basically if you really try to learn the underlying true distribution, then then you might be successful.",
                    "label": 0
                },
                {
                    "sent": "If the distribution is really like what you hoped it is, but if the distribution is different then there's no guarantee whatsoever.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Goes down again.",
                    "label": 0
                },
                {
                    "sent": "You don't want to keep realizing things would be more and more on the wrong side of the frontier.",
                    "label": 0
                },
                {
                    "sent": "Another one side?",
                    "label": 0
                },
                {
                    "sent": "Well, maybe intuitively, but the fact is that OK, so that's exactly what we do when we remove outliers, right?",
                    "label": 0
                },
                {
                    "sent": "We basically say this example.",
                    "label": 0
                },
                {
                    "sent": "We suffer on it.",
                    "label": 0
                },
                {
                    "sent": "Let's say in regression case so much we just wanted to pretend it's not there.",
                    "label": 0
                },
                {
                    "sent": "So no, in regression in particular because it's so sensitive to outliers, that's exactly what we do now.",
                    "label": 0
                },
                {
                    "sent": "The fact that we don't know how to analyze it is separate, but intuitively it makes sense, and it works.",
                    "label": 0
                },
                {
                    "sent": "So that's what people do.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Sonny concerns me, which is seems like in some sense you're you're learning to ignore a part of the problem space, right?",
                    "label": 0
                },
                {
                    "sent": "This algorithm into filtering situation where we have an Oracle with an infinite amount of data.",
                    "label": 0
                },
                {
                    "sent": "And so maybe maybe things are changing slowly.",
                    "label": 0
                },
                {
                    "sent": "Would you be able to track an on line situation?",
                    "label": 0
                },
                {
                    "sent": "Well, I don't have anything theoretical to say about it at this point, but actually there's there's a string of algorithms called online boosting which people have used and and quite successfully, and I believe that this can be used in online boosting just as well.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, if you basically are starting to give up on things that are that are that eventually become become different then, then you need to somehow overcome that.",
                    "label": 0
                },
                {
                    "sent": "So what you would essentially need to do.",
                    "label": 0
                },
                {
                    "sent": "Is you'd need to rerun the algorithm from from scratch from time to time.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "More questions, yes.",
                    "label": 0
                },
                {
                    "sent": "Big problem here.",
                    "label": 0
                },
                {
                    "sent": "Sometimes look better, so it's going to fail sometimes, right?",
                    "label": 0
                },
                {
                    "sent": "I don't know yet.",
                    "label": 0
                },
                {
                    "sent": "I don't know yet.",
                    "label": 0
                },
                {
                    "sent": "It's obviously going to fail on some on some distribution.",
                    "label": 0
                },
                {
                    "sent": "My intuition is the following.",
                    "label": 0
                },
                {
                    "sent": "There is, there is a kind of assumption that we usually make in learning which is never has been made explicit, which is that if we learn a good rule using, let's say, the best rule using a small amount of training data.",
                    "label": 0
                },
                {
                    "sent": "It's not very far from the good rule, using a lot of training data now none of our setups actually capture that, but I believe that under that assumption that essentially if you look at your empirical error surface and you add more and more examples, it's not like initially you see a minimum and then you see that the minimum is in a completely different place.",
                    "label": 0
                },
                {
                    "sent": "You just see that the minimum becomes finer and finer, and you know exactly where to put things.",
                    "label": 0
                },
                {
                    "sent": "So essentially, after a small amount of training data you can eliminate.",
                    "label": 0
                },
                {
                    "sent": "A lot of your version space if you will.",
                    "label": 0
                },
                {
                    "sent": "Even though even though you know you might kind of potentially be illuminating apart that when you had a lot of data, would be would be correct.",
                    "label": 0
                },
                {
                    "sent": "So I think that that's the underlying assumption.",
                    "label": 0
                },
                {
                    "sent": "I don't know yet how to formalize it, so at this point it's just kind of an intuition.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Very good question.",
                    "label": 0
                },
                {
                    "sent": "I think that the reason is that the that the program committee didn't understand it so.",
                    "label": 0
                },
                {
                    "sent": "So, so that is that is actually, I think the true reason.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you understood it any better.",
                    "label": 0
                },
                {
                    "sent": "However, the advantage of the situation that urine is that you can actually download the software and try it right.",
                    "label": 0
                },
                {
                    "sent": "So I think that the big advantage of Adaboost was not so much that it had great theory, but it was a one page algorithm.",
                    "label": 0
                },
                {
                    "sent": "So people just said, oh, I can code that and they coded it in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "This is too complicated for people to just code in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "But you can download it so.",
                    "label": 0
                },
                {
                    "sent": "So why it was rejected?",
                    "label": 0
                },
                {
                    "sent": "I don't know why I see it as a badge of honor.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Do you still do after?",
                    "label": 0
                },
                {
                    "sent": "Oh yes, active learning is one of the directions I really want to pursue with this and we've actually started.",
                    "label": 0
                },
                {
                    "sent": "So here's the interesting thing that happens when you get close to the end.",
                    "label": 0
                },
                {
                    "sent": "Is that basically any example that you get very correct or very incorrect?",
                    "label": 0
                },
                {
                    "sent": "Get 0 weight.",
                    "label": 0
                },
                {
                    "sent": "So that's interesting, right?",
                    "label": 0
                },
                {
                    "sent": "Because basically that means that for those examples we don't need to know the label 'cause we're not going to look at the label anyway, so in fact we're starting to use that, and it seems impractical problems that we're working on right now envision that it actually works pretty well now.",
                    "label": 0
                },
                {
                    "sent": "I don't have any theorems to show about it yet.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I think that it's going to be very useful thing for active learning.",
                    "label": 0
                },
                {
                    "sent": "Anymore.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Papers about.",
                    "label": 0
                },
                {
                    "sent": "Oh, OK, thank you for the question.",
                    "label": 0
                },
                {
                    "sent": "I'll pay you later.",
                    "label": 0
                },
                {
                    "sent": "So those were about using this for online learning, so this drifting game idea is actually something that is very close to an old algorithm that I worked with.",
                    "label": 0
                },
                {
                    "sent": "Manfred and Nicholas is a Bianchi and other people on binomial weights online learning, which is not exponential weight and and it uses very very similar kind of chip game in its underlying thing, and basically by making that chip game continuous time.",
                    "label": 0
                },
                {
                    "sent": "We managed to get a new performance.",
                    "label": 0
                },
                {
                    "sent": "Now that's paper.",
                    "label": 0
                },
                {
                    "sent": "That's the quote paper you AI paper is using that to come up with a better tracking algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we have a tracking algorithm that is much less sensitive to assumptions then.",
                    "label": 0
                },
                {
                    "sent": "Then the Kalman filter.",
                    "label": 0
                },
                {
                    "sent": "So so those are the papers.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I wonder about regression.",
                    "label": 0
                },
                {
                    "sent": "Does this apply to regression was close to classification?",
                    "label": 0
                },
                {
                    "sent": "Oh so OK. Yeah, I think it does apply to regression, but you need to be willing to rid yourself of the square error right?",
                    "label": 0
                },
                {
                    "sent": "Because the problem with regression is really that it minimizes square error, and that's not usually what we really want to minimize.",
                    "label": 0
                },
                {
                    "sent": "What am I saying that?",
                    "label": 0
                },
                {
                    "sent": "Why do we have outliers removal in regression?",
                    "label": 0
                },
                {
                    "sent": "Because outliers hurt us.",
                    "label": 0
                },
                {
                    "sent": "So much.",
                    "label": 0
                },
                {
                    "sent": "What we really want to do in regression, I believe is very close to what Vapnik suggests, which is we want.",
                    "label": 0
                },
                {
                    "sent": "Most of the examples to be very close to our function and then on the rest we don't care like 10% of the examples can be arbitrarily far for that kind of robust regression.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's called yes, we can use these ideas and we started to work on that.",
                    "label": 0
                },
                {
                    "sent": "But so so you need to basically say.",
                    "label": 0
                },
                {
                    "sent": "When am I going to give up on an example, right?",
                    "label": 0
                },
                {
                    "sent": "That's the underlying intuition.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you want examples in which you give up in regression.",
                    "label": 0
                },
                {
                    "sent": "You never give up on examples as long as you're using squared error.",
                    "label": 0
                },
                {
                    "sent": "But if you clip the square, which is basically like Huber robust statistics then then you start to do something more interesting and then you can use ideas like like Boost.",
                    "label": 0
                },
                {
                    "sent": "Majority.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Alright, thank you and Bon Appetit.",
                    "label": 0
                }
            ]
        }
    }
}