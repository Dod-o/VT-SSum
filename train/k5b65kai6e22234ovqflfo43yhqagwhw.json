{
    "id": "k5b65kai6e22234ovqflfo43yhqagwhw",
    "title": "Statistical Leverage and Improved Matrix Algorithms",
    "info": {
        "author": [
            "Michael Mahoney, Department of Computer Science, Stanford University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_mahoney_itslima/",
    "segmentation": [
        [
            "Alright, so the this is OK for the microphones alright for you alright so so thanks for the invitation.",
            "This is actually a nice and I think it's sort of a timely workshop because I mean there has been a lot of work.",
            "Done in the last maybe 10 years on.",
            "Large scale linear algebra and a lot of it's been motivated by very large data applications, I mean historically linear algebra done a lot of work instead of scientific computing where you're drawing very large matrices for partial differential equation applications or whatever, and they tend to have very particular structures given domains from which they are drawn.",
            "In particular, strong regularity structures because of the three dimensional or two dimensional nature of the systems that are typically being simulated.",
            "So what you see in a lot of as many of you know, what you're seeing, a lot of very large scale data applications are that those Reg.",
            "Clarity structures aren't there, and so that is forcing linear algebra and people in theory of algorithms that work on sort of large scale linear algebra problems to revisit some of the assumptions and as sort of what are the key assumptions to get the algorithms to work.",
            "So I wanted to talk about a couple of those things.",
            "In particular, I wanted to talk about.",
            "Improved two different improved matrix algorithms and the neat thing about these is that in order to get the improvement, and these are two very traditional numerical linear algebra problems.",
            "But in order to get the improvement we had to make critical use of a very fundamental statistical concept that if you think about it, would have a lot of applications in diagnostic data analysis and machine learning.",
            "So we'll be talking about worst case analysis of algorithms, but we need to use very.",
            "Sort of fundamental statistical concept and so maybe not surprisingly, there's a real interplay there and he said it can look under the hood as it were inside the black box and see how some of the algorithms work.",
            "And you can see how they'd be applicable to a bunch of different data applications, and so a couple I'll talk about a couple of those towards the end.",
            "So.",
            "Just to sort of say."
        ],
        [
            "Notation, I mean, I hope this is a review.",
            "If not, then the rest is going to be a little bit heavier going, but just to sort of set the notation.",
            "I want to talk about the so called least squares problem.",
            "Now.",
            "One of the problems, one of the two problems that I'll be talking bout will also be least squares.",
            "The other one will be something else, but will have sort of least squares under the hood.",
            "So to give you a little bit of intuition as to what's going on, let me just do it in the context of least squares.",
            "And very over constrained least squares, you have a matrix A.",
            "A in this case will be N by D, But then think of his large D is small.",
            "And you have a right hand side vector B and you want to know is there a vector X such that ax equals B.",
            "So since then is much larger than D. In general, the answer will be no.",
            "There's not going to be such a vector X, basically because be could have some peace sitting outside the column space of a right.",
            "The columns available, the final space be could have some peace sitting outside that.",
            "So if you're interested in overconstrained, well, I'm calling it LP here because we have this more General Patel, two regression problems.",
            "There's no extra today, X = B.",
            "A typically want to find the best, and the best depends on the particular application, but it's typically quantified by saying I want to minimize some norm.",
            "So it's L2 if the norm is the two norm, the so-called Euclidean norm, the sum of the squares of the errors of the residual, and certain applications.",
            "If you're just interested in sparsity, that might be the L1 norm.",
            "It could be any of a bunch of other norms, so we'll be talking about the L2 norm.",
            "I mean it will correspond to statistical assumptions having with Gaussian priors and that sort of thing is, as you probably know, so this least squares regression problem is very, very common and really central in a lot of applications in a lot of theoretical considerations, very natural statistical interpretation is providing the best best estimator within a certain class of estimators in a very natural geometric interpretation.",
            "You're taking B and you're projecting it down to the span of the columns of a."
        ],
        [
            "So lots and lots of applications of this starting back in 18, oh something Gaussian.",
            "Legendre and Adrian were interested in astronomy and some other applications and they described an algorithm that in modern language takes N * B squared times, basically Gaussian elimination O N * D squared will be something that will get back to you, so keep that in the back of your mind.",
            "And since then, yeah, lots and lots of applications, bioinformatics, medicine, engineering control theory... computer science, interesting analysis and then a lot of these applications you're doing some sort of fitting the parameters of some model to experimental data.",
            "So."
        ],
        [
            "There's a couple of ways to view this problem.",
            "One is, you know, is this sort of the right thing to be doing given the data two is, you know how long does it take, you know.",
            "So there's a range of ways you can do this.",
            "For a moment, let's just not ask, is this the right thing to do?",
            "But ask how long will this take?",
            "So open up Golovin, Vanloan or some standard textbook on numerical linear algebra and how long it will it take so there's a couple of ways to solve this, and these will be exact method as opposed to iterative methods mentioned something about that in a little bit.",
            "There's something called the Cholesky decomposition, so if a is nice in certain senses.",
            "Then you can decompose A transpose A as R, transpose R and then solve the normal equations on R. Transpose are now in general.",
            "It's a very, very bad thing to do, and the reason is that in less A is very nice and very well conditioned.",
            "You've amplified the complexity, measure the so-called condition number and things are going to get very bad, so it's typically done is something called the QR decomposition that's going to be slower, but more numerically stable, especially if as conditioning issues there's a bunch of different variants of QR, right?",
            "Revealing QR full strong low high will get back to little bit about those in a little bit, in which case you're going to write a SQR and solve this system and you can also do the so called singular value decomposition, which is overkill, but you could do it if you want your right as U Sigma V transpose an then U&V are well conditioned.",
            "They are easy to invert and so you can write the optimal vector acts this way so these differ a lot in practice.",
            "They all take some constant.",
            "And the constant depends times N times dsquared time.",
            "So for a moment step back a little bit and Ann view them all as sort of the same.",
            "Although in terms of stability in terms of constants only will differ a lot and they'll all computer vector X such that.",
            "X is equal to the pseudoinverse, which is the inverse of a on the column row space times B.",
            "Now one of the tricky things about one of those sort of take home lessons about a lot of these linear algebra problems is that they can be sort of unstable or tricky and in certain subtle ways.",
            "So what you're doing here is taking a pseudoinverse.",
            "If A is instead of being ranked D, it's really rank D -- 1.",
            "You want to say.",
            "Well, I should just ignore that other dimension and you can do that.",
            "But if it's rank D -- 1 But has some little epsilon in the deep space, if you do that, you blow up epsilon to one over epsilon.",
            "So people often time to do is add some prior, add some regularization term and you can do that.",
            "You can do that, but then you're changing the problem, solving a different problem and that may or may not be the right thing to do.",
            "But you can do that so.",
            "Another is that we are actually examples of sort of issues.",
            "One of the sort of pet peeves I have is so you know if a matrix.",
            "Is how to how does this work?",
            "If if a matrix is?",
            "Ranked degenerate then its determinant is 0, right?",
            "Does the converse follow?",
            "If it's zero, is it that it's ranked in general?",
            "If it's, if the determines approximately 0, then is the matrix going to approximately rank to generate?",
            "So who has an answer to that, aside from maybe energy too?",
            "I saw flinched when I ask that question.",
            "So think of something about as well conditioned as you can have right an identity matrix and maybe multiply all its elements by 1/2, so that shouldn't change anything.",
            "And if it's an end by inmate identity matrix then you're going to have 1/2 to the end.",
            "So exponentially bad.",
            "So there is another example of sort of the same instability that you see here where.",
            "No, a naive application of sort of off the shelf algorithms will not do so well.",
            "OK, so back to the."
        ],
        [
            "Squares problem so.",
            "So this is how long it takes N times dsquared time and you have to be careful about these stability issues and conditioning issues.",
            "But in times eastward.",
            "So when is this the right thing to do?",
            "So I told you you want to work with least squares.",
            "One is the right thing to do.",
            "This is the right thing to do in the following case one the relationship between the outcome and predictors is roughly linear.",
            "And when the error term is nice, maybe mean zero, constant variance errors are uncorrelated, normally distributed and so on and so forth.",
            "They don't even need to be normal.",
            "This need to be a nice enough so that you have large enough samples to rely on large sample theory.",
            "Now in practice, of course these will not hold perfectly, so the prudent machine learner data analyst will check the degree to which these assumptions will be violated."
        ],
        [
            "How would you do that?",
            "So there's a range of ways to do that, but one very common way to do that is to use something called so called regression diagnostics.",
            "So what we what we have here is on the top left, the Model B is equal to X plus epsilon B's response.",
            "They have the so called carriers.",
            "Epsilon is the noise process X opt.",
            "Is this thing A transpose, a inverse?",
            "A transposed be?",
            "That's an ugly expression, but if you're familiar with linear algebra is something you come to know and love.",
            "That's basically the projection onto the span of a or.",
            "I guess I should say when you multiply that by it is.",
            "So be.",
            "Is equal to HB prime.",
            "The predicted value of B is equal to H * B. Alright, this is just what I had on the previous page.",
            "The tax office is the same thing we computed, and if you multiply that by a Member ax equals B, so aksak should be something close to the approximation of B, then B prime.",
            "I should have put a hat there, but I couldn't get it in PowerPoint.",
            "Be prime is equal to H times be, so H is this ugly thing.",
            "It's just a projection matrix.",
            "It's a projection matrix onto the columns of a. I could have written it, you transpose you where you is any projection matrix onto the span of a OK.",
            "So we want to be talking bout this projection matrix and hi, Jay will measure the leverage or importance or influence in a certain precise sense exerted on the value of the predicted value of B by the J value of what you actually measured.",
            "So BJ's, which you measured by primers which you predict.",
            "And this is independent of B, since H is a projection matrix under the span of a. Alright, so the off diagonal elements are telling you something about how the predictions you're making depend on the things you measured.",
            "In terms of sensitivity, the vector of the vector of residuals will depend on this.",
            "There will be the expectation will be 0, but they'll be nontrivial variance.",
            "So in some sense, if there's some sensitivities.",
            "Encoded in this matrix H, then that will tell you where your predictions are going to be sensitive.",
            "In particular, don't let's not worry about the off diagonal, so just worry about the diagonal elements.",
            "Intuitively, with this is saying is that if the diagonal element is particularly large, that's a particularly sensitive data point.",
            "It's a particularly important data point, so intuitively which you want to be saying is that.",
            "Is that was the chocolate.",
            "Oh here, so you know what you want to say is, if I want to do a least squares fit and this is my data.",
            "This very I mean, what's the most important data point?",
            "So the most influential data point right now?",
            "Maybe you kicked the machine and that's an error, in which case you should do this sort of diagnostics and filter that out.",
            "But maybe that's a real data point.",
            "And maybe it's a real data point, because you know you shouldn't be doing L2 modeling 'cause the world is more complicated, but you're doing it for convenience or something.",
            "And if that's the most important data point, then maybe you should pay particular attention to it.",
            "And one way to quantify that is since the trace of H. It's a projection onto a D dimensional space.",
            "Since the trace of HSD, maybe a diagnostic rule of thumb and this is been used by statisticians is to say if the dynamics are greater than two D over and twice the average value of twice the expected value, then investigate and maybe a kick the machine and it's an outline which case you get rid of it.",
            "Or maybe it's a particularly important data point.",
            "So H this ugly expression, as I said, can be written as U transpose U.",
            "Which is which is.",
            "Where were you as any orthogonal matrix under the span of A and in particular the diagonal elements of H can be written as the dot product of the rows of you with itself.",
            "So you saw in the previous talk.",
            "I think it was called be there that be if correlation links decays slowly or something, then the links of all the rows of B are going to be roughly uniform and with those with that saying is that information doesn't propagate in particularly bad ways.",
            "So we're saying here is that the lengths of the rows of orthogonal.",
            "Bases onto the span of the columns of of a. Encode information about how sensitive things are.",
            "What things are particularly.",
            "Influential or particularly important in terms of having bad decay, correlational properties, so this will be a central thing to what we're going to be interested in.",
            "There's a lot more information in this projection made."
        ],
        [
            "Tricks and I don't know how good this is.",
            "This is a 10 by 10 example and this is basically the same picture I drew here, except a little bit more realistic and you might want to say that that guy up there or this guy down here are particularly influential.",
            "And if you look at all the numbers in here, you'll see that in fact that's the case.",
            "If you look at the diagonal elements.",
            "If you look at the off diagonal elements will tell you which ones are positively negatively correlated and all that sort of stuff so."
        ],
        [
            "How uniform and non uniform of these leverage scores in quote real data?",
            "Soap.",
            "The diagnostic rule from the statisticians used is to say if the leverage scores are greater than two or 3D / N two or three times the average or expected value flag it as a potential outlier.",
            "Investigate so this is the Enron email matrix.",
            "So think of this as a large social information graph because you get similar results in a lot of those.",
            "And what I've plotted here is the index of the I TH highest leverage term.",
            "So I've computed all these leverage scores I've set.",
            "I think the dimensionality parameter be 10 years, I think it's 10.",
            "And I looked at the first, second, third, 4th fifth highest leverage term and on the Y axis I will plot the cumulative leverage.",
            "The leverage of the first delivered the 1st to the 1st three, the first 4.",
            "The Matrix is 92,000 by by whatever.",
            "And so D / N is 10 to the minus four and the highest leverage term is 10 to the minus two SO2 orders of another factor of two or three after 100.",
            "And the second highest leverage term is just marginally less, and the third highest leverage is marginally less, and the fourth highest leverage is marginally less.",
            "So by the by the metrics, the traditional methods of sort of statistical data analysis or regression diagnostics, you have a huge huge number of things that are obscenely outlying.",
            "Now, if you think about it for a second, that shouldn't be surprising here.",
            "I mean, the Enron data is incredibly sparse.",
            "Right, there's no.",
            "You know.",
            "It's not like you have roughly an non zero entries per row.",
            "There's no reason to think, measure concentrates.",
            "There's no reason to think SVD or PCA or L2 based measures of the right thing to do.",
            "Now that being the case, this these sort of techniques of the heart of things like light and semantic indexing and hits and page rank in these sorts of things and these models are used for a range of reasons, but one because if you want more realistic statistical modeling, you're going to have to give yourself an incredible amount more descriptive flexibility, and that will give yourself a lot more places to hide your mistakes, namely overfit, and so you know you work, you're making decisions about the appropriateness of.",
            "Statistical models not based on statistical considerations but based on computational considerations.",
            "So that's going to be sort of an important theme, So what you see here is that what we're going to want to do is identify the non uniformity structure because we're dealing with.",
            "Matrices L2 based penalties the non uniformity structure in the two applications I'll be talking about will be defined by these leverage scores, but you get incredible.",
            "I mean, every data point looks like that you get an incredible number of very, very far outlying terms so.",
            "So is."
        ],
        [
            "Is sort of the rough idea clear?",
            "I mean, I hope at least the rough perspectives clear.",
            "So what I want to do is talk about two particular matrix problems.",
            "So there's been a lot of work done in theoretical computer science over the last 10 or 12 years on randomized algorithms for matrices.",
            "And a lot of this wasn't immediately applicable.",
            "I mean, it had a lot of new ideas, but a lot of it wasn't immediately applicable to traditional problems in linear algebra because at the end of the Devils in the details, and if you have a slightly different formulation of a problem, sometimes it's hard to web the tool.",
            "So the two problems will be talking bout a very traditional numerical linear algebra problems and will get better algorithms better.",
            "And worst case theory, but which I mean improved quality of approximation guarantees and or running time just theorems.",
            "And in addition, will be better in practice, and I'll say the sense in which it will be better in practice, and roughly the sense in which is going to be better in practice is because the album will be randomized, and instead of fitting doing computation of fitting to 10 digits of precision, in which case intuitively might be overfitting to the particular data set, you have what we want to say is why don't we?",
            "Why don't we smooth things out a little bit by drawing a random sample and then only roughly fitting and?",
            "So then we're not going to find the particular data set, and so the randomization will.",
            "Implicitly do a form of statistical regularization.",
            "What kind of regularization term, on or anything but the inside the algorithm you look inside the black box inside the algorithm?",
            "Implicitly it will be doing that sort of regularization.",
            "So the first problem I want to be talking about is in fact the problem I just talked about the least squares approximation problem.",
            "The second problem will be the problem of choosing exactly K columns from matrix feature selection if you will, and then I'll talk about how they perform in practice."
        ],
        [
            "So here is an algorithm for least squares approximation and is near and dear to my heart because we thought about it for a long time, but also the original paper the wrong way to view this algorithm.",
            "But a fairway is that this is an algorithm that's no faster than the best previously existing algorithms and is worse.",
            "Alright, so it's going to be faster than the traditional algorithms and will give you a worse result.",
            "The reason I want to start with this is that if you understand the key idea here, everything else will be easy and will in fact be able to do things faster and better.",
            "So the algorithm here is incredibly simple.",
            "Fix some set of probabilities.",
            "Bullet wants us.",
            "Fix some set of probabilities.",
            "From one to end, so we're going to be doing least squares approximation.",
            "It's N by D. Fix some set of probabilities over the rows.",
            "Alright, if you ever see a paper with this uniform sampling done, uniform sampling as good as a straw man, and I'll mention that later.",
            "But because of the non uniformly structure was talking about this, oftentimes something a lot more refined going on and so the devil will be in the detail here in terms of getting a good set of probabilities alright?",
            "Once you have that probability distribution fixed.",
            "Bullet two says pick the I throw away in the element of B with basically those probabilities.",
            "The R is going to be a parameter, but basically it's it's choose the ice constraint with probabilities proportional to that important sampling distribution.",
            "And we're going to choose our to be greater than D. It's going to be D. Log D or so.",
            "And so in particular, you're still going to be over constrained.",
            "And then call whatever your favorite solver is.",
            "The solver could be QR.",
            "It could be the SVD.",
            "It could be some iterative method.",
            "So call your black box and solve the induced subproblem.",
            "Alright.",
            "So that was very simple.",
            "Three steps compute some distribution to the small number of columns and solve the subproblem, the subproblem, the vector X is the same dimension, right?",
            "You've sampled constraints, you haven't sampled variables, so I'm going to give you back a vector X that's the same dimension as the original problem.",
            "You is going to be any basis for the subspace the left subspace of a.",
            "So if you write the SVD of a, it's equal to U Sigma V transpose.",
            "Call it QR if this is the exact left subspace.",
            "Different issues as you know in QR but.",
            "And and the reason is that you don't need to do this full computation, you need some information.",
            "Here U Sigma V transpose so you will be a basis for the left subspace, in particular the projection.",
            "On to a was a is equal to UU transpose, and so any basis for the subspace is you.",
            "The best result this week when he was SPD.",
            "Uses the SVD basis.",
            "Right, so that's why we're going to be faster in this particular case, yeah, so let RBD log D log one over DD is going to be a failure probability.",
            "Bait is a fudge factor.",
            "We'll get back to later.",
            "That's gonna be important factor for later one over epsilon.",
            "I think it's actually well.",
            "This will suffice and we can actually get a little bit better than that, but so it's D log D and he can do an iterative method to drive down the failure.",
            "Probably not the error on epsilon, so this is just think of this as a one pass thing, but if you want to do something if you could make that log one over epsilon.",
            "But let me not get into that for now.",
            "So R is equal to the logged in the constants reasonable.",
            "It's two or three if the probabilities are equal to the statistical leverage scores, the lengths of these rows.",
            "Are the diagonal elements of the projection matrix of a these statistical average scores if the probabilities are equal to that maybe up to a beta fudge factor?",
            "Then if you run this algorithm, you'll get a vector X head up.",
            "That's relative error in the following sense.",
            "The distance between the vector you compute and the exact vector.",
            "Is less than epsilon times the condition number of 8 * X opt.",
            "You can fold that condition on Brent if you understand the norm there, but I've written it out.",
            "And if you take the vector X and plug it back into the vector X, you compute and plug it back into X = B, then you within one plus epsilon of Z, where Z is the original residual.",
            "It's a xop minus B.",
            "The normal that thing.",
            "So relative error in both senses.",
            "Now the rub here is that in order to compute these probabilities.",
            "Even up to some nontrivial factor of beta.",
            "Takes N times dsquared time and the way to do it is exactly synergy said compute you.",
            "Alright, so it's no faster and it's worse.",
            "But what we've done is identified the relevant non uniformity structure and so now the question could be in this problem or another related problems where this is under the hood.",
            "Can you use this non uniformity structure in either preprocess it away?",
            "Or if this is not the bottleneck on time, just deal with that or whatever and the answer to both of those will be yes.",
            "In Europe.",
            "Angle.",
            "Basically, what we're going to do is approximating the operator a.",
            "You want to think of it at that level, so the so be.",
            "It doesn't matter neither.",
            "Angle between yeah.",
            "Spacefaring company I mean, I could point you.",
            "Yeah, so the original paper actually had something depending on B.",
            "It turns out we didn't need that.",
            "That was the weakness of the analysis.",
            "I could point you the exact spot in the proof why we don't.",
            "Intuitively, I don't know why.",
            "I don't know why the traditional numerical linear algebra methods needed as opposed to this not needing it.",
            "This, I think is related to the epsilon or the condition number versus the condition number squared.",
            "I think it's going to be related to that issue.",
            "Yeah.",
            "Yeah, the game is another fudge factor.",
            "Don't worry bout it.",
            "Alright."
        ],
        [
            "So here's a faster algorithm, so we're dealing.",
            "Just deal with the least squares problem.",
            "He's going to be faster algorithm, and the faster algorithm will be the following.",
            "Pre process A&B with what I'll call a randomized Hadamard transform.",
            "And by randomized Hadamard transform, I'll say what I mean.",
            "But a Hadamard transform is a real version of a Fourier transform, so it's 111 negative one, recursively defined and normalized.",
            "So think of it as a Fourier transform.",
            "I'll say exactly what I mean by a minute.",
            "The point about that is that you're going to uniform eyes.",
            "The relevant non uniformity structure, and so then you can sample uniformly.",
            "Now you haven't exactly uniform is that you only approximately uniform isatin, so you need the login log that messy thing, so slightly more.",
            "But think of it as basically the D log D that we had before, and I'll on the slider to say exactly what these numbers mean, but.",
            "The point is now the bottleneck will be in this preprocessing, you preprocess with randomized Hadamard transform and uniformly sample and uniform sampling is very easy solve the induced sub problem and if you solve into a subproblem you'll get the one plus or minus epsilon approximation and this is an expression only a theoretical computer scientist could love, but that's how much time it will take.",
            "This will be basically in the log of D. Alright, so little oh of ND squared.",
            "And if you have an intelligent implementation of the Hadamard transform, that'll be faster.",
            "So Tiger in Rocklin have an implementation of a variant of this where they do an iterative thing and I think they're faster on matrices as small as 1000 or 2500, so very reasonable size matrices.",
            "So."
        ],
        [
            "How does this work?",
            "So here's a structural lemma which gets to introduce you.",
            "Let's preprocess this with.",
            "You know this least squares system with Kai, where Kai is any preprocessing matrix.",
            "Any anything whatsoever, deterministic, randomized, wherever you like.",
            "And assume that the singular values of you are all one or zero, so right, they're all one, so the singular values of \u03c0 times you are all about one.",
            "And.",
            "You is perpendicular to be.",
            "Perp would be purpose defined to be the piece of be perpendicular to the aorta UA.",
            "So you is still roughly perpendicular to be perp.",
            "Once you've done the preprocessing.",
            "If you satisfy those two conditions, then you get relative error.",
            "So you can imagine having algorithm checking that and if the answer that's yes, you're done, use that as your preprocessor."
        ],
        [
            "But if you don't want to check that the randomized Hadamard will do exactly that, so randomized Hadamard, let H be an end by end of terministic Hadamard matrix.",
            "For technical reasons, you need to preprocess that with a + -- 1 random matrix, and I can talk offline about why that is.",
            "The point is that the Hadamard matrix.",
            "Is an orthogonal matrix.",
            "It's a unitary matrix, and so multiplying the linear system doesn't change the solution, right?",
            "'cause it's just a rotation basically.",
            "Multiplication is fast.",
            "Basically we're doing a random projection, but the typical way to do random projections would be slow 'cause you have to do a dense right into projection the Hadamard.",
            "You should think of as approximating a random projection.",
            "And approximating it in because you can do fast Fourier techniques an log in, but in fact you only need to touch the elements that are going to sample, so it's in lorgar and so that ugly complexity expression I showed you is basically this.",
            "An multiplication by this randomized Hadamard, approximately uniform sizes, all the leverage scores.",
            "So if you look at the leverage scores in the process system there ALDI over N up to that log factor and that's why you got this slightly uglier expression."
        ],
        [
            "You can do the same thing with projections also."
        ],
        [
            "The randomized Hadamard transform is a thing, and Chazelle basically used, and then we have uniform eyes.",
            "The leverage scores.",
            "Then we sample uniformly the projection thing that I'm skipping over is basically exactly what they did.",
            "Alright, so we're saying is if we've identified the relevant non uniform structure, then we can preprocess it away and the theory goes through and on.",
            "As I said, I think it's on the order of 1000.",
            "Two 1100 systems will be faster than using exact methods.",
            "So the second problem I want to mention is a slightly different problem.",
            "But will be very related.",
            "So.",
            "Say that you have a big matrix M by N and you want to choose K columns and you might want to skate columns 'cause these are the key features that matter and you might want to choose K columns 'cause you're interested in subspace.",
            "They define.",
            "The latter is more common in numerical linear algebra.",
            "The former and data analysis and machine learning.",
            "But for whatever reason, the goal is to get K columns.",
            "How can you do it and how well can you do it?",
            "And so we'll see is using.",
            "These ideas will have an algorithm that will do."
        ],
        [
            "Well, so here's the.",
            "The so called column subset selection problem.",
            "So, given an M by N matrix A and a rank parameter Ki want to choose exactly K columns of a. NA is adversarially given here, right?",
            "I mean they can be anything if you want to think of this random sampling that we did in the previous step.",
            "And in this case as something like generating a particular data set when you wake up in the morning and you get a particular data set, you could imagine that we haven't formulated it that way.",
            "But you could imagine formatting it that way, in which case the non uniformity structure has to do with the generative process.",
            "And you could imagine implicitly downsampling or possibly taking advantage of that.",
            "And that's actually an interesting direction to think about, but for right now the Matrix A is given rank, premise, given with choosing exactly K columns of a.",
            "Such that the M by K matrix CM as a number of rows and K as the number of columns.",
            "So the end by K matrix C minimizes some error and we're going to be interested in is a spectral norm error in the Frobenius norm error.",
            "So if you have a matrix A, the spectral norm is the largest singular value of a sort of a worst case norm.",
            "It's the direction.",
            "Well, it's the amount by which a stretch is a vector the most in any direction.",
            "The Frobenius norm is more of an averaging norm.",
            "You sum over all the directions of ES, and you sum the squares of the singular values of the elements of a.",
            "So we're going to be from two different norms.",
            "And um, for range of reasons in numerical linear algebra, you're typically interested in the spectral norm in a lot of applications in machine learning and data analysis.",
            "Here intervene IAS norm basically because you're in a bunch of different directions and you want the error on every particular thing, so that might be why intervene IAS normal?",
            "So P subsea is like that it's a projection.",
            "Onto the matrix, say on the Matrix C, which you can write as C * A pseudoinverse of.",
            "See now that's a bad way to compute it, which you should do is compute the left singular vectors of C and do you subsea times, U sub C transpose, and it's a bad way for conditioning reasons, not 'cause it's not right in principle, so the complexity this problem this, or variants of this will be intractable.",
            "To compute exactly.",
            "And so the question might be, can we come up with some approximation?"
        ],
        [
            "So the usual way approximation algorithm for this problem will be parameterized will be the following.",
            "We know a lower bound, right?",
            "I want exact columns and I want to optimize those two objectives if I if I relax the problem and I say listen, I don't want to take columns exactly, but I want a linear combinations of columns, then the answer to these two objectives is to do the singular value decomposition, keep the tip top K eigenvectors.",
            "So that will be a lower bound for these problems.",
            "So for any M by K matrix C. The projection onto the singular vectors will be a lower bound for the projection onto those columns, and So what we want is defined as."
        ],
        [
            "Of columns such that were not much worse such that.",
            "Such that weren't, you know, a factor of something where something is not too bad.",
            "Worse, so in numerical linear algebra have been huge amount of work on this, and the algorithms are deterministic and typically greedy.",
            "By greedy I mean they will make a decision about which column to choose, which column to pivot on at a given step, and then they'll keep that column.",
            "Sometimes I swap things back and forth, which is slightly less naively greedy method, but I'll get back to later a bunch of different algorithms for doing.",
            "A related problem having to do with rank revealing QR and one way to view that is just to treat them all the same.",
            "They're all black box.",
            "They all have whatever theorems associated with them, and I'm just going to run with it a different way to say what's going on in the black box.",
            "There's an incredible amount of complexity going on the black box, but at a very high level.",
            "Think of it as the following.",
            "A lot of these algorithms, the difference between them, is how they make decisions about what's called pivot rules, so you know you have a bunch of columns here and you need to decide whether to swap another column in or not swap it in as you look at different columns of matrix.",
            "And whether you want to keep the column depends on the columns you have.",
            "The spectrum of the columns you have, the spectrum of the columns you don't have.",
            "These sorts of things, and so the decision as to whether you keep the column will depend on that, and if you make more sophisticated decisions, you're making more sophisticated private rules and typically will get better.",
            "Worst case theorems and hopefully do better in practice.",
            "And if you make less sophisticated decisions, you might do worse in theory, and maybe you do better or worse in practice, so we'll get back to that.",
            "But think of that as sort of a slight refinement of bullet one.",
            "So as I said, this problem will have very close connections to rank, revealing QR, basically QR and rank, revealing QR.",
            "You don't just want to bound on the columns, you keep the bottom part of the spectrum of the columns to keep you about 1 pounds in the top part of the spectrum.",
            "You don't keep in a certain cross term, so very close connections rank, revealing QR.",
            "The strongest results now for the spectral norm will be a minus the projection of the columns will be root K root N. OK ruttkay ruden so keep that in the back.",
            "Your mind will get back to that and that takes a certain amount of time and you know how much time it takes you know.",
            "Depends on who you ask.",
            "The wave theoretical computer scientist parameterized the problem.",
            "You know the matrix is given and I want to get some exact answer in some sense, in which case if is a very small gap between the case and the K plus first singular vector, you need to do some huge number of iterations depending on the logarithm of that cap.",
            "In America linear algebra they'll say well, things are continuous and you do a stability issues.",
            "It makes no sense if they're that close, just return the other one.",
            "And in terms of these norm measures, you'll get the same answer, which is a fair, which is a fair observation.",
            "For now, I just want to say it will take some time.",
            "T. Alright, if you want to compute the full thing, it will be M * N ^2 if you want it a little faster, it takes some time T. And the reason is that for this algorithm, we're not going to be faster than the last one.",
            "We were faster and had this approximation guarantee one plus epsilon.",
            "Here we're not going to be faster.",
            "We're going to take time T basically 'cause we want to compute that basis, but will be better in terms of quality of approximation guarantees and how it performs an application.",
            "So it takes some time T, so that's how long that is the strongest results for the Frobenius norm are root K, root in by the spectral.",
            "Which is maybe not so good, but it's not time T it's just an exhaustive enumeration.",
            "This is an existential result.",
            "Alright."
        ],
        [
            "So there's many if you want to call this quantity.",
            "P of NNK There's been a huge amount of work and this must be about 10% of the papers that you could dig up on this problem on improving the factor of P. Some of these things have implementations, some don't, so huge amount of work on this."
        ],
        [
            "More recently, in theoretical computer science, there's been a lot of work, unrelated problems, low rank approximations, maybe in streaming or pass efficient models where where the data so large that you can't store them on disk.",
            "And these algorithms are typically randomized.",
            "Very much of the flavor of the album I gave previously, although sometimes you would compute much coarser, important sampling probabilities and still be able to do much faster, but will be randomized.",
            "And if you notice we had a D log D before all these algorithms will take polynomial in K, But that is typically K log K or D log DI, change the rank parameter on you and the reason you need a K, log K or D log D is that at the heart of these algorithms.",
            "There's something called coupon collecting, so you know this standard problem is, you know the Kellogg's or whoever makes Cheerios is running a contest.",
            "You have different coupons and their uniformly distributed cereal boxes and how many boxes of cereal do you need to win the game which is defined to be of 1 coupon of each.",
            "And so you need order D log D. Or cardinality of the number of coupons, log cardinality, the number of coupons.",
            "And that's just a very basic result from probability.",
            "So at the heart here in terms of bounding the spectral properties, you have something like that going on, and so in particular you're going to need to choose more than D log D columns or more than the columns.",
            "So we have very strong bounds for the Frobenius norm.",
            "And effectively no nontrivial spectrum bounds in this literature, so."
        ],
        [
            "On this particular problem, the best results so far was something I had with Petros Junious, where in time T you could get a one plus epsilon approximation by choosing K log K columns, and this improved the old additive error stuff that I had with him and Canaan Impala and freezing condoms and some other people had.",
            "In terms of the quality of approximation, but it was a bit slower because in those cases you could do it very very quickly.",
            "Desponding Impala in a slightly different model were also able to get one plus epsilon approximation using roughly the same number of columns, and they also proved in existence result, namely that there exists K columns such that the Frobenius norm is better than root K times of Frobenius, and this is the expression I had before ruden time to spectral.",
            "So these."
        ],
        [
            "The things that are the sort of prior work and to get the strongest Frobenius norm bounds before we have to do is the following.",
            "So the theorem is given by K matrix A.",
            "There will be.",
            "Order T time algorithm that picks K log K over epsilon columns.",
            "Such that will probably 1 minus Delta within one plus epsilon, and this analysis you should recognize that the form of this is very similar to the regression result we had, and this analysis boils down to that regression result.",
            "I mean the way we went from average relative error was to use that regression result at the end of the day, and time T is there is not a bottleneck in this case depending on your rank parameter and what your complexity which you think is important.",
            "So in that sense it's different than the previous result and the way we did it as you subspace sampling probabilities to sample this many columns subspace."
        ],
        [
            "Sampling properties is just a name for the thing I described before.",
            "Now we're sampling columns, so we want probability distribution over columns.",
            "So let's write a as U the best rank K approximation to a as UK Sigma K. VK transpose a probability distribution over columns of a will correspond to a probability distribution over columns with the right singular vectors.",
            "Before we had rose.",
            "So we're working on this side.",
            "Now we have columns, but it's all the same thing.",
            "Right?",
            "So up to normalization, these will just be this leverage scores that we had before."
        ],
        [
            "So how do you bridge these two things?",
            "We would like to do is sample exactly K columns, But if you look at the details of the pivot rules and you open up the black box, it's not at all clear that even keeping AK Plus first column is going to do any better.",
            "Never mind keeping more than that.",
            "And if you look at the theoretical computer science approaches, there's no way instead of 1 pass to get below.",
            "D log D or K log K columns for basically the coupon collecting reason.",
            "So there's been some work prior to it.",
            "I'll be talking about by Wolf and Liberty in Rocklin, Tygart, where they will choose K columns and get the same spectral norm bound this prior work and this is an application of the allergen cell result.",
            "Basically results I described before and a nice empirical evaluation.",
            "So I want to talk about is a different."
        ],
        [
            "Bridging here now.",
            "The obvious question is, will do a two phase algorithm.",
            "Randomized phase and then postprocess with some deterministic phase.",
            "So that's what we're going to be having, but the first half dozen obvious determine two stage albums that you come up with will not work, and so we're going to do something a little bit more refined, but the more refined thing will give actually a lot of insight into what's going on inside the black box.",
            "So we're going to be running two phase algorithm.",
            "The first phase will be randomized phase.",
            "It will basically be doing what we did before.",
            "Computer basis for the top K right singular subspace and take time T and that's going to be as fast as the traditional numerically algebra approaches, and we're no faster.",
            "We're just going to pay that cost.",
            "We're not going to have.",
            "Nothing was going to pay that cost to compute that basis exactly or approximately.",
            "And then in the deterministic phase we're going to run a black box QR algorithm.",
            "To choose from those K log K columns, exactly K columns.",
            "And if you remember the quality of approximation was ruden root K, you might think that you know if we choose a good basis, we can decrease that route in.",
            "And there's a lot of self analysis, but the answer is that we will be able to do that.",
            "But the way we'll do that will be a little bit tricky, so I'm going to skip a couple of slides and put a picture or two on the blackboard because it will be just to convey the intuition as to how the algorithm works.",
            "I think it'll be a little bit more revealing, so in the first phase we are going to.",
            "Compute, let me call it.",
            "I'll call it the subspace.",
            "Sampling.",
            "Probabilities.",
            "We're going to be computing these probabilities.",
            "That that that that I'll say bias you.",
            "Tord outliers.",
            "In the sense that.",
            "They'll put a lot of weight on that guy.",
            "So the bias in youth towards particularly influential things or things that stick out a bit.",
            "Anne will choose order.",
            "And the first phase is choose.",
            "Order K log K. And then over the epsilon and all that's the same as before.",
            "But choose order K, log K, and So what you've done now is choose a bunch of columns of a.",
            "But you've also done is choosing a bunch of columns of the right singular vectors of a 'cause.",
            "Remember that a is equal to you.",
            "Sigma an since a is arbitrary V transpose.",
            "So now in the deterministic phase.",
            "Run a QR algorithm.",
            "On the columns of VK that you chose, don't do it on A and don't do it on the columns of a that you chose.",
            "Run it on the columns of VK that you chose, so I'll say the columns of VK transpose you chose.",
            "So this.",
            "Is now not dimensionality Amaran not the high dimension.",
            "It's dimensionality K log K. So you might hope that applying Black box QR with that routine thing will give you something good here.",
            "But of course you can choose a stupid set of columns, so you need to choose a smart set and choosing columns in this way is a smart set.",
            "Intuitively, it's a smart set because what you're doing is biasing yourself towards things that are outlying.",
            "On the one hand, but you're also working with VK transpose directly rather than A and because a is VK, transpose modulated by the singular values, namely stretched in some bizarre way and then rotated by you.",
            "And you've done a lot of convolutions there that can mess a lot of things up, and so we're saying is the right.",
            "Thing to filter in the second stage to run QR on is this.",
            "And we'll see that that's true if you do the randomized algorithm and also show you what happens if you do a deterministic, you are on on just that thing.",
            "Which in fact will do better than the deterministic you are on that thing.",
            "And intuitively it's the same reason.",
            "Right?",
            "Pivot rules have problems when the columns of a are almost aligned and then you have your pointing off in some other direction.",
            "So when things cancel in bad ways and what I'm telling you is that I'm biasing the choice of columns.",
            "So it's things that stick out a bit.",
            "And so you're not going to have on average these bad cases quite as much.",
            "And so even doing deterministic you are on that thing will be better than that on that thing.",
            "That's not a theorem, that's an empirical claim based on a bunch of applications and so interesting question would be formalized.",
            "The conditions under which that's true.",
            "So these are the things I said I was going to skip just."
        ],
        [
            "And here's sort of the theorem for the algorithm I just described.",
            "Um?",
            "A minus the projection under the columns.",
            "The spectral norm now.",
            "The thing those root K routine before now is K to the 3/4 end of the one 4th and there's a log factor there.",
            "And that tension between's increasing K and decreasing and seems to be a real effect, will see that in practice don't worry bout the log factor doing about the constants.",
            "It's root K root N. This is a theorem and I'll get to practice in a minute, so the running time is the same as the traditional numerical linear algebra is no faster the spectral norm, however, is better as a function of N, and there's been a huge amount of work.",
            "It wasn't even clear that there existed columns better than Rudin.",
            "So I've given you an algorithm and time TM N ^2 M NK log K or whatever you want that runs in time T. That will actually give you the algorithms and more than giving you giving you give you the columns and more than giving you a column.",
            "It's a randomized algorithm, will give you a whole bunch of columns if you run at different times.",
            "So it wasn't even known that this existed and we found them.",
            "This is the first sort of this is the first asymptotically improvement, since there was a bunch of interest in this problem in the context of the full rank revealing QR by Gwen Eisenstadt and others in the mid 90s for the Frobenius norm."
        ],
        [
            "Are we are multiplicative factor of K RT log K, so this is an algorithmic result that will be just marginally worse than the best previous existence existential results.",
            "Yep.",
            "Alright, so the Frobenius norm result is marginal.",
            "Remember what exactly K columns here and were marginally worse than the best existential result.",
            "And and it's running at algorithmic time an I should say, you know, if you're dealing with the Enron data, you can set K to be at that.",
            "I mean, I think it's a bad idea for data dependent reasons, but if you just interested in how long it takes, you can set K to be 1000 and then run these things in very realistic times.",
            "So you shouldn't do that as a problem."
        ],
        [
            "Alright, so how do these things perform?",
            "I mean, I told you the original motivation was.",
            "You know with large scale data problems.",
            "That a lot of work in numerical linear algebra is been in sort of scientific computing applications, and so how might these things perform in practice?",
            "So I mentioned the application of least squares problem.",
            "So let me not talk about that.",
            "Now let me talk about how the column subset selection problem will perform.",
            "So this is a."
        ],
        [
            "Cool things datasets.",
            "I want to talk about.",
            "One will be the SNP, a certain SNP 500.",
            "Data set on historical stock prices for about 500 stocks over over 1000 or so days.",
            "This will be a very low rank matrix and so it's good, so there is a methodological test case much more realistic than some random matrix is some arbitrary matrix you choose, so it's good as mythological tests.",
            "It doesn't classify so well in the in the low dimensional space and so for instance feature selection or something.",
            "This may not be the best application for these sorts of techniques that will be good as methodological teskey tech TC is a corpus of term document data from the Open Directory project.",
            "Hundreds of matrices.",
            "Think of him as each one contains about 200 documents from two categories in the ODP hierarchy.",
            "So the Open directory project classifieds all the documents into some hierarchy, and I've chosen a bunch of documents.",
            "From one level here, a bunch of documents from one level here and these two guys might be close.",
            "It might be, you know, football and baseball.",
            "2 sports things that are pretty close.",
            "Or it might be, you know, football and.",
            "You know used car dealerships, which hopefully are farther apart and easier to discriminate between.",
            "And these sometimes classify well in the low dimensional space and sometimes don't.",
            "The sparsity structure here is particularly bad, especially relative to the other two situations.",
            "And so I want I'll show you examples where they do classify well since the motivation here is for feature selection where they don't, we can talk offline, and in fact in a lot of cases they don't.",
            "DNA data either either.",
            "Microarray data and I think the picture I have is actually going to migrate all I'm calling it sniff data.",
            "Here we have a bunch of papers on DNA on.",
            "Snip data and you get sort of similar results.",
            "Nip date is actually much nicer to work because it's much, much larger and the noise properties aren't quite so bad or I don't want to say the better or worse, but they're different in ways that are more amenable to maybe machine learning and linear algebraic techniques.",
            "So microarrays will measure basically the amount of gene product quantified in some way, and so there's obviously experimental issues here.",
            "Single nucleotide so it will measure the human genome at the gene expression level.",
            "Single nucleotide polymorphism data.",
            "That's the most common type of polymorphic variation.",
            "How do two people, not the human genome?",
            "How do any two particular people differ?",
            "And so this data from Hapmap and you should think about map is being on the order of 400 people, and depending on how you define the minor allele frequency between 1 million and five, or 10,000,000 snips across the across the genome, and this will typically classify pretty well in the low dimensional space, which isn't to say that.",
            "The data is low rank, it just says if you want to very coarse approximation to the data that that you can get that in the dimensional."
        ],
        [
            "So here is about 10 different QR algorithms, so this is your black box.",
            "And if you arbitrarily choose one of 'em, you know you can ask how how your machine learning results going to depend if you happen to choose some other one.",
            "So we looked at about a half dozen of these and that not all have publicly available implementations.",
            "We looked at about a half dozen of these and what you saw in the financial data.",
            "The following sort of typical, so the exa."
        ],
        [
            "This here remember in the first step we're choosing more than K columns, and in the second step we will back to exactly K. So here I think the goal is to get 1010 columns or maybe was 20 and at the end of the day and the X axis refers to the number of columns kept in the first step and the Y axis is the quality measure.",
            "One is the basal level set by SVD.",
            "I will reference ourselves to that, so these are four different QR algorithms and the first thing you see is it the four different programs behave in four different ways.",
            "That this.",
            "This top thing there, which is up at about 3:00 is the residual error returned by that algorithm on a.",
            "In all those cases, and I didn't plot it here just for space reasons.",
            "In all those cases, if you do Q are not on a, But on this thing you get something intermediate between this and the best thing returned by the SVD.",
            "So in all those cases, by doing QR.",
            "Whether non uniform structures you'll do better because you're by yourself with things that are outlined and you tend to do better on worse implementations of QR.",
            "If in addition you do the two phase randomized thing, the same will be true.",
            "Oftentimes, as a function of the rank parameter K, they'll be a sweet spot, so we saw sweet spot in theory and you'll do better and better.",
            "At some point you'll sort of Max out the space you're trying to span, and you'll do a little bit worse and worse.",
            "So think of that as sort of a sweet spot in a bias variance sense.",
            "There will be a sweet spot there, so that's a parameter to play with if you want, but typically in all those cases, unless you have very sophisticated implementations of QR, you'll do better than that, so both in the randomized case and the deterministic case.",
            "So here is an example of that."
        ],
        [
            "PC data.",
            "In which you can see that."
        ],
        [
            "The cluster on low dimensional space.",
            "This is actually a very strong filtering, I mean so in terms of selection bias.",
            "If you want a picture like this, we're actually making a very strong bias on the exact data that you want to work with here.",
            "But for the purpose illustration, I'll do that.",
            "And the reason it's strong biases is basically two clusters here and you can look at it by ion sort of validate, but that's a very very strong bias.",
            "What we'll see is that."
        ],
        [
            "On the left hand side is the clustering.",
            "In the top the space spanned by the top two eigenvectors.",
            "So this two pieces here and clusters sort of by K means or by whatever you like by I in the right is what you'll get if you do this down sampling procedure, which you see is the data is very sparse.",
            "In particular just so much more axis aligned here that could be a good or bad thing depending on your application you get a little bit more denoising in certain cases.",
            "Here you lose interpretability and things like that.",
            "This case, you'll oftentimes do better by.",
            "Various metrics, and the reason intuitively is because you're going to low dimensional space, but the low dimensional space return via SVD does a lot of damage if the data is very sparse, 'cause now you'll be dense, whereas the low dimensional space.",
            "We haven't optimized an objective to be sparse, but intuitively it's low dimensional.",
            "It's maybe the best low dimensional space that has a certain sparsity structure, and the reason it has a certain sparsity structure is not because we put that in the algorithm, because in this particular case the original columns have the sparsity structure, and we've downsampled an actual columns.",
            "Now what you should notice here?",
            "This is, it will be a little bit more obvious on the next slide, but this will be the plot of.",
            "We've ordered the 10th or whatever the number of terms we have here is.",
            "We've ordered the number of terms, which is a couple 1000 or whatever arbitrarily and on the Y axis.",
            "Here we plot the statistical leverage score so you see is an incredible non uniformity.",
            "And so uniform sampling wouldn't be expected to work.",
            "There may be doing something like this would, but in addition, if you use.",
            "Something like informativeness a mutual information based measure which is what I plotted down here.",
            "What you see is an incredible non uniformity.",
            "Also if you eyeball this the non uniformity not supply not surprisingly is on words that are more discriminative.",
            "That's a claim about the algorithm but also the particular data.",
            "But that's more ubiquitous than just this particular data set.",
            "And so be interesting to theorem of the following form that says you know this isn't true in general, but conditioned on clustering?",
            "Well, in the low dimensional space.",
            "Then a an unsupervised measure like these leverage scores are basically unsupervised, 'cause I just used information in the data matrix.",
            "They will correlate particularly well with a supervised metric like Informativeness.",
            "Now that will be easy if you set certain residual spaces to be 0.",
            "But doing it without doing that probably will be true, but but that would be actually interesting theorem to prove a model in that sense."
        ],
        [
            "So the Hapmap data, particularly on the left.",
            "You see the same sort of selection affect, but which the left is the Chinese in Japanese.",
            "The straw man on the right is uniform sampling.",
            "You tend to do much worse, and I think the circles here are using a slightly different but also mutual information or phone based measure.",
            "And here's sampling based on the leverage scores.",
            "So this is 90 or so by 2 million 90s because the particular populations it's tricky to get when you're older code to run on that, but you've done.",
            "If you compute computer basis and then you downsample on something that has 3000 rather than 3 million entries.",
            "In exactly that way, then you get what you want.",
            "And this informance is of relevance there be."
        ],
        [
            "That's sort of the state of the art technique in genetics, so here's one of these.",
            "Well, they called raster plots I guess on the top is the color coding of of the microarrays.",
            "What you see is that.",
            "In in the same way as you had before, you have a very strong correlation between the leverage scores and the informativeness metric and totally get it for exactly the same reason.",
            "The clustering in the in the low dimensional space defined by the SVD's versus the low dimensional space on the columns you chose.",
            "So.",
            "It was actually pretty remarkable because in this case you know the theorems will suggest that you'll get.",
            "You reproduce norms and residuals and that sort of thing.",
            "But you saw a very strong correlation here in terms of the actual non uniformity structure.",
            "So which is a little bit of the flavor of some of the sparse.",
            "Maybe the sparse regression models are there out there in terms of L1 based measures, but yet when you go from vectors to matrices, a lot of subtle sub stuff that's going on the subspace.",
            "So naively asking for a low rank matrix isn't going to do so well, but we're going to model that would.",
            "You explained these sorts of phenomenon, so with that, let me wrap up so."
        ],
        [
            "I mean, I think the take home messages.",
            "I mean, you know there is the time, is sort of ripe for thinking about numerical issues and looking inside the black box for a lot of these large scale machine learning problems.",
            "Black boxes are good.",
            "Things are sort of well defined and problem solved, but you know when they're not, you need to look inside the code and understand what the code does, which means you could understand all the details of the code, or even you couldn't understand.",
            "Sort of intuitively what it does and bias yourself towards non uniformly structure work with better codes or worse codes and so hopefully empirical stuff I described gave you a little bit of a flavor of some of those things and the Internet connection is, you know that there's a variation connection between the worst case theory.",
            "And a very traditional statistical concept having to do with statistical leverage such that we can get better bounds in worst case theory using this concept and that sort of explains why in a lot of these cases these sorts of techniques will do better in data analysis and machine learning applications.",
            "So thanks for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so the this is OK for the microphones alright for you alright so so thanks for the invitation.",
                    "label": 0
                },
                {
                    "sent": "This is actually a nice and I think it's sort of a timely workshop because I mean there has been a lot of work.",
                    "label": 0
                },
                {
                    "sent": "Done in the last maybe 10 years on.",
                    "label": 0
                },
                {
                    "sent": "Large scale linear algebra and a lot of it's been motivated by very large data applications, I mean historically linear algebra done a lot of work instead of scientific computing where you're drawing very large matrices for partial differential equation applications or whatever, and they tend to have very particular structures given domains from which they are drawn.",
                    "label": 0
                },
                {
                    "sent": "In particular, strong regularity structures because of the three dimensional or two dimensional nature of the systems that are typically being simulated.",
                    "label": 0
                },
                {
                    "sent": "So what you see in a lot of as many of you know, what you're seeing, a lot of very large scale data applications are that those Reg.",
                    "label": 0
                },
                {
                    "sent": "Clarity structures aren't there, and so that is forcing linear algebra and people in theory of algorithms that work on sort of large scale linear algebra problems to revisit some of the assumptions and as sort of what are the key assumptions to get the algorithms to work.",
                    "label": 0
                },
                {
                    "sent": "So I wanted to talk about a couple of those things.",
                    "label": 0
                },
                {
                    "sent": "In particular, I wanted to talk about.",
                    "label": 0
                },
                {
                    "sent": "Improved two different improved matrix algorithms and the neat thing about these is that in order to get the improvement, and these are two very traditional numerical linear algebra problems.",
                    "label": 1
                },
                {
                    "sent": "But in order to get the improvement we had to make critical use of a very fundamental statistical concept that if you think about it, would have a lot of applications in diagnostic data analysis and machine learning.",
                    "label": 0
                },
                {
                    "sent": "So we'll be talking about worst case analysis of algorithms, but we need to use very.",
                    "label": 0
                },
                {
                    "sent": "Sort of fundamental statistical concept and so maybe not surprisingly, there's a real interplay there and he said it can look under the hood as it were inside the black box and see how some of the algorithms work.",
                    "label": 0
                },
                {
                    "sent": "And you can see how they'd be applicable to a bunch of different data applications, and so a couple I'll talk about a couple of those towards the end.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just to sort of say.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Notation, I mean, I hope this is a review.",
                    "label": 0
                },
                {
                    "sent": "If not, then the rest is going to be a little bit heavier going, but just to sort of set the notation.",
                    "label": 0
                },
                {
                    "sent": "I want to talk about the so called least squares problem.",
                    "label": 1
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "One of the problems, one of the two problems that I'll be talking bout will also be least squares.",
                    "label": 0
                },
                {
                    "sent": "The other one will be something else, but will have sort of least squares under the hood.",
                    "label": 0
                },
                {
                    "sent": "So to give you a little bit of intuition as to what's going on, let me just do it in the context of least squares.",
                    "label": 0
                },
                {
                    "sent": "And very over constrained least squares, you have a matrix A.",
                    "label": 0
                },
                {
                    "sent": "A in this case will be N by D, But then think of his large D is small.",
                    "label": 0
                },
                {
                    "sent": "And you have a right hand side vector B and you want to know is there a vector X such that ax equals B.",
                    "label": 1
                },
                {
                    "sent": "So since then is much larger than D. In general, the answer will be no.",
                    "label": 0
                },
                {
                    "sent": "There's not going to be such a vector X, basically because be could have some peace sitting outside the column space of a right.",
                    "label": 0
                },
                {
                    "sent": "The columns available, the final space be could have some peace sitting outside that.",
                    "label": 1
                },
                {
                    "sent": "So if you're interested in overconstrained, well, I'm calling it LP here because we have this more General Patel, two regression problems.",
                    "label": 1
                },
                {
                    "sent": "There's no extra today, X = B.",
                    "label": 0
                },
                {
                    "sent": "A typically want to find the best, and the best depends on the particular application, but it's typically quantified by saying I want to minimize some norm.",
                    "label": 0
                },
                {
                    "sent": "So it's L2 if the norm is the two norm, the so-called Euclidean norm, the sum of the squares of the errors of the residual, and certain applications.",
                    "label": 0
                },
                {
                    "sent": "If you're just interested in sparsity, that might be the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "It could be any of a bunch of other norms, so we'll be talking about the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "I mean it will correspond to statistical assumptions having with Gaussian priors and that sort of thing is, as you probably know, so this least squares regression problem is very, very common and really central in a lot of applications in a lot of theoretical considerations, very natural statistical interpretation is providing the best best estimator within a certain class of estimators in a very natural geometric interpretation.",
                    "label": 0
                },
                {
                    "sent": "You're taking B and you're projecting it down to the span of the columns of a.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So lots and lots of applications of this starting back in 18, oh something Gaussian.",
                    "label": 1
                },
                {
                    "sent": "Legendre and Adrian were interested in astronomy and some other applications and they described an algorithm that in modern language takes N * B squared times, basically Gaussian elimination O N * D squared will be something that will get back to you, so keep that in the back of your mind.",
                    "label": 0
                },
                {
                    "sent": "And since then, yeah, lots and lots of applications, bioinformatics, medicine, engineering control theory... computer science, interesting analysis and then a lot of these applications you're doing some sort of fitting the parameters of some model to experimental data.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a couple of ways to view this problem.",
                    "label": 0
                },
                {
                    "sent": "One is, you know, is this sort of the right thing to be doing given the data two is, you know how long does it take, you know.",
                    "label": 0
                },
                {
                    "sent": "So there's a range of ways you can do this.",
                    "label": 0
                },
                {
                    "sent": "For a moment, let's just not ask, is this the right thing to do?",
                    "label": 0
                },
                {
                    "sent": "But ask how long will this take?",
                    "label": 0
                },
                {
                    "sent": "So open up Golovin, Vanloan or some standard textbook on numerical linear algebra and how long it will it take so there's a couple of ways to solve this, and these will be exact method as opposed to iterative methods mentioned something about that in a little bit.",
                    "label": 0
                },
                {
                    "sent": "There's something called the Cholesky decomposition, so if a is nice in certain senses.",
                    "label": 1
                },
                {
                    "sent": "Then you can decompose A transpose A as R, transpose R and then solve the normal equations on R. Transpose are now in general.",
                    "label": 0
                },
                {
                    "sent": "It's a very, very bad thing to do, and the reason is that in less A is very nice and very well conditioned.",
                    "label": 0
                },
                {
                    "sent": "You've amplified the complexity, measure the so-called condition number and things are going to get very bad, so it's typically done is something called the QR decomposition that's going to be slower, but more numerically stable, especially if as conditioning issues there's a bunch of different variants of QR, right?",
                    "label": 0
                },
                {
                    "sent": "Revealing QR full strong low high will get back to little bit about those in a little bit, in which case you're going to write a SQR and solve this system and you can also do the so called singular value decomposition, which is overkill, but you could do it if you want your right as U Sigma V transpose an then U&V are well conditioned.",
                    "label": 1
                },
                {
                    "sent": "They are easy to invert and so you can write the optimal vector acts this way so these differ a lot in practice.",
                    "label": 0
                },
                {
                    "sent": "They all take some constant.",
                    "label": 0
                },
                {
                    "sent": "And the constant depends times N times dsquared time.",
                    "label": 0
                },
                {
                    "sent": "So for a moment step back a little bit and Ann view them all as sort of the same.",
                    "label": 0
                },
                {
                    "sent": "Although in terms of stability in terms of constants only will differ a lot and they'll all computer vector X such that.",
                    "label": 0
                },
                {
                    "sent": "X is equal to the pseudoinverse, which is the inverse of a on the column row space times B.",
                    "label": 1
                },
                {
                    "sent": "Now one of the tricky things about one of those sort of take home lessons about a lot of these linear algebra problems is that they can be sort of unstable or tricky and in certain subtle ways.",
                    "label": 0
                },
                {
                    "sent": "So what you're doing here is taking a pseudoinverse.",
                    "label": 0
                },
                {
                    "sent": "If A is instead of being ranked D, it's really rank D -- 1.",
                    "label": 0
                },
                {
                    "sent": "You want to say.",
                    "label": 0
                },
                {
                    "sent": "Well, I should just ignore that other dimension and you can do that.",
                    "label": 0
                },
                {
                    "sent": "But if it's rank D -- 1 But has some little epsilon in the deep space, if you do that, you blow up epsilon to one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "So people often time to do is add some prior, add some regularization term and you can do that.",
                    "label": 0
                },
                {
                    "sent": "You can do that, but then you're changing the problem, solving a different problem and that may or may not be the right thing to do.",
                    "label": 0
                },
                {
                    "sent": "But you can do that so.",
                    "label": 0
                },
                {
                    "sent": "Another is that we are actually examples of sort of issues.",
                    "label": 0
                },
                {
                    "sent": "One of the sort of pet peeves I have is so you know if a matrix.",
                    "label": 0
                },
                {
                    "sent": "Is how to how does this work?",
                    "label": 0
                },
                {
                    "sent": "If if a matrix is?",
                    "label": 0
                },
                {
                    "sent": "Ranked degenerate then its determinant is 0, right?",
                    "label": 0
                },
                {
                    "sent": "Does the converse follow?",
                    "label": 0
                },
                {
                    "sent": "If it's zero, is it that it's ranked in general?",
                    "label": 0
                },
                {
                    "sent": "If it's, if the determines approximately 0, then is the matrix going to approximately rank to generate?",
                    "label": 0
                },
                {
                    "sent": "So who has an answer to that, aside from maybe energy too?",
                    "label": 0
                },
                {
                    "sent": "I saw flinched when I ask that question.",
                    "label": 0
                },
                {
                    "sent": "So think of something about as well conditioned as you can have right an identity matrix and maybe multiply all its elements by 1/2, so that shouldn't change anything.",
                    "label": 0
                },
                {
                    "sent": "And if it's an end by inmate identity matrix then you're going to have 1/2 to the end.",
                    "label": 0
                },
                {
                    "sent": "So exponentially bad.",
                    "label": 0
                },
                {
                    "sent": "So there is another example of sort of the same instability that you see here where.",
                    "label": 0
                },
                {
                    "sent": "No, a naive application of sort of off the shelf algorithms will not do so well.",
                    "label": 0
                },
                {
                    "sent": "OK, so back to the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Squares problem so.",
                    "label": 0
                },
                {
                    "sent": "So this is how long it takes N times dsquared time and you have to be careful about these stability issues and conditioning issues.",
                    "label": 0
                },
                {
                    "sent": "But in times eastward.",
                    "label": 0
                },
                {
                    "sent": "So when is this the right thing to do?",
                    "label": 0
                },
                {
                    "sent": "So I told you you want to work with least squares.",
                    "label": 1
                },
                {
                    "sent": "One is the right thing to do.",
                    "label": 1
                },
                {
                    "sent": "This is the right thing to do in the following case one the relationship between the outcome and predictors is roughly linear.",
                    "label": 0
                },
                {
                    "sent": "And when the error term is nice, maybe mean zero, constant variance errors are uncorrelated, normally distributed and so on and so forth.",
                    "label": 1
                },
                {
                    "sent": "They don't even need to be normal.",
                    "label": 1
                },
                {
                    "sent": "This need to be a nice enough so that you have large enough samples to rely on large sample theory.",
                    "label": 0
                },
                {
                    "sent": "Now in practice, of course these will not hold perfectly, so the prudent machine learner data analyst will check the degree to which these assumptions will be violated.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How would you do that?",
                    "label": 0
                },
                {
                    "sent": "So there's a range of ways to do that, but one very common way to do that is to use something called so called regression diagnostics.",
                    "label": 0
                },
                {
                    "sent": "So what we what we have here is on the top left, the Model B is equal to X plus epsilon B's response.",
                    "label": 1
                },
                {
                    "sent": "They have the so called carriers.",
                    "label": 0
                },
                {
                    "sent": "Epsilon is the noise process X opt.",
                    "label": 0
                },
                {
                    "sent": "Is this thing A transpose, a inverse?",
                    "label": 0
                },
                {
                    "sent": "A transposed be?",
                    "label": 0
                },
                {
                    "sent": "That's an ugly expression, but if you're familiar with linear algebra is something you come to know and love.",
                    "label": 0
                },
                {
                    "sent": "That's basically the projection onto the span of a or.",
                    "label": 0
                },
                {
                    "sent": "I guess I should say when you multiply that by it is.",
                    "label": 0
                },
                {
                    "sent": "So be.",
                    "label": 0
                },
                {
                    "sent": "Is equal to HB prime.",
                    "label": 0
                },
                {
                    "sent": "The predicted value of B is equal to H * B. Alright, this is just what I had on the previous page.",
                    "label": 0
                },
                {
                    "sent": "The tax office is the same thing we computed, and if you multiply that by a Member ax equals B, so aksak should be something close to the approximation of B, then B prime.",
                    "label": 0
                },
                {
                    "sent": "I should have put a hat there, but I couldn't get it in PowerPoint.",
                    "label": 0
                },
                {
                    "sent": "Be prime is equal to H times be, so H is this ugly thing.",
                    "label": 0
                },
                {
                    "sent": "It's just a projection matrix.",
                    "label": 0
                },
                {
                    "sent": "It's a projection matrix onto the columns of a. I could have written it, you transpose you where you is any projection matrix onto the span of a OK.",
                    "label": 0
                },
                {
                    "sent": "So we want to be talking bout this projection matrix and hi, Jay will measure the leverage or importance or influence in a certain precise sense exerted on the value of the predicted value of B by the J value of what you actually measured.",
                    "label": 1
                },
                {
                    "sent": "So BJ's, which you measured by primers which you predict.",
                    "label": 0
                },
                {
                    "sent": "And this is independent of B, since H is a projection matrix under the span of a. Alright, so the off diagonal elements are telling you something about how the predictions you're making depend on the things you measured.",
                    "label": 1
                },
                {
                    "sent": "In terms of sensitivity, the vector of the vector of residuals will depend on this.",
                    "label": 0
                },
                {
                    "sent": "There will be the expectation will be 0, but they'll be nontrivial variance.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, if there's some sensitivities.",
                    "label": 0
                },
                {
                    "sent": "Encoded in this matrix H, then that will tell you where your predictions are going to be sensitive.",
                    "label": 0
                },
                {
                    "sent": "In particular, don't let's not worry about the off diagonal, so just worry about the diagonal elements.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, with this is saying is that if the diagonal element is particularly large, that's a particularly sensitive data point.",
                    "label": 0
                },
                {
                    "sent": "It's a particularly important data point, so intuitively which you want to be saying is that.",
                    "label": 0
                },
                {
                    "sent": "Is that was the chocolate.",
                    "label": 0
                },
                {
                    "sent": "Oh here, so you know what you want to say is, if I want to do a least squares fit and this is my data.",
                    "label": 0
                },
                {
                    "sent": "This very I mean, what's the most important data point?",
                    "label": 0
                },
                {
                    "sent": "So the most influential data point right now?",
                    "label": 0
                },
                {
                    "sent": "Maybe you kicked the machine and that's an error, in which case you should do this sort of diagnostics and filter that out.",
                    "label": 0
                },
                {
                    "sent": "But maybe that's a real data point.",
                    "label": 0
                },
                {
                    "sent": "And maybe it's a real data point, because you know you shouldn't be doing L2 modeling 'cause the world is more complicated, but you're doing it for convenience or something.",
                    "label": 0
                },
                {
                    "sent": "And if that's the most important data point, then maybe you should pay particular attention to it.",
                    "label": 0
                },
                {
                    "sent": "And one way to quantify that is since the trace of H. It's a projection onto a D dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Since the trace of HSD, maybe a diagnostic rule of thumb and this is been used by statisticians is to say if the dynamics are greater than two D over and twice the average value of twice the expected value, then investigate and maybe a kick the machine and it's an outline which case you get rid of it.",
                    "label": 0
                },
                {
                    "sent": "Or maybe it's a particularly important data point.",
                    "label": 0
                },
                {
                    "sent": "So H this ugly expression, as I said, can be written as U transpose U.",
                    "label": 0
                },
                {
                    "sent": "Which is which is.",
                    "label": 0
                },
                {
                    "sent": "Where were you as any orthogonal matrix under the span of A and in particular the diagonal elements of H can be written as the dot product of the rows of you with itself.",
                    "label": 0
                },
                {
                    "sent": "So you saw in the previous talk.",
                    "label": 0
                },
                {
                    "sent": "I think it was called be there that be if correlation links decays slowly or something, then the links of all the rows of B are going to be roughly uniform and with those with that saying is that information doesn't propagate in particularly bad ways.",
                    "label": 0
                },
                {
                    "sent": "So we're saying here is that the lengths of the rows of orthogonal.",
                    "label": 0
                },
                {
                    "sent": "Bases onto the span of the columns of of a. Encode information about how sensitive things are.",
                    "label": 0
                },
                {
                    "sent": "What things are particularly.",
                    "label": 0
                },
                {
                    "sent": "Influential or particularly important in terms of having bad decay, correlational properties, so this will be a central thing to what we're going to be interested in.",
                    "label": 0
                },
                {
                    "sent": "There's a lot more information in this projection made.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tricks and I don't know how good this is.",
                    "label": 0
                },
                {
                    "sent": "This is a 10 by 10 example and this is basically the same picture I drew here, except a little bit more realistic and you might want to say that that guy up there or this guy down here are particularly influential.",
                    "label": 0
                },
                {
                    "sent": "And if you look at all the numbers in here, you'll see that in fact that's the case.",
                    "label": 0
                },
                {
                    "sent": "If you look at the diagonal elements.",
                    "label": 0
                },
                {
                    "sent": "If you look at the off diagonal elements will tell you which ones are positively negatively correlated and all that sort of stuff so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How uniform and non uniform of these leverage scores in quote real data?",
                    "label": 0
                },
                {
                    "sent": "Soap.",
                    "label": 0
                },
                {
                    "sent": "The diagnostic rule from the statisticians used is to say if the leverage scores are greater than two or 3D / N two or three times the average or expected value flag it as a potential outlier.",
                    "label": 0
                },
                {
                    "sent": "Investigate so this is the Enron email matrix.",
                    "label": 0
                },
                {
                    "sent": "So think of this as a large social information graph because you get similar results in a lot of those.",
                    "label": 0
                },
                {
                    "sent": "And what I've plotted here is the index of the I TH highest leverage term.",
                    "label": 0
                },
                {
                    "sent": "So I've computed all these leverage scores I've set.",
                    "label": 0
                },
                {
                    "sent": "I think the dimensionality parameter be 10 years, I think it's 10.",
                    "label": 0
                },
                {
                    "sent": "And I looked at the first, second, third, 4th fifth highest leverage term and on the Y axis I will plot the cumulative leverage.",
                    "label": 0
                },
                {
                    "sent": "The leverage of the first delivered the 1st to the 1st three, the first 4.",
                    "label": 0
                },
                {
                    "sent": "The Matrix is 92,000 by by whatever.",
                    "label": 0
                },
                {
                    "sent": "And so D / N is 10 to the minus four and the highest leverage term is 10 to the minus two SO2 orders of another factor of two or three after 100.",
                    "label": 0
                },
                {
                    "sent": "And the second highest leverage term is just marginally less, and the third highest leverage is marginally less, and the fourth highest leverage is marginally less.",
                    "label": 0
                },
                {
                    "sent": "So by the by the metrics, the traditional methods of sort of statistical data analysis or regression diagnostics, you have a huge huge number of things that are obscenely outlying.",
                    "label": 0
                },
                {
                    "sent": "Now, if you think about it for a second, that shouldn't be surprising here.",
                    "label": 0
                },
                {
                    "sent": "I mean, the Enron data is incredibly sparse.",
                    "label": 0
                },
                {
                    "sent": "Right, there's no.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "It's not like you have roughly an non zero entries per row.",
                    "label": 0
                },
                {
                    "sent": "There's no reason to think, measure concentrates.",
                    "label": 0
                },
                {
                    "sent": "There's no reason to think SVD or PCA or L2 based measures of the right thing to do.",
                    "label": 0
                },
                {
                    "sent": "Now that being the case, this these sort of techniques of the heart of things like light and semantic indexing and hits and page rank in these sorts of things and these models are used for a range of reasons, but one because if you want more realistic statistical modeling, you're going to have to give yourself an incredible amount more descriptive flexibility, and that will give yourself a lot more places to hide your mistakes, namely overfit, and so you know you work, you're making decisions about the appropriateness of.",
                    "label": 0
                },
                {
                    "sent": "Statistical models not based on statistical considerations but based on computational considerations.",
                    "label": 0
                },
                {
                    "sent": "So that's going to be sort of an important theme, So what you see here is that what we're going to want to do is identify the non uniformity structure because we're dealing with.",
                    "label": 0
                },
                {
                    "sent": "Matrices L2 based penalties the non uniformity structure in the two applications I'll be talking about will be defined by these leverage scores, but you get incredible.",
                    "label": 0
                },
                {
                    "sent": "I mean, every data point looks like that you get an incredible number of very, very far outlying terms so.",
                    "label": 0
                },
                {
                    "sent": "So is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is sort of the rough idea clear?",
                    "label": 0
                },
                {
                    "sent": "I mean, I hope at least the rough perspectives clear.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do is talk about two particular matrix problems.",
                    "label": 0
                },
                {
                    "sent": "So there's been a lot of work done in theoretical computer science over the last 10 or 12 years on randomized algorithms for matrices.",
                    "label": 0
                },
                {
                    "sent": "And a lot of this wasn't immediately applicable.",
                    "label": 0
                },
                {
                    "sent": "I mean, it had a lot of new ideas, but a lot of it wasn't immediately applicable to traditional problems in linear algebra because at the end of the Devils in the details, and if you have a slightly different formulation of a problem, sometimes it's hard to web the tool.",
                    "label": 0
                },
                {
                    "sent": "So the two problems will be talking bout a very traditional numerical linear algebra problems and will get better algorithms better.",
                    "label": 0
                },
                {
                    "sent": "And worst case theory, but which I mean improved quality of approximation guarantees and or running time just theorems.",
                    "label": 0
                },
                {
                    "sent": "And in addition, will be better in practice, and I'll say the sense in which it will be better in practice, and roughly the sense in which is going to be better in practice is because the album will be randomized, and instead of fitting doing computation of fitting to 10 digits of precision, in which case intuitively might be overfitting to the particular data set, you have what we want to say is why don't we?",
                    "label": 0
                },
                {
                    "sent": "Why don't we smooth things out a little bit by drawing a random sample and then only roughly fitting and?",
                    "label": 0
                },
                {
                    "sent": "So then we're not going to find the particular data set, and so the randomization will.",
                    "label": 0
                },
                {
                    "sent": "Implicitly do a form of statistical regularization.",
                    "label": 0
                },
                {
                    "sent": "What kind of regularization term, on or anything but the inside the algorithm you look inside the black box inside the algorithm?",
                    "label": 0
                },
                {
                    "sent": "Implicitly it will be doing that sort of regularization.",
                    "label": 0
                },
                {
                    "sent": "So the first problem I want to be talking about is in fact the problem I just talked about the least squares approximation problem.",
                    "label": 0
                },
                {
                    "sent": "The second problem will be the problem of choosing exactly K columns from matrix feature selection if you will, and then I'll talk about how they perform in practice.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is an algorithm for least squares approximation and is near and dear to my heart because we thought about it for a long time, but also the original paper the wrong way to view this algorithm.",
                    "label": 0
                },
                {
                    "sent": "But a fairway is that this is an algorithm that's no faster than the best previously existing algorithms and is worse.",
                    "label": 0
                },
                {
                    "sent": "Alright, so it's going to be faster than the traditional algorithms and will give you a worse result.",
                    "label": 0
                },
                {
                    "sent": "The reason I want to start with this is that if you understand the key idea here, everything else will be easy and will in fact be able to do things faster and better.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm here is incredibly simple.",
                    "label": 0
                },
                {
                    "sent": "Fix some set of probabilities.",
                    "label": 1
                },
                {
                    "sent": "Bullet wants us.",
                    "label": 0
                },
                {
                    "sent": "Fix some set of probabilities.",
                    "label": 0
                },
                {
                    "sent": "From one to end, so we're going to be doing least squares approximation.",
                    "label": 0
                },
                {
                    "sent": "It's N by D. Fix some set of probabilities over the rows.",
                    "label": 0
                },
                {
                    "sent": "Alright, if you ever see a paper with this uniform sampling done, uniform sampling as good as a straw man, and I'll mention that later.",
                    "label": 0
                },
                {
                    "sent": "But because of the non uniformly structure was talking about this, oftentimes something a lot more refined going on and so the devil will be in the detail here in terms of getting a good set of probabilities alright?",
                    "label": 0
                },
                {
                    "sent": "Once you have that probability distribution fixed.",
                    "label": 0
                },
                {
                    "sent": "Bullet two says pick the I throw away in the element of B with basically those probabilities.",
                    "label": 1
                },
                {
                    "sent": "The R is going to be a parameter, but basically it's it's choose the ice constraint with probabilities proportional to that important sampling distribution.",
                    "label": 0
                },
                {
                    "sent": "And we're going to choose our to be greater than D. It's going to be D. Log D or so.",
                    "label": 0
                },
                {
                    "sent": "And so in particular, you're still going to be over constrained.",
                    "label": 0
                },
                {
                    "sent": "And then call whatever your favorite solver is.",
                    "label": 0
                },
                {
                    "sent": "The solver could be QR.",
                    "label": 0
                },
                {
                    "sent": "It could be the SVD.",
                    "label": 1
                },
                {
                    "sent": "It could be some iterative method.",
                    "label": 0
                },
                {
                    "sent": "So call your black box and solve the induced subproblem.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So that was very simple.",
                    "label": 0
                },
                {
                    "sent": "Three steps compute some distribution to the small number of columns and solve the subproblem, the subproblem, the vector X is the same dimension, right?",
                    "label": 0
                },
                {
                    "sent": "You've sampled constraints, you haven't sampled variables, so I'm going to give you back a vector X that's the same dimension as the original problem.",
                    "label": 0
                },
                {
                    "sent": "You is going to be any basis for the subspace the left subspace of a.",
                    "label": 0
                },
                {
                    "sent": "So if you write the SVD of a, it's equal to U Sigma V transpose.",
                    "label": 0
                },
                {
                    "sent": "Call it QR if this is the exact left subspace.",
                    "label": 0
                },
                {
                    "sent": "Different issues as you know in QR but.",
                    "label": 0
                },
                {
                    "sent": "And and the reason is that you don't need to do this full computation, you need some information.",
                    "label": 0
                },
                {
                    "sent": "Here U Sigma V transpose so you will be a basis for the left subspace, in particular the projection.",
                    "label": 0
                },
                {
                    "sent": "On to a was a is equal to UU transpose, and so any basis for the subspace is you.",
                    "label": 0
                },
                {
                    "sent": "The best result this week when he was SPD.",
                    "label": 0
                },
                {
                    "sent": "Uses the SVD basis.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's why we're going to be faster in this particular case, yeah, so let RBD log D log one over DD is going to be a failure probability.",
                    "label": 0
                },
                {
                    "sent": "Bait is a fudge factor.",
                    "label": 0
                },
                {
                    "sent": "We'll get back to later.",
                    "label": 0
                },
                {
                    "sent": "That's gonna be important factor for later one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "I think it's actually well.",
                    "label": 0
                },
                {
                    "sent": "This will suffice and we can actually get a little bit better than that, but so it's D log D and he can do an iterative method to drive down the failure.",
                    "label": 0
                },
                {
                    "sent": "Probably not the error on epsilon, so this is just think of this as a one pass thing, but if you want to do something if you could make that log one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "But let me not get into that for now.",
                    "label": 0
                },
                {
                    "sent": "So R is equal to the logged in the constants reasonable.",
                    "label": 1
                },
                {
                    "sent": "It's two or three if the probabilities are equal to the statistical leverage scores, the lengths of these rows.",
                    "label": 1
                },
                {
                    "sent": "Are the diagonal elements of the projection matrix of a these statistical average scores if the probabilities are equal to that maybe up to a beta fudge factor?",
                    "label": 0
                },
                {
                    "sent": "Then if you run this algorithm, you'll get a vector X head up.",
                    "label": 0
                },
                {
                    "sent": "That's relative error in the following sense.",
                    "label": 0
                },
                {
                    "sent": "The distance between the vector you compute and the exact vector.",
                    "label": 0
                },
                {
                    "sent": "Is less than epsilon times the condition number of 8 * X opt.",
                    "label": 0
                },
                {
                    "sent": "You can fold that condition on Brent if you understand the norm there, but I've written it out.",
                    "label": 0
                },
                {
                    "sent": "And if you take the vector X and plug it back into the vector X, you compute and plug it back into X = B, then you within one plus epsilon of Z, where Z is the original residual.",
                    "label": 1
                },
                {
                    "sent": "It's a xop minus B.",
                    "label": 0
                },
                {
                    "sent": "The normal that thing.",
                    "label": 0
                },
                {
                    "sent": "So relative error in both senses.",
                    "label": 0
                },
                {
                    "sent": "Now the rub here is that in order to compute these probabilities.",
                    "label": 0
                },
                {
                    "sent": "Even up to some nontrivial factor of beta.",
                    "label": 0
                },
                {
                    "sent": "Takes N times dsquared time and the way to do it is exactly synergy said compute you.",
                    "label": 0
                },
                {
                    "sent": "Alright, so it's no faster and it's worse.",
                    "label": 0
                },
                {
                    "sent": "But what we've done is identified the relevant non uniformity structure and so now the question could be in this problem or another related problems where this is under the hood.",
                    "label": 0
                },
                {
                    "sent": "Can you use this non uniformity structure in either preprocess it away?",
                    "label": 0
                },
                {
                    "sent": "Or if this is not the bottleneck on time, just deal with that or whatever and the answer to both of those will be yes.",
                    "label": 0
                },
                {
                    "sent": "In Europe.",
                    "label": 0
                },
                {
                    "sent": "Angle.",
                    "label": 0
                },
                {
                    "sent": "Basically, what we're going to do is approximating the operator a.",
                    "label": 0
                },
                {
                    "sent": "You want to think of it at that level, so the so be.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter neither.",
                    "label": 0
                },
                {
                    "sent": "Angle between yeah.",
                    "label": 0
                },
                {
                    "sent": "Spacefaring company I mean, I could point you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the original paper actually had something depending on B.",
                    "label": 0
                },
                {
                    "sent": "It turns out we didn't need that.",
                    "label": 0
                },
                {
                    "sent": "That was the weakness of the analysis.",
                    "label": 0
                },
                {
                    "sent": "I could point you the exact spot in the proof why we don't.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, I don't know why.",
                    "label": 0
                },
                {
                    "sent": "I don't know why the traditional numerical linear algebra methods needed as opposed to this not needing it.",
                    "label": 0
                },
                {
                    "sent": "This, I think is related to the epsilon or the condition number versus the condition number squared.",
                    "label": 0
                },
                {
                    "sent": "I think it's going to be related to that issue.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the game is another fudge factor.",
                    "label": 0
                },
                {
                    "sent": "Don't worry bout it.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a faster algorithm, so we're dealing.",
                    "label": 0
                },
                {
                    "sent": "Just deal with the least squares problem.",
                    "label": 0
                },
                {
                    "sent": "He's going to be faster algorithm, and the faster algorithm will be the following.",
                    "label": 0
                },
                {
                    "sent": "Pre process A&B with what I'll call a randomized Hadamard transform.",
                    "label": 1
                },
                {
                    "sent": "And by randomized Hadamard transform, I'll say what I mean.",
                    "label": 0
                },
                {
                    "sent": "But a Hadamard transform is a real version of a Fourier transform, so it's 111 negative one, recursively defined and normalized.",
                    "label": 0
                },
                {
                    "sent": "So think of it as a Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "I'll say exactly what I mean by a minute.",
                    "label": 0
                },
                {
                    "sent": "The point about that is that you're going to uniform eyes.",
                    "label": 0
                },
                {
                    "sent": "The relevant non uniformity structure, and so then you can sample uniformly.",
                    "label": 0
                },
                {
                    "sent": "Now you haven't exactly uniform is that you only approximately uniform isatin, so you need the login log that messy thing, so slightly more.",
                    "label": 0
                },
                {
                    "sent": "But think of it as basically the D log D that we had before, and I'll on the slider to say exactly what these numbers mean, but.",
                    "label": 0
                },
                {
                    "sent": "The point is now the bottleneck will be in this preprocessing, you preprocess with randomized Hadamard transform and uniformly sample and uniform sampling is very easy solve the induced sub problem and if you solve into a subproblem you'll get the one plus or minus epsilon approximation and this is an expression only a theoretical computer scientist could love, but that's how much time it will take.",
                    "label": 0
                },
                {
                    "sent": "This will be basically in the log of D. Alright, so little oh of ND squared.",
                    "label": 0
                },
                {
                    "sent": "And if you have an intelligent implementation of the Hadamard transform, that'll be faster.",
                    "label": 0
                },
                {
                    "sent": "So Tiger in Rocklin have an implementation of a variant of this where they do an iterative thing and I think they're faster on matrices as small as 1000 or 2500, so very reasonable size matrices.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How does this work?",
                    "label": 0
                },
                {
                    "sent": "So here's a structural lemma which gets to introduce you.",
                    "label": 1
                },
                {
                    "sent": "Let's preprocess this with.",
                    "label": 1
                },
                {
                    "sent": "You know this least squares system with Kai, where Kai is any preprocessing matrix.",
                    "label": 0
                },
                {
                    "sent": "Any anything whatsoever, deterministic, randomized, wherever you like.",
                    "label": 0
                },
                {
                    "sent": "And assume that the singular values of you are all one or zero, so right, they're all one, so the singular values of \u03c0 times you are all about one.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "You is perpendicular to be.",
                    "label": 0
                },
                {
                    "sent": "Perp would be purpose defined to be the piece of be perpendicular to the aorta UA.",
                    "label": 0
                },
                {
                    "sent": "So you is still roughly perpendicular to be perp.",
                    "label": 0
                },
                {
                    "sent": "Once you've done the preprocessing.",
                    "label": 0
                },
                {
                    "sent": "If you satisfy those two conditions, then you get relative error.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine having algorithm checking that and if the answer that's yes, you're done, use that as your preprocessor.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if you don't want to check that the randomized Hadamard will do exactly that, so randomized Hadamard, let H be an end by end of terministic Hadamard matrix.",
                    "label": 1
                },
                {
                    "sent": "For technical reasons, you need to preprocess that with a + -- 1 random matrix, and I can talk offline about why that is.",
                    "label": 0
                },
                {
                    "sent": "The point is that the Hadamard matrix.",
                    "label": 0
                },
                {
                    "sent": "Is an orthogonal matrix.",
                    "label": 0
                },
                {
                    "sent": "It's a unitary matrix, and so multiplying the linear system doesn't change the solution, right?",
                    "label": 1
                },
                {
                    "sent": "'cause it's just a rotation basically.",
                    "label": 0
                },
                {
                    "sent": "Multiplication is fast.",
                    "label": 0
                },
                {
                    "sent": "Basically we're doing a random projection, but the typical way to do random projections would be slow 'cause you have to do a dense right into projection the Hadamard.",
                    "label": 0
                },
                {
                    "sent": "You should think of as approximating a random projection.",
                    "label": 0
                },
                {
                    "sent": "And approximating it in because you can do fast Fourier techniques an log in, but in fact you only need to touch the elements that are going to sample, so it's in lorgar and so that ugly complexity expression I showed you is basically this.",
                    "label": 1
                },
                {
                    "sent": "An multiplication by this randomized Hadamard, approximately uniform sizes, all the leverage scores.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the leverage scores in the process system there ALDI over N up to that log factor and that's why you got this slightly uglier expression.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do the same thing with projections also.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The randomized Hadamard transform is a thing, and Chazelle basically used, and then we have uniform eyes.",
                    "label": 0
                },
                {
                    "sent": "The leverage scores.",
                    "label": 0
                },
                {
                    "sent": "Then we sample uniformly the projection thing that I'm skipping over is basically exactly what they did.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we're saying is if we've identified the relevant non uniform structure, then we can preprocess it away and the theory goes through and on.",
                    "label": 0
                },
                {
                    "sent": "As I said, I think it's on the order of 1000.",
                    "label": 0
                },
                {
                    "sent": "Two 1100 systems will be faster than using exact methods.",
                    "label": 0
                },
                {
                    "sent": "So the second problem I want to mention is a slightly different problem.",
                    "label": 0
                },
                {
                    "sent": "But will be very related.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Say that you have a big matrix M by N and you want to choose K columns and you might want to skate columns 'cause these are the key features that matter and you might want to choose K columns 'cause you're interested in subspace.",
                    "label": 0
                },
                {
                    "sent": "They define.",
                    "label": 0
                },
                {
                    "sent": "The latter is more common in numerical linear algebra.",
                    "label": 0
                },
                {
                    "sent": "The former and data analysis and machine learning.",
                    "label": 0
                },
                {
                    "sent": "But for whatever reason, the goal is to get K columns.",
                    "label": 0
                },
                {
                    "sent": "How can you do it and how well can you do it?",
                    "label": 0
                },
                {
                    "sent": "And so we'll see is using.",
                    "label": 0
                },
                {
                    "sent": "These ideas will have an algorithm that will do.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, so here's the.",
                    "label": 0
                },
                {
                    "sent": "The so called column subset selection problem.",
                    "label": 0
                },
                {
                    "sent": "So, given an M by N matrix A and a rank parameter Ki want to choose exactly K columns of a. NA is adversarially given here, right?",
                    "label": 1
                },
                {
                    "sent": "I mean they can be anything if you want to think of this random sampling that we did in the previous step.",
                    "label": 0
                },
                {
                    "sent": "And in this case as something like generating a particular data set when you wake up in the morning and you get a particular data set, you could imagine that we haven't formulated it that way.",
                    "label": 0
                },
                {
                    "sent": "But you could imagine formatting it that way, in which case the non uniformity structure has to do with the generative process.",
                    "label": 0
                },
                {
                    "sent": "And you could imagine implicitly downsampling or possibly taking advantage of that.",
                    "label": 0
                },
                {
                    "sent": "And that's actually an interesting direction to think about, but for right now the Matrix A is given rank, premise, given with choosing exactly K columns of a.",
                    "label": 0
                },
                {
                    "sent": "Such that the M by K matrix CM as a number of rows and K as the number of columns.",
                    "label": 0
                },
                {
                    "sent": "So the end by K matrix C minimizes some error and we're going to be interested in is a spectral norm error in the Frobenius norm error.",
                    "label": 0
                },
                {
                    "sent": "So if you have a matrix A, the spectral norm is the largest singular value of a sort of a worst case norm.",
                    "label": 0
                },
                {
                    "sent": "It's the direction.",
                    "label": 0
                },
                {
                    "sent": "Well, it's the amount by which a stretch is a vector the most in any direction.",
                    "label": 0
                },
                {
                    "sent": "The Frobenius norm is more of an averaging norm.",
                    "label": 0
                },
                {
                    "sent": "You sum over all the directions of ES, and you sum the squares of the singular values of the elements of a.",
                    "label": 0
                },
                {
                    "sent": "So we're going to be from two different norms.",
                    "label": 0
                },
                {
                    "sent": "And um, for range of reasons in numerical linear algebra, you're typically interested in the spectral norm in a lot of applications in machine learning and data analysis.",
                    "label": 0
                },
                {
                    "sent": "Here intervene IAS norm basically because you're in a bunch of different directions and you want the error on every particular thing, so that might be why intervene IAS normal?",
                    "label": 0
                },
                {
                    "sent": "So P subsea is like that it's a projection.",
                    "label": 0
                },
                {
                    "sent": "Onto the matrix, say on the Matrix C, which you can write as C * A pseudoinverse of.",
                    "label": 0
                },
                {
                    "sent": "See now that's a bad way to compute it, which you should do is compute the left singular vectors of C and do you subsea times, U sub C transpose, and it's a bad way for conditioning reasons, not 'cause it's not right in principle, so the complexity this problem this, or variants of this will be intractable.",
                    "label": 0
                },
                {
                    "sent": "To compute exactly.",
                    "label": 0
                },
                {
                    "sent": "And so the question might be, can we come up with some approximation?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the usual way approximation algorithm for this problem will be parameterized will be the following.",
                    "label": 0
                },
                {
                    "sent": "We know a lower bound, right?",
                    "label": 0
                },
                {
                    "sent": "I want exact columns and I want to optimize those two objectives if I if I relax the problem and I say listen, I don't want to take columns exactly, but I want a linear combinations of columns, then the answer to these two objectives is to do the singular value decomposition, keep the tip top K eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "So that will be a lower bound for these problems.",
                    "label": 0
                },
                {
                    "sent": "So for any M by K matrix C. The projection onto the singular vectors will be a lower bound for the projection onto those columns, and So what we want is defined as.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of columns such that were not much worse such that.",
                    "label": 0
                },
                {
                    "sent": "Such that weren't, you know, a factor of something where something is not too bad.",
                    "label": 0
                },
                {
                    "sent": "Worse, so in numerical linear algebra have been huge amount of work on this, and the algorithms are deterministic and typically greedy.",
                    "label": 1
                },
                {
                    "sent": "By greedy I mean they will make a decision about which column to choose, which column to pivot on at a given step, and then they'll keep that column.",
                    "label": 0
                },
                {
                    "sent": "Sometimes I swap things back and forth, which is slightly less naively greedy method, but I'll get back to later a bunch of different algorithms for doing.",
                    "label": 0
                },
                {
                    "sent": "A related problem having to do with rank revealing QR and one way to view that is just to treat them all the same.",
                    "label": 0
                },
                {
                    "sent": "They're all black box.",
                    "label": 0
                },
                {
                    "sent": "They all have whatever theorems associated with them, and I'm just going to run with it a different way to say what's going on in the black box.",
                    "label": 0
                },
                {
                    "sent": "There's an incredible amount of complexity going on the black box, but at a very high level.",
                    "label": 0
                },
                {
                    "sent": "Think of it as the following.",
                    "label": 0
                },
                {
                    "sent": "A lot of these algorithms, the difference between them, is how they make decisions about what's called pivot rules, so you know you have a bunch of columns here and you need to decide whether to swap another column in or not swap it in as you look at different columns of matrix.",
                    "label": 0
                },
                {
                    "sent": "And whether you want to keep the column depends on the columns you have.",
                    "label": 0
                },
                {
                    "sent": "The spectrum of the columns you have, the spectrum of the columns you don't have.",
                    "label": 0
                },
                {
                    "sent": "These sorts of things, and so the decision as to whether you keep the column will depend on that, and if you make more sophisticated decisions, you're making more sophisticated private rules and typically will get better.",
                    "label": 0
                },
                {
                    "sent": "Worst case theorems and hopefully do better in practice.",
                    "label": 0
                },
                {
                    "sent": "And if you make less sophisticated decisions, you might do worse in theory, and maybe you do better or worse in practice, so we'll get back to that.",
                    "label": 0
                },
                {
                    "sent": "But think of that as sort of a slight refinement of bullet one.",
                    "label": 0
                },
                {
                    "sent": "So as I said, this problem will have very close connections to rank, revealing QR, basically QR and rank, revealing QR.",
                    "label": 0
                },
                {
                    "sent": "You don't just want to bound on the columns, you keep the bottom part of the spectrum of the columns to keep you about 1 pounds in the top part of the spectrum.",
                    "label": 1
                },
                {
                    "sent": "You don't keep in a certain cross term, so very close connections rank, revealing QR.",
                    "label": 0
                },
                {
                    "sent": "The strongest results now for the spectral norm will be a minus the projection of the columns will be root K root N. OK ruttkay ruden so keep that in the back.",
                    "label": 0
                },
                {
                    "sent": "Your mind will get back to that and that takes a certain amount of time and you know how much time it takes you know.",
                    "label": 0
                },
                {
                    "sent": "Depends on who you ask.",
                    "label": 0
                },
                {
                    "sent": "The wave theoretical computer scientist parameterized the problem.",
                    "label": 0
                },
                {
                    "sent": "You know the matrix is given and I want to get some exact answer in some sense, in which case if is a very small gap between the case and the K plus first singular vector, you need to do some huge number of iterations depending on the logarithm of that cap.",
                    "label": 0
                },
                {
                    "sent": "In America linear algebra they'll say well, things are continuous and you do a stability issues.",
                    "label": 0
                },
                {
                    "sent": "It makes no sense if they're that close, just return the other one.",
                    "label": 0
                },
                {
                    "sent": "And in terms of these norm measures, you'll get the same answer, which is a fair, which is a fair observation.",
                    "label": 0
                },
                {
                    "sent": "For now, I just want to say it will take some time.",
                    "label": 0
                },
                {
                    "sent": "T. Alright, if you want to compute the full thing, it will be M * N ^2 if you want it a little faster, it takes some time T. And the reason is that for this algorithm, we're not going to be faster than the last one.",
                    "label": 0
                },
                {
                    "sent": "We were faster and had this approximation guarantee one plus epsilon.",
                    "label": 0
                },
                {
                    "sent": "Here we're not going to be faster.",
                    "label": 0
                },
                {
                    "sent": "We're going to take time T basically 'cause we want to compute that basis, but will be better in terms of quality of approximation guarantees and how it performs an application.",
                    "label": 1
                },
                {
                    "sent": "So it takes some time T, so that's how long that is the strongest results for the Frobenius norm are root K, root in by the spectral.",
                    "label": 0
                },
                {
                    "sent": "Which is maybe not so good, but it's not time T it's just an exhaustive enumeration.",
                    "label": 0
                },
                {
                    "sent": "This is an existential result.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's many if you want to call this quantity.",
                    "label": 0
                },
                {
                    "sent": "P of NNK There's been a huge amount of work and this must be about 10% of the papers that you could dig up on this problem on improving the factor of P. Some of these things have implementations, some don't, so huge amount of work on this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More recently, in theoretical computer science, there's been a lot of work, unrelated problems, low rank approximations, maybe in streaming or pass efficient models where where the data so large that you can't store them on disk.",
                    "label": 0
                },
                {
                    "sent": "And these algorithms are typically randomized.",
                    "label": 0
                },
                {
                    "sent": "Very much of the flavor of the album I gave previously, although sometimes you would compute much coarser, important sampling probabilities and still be able to do much faster, but will be randomized.",
                    "label": 0
                },
                {
                    "sent": "And if you notice we had a D log D before all these algorithms will take polynomial in K, But that is typically K log K or D log DI, change the rank parameter on you and the reason you need a K, log K or D log D is that at the heart of these algorithms.",
                    "label": 0
                },
                {
                    "sent": "There's something called coupon collecting, so you know this standard problem is, you know the Kellogg's or whoever makes Cheerios is running a contest.",
                    "label": 0
                },
                {
                    "sent": "You have different coupons and their uniformly distributed cereal boxes and how many boxes of cereal do you need to win the game which is defined to be of 1 coupon of each.",
                    "label": 0
                },
                {
                    "sent": "And so you need order D log D. Or cardinality of the number of coupons, log cardinality, the number of coupons.",
                    "label": 0
                },
                {
                    "sent": "And that's just a very basic result from probability.",
                    "label": 0
                },
                {
                    "sent": "So at the heart here in terms of bounding the spectral properties, you have something like that going on, and so in particular you're going to need to choose more than D log D columns or more than the columns.",
                    "label": 0
                },
                {
                    "sent": "So we have very strong bounds for the Frobenius norm.",
                    "label": 1
                },
                {
                    "sent": "And effectively no nontrivial spectrum bounds in this literature, so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On this particular problem, the best results so far was something I had with Petros Junious, where in time T you could get a one plus epsilon approximation by choosing K log K columns, and this improved the old additive error stuff that I had with him and Canaan Impala and freezing condoms and some other people had.",
                    "label": 0
                },
                {
                    "sent": "In terms of the quality of approximation, but it was a bit slower because in those cases you could do it very very quickly.",
                    "label": 0
                },
                {
                    "sent": "Desponding Impala in a slightly different model were also able to get one plus epsilon approximation using roughly the same number of columns, and they also proved in existence result, namely that there exists K columns such that the Frobenius norm is better than root K times of Frobenius, and this is the expression I had before ruden time to spectral.",
                    "label": 1
                },
                {
                    "sent": "So these.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The things that are the sort of prior work and to get the strongest Frobenius norm bounds before we have to do is the following.",
                    "label": 1
                },
                {
                    "sent": "So the theorem is given by K matrix A.",
                    "label": 0
                },
                {
                    "sent": "There will be.",
                    "label": 0
                },
                {
                    "sent": "Order T time algorithm that picks K log K over epsilon columns.",
                    "label": 1
                },
                {
                    "sent": "Such that will probably 1 minus Delta within one plus epsilon, and this analysis you should recognize that the form of this is very similar to the regression result we had, and this analysis boils down to that regression result.",
                    "label": 0
                },
                {
                    "sent": "I mean the way we went from average relative error was to use that regression result at the end of the day, and time T is there is not a bottleneck in this case depending on your rank parameter and what your complexity which you think is important.",
                    "label": 1
                },
                {
                    "sent": "So in that sense it's different than the previous result and the way we did it as you subspace sampling probabilities to sample this many columns subspace.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sampling properties is just a name for the thing I described before.",
                    "label": 0
                },
                {
                    "sent": "Now we're sampling columns, so we want probability distribution over columns.",
                    "label": 0
                },
                {
                    "sent": "So let's write a as U the best rank K approximation to a as UK Sigma K. VK transpose a probability distribution over columns of a will correspond to a probability distribution over columns with the right singular vectors.",
                    "label": 1
                },
                {
                    "sent": "Before we had rose.",
                    "label": 0
                },
                {
                    "sent": "So we're working on this side.",
                    "label": 1
                },
                {
                    "sent": "Now we have columns, but it's all the same thing.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So up to normalization, these will just be this leverage scores that we had before.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do you bridge these two things?",
                    "label": 0
                },
                {
                    "sent": "We would like to do is sample exactly K columns, But if you look at the details of the pivot rules and you open up the black box, it's not at all clear that even keeping AK Plus first column is going to do any better.",
                    "label": 0
                },
                {
                    "sent": "Never mind keeping more than that.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the theoretical computer science approaches, there's no way instead of 1 pass to get below.",
                    "label": 0
                },
                {
                    "sent": "D log D or K log K columns for basically the coupon collecting reason.",
                    "label": 1
                },
                {
                    "sent": "So there's been some work prior to it.",
                    "label": 0
                },
                {
                    "sent": "I'll be talking about by Wolf and Liberty in Rocklin, Tygart, where they will choose K columns and get the same spectral norm bound this prior work and this is an application of the allergen cell result.",
                    "label": 1
                },
                {
                    "sent": "Basically results I described before and a nice empirical evaluation.",
                    "label": 0
                },
                {
                    "sent": "So I want to talk about is a different.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bridging here now.",
                    "label": 0
                },
                {
                    "sent": "The obvious question is, will do a two phase algorithm.",
                    "label": 0
                },
                {
                    "sent": "Randomized phase and then postprocess with some deterministic phase.",
                    "label": 1
                },
                {
                    "sent": "So that's what we're going to be having, but the first half dozen obvious determine two stage albums that you come up with will not work, and so we're going to do something a little bit more refined, but the more refined thing will give actually a lot of insight into what's going on inside the black box.",
                    "label": 0
                },
                {
                    "sent": "So we're going to be running two phase algorithm.",
                    "label": 0
                },
                {
                    "sent": "The first phase will be randomized phase.",
                    "label": 0
                },
                {
                    "sent": "It will basically be doing what we did before.",
                    "label": 0
                },
                {
                    "sent": "Computer basis for the top K right singular subspace and take time T and that's going to be as fast as the traditional numerically algebra approaches, and we're no faster.",
                    "label": 0
                },
                {
                    "sent": "We're just going to pay that cost.",
                    "label": 0
                },
                {
                    "sent": "We're not going to have.",
                    "label": 0
                },
                {
                    "sent": "Nothing was going to pay that cost to compute that basis exactly or approximately.",
                    "label": 0
                },
                {
                    "sent": "And then in the deterministic phase we're going to run a black box QR algorithm.",
                    "label": 1
                },
                {
                    "sent": "To choose from those K log K columns, exactly K columns.",
                    "label": 0
                },
                {
                    "sent": "And if you remember the quality of approximation was ruden root K, you might think that you know if we choose a good basis, we can decrease that route in.",
                    "label": 0
                },
                {
                    "sent": "And there's a lot of self analysis, but the answer is that we will be able to do that.",
                    "label": 0
                },
                {
                    "sent": "But the way we'll do that will be a little bit tricky, so I'm going to skip a couple of slides and put a picture or two on the blackboard because it will be just to convey the intuition as to how the algorithm works.",
                    "label": 0
                },
                {
                    "sent": "I think it'll be a little bit more revealing, so in the first phase we are going to.",
                    "label": 0
                },
                {
                    "sent": "Compute, let me call it.",
                    "label": 0
                },
                {
                    "sent": "I'll call it the subspace.",
                    "label": 0
                },
                {
                    "sent": "Sampling.",
                    "label": 0
                },
                {
                    "sent": "Probabilities.",
                    "label": 0
                },
                {
                    "sent": "We're going to be computing these probabilities.",
                    "label": 0
                },
                {
                    "sent": "That that that that I'll say bias you.",
                    "label": 0
                },
                {
                    "sent": "Tord outliers.",
                    "label": 0
                },
                {
                    "sent": "In the sense that.",
                    "label": 0
                },
                {
                    "sent": "They'll put a lot of weight on that guy.",
                    "label": 0
                },
                {
                    "sent": "So the bias in youth towards particularly influential things or things that stick out a bit.",
                    "label": 0
                },
                {
                    "sent": "Anne will choose order.",
                    "label": 0
                },
                {
                    "sent": "And the first phase is choose.",
                    "label": 0
                },
                {
                    "sent": "Order K log K. And then over the epsilon and all that's the same as before.",
                    "label": 0
                },
                {
                    "sent": "But choose order K, log K, and So what you've done now is choose a bunch of columns of a.",
                    "label": 0
                },
                {
                    "sent": "But you've also done is choosing a bunch of columns of the right singular vectors of a 'cause.",
                    "label": 0
                },
                {
                    "sent": "Remember that a is equal to you.",
                    "label": 0
                },
                {
                    "sent": "Sigma an since a is arbitrary V transpose.",
                    "label": 0
                },
                {
                    "sent": "So now in the deterministic phase.",
                    "label": 0
                },
                {
                    "sent": "Run a QR algorithm.",
                    "label": 0
                },
                {
                    "sent": "On the columns of VK that you chose, don't do it on A and don't do it on the columns of a that you chose.",
                    "label": 1
                },
                {
                    "sent": "Run it on the columns of VK that you chose, so I'll say the columns of VK transpose you chose.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Is now not dimensionality Amaran not the high dimension.",
                    "label": 0
                },
                {
                    "sent": "It's dimensionality K log K. So you might hope that applying Black box QR with that routine thing will give you something good here.",
                    "label": 0
                },
                {
                    "sent": "But of course you can choose a stupid set of columns, so you need to choose a smart set and choosing columns in this way is a smart set.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, it's a smart set because what you're doing is biasing yourself towards things that are outlying.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, but you're also working with VK transpose directly rather than A and because a is VK, transpose modulated by the singular values, namely stretched in some bizarre way and then rotated by you.",
                    "label": 0
                },
                {
                    "sent": "And you've done a lot of convolutions there that can mess a lot of things up, and so we're saying is the right.",
                    "label": 0
                },
                {
                    "sent": "Thing to filter in the second stage to run QR on is this.",
                    "label": 0
                },
                {
                    "sent": "And we'll see that that's true if you do the randomized algorithm and also show you what happens if you do a deterministic, you are on on just that thing.",
                    "label": 0
                },
                {
                    "sent": "Which in fact will do better than the deterministic you are on that thing.",
                    "label": 0
                },
                {
                    "sent": "And intuitively it's the same reason.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Pivot rules have problems when the columns of a are almost aligned and then you have your pointing off in some other direction.",
                    "label": 0
                },
                {
                    "sent": "So when things cancel in bad ways and what I'm telling you is that I'm biasing the choice of columns.",
                    "label": 0
                },
                {
                    "sent": "So it's things that stick out a bit.",
                    "label": 0
                },
                {
                    "sent": "And so you're not going to have on average these bad cases quite as much.",
                    "label": 0
                },
                {
                    "sent": "And so even doing deterministic you are on that thing will be better than that on that thing.",
                    "label": 0
                },
                {
                    "sent": "That's not a theorem, that's an empirical claim based on a bunch of applications and so interesting question would be formalized.",
                    "label": 0
                },
                {
                    "sent": "The conditions under which that's true.",
                    "label": 0
                },
                {
                    "sent": "So these are the things I said I was going to skip just.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here's sort of the theorem for the algorithm I just described.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "A minus the projection under the columns.",
                    "label": 0
                },
                {
                    "sent": "The spectral norm now.",
                    "label": 0
                },
                {
                    "sent": "The thing those root K routine before now is K to the 3/4 end of the one 4th and there's a log factor there.",
                    "label": 1
                },
                {
                    "sent": "And that tension between's increasing K and decreasing and seems to be a real effect, will see that in practice don't worry bout the log factor doing about the constants.",
                    "label": 0
                },
                {
                    "sent": "It's root K root N. This is a theorem and I'll get to practice in a minute, so the running time is the same as the traditional numerical linear algebra is no faster the spectral norm, however, is better as a function of N, and there's been a huge amount of work.",
                    "label": 1
                },
                {
                    "sent": "It wasn't even clear that there existed columns better than Rudin.",
                    "label": 0
                },
                {
                    "sent": "So I've given you an algorithm and time TM N ^2 M NK log K or whatever you want that runs in time T. That will actually give you the algorithms and more than giving you giving you give you the columns and more than giving you a column.",
                    "label": 0
                },
                {
                    "sent": "It's a randomized algorithm, will give you a whole bunch of columns if you run at different times.",
                    "label": 0
                },
                {
                    "sent": "So it wasn't even known that this existed and we found them.",
                    "label": 0
                },
                {
                    "sent": "This is the first sort of this is the first asymptotically improvement, since there was a bunch of interest in this problem in the context of the full rank revealing QR by Gwen Eisenstadt and others in the mid 90s for the Frobenius norm.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are we are multiplicative factor of K RT log K, so this is an algorithmic result that will be just marginally worse than the best previous existence existential results.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the Frobenius norm result is marginal.",
                    "label": 1
                },
                {
                    "sent": "Remember what exactly K columns here and were marginally worse than the best existential result.",
                    "label": 1
                },
                {
                    "sent": "And and it's running at algorithmic time an I should say, you know, if you're dealing with the Enron data, you can set K to be at that.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think it's a bad idea for data dependent reasons, but if you just interested in how long it takes, you can set K to be 1000 and then run these things in very realistic times.",
                    "label": 0
                },
                {
                    "sent": "So you shouldn't do that as a problem.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so how do these things perform?",
                    "label": 0
                },
                {
                    "sent": "I mean, I told you the original motivation was.",
                    "label": 0
                },
                {
                    "sent": "You know with large scale data problems.",
                    "label": 0
                },
                {
                    "sent": "That a lot of work in numerical linear algebra is been in sort of scientific computing applications, and so how might these things perform in practice?",
                    "label": 0
                },
                {
                    "sent": "So I mentioned the application of least squares problem.",
                    "label": 1
                },
                {
                    "sent": "So let me not talk about that.",
                    "label": 0
                },
                {
                    "sent": "Now let me talk about how the column subset selection problem will perform.",
                    "label": 1
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cool things datasets.",
                    "label": 0
                },
                {
                    "sent": "I want to talk about.",
                    "label": 0
                },
                {
                    "sent": "One will be the SNP, a certain SNP 500.",
                    "label": 0
                },
                {
                    "sent": "Data set on historical stock prices for about 500 stocks over over 1000 or so days.",
                    "label": 1
                },
                {
                    "sent": "This will be a very low rank matrix and so it's good, so there is a methodological test case much more realistic than some random matrix is some arbitrary matrix you choose, so it's good as mythological tests.",
                    "label": 1
                },
                {
                    "sent": "It doesn't classify so well in the in the low dimensional space and so for instance feature selection or something.",
                    "label": 1
                },
                {
                    "sent": "This may not be the best application for these sorts of techniques that will be good as methodological teskey tech TC is a corpus of term document data from the Open Directory project.",
                    "label": 0
                },
                {
                    "sent": "Hundreds of matrices.",
                    "label": 0
                },
                {
                    "sent": "Think of him as each one contains about 200 documents from two categories in the ODP hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So the Open directory project classifieds all the documents into some hierarchy, and I've chosen a bunch of documents.",
                    "label": 0
                },
                {
                    "sent": "From one level here, a bunch of documents from one level here and these two guys might be close.",
                    "label": 0
                },
                {
                    "sent": "It might be, you know, football and baseball.",
                    "label": 0
                },
                {
                    "sent": "2 sports things that are pretty close.",
                    "label": 0
                },
                {
                    "sent": "Or it might be, you know, football and.",
                    "label": 0
                },
                {
                    "sent": "You know used car dealerships, which hopefully are farther apart and easier to discriminate between.",
                    "label": 0
                },
                {
                    "sent": "And these sometimes classify well in the low dimensional space and sometimes don't.",
                    "label": 0
                },
                {
                    "sent": "The sparsity structure here is particularly bad, especially relative to the other two situations.",
                    "label": 0
                },
                {
                    "sent": "And so I want I'll show you examples where they do classify well since the motivation here is for feature selection where they don't, we can talk offline, and in fact in a lot of cases they don't.",
                    "label": 0
                },
                {
                    "sent": "DNA data either either.",
                    "label": 0
                },
                {
                    "sent": "Microarray data and I think the picture I have is actually going to migrate all I'm calling it sniff data.",
                    "label": 0
                },
                {
                    "sent": "Here we have a bunch of papers on DNA on.",
                    "label": 0
                },
                {
                    "sent": "Snip data and you get sort of similar results.",
                    "label": 0
                },
                {
                    "sent": "Nip date is actually much nicer to work because it's much, much larger and the noise properties aren't quite so bad or I don't want to say the better or worse, but they're different in ways that are more amenable to maybe machine learning and linear algebraic techniques.",
                    "label": 0
                },
                {
                    "sent": "So microarrays will measure basically the amount of gene product quantified in some way, and so there's obviously experimental issues here.",
                    "label": 0
                },
                {
                    "sent": "Single nucleotide so it will measure the human genome at the gene expression level.",
                    "label": 0
                },
                {
                    "sent": "Single nucleotide polymorphism data.",
                    "label": 0
                },
                {
                    "sent": "That's the most common type of polymorphic variation.",
                    "label": 0
                },
                {
                    "sent": "How do two people, not the human genome?",
                    "label": 0
                },
                {
                    "sent": "How do any two particular people differ?",
                    "label": 0
                },
                {
                    "sent": "And so this data from Hapmap and you should think about map is being on the order of 400 people, and depending on how you define the minor allele frequency between 1 million and five, or 10,000,000 snips across the across the genome, and this will typically classify pretty well in the low dimensional space, which isn't to say that.",
                    "label": 0
                },
                {
                    "sent": "The data is low rank, it just says if you want to very coarse approximation to the data that that you can get that in the dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is about 10 different QR algorithms, so this is your black box.",
                    "label": 0
                },
                {
                    "sent": "And if you arbitrarily choose one of 'em, you know you can ask how how your machine learning results going to depend if you happen to choose some other one.",
                    "label": 0
                },
                {
                    "sent": "So we looked at about a half dozen of these and that not all have publicly available implementations.",
                    "label": 0
                },
                {
                    "sent": "We looked at about a half dozen of these and what you saw in the financial data.",
                    "label": 0
                },
                {
                    "sent": "The following sort of typical, so the exa.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This here remember in the first step we're choosing more than K columns, and in the second step we will back to exactly K. So here I think the goal is to get 1010 columns or maybe was 20 and at the end of the day and the X axis refers to the number of columns kept in the first step and the Y axis is the quality measure.",
                    "label": 0
                },
                {
                    "sent": "One is the basal level set by SVD.",
                    "label": 0
                },
                {
                    "sent": "I will reference ourselves to that, so these are four different QR algorithms and the first thing you see is it the four different programs behave in four different ways.",
                    "label": 0
                },
                {
                    "sent": "That this.",
                    "label": 0
                },
                {
                    "sent": "This top thing there, which is up at about 3:00 is the residual error returned by that algorithm on a.",
                    "label": 0
                },
                {
                    "sent": "In all those cases, and I didn't plot it here just for space reasons.",
                    "label": 0
                },
                {
                    "sent": "In all those cases, if you do Q are not on a, But on this thing you get something intermediate between this and the best thing returned by the SVD.",
                    "label": 0
                },
                {
                    "sent": "So in all those cases, by doing QR.",
                    "label": 0
                },
                {
                    "sent": "Whether non uniform structures you'll do better because you're by yourself with things that are outlined and you tend to do better on worse implementations of QR.",
                    "label": 0
                },
                {
                    "sent": "If in addition you do the two phase randomized thing, the same will be true.",
                    "label": 0
                },
                {
                    "sent": "Oftentimes, as a function of the rank parameter K, they'll be a sweet spot, so we saw sweet spot in theory and you'll do better and better.",
                    "label": 0
                },
                {
                    "sent": "At some point you'll sort of Max out the space you're trying to span, and you'll do a little bit worse and worse.",
                    "label": 0
                },
                {
                    "sent": "So think of that as sort of a sweet spot in a bias variance sense.",
                    "label": 0
                },
                {
                    "sent": "There will be a sweet spot there, so that's a parameter to play with if you want, but typically in all those cases, unless you have very sophisticated implementations of QR, you'll do better than that, so both in the randomized case and the deterministic case.",
                    "label": 0
                },
                {
                    "sent": "So here is an example of that.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "PC data.",
                    "label": 0
                },
                {
                    "sent": "In which you can see that.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The cluster on low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "This is actually a very strong filtering, I mean so in terms of selection bias.",
                    "label": 0
                },
                {
                    "sent": "If you want a picture like this, we're actually making a very strong bias on the exact data that you want to work with here.",
                    "label": 0
                },
                {
                    "sent": "But for the purpose illustration, I'll do that.",
                    "label": 0
                },
                {
                    "sent": "And the reason it's strong biases is basically two clusters here and you can look at it by ion sort of validate, but that's a very very strong bias.",
                    "label": 0
                },
                {
                    "sent": "What we'll see is that.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the left hand side is the clustering.",
                    "label": 0
                },
                {
                    "sent": "In the top the space spanned by the top two eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "So this two pieces here and clusters sort of by K means or by whatever you like by I in the right is what you'll get if you do this down sampling procedure, which you see is the data is very sparse.",
                    "label": 0
                },
                {
                    "sent": "In particular just so much more axis aligned here that could be a good or bad thing depending on your application you get a little bit more denoising in certain cases.",
                    "label": 0
                },
                {
                    "sent": "Here you lose interpretability and things like that.",
                    "label": 0
                },
                {
                    "sent": "This case, you'll oftentimes do better by.",
                    "label": 0
                },
                {
                    "sent": "Various metrics, and the reason intuitively is because you're going to low dimensional space, but the low dimensional space return via SVD does a lot of damage if the data is very sparse, 'cause now you'll be dense, whereas the low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "We haven't optimized an objective to be sparse, but intuitively it's low dimensional.",
                    "label": 0
                },
                {
                    "sent": "It's maybe the best low dimensional space that has a certain sparsity structure, and the reason it has a certain sparsity structure is not because we put that in the algorithm, because in this particular case the original columns have the sparsity structure, and we've downsampled an actual columns.",
                    "label": 0
                },
                {
                    "sent": "Now what you should notice here?",
                    "label": 0
                },
                {
                    "sent": "This is, it will be a little bit more obvious on the next slide, but this will be the plot of.",
                    "label": 0
                },
                {
                    "sent": "We've ordered the 10th or whatever the number of terms we have here is.",
                    "label": 0
                },
                {
                    "sent": "We've ordered the number of terms, which is a couple 1000 or whatever arbitrarily and on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "Here we plot the statistical leverage score so you see is an incredible non uniformity.",
                    "label": 0
                },
                {
                    "sent": "And so uniform sampling wouldn't be expected to work.",
                    "label": 0
                },
                {
                    "sent": "There may be doing something like this would, but in addition, if you use.",
                    "label": 0
                },
                {
                    "sent": "Something like informativeness a mutual information based measure which is what I plotted down here.",
                    "label": 0
                },
                {
                    "sent": "What you see is an incredible non uniformity.",
                    "label": 0
                },
                {
                    "sent": "Also if you eyeball this the non uniformity not supply not surprisingly is on words that are more discriminative.",
                    "label": 0
                },
                {
                    "sent": "That's a claim about the algorithm but also the particular data.",
                    "label": 0
                },
                {
                    "sent": "But that's more ubiquitous than just this particular data set.",
                    "label": 0
                },
                {
                    "sent": "And so be interesting to theorem of the following form that says you know this isn't true in general, but conditioned on clustering?",
                    "label": 0
                },
                {
                    "sent": "Well, in the low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Then a an unsupervised measure like these leverage scores are basically unsupervised, 'cause I just used information in the data matrix.",
                    "label": 0
                },
                {
                    "sent": "They will correlate particularly well with a supervised metric like Informativeness.",
                    "label": 0
                },
                {
                    "sent": "Now that will be easy if you set certain residual spaces to be 0.",
                    "label": 0
                },
                {
                    "sent": "But doing it without doing that probably will be true, but but that would be actually interesting theorem to prove a model in that sense.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the Hapmap data, particularly on the left.",
                    "label": 0
                },
                {
                    "sent": "You see the same sort of selection affect, but which the left is the Chinese in Japanese.",
                    "label": 0
                },
                {
                    "sent": "The straw man on the right is uniform sampling.",
                    "label": 0
                },
                {
                    "sent": "You tend to do much worse, and I think the circles here are using a slightly different but also mutual information or phone based measure.",
                    "label": 0
                },
                {
                    "sent": "And here's sampling based on the leverage scores.",
                    "label": 0
                },
                {
                    "sent": "So this is 90 or so by 2 million 90s because the particular populations it's tricky to get when you're older code to run on that, but you've done.",
                    "label": 0
                },
                {
                    "sent": "If you compute computer basis and then you downsample on something that has 3000 rather than 3 million entries.",
                    "label": 0
                },
                {
                    "sent": "In exactly that way, then you get what you want.",
                    "label": 0
                },
                {
                    "sent": "And this informance is of relevance there be.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's sort of the state of the art technique in genetics, so here's one of these.",
                    "label": 0
                },
                {
                    "sent": "Well, they called raster plots I guess on the top is the color coding of of the microarrays.",
                    "label": 0
                },
                {
                    "sent": "What you see is that.",
                    "label": 0
                },
                {
                    "sent": "In in the same way as you had before, you have a very strong correlation between the leverage scores and the informativeness metric and totally get it for exactly the same reason.",
                    "label": 0
                },
                {
                    "sent": "The clustering in the in the low dimensional space defined by the SVD's versus the low dimensional space on the columns you chose.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It was actually pretty remarkable because in this case you know the theorems will suggest that you'll get.",
                    "label": 0
                },
                {
                    "sent": "You reproduce norms and residuals and that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "But you saw a very strong correlation here in terms of the actual non uniformity structure.",
                    "label": 0
                },
                {
                    "sent": "So which is a little bit of the flavor of some of the sparse.",
                    "label": 0
                },
                {
                    "sent": "Maybe the sparse regression models are there out there in terms of L1 based measures, but yet when you go from vectors to matrices, a lot of subtle sub stuff that's going on the subspace.",
                    "label": 0
                },
                {
                    "sent": "So naively asking for a low rank matrix isn't going to do so well, but we're going to model that would.",
                    "label": 0
                },
                {
                    "sent": "You explained these sorts of phenomenon, so with that, let me wrap up so.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, I think the take home messages.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know there is the time, is sort of ripe for thinking about numerical issues and looking inside the black box for a lot of these large scale machine learning problems.",
                    "label": 0
                },
                {
                    "sent": "Black boxes are good.",
                    "label": 0
                },
                {
                    "sent": "Things are sort of well defined and problem solved, but you know when they're not, you need to look inside the code and understand what the code does, which means you could understand all the details of the code, or even you couldn't understand.",
                    "label": 0
                },
                {
                    "sent": "Sort of intuitively what it does and bias yourself towards non uniformly structure work with better codes or worse codes and so hopefully empirical stuff I described gave you a little bit of a flavor of some of those things and the Internet connection is, you know that there's a variation connection between the worst case theory.",
                    "label": 0
                },
                {
                    "sent": "And a very traditional statistical concept having to do with statistical leverage such that we can get better bounds in worst case theory using this concept and that sort of explains why in a lot of these cases these sorts of techniques will do better in data analysis and machine learning applications.",
                    "label": 0
                },
                {
                    "sent": "So thanks for your attention.",
                    "label": 0
                }
            ]
        }
    }
}