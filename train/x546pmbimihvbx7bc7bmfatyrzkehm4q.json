{
    "id": "x546pmbimihvbx7bc7bmfatyrzkehm4q",
    "title": "A Transductive Framework of Distance Metric Learning by Spectral Dimensionality Reduction",
    "info": {
        "author": [
            "Fuxin Li, Institute of Automation, Chinese Academy of Sciences"
        ],
        "published": "June 23, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Preprocessing"
        ]
    },
    "url": "http://videolectures.net/icml07_li_atfd/",
    "segmentation": [
        [
            "The spectral dimensionality reduction.",
            "That finish it, and this this is John work with.",
            "OK, that started first.",
            "This is John with Joanne Joanne from jail from the Beijing University of Technology and Young and me from the Institute of Automation of Chinese Academy of Sciences."
        ],
        [
            "And basically this is a very simple simple thing, but we started with the metric learning do actually.",
            "I don't think I need to develop that on this too much given after a priori metric and metric learnings that tries to adapt it to the information coming from the training items.",
            "See if the metric on the left, maybe the red red items are more close to the blue items and.",
            "Since we know that the right items from the same class and blue attempts from another class, we can adapt the magic so that the red items are closer and blue items are closer to each other so."
        ],
        [
            "Basically an what's what's good about it, and since PAC man pattern recognition, machine learning methods, mental depend on the good choice of metric and from this kind methods from Ken and two as well and too many other measures, but good magic may not be available every time.",
            "So as a compromise many use use Lydian metrics which very often on the undesirable I don't need to sync.",
            "Say I see I think many, many of you already know that, so matching learning can be used to make things better by adapting this, adapting the private April Re metric to some training set."
        ],
        [
            "Or some other information and the another good property of matching learning is that when you learn SVM, you started from training data and get a class fair.",
            "So you get a class where an nothing else.",
            "And imagine learning you start with the magic you put training data and you end with the magic and you end with the metric.",
            "You can still get more training data to it and end with another magic so.",
            "It can be continued endlessly."
        ],
        [
            "Something like a basin bashing you from a probability and get probability metric.",
            "Learning from magic and get another metric.",
            "So today my work is to do with to do something with the Mahalanobis metric.",
            "As well as we can see says actually can see just now.",
            "Many many metric learning papers in this years just seek to learn holonomic magic either in the original space or in the feature space induced by the kernels, and this three of them Shingen klokan, Rheinberger at all, and there's certainly not an exhaustive list.",
            "There's many papers on them."
        ],
        [
            "Sir.",
            "However, we know that we can get him having all this magic by linear transforming the input data and take the Euclidean metric in the transform space.",
            "So basically Mahalanobis metric is to enlarge the impact of some directions and reduce the impact of some other directions.",
            "So we are looking for a plainly looking for good directions in the input space or in the feature space and projecting on them so.",
            "This this fact reminds others of another field of research that is dementia."
        ],
        [
            "That reduction which can be linear or nonlinear linear and have been have been researched for.",
            "Probably if if you will track up to the beginnings of PCA, probably 100 years Ann.",
            "And the dimensionality reduction measures try to find the low dimensional representation of the original data, which of which nonlinear runs and termed as manifold learning, has been quite popular in those years and this space role data set maybe many of you are very very familiar wrists.",
            "Is 1 manifold learning an in manifold learning?",
            "They enroll this wastrel data set and two.",
            "Rectangle in the in the plane.",
            "So the those dimensionality reduction methods are mainly unsupervised methods, so."
        ],
        [
            "So what we what our idea is dimensionality reduction, learn metrics.",
            "Since if you project the points to the low dimension of space.",
            "Do you inherently get a Euclidean magic in the low dimensional space you projected?",
            "So we think metric learning can be regarded as a supervised, semi supervised version of dimensionality reduction so.",
            "And a combined view of this politics can give something."
        ],
        [
            "Interesting, so we first give a magic learning formulation and then we should show that we can solve it with dimensionality reduction.",
            "This formulation, oh, there's a lot of sigmas is.",
            "Test is just looks like this is.",
            "It will be clearer.",
            "In the first term.",
            "The first term gives the information from the training data and.",
            "If the training that if the if we know that the edges should be larger, the idea is the distance of XI.",
            "An XJ DJ should be larger than DK.",
            "Then XJ and XK are neighbors of X XI, so we imagine learning we try to push XC nearer to zyan push XJ further to further from X XI so.",
            "That's the first time.",
            "So we we we try to minimize the DJ.",
            "The distance between ING Zion exit minus DK.",
            "The distance between INK and in the second term we try to preserve some preserve some information given from the from prime from April restructure."
        ],
        [
            "And a possible way to specify April, April right structure is to link the items as a graph and use the graph lab plush koala pleasure as a penalty.",
            "So if we do this kind of price structure, we can get some intuition like like that if there's a pool of blue items here and.",
            "We are we are going to try to use the first user.",
            "First part of the cost function to push the blue to push this blue item to the pool towards the pool and then.",
            "And then did unable atoms linked to this blue item will be pushed nearer to the to the proof?",
            "Ended the read those red items and they will prove far away from the pool.",
            "Will also link the corresponding and label items, so in this way the available the unlabeled items are moved an correspondently with the label."
        ],
        [
            "Items.",
            "Furthermore, we have a.",
            "We have a assumption to make it solvable by the national at reduction as to assume that that metric is exceeded.",
            "Which means that there exists an Euclidean space Euclidean space that we can put all input items there as points and we just take the Euclidean metric in that space.",
            "For example, we can put the.",
            "Many nations, as our training items and we put them into a space and this probably reflects some something like geopolitical relations of these nations.",
            "But anyway, it can be put into an Euclidean space so it satisfies the Euclidean assumption.",
            "This assumption is very, very old.",
            "Actually, date back to the 1930s, where in the classical multidimensional scaling.",
            "And but in magic learning it is first used, at least as we know it is.",
            "It was first used in junk 2003, but in that paper they didn't make the connection with spectral methods."
        ],
        [
            "So today, Euclidian implies that there exists an inner product in this space.",
            "Which which shows that which which completely decides the distance magic.",
            "So it suffers suffices to learn the inner product only in that space.",
            "For fineness finite sample this will mean that itself is to learn the gram matrix.",
            "So the metric learning problem in energy Euclidean assumption is the same as a kernel learning problem."
        ],
        [
            "This is our main man algorithms to solve this problem.",
            "To learn a kernel matrix G of order N + N, where N is the number of labeled examples, an M is the number of unlabeled examples.",
            "Yes, you can see that from from this from this task, if you.",
            "If I'm asked if I said oh CIS 20 then we just we just get some not only some nonlinear dimensionality reductions, such as the flag map or ISOMAP, or Eli as as as a paper by banjo at 2004 shows this.",
            "These are all variants of kernel PCA.",
            "So currently they can also be get from this minimize trace X transpose PX and what we do is actually very simple, it's just as OK, it's just as some some some.",
            "Just add some metrics representing the costs of from training items."
        ],
        [
            "So it is.",
            "It is very simple idea is just kernel please A plus semi separation and we get magic learning under the Euclidean assumption with the particularly choose chosen loss function it returns it as a negative value problem which is which can be a sparse one.",
            "If you set the penalty metrics right.",
            "For example you can use this sparse graph blood pressure, so the algorithm will have.",
            "Time complexity of N + M to the square of N + N. Which is very which is acceptable and can compute, computes data sets up to some 10,000 some.",
            "1050 thousands of items."
        ],
        [
            "Hey, Furthermore we can we can prove that the this this metric learning problem is equivalent with anarchy chess regularization problem.",
            "In the in the van dimensional case.",
            "In the multidimensional case, I believe something similar holds, but it will need some different representar O ramps.",
            "This represented this represent a theorem is different from the from any previous representative from many previous regularization problems in that it penalizes XI minus FXJ.",
            "Instead of penalizing something like FX 5 -- Y and do a log of Dua square, do exponential anything and the regularization problem also gives a natural auto center.",
            "Although sample extent extension to our algorithm so it can handle new data, not only transductive Lee."
        ],
        [
            "Penalizing the penalizing, the FX I minus FX chair has so many that the information invite actually moves onto the weights of the weights WIG of XI of the of the FX INF XJ.",
            "So we can set something something like WJ to some function of the Zi, ZI and the distance of X&XJ and the distance of YG and it will be possible to solve problems without provide in any metric space and not only not only binary classification.",
            "For example, we can solve multiclass classification if you set WJ to this thing.",
            "That's what we've done in our paper and.",
            "We can also solve something like ranking if this if we set W IGS dependent on the difference of rank on the difference of ranking and the difference of ranks between between XI and XJ and many other.",
            "Many other problems may might be safe, so in this kind of settings."
        ],
        [
            "We finally discuss the parameter Lambda of the regularization which controls the strength of our prior belief.",
            "Intuitively learn guy should have stronger prior beliefs that could not be easily changed by what he sees in the training set.",
            "Well, newbie or some, or some infants or something like that.",
            "We have weaker prior beliefs that I gave him some training set and he changes.",
            "However, currently it is it is a problem of our algorithm since we can't have a good idea, have a good idea to decide a good value of good value of Lambda.",
            "So it is it is decided by researching in the experiments and it is quite sensitive to the algorithm is quite sensitive to the parameter Lambda, so it is something some like something like."
        ],
        [
            "The problem, so let's see the experiments in front for the synthetic experiments we use the harder two moons data set.",
            "The original two moons data set is very simple for spectrum method things you just do a mark of random work on that and all set.",
            "So we push the two moons nearer to each other so that it will be harder to distinguish them.",
            "And the Laplacian Tiger map in this case will give some classification very bad, something like that, and our our algorithm will give a classification that is very near to the original problem.",
            "So that shows the superiority for our algorithm to the unsupervised dimensionality reduction algorithm."
        ],
        [
            "And the UCI data.",
            "We also did some experiments on the user data, which we compared with two algorithms GW PCN ktda in junk in junk 2006 in June 2006 and we show that we are compatible with them.",
            "So we learned a good metric."
        ],
        [
            "And we also did something in an honest and try to try to see if better April rain knowledge can result in better results.",
            "So we use the tangent distance and it seems to have improved distance that have improvements on the Euclidean distance.",
            "But our methods in this case have only marginal improvements overlap.",
            "Plaster again Maps, so maybe our loss function is not good enough so."
        ],
        [
            "We are now trying to oh OK, let me think that.",
            "Get conclusion first, this work is very preliminary, but I think that framework of distance metric learning in the same supervised case in kernels is very useful.",
            "An energy Euclidean assumption.",
            "The distance measured learning problem can be done by adding label information to spectral dimensionality reduction methods.",
            "This also gives a regularization problem that it's.",
            "Different from previous regularization problems and it's quite interesting."
        ],
        [
            "And we're currently where we currently expect parenting with different loss functions other than the current 1.",
            "Something like handles or exponential loss, but that would be requiring some definite programming or convex optimization, so the scaleability currently is not very good, it's just currently just can't handle something like 600 to 700 points, so we are still working to make it more scalable."
        ],
        [
            "OK, and the last last night is what will happen when you played in.",
            "This assumption is not satisfied so many can many things that you can you can do from here and and one scanner that I am particularly interested is that when the Euclidean assumption only satisfies locali.",
            "So then the datasets will.",
            "The data set will be locally homeomorphic to a subset on the Euclidean space, which means.",
            "It lies on a topological manifold, so that's so.",
            "This means if we said the Euclid Euclidian assumption to satisfy locally, we can do something like locali Euclidean techniques to manifold learning, not like currently locally linear methods.",
            "What's good locally?",
            "Locally linear methods may suffer from from the curse of dimensionality, since in high dimension it is, it is hard to.",
            "Specifier linear a small linear neighborhood that we can we can use to drop its dimension locali, but if we use locally Euclidean, we can use much larger neighborhoods, so it would be much better.",
            "So I think this will be a good direction in the field."
        ],
        [
            "And thank you.",
            "Any questions?",
            "I'm just wondering so you sure.",
            "I.",
            "A bag of powder.",
            "Question.",
            "I still didn't get it.",
            "So I I have a couple of questions, yes.",
            "Unified framework yes.",
            "Is many different methods, yes, but that also suggests that you might be able to give a unified generalization theory.",
            "Yes, these you know that would also cover all of these frameworks.",
            "Is that one of the avenues that you have been?",
            "Yes, if you yes.",
            "That will be actually.",
            "My generalization is not very different from the semi supervised generalization instinct.",
            "Bonnie 2005 that works on manifold regularization, so I don't think this is a very big problem.",
            "The second thing is I was wondering if you can elaborate on you know what you said about this extreme sensitivity to the generatio.",
            "I mean to the yeah, yeah, yeah.",
            "Right, it's I mean you know was it to an unusual degree or four?",
            "Or were you referring to?",
            "It's not very, but it's it is quite sensitive that you must set the Lambda to a certain range in the range is just OK, but outside the range it just deteriorates very badly.",
            "OK, well thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The spectral dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "That finish it, and this this is John work with.",
                    "label": 0
                },
                {
                    "sent": "OK, that started first.",
                    "label": 0
                },
                {
                    "sent": "This is John with Joanne Joanne from jail from the Beijing University of Technology and Young and me from the Institute of Automation of Chinese Academy of Sciences.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And basically this is a very simple simple thing, but we started with the metric learning do actually.",
                    "label": 0
                },
                {
                    "sent": "I don't think I need to develop that on this too much given after a priori metric and metric learnings that tries to adapt it to the information coming from the training items.",
                    "label": 1
                },
                {
                    "sent": "See if the metric on the left, maybe the red red items are more close to the blue items and.",
                    "label": 0
                },
                {
                    "sent": "Since we know that the right items from the same class and blue attempts from another class, we can adapt the magic so that the red items are closer and blue items are closer to each other so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically an what's what's good about it, and since PAC man pattern recognition, machine learning methods, mental depend on the good choice of metric and from this kind methods from Ken and two as well and too many other measures, but good magic may not be available every time.",
                    "label": 1
                },
                {
                    "sent": "So as a compromise many use use Lydian metrics which very often on the undesirable I don't need to sync.",
                    "label": 1
                },
                {
                    "sent": "Say I see I think many, many of you already know that, so matching learning can be used to make things better by adapting this, adapting the private April Re metric to some training set.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or some other information and the another good property of matching learning is that when you learn SVM, you started from training data and get a class fair.",
                    "label": 0
                },
                {
                    "sent": "So you get a class where an nothing else.",
                    "label": 0
                },
                {
                    "sent": "And imagine learning you start with the magic you put training data and you end with the magic and you end with the metric.",
                    "label": 0
                },
                {
                    "sent": "You can still get more training data to it and end with another magic so.",
                    "label": 1
                },
                {
                    "sent": "It can be continued endlessly.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something like a basin bashing you from a probability and get probability metric.",
                    "label": 0
                },
                {
                    "sent": "Learning from magic and get another metric.",
                    "label": 0
                },
                {
                    "sent": "So today my work is to do with to do something with the Mahalanobis metric.",
                    "label": 0
                },
                {
                    "sent": "As well as we can see says actually can see just now.",
                    "label": 0
                },
                {
                    "sent": "Many many metric learning papers in this years just seek to learn holonomic magic either in the original space or in the feature space induced by the kernels, and this three of them Shingen klokan, Rheinberger at all, and there's certainly not an exhaustive list.",
                    "label": 1
                },
                {
                    "sent": "There's many papers on them.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sir.",
                    "label": 0
                },
                {
                    "sent": "However, we know that we can get him having all this magic by linear transforming the input data and take the Euclidean metric in the transform space.",
                    "label": 0
                },
                {
                    "sent": "So basically Mahalanobis metric is to enlarge the impact of some directions and reduce the impact of some other directions.",
                    "label": 0
                },
                {
                    "sent": "So we are looking for a plainly looking for good directions in the input space or in the feature space and projecting on them so.",
                    "label": 1
                },
                {
                    "sent": "This this fact reminds others of another field of research that is dementia.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That reduction which can be linear or nonlinear linear and have been have been researched for.",
                    "label": 0
                },
                {
                    "sent": "Probably if if you will track up to the beginnings of PCA, probably 100 years Ann.",
                    "label": 0
                },
                {
                    "sent": "And the dimensionality reduction measures try to find the low dimensional representation of the original data, which of which nonlinear runs and termed as manifold learning, has been quite popular in those years and this space role data set maybe many of you are very very familiar wrists.",
                    "label": 0
                },
                {
                    "sent": "Is 1 manifold learning an in manifold learning?",
                    "label": 0
                },
                {
                    "sent": "They enroll this wastrel data set and two.",
                    "label": 0
                },
                {
                    "sent": "Rectangle in the in the plane.",
                    "label": 1
                },
                {
                    "sent": "So the those dimensionality reduction methods are mainly unsupervised methods, so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we what our idea is dimensionality reduction, learn metrics.",
                    "label": 0
                },
                {
                    "sent": "Since if you project the points to the low dimension of space.",
                    "label": 0
                },
                {
                    "sent": "Do you inherently get a Euclidean magic in the low dimensional space you projected?",
                    "label": 0
                },
                {
                    "sent": "So we think metric learning can be regarded as a supervised, semi supervised version of dimensionality reduction so.",
                    "label": 1
                },
                {
                    "sent": "And a combined view of this politics can give something.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interesting, so we first give a magic learning formulation and then we should show that we can solve it with dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "This formulation, oh, there's a lot of sigmas is.",
                    "label": 0
                },
                {
                    "sent": "Test is just looks like this is.",
                    "label": 0
                },
                {
                    "sent": "It will be clearer.",
                    "label": 0
                },
                {
                    "sent": "In the first term.",
                    "label": 0
                },
                {
                    "sent": "The first term gives the information from the training data and.",
                    "label": 0
                },
                {
                    "sent": "If the training that if the if we know that the edges should be larger, the idea is the distance of XI.",
                    "label": 0
                },
                {
                    "sent": "An XJ DJ should be larger than DK.",
                    "label": 1
                },
                {
                    "sent": "Then XJ and XK are neighbors of X XI, so we imagine learning we try to push XC nearer to zyan push XJ further to further from X XI so.",
                    "label": 0
                },
                {
                    "sent": "That's the first time.",
                    "label": 0
                },
                {
                    "sent": "So we we we try to minimize the DJ.",
                    "label": 0
                },
                {
                    "sent": "The distance between ING Zion exit minus DK.",
                    "label": 0
                },
                {
                    "sent": "The distance between INK and in the second term we try to preserve some preserve some information given from the from prime from April restructure.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And a possible way to specify April, April right structure is to link the items as a graph and use the graph lab plush koala pleasure as a penalty.",
                    "label": 1
                },
                {
                    "sent": "So if we do this kind of price structure, we can get some intuition like like that if there's a pool of blue items here and.",
                    "label": 0
                },
                {
                    "sent": "We are we are going to try to use the first user.",
                    "label": 0
                },
                {
                    "sent": "First part of the cost function to push the blue to push this blue item to the pool towards the pool and then.",
                    "label": 0
                },
                {
                    "sent": "And then did unable atoms linked to this blue item will be pushed nearer to the to the proof?",
                    "label": 0
                },
                {
                    "sent": "Ended the read those red items and they will prove far away from the pool.",
                    "label": 0
                },
                {
                    "sent": "Will also link the corresponding and label items, so in this way the available the unlabeled items are moved an correspondently with the label.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Items.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, we have a.",
                    "label": 0
                },
                {
                    "sent": "We have a assumption to make it solvable by the national at reduction as to assume that that metric is exceeded.",
                    "label": 0
                },
                {
                    "sent": "Which means that there exists an Euclidean space Euclidean space that we can put all input items there as points and we just take the Euclidean metric in that space.",
                    "label": 1
                },
                {
                    "sent": "For example, we can put the.",
                    "label": 0
                },
                {
                    "sent": "Many nations, as our training items and we put them into a space and this probably reflects some something like geopolitical relations of these nations.",
                    "label": 1
                },
                {
                    "sent": "But anyway, it can be put into an Euclidean space so it satisfies the Euclidean assumption.",
                    "label": 0
                },
                {
                    "sent": "This assumption is very, very old.",
                    "label": 0
                },
                {
                    "sent": "Actually, date back to the 1930s, where in the classical multidimensional scaling.",
                    "label": 0
                },
                {
                    "sent": "And but in magic learning it is first used, at least as we know it is.",
                    "label": 1
                },
                {
                    "sent": "It was first used in junk 2003, but in that paper they didn't make the connection with spectral methods.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So today, Euclidian implies that there exists an inner product in this space.",
                    "label": 1
                },
                {
                    "sent": "Which which shows that which which completely decides the distance magic.",
                    "label": 1
                },
                {
                    "sent": "So it suffers suffices to learn the inner product only in that space.",
                    "label": 0
                },
                {
                    "sent": "For fineness finite sample this will mean that itself is to learn the gram matrix.",
                    "label": 1
                },
                {
                    "sent": "So the metric learning problem in energy Euclidean assumption is the same as a kernel learning problem.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is our main man algorithms to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "To learn a kernel matrix G of order N + N, where N is the number of labeled examples, an M is the number of unlabeled examples.",
                    "label": 1
                },
                {
                    "sent": "Yes, you can see that from from this from this task, if you.",
                    "label": 0
                },
                {
                    "sent": "If I'm asked if I said oh CIS 20 then we just we just get some not only some nonlinear dimensionality reductions, such as the flag map or ISOMAP, or Eli as as as a paper by banjo at 2004 shows this.",
                    "label": 0
                },
                {
                    "sent": "These are all variants of kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "So currently they can also be get from this minimize trace X transpose PX and what we do is actually very simple, it's just as OK, it's just as some some some.",
                    "label": 0
                },
                {
                    "sent": "Just add some metrics representing the costs of from training items.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it is.",
                    "label": 0
                },
                {
                    "sent": "It is very simple idea is just kernel please A plus semi separation and we get magic learning under the Euclidean assumption with the particularly choose chosen loss function it returns it as a negative value problem which is which can be a sparse one.",
                    "label": 1
                },
                {
                    "sent": "If you set the penalty metrics right.",
                    "label": 0
                },
                {
                    "sent": "For example you can use this sparse graph blood pressure, so the algorithm will have.",
                    "label": 0
                },
                {
                    "sent": "Time complexity of N + M to the square of N + N. Which is very which is acceptable and can compute, computes data sets up to some 10,000 some.",
                    "label": 0
                },
                {
                    "sent": "1050 thousands of items.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hey, Furthermore we can we can prove that the this this metric learning problem is equivalent with anarchy chess regularization problem.",
                    "label": 0
                },
                {
                    "sent": "In the in the van dimensional case.",
                    "label": 1
                },
                {
                    "sent": "In the multidimensional case, I believe something similar holds, but it will need some different representar O ramps.",
                    "label": 0
                },
                {
                    "sent": "This represented this represent a theorem is different from the from any previous representative from many previous regularization problems in that it penalizes XI minus FXJ.",
                    "label": 1
                },
                {
                    "sent": "Instead of penalizing something like FX 5 -- Y and do a log of Dua square, do exponential anything and the regularization problem also gives a natural auto center.",
                    "label": 1
                },
                {
                    "sent": "Although sample extent extension to our algorithm so it can handle new data, not only transductive Lee.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Penalizing the penalizing, the FX I minus FX chair has so many that the information invite actually moves onto the weights of the weights WIG of XI of the of the FX INF XJ.",
                    "label": 1
                },
                {
                    "sent": "So we can set something something like WJ to some function of the Zi, ZI and the distance of X&XJ and the distance of YG and it will be possible to solve problems without provide in any metric space and not only not only binary classification.",
                    "label": 1
                },
                {
                    "sent": "For example, we can solve multiclass classification if you set WJ to this thing.",
                    "label": 0
                },
                {
                    "sent": "That's what we've done in our paper and.",
                    "label": 0
                },
                {
                    "sent": "We can also solve something like ranking if this if we set W IGS dependent on the difference of rank on the difference of ranking and the difference of ranks between between XI and XJ and many other.",
                    "label": 0
                },
                {
                    "sent": "Many other problems may might be safe, so in this kind of settings.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We finally discuss the parameter Lambda of the regularization which controls the strength of our prior belief.",
                    "label": 0
                },
                {
                    "sent": "Intuitively learn guy should have stronger prior beliefs that could not be easily changed by what he sees in the training set.",
                    "label": 0
                },
                {
                    "sent": "Well, newbie or some, or some infants or something like that.",
                    "label": 0
                },
                {
                    "sent": "We have weaker prior beliefs that I gave him some training set and he changes.",
                    "label": 0
                },
                {
                    "sent": "However, currently it is it is a problem of our algorithm since we can't have a good idea, have a good idea to decide a good value of good value of Lambda.",
                    "label": 0
                },
                {
                    "sent": "So it is it is decided by researching in the experiments and it is quite sensitive to the algorithm is quite sensitive to the parameter Lambda, so it is something some like something like.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem, so let's see the experiments in front for the synthetic experiments we use the harder two moons data set.",
                    "label": 0
                },
                {
                    "sent": "The original two moons data set is very simple for spectrum method things you just do a mark of random work on that and all set.",
                    "label": 0
                },
                {
                    "sent": "So we push the two moons nearer to each other so that it will be harder to distinguish them.",
                    "label": 0
                },
                {
                    "sent": "And the Laplacian Tiger map in this case will give some classification very bad, something like that, and our our algorithm will give a classification that is very near to the original problem.",
                    "label": 0
                },
                {
                    "sent": "So that shows the superiority for our algorithm to the unsupervised dimensionality reduction algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the UCI data.",
                    "label": 0
                },
                {
                    "sent": "We also did some experiments on the user data, which we compared with two algorithms GW PCN ktda in junk in junk 2006 in June 2006 and we show that we are compatible with them.",
                    "label": 0
                },
                {
                    "sent": "So we learned a good metric.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we also did something in an honest and try to try to see if better April rain knowledge can result in better results.",
                    "label": 1
                },
                {
                    "sent": "So we use the tangent distance and it seems to have improved distance that have improvements on the Euclidean distance.",
                    "label": 1
                },
                {
                    "sent": "But our methods in this case have only marginal improvements overlap.",
                    "label": 0
                },
                {
                    "sent": "Plaster again Maps, so maybe our loss function is not good enough so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are now trying to oh OK, let me think that.",
                    "label": 0
                },
                {
                    "sent": "Get conclusion first, this work is very preliminary, but I think that framework of distance metric learning in the same supervised case in kernels is very useful.",
                    "label": 0
                },
                {
                    "sent": "An energy Euclidean assumption.",
                    "label": 0
                },
                {
                    "sent": "The distance measured learning problem can be done by adding label information to spectral dimensionality reduction methods.",
                    "label": 1
                },
                {
                    "sent": "This also gives a regularization problem that it's.",
                    "label": 0
                },
                {
                    "sent": "Different from previous regularization problems and it's quite interesting.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we're currently where we currently expect parenting with different loss functions other than the current 1.",
                    "label": 0
                },
                {
                    "sent": "Something like handles or exponential loss, but that would be requiring some definite programming or convex optimization, so the scaleability currently is not very good, it's just currently just can't handle something like 600 to 700 points, so we are still working to make it more scalable.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and the last last night is what will happen when you played in.",
                    "label": 0
                },
                {
                    "sent": "This assumption is not satisfied so many can many things that you can you can do from here and and one scanner that I am particularly interested is that when the Euclidean assumption only satisfies locali.",
                    "label": 1
                },
                {
                    "sent": "So then the datasets will.",
                    "label": 0
                },
                {
                    "sent": "The data set will be locally homeomorphic to a subset on the Euclidean space, which means.",
                    "label": 1
                },
                {
                    "sent": "It lies on a topological manifold, so that's so.",
                    "label": 0
                },
                {
                    "sent": "This means if we said the Euclid Euclidian assumption to satisfy locally, we can do something like locali Euclidean techniques to manifold learning, not like currently locally linear methods.",
                    "label": 0
                },
                {
                    "sent": "What's good locally?",
                    "label": 0
                },
                {
                    "sent": "Locally linear methods may suffer from from the curse of dimensionality, since in high dimension it is, it is hard to.",
                    "label": 0
                },
                {
                    "sent": "Specifier linear a small linear neighborhood that we can we can use to drop its dimension locali, but if we use locally Euclidean, we can use much larger neighborhoods, so it would be much better.",
                    "label": 0
                },
                {
                    "sent": "So I think this will be a good direction in the field.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And thank you.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "I'm just wondering so you sure.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "A bag of powder.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "I still didn't get it.",
                    "label": 0
                },
                {
                    "sent": "So I I have a couple of questions, yes.",
                    "label": 0
                },
                {
                    "sent": "Unified framework yes.",
                    "label": 0
                },
                {
                    "sent": "Is many different methods, yes, but that also suggests that you might be able to give a unified generalization theory.",
                    "label": 0
                },
                {
                    "sent": "Yes, these you know that would also cover all of these frameworks.",
                    "label": 0
                },
                {
                    "sent": "Is that one of the avenues that you have been?",
                    "label": 0
                },
                {
                    "sent": "Yes, if you yes.",
                    "label": 0
                },
                {
                    "sent": "That will be actually.",
                    "label": 0
                },
                {
                    "sent": "My generalization is not very different from the semi supervised generalization instinct.",
                    "label": 0
                },
                {
                    "sent": "Bonnie 2005 that works on manifold regularization, so I don't think this is a very big problem.",
                    "label": 0
                },
                {
                    "sent": "The second thing is I was wondering if you can elaborate on you know what you said about this extreme sensitivity to the generatio.",
                    "label": 0
                },
                {
                    "sent": "I mean to the yeah, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, it's I mean you know was it to an unusual degree or four?",
                    "label": 0
                },
                {
                    "sent": "Or were you referring to?",
                    "label": 0
                },
                {
                    "sent": "It's not very, but it's it is quite sensitive that you must set the Lambda to a certain range in the range is just OK, but outside the range it just deteriorates very badly.",
                    "label": 0
                },
                {
                    "sent": "OK, well thank you.",
                    "label": 0
                }
            ]
        }
    }
}