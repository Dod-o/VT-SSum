{
    "id": "t4vy3c23ea4whasnmnwzohpivz5cocpx",
    "title": "The Multi-layer Perceptron",
    "info": {
        "author": [
            "Robert F Harrison, Department of Molecular Biology and Biotechnology, University of Sheffield"
        ],
        "published": "Feb. 5, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Computer Science->Data Modeling"
        ]
    },
    "url": "http://videolectures.net/epsrcws08_harison_tmp/",
    "segmentation": [
        [
            "Morning I'm I'm Rob Harrison.",
            "I'm a reader in the Department of Automatic Control Systems engineering.",
            "Um?",
            "Keeping with the spirit of what Tony mentioned about just trying to deal with the issues and what we're going to be looking at over the week.",
            "I'm going to continue that and just try and look at what?",
            "Some of the issues are as we move away from the linear models, particularly the linear in the parameters models which Tony talks about.",
            "Um?",
            "Let's see what additional issues there are there.",
            "What are the pros and what are the cons?",
            "OK, I am going to keep it finally enough for a mathematics.",
            "For engineers course, I'm keeping it fairly mathematics free, so.",
            "My belief is if you can't do it by pictures, it's probably not worth doing in the 1st place, so.",
            "I'm going to.",
            "I'm going to do most of it by pictures and just refer to a couple of equations before I get on to talking about that.",
            "Specifically, I do want to sort of try to motivate it a bit by looking at again the problem that we have with data modeling Tony, particularly homed in on errors.",
            "Which is very important.",
            "But actually my my view is.",
            "Where the real problem lies in data modeling is in the gaps.",
            "In between your data points bit Swede."
        ],
        [
            "No.",
            "So just to re establish and again show you the notation we assume we've got a model Y is equal to F of X.",
            "There's some real thing I know there's some philosophical discussion earlier about the truth.",
            "Um?",
            "And we so just to take that on board.",
            "Yeah, we're not assuming that Y is equal to X actually is the truth.",
            "If it turned out to be the truth, that would be wonderful.",
            "But we assume that there is some underlying model that is good enough to represent the relationship between the inputs X and some putative output Y.",
            "But all we know about why is that?",
            "Well, it is our measurement said which are the noise corrupted or error corrupted versions of the.",
            "Of the values of the function that we're trying to find.",
            "Again.",
            "There are no problems if we don't have multivariate data.",
            "OK, so multivariate, as in X, is a vector, typically of some quite high dimension.",
            "But in the case of X being one or two dimensional.",
            "We don't really care.",
            "OK, we don't need all these fantastic techniques, of course, are applicable, and they're very good for examples, but we don't really need them, so we're assuming that the data is fairly high dimensional, and increasingly those dimensions can be very large indeed when you're talking about searching the web when you're talking about dealing with.",
            "Micro arrays and so on and so forth.",
            "Those those dimensions can be quite sizable.",
            "OK, also, we're assuming the relationship we're looking for is nonlinear.",
            "OK, the the world has.",
            "Enough multivariate linear techniques to last it a lifetime.",
            "OK, so we're assuming the relationship F is not linear.",
            "Otherwise it's a relatively simple problem to solve.",
            "OK.",
            "So we've got measurement errors because nothing can be measured perfectly.",
            "Ever matter how hard you try, you know, ultimately, Heisenberg's uncertainty principle kicks in at some scale or other.",
            "And all we know about this relationship is a finite set an the number of the size of the sample of measurements.",
            "The X is.",
            "We assume we're measuring without error.",
            "If there are errors on the X is then.",
            "The problem becomes more dramatic still, so we'll just assume that we know what the X is are.",
            "And this leads the measured values, which you've probably got some noise on, probably about which we know little.",
            "So we have to make some assumptions.",
            "And what we're trying to do is to infer the behavior everywhere in the domain of X.",
            "From a few points.",
            "But we know about which are possibly corrupt anyway, so they're not very accurate values of the function that we're interested in, so it's a pretty unreasonable thing to be trying to do really, it's Anil.",
            "Posed problem OK so.",
            "What we're setting out to do is unreasonable to start with.",
            "OK, we have little or no prior knowledge about F. If we do, then we would like to build that in for certain, and again that comes back to where the sort of idea of physical laws or whatever.",
            "If you know that there's a square law.",
            "Or suspect very strongly that there's a square law relationship.",
            "Then use a quadratic basis function for your pre processing step of your data.",
            "Linear in the parameters model bang outcomes your answer and you're likely to be pretty good.",
            "OK, so if you've got information you need to build that in, but we'll make the assumption that there's little or no prior knowledge about F itself.",
            "Although.",
            "Our intuition, particularly for engineers, tells us that.",
            "If we have to choose between a function that varies wildly.",
            "Over some short distance and a function that is fairly smooth over that same distance, and maybe we'd probably choose the smooth one.",
            "If you're a physicist, you probably choose that as well.",
            "OK, if you had to choose one in preference to the other, so that idea of smoothness can be formally.",
            "Built in to the way we do modeling OK and the whole problem that that may have at Nick.",
            "Set up and discussed in great detail of structural risk minimization.",
            "Comes back ultimately to saying choose the smoothest model that.",
            "Explain your data well enough.",
            "OK, it's another expression of Einstein saying.",
            "Model should be as simple as possible, but no simpler.",
            "OK, smoothness.",
            "Waviness being the opposite of smoothness is our measure of simplicity or complexity.",
            "And again, just I took the opportunity when I saw that Tony was having trouble with his notation just to make a little adjustment.",
            "So we use hat to indicate the estimate that we are making from the finite set of data that we are using.",
            "So YF hat W hat for the estimated weights, etc."
        ],
        [
            "OK, so when we can see what we're looking at, we can.",
            "Have a bit of a guess at what the shape of the function is.",
            "OK, what do you care to guess that one sorry, gotta question."
        ],
        [
            "It is, I should mention it later."
        ],
        [
            "OK. Um?",
            "Anybody wanna guess what function generated that data?",
            "Sinusoid plus a constant.",
            "Yeah, quite right.",
            "There's no error there we've got.",
            "What is it?",
            "12346 points.",
            "OK so.",
            "And actually, if I were to sit down and choose."
        ],
        [
            "A sinusoidal basis function basically do Fourier analysis of those six points.",
            "I'd get that very accurately OK, but it could just as well have been generated by that rather more.",
            "Complex is a more complex function.",
            "Exactly, sometimes in some senses it's a more complex function.",
            "In other senses it's a less complex function 'cause it's just the nearest neighbor or look up table version of that data.",
            "Might be good enough for your engineering purpose.",
            "Or it might not be good enough."
        ],
        [
            "OK, but the point is is there are many, many possibilities.",
            "There's all kinds of wiggly wobbly lines that you could, you or curves that you could draw that went through all those data points, hence giving us 0 error in any sense of measuring the error, whether it's least squares or some other.",
            "Measure of error.",
            "OK, so.",
            "Basic problem.",
            "Another problem is.",
            "So what we don't know is what goes on in the intersample space there."
        ],
        [
            "Gap OK, here's another one.",
            "This data was generated by precisely the same sine wave, but we've got two points.",
            "So we really don't have much option.",
            "The best possible model we could come."
        ],
        [
            "With four that is.",
            "A straight line.",
            "You got no basis for doing anything else.",
            "OK, it's smooth, it's simple.",
            "But it's wrong.",
            "But that indicates of course that we have got enough data.",
            "If we have 100 data points.",
            "Up there 100 data points up there would be no further ahead, so simply having lots of samples doesn't help either unless they are well spread out over.",
            "The domain over X. Yeah OK. Um?",
            "No noise anywhere here is not relevant, noise just makes it a little bit more difficult, but not very much more diff."
        ],
        [
            "Cool.",
            "If you've got loads of data that is well spread out, there's 200 points on that one.",
            "Then you don't need a model.",
            "OK.",
            "The data and the model.",
            "Other same damn thing.",
            "So no problem there either.",
            "If you've got some noise on that, you could still eyeball it and get a very good."
        ],
        [
            "OK, so.",
            "The questions are.",
            "Have we got enough data dimensionality?",
            "OK, we can see what we've been looking at.",
            "It's a 1 dimensional problem that is one dimension to X OK. As the dimensionality of X goes up, then the possibility of seeing what F of X looks like.",
            "Falls very, very rapidly.",
            "OK in two dimensions it's quite difficult.",
            "On a screen to see what a 2 dimensional function looks like if you've just got a few points dotted around on it.",
            "OK, it's quite good in one dimension with that sine wave.",
            "It becomes much more difficult in three dimensions and higher.",
            "Then, without the assistance of LSD or something else, we have no chance whatsoever.",
            "May there may be.",
            "You know, savants that can see these things in their heads or whatever.",
            "I don't know, but I can't.",
            "I like doubt that many of you can 'cause you have been in the papers or something.",
            "And I would have heard of you.",
            "So we lose the ability to see, so we can't really get any good information about what sort of shape we're expecting.",
            "OK, high dimensionality.",
            "Have we got enough data?",
            "Well, the number of samples that we're going to require turns out to be exponential in D. The dimension of X. OK, so without going into any technicalities, if you've N samples is enough in one dimension.",
            "So for our sine wave 6 samples turned out to be enough.",
            "OK then in end dimensions, sorry indeed, dimensions were going to enter the power D. So in two dimensions you need 36 samples to get the same density of sampling for our sine wave.",
            "Original sine wave problem and in 10 dimensions then you're going to have six of the power 10, which is quite a large number.",
            "OK so.",
            "There's a problem there, OK, but the amount of data we need goes up in order to say that we are well sampled, goes up quite rapidly with the dimensionality of the problem.",
            "Another question is how do we know if the data are well spaced?",
            "Tony mentioned this OK, putting them on us on a fixed grid like like I did for that sine wave is 1.",
            "Possibility, a situation of being able to do experiments.",
            "OK, so somebody over here mentioned doing multiple experiments at single conditions.",
            "That is something to be aspired to.",
            "It's not a problem.",
            "We'd love to be able to do.",
            "Many, many experiments at every single value of X and to choose our values of X carefully and then we'd have superb information.",
            "Typically in data modeling problems, particularly engineering ones, we don't have the luxury of doing experiments at all, OK?",
            "What we do is get samples from something we're measuring anyway.",
            "OK, I did some work some years ago for what was then British Steel and they measured every minute.",
            "About 160 pressures and 160 temperatures on a blast furnace, and they did that for some years OK, and most of that data was completely, utterly useless, but there was a lot of data.",
            "Mostly it never ventured into the areas where things were interesting.",
            "So we need to sample as it were, where the action is, where the interesting things are taking place, we need to be well enough sample there.",
            "So if we can.",
            "Conduct experiments, that's great if we only have what is what we would call observational data data that is.",
            "Being collected for some other purpose or is in some other way out of our control.",
            "We just get the data and we have no control over.",
            "The points in the X space where we can collect that data.",
            "Then we will have to somehow select from the data the useful data.",
            "And in my view, we've never got enough data.",
            "OK, if you've got a high dimensional problem, you're always undersampled.",
            "Well, if you think in just a very rough term, if you wanted to estimate the mean of something, you probably wouldn't accept less than 10 samples as a reasonable sample size for estimating the mean of the Heights of people in this room.",
            "For instance, OK.",
            "So you know and intend dimensions.",
            "That's to estimate one parameter.",
            "So in 10 dimensions you're going to need 10 to the power 10.",
            "To estimate some parameters there, that's.",
            "You're undersampled, so you've got to do something."
        ],
        [
            "Fill in your gaps.",
            "OK, we gotta make some decisions.",
            "Luckily we are blessed with a whole raft of things, probably not using this in the most.",
            "Perfect.",
            "Sense of the word, but universal approximators.",
            "OK, the things that Tony was talking about, the linear in the parameters models using.",
            "Basis function expansions or series approximations.",
            "Things like polynomials, rational polynomials, radial basis functions, etc etc.",
            "Are all examples of universal approximators.",
            "You can get as close as you like to some reasonable function by making that expansion bigger and bigger and bigger.",
            "That's all those things like Stone Vierstra's theorem.",
            "Tell us in words.",
            "OK, you can get as close as you like by taking a high enough degree.",
            "Expansion, polynomial expansion, or high enough?",
            "A large enough number of basis functions of a particular type.",
            "OK, so we're blessed with these things.",
            "We've got them and all that means is.",
            "If we is that we could find exits capable of representing X doesn't tell us how to find X, well, it says it capable of representing sorry, not XF of X. OK, so.",
            "We've got a tool that will let us do the job.",
            "Another tool that let us do the job, which I'm going to get onto is the multilayer perceptrons, which is a form of neural network.",
            "OK, but there are many of these things.",
            "You pick your favorite as Tony put it OK.",
            "The advantage of these things.",
            "They can bend themselves into almost any shape you like in high dimensions.",
            "OK, so that's a great advantage.",
            "It can do the job.",
            "The disadvantage is that it can bend itself into almost any shape you like, OK?",
            "So.",
            "It's a problem as well as a blessing because all we've got is a finite sample of data with gaps in between.",
            "So it can get close to the universal approximation, get close to the data.",
            "The question is what does it do in between the points?"
        ],
        [
            "Especially when the gaps are large.",
            "So here's an example.",
            "This is just taking some of these radial basis functions and doing a least squares fit.",
            "As Tony was talking about for our sine wave example that was there earlier.",
            "This is what happens if you get things.",
            "Badly wrong by choosing a set of basis functions here, I've just chosen them OK.",
            "I mean again, the question of how you choose them optimally, etc.",
            "Big problem alright, so I've chosen.",
            "At every sample point to put a Gaussian function and put a very narrow one OK.",
            "Brilliant, I get 0 error.",
            "I fit my data points perfectly.",
            "OK, and the reconstructed function is this like green line here.",
            "It's complete rubbish.",
            "It tells us nothing about the sinusoidal nature of the function that we're looking at OK.",
            "So there we've got a set of components basis functions which are too rough.",
            "For a.",
            "Problem.",
            "Unfortunately.",
            "Pretend to be blind.",
            "We don't know.",
            "It's sinusoidal.",
            "We can't see it in high dimensions.",
            "So we have to figure out how we're going to tell."
        ],
        [
            "What's going on?",
            "On the other hand, we take it the other way around.",
            "I've taken some very wide radial basis functions here.",
            "Again, just cited at all the.",
            "Data points and we get something that's.",
            "Actually, quite sinusoidal, so it's doing alright, but it's the wrong sinusoid.",
            "OK, it's got roughly the right frequency, but it's got the wrong amplitude.",
            "OK, so these are two."
        ],
        [
            "Smooth.",
            "And then we've got our Goldilocks version of it.",
            "That's just right.",
            "It's like the porridge.",
            "OK, where we're getting something that is a perfect fit.",
            "As far as I'm concerned, it's not quite perfect, but I'm an engineer.",
            "I approximate.",
            "OK, we're getting a small error, which is all we really want.",
            "In the training data?",
            "OK, but we're getting very good into sample behavior and we've got a set of components which when all.",
            "Add it up in their appropriate linear combinations.",
            "Its weighted sum together give us.",
            "The right shape in between the samples.",
            "So that's what we're aiming for, and it doesn't matter whether it's a radial basis function or a polynomial or whatever, or."
        ],
        [
            "Or whatever, OK?",
            "So one thing we can do is to attempt to restrict the flexibility of these universal approximators.",
            "Somebody said, can we measure the smoothness?",
            "OK, and yes, we can look formally at things, and I suspect tomorrow in the support vector machines talk.",
            "There will be some discussion of measuring things like the the what's called the VC dimension of these approximators, which in some sense measure how complex well in some sense they do measure how complex these estimators are.",
            "So that's one way of going forward, and I'm not going to talk about that.",
            "Um?",
            "Estimating the VC Dimension is an important tool and it would be great if we could do it easily.",
            "And accurately for.",
            "Everything but it's actually quite a difficult thing to do.",
            "You get upper bounds on the VC dimension quite straightforwardly, but sometimes they are so upper bound that they don't tell you anything.",
            "But we can measure the flexibility or the roughness of a function in a variety of ways, and if we could restrict it so it doesn't go mad in between the samples, then maybe we've got a way forward.",
            "So what we're going to do is use our data to tell our estimator.",
            "Weighted sum of basis functions or other estimator how to behave.",
            "In between.",
            "The sample points and we do this through a method which is generally known as regularization.",
            "Statisticians tend to refer to it as penalization.",
            "OK, but they both mean the same thing.",
            "OK, what we do is place a penalty or a cost on the roughness.",
            "I've used quotation marks there because.",
            "There are a number of ways of.",
            "By defining what we mean by roughness.",
            "OK, so we place a penalty on that typically will just simply add it to the cost function that we're trying to work with in the 1st place.",
            "So as an example, if we're looking at the sum of squares error, so we're looking at doing a meet at least squares fit.",
            "Then the first term there is just the ordinary sum of squares error that we seek to minimize by choosing a set of weights by some method.",
            "We add to that row I just some.",
            "Non negative constant times Q where Q is a is something that stands in for how rough the function is.",
            "OK. And then instead of minimizing just the mean squared error or the sum of squares error, we minimize that entire thing there.",
            "And that gives us a bit of control.",
            "OK, don't worry about what Q is for the minute.",
            "Because row there if it's 0.",
            "That basically brings us back to our original problem and says OK, just minimize the sum of squares ever.",
            "Give me the basic least squares solution.",
            "If I make row very large, it says give me a very very smooth.",
            "Fit to that data and so basically you're making the smoothness of the problem that is.",
            "Trying to minimize the roughness.",
            "The major component of your optimization and so of course again being a good engineer.",
            "What I'm looking for is a tradeoff which tells me where I get an acceptable level of fit to the data with the smoothest.",
            "Curve.",
            "Or function that I can get away with?",
            "OK.",
            "So as a strategy, I take a very flexible structure, a lot of radial basis functions with quite narrow.",
            "Um?",
            "Wits, for instance.",
            "So nasty rough thing that is capable of doing all sorts of weird things.",
            "And then use row here to tune.",
            "How?",
            "How flexible I let the final estimate become?",
            "OK.",
            "So what we want is something that is very simple here and nice to deal with OK. A good measure of roughness or something that's very strongly associated to the roughness of the function, is its second derivative with respect to the.",
            "So the argument to to the X is OK. Um?",
            "Because that's the curvature of the function, so areas of high curvature are areas which are doing this kind of thing.",
            "So that's a nice thing to use, and you can use it formally.",
            "It's what ultimately leads to spline fits.",
            "OK, that's where it all comes from, the optimal choice.",
            "To minimize a sum of squares error and a.",
            "A measure of curvature penalized curvature.",
            "Leads us to spline fits, but I'm not going to talk about that.",
            "What happens then is you get rather a complex optimization or solution to the optimization problem.",
            "There are a number of other possibilities, but one choice which is very, very nice and leads to a very, very simple solution is this one.",
            "We let Q just equal to the sum of the squares of all the weights in our estimator.",
            "I've used a double.",
            "Index there because sometimes we have multiple layers of things I'll come on to that in a minute.",
            "But anyway, just think of that as a sum of the squares of all the weights in the.",
            "In the estimator we.",
            "Do that.",
            "It turns out in this linear in the parameters case.",
            "OK, but we just have a very simple modification to our.",
            "Solution OK, we have W hat now the estimated set of weight is equal to five.",
            "Transpose Phi.",
            "Plus, row times the identity matrix times 5.",
            "Transpose times Ed, so it's pretty much the same as we had before and when row is equal to 0, of course we get.",
            "The same answer that should say inverse.",
            "There I'm sorry.",
            "OK, sorry there's an inverse missing.",
            "OK.",
            "I promise this is going on on video.",
            "I promise that we will fix up any typos before we release the slides.",
            "I'm sorry, yeah, user potentially complex structure and then play with RO for instance in some systematic way, increasing it from zero etc etc.",
            "And you can come up with.",
            "And an optimal.",
            "Not necessarily optimal in the mathematical sense, but a good tradeoff between.",
            "Accuracy on the training sample.",
            "The sum of the squares and.",
            "The smoothness of the function and in many ways that's what we do.",
            "Unknowingly, in some cases.",
            "So what happens here?",
            "The data will constrain the shape of the function wherever it can.",
            "Because we've got the data, a number of points and the.",
            "This demand for smoothness will constrain the function from doing anything too silly.",
            "When it's when you're away from the data in the gaps between the dates.",
            "So the whole variety of regularization approaches again if we just simply take the kinds of models that Tony looked at for classification, like logistic regression or something, then we don't get these nice closed form solutions, but nonetheless we can still do the same things.",
            "We can still constrain the functional forms, so restricting the flexibility is a typical way of dealing with this problem of what takes place.",
            "In between the data points OK and it will crop up in a number of ways during the week."
        ],
        [
            "So here's my little.",
            "Multimedia animated thing I said proud of this PowerPoint drives me crazy at times.",
            "What we have here.",
            "Is there a way now of finding out what is happening in between the samples?",
            "OK, because it's all very well having a way of restricting the flexibility.",
            "So we need a basis on which to make that tradeoff OK, and so we need to know what is happening in between.",
            "And we don't have the true function.",
            "If we did, we wouldn't be bothering with any of this.",
            "We'd all packed up and go home so.",
            "What we do is some form of validation.",
            "What is illustrated here is the most basic form of validation.",
            "Which is called.",
            "Usually the holdout method.",
            "The holdout method.",
            "So here we go.",
            "Hold out up there.",
            "The little green circles that are flashing on and off there just indicating.",
            "The data that has been used for training.",
            "OK, so Tony mentioned splitting up your data into a number of components.",
            "Here we've done.",
            "Very basic thing.",
            "We've got one, so it's ten data points.",
            "I've taken 123455 of them.",
            "Untrained.",
            "A flexible structure with those five, so the five are the green flashing... and we can see because we've got a very flexible structure.",
            "We get very close to these data points.",
            "OK, so we're getting very small mean squared errors.",
            "In these areas here OK.",
            "But I've got another five samples which I've held out of my entire training set.",
            "OK, so I've now because of my best guess at what the true function is doing.",
            "In between my training samples.",
            "I can measure the distance.",
            "Between my estimated function, this turquoise function here and these held out data points and get a measure of my performance.",
            "I can for instance.",
            "Workout the mean squared error or the sum squared error, or I can workout some other measure of performance, sum, sum of absolute errors or something.",
            "It doesn't matter whatever I'm interested in.",
            "How is my performance indicator?",
            "Um?",
            "So I measure my performance on these, held out samples and that tells me something about how well I'm doing.",
            "OK.",
            "So in this particular case, what I've looked at is the.",
            "The root mean squared error.",
            "OK across.",
            "These samples here and in training I get .23.",
            "So I get a small error and.",
            "When I test it.",
            "With these new samples I get .38, so I'm doing rather worse than I thought I was doing, so it's illustrative there of.",
            "This this fact that typically it's not unsurprising that you get very small training set errors because what you're trying to do is minimize the training set error.",
            "Self fulfilling prophecy really.",
            "So the only thing you're interested in is the what we call the out of sample error.",
            "OK, that is the these new points these on previously unseen points, and they're doing rather poorly, OK?",
            "So.",
            "However, what we've got is a scheme which announced allows us to work blind.",
            "OK, because whilst we can still see things and I do that for obvious reasons, when I'm giving a presentation you could still do that now without.",
            "Having to see the pictures OK, it's just a purely computational thing that you can compute.",
            "So that's one way of going forward, and it's still widely used by lots of people in applications.",
            "OK, so we hold out a small 8 percentage P percent for testing.",
            "We have to make those choices OK.",
            "Very wasteful because you only get.",
            "1 -- P percent 100 -- P percent for training your sample, so you're not using all of the data you possibly can, so that might.",
            "Compromise the final quality of the final estimate.",
            "And.",
            "It's sample dependent.",
            "If I were to choose a different 5 to train and a different 5 to test, I'd get.",
            "A different answer.",
            "If I had 10 million data points and I chose half and half differently, it wouldn't make much difference, but we have to think that in the kinds of problems we likely to be looking at, we will not have.",
            "A lot of data will have a little data."
        ],
        [
            "In some sense.",
            "A better way forward.",
            "In terms of using making use of your data.",
            "Comes through cross validation.",
            "And the the most commonly discussed version of this is called the Leave one out cross validation strategy.",
            "Which says you train your estimator on every.",
            "Sample except for one which you leave out.",
            "As in the holdout case.",
            "And then you put it back in.",
            "You take out another one.",
            "And you train it all again.",
            "At each stage you test your.",
            "Trained estimator, you gather up all that information.",
            "OK, by doing it.",
            "End times.",
            "And you compute your performance measure.",
            "OK, so.",
            "It's a very simple idea.",
            "Of course you've got to train N estimators, which might.",
            "Incur a lot of work.",
            "In the specific case of sum of squares.",
            "With linear in the parameter models you can actually do the whole thing.",
            "Fire a matrix manipulation which is no worse than the.",
            "Um?",
            "Conventional pseudoinverse problem?",
            "OK, but that's a very special situation.",
            "So for instance, you can do this many times with different values of that regularization parameter rho and choose.",
            "Row which corresponds to the best.",
            "Value of your performance.",
            "Measure that you've done over your cross validation.",
            "OK, something that is typically more frequently used now is M fold or many fold cross validation where you do the same sort of thing except you divide your sample up into M. M is an integer.",
            "Obviously non overlapping sets.",
            "OK, you proceed as above.",
            "So you take out one of these sets, you train.",
            "Your estimator you test it using the set that you've removed.",
            "When you put that set back in, take out another set and do this over and over again.",
            "The advantage of these techniques is that all of the data gets used for training and for testing.",
            "OK.",
            "It's obviously a lot more work, so in M fold cross validation strategy you have to do M. Lots of training.",
            "And leave one out.",
            "You do N lots of training, OK?",
            "But what you get is a much closer estimate of the so called generalization error, or.",
            "More loosely, just.",
            "Performance how it's going to perform in the real world as opposed to how it performs on the data you've used to train the estimator.",
            "And we use it very often to to choose these hyperparameters.",
            "Here, my hyperparameter innocence is row, but you could use it for tuning the width of your radial basis functions, or for finding the degree of your polynomial expansion or some other parameter that exists within your problem.",
            "The number of basis functions etc."
        ],
        [
            "OK, so that's what we do.",
            "So here's a number.",
            "Bit of PowerPoint wizardry.",
            "OK, so just to put that into pictures.",
            "We assume we've got all of our training data.",
            "The ex is and the zeds.",
            "And the zeds can be multidimensional when we typically talk about 1 dimensional targets.",
            "But we could have multidimensional targets.",
            "We break them up in this case, this is a five fold cross validation strategy, so we break them up into five non overlapping sets labeled an obvious way.",
            "OK, so I'm going to look at the fourth stage of this process, so we've already been through.",
            "We've taken.",
            "X1Z1 trained our estimator and produce the outputs from that estimator.",
            "That's why one we've done that all the way up to the third stage.",
            "We take out X4.",
            "We use all this lot here to train with.",
            "OK we test on X4.",
            "We don't need said four, so a typo.",
            "It shouldn't be there.",
            "We test using X4 and we produce.",
            "Wife'll be.",
            "Output from this trained estimator using that block of the."
        ],
        [
            "Later.",
            "OK.",
            "So we now finished at the fourth stage we carry on."
        ],
        [
            "Do the same thing for the fifth stage.",
            "Now we have a complete.",
            "Set of outputs.",
            "Based on every.",
            "Member of the sample.",
            "OK. And.",
            "We can compare in some way which is convenient to us Y&Z.",
            "OK, and that will give us some estimate of our what's generally called the generalization error.",
            "But it can be any measure of performance you like."
        ],
        [
            "So that's what happens with."
        ],
        [
            "Cross validation.",
            "The result depends on the order you process the data, or shouldn't they?",
            "Better not.",
            "No, it doesn't.",
            "I mean there are this good practice.",
            "You come across bits of this in the lab this afternoon, but for instance, it's very important that there's no.",
            "Irrelevant ordering.",
            "In your training sample and if you put everything in.",
            "Let's say it was a classification problem and you put all the Class A ones in the first half of the sample and all the Class B ones in the second half of the sample.",
            "The whole thing will breakdown.",
            "So typically one assumes that these are independent samples down here and you would randomly order them to start with in order to break up any.",
            "Artificial ordering.",
            "Within the sample.",
            "That answer your.",
            "Question.",
            "OK, we can talk about in the lab this afternoon, 'cause you'll see."
        ],
        [
            "This election.",
            "OK, so.",
            "All that fits very nicely into the linear in the parameters type problem.",
            "Mainly because we have a very nice property there, which Tony's already mentioned, and that's this idea of a unique minimum.",
            "Popping up so there's only one right answer for any given structure.",
            "Any given choice of basis functions.",
            "You only get one answer.",
            "It's the best answer.",
            "But the question of how do we choose these basis functions which has already been asked?",
            "Hannah spin answered by the fact that it's hard.",
            "We don't know what the right answer is.",
            "OK, so one way we could do it is to.",
            "Try to adapt them.",
            "So linear models essentially are based on an idea of taking our data.",
            "RX is pre processing them in some way through the Phis.",
            "Yeah.",
            "And that's a fixed function.",
            "We treat it as a fixed operation.",
            "And then.",
            "We estimate the parameters.",
            "OK, typically the cost function is in some way by 9.",
            "That is, it has this nice property of a unique minimum, so that's why we like doing it.",
            "'cause it's a nice easy optimization problem and no surprises.",
            "However.",
            "As the dimension goes up.",
            "These problems can become combinatorial OK, particularly if we look at the polynomial problem.",
            "Then you have this.",
            "Massive explosion in the number of terms as the dimension goes up.",
            "So if you take a.",
            "An example often gave if you've got a 16 by 16 bitmap.",
            "OK, and you want to treat that in some picture processing problem with a cubic polynomial function, you get 3 million terms, or nearly 3 million terms.",
            "OK, most of which will be totally redundant.",
            "So.",
            "You want it with a very, very big problem, starting with a relatively modest picture processing problem.",
            "So there's a downside to doing these linear things.",
            "You typically wind up with lots and lots and lots of things that maybe you don't need.",
            "And we've got the arbitrary choices.",
            "Things like row, things like, the width, etc degrees we have to.",
            "Work with that, maybe in a cross validation strategy to to make these choices.",
            "So the question of what is the best pre processor?",
            "We think about it in that way."
        ],
        [
            "To choose.",
            "Is.",
            "Major.",
            "So this is what we've got as a kind of control engineer, really, so this is sort of way.",
            "I tend to tend to think of things, signal flow flow through block diagrams, or we have is the input data going into some block which does something fi of X?",
            "That goes into an adaptive layer which generates an output which is compared to the target, generates an error and the error is fed back into the adaptive layer there to find those WS those weights.",
            "So one way forward.",
            "Might be instead of trying to pick fixed preprocessing layer using some.",
            "Strategy.",
            "Let's go straight for the jugular.",
            "And try to adapt it.",
            "Directly in the same way as you're adapting the output layer or optimizing the output layer.",
            "What about adapting or optimizing this preprocessing layer?",
            "And that's what I'm going to talk about for the rest of."
        ],
        [
            "This.",
            "Talk OK, the multilayer perceptrons is the archetypal version of.",
            "This approach is not the only one, but it's the most widely looked at.",
            "The most widely talked about and those widely studied.",
            "Most widely misunderstood of all the neural networks.",
            "OK, it's only very very vaguely related to anything to do with neurons, so we don't have to talk about any of that.",
            "Um?",
            "OK, it was popularized in the mid 80s by Rumelhart, Hinton and Williams in a paper in nature and completely changed the face of.",
            "Data modeling introduced the modern phase of data modeling that we know about apart from the Bayesians who were doing stuff elsewhere you'll hear from them later in the week, but actually have been developed.",
            "So 10 years before by Paul Verbas in 1974 and produced a massive thesis on this topic.",
            "But in those days, nobody had computers that could even begin to do this kind of work, so it didn't capture the.",
            "Public imagination and in fact you chiho another great control engineer in 1964 published a very similar.",
            "Algorithm so it's been kicking around for a long time, but in those days they were pretty much a theoretical interest mid 80s.",
            "We started to get the compute power that you needed to play with these things.",
            "Basically, we're going to do is learn this preprocessing stage from the data instead of.",
            "Trying to fix it or make the choice just out of the blue OK. And what we have is a layered feedforward structure that is a signal direction or signal flow direction is strictly from the input through to the output with no loops or sideways connections or anything like that.",
            "The preprocessing part is done by a set of.",
            "You would think of them as basis functions.",
            "With a strictly sigmoidal shape and S shape, some people call them squashing functions.",
            "The output function can be task specific if you're trying to do.",
            "App.",
            "Curve fitting type problem.",
            "Then you typically have a linear output function.",
            "If you're trying to do a classification type problem, you probably have another one of these S shaped things like the logistic function, which is restricted between zero and one, and we're going to try and model probabilities.",
            "We don't think about it.",
            "So here we have a completely non linear model.",
            "This model will be not only nonlinear in its input output behavior, but it's also non linear in the parameters.",
            "So it's a nonlinear."
        ],
        [
            "No question about that.",
            "There's a picture of a two layer multilayer perceptrons.",
            "OK, the inputs arrive here.",
            "They're all fed into every one of these little processing units in between, and then the outputs from those processing units are all fed into the output and we could have multiple outputs if we want.",
            "It's no more difficult, it's just more.",
            "Junk on the screen, basically.",
            "So that's a two layer.",
            "MLP because there are two active layers that are doing some computation.",
            "That's just a buffer layer.",
            "It doesn't do anything, so we don't include it in the count.",
            "So the output is just some function Theta.",
            "Depending on the task of the weighted sum of the outputs from all of these.",
            "Plus some bias.",
            "Add the outputs from each of these.",
            "Vijay is some strictly sigmoidal function.",
            "That's my views to Sigma.",
            "Of the weighted input again, plus a bias.",
            "Or a constant?",
            "So we get this kind of nested.",
            "View of the whole thing.",
            "OK, this is very simple.",
            "To compute this you can write the code in a few lines.",
            "It's nothing at all."
        ],
        [
            "Hum.",
            "You can have more layers if you wish.",
            "Was just think about two sigmoidal unit.",
            "Therefore take some inputs like that does some weighting of them.",
            "Excuse me I'm passes them this the weighted sum through some function.",
            "With and give us an output there OK, and the function is of that shape and there are many of these.",
            "OK, I tend to use the logistic function.",
            "For no good reason, you can use hyperbolic tangent or whatever you like, so long as it's got that shape and it's smooth.",
            "OK, I'm going to sharp corners then it's good enough."
        ],
        [
            "OK. And essentially all that's happening in that middle layer is we're making linear combinations of things that are S shaped.",
            "And the weights on those.",
            "Shift them.",
            "Up and down the X axis if you like.",
            "And the biases.",
            "Play some up and down the Y axis.",
            "OK so.",
            "And the weights.",
            "Can scale them and.",
            "That's that, so you take linear combinations of these S shaped things.",
            "As an example we have.",
            "The primary primal S shaped thing there going in that direction and then awaited version of it, which is obviously a negative sign which goes like that.",
            "So it's been flipped around.",
            "OK, add the two together, you get the red line there, you get a hump.",
            "So in a sense.",
            "We're able to represent in one dimension here a radial basis function.",
            "It's not quite a Gaussian, but it's not far off against in radial basis function by taking two of these little guys.",
            "OK.",
            "So by adding together enough of these, or we hope by adding enough of these things together in appropriate linear combinations, we will get the shape of function."
        ],
        [
            "What we want is a 2 dimensional version.",
            "Which I couldn't color in nicely, which does the same sort of thing which takes here.",
            "We've got 2 dimensional sigmoids so you get.",
            "Please kind of.",
            "Structures which when added together with appropriate weightings.",
            "Result in a function that again looks like now a 2 dimensional radial basis function.",
            "There's nothing special about the radial basis function Ness of it, it's just a nice visual illustration."
        ],
        [
            "So.",
            "First thing is, can a multilayer perceptrons do the job?",
            "Is it a universal approximator?",
            "OK, and the answer is yes, it can basically.",
            "Cybenko, Russian mathematician in 1990 showed that.",
            "By making a linear combination of enough of these sigmoids, appropriate linear combination of enough of them.",
            "You could represent any continuous function as closely as you like, so it's a similar result to the stone Vestra theorem for polynomials.",
            "OK.",
            "Unfortunately, it's not a constructive proof, so it doesn't tell you what enough is OK, so we're into making choices once again, OK, but one of the key things is that a single hidden layer layer in the middle there, the preprocessing layer is called a hidden layer by neural networks folk.",
            "Becausw it doesn't directly connect to the outside world either.",
            "The outputs or the inputs buffered from both, so it's called hidden.",
            "Um?",
            "All you need is a single one of those layers.",
            "OK, it's not to say that you might not get a better result, perhaps with fewer of these processing units.",
            "If you have more hidden layers.",
            "OK, but then of course you've got more choices to make and.",
            "Unsurprisingly, the more choices you've got more difficult your problem becomes, so knowing that having a single hidden layer.",
            "Is good enough.",
            "Is a bit of a benefit.",
            "OK, so.",
            "Basically.",
            "The MLP is one of these types of things that can represent any reasonable function.",
            "The sorts of functions we might want to represent as engineers as closely as we like.",
            "I'm sure mathematicians will find functions that it can't represent.",
            "But we'd probably say, well, as engineers, we don't care.",
            "Will approximate at that point.",
            "OK, so.",
            "The question is then, if we've got this structure, what we've got to do is choose.",
            "The parameters in that hidden layer.",
            "In some optimal way, OK?",
            "We're already going to choose the output layer parameters in an optimal way.",
            "We've seen how to do that in the previous talk, so we've just got to do these as well.",
            "So you think therefore, that the problem of.",
            "Modeling.",
            "Anything is solved.",
            "Completely OK.",
            "But actually all of the problems.",
            "Tony talked about and I've talked about up until now.",
            "Still exist.",
            "OK, we've got a structure.",
            "It can do the job.",
            "If we can pick."
        ],
        [
            "These guys.",
            "OK, so.",
            "One of the questions that we might ask first of all, is what does it do?",
            "This is the only equation really that I'm going to show.",
            "I'm just going to talk about this in terms of minimizing the sum of squares error, but there's similar results for whole variety of tasks that we might be.",
            "Seeking to carry out.",
            "OK, it turns out it's not actually that difficult to prove that minimizing the sum of squares error is equivalent to finding the conditional mean of the target data, which is precisely what we want to do.",
            "This is this regression property that was mentioned earlier.",
            "So if we just look at the theoretical problem rather than our real problem, which is yes, we've got a bit of data and.",
            "This question, which I haven't discussed yet but will look at in a minute if we assume that we've found the set of weights that gives us the best possible answer.",
            "To this problem.",
            "And that we've got an infinite amount of data.",
            "Then we are in a position to rewrite the sum of squares error.",
            "Coded J Infinity.",
            "I put the Infinity there just to indicate.",
            "It's a theoretical.",
            "Result.",
            "We can rewrite it as a sum of two components OK.",
            "It's the average of the square of this thing in brackets here, plus the average of the square of this thing.",
            "In brackets here, OK the sum Asian here is because I've written this in the general form where you might have multiple outputs.",
            "OK. P is just the probability density function of X&Y is the expectation operator.",
            "So we say we got two components here.",
            "OK.",
            "The first one.",
            "Is the expectation or mean value of?",
            "The target data.",
            "At a particular point X, so for a particular value of the input.",
            "Minus our estimate of the function.",
            "Which turns out to be the multilayer perception itself, of course, which is self is a function of X and a function of.",
            "I've used the curly W to indicate all of the weights in the thing, not just a single vector away.",
            "OK, so we take that difference there and we square it and we find the average of it.",
            "Added to that, is this other term here, which is the difference between.",
            "The target data and the average of the target data at a particular value of X. OK.",
            "I actually should be subscripted with.",
            "I those X is how I look at it, OK?",
            "Um?",
            "Or should they?",
            "Think about that.",
            "Crucial thing is this one here doesn't depend on the weights of the multilayer perception at all.",
            "OK, this is just some residual error that cannot be.",
            "Cannot be changed by the introduction of this this neural network.",
            "OK, So what we can see is that if we want to make J as small as possible, we can't do anything about that by choosing W, so we can ignore that.",
            "The error as small as possible.",
            "We have to make the function itself equal to.",
            "What's called the conditional mean.",
            "The expected value of the Z given the X is OK.",
            "So under these very extreme theoretical conditions, we see that precisely.",
            "What we're trying to do is.",
            "The right thing.",
            "OK, in reality we don't have a finite amount of data, so.",
            "It will just simply be.",
            "An estimate of the right thing.",
            "Nonetheless, it's encouraging.",
            "If you do it, yes.",
            "What is indicating is that if you.",
            "Extreme case where you had an infinite amount of data, so covering all of X basically, then by making by driving J to ITS global minimum by an appropriate choice of W. You would get equality there and this term would disappear.",
            "OK. No no.",
            "No no, all it's saying is if you could do this, you would be getting the right thing.",
            "OK. And that's what we're trying to do with trying to minimize the mean squared error over a finite amount of data and all these other problems that come with that.",
            "All it's sort of suggesting is that we're heading in the right direction that we're trying to estimate the conditional mean, and it's actually it's a probability of.",
            "Prop.",
            "It's a property of the sum of squares error.",
            "It's nothing to do with the multilayer perception itself.",
            "Any estimator that you put in there.",
            "Would have the same properties as I say this widens completely to other forms of cost as well, so in the cases of classifications and stuff we get the same desirable effect.",
            "OK.",
            "So the best we can say is that if we make our mean squared error small.",
            "Then the function that we estimate can be interpreted as an approximation of.",
            "The conditional mean.",
            "Of the target data, which is what we would like it to be."
        ],
        [
            "So we're doing the right thing.",
            "I think is the message from that.",
            "OK, it's not a silly thing to be doing.",
            "The pros of the multilayer perception.",
            "Because obviously we're going to have some difficulties creeping in here.",
            "The pros are it's compactness.",
            "OK. That you're capable of getting.",
            "Very high.",
            "Accuracy models.",
            "With very small networks networks very few.",
            "Processing units in.",
            "In contrast, for instance to the polynomial situation where you have this combinatorial explosion.",
            "Um?",
            "This whole question of compact compactness or sparsity is you'll hear it referred to in.",
            "Other arenas come up during this week is very important.",
            "You shouldn't have more than you need in any of these things.",
            "OK again, it goes back to Einstein and Occam's Razor and all these other things that say your model should be as simple as possible, but no.",
            "Simply so.",
            "In some of the linear in the parameters, types of approaches that will crop up in the week.",
            "You then have to try to control their size in some other way, so the multilayer perceptron is a is something that can.",
            "Yield.",
            "A naturally compact or sparse model?",
            "It's also got a very simple training algorithm.",
            "In its simplest sense, OK, and that was very attractive back in 1984, where you know fairly modest problem.",
            "On a PC in those days would have taken, you know, took four 5 days to run OK. Something will take a minute or two nowadays on a decent PC would take some days and you had very little ram, so you needed a very simple.",
            "Thanks."
        ],
        [
            "Compactness, this is, this comes from a piece of work by Andrew Barron back in the 90s, which.",
            "I must confess I never followed the proof, I just couldn't follow it.",
            "It was too obscured for me, but this is the result, and nobody's ever argued with it seriously.",
            "Which demonstrates for the situation of using polynomial bases.",
            "For increasing dimensions of input OK versus using the multilayer perceptrons structure.",
            "And the message here is that.",
            "The multilayer perceptron structure is capable of reducing the sum of squares error.",
            "I've done it as on a normalized scale there.",
            "Um?",
            "In off reducing the.",
            "Total sum of squares error OK, as a function of the number of units in its hidden layer.",
            "OK, is not dependent on the dimensionality of the input.",
            "What we're seeing elsewhere is for the series solution with this, strictly with the polynomial is as the dimension of the input increases.",
            "From this situation here we got the light blue line.",
            "D is equal to 5D is equal to 10 and then D is equal to 50.",
            "Then what it's basically showing us is the extreme sensitivity.",
            "So if you want to reduce your.",
            "Make some squares error by a factor of 10.",
            "With a multi level set Tron.",
            "And if we read across down to here to 110th.",
            ".1 it says that you require about.",
            "A size of about 10 hidden units.",
            "This is relative OK, whereas if you've got a regardless of the input dimension, whereas if you've got a 5 dimensional 5 dimensional problem to achieve the same amount, you're going to need kind of 150 sort of hidden units to get the same reduction in the amount of.",
            "Some squares are not an easy thing to see.",
            "But certainly in practice, it does seem that you can get very compact models with these multilayer perceptrons, so that's a big plus point.",
            "Particularly, you wanted to do something in real time on our Mike."
        ],
        [
            "Processor or something like that?",
            "The backpropagation algorithm is essentially one line of code for each of the weights.",
            "OK, obviously the loop is very, very simple indeed.",
            "And that is the thing that makes the weight change over overtime and seeks to minimize the mean squared error.",
            "I'm only showing that to show you how sick."
        ],
        [
            "Cool, it looks OK, nobody uses it anymore.",
            "The cons, which are the things I want to concentrate on our the relationship between the parameters.",
            "Sorry."
        ],
        [
            "Sorry, we have to end up with two slides.",
            "I'm surprised as well.",
            "Is it then?",
            "Experimental.",
            "No, this is this is a lengthy mathematical proof, which as I said.",
            "More.",
            "More.",
            "Different combinations in the feature space.",
            "Well, yes, but this is not.",
            "It's not to do with representing in the feature space, it's to do with.",
            "How much bigger you need to make your network in order to achieve the same level of reduction in sum of squares error.",
            "I'm not 100% convinced by this.",
            "OK, I'm only using it to illustrate that.",
            "Typically you will get away with much smaller.",
            "Structures than you will if you use polynomial bases OK. Yeah, the series, sorry.",
            "I see how many series yeah?",
            "You might, it depends on how complex the function underlying function is.",
            "Of course it's more to do with how complex the relationship is than accurately representing the features in the.",
            "In the data, but yeah, I I find this quite hard to stomach exactly.",
            "But it conveys a message."
        ],
        [
            "That OK?",
            "Herbal yeah, so I call it a malign relationship because the parameters are now non linearly related to the output, hence the error.",
            "Hence any function of that error becomes rather more complicated than than ice bowl shaped function that we had for our linear in the parameters models.",
            "OK, the optimization becomes much more difficult.",
            "OK this is not to do with neural networks, it's to do with.",
            "On the near optimizations.",
            "In particular, there are many possible solutions.",
            "That are mathematically minima.",
            "Downward turning points.",
            "OK. Um?",
            "So yeah, so even with a linear output unit, the effect of the hidden weights in the output is nonlinear, so."
        ],
        [
            "We're in trouble, so here's another.",
            "This is this is my piester resistance bits of.",
            "PowerPoint.",
            "Here's a physical example in the nice linear in the parameters case OK, one dimension.",
            "We've got a nice quadratic error surface.",
            "OK, if we roll a ball down a nice quadratic error surface notwithstanding, the way the acceleration is enormously, it went at the end there.",
            "It rolls down, it goes back to the forwards a bit, it gives up its energy because of friction or whatever, and it comes to a halt at the only possible place it can come to a halt.",
            "OK.",
            "So what it does, it sets off in the negative gradient direction.",
            "Um and.",
            "In reality.",
            "Does a bit of this and then comes to a halt.",
            "OK, at the point where the gradient of J with respect to W is equal to."
        ],
        [
            "Error.",
            "We can mimic that.",
            "Everybody I hope, knows about the gradient descent algorithm, but I couldn't resist doing another one of these.",
            "OK, we make the rate of change of the weights.",
            "With respect to time proportional to the negative gradient direction, so we make it go downhill and we step downhill.",
            "OK.",
            "So we discretize the whole problem and instead of rolling up the other side of it and doing all this, it just keeps stepping downhill until such time as it's unable to move.",
            "Without going uphill and you stop the algorithm, OK?",
            "So.",
            "We get to our minimum point without any real difficulty, so even in the case when we can't do our closed form solution, we can still find these minima.",
            "Quite easily with some iterative algorithm given by something that looks like."
        ],
        [
            "Fat however, as soon as we have a nonlinear relationship between the weights and the output, then.",
            "Everything changes and we wind up with cost services that can look like this.",
            "It's very simple looking cost for service here.",
            "That should say local minimum there is cut and paste for you.",
            "That should say local minima.",
            "OK, here we have two downward facing turning points, but actually there could be any number of 'em there could be any number of them that all achieve the global minimum as well, which is a little bit tricky.",
            "So in this case.",
            "If we can only move downhill.",
            "We wind up at that point there, which is a local minimum, not a global minimum.",
            "We start there.",
            "We wind up where we'd like to be.",
            "If we start there, we wind up where we'd like to be.",
            "And if we start there, we wind up where we don't want to be.",
            "OK.",
            "The problem is, once again is being blind to all this, we have no idea where to start.",
            "OK.",
            "So this is a fundamental problem.",
            "Nonconvex optimization.",
            "There are books and books or department's devoted to it.",
            "There are people that devote their entire research lives to the problem.",
            "It's fundamentally difficult.",
            "It's like trying to find your way down off the Scottish Highlands in the mist without a map.",
            "Yeah, if you had the map, you'd know where the best answer was anyway, and we could all go home.",
            "You wouldn't have to do any kind of optimization.",
            "We don't have the map.",
            "So.",
            "All of these.",
            "You're all these multilayer perceptron things are.",
            "Suffer from a slight problem that is you don't know."
        ],
        [
            "You've got the best solution or not.",
            "Here's the same thing in two dimensions.",
            "I couldn't make an animation of that, but it's the same basic idea.",
            "It looks a bit more like a mountain range now where you start determines where you finish.",
            "OK. And."
        ],
        [
            "That's a big problem.",
            "Um?",
            "So we'll assume that we're looking at minimization.",
            "We assume that it's an analytically intractable problem, which it is for almost everything except for the nice.",
            "Linear in the parameters summer squares case.",
            "We step our parameters downhill.",
            "That's the best we can do.",
            "OK. Basically a new set of weights is equal to our own set of weights plus a step in the right direction.",
            "And all of the techniques for doing this kind of optimization are a matter of choosing a better step.",
            "Yeah, the backpropagation algorithm.",
            "Is one that just takes a step that says what's what's the steepest direction to go in right?",
            "I'll go that way one step OK?",
            "You can do better than that, so back prop.",
            "It's very lightweight from a computational point of view.",
            "But it's very, very slow to converge.",
            "Nowadays, much smarter algorithms.",
            "Scaled conjugate gradients, which will use this afternoon, levenberg marquardt's probably the one of the most sophisticated ones, so these are the things that people use nowadays for preference over the so called back prop.",
            "But at the end of the day, all these do pick a smarter step in the right direction.",
            "OK, they're still just trying to travel downhill based only on local."
        ],
        [
            "Information.",
            "OK, there's just a picture of in a 2 dimensional space of.",
            "An algorithm trying to converge down to its.",
            "Minimum point offer cost surface that looks like that.",
            "OK, so this is a very small."
        ],
        [
            "Multilayer perceptrons?",
            "OK, if we start it in a slightly different position, it rolls off down.",
            "That slope there.",
            "OK, so we got a minimum down there and we got a minimum.",
            "Off over there as well, where you start determines where you finish.",
            "Stunning."
        ],
        [
            "MATLAB Neural networks toolbox.",
            "And that's just a.",
            "An illustration of how much quicker.",
            "Different algorithms can be, so there's the back prop algorithm, which staggers slowly, taking many, many many many steps to get down to this minimum point here.",
            "OK. Conjugate gradients, on the other hand goes wrong, goes right off the.",
            "Screen here it comes back here.",
            "That's two steps 3, four and then anymore than that and you're already very very close to the minimum point.",
            "So it takes it's much quicker in the sense that it takes far fewer steps to arrive at a local."
        ],
        [
            "Solution to the problem.",
            "OK, the implications of this are you can get a perfect structure.",
            "It could be the structure that actually generated the data in the first place.",
            "And you still get the wrong answer because you didn't know where to start your optimization from.",
            "So we got this extreme sort of dependency on initial conditions.",
            "It's real, you will find this happening, will probably find it happens this afternoon on very simple examples.",
            "But any local solution might be good enough again.",
            "If your objective is to develop a system that solves the problem.",
            "Your boss says improve this process by 10% is going to give you any extra marks by improving it by 90%.",
            "Yeah, the objective is to improve it by 10%.",
            "Is good enough.",
            "So it might be good enough.",
            "You've gotta train it.",
            "You've gotta test it and see, OK?",
            "Well, that's fair enough.",
            "It's not good enough for mathematicians, maybe, but it's good enough for engineers who are trying to solve a practical problem.",
            "So again, we've got all this training and testing, cross validation, etc.",
            "But we have some slight.",
            "Additional problems with cross validation because every time you train an estimator, you now have that additional question of.",
            "If the performance isn't good enough.",
            "Is it because?",
            "The network itself, or the estimator itself is deficient in some way.",
            "Maybe it's not got enough hidden units in it.",
            "Or is it simply not giving a good answer?",
            "Because if I chosen a different set of initial weights?",
            "I would have come to a better.",
            "Solution.",
            "So.",
            "You can get.",
            "The wrong answer for the wrong reason this this weights initial weights business which which causes some difficulty.",
            "So it gives us additional dimension to our development process which you have to be aware of.",
            "There's not much you can do about it.",
            "Multiple starts, which adds to the workload.",
            "So you just try out lots and lots of different values and see if they all converge to roughly the same final.",
            "Set of values.",
            "That's one way of doing it.",
            "There are other possibilities where you train with lots of starts and then combine all those neural networks together.",
            "Again, you end up with a much bigger thing, of course.",
            "And you know this whole research area is devoted to looking at those sorts of things.",
            "But anyway, so this problem of multiple solutions is."
        ],
        [
            "Real it's there, it happens.",
            "It happens for these linear in the parameters type models as well.",
            "I've picked on radial basis function neural networks.",
            "But it's the same.",
            "Tony's already alluded to it really.",
            "You know, people often say all, I use it radial basis function network because it's got a unique solution to unique solution.",
            "Once you've chosen the number, the widths and the positions of your radial basis functions.",
            "And choosing those is a highly nonlinear.",
            "Problem.",
            "OK, so the problem never goes away.",
            "Yeah.",
            "You just move it around and pick your favorite OK. One way of doing this for these kind of things is to go for direct optimization, in which case you've got something which is pretty much like the multilayer perceptron.",
            "If you try to optimize the centers and the.",
            "And the widths directly.",
            "OK, so you don't gain anything that way.",
            "So most linear in the parameters models.",
            "Have nonlinear parameters which have got to be selected parameters which have an effect.",
            "Nonlinearly on the app.",
            "In a sense, you've got a kind of an outer loop of optimization to do.",
            "So it's all part of the no such thing as a free lunch theorem.",
            "Yeah, you just shift the problems around."
        ],
        [
            "Are they?",
            "Are they really a problem?",
            "Well, you know the pros seem to outweigh the cons.",
            "The compactness quite rapid speed of development and multilayer perceptrons.",
            "Um?",
            "We can often get good solutions very quickly with a little bit of experimentation.",
            "I'm.",
            "All of the previous things apply, but you know these things are out there.",
            "They're being used, they work OK. Um?",
            "If you haven't got enough data, if it's not in the right place.",
            "If it's not representative of where the network is going to be used ultimately, then you're going to have problems.",
            "OK, that's not to do with multilayer perceptrons, it's to do with any form of prediction or estimation.",
            "Um, lack of prior knowledge again is a problem.",
            "Going back to this idea of the truth.",
            "Of course, if we fit on multilayer perception, it's unlikely to be the truth because they don't exist in nature.",
            "They don't even exist as engineering structures, so they're unlikely to have generated your real data in the first place.",
            "However, they are able to get arbitrarily close to.",
            "The true function.",
            "Even though we don't."
        ],
        [
            "The tree function is.",
            "We're running over just on time.",
            "So the areas you can use them in is essentially to generalize what.",
            "Stand decisions would normally refer to as generalized linear models, so the linear model, the straightforward straight line relationship for hyperplane, our relationship in multi dimensions.",
            "Um?",
            "Yeah, linear regression as we call it or curve fitting.",
            "Typically you would have a linear output unit, sum of squares error criterion you minimize that.",
            "Can regularize so you can choose a very large network and then regularize it using cross validation to get a good answer.",
            "Logistic regression is another example of a generalized linear model where we want to classify things where we learn the posterior probability of class membership.",
            "Then we use a logistic.",
            "Output function.",
            "And an appropriate measure of error, not sum of squares error anymore, but something called the cross entropy or the deviance.",
            "You take those two things together.",
            "All makes perfect sense and you can have.",
            "Multiple classes, multinomial logistic regression, ordinal logistic regression, etc.",
            "Forget about that, Poisson regression is another form of generalized linear model.",
            "When you, when your data that you're trying to model accounts, things like radiation counts or whatever.",
            "Um?",
            "Failure time problems and stuff like OK, so anywhere where you might use a generalized linear model.",
            "Cox regression for survival and stuff like that you can.",
            "Front end with.",
            "And MLP hidden layer and have.",
            "A more exotic relationship between the inputs and the outputs.",
            "Let me just."
        ],
        [
            "Forget that.",
            "And give you.",
            "A quick example of local solutions.",
            "OK again is done with the MATLAB neural networks toolbox, but this illustrates this question of what happens when you get to one of the many possible minima in your cost function.",
            "OK, so here we've chosen the number of hidden neurons to be quite large.",
            "There are nine of them.",
            "OK, and this difficulty index is just the shape of the function that's trying to be modelled, which is clearly a sine wave.",
            "OK, so in both cases we're doing the same thing.",
            "We just have different random starts or use a random number generator to choose the initial weights for a multilayer perceptron with 9 hidden units in a single linear output unit.",
            "We allow things to minimize the mean squared error until it comes to.",
            "The bottom of a downward facing.",
            "Valley nine dimension.",
            "And.",
            "What do we see?",
            "Well, we get something that looks pretty much like a sinusoid mostly, but it's got this odd bit here.",
            "We start from a different.",
            "Place in the.",
            "Landscape, then we get something that's completely different, completely different shape.",
            "It's still doing its best to try and get to around some of the data neisen.",
            "Tightly, but the point is is that it's the same neural network, it's just that its weights have been optimized by beginning at a different.",
            "Position and we get a completely different function.",
            "So neither is particularly good, and actually if you do this a few times, you'll find that most of the times you get perfectly the sine wave, so it took some time.",
            "For me to find a couple of examples where it just shut off to the wrong solutions, but it does illustrate the point that you get different answers depending on.",
            "Where you start from?",
            "So that's the only additional problem with these nonlinear estimators.",
            "Um?",
            "And.",
            "I think we'll forget that it, but I've had enough.",
            "Yeah, so thanks for your attention.",
            "Anybody any questions you want to ask now?",
            "So this afternoon will go into the lab upstairs top floor, and I mean it's probably best if people kind of pair up or triple up or whatever, because you know, we all learn from each other.",
            "Probably better than we learn on our own.",
            "We don't have to.",
            "And will be Tony, myself and Andrew.",
            "So sort of assist and then we've got some exercise we go through which will illustrate some of these points that have been raised this morning, OK?",
            "Play."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Morning I'm I'm Rob Harrison.",
                    "label": 0
                },
                {
                    "sent": "I'm a reader in the Department of Automatic Control Systems engineering.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Keeping with the spirit of what Tony mentioned about just trying to deal with the issues and what we're going to be looking at over the week.",
                    "label": 0
                },
                {
                    "sent": "I'm going to continue that and just try and look at what?",
                    "label": 0
                },
                {
                    "sent": "Some of the issues are as we move away from the linear models, particularly the linear in the parameters models which Tony talks about.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Let's see what additional issues there are there.",
                    "label": 0
                },
                {
                    "sent": "What are the pros and what are the cons?",
                    "label": 0
                },
                {
                    "sent": "OK, I am going to keep it finally enough for a mathematics.",
                    "label": 0
                },
                {
                    "sent": "For engineers course, I'm keeping it fairly mathematics free, so.",
                    "label": 0
                },
                {
                    "sent": "My belief is if you can't do it by pictures, it's probably not worth doing in the 1st place, so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do most of it by pictures and just refer to a couple of equations before I get on to talking about that.",
                    "label": 0
                },
                {
                    "sent": "Specifically, I do want to sort of try to motivate it a bit by looking at again the problem that we have with data modeling Tony, particularly homed in on errors.",
                    "label": 0
                },
                {
                    "sent": "Which is very important.",
                    "label": 0
                },
                {
                    "sent": "But actually my my view is.",
                    "label": 0
                },
                {
                    "sent": "Where the real problem lies in data modeling is in the gaps.",
                    "label": 0
                },
                {
                    "sent": "In between your data points bit Swede.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So just to re establish and again show you the notation we assume we've got a model Y is equal to F of X.",
                    "label": 0
                },
                {
                    "sent": "There's some real thing I know there's some philosophical discussion earlier about the truth.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And we so just to take that on board.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we're not assuming that Y is equal to X actually is the truth.",
                    "label": 0
                },
                {
                    "sent": "If it turned out to be the truth, that would be wonderful.",
                    "label": 0
                },
                {
                    "sent": "But we assume that there is some underlying model that is good enough to represent the relationship between the inputs X and some putative output Y.",
                    "label": 0
                },
                {
                    "sent": "But all we know about why is that?",
                    "label": 0
                },
                {
                    "sent": "Well, it is our measurement said which are the noise corrupted or error corrupted versions of the.",
                    "label": 0
                },
                {
                    "sent": "Of the values of the function that we're trying to find.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "There are no problems if we don't have multivariate data.",
                    "label": 0
                },
                {
                    "sent": "OK, so multivariate, as in X, is a vector, typically of some quite high dimension.",
                    "label": 0
                },
                {
                    "sent": "But in the case of X being one or two dimensional.",
                    "label": 0
                },
                {
                    "sent": "We don't really care.",
                    "label": 0
                },
                {
                    "sent": "OK, we don't need all these fantastic techniques, of course, are applicable, and they're very good for examples, but we don't really need them, so we're assuming that the data is fairly high dimensional, and increasingly those dimensions can be very large indeed when you're talking about searching the web when you're talking about dealing with.",
                    "label": 0
                },
                {
                    "sent": "Micro arrays and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "Those those dimensions can be quite sizable.",
                    "label": 0
                },
                {
                    "sent": "OK, also, we're assuming the relationship we're looking for is nonlinear.",
                    "label": 0
                },
                {
                    "sent": "OK, the the world has.",
                    "label": 0
                },
                {
                    "sent": "Enough multivariate linear techniques to last it a lifetime.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're assuming the relationship F is not linear.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's a relatively simple problem to solve.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we've got measurement errors because nothing can be measured perfectly.",
                    "label": 1
                },
                {
                    "sent": "Ever matter how hard you try, you know, ultimately, Heisenberg's uncertainty principle kicks in at some scale or other.",
                    "label": 0
                },
                {
                    "sent": "And all we know about this relationship is a finite set an the number of the size of the sample of measurements.",
                    "label": 0
                },
                {
                    "sent": "The X is.",
                    "label": 0
                },
                {
                    "sent": "We assume we're measuring without error.",
                    "label": 0
                },
                {
                    "sent": "If there are errors on the X is then.",
                    "label": 0
                },
                {
                    "sent": "The problem becomes more dramatic still, so we'll just assume that we know what the X is are.",
                    "label": 0
                },
                {
                    "sent": "And this leads the measured values, which you've probably got some noise on, probably about which we know little.",
                    "label": 0
                },
                {
                    "sent": "So we have to make some assumptions.",
                    "label": 0
                },
                {
                    "sent": "And what we're trying to do is to infer the behavior everywhere in the domain of X.",
                    "label": 0
                },
                {
                    "sent": "From a few points.",
                    "label": 0
                },
                {
                    "sent": "But we know about which are possibly corrupt anyway, so they're not very accurate values of the function that we're interested in, so it's a pretty unreasonable thing to be trying to do really, it's Anil.",
                    "label": 0
                },
                {
                    "sent": "Posed problem OK so.",
                    "label": 0
                },
                {
                    "sent": "What we're setting out to do is unreasonable to start with.",
                    "label": 0
                },
                {
                    "sent": "OK, we have little or no prior knowledge about F. If we do, then we would like to build that in for certain, and again that comes back to where the sort of idea of physical laws or whatever.",
                    "label": 0
                },
                {
                    "sent": "If you know that there's a square law.",
                    "label": 0
                },
                {
                    "sent": "Or suspect very strongly that there's a square law relationship.",
                    "label": 0
                },
                {
                    "sent": "Then use a quadratic basis function for your pre processing step of your data.",
                    "label": 0
                },
                {
                    "sent": "Linear in the parameters model bang outcomes your answer and you're likely to be pretty good.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you've got information you need to build that in, but we'll make the assumption that there's little or no prior knowledge about F itself.",
                    "label": 1
                },
                {
                    "sent": "Although.",
                    "label": 0
                },
                {
                    "sent": "Our intuition, particularly for engineers, tells us that.",
                    "label": 0
                },
                {
                    "sent": "If we have to choose between a function that varies wildly.",
                    "label": 0
                },
                {
                    "sent": "Over some short distance and a function that is fairly smooth over that same distance, and maybe we'd probably choose the smooth one.",
                    "label": 0
                },
                {
                    "sent": "If you're a physicist, you probably choose that as well.",
                    "label": 0
                },
                {
                    "sent": "OK, if you had to choose one in preference to the other, so that idea of smoothness can be formally.",
                    "label": 0
                },
                {
                    "sent": "Built in to the way we do modeling OK and the whole problem that that may have at Nick.",
                    "label": 0
                },
                {
                    "sent": "Set up and discussed in great detail of structural risk minimization.",
                    "label": 0
                },
                {
                    "sent": "Comes back ultimately to saying choose the smoothest model that.",
                    "label": 0
                },
                {
                    "sent": "Explain your data well enough.",
                    "label": 0
                },
                {
                    "sent": "OK, it's another expression of Einstein saying.",
                    "label": 0
                },
                {
                    "sent": "Model should be as simple as possible, but no simpler.",
                    "label": 0
                },
                {
                    "sent": "OK, smoothness.",
                    "label": 0
                },
                {
                    "sent": "Waviness being the opposite of smoothness is our measure of simplicity or complexity.",
                    "label": 0
                },
                {
                    "sent": "And again, just I took the opportunity when I saw that Tony was having trouble with his notation just to make a little adjustment.",
                    "label": 0
                },
                {
                    "sent": "So we use hat to indicate the estimate that we are making from the finite set of data that we are using.",
                    "label": 0
                },
                {
                    "sent": "So YF hat W hat for the estimated weights, etc.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so when we can see what we're looking at, we can.",
                    "label": 0
                },
                {
                    "sent": "Have a bit of a guess at what the shape of the function is.",
                    "label": 0
                },
                {
                    "sent": "OK, what do you care to guess that one sorry, gotta question.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is, I should mention it later.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Um?",
                    "label": 0
                },
                {
                    "sent": "Anybody wanna guess what function generated that data?",
                    "label": 0
                },
                {
                    "sent": "Sinusoid plus a constant.",
                    "label": 0
                },
                {
                    "sent": "Yeah, quite right.",
                    "label": 0
                },
                {
                    "sent": "There's no error there we've got.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "12346 points.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "And actually, if I were to sit down and choose.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A sinusoidal basis function basically do Fourier analysis of those six points.",
                    "label": 0
                },
                {
                    "sent": "I'd get that very accurately OK, but it could just as well have been generated by that rather more.",
                    "label": 0
                },
                {
                    "sent": "Complex is a more complex function.",
                    "label": 0
                },
                {
                    "sent": "Exactly, sometimes in some senses it's a more complex function.",
                    "label": 0
                },
                {
                    "sent": "In other senses it's a less complex function 'cause it's just the nearest neighbor or look up table version of that data.",
                    "label": 0
                },
                {
                    "sent": "Might be good enough for your engineering purpose.",
                    "label": 0
                },
                {
                    "sent": "Or it might not be good enough.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, but the point is is there are many, many possibilities.",
                    "label": 0
                },
                {
                    "sent": "There's all kinds of wiggly wobbly lines that you could, you or curves that you could draw that went through all those data points, hence giving us 0 error in any sense of measuring the error, whether it's least squares or some other.",
                    "label": 0
                },
                {
                    "sent": "Measure of error.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Basic problem.",
                    "label": 0
                },
                {
                    "sent": "Another problem is.",
                    "label": 0
                },
                {
                    "sent": "So what we don't know is what goes on in the intersample space there.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gap OK, here's another one.",
                    "label": 0
                },
                {
                    "sent": "This data was generated by precisely the same sine wave, but we've got two points.",
                    "label": 0
                },
                {
                    "sent": "So we really don't have much option.",
                    "label": 0
                },
                {
                    "sent": "The best possible model we could come.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With four that is.",
                    "label": 0
                },
                {
                    "sent": "A straight line.",
                    "label": 0
                },
                {
                    "sent": "You got no basis for doing anything else.",
                    "label": 0
                },
                {
                    "sent": "OK, it's smooth, it's simple.",
                    "label": 0
                },
                {
                    "sent": "But it's wrong.",
                    "label": 0
                },
                {
                    "sent": "But that indicates of course that we have got enough data.",
                    "label": 0
                },
                {
                    "sent": "If we have 100 data points.",
                    "label": 0
                },
                {
                    "sent": "Up there 100 data points up there would be no further ahead, so simply having lots of samples doesn't help either unless they are well spread out over.",
                    "label": 0
                },
                {
                    "sent": "The domain over X. Yeah OK. Um?",
                    "label": 0
                },
                {
                    "sent": "No noise anywhere here is not relevant, noise just makes it a little bit more difficult, but not very much more diff.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cool.",
                    "label": 0
                },
                {
                    "sent": "If you've got loads of data that is well spread out, there's 200 points on that one.",
                    "label": 0
                },
                {
                    "sent": "Then you don't need a model.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The data and the model.",
                    "label": 0
                },
                {
                    "sent": "Other same damn thing.",
                    "label": 0
                },
                {
                    "sent": "So no problem there either.",
                    "label": 0
                },
                {
                    "sent": "If you've got some noise on that, you could still eyeball it and get a very good.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The questions are.",
                    "label": 0
                },
                {
                    "sent": "Have we got enough data dimensionality?",
                    "label": 0
                },
                {
                    "sent": "OK, we can see what we've been looking at.",
                    "label": 0
                },
                {
                    "sent": "It's a 1 dimensional problem that is one dimension to X OK. As the dimensionality of X goes up, then the possibility of seeing what F of X looks like.",
                    "label": 0
                },
                {
                    "sent": "Falls very, very rapidly.",
                    "label": 0
                },
                {
                    "sent": "OK in two dimensions it's quite difficult.",
                    "label": 1
                },
                {
                    "sent": "On a screen to see what a 2 dimensional function looks like if you've just got a few points dotted around on it.",
                    "label": 0
                },
                {
                    "sent": "OK, it's quite good in one dimension with that sine wave.",
                    "label": 0
                },
                {
                    "sent": "It becomes much more difficult in three dimensions and higher.",
                    "label": 0
                },
                {
                    "sent": "Then, without the assistance of LSD or something else, we have no chance whatsoever.",
                    "label": 0
                },
                {
                    "sent": "May there may be.",
                    "label": 0
                },
                {
                    "sent": "You know, savants that can see these things in their heads or whatever.",
                    "label": 0
                },
                {
                    "sent": "I don't know, but I can't.",
                    "label": 0
                },
                {
                    "sent": "I like doubt that many of you can 'cause you have been in the papers or something.",
                    "label": 0
                },
                {
                    "sent": "And I would have heard of you.",
                    "label": 0
                },
                {
                    "sent": "So we lose the ability to see, so we can't really get any good information about what sort of shape we're expecting.",
                    "label": 0
                },
                {
                    "sent": "OK, high dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Have we got enough data?",
                    "label": 0
                },
                {
                    "sent": "Well, the number of samples that we're going to require turns out to be exponential in D. The dimension of X. OK, so without going into any technicalities, if you've N samples is enough in one dimension.",
                    "label": 1
                },
                {
                    "sent": "So for our sine wave 6 samples turned out to be enough.",
                    "label": 0
                },
                {
                    "sent": "OK then in end dimensions, sorry indeed, dimensions were going to enter the power D. So in two dimensions you need 36 samples to get the same density of sampling for our sine wave.",
                    "label": 0
                },
                {
                    "sent": "Original sine wave problem and in 10 dimensions then you're going to have six of the power 10, which is quite a large number.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "There's a problem there, OK, but the amount of data we need goes up in order to say that we are well sampled, goes up quite rapidly with the dimensionality of the problem.",
                    "label": 0
                },
                {
                    "sent": "Another question is how do we know if the data are well spaced?",
                    "label": 1
                },
                {
                    "sent": "Tony mentioned this OK, putting them on us on a fixed grid like like I did for that sine wave is 1.",
                    "label": 0
                },
                {
                    "sent": "Possibility, a situation of being able to do experiments.",
                    "label": 0
                },
                {
                    "sent": "OK, so somebody over here mentioned doing multiple experiments at single conditions.",
                    "label": 0
                },
                {
                    "sent": "That is something to be aspired to.",
                    "label": 0
                },
                {
                    "sent": "It's not a problem.",
                    "label": 0
                },
                {
                    "sent": "We'd love to be able to do.",
                    "label": 0
                },
                {
                    "sent": "Many, many experiments at every single value of X and to choose our values of X carefully and then we'd have superb information.",
                    "label": 0
                },
                {
                    "sent": "Typically in data modeling problems, particularly engineering ones, we don't have the luxury of doing experiments at all, OK?",
                    "label": 0
                },
                {
                    "sent": "What we do is get samples from something we're measuring anyway.",
                    "label": 0
                },
                {
                    "sent": "OK, I did some work some years ago for what was then British Steel and they measured every minute.",
                    "label": 0
                },
                {
                    "sent": "About 160 pressures and 160 temperatures on a blast furnace, and they did that for some years OK, and most of that data was completely, utterly useless, but there was a lot of data.",
                    "label": 1
                },
                {
                    "sent": "Mostly it never ventured into the areas where things were interesting.",
                    "label": 0
                },
                {
                    "sent": "So we need to sample as it were, where the action is, where the interesting things are taking place, we need to be well enough sample there.",
                    "label": 0
                },
                {
                    "sent": "So if we can.",
                    "label": 0
                },
                {
                    "sent": "Conduct experiments, that's great if we only have what is what we would call observational data data that is.",
                    "label": 0
                },
                {
                    "sent": "Being collected for some other purpose or is in some other way out of our control.",
                    "label": 0
                },
                {
                    "sent": "We just get the data and we have no control over.",
                    "label": 0
                },
                {
                    "sent": "The points in the X space where we can collect that data.",
                    "label": 0
                },
                {
                    "sent": "Then we will have to somehow select from the data the useful data.",
                    "label": 0
                },
                {
                    "sent": "And in my view, we've never got enough data.",
                    "label": 0
                },
                {
                    "sent": "OK, if you've got a high dimensional problem, you're always undersampled.",
                    "label": 0
                },
                {
                    "sent": "Well, if you think in just a very rough term, if you wanted to estimate the mean of something, you probably wouldn't accept less than 10 samples as a reasonable sample size for estimating the mean of the Heights of people in this room.",
                    "label": 0
                },
                {
                    "sent": "For instance, OK.",
                    "label": 0
                },
                {
                    "sent": "So you know and intend dimensions.",
                    "label": 0
                },
                {
                    "sent": "That's to estimate one parameter.",
                    "label": 0
                },
                {
                    "sent": "So in 10 dimensions you're going to need 10 to the power 10.",
                    "label": 0
                },
                {
                    "sent": "To estimate some parameters there, that's.",
                    "label": 0
                },
                {
                    "sent": "You're undersampled, so you've got to do something.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fill in your gaps.",
                    "label": 0
                },
                {
                    "sent": "OK, we gotta make some decisions.",
                    "label": 0
                },
                {
                    "sent": "Luckily we are blessed with a whole raft of things, probably not using this in the most.",
                    "label": 0
                },
                {
                    "sent": "Perfect.",
                    "label": 0
                },
                {
                    "sent": "Sense of the word, but universal approximators.",
                    "label": 0
                },
                {
                    "sent": "OK, the things that Tony was talking about, the linear in the parameters models using.",
                    "label": 0
                },
                {
                    "sent": "Basis function expansions or series approximations.",
                    "label": 0
                },
                {
                    "sent": "Things like polynomials, rational polynomials, radial basis functions, etc etc.",
                    "label": 0
                },
                {
                    "sent": "Are all examples of universal approximators.",
                    "label": 0
                },
                {
                    "sent": "You can get as close as you like to some reasonable function by making that expansion bigger and bigger and bigger.",
                    "label": 0
                },
                {
                    "sent": "That's all those things like Stone Vierstra's theorem.",
                    "label": 0
                },
                {
                    "sent": "Tell us in words.",
                    "label": 0
                },
                {
                    "sent": "OK, you can get as close as you like by taking a high enough degree.",
                    "label": 0
                },
                {
                    "sent": "Expansion, polynomial expansion, or high enough?",
                    "label": 0
                },
                {
                    "sent": "A large enough number of basis functions of a particular type.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're blessed with these things.",
                    "label": 0
                },
                {
                    "sent": "We've got them and all that means is.",
                    "label": 0
                },
                {
                    "sent": "If we is that we could find exits capable of representing X doesn't tell us how to find X, well, it says it capable of representing sorry, not XF of X. OK, so.",
                    "label": 0
                },
                {
                    "sent": "We've got a tool that will let us do the job.",
                    "label": 0
                },
                {
                    "sent": "Another tool that let us do the job, which I'm going to get onto is the multilayer perceptrons, which is a form of neural network.",
                    "label": 0
                },
                {
                    "sent": "OK, but there are many of these things.",
                    "label": 0
                },
                {
                    "sent": "You pick your favorite as Tony put it OK.",
                    "label": 0
                },
                {
                    "sent": "The advantage of these things.",
                    "label": 0
                },
                {
                    "sent": "They can bend themselves into almost any shape you like in high dimensions.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's a great advantage.",
                    "label": 0
                },
                {
                    "sent": "It can do the job.",
                    "label": 1
                },
                {
                    "sent": "The disadvantage is that it can bend itself into almost any shape you like, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's a problem as well as a blessing because all we've got is a finite sample of data with gaps in between.",
                    "label": 0
                },
                {
                    "sent": "So it can get close to the universal approximation, get close to the data.",
                    "label": 0
                },
                {
                    "sent": "The question is what does it do in between the points?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Especially when the gaps are large.",
                    "label": 0
                },
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "This is just taking some of these radial basis functions and doing a least squares fit.",
                    "label": 0
                },
                {
                    "sent": "As Tony was talking about for our sine wave example that was there earlier.",
                    "label": 0
                },
                {
                    "sent": "This is what happens if you get things.",
                    "label": 0
                },
                {
                    "sent": "Badly wrong by choosing a set of basis functions here, I've just chosen them OK.",
                    "label": 0
                },
                {
                    "sent": "I mean again, the question of how you choose them optimally, etc.",
                    "label": 0
                },
                {
                    "sent": "Big problem alright, so I've chosen.",
                    "label": 0
                },
                {
                    "sent": "At every sample point to put a Gaussian function and put a very narrow one OK.",
                    "label": 0
                },
                {
                    "sent": "Brilliant, I get 0 error.",
                    "label": 0
                },
                {
                    "sent": "I fit my data points perfectly.",
                    "label": 0
                },
                {
                    "sent": "OK, and the reconstructed function is this like green line here.",
                    "label": 0
                },
                {
                    "sent": "It's complete rubbish.",
                    "label": 0
                },
                {
                    "sent": "It tells us nothing about the sinusoidal nature of the function that we're looking at OK.",
                    "label": 0
                },
                {
                    "sent": "So there we've got a set of components basis functions which are too rough.",
                    "label": 0
                },
                {
                    "sent": "For a.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately.",
                    "label": 0
                },
                {
                    "sent": "Pretend to be blind.",
                    "label": 0
                },
                {
                    "sent": "We don't know.",
                    "label": 0
                },
                {
                    "sent": "It's sinusoidal.",
                    "label": 0
                },
                {
                    "sent": "We can't see it in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "So we have to figure out how we're going to tell.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's going on?",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we take it the other way around.",
                    "label": 0
                },
                {
                    "sent": "I've taken some very wide radial basis functions here.",
                    "label": 0
                },
                {
                    "sent": "Again, just cited at all the.",
                    "label": 0
                },
                {
                    "sent": "Data points and we get something that's.",
                    "label": 0
                },
                {
                    "sent": "Actually, quite sinusoidal, so it's doing alright, but it's the wrong sinusoid.",
                    "label": 0
                },
                {
                    "sent": "OK, it's got roughly the right frequency, but it's got the wrong amplitude.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are two.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Smooth.",
                    "label": 0
                },
                {
                    "sent": "And then we've got our Goldilocks version of it.",
                    "label": 0
                },
                {
                    "sent": "That's just right.",
                    "label": 0
                },
                {
                    "sent": "It's like the porridge.",
                    "label": 0
                },
                {
                    "sent": "OK, where we're getting something that is a perfect fit.",
                    "label": 0
                },
                {
                    "sent": "As far as I'm concerned, it's not quite perfect, but I'm an engineer.",
                    "label": 0
                },
                {
                    "sent": "I approximate.",
                    "label": 0
                },
                {
                    "sent": "OK, we're getting a small error, which is all we really want.",
                    "label": 0
                },
                {
                    "sent": "In the training data?",
                    "label": 0
                },
                {
                    "sent": "OK, but we're getting very good into sample behavior and we've got a set of components which when all.",
                    "label": 0
                },
                {
                    "sent": "Add it up in their appropriate linear combinations.",
                    "label": 0
                },
                {
                    "sent": "Its weighted sum together give us.",
                    "label": 0
                },
                {
                    "sent": "The right shape in between the samples.",
                    "label": 0
                },
                {
                    "sent": "So that's what we're aiming for, and it doesn't matter whether it's a radial basis function or a polynomial or whatever, or.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or whatever, OK?",
                    "label": 0
                },
                {
                    "sent": "So one thing we can do is to attempt to restrict the flexibility of these universal approximators.",
                    "label": 0
                },
                {
                    "sent": "Somebody said, can we measure the smoothness?",
                    "label": 0
                },
                {
                    "sent": "OK, and yes, we can look formally at things, and I suspect tomorrow in the support vector machines talk.",
                    "label": 0
                },
                {
                    "sent": "There will be some discussion of measuring things like the the what's called the VC dimension of these approximators, which in some sense measure how complex well in some sense they do measure how complex these estimators are.",
                    "label": 0
                },
                {
                    "sent": "So that's one way of going forward, and I'm not going to talk about that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Estimating the VC Dimension is an important tool and it would be great if we could do it easily.",
                    "label": 0
                },
                {
                    "sent": "And accurately for.",
                    "label": 0
                },
                {
                    "sent": "Everything but it's actually quite a difficult thing to do.",
                    "label": 0
                },
                {
                    "sent": "You get upper bounds on the VC dimension quite straightforwardly, but sometimes they are so upper bound that they don't tell you anything.",
                    "label": 0
                },
                {
                    "sent": "But we can measure the flexibility or the roughness of a function in a variety of ways, and if we could restrict it so it doesn't go mad in between the samples, then maybe we've got a way forward.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is use our data to tell our estimator.",
                    "label": 1
                },
                {
                    "sent": "Weighted sum of basis functions or other estimator how to behave.",
                    "label": 1
                },
                {
                    "sent": "In between.",
                    "label": 0
                },
                {
                    "sent": "The sample points and we do this through a method which is generally known as regularization.",
                    "label": 0
                },
                {
                    "sent": "Statisticians tend to refer to it as penalization.",
                    "label": 0
                },
                {
                    "sent": "OK, but they both mean the same thing.",
                    "label": 0
                },
                {
                    "sent": "OK, what we do is place a penalty or a cost on the roughness.",
                    "label": 0
                },
                {
                    "sent": "I've used quotation marks there because.",
                    "label": 0
                },
                {
                    "sent": "There are a number of ways of.",
                    "label": 0
                },
                {
                    "sent": "By defining what we mean by roughness.",
                    "label": 0
                },
                {
                    "sent": "OK, so we place a penalty on that typically will just simply add it to the cost function that we're trying to work with in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "So as an example, if we're looking at the sum of squares error, so we're looking at doing a meet at least squares fit.",
                    "label": 0
                },
                {
                    "sent": "Then the first term there is just the ordinary sum of squares error that we seek to minimize by choosing a set of weights by some method.",
                    "label": 0
                },
                {
                    "sent": "We add to that row I just some.",
                    "label": 0
                },
                {
                    "sent": "Non negative constant times Q where Q is a is something that stands in for how rough the function is.",
                    "label": 0
                },
                {
                    "sent": "OK. And then instead of minimizing just the mean squared error or the sum of squares error, we minimize that entire thing there.",
                    "label": 0
                },
                {
                    "sent": "And that gives us a bit of control.",
                    "label": 0
                },
                {
                    "sent": "OK, don't worry about what Q is for the minute.",
                    "label": 0
                },
                {
                    "sent": "Because row there if it's 0.",
                    "label": 0
                },
                {
                    "sent": "That basically brings us back to our original problem and says OK, just minimize the sum of squares ever.",
                    "label": 0
                },
                {
                    "sent": "Give me the basic least squares solution.",
                    "label": 0
                },
                {
                    "sent": "If I make row very large, it says give me a very very smooth.",
                    "label": 0
                },
                {
                    "sent": "Fit to that data and so basically you're making the smoothness of the problem that is.",
                    "label": 0
                },
                {
                    "sent": "Trying to minimize the roughness.",
                    "label": 0
                },
                {
                    "sent": "The major component of your optimization and so of course again being a good engineer.",
                    "label": 0
                },
                {
                    "sent": "What I'm looking for is a tradeoff which tells me where I get an acceptable level of fit to the data with the smoothest.",
                    "label": 0
                },
                {
                    "sent": "Curve.",
                    "label": 0
                },
                {
                    "sent": "Or function that I can get away with?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So as a strategy, I take a very flexible structure, a lot of radial basis functions with quite narrow.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Wits, for instance.",
                    "label": 0
                },
                {
                    "sent": "So nasty rough thing that is capable of doing all sorts of weird things.",
                    "label": 0
                },
                {
                    "sent": "And then use row here to tune.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "How flexible I let the final estimate become?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what we want is something that is very simple here and nice to deal with OK. A good measure of roughness or something that's very strongly associated to the roughness of the function, is its second derivative with respect to the.",
                    "label": 0
                },
                {
                    "sent": "So the argument to to the X is OK. Um?",
                    "label": 0
                },
                {
                    "sent": "Because that's the curvature of the function, so areas of high curvature are areas which are doing this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "So that's a nice thing to use, and you can use it formally.",
                    "label": 1
                },
                {
                    "sent": "It's what ultimately leads to spline fits.",
                    "label": 0
                },
                {
                    "sent": "OK, that's where it all comes from, the optimal choice.",
                    "label": 0
                },
                {
                    "sent": "To minimize a sum of squares error and a.",
                    "label": 0
                },
                {
                    "sent": "A measure of curvature penalized curvature.",
                    "label": 0
                },
                {
                    "sent": "Leads us to spline fits, but I'm not going to talk about that.",
                    "label": 0
                },
                {
                    "sent": "What happens then is you get rather a complex optimization or solution to the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "There are a number of other possibilities, but one choice which is very, very nice and leads to a very, very simple solution is this one.",
                    "label": 0
                },
                {
                    "sent": "We let Q just equal to the sum of the squares of all the weights in our estimator.",
                    "label": 0
                },
                {
                    "sent": "I've used a double.",
                    "label": 0
                },
                {
                    "sent": "Index there because sometimes we have multiple layers of things I'll come on to that in a minute.",
                    "label": 0
                },
                {
                    "sent": "But anyway, just think of that as a sum of the squares of all the weights in the.",
                    "label": 0
                },
                {
                    "sent": "In the estimator we.",
                    "label": 0
                },
                {
                    "sent": "Do that.",
                    "label": 0
                },
                {
                    "sent": "It turns out in this linear in the parameters case.",
                    "label": 0
                },
                {
                    "sent": "OK, but we just have a very simple modification to our.",
                    "label": 0
                },
                {
                    "sent": "Solution OK, we have W hat now the estimated set of weight is equal to five.",
                    "label": 0
                },
                {
                    "sent": "Transpose Phi.",
                    "label": 0
                },
                {
                    "sent": "Plus, row times the identity matrix times 5.",
                    "label": 0
                },
                {
                    "sent": "Transpose times Ed, so it's pretty much the same as we had before and when row is equal to 0, of course we get.",
                    "label": 0
                },
                {
                    "sent": "The same answer that should say inverse.",
                    "label": 0
                },
                {
                    "sent": "There I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, sorry there's an inverse missing.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I promise this is going on on video.",
                    "label": 1
                },
                {
                    "sent": "I promise that we will fix up any typos before we release the slides.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, yeah, user potentially complex structure and then play with RO for instance in some systematic way, increasing it from zero etc etc.",
                    "label": 0
                },
                {
                    "sent": "And you can come up with.",
                    "label": 0
                },
                {
                    "sent": "And an optimal.",
                    "label": 0
                },
                {
                    "sent": "Not necessarily optimal in the mathematical sense, but a good tradeoff between.",
                    "label": 0
                },
                {
                    "sent": "Accuracy on the training sample.",
                    "label": 0
                },
                {
                    "sent": "The sum of the squares and.",
                    "label": 1
                },
                {
                    "sent": "The smoothness of the function and in many ways that's what we do.",
                    "label": 0
                },
                {
                    "sent": "Unknowingly, in some cases.",
                    "label": 0
                },
                {
                    "sent": "So what happens here?",
                    "label": 0
                },
                {
                    "sent": "The data will constrain the shape of the function wherever it can.",
                    "label": 0
                },
                {
                    "sent": "Because we've got the data, a number of points and the.",
                    "label": 0
                },
                {
                    "sent": "This demand for smoothness will constrain the function from doing anything too silly.",
                    "label": 0
                },
                {
                    "sent": "When it's when you're away from the data in the gaps between the dates.",
                    "label": 0
                },
                {
                    "sent": "So the whole variety of regularization approaches again if we just simply take the kinds of models that Tony looked at for classification, like logistic regression or something, then we don't get these nice closed form solutions, but nonetheless we can still do the same things.",
                    "label": 0
                },
                {
                    "sent": "We can still constrain the functional forms, so restricting the flexibility is a typical way of dealing with this problem of what takes place.",
                    "label": 0
                },
                {
                    "sent": "In between the data points OK and it will crop up in a number of ways during the week.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's my little.",
                    "label": 0
                },
                {
                    "sent": "Multimedia animated thing I said proud of this PowerPoint drives me crazy at times.",
                    "label": 0
                },
                {
                    "sent": "What we have here.",
                    "label": 0
                },
                {
                    "sent": "Is there a way now of finding out what is happening in between the samples?",
                    "label": 0
                },
                {
                    "sent": "OK, because it's all very well having a way of restricting the flexibility.",
                    "label": 0
                },
                {
                    "sent": "So we need a basis on which to make that tradeoff OK, and so we need to know what is happening in between.",
                    "label": 0
                },
                {
                    "sent": "And we don't have the true function.",
                    "label": 0
                },
                {
                    "sent": "If we did, we wouldn't be bothering with any of this.",
                    "label": 0
                },
                {
                    "sent": "We'd all packed up and go home so.",
                    "label": 0
                },
                {
                    "sent": "What we do is some form of validation.",
                    "label": 0
                },
                {
                    "sent": "What is illustrated here is the most basic form of validation.",
                    "label": 0
                },
                {
                    "sent": "Which is called.",
                    "label": 0
                },
                {
                    "sent": "Usually the holdout method.",
                    "label": 0
                },
                {
                    "sent": "The holdout method.",
                    "label": 0
                },
                {
                    "sent": "So here we go.",
                    "label": 0
                },
                {
                    "sent": "Hold out up there.",
                    "label": 0
                },
                {
                    "sent": "The little green circles that are flashing on and off there just indicating.",
                    "label": 0
                },
                {
                    "sent": "The data that has been used for training.",
                    "label": 0
                },
                {
                    "sent": "OK, so Tony mentioned splitting up your data into a number of components.",
                    "label": 0
                },
                {
                    "sent": "Here we've done.",
                    "label": 0
                },
                {
                    "sent": "Very basic thing.",
                    "label": 0
                },
                {
                    "sent": "We've got one, so it's ten data points.",
                    "label": 0
                },
                {
                    "sent": "I've taken 123455 of them.",
                    "label": 0
                },
                {
                    "sent": "Untrained.",
                    "label": 0
                },
                {
                    "sent": "A flexible structure with those five, so the five are the green flashing... and we can see because we've got a very flexible structure.",
                    "label": 0
                },
                {
                    "sent": "We get very close to these data points.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're getting very small mean squared errors.",
                    "label": 0
                },
                {
                    "sent": "In these areas here OK.",
                    "label": 0
                },
                {
                    "sent": "But I've got another five samples which I've held out of my entire training set.",
                    "label": 0
                },
                {
                    "sent": "OK, so I've now because of my best guess at what the true function is doing.",
                    "label": 0
                },
                {
                    "sent": "In between my training samples.",
                    "label": 0
                },
                {
                    "sent": "I can measure the distance.",
                    "label": 0
                },
                {
                    "sent": "Between my estimated function, this turquoise function here and these held out data points and get a measure of my performance.",
                    "label": 0
                },
                {
                    "sent": "I can for instance.",
                    "label": 0
                },
                {
                    "sent": "Workout the mean squared error or the sum squared error, or I can workout some other measure of performance, sum, sum of absolute errors or something.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter whatever I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "How is my performance indicator?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So I measure my performance on these, held out samples and that tells me something about how well I'm doing.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in this particular case, what I've looked at is the.",
                    "label": 0
                },
                {
                    "sent": "The root mean squared error.",
                    "label": 0
                },
                {
                    "sent": "OK across.",
                    "label": 0
                },
                {
                    "sent": "These samples here and in training I get .23.",
                    "label": 0
                },
                {
                    "sent": "So I get a small error and.",
                    "label": 0
                },
                {
                    "sent": "When I test it.",
                    "label": 0
                },
                {
                    "sent": "With these new samples I get .38, so I'm doing rather worse than I thought I was doing, so it's illustrative there of.",
                    "label": 0
                },
                {
                    "sent": "This this fact that typically it's not unsurprising that you get very small training set errors because what you're trying to do is minimize the training set error.",
                    "label": 0
                },
                {
                    "sent": "Self fulfilling prophecy really.",
                    "label": 0
                },
                {
                    "sent": "So the only thing you're interested in is the what we call the out of sample error.",
                    "label": 0
                },
                {
                    "sent": "OK, that is the these new points these on previously unseen points, and they're doing rather poorly, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "However, what we've got is a scheme which announced allows us to work blind.",
                    "label": 0
                },
                {
                    "sent": "OK, because whilst we can still see things and I do that for obvious reasons, when I'm giving a presentation you could still do that now without.",
                    "label": 0
                },
                {
                    "sent": "Having to see the pictures OK, it's just a purely computational thing that you can compute.",
                    "label": 0
                },
                {
                    "sent": "So that's one way of going forward, and it's still widely used by lots of people in applications.",
                    "label": 0
                },
                {
                    "sent": "OK, so we hold out a small 8 percentage P percent for testing.",
                    "label": 0
                },
                {
                    "sent": "We have to make those choices OK.",
                    "label": 0
                },
                {
                    "sent": "Very wasteful because you only get.",
                    "label": 0
                },
                {
                    "sent": "1 -- P percent 100 -- P percent for training your sample, so you're not using all of the data you possibly can, so that might.",
                    "label": 0
                },
                {
                    "sent": "Compromise the final quality of the final estimate.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It's sample dependent.",
                    "label": 0
                },
                {
                    "sent": "If I were to choose a different 5 to train and a different 5 to test, I'd get.",
                    "label": 0
                },
                {
                    "sent": "A different answer.",
                    "label": 0
                },
                {
                    "sent": "If I had 10 million data points and I chose half and half differently, it wouldn't make much difference, but we have to think that in the kinds of problems we likely to be looking at, we will not have.",
                    "label": 0
                },
                {
                    "sent": "A lot of data will have a little data.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In some sense.",
                    "label": 0
                },
                {
                    "sent": "A better way forward.",
                    "label": 0
                },
                {
                    "sent": "In terms of using making use of your data.",
                    "label": 0
                },
                {
                    "sent": "Comes through cross validation.",
                    "label": 0
                },
                {
                    "sent": "And the the most commonly discussed version of this is called the Leave one out cross validation strategy.",
                    "label": 0
                },
                {
                    "sent": "Which says you train your estimator on every.",
                    "label": 0
                },
                {
                    "sent": "Sample except for one which you leave out.",
                    "label": 0
                },
                {
                    "sent": "As in the holdout case.",
                    "label": 0
                },
                {
                    "sent": "And then you put it back in.",
                    "label": 0
                },
                {
                    "sent": "You take out another one.",
                    "label": 0
                },
                {
                    "sent": "And you train it all again.",
                    "label": 0
                },
                {
                    "sent": "At each stage you test your.",
                    "label": 0
                },
                {
                    "sent": "Trained estimator, you gather up all that information.",
                    "label": 0
                },
                {
                    "sent": "OK, by doing it.",
                    "label": 0
                },
                {
                    "sent": "End times.",
                    "label": 0
                },
                {
                    "sent": "And you compute your performance measure.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple idea.",
                    "label": 0
                },
                {
                    "sent": "Of course you've got to train N estimators, which might.",
                    "label": 0
                },
                {
                    "sent": "Incur a lot of work.",
                    "label": 0
                },
                {
                    "sent": "In the specific case of sum of squares.",
                    "label": 0
                },
                {
                    "sent": "With linear in the parameter models you can actually do the whole thing.",
                    "label": 0
                },
                {
                    "sent": "Fire a matrix manipulation which is no worse than the.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Conventional pseudoinverse problem?",
                    "label": 0
                },
                {
                    "sent": "OK, but that's a very special situation.",
                    "label": 0
                },
                {
                    "sent": "So for instance, you can do this many times with different values of that regularization parameter rho and choose.",
                    "label": 0
                },
                {
                    "sent": "Row which corresponds to the best.",
                    "label": 0
                },
                {
                    "sent": "Value of your performance.",
                    "label": 0
                },
                {
                    "sent": "Measure that you've done over your cross validation.",
                    "label": 0
                },
                {
                    "sent": "OK, something that is typically more frequently used now is M fold or many fold cross validation where you do the same sort of thing except you divide your sample up into M. M is an integer.",
                    "label": 0
                },
                {
                    "sent": "Obviously non overlapping sets.",
                    "label": 0
                },
                {
                    "sent": "OK, you proceed as above.",
                    "label": 1
                },
                {
                    "sent": "So you take out one of these sets, you train.",
                    "label": 0
                },
                {
                    "sent": "Your estimator you test it using the set that you've removed.",
                    "label": 0
                },
                {
                    "sent": "When you put that set back in, take out another set and do this over and over again.",
                    "label": 0
                },
                {
                    "sent": "The advantage of these techniques is that all of the data gets used for training and for testing.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It's obviously a lot more work, so in M fold cross validation strategy you have to do M. Lots of training.",
                    "label": 0
                },
                {
                    "sent": "And leave one out.",
                    "label": 0
                },
                {
                    "sent": "You do N lots of training, OK?",
                    "label": 0
                },
                {
                    "sent": "But what you get is a much closer estimate of the so called generalization error, or.",
                    "label": 1
                },
                {
                    "sent": "More loosely, just.",
                    "label": 0
                },
                {
                    "sent": "Performance how it's going to perform in the real world as opposed to how it performs on the data you've used to train the estimator.",
                    "label": 0
                },
                {
                    "sent": "And we use it very often to to choose these hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "Here, my hyperparameter innocence is row, but you could use it for tuning the width of your radial basis functions, or for finding the degree of your polynomial expansion or some other parameter that exists within your problem.",
                    "label": 0
                },
                {
                    "sent": "The number of basis functions etc.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's what we do.",
                    "label": 0
                },
                {
                    "sent": "So here's a number.",
                    "label": 0
                },
                {
                    "sent": "Bit of PowerPoint wizardry.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to put that into pictures.",
                    "label": 0
                },
                {
                    "sent": "We assume we've got all of our training data.",
                    "label": 1
                },
                {
                    "sent": "The ex is and the zeds.",
                    "label": 0
                },
                {
                    "sent": "And the zeds can be multidimensional when we typically talk about 1 dimensional targets.",
                    "label": 0
                },
                {
                    "sent": "But we could have multidimensional targets.",
                    "label": 0
                },
                {
                    "sent": "We break them up in this case, this is a five fold cross validation strategy, so we break them up into five non overlapping sets labeled an obvious way.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to look at the fourth stage of this process, so we've already been through.",
                    "label": 0
                },
                {
                    "sent": "We've taken.",
                    "label": 0
                },
                {
                    "sent": "X1Z1 trained our estimator and produce the outputs from that estimator.",
                    "label": 0
                },
                {
                    "sent": "That's why one we've done that all the way up to the third stage.",
                    "label": 0
                },
                {
                    "sent": "We take out X4.",
                    "label": 0
                },
                {
                    "sent": "We use all this lot here to train with.",
                    "label": 0
                },
                {
                    "sent": "OK we test on X4.",
                    "label": 0
                },
                {
                    "sent": "We don't need said four, so a typo.",
                    "label": 0
                },
                {
                    "sent": "It shouldn't be there.",
                    "label": 0
                },
                {
                    "sent": "We test using X4 and we produce.",
                    "label": 0
                },
                {
                    "sent": "Wife'll be.",
                    "label": 0
                },
                {
                    "sent": "Output from this trained estimator using that block of the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we now finished at the fourth stage we carry on.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do the same thing for the fifth stage.",
                    "label": 0
                },
                {
                    "sent": "Now we have a complete.",
                    "label": 0
                },
                {
                    "sent": "Set of outputs.",
                    "label": 0
                },
                {
                    "sent": "Based on every.",
                    "label": 0
                },
                {
                    "sent": "Member of the sample.",
                    "label": 0
                },
                {
                    "sent": "OK. And.",
                    "label": 0
                },
                {
                    "sent": "We can compare in some way which is convenient to us Y&Z.",
                    "label": 0
                },
                {
                    "sent": "OK, and that will give us some estimate of our what's generally called the generalization error.",
                    "label": 0
                },
                {
                    "sent": "But it can be any measure of performance you like.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's what happens with.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cross validation.",
                    "label": 0
                },
                {
                    "sent": "The result depends on the order you process the data, or shouldn't they?",
                    "label": 0
                },
                {
                    "sent": "Better not.",
                    "label": 0
                },
                {
                    "sent": "No, it doesn't.",
                    "label": 0
                },
                {
                    "sent": "I mean there are this good practice.",
                    "label": 0
                },
                {
                    "sent": "You come across bits of this in the lab this afternoon, but for instance, it's very important that there's no.",
                    "label": 0
                },
                {
                    "sent": "Irrelevant ordering.",
                    "label": 0
                },
                {
                    "sent": "In your training sample and if you put everything in.",
                    "label": 0
                },
                {
                    "sent": "Let's say it was a classification problem and you put all the Class A ones in the first half of the sample and all the Class B ones in the second half of the sample.",
                    "label": 0
                },
                {
                    "sent": "The whole thing will breakdown.",
                    "label": 0
                },
                {
                    "sent": "So typically one assumes that these are independent samples down here and you would randomly order them to start with in order to break up any.",
                    "label": 0
                },
                {
                    "sent": "Artificial ordering.",
                    "label": 0
                },
                {
                    "sent": "Within the sample.",
                    "label": 0
                },
                {
                    "sent": "That answer your.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "OK, we can talk about in the lab this afternoon, 'cause you'll see.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This election.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "All that fits very nicely into the linear in the parameters type problem.",
                    "label": 0
                },
                {
                    "sent": "Mainly because we have a very nice property there, which Tony's already mentioned, and that's this idea of a unique minimum.",
                    "label": 0
                },
                {
                    "sent": "Popping up so there's only one right answer for any given structure.",
                    "label": 0
                },
                {
                    "sent": "Any given choice of basis functions.",
                    "label": 1
                },
                {
                    "sent": "You only get one answer.",
                    "label": 0
                },
                {
                    "sent": "It's the best answer.",
                    "label": 1
                },
                {
                    "sent": "But the question of how do we choose these basis functions which has already been asked?",
                    "label": 0
                },
                {
                    "sent": "Hannah spin answered by the fact that it's hard.",
                    "label": 0
                },
                {
                    "sent": "We don't know what the right answer is.",
                    "label": 0
                },
                {
                    "sent": "OK, so one way we could do it is to.",
                    "label": 0
                },
                {
                    "sent": "Try to adapt them.",
                    "label": 1
                },
                {
                    "sent": "So linear models essentially are based on an idea of taking our data.",
                    "label": 0
                },
                {
                    "sent": "RX is pre processing them in some way through the Phis.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And that's a fixed function.",
                    "label": 0
                },
                {
                    "sent": "We treat it as a fixed operation.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "We estimate the parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, typically the cost function is in some way by 9.",
                    "label": 0
                },
                {
                    "sent": "That is, it has this nice property of a unique minimum, so that's why we like doing it.",
                    "label": 0
                },
                {
                    "sent": "'cause it's a nice easy optimization problem and no surprises.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "As the dimension goes up.",
                    "label": 0
                },
                {
                    "sent": "These problems can become combinatorial OK, particularly if we look at the polynomial problem.",
                    "label": 0
                },
                {
                    "sent": "Then you have this.",
                    "label": 0
                },
                {
                    "sent": "Massive explosion in the number of terms as the dimension goes up.",
                    "label": 0
                },
                {
                    "sent": "So if you take a.",
                    "label": 0
                },
                {
                    "sent": "An example often gave if you've got a 16 by 16 bitmap.",
                    "label": 0
                },
                {
                    "sent": "OK, and you want to treat that in some picture processing problem with a cubic polynomial function, you get 3 million terms, or nearly 3 million terms.",
                    "label": 0
                },
                {
                    "sent": "OK, most of which will be totally redundant.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You want it with a very, very big problem, starting with a relatively modest picture processing problem.",
                    "label": 0
                },
                {
                    "sent": "So there's a downside to doing these linear things.",
                    "label": 0
                },
                {
                    "sent": "You typically wind up with lots and lots and lots of things that maybe you don't need.",
                    "label": 1
                },
                {
                    "sent": "And we've got the arbitrary choices.",
                    "label": 0
                },
                {
                    "sent": "Things like row, things like, the width, etc degrees we have to.",
                    "label": 1
                },
                {
                    "sent": "Work with that, maybe in a cross validation strategy to to make these choices.",
                    "label": 0
                },
                {
                    "sent": "So the question of what is the best pre processor?",
                    "label": 0
                },
                {
                    "sent": "We think about it in that way.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To choose.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Major.",
                    "label": 0
                },
                {
                    "sent": "So this is what we've got as a kind of control engineer, really, so this is sort of way.",
                    "label": 0
                },
                {
                    "sent": "I tend to tend to think of things, signal flow flow through block diagrams, or we have is the input data going into some block which does something fi of X?",
                    "label": 0
                },
                {
                    "sent": "That goes into an adaptive layer which generates an output which is compared to the target, generates an error and the error is fed back into the adaptive layer there to find those WS those weights.",
                    "label": 0
                },
                {
                    "sent": "So one way forward.",
                    "label": 0
                },
                {
                    "sent": "Might be instead of trying to pick fixed preprocessing layer using some.",
                    "label": 0
                },
                {
                    "sent": "Strategy.",
                    "label": 0
                },
                {
                    "sent": "Let's go straight for the jugular.",
                    "label": 0
                },
                {
                    "sent": "And try to adapt it.",
                    "label": 0
                },
                {
                    "sent": "Directly in the same way as you're adapting the output layer or optimizing the output layer.",
                    "label": 0
                },
                {
                    "sent": "What about adapting or optimizing this preprocessing layer?",
                    "label": 0
                },
                {
                    "sent": "And that's what I'm going to talk about for the rest of.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Talk OK, the multilayer perceptrons is the archetypal version of.",
                    "label": 1
                },
                {
                    "sent": "This approach is not the only one, but it's the most widely looked at.",
                    "label": 0
                },
                {
                    "sent": "The most widely talked about and those widely studied.",
                    "label": 0
                },
                {
                    "sent": "Most widely misunderstood of all the neural networks.",
                    "label": 0
                },
                {
                    "sent": "OK, it's only very very vaguely related to anything to do with neurons, so we don't have to talk about any of that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "OK, it was popularized in the mid 80s by Rumelhart, Hinton and Williams in a paper in nature and completely changed the face of.",
                    "label": 0
                },
                {
                    "sent": "Data modeling introduced the modern phase of data modeling that we know about apart from the Bayesians who were doing stuff elsewhere you'll hear from them later in the week, but actually have been developed.",
                    "label": 0
                },
                {
                    "sent": "So 10 years before by Paul Verbas in 1974 and produced a massive thesis on this topic.",
                    "label": 0
                },
                {
                    "sent": "But in those days, nobody had computers that could even begin to do this kind of work, so it didn't capture the.",
                    "label": 0
                },
                {
                    "sent": "Public imagination and in fact you chiho another great control engineer in 1964 published a very similar.",
                    "label": 0
                },
                {
                    "sent": "Algorithm so it's been kicking around for a long time, but in those days they were pretty much a theoretical interest mid 80s.",
                    "label": 0
                },
                {
                    "sent": "We started to get the compute power that you needed to play with these things.",
                    "label": 0
                },
                {
                    "sent": "Basically, we're going to do is learn this preprocessing stage from the data instead of.",
                    "label": 1
                },
                {
                    "sent": "Trying to fix it or make the choice just out of the blue OK. And what we have is a layered feedforward structure that is a signal direction or signal flow direction is strictly from the input through to the output with no loops or sideways connections or anything like that.",
                    "label": 0
                },
                {
                    "sent": "The preprocessing part is done by a set of.",
                    "label": 0
                },
                {
                    "sent": "You would think of them as basis functions.",
                    "label": 0
                },
                {
                    "sent": "With a strictly sigmoidal shape and S shape, some people call them squashing functions.",
                    "label": 0
                },
                {
                    "sent": "The output function can be task specific if you're trying to do.",
                    "label": 0
                },
                {
                    "sent": "App.",
                    "label": 0
                },
                {
                    "sent": "Curve fitting type problem.",
                    "label": 0
                },
                {
                    "sent": "Then you typically have a linear output function.",
                    "label": 0
                },
                {
                    "sent": "If you're trying to do a classification type problem, you probably have another one of these S shaped things like the logistic function, which is restricted between zero and one, and we're going to try and model probabilities.",
                    "label": 0
                },
                {
                    "sent": "We don't think about it.",
                    "label": 0
                },
                {
                    "sent": "So here we have a completely non linear model.",
                    "label": 0
                },
                {
                    "sent": "This model will be not only nonlinear in its input output behavior, but it's also non linear in the parameters.",
                    "label": 0
                },
                {
                    "sent": "So it's a nonlinear.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No question about that.",
                    "label": 0
                },
                {
                    "sent": "There's a picture of a two layer multilayer perceptrons.",
                    "label": 0
                },
                {
                    "sent": "OK, the inputs arrive here.",
                    "label": 0
                },
                {
                    "sent": "They're all fed into every one of these little processing units in between, and then the outputs from those processing units are all fed into the output and we could have multiple outputs if we want.",
                    "label": 0
                },
                {
                    "sent": "It's no more difficult, it's just more.",
                    "label": 0
                },
                {
                    "sent": "Junk on the screen, basically.",
                    "label": 0
                },
                {
                    "sent": "So that's a two layer.",
                    "label": 0
                },
                {
                    "sent": "MLP because there are two active layers that are doing some computation.",
                    "label": 0
                },
                {
                    "sent": "That's just a buffer layer.",
                    "label": 0
                },
                {
                    "sent": "It doesn't do anything, so we don't include it in the count.",
                    "label": 0
                },
                {
                    "sent": "So the output is just some function Theta.",
                    "label": 0
                },
                {
                    "sent": "Depending on the task of the weighted sum of the outputs from all of these.",
                    "label": 0
                },
                {
                    "sent": "Plus some bias.",
                    "label": 0
                },
                {
                    "sent": "Add the outputs from each of these.",
                    "label": 0
                },
                {
                    "sent": "Vijay is some strictly sigmoidal function.",
                    "label": 0
                },
                {
                    "sent": "That's my views to Sigma.",
                    "label": 0
                },
                {
                    "sent": "Of the weighted input again, plus a bias.",
                    "label": 0
                },
                {
                    "sent": "Or a constant?",
                    "label": 0
                },
                {
                    "sent": "So we get this kind of nested.",
                    "label": 0
                },
                {
                    "sent": "View of the whole thing.",
                    "label": 0
                },
                {
                    "sent": "OK, this is very simple.",
                    "label": 0
                },
                {
                    "sent": "To compute this you can write the code in a few lines.",
                    "label": 0
                },
                {
                    "sent": "It's nothing at all.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hum.",
                    "label": 0
                },
                {
                    "sent": "You can have more layers if you wish.",
                    "label": 0
                },
                {
                    "sent": "Was just think about two sigmoidal unit.",
                    "label": 0
                },
                {
                    "sent": "Therefore take some inputs like that does some weighting of them.",
                    "label": 0
                },
                {
                    "sent": "Excuse me I'm passes them this the weighted sum through some function.",
                    "label": 0
                },
                {
                    "sent": "With and give us an output there OK, and the function is of that shape and there are many of these.",
                    "label": 0
                },
                {
                    "sent": "OK, I tend to use the logistic function.",
                    "label": 0
                },
                {
                    "sent": "For no good reason, you can use hyperbolic tangent or whatever you like, so long as it's got that shape and it's smooth.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to sharp corners then it's good enough.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. And essentially all that's happening in that middle layer is we're making linear combinations of things that are S shaped.",
                    "label": 1
                },
                {
                    "sent": "And the weights on those.",
                    "label": 0
                },
                {
                    "sent": "Shift them.",
                    "label": 0
                },
                {
                    "sent": "Up and down the X axis if you like.",
                    "label": 0
                },
                {
                    "sent": "And the biases.",
                    "label": 0
                },
                {
                    "sent": "Play some up and down the Y axis.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "And the weights.",
                    "label": 0
                },
                {
                    "sent": "Can scale them and.",
                    "label": 0
                },
                {
                    "sent": "That's that, so you take linear combinations of these S shaped things.",
                    "label": 1
                },
                {
                    "sent": "As an example we have.",
                    "label": 0
                },
                {
                    "sent": "The primary primal S shaped thing there going in that direction and then awaited version of it, which is obviously a negative sign which goes like that.",
                    "label": 0
                },
                {
                    "sent": "So it's been flipped around.",
                    "label": 0
                },
                {
                    "sent": "OK, add the two together, you get the red line there, you get a hump.",
                    "label": 0
                },
                {
                    "sent": "So in a sense.",
                    "label": 0
                },
                {
                    "sent": "We're able to represent in one dimension here a radial basis function.",
                    "label": 0
                },
                {
                    "sent": "It's not quite a Gaussian, but it's not far off against in radial basis function by taking two of these little guys.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So by adding together enough of these, or we hope by adding enough of these things together in appropriate linear combinations, we will get the shape of function.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we want is a 2 dimensional version.",
                    "label": 0
                },
                {
                    "sent": "Which I couldn't color in nicely, which does the same sort of thing which takes here.",
                    "label": 0
                },
                {
                    "sent": "We've got 2 dimensional sigmoids so you get.",
                    "label": 0
                },
                {
                    "sent": "Please kind of.",
                    "label": 0
                },
                {
                    "sent": "Structures which when added together with appropriate weightings.",
                    "label": 0
                },
                {
                    "sent": "Result in a function that again looks like now a 2 dimensional radial basis function.",
                    "label": 0
                },
                {
                    "sent": "There's nothing special about the radial basis function Ness of it, it's just a nice visual illustration.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "First thing is, can a multilayer perceptrons do the job?",
                    "label": 0
                },
                {
                    "sent": "Is it a universal approximator?",
                    "label": 0
                },
                {
                    "sent": "OK, and the answer is yes, it can basically.",
                    "label": 0
                },
                {
                    "sent": "Cybenko, Russian mathematician in 1990 showed that.",
                    "label": 0
                },
                {
                    "sent": "By making a linear combination of enough of these sigmoids, appropriate linear combination of enough of them.",
                    "label": 1
                },
                {
                    "sent": "You could represent any continuous function as closely as you like, so it's a similar result to the stone Vestra theorem for polynomials.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it's not a constructive proof, so it doesn't tell you what enough is OK, so we're into making choices once again, OK, but one of the key things is that a single hidden layer layer in the middle there, the preprocessing layer is called a hidden layer by neural networks folk.",
                    "label": 0
                },
                {
                    "sent": "Becausw it doesn't directly connect to the outside world either.",
                    "label": 0
                },
                {
                    "sent": "The outputs or the inputs buffered from both, so it's called hidden.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "All you need is a single one of those layers.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not to say that you might not get a better result, perhaps with fewer of these processing units.",
                    "label": 0
                },
                {
                    "sent": "If you have more hidden layers.",
                    "label": 0
                },
                {
                    "sent": "OK, but then of course you've got more choices to make and.",
                    "label": 1
                },
                {
                    "sent": "Unsurprisingly, the more choices you've got more difficult your problem becomes, so knowing that having a single hidden layer.",
                    "label": 0
                },
                {
                    "sent": "Is good enough.",
                    "label": 0
                },
                {
                    "sent": "Is a bit of a benefit.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "The MLP is one of these types of things that can represent any reasonable function.",
                    "label": 0
                },
                {
                    "sent": "The sorts of functions we might want to represent as engineers as closely as we like.",
                    "label": 0
                },
                {
                    "sent": "I'm sure mathematicians will find functions that it can't represent.",
                    "label": 0
                },
                {
                    "sent": "But we'd probably say, well, as engineers, we don't care.",
                    "label": 0
                },
                {
                    "sent": "Will approximate at that point.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The question is then, if we've got this structure, what we've got to do is choose.",
                    "label": 0
                },
                {
                    "sent": "The parameters in that hidden layer.",
                    "label": 0
                },
                {
                    "sent": "In some optimal way, OK?",
                    "label": 0
                },
                {
                    "sent": "We're already going to choose the output layer parameters in an optimal way.",
                    "label": 0
                },
                {
                    "sent": "We've seen how to do that in the previous talk, so we've just got to do these as well.",
                    "label": 0
                },
                {
                    "sent": "So you think therefore, that the problem of.",
                    "label": 0
                },
                {
                    "sent": "Modeling.",
                    "label": 0
                },
                {
                    "sent": "Anything is solved.",
                    "label": 0
                },
                {
                    "sent": "Completely OK.",
                    "label": 0
                },
                {
                    "sent": "But actually all of the problems.",
                    "label": 0
                },
                {
                    "sent": "Tony talked about and I've talked about up until now.",
                    "label": 0
                },
                {
                    "sent": "Still exist.",
                    "label": 0
                },
                {
                    "sent": "OK, we've got a structure.",
                    "label": 0
                },
                {
                    "sent": "It can do the job.",
                    "label": 0
                },
                {
                    "sent": "If we can pick.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These guys.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "One of the questions that we might ask first of all, is what does it do?",
                    "label": 0
                },
                {
                    "sent": "This is the only equation really that I'm going to show.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to talk about this in terms of minimizing the sum of squares error, but there's similar results for whole variety of tasks that we might be.",
                    "label": 0
                },
                {
                    "sent": "Seeking to carry out.",
                    "label": 0
                },
                {
                    "sent": "OK, it turns out it's not actually that difficult to prove that minimizing the sum of squares error is equivalent to finding the conditional mean of the target data, which is precisely what we want to do.",
                    "label": 1
                },
                {
                    "sent": "This is this regression property that was mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "So if we just look at the theoretical problem rather than our real problem, which is yes, we've got a bit of data and.",
                    "label": 0
                },
                {
                    "sent": "This question, which I haven't discussed yet but will look at in a minute if we assume that we've found the set of weights that gives us the best possible answer.",
                    "label": 0
                },
                {
                    "sent": "To this problem.",
                    "label": 0
                },
                {
                    "sent": "And that we've got an infinite amount of data.",
                    "label": 0
                },
                {
                    "sent": "Then we are in a position to rewrite the sum of squares error.",
                    "label": 0
                },
                {
                    "sent": "Coded J Infinity.",
                    "label": 0
                },
                {
                    "sent": "I put the Infinity there just to indicate.",
                    "label": 0
                },
                {
                    "sent": "It's a theoretical.",
                    "label": 0
                },
                {
                    "sent": "Result.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite it as a sum of two components OK.",
                    "label": 0
                },
                {
                    "sent": "It's the average of the square of this thing in brackets here, plus the average of the square of this thing.",
                    "label": 0
                },
                {
                    "sent": "In brackets here, OK the sum Asian here is because I've written this in the general form where you might have multiple outputs.",
                    "label": 0
                },
                {
                    "sent": "OK. P is just the probability density function of X&Y is the expectation operator.",
                    "label": 0
                },
                {
                    "sent": "So we say we got two components here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The first one.",
                    "label": 0
                },
                {
                    "sent": "Is the expectation or mean value of?",
                    "label": 0
                },
                {
                    "sent": "The target data.",
                    "label": 0
                },
                {
                    "sent": "At a particular point X, so for a particular value of the input.",
                    "label": 0
                },
                {
                    "sent": "Minus our estimate of the function.",
                    "label": 0
                },
                {
                    "sent": "Which turns out to be the multilayer perception itself, of course, which is self is a function of X and a function of.",
                    "label": 0
                },
                {
                    "sent": "I've used the curly W to indicate all of the weights in the thing, not just a single vector away.",
                    "label": 0
                },
                {
                    "sent": "OK, so we take that difference there and we square it and we find the average of it.",
                    "label": 0
                },
                {
                    "sent": "Added to that, is this other term here, which is the difference between.",
                    "label": 0
                },
                {
                    "sent": "The target data and the average of the target data at a particular value of X. OK.",
                    "label": 0
                },
                {
                    "sent": "I actually should be subscripted with.",
                    "label": 0
                },
                {
                    "sent": "I those X is how I look at it, OK?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Or should they?",
                    "label": 0
                },
                {
                    "sent": "Think about that.",
                    "label": 1
                },
                {
                    "sent": "Crucial thing is this one here doesn't depend on the weights of the multilayer perception at all.",
                    "label": 0
                },
                {
                    "sent": "OK, this is just some residual error that cannot be.",
                    "label": 0
                },
                {
                    "sent": "Cannot be changed by the introduction of this this neural network.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we can see is that if we want to make J as small as possible, we can't do anything about that by choosing W, so we can ignore that.",
                    "label": 0
                },
                {
                    "sent": "The error as small as possible.",
                    "label": 0
                },
                {
                    "sent": "We have to make the function itself equal to.",
                    "label": 0
                },
                {
                    "sent": "What's called the conditional mean.",
                    "label": 0
                },
                {
                    "sent": "The expected value of the Z given the X is OK.",
                    "label": 0
                },
                {
                    "sent": "So under these very extreme theoretical conditions, we see that precisely.",
                    "label": 0
                },
                {
                    "sent": "What we're trying to do is.",
                    "label": 0
                },
                {
                    "sent": "The right thing.",
                    "label": 0
                },
                {
                    "sent": "OK, in reality we don't have a finite amount of data, so.",
                    "label": 0
                },
                {
                    "sent": "It will just simply be.",
                    "label": 0
                },
                {
                    "sent": "An estimate of the right thing.",
                    "label": 0
                },
                {
                    "sent": "Nonetheless, it's encouraging.",
                    "label": 0
                },
                {
                    "sent": "If you do it, yes.",
                    "label": 0
                },
                {
                    "sent": "What is indicating is that if you.",
                    "label": 0
                },
                {
                    "sent": "Extreme case where you had an infinite amount of data, so covering all of X basically, then by making by driving J to ITS global minimum by an appropriate choice of W. You would get equality there and this term would disappear.",
                    "label": 0
                },
                {
                    "sent": "OK. No no.",
                    "label": 0
                },
                {
                    "sent": "No no, all it's saying is if you could do this, you would be getting the right thing.",
                    "label": 0
                },
                {
                    "sent": "OK. And that's what we're trying to do with trying to minimize the mean squared error over a finite amount of data and all these other problems that come with that.",
                    "label": 0
                },
                {
                    "sent": "All it's sort of suggesting is that we're heading in the right direction that we're trying to estimate the conditional mean, and it's actually it's a probability of.",
                    "label": 0
                },
                {
                    "sent": "Prop.",
                    "label": 0
                },
                {
                    "sent": "It's a property of the sum of squares error.",
                    "label": 0
                },
                {
                    "sent": "It's nothing to do with the multilayer perception itself.",
                    "label": 0
                },
                {
                    "sent": "Any estimator that you put in there.",
                    "label": 0
                },
                {
                    "sent": "Would have the same properties as I say this widens completely to other forms of cost as well, so in the cases of classifications and stuff we get the same desirable effect.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the best we can say is that if we make our mean squared error small.",
                    "label": 0
                },
                {
                    "sent": "Then the function that we estimate can be interpreted as an approximation of.",
                    "label": 0
                },
                {
                    "sent": "The conditional mean.",
                    "label": 0
                },
                {
                    "sent": "Of the target data, which is what we would like it to be.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're doing the right thing.",
                    "label": 0
                },
                {
                    "sent": "I think is the message from that.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not a silly thing to be doing.",
                    "label": 0
                },
                {
                    "sent": "The pros of the multilayer perception.",
                    "label": 0
                },
                {
                    "sent": "Because obviously we're going to have some difficulties creeping in here.",
                    "label": 0
                },
                {
                    "sent": "The pros are it's compactness.",
                    "label": 0
                },
                {
                    "sent": "OK. That you're capable of getting.",
                    "label": 0
                },
                {
                    "sent": "Very high.",
                    "label": 0
                },
                {
                    "sent": "Accuracy models.",
                    "label": 0
                },
                {
                    "sent": "With very small networks networks very few.",
                    "label": 0
                },
                {
                    "sent": "Processing units in.",
                    "label": 0
                },
                {
                    "sent": "In contrast, for instance to the polynomial situation where you have this combinatorial explosion.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This whole question of compact compactness or sparsity is you'll hear it referred to in.",
                    "label": 0
                },
                {
                    "sent": "Other arenas come up during this week is very important.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't have more than you need in any of these things.",
                    "label": 0
                },
                {
                    "sent": "OK again, it goes back to Einstein and Occam's Razor and all these other things that say your model should be as simple as possible, but no.",
                    "label": 0
                },
                {
                    "sent": "Simply so.",
                    "label": 0
                },
                {
                    "sent": "In some of the linear in the parameters, types of approaches that will crop up in the week.",
                    "label": 0
                },
                {
                    "sent": "You then have to try to control their size in some other way, so the multilayer perceptron is a is something that can.",
                    "label": 0
                },
                {
                    "sent": "Yield.",
                    "label": 0
                },
                {
                    "sent": "A naturally compact or sparse model?",
                    "label": 0
                },
                {
                    "sent": "It's also got a very simple training algorithm.",
                    "label": 1
                },
                {
                    "sent": "In its simplest sense, OK, and that was very attractive back in 1984, where you know fairly modest problem.",
                    "label": 0
                },
                {
                    "sent": "On a PC in those days would have taken, you know, took four 5 days to run OK. Something will take a minute or two nowadays on a decent PC would take some days and you had very little ram, so you needed a very simple.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compactness, this is, this comes from a piece of work by Andrew Barron back in the 90s, which.",
                    "label": 0
                },
                {
                    "sent": "I must confess I never followed the proof, I just couldn't follow it.",
                    "label": 0
                },
                {
                    "sent": "It was too obscured for me, but this is the result, and nobody's ever argued with it seriously.",
                    "label": 0
                },
                {
                    "sent": "Which demonstrates for the situation of using polynomial bases.",
                    "label": 0
                },
                {
                    "sent": "For increasing dimensions of input OK versus using the multilayer perceptrons structure.",
                    "label": 0
                },
                {
                    "sent": "And the message here is that.",
                    "label": 0
                },
                {
                    "sent": "The multilayer perceptron structure is capable of reducing the sum of squares error.",
                    "label": 0
                },
                {
                    "sent": "I've done it as on a normalized scale there.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In off reducing the.",
                    "label": 0
                },
                {
                    "sent": "Total sum of squares error OK, as a function of the number of units in its hidden layer.",
                    "label": 0
                },
                {
                    "sent": "OK, is not dependent on the dimensionality of the input.",
                    "label": 0
                },
                {
                    "sent": "What we're seeing elsewhere is for the series solution with this, strictly with the polynomial is as the dimension of the input increases.",
                    "label": 0
                },
                {
                    "sent": "From this situation here we got the light blue line.",
                    "label": 0
                },
                {
                    "sent": "D is equal to 5D is equal to 10 and then D is equal to 50.",
                    "label": 0
                },
                {
                    "sent": "Then what it's basically showing us is the extreme sensitivity.",
                    "label": 0
                },
                {
                    "sent": "So if you want to reduce your.",
                    "label": 0
                },
                {
                    "sent": "Make some squares error by a factor of 10.",
                    "label": 0
                },
                {
                    "sent": "With a multi level set Tron.",
                    "label": 0
                },
                {
                    "sent": "And if we read across down to here to 110th.",
                    "label": 0
                },
                {
                    "sent": ".1 it says that you require about.",
                    "label": 0
                },
                {
                    "sent": "A size of about 10 hidden units.",
                    "label": 0
                },
                {
                    "sent": "This is relative OK, whereas if you've got a regardless of the input dimension, whereas if you've got a 5 dimensional 5 dimensional problem to achieve the same amount, you're going to need kind of 150 sort of hidden units to get the same reduction in the amount of.",
                    "label": 0
                },
                {
                    "sent": "Some squares are not an easy thing to see.",
                    "label": 0
                },
                {
                    "sent": "But certainly in practice, it does seem that you can get very compact models with these multilayer perceptrons, so that's a big plus point.",
                    "label": 0
                },
                {
                    "sent": "Particularly, you wanted to do something in real time on our Mike.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Processor or something like that?",
                    "label": 0
                },
                {
                    "sent": "The backpropagation algorithm is essentially one line of code for each of the weights.",
                    "label": 0
                },
                {
                    "sent": "OK, obviously the loop is very, very simple indeed.",
                    "label": 0
                },
                {
                    "sent": "And that is the thing that makes the weight change over overtime and seeks to minimize the mean squared error.",
                    "label": 0
                },
                {
                    "sent": "I'm only showing that to show you how sick.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cool, it looks OK, nobody uses it anymore.",
                    "label": 0
                },
                {
                    "sent": "The cons, which are the things I want to concentrate on our the relationship between the parameters.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry, we have to end up with two slides.",
                    "label": 0
                },
                {
                    "sent": "I'm surprised as well.",
                    "label": 0
                },
                {
                    "sent": "Is it then?",
                    "label": 0
                },
                {
                    "sent": "Experimental.",
                    "label": 0
                },
                {
                    "sent": "No, this is this is a lengthy mathematical proof, which as I said.",
                    "label": 0
                },
                {
                    "sent": "More.",
                    "label": 0
                },
                {
                    "sent": "More.",
                    "label": 0
                },
                {
                    "sent": "Different combinations in the feature space.",
                    "label": 0
                },
                {
                    "sent": "Well, yes, but this is not.",
                    "label": 0
                },
                {
                    "sent": "It's not to do with representing in the feature space, it's to do with.",
                    "label": 0
                },
                {
                    "sent": "How much bigger you need to make your network in order to achieve the same level of reduction in sum of squares error.",
                    "label": 0
                },
                {
                    "sent": "I'm not 100% convinced by this.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm only using it to illustrate that.",
                    "label": 0
                },
                {
                    "sent": "Typically you will get away with much smaller.",
                    "label": 0
                },
                {
                    "sent": "Structures than you will if you use polynomial bases OK. Yeah, the series, sorry.",
                    "label": 0
                },
                {
                    "sent": "I see how many series yeah?",
                    "label": 0
                },
                {
                    "sent": "You might, it depends on how complex the function underlying function is.",
                    "label": 0
                },
                {
                    "sent": "Of course it's more to do with how complex the relationship is than accurately representing the features in the.",
                    "label": 0
                },
                {
                    "sent": "In the data, but yeah, I I find this quite hard to stomach exactly.",
                    "label": 0
                },
                {
                    "sent": "But it conveys a message.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That OK?",
                    "label": 0
                },
                {
                    "sent": "Herbal yeah, so I call it a malign relationship because the parameters are now non linearly related to the output, hence the error.",
                    "label": 0
                },
                {
                    "sent": "Hence any function of that error becomes rather more complicated than than ice bowl shaped function that we had for our linear in the parameters models.",
                    "label": 0
                },
                {
                    "sent": "OK, the optimization becomes much more difficult.",
                    "label": 0
                },
                {
                    "sent": "OK this is not to do with neural networks, it's to do with.",
                    "label": 0
                },
                {
                    "sent": "On the near optimizations.",
                    "label": 0
                },
                {
                    "sent": "In particular, there are many possible solutions.",
                    "label": 0
                },
                {
                    "sent": "That are mathematically minima.",
                    "label": 0
                },
                {
                    "sent": "Downward turning points.",
                    "label": 0
                },
                {
                    "sent": "OK. Um?",
                    "label": 0
                },
                {
                    "sent": "So yeah, so even with a linear output unit, the effect of the hidden weights in the output is nonlinear, so.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're in trouble, so here's another.",
                    "label": 0
                },
                {
                    "sent": "This is this is my piester resistance bits of.",
                    "label": 0
                },
                {
                    "sent": "PowerPoint.",
                    "label": 0
                },
                {
                    "sent": "Here's a physical example in the nice linear in the parameters case OK, one dimension.",
                    "label": 0
                },
                {
                    "sent": "We've got a nice quadratic error surface.",
                    "label": 0
                },
                {
                    "sent": "OK, if we roll a ball down a nice quadratic error surface notwithstanding, the way the acceleration is enormously, it went at the end there.",
                    "label": 0
                },
                {
                    "sent": "It rolls down, it goes back to the forwards a bit, it gives up its energy because of friction or whatever, and it comes to a halt at the only possible place it can come to a halt.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what it does, it sets off in the negative gradient direction.",
                    "label": 0
                },
                {
                    "sent": "Um and.",
                    "label": 0
                },
                {
                    "sent": "In reality.",
                    "label": 0
                },
                {
                    "sent": "Does a bit of this and then comes to a halt.",
                    "label": 0
                },
                {
                    "sent": "OK, at the point where the gradient of J with respect to W is equal to.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Error.",
                    "label": 0
                },
                {
                    "sent": "We can mimic that.",
                    "label": 0
                },
                {
                    "sent": "Everybody I hope, knows about the gradient descent algorithm, but I couldn't resist doing another one of these.",
                    "label": 0
                },
                {
                    "sent": "OK, we make the rate of change of the weights.",
                    "label": 0
                },
                {
                    "sent": "With respect to time proportional to the negative gradient direction, so we make it go downhill and we step downhill.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we discretize the whole problem and instead of rolling up the other side of it and doing all this, it just keeps stepping downhill until such time as it's unable to move.",
                    "label": 0
                },
                {
                    "sent": "Without going uphill and you stop the algorithm, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We get to our minimum point without any real difficulty, so even in the case when we can't do our closed form solution, we can still find these minima.",
                    "label": 0
                },
                {
                    "sent": "Quite easily with some iterative algorithm given by something that looks like.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fat however, as soon as we have a nonlinear relationship between the weights and the output, then.",
                    "label": 0
                },
                {
                    "sent": "Everything changes and we wind up with cost services that can look like this.",
                    "label": 0
                },
                {
                    "sent": "It's very simple looking cost for service here.",
                    "label": 0
                },
                {
                    "sent": "That should say local minimum there is cut and paste for you.",
                    "label": 0
                },
                {
                    "sent": "That should say local minima.",
                    "label": 0
                },
                {
                    "sent": "OK, here we have two downward facing turning points, but actually there could be any number of 'em there could be any number of them that all achieve the global minimum as well, which is a little bit tricky.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "If we can only move downhill.",
                    "label": 0
                },
                {
                    "sent": "We wind up at that point there, which is a local minimum, not a global minimum.",
                    "label": 1
                },
                {
                    "sent": "We start there.",
                    "label": 0
                },
                {
                    "sent": "We wind up where we'd like to be.",
                    "label": 0
                },
                {
                    "sent": "If we start there, we wind up where we'd like to be.",
                    "label": 0
                },
                {
                    "sent": "And if we start there, we wind up where we don't want to be.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The problem is, once again is being blind to all this, we have no idea where to start.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is a fundamental problem.",
                    "label": 0
                },
                {
                    "sent": "Nonconvex optimization.",
                    "label": 0
                },
                {
                    "sent": "There are books and books or department's devoted to it.",
                    "label": 0
                },
                {
                    "sent": "There are people that devote their entire research lives to the problem.",
                    "label": 0
                },
                {
                    "sent": "It's fundamentally difficult.",
                    "label": 0
                },
                {
                    "sent": "It's like trying to find your way down off the Scottish Highlands in the mist without a map.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you had the map, you'd know where the best answer was anyway, and we could all go home.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't have to do any kind of optimization.",
                    "label": 0
                },
                {
                    "sent": "We don't have the map.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "All of these.",
                    "label": 0
                },
                {
                    "sent": "You're all these multilayer perceptron things are.",
                    "label": 0
                },
                {
                    "sent": "Suffer from a slight problem that is you don't know.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You've got the best solution or not.",
                    "label": 0
                },
                {
                    "sent": "Here's the same thing in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "I couldn't make an animation of that, but it's the same basic idea.",
                    "label": 0
                },
                {
                    "sent": "It looks a bit more like a mountain range now where you start determines where you finish.",
                    "label": 0
                },
                {
                    "sent": "OK. And.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's a big problem.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we'll assume that we're looking at minimization.",
                    "label": 0
                },
                {
                    "sent": "We assume that it's an analytically intractable problem, which it is for almost everything except for the nice.",
                    "label": 1
                },
                {
                    "sent": "Linear in the parameters summer squares case.",
                    "label": 0
                },
                {
                    "sent": "We step our parameters downhill.",
                    "label": 1
                },
                {
                    "sent": "That's the best we can do.",
                    "label": 0
                },
                {
                    "sent": "OK. Basically a new set of weights is equal to our own set of weights plus a step in the right direction.",
                    "label": 0
                },
                {
                    "sent": "And all of the techniques for doing this kind of optimization are a matter of choosing a better step.",
                    "label": 1
                },
                {
                    "sent": "Yeah, the backpropagation algorithm.",
                    "label": 0
                },
                {
                    "sent": "Is one that just takes a step that says what's what's the steepest direction to go in right?",
                    "label": 0
                },
                {
                    "sent": "I'll go that way one step OK?",
                    "label": 0
                },
                {
                    "sent": "You can do better than that, so back prop.",
                    "label": 0
                },
                {
                    "sent": "It's very lightweight from a computational point of view.",
                    "label": 0
                },
                {
                    "sent": "But it's very, very slow to converge.",
                    "label": 0
                },
                {
                    "sent": "Nowadays, much smarter algorithms.",
                    "label": 1
                },
                {
                    "sent": "Scaled conjugate gradients, which will use this afternoon, levenberg marquardt's probably the one of the most sophisticated ones, so these are the things that people use nowadays for preference over the so called back prop.",
                    "label": 0
                },
                {
                    "sent": "But at the end of the day, all these do pick a smarter step in the right direction.",
                    "label": 1
                },
                {
                    "sent": "OK, they're still just trying to travel downhill based only on local.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Information.",
                    "label": 0
                },
                {
                    "sent": "OK, there's just a picture of in a 2 dimensional space of.",
                    "label": 0
                },
                {
                    "sent": "An algorithm trying to converge down to its.",
                    "label": 0
                },
                {
                    "sent": "Minimum point offer cost surface that looks like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a very small.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Multilayer perceptrons?",
                    "label": 0
                },
                {
                    "sent": "OK, if we start it in a slightly different position, it rolls off down.",
                    "label": 0
                },
                {
                    "sent": "That slope there.",
                    "label": 0
                },
                {
                    "sent": "OK, so we got a minimum down there and we got a minimum.",
                    "label": 0
                },
                {
                    "sent": "Off over there as well, where you start determines where you finish.",
                    "label": 0
                },
                {
                    "sent": "Stunning.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "MATLAB Neural networks toolbox.",
                    "label": 0
                },
                {
                    "sent": "And that's just a.",
                    "label": 0
                },
                {
                    "sent": "An illustration of how much quicker.",
                    "label": 0
                },
                {
                    "sent": "Different algorithms can be, so there's the back prop algorithm, which staggers slowly, taking many, many many many steps to get down to this minimum point here.",
                    "label": 0
                },
                {
                    "sent": "OK. Conjugate gradients, on the other hand goes wrong, goes right off the.",
                    "label": 1
                },
                {
                    "sent": "Screen here it comes back here.",
                    "label": 0
                },
                {
                    "sent": "That's two steps 3, four and then anymore than that and you're already very very close to the minimum point.",
                    "label": 0
                },
                {
                    "sent": "So it takes it's much quicker in the sense that it takes far fewer steps to arrive at a local.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solution to the problem.",
                    "label": 0
                },
                {
                    "sent": "OK, the implications of this are you can get a perfect structure.",
                    "label": 0
                },
                {
                    "sent": "It could be the structure that actually generated the data in the first place.",
                    "label": 0
                },
                {
                    "sent": "And you still get the wrong answer because you didn't know where to start your optimization from.",
                    "label": 0
                },
                {
                    "sent": "So we got this extreme sort of dependency on initial conditions.",
                    "label": 1
                },
                {
                    "sent": "It's real, you will find this happening, will probably find it happens this afternoon on very simple examples.",
                    "label": 0
                },
                {
                    "sent": "But any local solution might be good enough again.",
                    "label": 0
                },
                {
                    "sent": "If your objective is to develop a system that solves the problem.",
                    "label": 0
                },
                {
                    "sent": "Your boss says improve this process by 10% is going to give you any extra marks by improving it by 90%.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the objective is to improve it by 10%.",
                    "label": 0
                },
                {
                    "sent": "Is good enough.",
                    "label": 0
                },
                {
                    "sent": "So it might be good enough.",
                    "label": 1
                },
                {
                    "sent": "You've gotta train it.",
                    "label": 0
                },
                {
                    "sent": "You've gotta test it and see, OK?",
                    "label": 0
                },
                {
                    "sent": "Well, that's fair enough.",
                    "label": 0
                },
                {
                    "sent": "It's not good enough for mathematicians, maybe, but it's good enough for engineers who are trying to solve a practical problem.",
                    "label": 0
                },
                {
                    "sent": "So again, we've got all this training and testing, cross validation, etc.",
                    "label": 0
                },
                {
                    "sent": "But we have some slight.",
                    "label": 0
                },
                {
                    "sent": "Additional problems with cross validation because every time you train an estimator, you now have that additional question of.",
                    "label": 0
                },
                {
                    "sent": "If the performance isn't good enough.",
                    "label": 0
                },
                {
                    "sent": "Is it because?",
                    "label": 0
                },
                {
                    "sent": "The network itself, or the estimator itself is deficient in some way.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's not got enough hidden units in it.",
                    "label": 0
                },
                {
                    "sent": "Or is it simply not giving a good answer?",
                    "label": 1
                },
                {
                    "sent": "Because if I chosen a different set of initial weights?",
                    "label": 0
                },
                {
                    "sent": "I would have come to a better.",
                    "label": 0
                },
                {
                    "sent": "Solution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You can get.",
                    "label": 0
                },
                {
                    "sent": "The wrong answer for the wrong reason this this weights initial weights business which which causes some difficulty.",
                    "label": 0
                },
                {
                    "sent": "So it gives us additional dimension to our development process which you have to be aware of.",
                    "label": 0
                },
                {
                    "sent": "There's not much you can do about it.",
                    "label": 0
                },
                {
                    "sent": "Multiple starts, which adds to the workload.",
                    "label": 0
                },
                {
                    "sent": "So you just try out lots and lots of different values and see if they all converge to roughly the same final.",
                    "label": 0
                },
                {
                    "sent": "Set of values.",
                    "label": 0
                },
                {
                    "sent": "That's one way of doing it.",
                    "label": 0
                },
                {
                    "sent": "There are other possibilities where you train with lots of starts and then combine all those neural networks together.",
                    "label": 0
                },
                {
                    "sent": "Again, you end up with a much bigger thing, of course.",
                    "label": 0
                },
                {
                    "sent": "And you know this whole research area is devoted to looking at those sorts of things.",
                    "label": 0
                },
                {
                    "sent": "But anyway, so this problem of multiple solutions is.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Real it's there, it happens.",
                    "label": 0
                },
                {
                    "sent": "It happens for these linear in the parameters type models as well.",
                    "label": 0
                },
                {
                    "sent": "I've picked on radial basis function neural networks.",
                    "label": 0
                },
                {
                    "sent": "But it's the same.",
                    "label": 0
                },
                {
                    "sent": "Tony's already alluded to it really.",
                    "label": 0
                },
                {
                    "sent": "You know, people often say all, I use it radial basis function network because it's got a unique solution to unique solution.",
                    "label": 1
                },
                {
                    "sent": "Once you've chosen the number, the widths and the positions of your radial basis functions.",
                    "label": 0
                },
                {
                    "sent": "And choosing those is a highly nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so the problem never goes away.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You just move it around and pick your favorite OK. One way of doing this for these kind of things is to go for direct optimization, in which case you've got something which is pretty much like the multilayer perceptron.",
                    "label": 0
                },
                {
                    "sent": "If you try to optimize the centers and the.",
                    "label": 0
                },
                {
                    "sent": "And the widths directly.",
                    "label": 0
                },
                {
                    "sent": "OK, so you don't gain anything that way.",
                    "label": 0
                },
                {
                    "sent": "So most linear in the parameters models.",
                    "label": 0
                },
                {
                    "sent": "Have nonlinear parameters which have got to be selected parameters which have an effect.",
                    "label": 1
                },
                {
                    "sent": "Nonlinearly on the app.",
                    "label": 0
                },
                {
                    "sent": "In a sense, you've got a kind of an outer loop of optimization to do.",
                    "label": 0
                },
                {
                    "sent": "So it's all part of the no such thing as a free lunch theorem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you just shift the problems around.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are they?",
                    "label": 0
                },
                {
                    "sent": "Are they really a problem?",
                    "label": 1
                },
                {
                    "sent": "Well, you know the pros seem to outweigh the cons.",
                    "label": 1
                },
                {
                    "sent": "The compactness quite rapid speed of development and multilayer perceptrons.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We can often get good solutions very quickly with a little bit of experimentation.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "All of the previous things apply, but you know these things are out there.",
                    "label": 0
                },
                {
                    "sent": "They're being used, they work OK. Um?",
                    "label": 0
                },
                {
                    "sent": "If you haven't got enough data, if it's not in the right place.",
                    "label": 0
                },
                {
                    "sent": "If it's not representative of where the network is going to be used ultimately, then you're going to have problems.",
                    "label": 0
                },
                {
                    "sent": "OK, that's not to do with multilayer perceptrons, it's to do with any form of prediction or estimation.",
                    "label": 0
                },
                {
                    "sent": "Um, lack of prior knowledge again is a problem.",
                    "label": 1
                },
                {
                    "sent": "Going back to this idea of the truth.",
                    "label": 0
                },
                {
                    "sent": "Of course, if we fit on multilayer perception, it's unlikely to be the truth because they don't exist in nature.",
                    "label": 0
                },
                {
                    "sent": "They don't even exist as engineering structures, so they're unlikely to have generated your real data in the first place.",
                    "label": 0
                },
                {
                    "sent": "However, they are able to get arbitrarily close to.",
                    "label": 0
                },
                {
                    "sent": "The true function.",
                    "label": 0
                },
                {
                    "sent": "Even though we don't.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The tree function is.",
                    "label": 0
                },
                {
                    "sent": "We're running over just on time.",
                    "label": 0
                },
                {
                    "sent": "So the areas you can use them in is essentially to generalize what.",
                    "label": 0
                },
                {
                    "sent": "Stand decisions would normally refer to as generalized linear models, so the linear model, the straightforward straight line relationship for hyperplane, our relationship in multi dimensions.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, linear regression as we call it or curve fitting.",
                    "label": 1
                },
                {
                    "sent": "Typically you would have a linear output unit, sum of squares error criterion you minimize that.",
                    "label": 0
                },
                {
                    "sent": "Can regularize so you can choose a very large network and then regularize it using cross validation to get a good answer.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression is another example of a generalized linear model where we want to classify things where we learn the posterior probability of class membership.",
                    "label": 0
                },
                {
                    "sent": "Then we use a logistic.",
                    "label": 0
                },
                {
                    "sent": "Output function.",
                    "label": 0
                },
                {
                    "sent": "And an appropriate measure of error, not sum of squares error anymore, but something called the cross entropy or the deviance.",
                    "label": 0
                },
                {
                    "sent": "You take those two things together.",
                    "label": 0
                },
                {
                    "sent": "All makes perfect sense and you can have.",
                    "label": 1
                },
                {
                    "sent": "Multiple classes, multinomial logistic regression, ordinal logistic regression, etc.",
                    "label": 1
                },
                {
                    "sent": "Forget about that, Poisson regression is another form of generalized linear model.",
                    "label": 0
                },
                {
                    "sent": "When you, when your data that you're trying to model accounts, things like radiation counts or whatever.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Failure time problems and stuff like OK, so anywhere where you might use a generalized linear model.",
                    "label": 0
                },
                {
                    "sent": "Cox regression for survival and stuff like that you can.",
                    "label": 0
                },
                {
                    "sent": "Front end with.",
                    "label": 0
                },
                {
                    "sent": "And MLP hidden layer and have.",
                    "label": 0
                },
                {
                    "sent": "A more exotic relationship between the inputs and the outputs.",
                    "label": 0
                },
                {
                    "sent": "Let me just.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Forget that.",
                    "label": 0
                },
                {
                    "sent": "And give you.",
                    "label": 0
                },
                {
                    "sent": "A quick example of local solutions.",
                    "label": 0
                },
                {
                    "sent": "OK again is done with the MATLAB neural networks toolbox, but this illustrates this question of what happens when you get to one of the many possible minima in your cost function.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we've chosen the number of hidden neurons to be quite large.",
                    "label": 0
                },
                {
                    "sent": "There are nine of them.",
                    "label": 0
                },
                {
                    "sent": "OK, and this difficulty index is just the shape of the function that's trying to be modelled, which is clearly a sine wave.",
                    "label": 0
                },
                {
                    "sent": "OK, so in both cases we're doing the same thing.",
                    "label": 0
                },
                {
                    "sent": "We just have different random starts or use a random number generator to choose the initial weights for a multilayer perceptron with 9 hidden units in a single linear output unit.",
                    "label": 0
                },
                {
                    "sent": "We allow things to minimize the mean squared error until it comes to.",
                    "label": 0
                },
                {
                    "sent": "The bottom of a downward facing.",
                    "label": 0
                },
                {
                    "sent": "Valley nine dimension.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What do we see?",
                    "label": 0
                },
                {
                    "sent": "Well, we get something that looks pretty much like a sinusoid mostly, but it's got this odd bit here.",
                    "label": 0
                },
                {
                    "sent": "We start from a different.",
                    "label": 0
                },
                {
                    "sent": "Place in the.",
                    "label": 0
                },
                {
                    "sent": "Landscape, then we get something that's completely different, completely different shape.",
                    "label": 0
                },
                {
                    "sent": "It's still doing its best to try and get to around some of the data neisen.",
                    "label": 0
                },
                {
                    "sent": "Tightly, but the point is is that it's the same neural network, it's just that its weights have been optimized by beginning at a different.",
                    "label": 0
                },
                {
                    "sent": "Position and we get a completely different function.",
                    "label": 0
                },
                {
                    "sent": "So neither is particularly good, and actually if you do this a few times, you'll find that most of the times you get perfectly the sine wave, so it took some time.",
                    "label": 0
                },
                {
                    "sent": "For me to find a couple of examples where it just shut off to the wrong solutions, but it does illustrate the point that you get different answers depending on.",
                    "label": 0
                },
                {
                    "sent": "Where you start from?",
                    "label": 0
                },
                {
                    "sent": "So that's the only additional problem with these nonlinear estimators.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I think we'll forget that it, but I've had enough.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so thanks for your attention.",
                    "label": 0
                },
                {
                    "sent": "Anybody any questions you want to ask now?",
                    "label": 0
                },
                {
                    "sent": "So this afternoon will go into the lab upstairs top floor, and I mean it's probably best if people kind of pair up or triple up or whatever, because you know, we all learn from each other.",
                    "label": 0
                },
                {
                    "sent": "Probably better than we learn on our own.",
                    "label": 0
                },
                {
                    "sent": "We don't have to.",
                    "label": 0
                },
                {
                    "sent": "And will be Tony, myself and Andrew.",
                    "label": 0
                },
                {
                    "sent": "So sort of assist and then we've got some exercise we go through which will illustrate some of these points that have been raised this morning, OK?",
                    "label": 0
                },
                {
                    "sent": "Play.",
                    "label": 0
                }
            ]
        }
    }
}