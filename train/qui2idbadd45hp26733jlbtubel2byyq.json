{
    "id": "qui2idbadd45hp26733jlbtubel2byyq",
    "title": "Conjugate gradient iterative hard thresholding for compressed sensing and matrix completion",
    "info": {
        "author": [
            "Jared Tanner, Mathematical Institute, University of Oxford"
        ],
        "published": "Oct. 29, 2014",
        "recorded": "September 2014",
        "category": [
            "Top->Computer Science->Digital Signal Processing",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Information Theory",
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/sahd2014_tanner_compressed_sensing/",
    "segmentation": [
        [
            "Let me begin by thanking the organizers and in particular, Miguel, who I had a bunch of emails with for inviting me.",
            "You if you looked at the schedule some time ago, you would have seen that you were supposed to be hearing from Rich this morning, but Rich is being a bit late, so it will be a lot less comedy than usual, though maybe some unintentional comedy, so I'll be talking about a new algorithm for compressed sensing in matrix completion.",
            "I was looking at the program.",
            "It seems like about half of the audience are experts and know everything about compressed sensing.",
            "And then about half are maybe not core in the area, so I'm going to begin with a brief introduction to compress, sensing brief to set the stage will go through that relatively quickly, but please stop me if I'm going too fast and then we'll go on and we'll talk about this new algorithm and what I think it brings to the table.",
            "OK, so."
        ],
        [
            "Here's the general idea.",
            "This is what this community is doing.",
            "We're looking at the simplicity of large data, and we're going this side community.",
            "There we go and we are finding ways of more efficiently sensing and analyzing this data.",
            "OK, now in the compressed sensing POV, we're going to take some data like this an image, and we're going to use it is compressible.",
            "So here we have Usher Hall up in Edinburgh, and here we have the wavelet coefficients of Usher Hall, and as we see as we all know.",
            "It's highly compressible, so we have a low resolution course version of the image and then we have the wavelet coefficients at different scales in different orientations and we can see that most of the coefficients are small, so it's very compressible fine, so we know in compressed sensing that we're going to want to do that, we're going to want to go, and we're going to want to measure this.",
            "High Energy part in the low frequencies, and then we're going to want to do some kind of randomized mixed sampling of all these high frequency components that we know are there.",
            "We just don't know which parts of them were active.",
            "OK, you're going to hear more about compressed sensing in the following two talks by Anderzon Gita and then tomorrow by Joel, who snuck in at the end.",
            "He's over there.",
            "The other thing I'm going to be talking about this compressed this Sgt algorithm is how you can apply to matrix completion.",
            "So in matrix completion we have the same kind of idea.",
            "We're going to have some notion of simplicity.",
            "In that case, it's going to be low rank, so here's Usher Hall in the low rank approximation, you can see that it's not exactly the same.",
            "For instance, it looks like there's clouds here, but if you've been to Edinburgh, you know that we don't have clouds in Edinburgh, so clearly that's that's an artifact.",
            "OK, so compressed sensing."
        ],
        [
            "Let's just put."
        ],
        [
            "About some, some some variables and get things slightly technical, so we're going to try to find the compressed information so we have an image.",
            "We know that in some domain it's compressible and we're going to go and measure in this domain now.",
            "As I said before, if we know prior information, like most of the energy is in the low frequency, we can just go and measure that you'll hear about things like that from honors.",
            "On the other hand, there's a lot of information that we just don't really know where it is, and we're going to do something sort of random, so we'll do some kind of inner product between.",
            "The trash"
        ],
        [
            "From a sparse signal an some kind of random measurements are going to stack that in a matrix's rows in a matrix, and we're going to go from some sparse vector here.",
            "VX to some measurement vector Y, and the number of measurements is going to be far less than the ambient dimension of our of our vector that were started with.",
            "OK, fine.",
            "What's going on in matrix completion, well?"
        ],
        [
            "It isn't just that you're going from a vector to matrix.",
            "If you do that, and you're willing to say that the matrix is just sparse, then there's no difference.",
            "You can just vectorize everything and it's exactly compress, sensing where matrix completion is interesting is the idea that you know that there is this underlying notion of simplicity, but you don't know what representation that is in, so you have to learn the representation in which the data is simple.",
            "Those are going to be your left and right singular vectors, and building upon that.",
            "You're going to come up with the simple representation of the data.",
            "So here's the model.",
            "We have a matrix XM by N rank R. We're going.",
            "We're going to measure it in some linear way with some linear operator here.",
            "Calligraphy a, so you see calligraphy A. I'm talking about a matrix completion when it's regular a it's going to be compressed sensing, so we're going to make P measurements of our low rank matrix and we have a couple ways we could do this.",
            "So the way that inspires the name matrix completion is we can just go and read off P entries from X and then try to fill in the rest or completing the matrix.",
            "And more compressed sensing S kind of version of this is usually referred to as low rank matrix approximation, where you take matrices which are dense and you do matrix matrix product's against your target low rank matrix and those are your P measurements and then you want to try to recover.",
            "In either case the original matrix from your measurements OK.",
            "In the case of matrix completion, things are a little more complicated when you do this entry."
        ],
        [
            "Dancing where you directly measuring the entries, then when you're doing these more dense sensing and that's related to the spread and the democracy of the singular vectors.",
            "Alright, now both of these techniques compressed sensing matrix completion is very easy to just state what it is one is trying to do and that is I'm looking for a vector which is sparse in compressed sensing.",
            "That's what I'm leveraging and I wanted to fit my measurements with some tolerance fine in matrix completion I want something which is low rank.",
            "That's what I'm leveraging.",
            "The simplicity of low rank and I want to fit my measurements fine.",
            "OK so this is the normal way compressed sensing begins.",
            "But we can't solve these directly.",
            "By naively trying to look at all the support sets here or here, it's even more difficult.",
            "You could try to set up a representation in terms of the number of degrees of freedom and solve a coupled system of equations.",
            "Neither of those are going to be productive ways to go.",
            "There are a lot of other techniques.",
            "There are a lot of algorithms have been proposed which will try to get the solution to these problems in a way that's going to be efficient.",
            "So convex relaxation is one of the main ways you'll hear more about this."
        ],
        [
            "Anjo tomorrow, which is you take your problem and you replace the objective here with its convex regularizer.",
            "So L1 minimization for compressed sensing, fine or nuclear norm shattering, one norm minimization for nuclear, for for the low rank approximation fine.",
            "So these are standard ways we can go about solving it.",
            "I'm not going to want to talk about this too much, but I want to just set the stage with them and then we'll go on and talk about iterative hard thresholding algorithms.",
            "So in compressed sensing we have 3.",
            "Problem dimensions we're interested in.",
            "We have the ambient dimension, the length of the vector."
        ],
        [
            "Have the complexity of the signal, the number of nonzeros, and then we go when we make some measurements.",
            "OK it matrix completion.",
            "We have much the same thing.",
            "We have an ambient dimension which is the size of the matrix M by N. There's MN entries in the matrix.",
            "Its rank is R. You can think of that as its notion of the information content, though in truth the degrees of freedom is something a bit different.",
            "It's R * N plus and minus are so.",
            "You'll see this floating around instead of K in the case of matrix completion.",
            "And then we're going to make P measurements fine, and what we're going to want to know with these algorithms is how well we can.",
            "Leverage this information of the simplicity and still be able to recover the algorithm.",
            "So we want to make some number of measurements which is going to be less than the ambient dimension.",
            "So we're going under sample the data that's our intelligent sensing, but the algorithm is going to suffer from this in some way.",
            "We're not going to be able to exactly recover the number of degrees of freedom being the number of measurements, so we're going to have this ratio, which is the oversampling factor, is how poorly the algorithm is doing.",
            "It's how many more measurements we need then the number of degrees of freedom.",
            "So if you take the inverse of this, that tells you the number of multiples of the degrees of freedom the optimal rate you could have, you're going to have to measure OK, fine, and the remarkable thing in compressed sensing is that if you fix, sorry if you fix Delta, then in fact we know and we can prove, and in some cases we can prove quite precisely.",
            "What this loss is so we can find out a sampling theorem will tell us how many measurements we need compared to the number of freedom for a given savings rate that we're having.",
            "OK, so let's."
        ],
        [
            "Get two of those as a simple case.",
            "Ignore this bottom curve.",
            "What we have and I want to show you is a bunch of curves that are of this form.",
            "We have our under sampling rate here.",
            "How many measurements we make compared to the ambient dimension and then we have this ratio of how many measurements we may compare to the information content in compressed sensing and what we see is a separation of this domain.",
            "This unit square from where we can typically recover using that algorithm here.",
            "Then you're programming the measured vector and in this case where we typically cannot.",
            "Recover the measured vector using that algorithm.",
            "In matrix completion, we see the similar kind of thing this is."
        ],
        [
            "Recent results by Joel from 2013.",
            "Yeah, OK, where we have the same kind of thing?",
            "We have a region where you have recovery and a region where you typically don't have recovery.",
            "Now we have the number of measurements compared to the number of entries in the matrix and we have the number of measurements compared to the number of degrees of freedom of a low rank matrix and but we would like to understand is how well we can do at recovering these matrices.",
            "And in particular I'm interested in another set of algorithms so not convex relaxations, but there's a lot of algorithms that have also been proposed.",
            "Try to directly attack the nonconvex problem and the ones that intrigues me the most.",
            "Where these algorithms referred to as iterative hard thresholding algorithms.",
            "So what I was interested in understanding is what is going on with iterative hard thresholding algorithms, which seems to be very effective WHI.",
            "Are they behaving well and have these algorithms sort of been designed as well as possible, or is there still scope for improving them?",
            "And the answer to the later is I believe there.",
            "Was scope and I think we."
        ],
        [
            "Done that.",
            "So here is a particularly simple example of an iterative hard thresholding algorithm for compressed sensing.",
            "It's very simple if you're going to code up your first compressed sensing problem.",
            "This is a great algorithm to use.",
            "So in this case we're going to directly try to minimize the objective which is fitting our data.",
            "We're looking for a vector which fits our measurements, and we wanted to have K nonzeros.",
            "OK, now how we're going to solve this is going to be an alternating projection kind of approach.",
            "We're going to take a current estimate of current K sparse vector.",
            "We're going to look at the direction of steepest descent for this objective, and we're going to take a step in that direction by some step Kappa.",
            "That's going to give us a vector which will have decreased.",
            "This objective, but typically will no longer be sparse.",
            "The reason it won't be sparse is this.",
            "A transpose is going to interact with all of the residual, and it's going to inflate.",
            "It's going to look at all these coherent interactions between the columns of a.",
            "Now this vector, which is in case bars.",
            "We want to go back to it being K sparse, so we just threshold we project onto the nearest K sparse approximation.",
            "We threshold we keep the K largest entries.",
            "We set the rest of zero.",
            "Very simple algorithm.",
            "This is NHT by Blumas, often Davies.",
            "Very straightforward, works surprisingly well.",
            "Hard thresholding pursuit.",
            "This is another one to give you an idea of the sort of complexity of how one can build up these algorithms.",
            "So inside this soup we have exactly NIH T we take NIH T and once we get our new vector, which is case sparse, we say to ourselves, look if I think that's the right support set, then I should just take that support.",
            "I think those are the right locations for the non zeros and I should do a pseudoinverse and I'm done.",
            "OK, that's the motivation behind this algorithm.",
            "And you keep trying these new supports.",
            "You're moving along trying to figure out what's going on, where the largest coefficients are, and if you ever stumble across them, you have one step of the pseudoinverse and you're there.",
            "OK, so that's the idea behind hard thresholding pursuit.",
            "This is also known as.",
            "Iterative thresholding with inversion by Malachy.",
            "And then there are other algorithms.",
            "There's many other algorithms.",
            "There's two stage thresholding algorithms.",
            "This is in particular subspace pursuit from Diane Linkovich and Co. Samp by Nadal antrop.",
            "So here you have the same kind of idea.",
            "You have a step along the gradient, you threshold to some size.",
            "You look at this vector, you look at its support.",
            "You look at your last support.",
            "You think maybe I know where the nonzeros are.",
            "You join those you think if I know where the nonzeros or I can do a pseudoinverse.",
            "Do a pseudoinverse project back to the right size and you can imagine growing these in complexity.",
            "These algorithms now all of these algorithms are effective.",
            "You can run them, they worked well, they seem to behave very very nicely and they all have proofs of optimal order recovery, which is what I was showing you on those convex relaxation graphs, which is that if you fix Delta there is a row which is non zero and as you scale the problem size up the row for which you can get recovery does not go down.",
            "It stays fixed, asymptotically fixed.",
            "OK, that's great.",
            "So these algorithms are all optimal.",
            "We don't know the constants, but their optimal and would like to see how they behave.",
            "So let's see how they work on a typical problem rather than the worst case kind of analysis is where there's a big gap between the theory and practice, so."
        ],
        [
            "Here's the behavior of these three algorithms, so we have an IHT.",
            "We have HTTP, and we have to sample.",
            "We can see that three algorithms are a little bit different.",
            "This Co Samp subspace pursuit for the larger Delta is quite a bit better than the other two.",
            "There's a little gap here, and as we go down into the region where Delta small, where a number of measurements is quite small compared to the ambient dimension, the algorithms all behave more and more.",
            "Similarly, as we approach that.",
            "So you might wonder, then, well, if they're all beginning to look a lot alike.",
            "In the region where I'm most interested where I want to save on my measurements, and in particular in this region, all the algorithms are succeeding.",
            "I need to know how to choose them so you can choose the algorithm by choosing the one that's fastest.",
            "OK, so you can do many many tests we wanted to get an idea of how this was behaving, so we wrote some software on GPS and fired off hundreds of thousands of tests and we can find out for different values of Delta in row, which algorithm is fastest.",
            "So what we see is something quite clear.",
            "When Co Samp is the only algorithm that works, it's the winner.",
            "But as soon as NIHT begins to work.",
            "It's the fastest, and then there's a little sliver down here at the bottom where it goes back to hard thresholding, pursuit or Co. Samp, and in this case what's actually happening is this converging in one iteration, it gets the correct support set.",
            "It does a pseudoinverse, it's done, and you can't really do any better than that, and that is definitely going to be 10 HD, but you see clear layering between these different algorithms, OK?"
        ],
        [
            "Fine, so now we have three algorithms and we're trying to.",
            "We're rolling this understanding of what is in fact, working well here.",
            "What is the data?",
            "So this is synthetic problems and I'm solving them to moderate accuracy, so I'm solving them to a residual which is about 10 to the minus three on synthetic problems.",
            "I'm constructing AK sparse vector.",
            "I'm using a Gaussian matrix.",
            "I'm taking the worst case of in terms of the non zeros in the vector which is assigned vector and I'm seeing how it behaves on that.",
            "If you change this to being decaying, say Gaussian or decaying from a power law, you see the same thing, but the curves all move up a little if you change from DC from Gaussian matrix to a sparse matrix or DCT matrix, you see the same thing.",
            "With slight variations on the on the locations.",
            "Send this to your same behavior.",
            "We haven't tried an immense amount of real data.",
            "We've tried a few images here and there, but we haven't tried.",
            "This is hundreds of thousands of problems, but we see the same thing.",
            "Yes, of course, of course yeah, so in fact, we're trying to give HTP inco samples much possibility, as they can.",
            "So we're using a CG or using kind of gradient to do the pseudoinverse, and we've chosen the number of bits to try to keep the phase transition high, but not take too long, and that's its own little bit of art, but we tried to make him work as well as possible.",
            "I'll show you that in a moment.",
            "Yeah yeah there are issues related to noise stability.",
            "We'll come back to that.",
            "Not as much for these.",
            "More for other algorithms.",
            "Any other questions?",
            "OK, So what goes into those algorithms working well?"
        ],
        [
            "Well, NHT works very well for moderate accuracy for the very simple reason that in the beginning you don't know where the correct location of the non zeros are and you're trying to explore it.",
            "And when you're moving around in your exploring where those non zeros are, you don't want to spend a ton of time and effort computing the non zeros between the values of the nonzeros.",
            "'cause if you have the wrong support those values are probably not that useful.",
            "You're going to set some to zero and abandon them.",
            "You're going to set some new entry to be non zero or allowed to be non 0.",
            "And then you want to do some some linear algebra, like CG and that's going to take a bit of effort.",
            "In particular, our threshold in pursuit.",
            "What you're doing in Harthorn procedure saying, I think I have the right support set.",
            "I'm going to take my objective, which is the L2 difference between Y minus axis and on that support on that subspace.",
            "I'm gonna drive the error to 0.",
            "There's a local minima there an I want to find it and I'm going to drive myself to the bottom of that local minima.",
            "And if I'm wrong I'm going to try and get out of that local minima and surprise surprise, if you drive yourself to the bottom of the local minimums, but harder to get out.",
            "So that's not exactly ideal.",
            "And here there's just a fair bit going on, which tends to slow it down.",
            "OK, so here's an algorithm which tries."
        ],
        [
            "To balance these various features, we want an algorithm which is going to be looking at the support set and wondering the confidence that it has in the support set.",
            "If it doesn't have confidence in the support set, it shouldn't be doing something complicated.",
            "If it does have confidence, it should be start doing something more like hard thresholding pursuit, so this algorithm is very simple.",
            "It's going to go and it's going to look at your past support setting your current support set and if the supports that hasn't changed then it's going to compute an orthogonal isation for your search function and it's going to search direction.",
            "All a conjugate gradient, though not restricted to that support set, that's important if you do that, things go pretty badly wrong.",
            "If the supports that has changed, you're going to think, well, I really don't have confidence in this support set, and I'm not going to try to be complicated.",
            "I'm not going to imagine that my direction was particularly clever, and I'm just going to take a step along the gradient again, so you would set your beta B0 in your search direction.",
            "Here would be just the residual.",
            "You take a step size in the same way you do for NHT, and then you move forward by adding that stepsize, finding where the support is an thresholding, projecting onto that support set.",
            "OK, so it's a very simple algorithm, is sort of part way between NIH TNC GHT.",
            "So."
        ],
        [
            "So let's try it out.",
            "We try it out.",
            "We try to use many algorithms so.",
            "The only important thing on this part is that the you know it's slightly better than the other algorithms were, and there are some other algorithms here that I'm not mentioning.",
            "It is a little bit better, but no.",
            "OK, that's not very interesting, but what about the timing?"
        ],
        [
            "So in terms of recovery abilities about the same, so a little bit better, but in terms of timing, it turns out it essentially beats everything except again there's a region where Cosette has a slight advantage up here where it works and nothing else does, and then it wins.",
            "And then there's a region down here where there's this mysterious algorithm that I haven't explained called FHT, which wins, but we get a clear layering between one algorithm working and then another algorithm working."
        ],
        [
            "Now this algorithm FHT.",
            "This is a variant of the Matrix Alps algorithms by Vulcan Sub here, which is taking the nestro optimal gradient method and applying it to the nonconvex problem.",
            "Unfortunately, that's known not to be stable to noise.",
            "So if you add a little bit of noise, it turns out it's gone, so you end up with the CG being the only algorithm of those that is in fact the fastest with a few small exceptions where every once in awhile an IHT or hard thresholding pursuit wins.",
            "These are average times over many, many tests.",
            "It gives you a good sense as to what's going on, and you'll see the same kind of behavior for other ensembles and vectors and yadda yadda yadda yadda.",
            "OK, what kind of theory can we have for this?"
        ],
        [
            "Unfortunately, it's quite sad it's not very interesting theory.",
            "It's just like the theory we have for all other iterative hard thresholding algorithms, and that is if you take a big enough hammer, the researcher isometry constant and you bang on it, you can prove that it works and you can prove that if something implausible is true, then in fact the algorithm will converge and you get optimal order, which is somewhat satisfying, but.",
            "OK.",
            "So how about for matrix completion, where we can take the same idea as most?",
            "Any good idea you have in compressed sensing, you can put it over to matrix completion.",
            "So for matrix completion we're going to have a setup which is take the ad joint later applied to the measurements to get initial guess.",
            "Compute the singular vectors.",
            "Let's just serebii, the left singular vectors.",
            "All of that approximation computed.",
            "Initial guess you're ready to go.",
            "And then let's begin the algorithm.",
            "So the algorithm is slightly different.",
            "And here's where it's slightly different.",
            "So in the compressed sensing case.",
            "We said I think I have the right support set, but maybe I'm wrong.",
            "In matrix completion you don't have this.",
            "You are never going to get the correct singular space.",
            "Exactly, that doesn't happen.",
            "You approach it slowly, so you have to have a continuous analogue to this kind of notion of how close you are.",
            "Your confidence level to that subspace.",
            "So in this case."
        ],
        [
            "What we do is we look at the search direction we had before we project it onto the subspace that we were looking at in our prior iterate and we see if that search space is well aligned with the residual.",
            "So if the residual.",
            "Is well aligned with the current belief of the subspace.",
            "Then sorry is not well aligned.",
            "Then we're going to go.",
            "We take our step as if we don't have confidence in the support set or in the subspace.",
            "On the other hand, if we do have sufficient confidence, we are going to in fact take a step which is going to have the search direction be only only on the current subspace.",
            "So you're going to restrict to just that subspace in work just on that subspace.",
            "So think of it.",
            "As simply a nonlinear version of CG applied based upon our restarting condition, telling you about how much your current search direction does or does not align with the residual.",
            "OK.",
            "So this works quite."
        ],
        [
            "Well, so I showed you this curve of recovery ability for the shadden one norm nuclear normalization which is here.",
            "Here are the recovery levels for the matrix completion version of NHT SGH T and that mysterious FHT and you can see that actually these have a much higher phase transition.",
            "I'll explain a little bit more quantitative write down here what's going on.",
            "It's also typically much faster.",
            "We have not been able to come up with a fast enough version of this to give anything that isn't silly, so we're not showing timings."
        ],
        [
            "But if you want to get a sense as to what's going on, sorry, let's go back one slide.",
            "Let's take a look at what's happening here, but for the case of Delta being very small, let's take Delta to be .051, / 20 down here, and let's see what's happening for CGI HT, which seems to be working spectacularly well.",
            "In fact, it looks like here the number of measurements is just very much very slightly more than one times the number of degrees of freedom the universe of this, about 1.1 times the number of degrees of freedom.",
            "Well, at the case where we have the number of measurements be a 20th of the number of entries, we've run 100 tests for each rank.",
            "This is a for 2000 by 2000 matrix and record the success and we record the asymptotic convergence rate and what we see taking a vertical slice on those prior slots prior plots, we see that the convergence rate gets worse and worse and worse.",
            "These are plus or minus one.",
            "Standard deviation gets worse and worse and worse as you approach the phase transition and then at some point.",
            "It fails well.",
            "Why is it failing?",
            "Well, it turns out the reason it fails is because we stop running the code because this converging so slowly that we just can't wait any longer.",
            "That seems to be the only reason it stops.",
            "In fact, for Ro the number of degrees of freedom over the number of measurements being less than .9, we get recovery in at least a 99 out of 100.",
            "Test for every rank all the way down from one up to the maximum recoverable for this row, whereas for the shadden one norm.",
            "We know that if you don't have rolled less than .41 from the theory that Joel will present tomorrow, you will typically not have successful recovery.",
            "In the limit, it seems like for the shadden one norm, this is in fact a third, and we don't seem to know whether this has a drop off for these iterative hard thresholding algorithms, so it seems like there's actually quite a bit more going on in the matrix completion setting.",
            "OK, so that's enough so.",
            "What are some?"
        ],
        [
            "Takeaways so some takeaways are when designing these compressed sensing matrix completion algorithms.",
            "What we need to be thinking about is what the algorithm is doing so it has an exploration phase where it really doesn't know what's going on.",
            "Is trying to figure out what is the dominant subspace where the non zero values are.",
            "This kind of thing and in that case it needs to be cheap, fast, quick, not sophisticated and then it needs to have a way of realizing that as it's going along it is gaining confidence and then to start doing more and more sophisticated things the less support is changing.",
            "Closer you are to the subspace, the number of iterates that you're doing.",
            "Something like kanji gradient on should be going up, and that is in fact exactly what's happening here, and the higher order method of using constant gradient in the appropriate way can both accelerate convergence, and in fact increase the phase transition region right?",
            "That's it, thank you."
        ],
        [
            "Very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me begin by thanking the organizers and in particular, Miguel, who I had a bunch of emails with for inviting me.",
                    "label": 0
                },
                {
                    "sent": "You if you looked at the schedule some time ago, you would have seen that you were supposed to be hearing from Rich this morning, but Rich is being a bit late, so it will be a lot less comedy than usual, though maybe some unintentional comedy, so I'll be talking about a new algorithm for compressed sensing in matrix completion.",
                    "label": 1
                },
                {
                    "sent": "I was looking at the program.",
                    "label": 1
                },
                {
                    "sent": "It seems like about half of the audience are experts and know everything about compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "And then about half are maybe not core in the area, so I'm going to begin with a brief introduction to compress, sensing brief to set the stage will go through that relatively quickly, but please stop me if I'm going too fast and then we'll go on and we'll talk about this new algorithm and what I think it brings to the table.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the general idea.",
                    "label": 0
                },
                {
                    "sent": "This is what this community is doing.",
                    "label": 1
                },
                {
                    "sent": "We're looking at the simplicity of large data, and we're going this side community.",
                    "label": 1
                },
                {
                    "sent": "There we go and we are finding ways of more efficiently sensing and analyzing this data.",
                    "label": 0
                },
                {
                    "sent": "OK, now in the compressed sensing POV, we're going to take some data like this an image, and we're going to use it is compressible.",
                    "label": 0
                },
                {
                    "sent": "So here we have Usher Hall up in Edinburgh, and here we have the wavelet coefficients of Usher Hall, and as we see as we all know.",
                    "label": 0
                },
                {
                    "sent": "It's highly compressible, so we have a low resolution course version of the image and then we have the wavelet coefficients at different scales in different orientations and we can see that most of the coefficients are small, so it's very compressible fine, so we know in compressed sensing that we're going to want to do that, we're going to want to go, and we're going to want to measure this.",
                    "label": 0
                },
                {
                    "sent": "High Energy part in the low frequencies, and then we're going to want to do some kind of randomized mixed sampling of all these high frequency components that we know are there.",
                    "label": 0
                },
                {
                    "sent": "We just don't know which parts of them were active.",
                    "label": 0
                },
                {
                    "sent": "OK, you're going to hear more about compressed sensing in the following two talks by Anderzon Gita and then tomorrow by Joel, who snuck in at the end.",
                    "label": 1
                },
                {
                    "sent": "He's over there.",
                    "label": 0
                },
                {
                    "sent": "The other thing I'm going to be talking about this compressed this Sgt algorithm is how you can apply to matrix completion.",
                    "label": 0
                },
                {
                    "sent": "So in matrix completion we have the same kind of idea.",
                    "label": 0
                },
                {
                    "sent": "We're going to have some notion of simplicity.",
                    "label": 0
                },
                {
                    "sent": "In that case, it's going to be low rank, so here's Usher Hall in the low rank approximation, you can see that it's not exactly the same.",
                    "label": 0
                },
                {
                    "sent": "For instance, it looks like there's clouds here, but if you've been to Edinburgh, you know that we don't have clouds in Edinburgh, so clearly that's that's an artifact.",
                    "label": 0
                },
                {
                    "sent": "OK, so compressed sensing.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's just put.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About some, some some variables and get things slightly technical, so we're going to try to find the compressed information so we have an image.",
                    "label": 0
                },
                {
                    "sent": "We know that in some domain it's compressible and we're going to go and measure in this domain now.",
                    "label": 0
                },
                {
                    "sent": "As I said before, if we know prior information, like most of the energy is in the low frequency, we can just go and measure that you'll hear about things like that from honors.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, there's a lot of information that we just don't really know where it is, and we're going to do something sort of random, so we'll do some kind of inner product between.",
                    "label": 0
                },
                {
                    "sent": "The trash",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From a sparse signal an some kind of random measurements are going to stack that in a matrix's rows in a matrix, and we're going to go from some sparse vector here.",
                    "label": 0
                },
                {
                    "sent": "VX to some measurement vector Y, and the number of measurements is going to be far less than the ambient dimension of our of our vector that were started with.",
                    "label": 0
                },
                {
                    "sent": "OK, fine.",
                    "label": 0
                },
                {
                    "sent": "What's going on in matrix completion, well?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It isn't just that you're going from a vector to matrix.",
                    "label": 0
                },
                {
                    "sent": "If you do that, and you're willing to say that the matrix is just sparse, then there's no difference.",
                    "label": 0
                },
                {
                    "sent": "You can just vectorize everything and it's exactly compress, sensing where matrix completion is interesting is the idea that you know that there is this underlying notion of simplicity, but you don't know what representation that is in, so you have to learn the representation in which the data is simple.",
                    "label": 0
                },
                {
                    "sent": "Those are going to be your left and right singular vectors, and building upon that.",
                    "label": 0
                },
                {
                    "sent": "You're going to come up with the simple representation of the data.",
                    "label": 0
                },
                {
                    "sent": "So here's the model.",
                    "label": 0
                },
                {
                    "sent": "We have a matrix XM by N rank R. We're going.",
                    "label": 0
                },
                {
                    "sent": "We're going to measure it in some linear way with some linear operator here.",
                    "label": 0
                },
                {
                    "sent": "Calligraphy a, so you see calligraphy A. I'm talking about a matrix completion when it's regular a it's going to be compressed sensing, so we're going to make P measurements of our low rank matrix and we have a couple ways we could do this.",
                    "label": 1
                },
                {
                    "sent": "So the way that inspires the name matrix completion is we can just go and read off P entries from X and then try to fill in the rest or completing the matrix.",
                    "label": 0
                },
                {
                    "sent": "And more compressed sensing S kind of version of this is usually referred to as low rank matrix approximation, where you take matrices which are dense and you do matrix matrix product's against your target low rank matrix and those are your P measurements and then you want to try to recover.",
                    "label": 0
                },
                {
                    "sent": "In either case the original matrix from your measurements OK.",
                    "label": 1
                },
                {
                    "sent": "In the case of matrix completion, things are a little more complicated when you do this entry.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dancing where you directly measuring the entries, then when you're doing these more dense sensing and that's related to the spread and the democracy of the singular vectors.",
                    "label": 0
                },
                {
                    "sent": "Alright, now both of these techniques compressed sensing matrix completion is very easy to just state what it is one is trying to do and that is I'm looking for a vector which is sparse in compressed sensing.",
                    "label": 1
                },
                {
                    "sent": "That's what I'm leveraging and I wanted to fit my measurements with some tolerance fine in matrix completion I want something which is low rank.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm leveraging.",
                    "label": 1
                },
                {
                    "sent": "The simplicity of low rank and I want to fit my measurements fine.",
                    "label": 0
                },
                {
                    "sent": "OK so this is the normal way compressed sensing begins.",
                    "label": 0
                },
                {
                    "sent": "But we can't solve these directly.",
                    "label": 0
                },
                {
                    "sent": "By naively trying to look at all the support sets here or here, it's even more difficult.",
                    "label": 0
                },
                {
                    "sent": "You could try to set up a representation in terms of the number of degrees of freedom and solve a coupled system of equations.",
                    "label": 0
                },
                {
                    "sent": "Neither of those are going to be productive ways to go.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of other techniques.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of algorithms have been proposed which will try to get the solution to these problems in a way that's going to be efficient.",
                    "label": 0
                },
                {
                    "sent": "So convex relaxation is one of the main ways you'll hear more about this.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anjo tomorrow, which is you take your problem and you replace the objective here with its convex regularizer.",
                    "label": 0
                },
                {
                    "sent": "So L1 minimization for compressed sensing, fine or nuclear norm shattering, one norm minimization for nuclear, for for the low rank approximation fine.",
                    "label": 0
                },
                {
                    "sent": "So these are standard ways we can go about solving it.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to want to talk about this too much, but I want to just set the stage with them and then we'll go on and talk about iterative hard thresholding algorithms.",
                    "label": 0
                },
                {
                    "sent": "So in compressed sensing we have 3.",
                    "label": 0
                },
                {
                    "sent": "Problem dimensions we're interested in.",
                    "label": 0
                },
                {
                    "sent": "We have the ambient dimension, the length of the vector.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have the complexity of the signal, the number of nonzeros, and then we go when we make some measurements.",
                    "label": 0
                },
                {
                    "sent": "OK it matrix completion.",
                    "label": 0
                },
                {
                    "sent": "We have much the same thing.",
                    "label": 0
                },
                {
                    "sent": "We have an ambient dimension which is the size of the matrix M by N. There's MN entries in the matrix.",
                    "label": 0
                },
                {
                    "sent": "Its rank is R. You can think of that as its notion of the information content, though in truth the degrees of freedom is something a bit different.",
                    "label": 0
                },
                {
                    "sent": "It's R * N plus and minus are so.",
                    "label": 0
                },
                {
                    "sent": "You'll see this floating around instead of K in the case of matrix completion.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to make P measurements fine, and what we're going to want to know with these algorithms is how well we can.",
                    "label": 0
                },
                {
                    "sent": "Leverage this information of the simplicity and still be able to recover the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we want to make some number of measurements which is going to be less than the ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "So we're going under sample the data that's our intelligent sensing, but the algorithm is going to suffer from this in some way.",
                    "label": 0
                },
                {
                    "sent": "We're not going to be able to exactly recover the number of degrees of freedom being the number of measurements, so we're going to have this ratio, which is the oversampling factor, is how poorly the algorithm is doing.",
                    "label": 0
                },
                {
                    "sent": "It's how many more measurements we need then the number of degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "So if you take the inverse of this, that tells you the number of multiples of the degrees of freedom the optimal rate you could have, you're going to have to measure OK, fine, and the remarkable thing in compressed sensing is that if you fix, sorry if you fix Delta, then in fact we know and we can prove, and in some cases we can prove quite precisely.",
                    "label": 0
                },
                {
                    "sent": "What this loss is so we can find out a sampling theorem will tell us how many measurements we need compared to the number of freedom for a given savings rate that we're having.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get two of those as a simple case.",
                    "label": 0
                },
                {
                    "sent": "Ignore this bottom curve.",
                    "label": 0
                },
                {
                    "sent": "What we have and I want to show you is a bunch of curves that are of this form.",
                    "label": 0
                },
                {
                    "sent": "We have our under sampling rate here.",
                    "label": 0
                },
                {
                    "sent": "How many measurements we make compared to the ambient dimension and then we have this ratio of how many measurements we may compare to the information content in compressed sensing and what we see is a separation of this domain.",
                    "label": 1
                },
                {
                    "sent": "This unit square from where we can typically recover using that algorithm here.",
                    "label": 0
                },
                {
                    "sent": "Then you're programming the measured vector and in this case where we typically cannot.",
                    "label": 0
                },
                {
                    "sent": "Recover the measured vector using that algorithm.",
                    "label": 1
                },
                {
                    "sent": "In matrix completion, we see the similar kind of thing this is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recent results by Joel from 2013.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, where we have the same kind of thing?",
                    "label": 0
                },
                {
                    "sent": "We have a region where you have recovery and a region where you typically don't have recovery.",
                    "label": 0
                },
                {
                    "sent": "Now we have the number of measurements compared to the number of entries in the matrix and we have the number of measurements compared to the number of degrees of freedom of a low rank matrix and but we would like to understand is how well we can do at recovering these matrices.",
                    "label": 0
                },
                {
                    "sent": "And in particular I'm interested in another set of algorithms so not convex relaxations, but there's a lot of algorithms that have also been proposed.",
                    "label": 0
                },
                {
                    "sent": "Try to directly attack the nonconvex problem and the ones that intrigues me the most.",
                    "label": 0
                },
                {
                    "sent": "Where these algorithms referred to as iterative hard thresholding algorithms.",
                    "label": 0
                },
                {
                    "sent": "So what I was interested in understanding is what is going on with iterative hard thresholding algorithms, which seems to be very effective WHI.",
                    "label": 0
                },
                {
                    "sent": "Are they behaving well and have these algorithms sort of been designed as well as possible, or is there still scope for improving them?",
                    "label": 0
                },
                {
                    "sent": "And the answer to the later is I believe there.",
                    "label": 0
                },
                {
                    "sent": "Was scope and I think we.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Done that.",
                    "label": 0
                },
                {
                    "sent": "So here is a particularly simple example of an iterative hard thresholding algorithm for compressed sensing.",
                    "label": 1
                },
                {
                    "sent": "It's very simple if you're going to code up your first compressed sensing problem.",
                    "label": 0
                },
                {
                    "sent": "This is a great algorithm to use.",
                    "label": 0
                },
                {
                    "sent": "So in this case we're going to directly try to minimize the objective which is fitting our data.",
                    "label": 0
                },
                {
                    "sent": "We're looking for a vector which fits our measurements, and we wanted to have K nonzeros.",
                    "label": 0
                },
                {
                    "sent": "OK, now how we're going to solve this is going to be an alternating projection kind of approach.",
                    "label": 0
                },
                {
                    "sent": "We're going to take a current estimate of current K sparse vector.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at the direction of steepest descent for this objective, and we're going to take a step in that direction by some step Kappa.",
                    "label": 0
                },
                {
                    "sent": "That's going to give us a vector which will have decreased.",
                    "label": 0
                },
                {
                    "sent": "This objective, but typically will no longer be sparse.",
                    "label": 0
                },
                {
                    "sent": "The reason it won't be sparse is this.",
                    "label": 0
                },
                {
                    "sent": "A transpose is going to interact with all of the residual, and it's going to inflate.",
                    "label": 0
                },
                {
                    "sent": "It's going to look at all these coherent interactions between the columns of a.",
                    "label": 0
                },
                {
                    "sent": "Now this vector, which is in case bars.",
                    "label": 0
                },
                {
                    "sent": "We want to go back to it being K sparse, so we just threshold we project onto the nearest K sparse approximation.",
                    "label": 0
                },
                {
                    "sent": "We threshold we keep the K largest entries.",
                    "label": 0
                },
                {
                    "sent": "We set the rest of zero.",
                    "label": 0
                },
                {
                    "sent": "Very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is NHT by Blumas, often Davies.",
                    "label": 0
                },
                {
                    "sent": "Very straightforward, works surprisingly well.",
                    "label": 0
                },
                {
                    "sent": "Hard thresholding pursuit.",
                    "label": 0
                },
                {
                    "sent": "This is another one to give you an idea of the sort of complexity of how one can build up these algorithms.",
                    "label": 0
                },
                {
                    "sent": "So inside this soup we have exactly NIH T we take NIH T and once we get our new vector, which is case sparse, we say to ourselves, look if I think that's the right support set, then I should just take that support.",
                    "label": 0
                },
                {
                    "sent": "I think those are the right locations for the non zeros and I should do a pseudoinverse and I'm done.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the motivation behind this algorithm.",
                    "label": 0
                },
                {
                    "sent": "And you keep trying these new supports.",
                    "label": 0
                },
                {
                    "sent": "You're moving along trying to figure out what's going on, where the largest coefficients are, and if you ever stumble across them, you have one step of the pseudoinverse and you're there.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the idea behind hard thresholding pursuit.",
                    "label": 0
                },
                {
                    "sent": "This is also known as.",
                    "label": 0
                },
                {
                    "sent": "Iterative thresholding with inversion by Malachy.",
                    "label": 1
                },
                {
                    "sent": "And then there are other algorithms.",
                    "label": 0
                },
                {
                    "sent": "There's many other algorithms.",
                    "label": 0
                },
                {
                    "sent": "There's two stage thresholding algorithms.",
                    "label": 0
                },
                {
                    "sent": "This is in particular subspace pursuit from Diane Linkovich and Co. Samp by Nadal antrop.",
                    "label": 0
                },
                {
                    "sent": "So here you have the same kind of idea.",
                    "label": 0
                },
                {
                    "sent": "You have a step along the gradient, you threshold to some size.",
                    "label": 0
                },
                {
                    "sent": "You look at this vector, you look at its support.",
                    "label": 0
                },
                {
                    "sent": "You look at your last support.",
                    "label": 0
                },
                {
                    "sent": "You think maybe I know where the nonzeros are.",
                    "label": 0
                },
                {
                    "sent": "You join those you think if I know where the nonzeros or I can do a pseudoinverse.",
                    "label": 0
                },
                {
                    "sent": "Do a pseudoinverse project back to the right size and you can imagine growing these in complexity.",
                    "label": 0
                },
                {
                    "sent": "These algorithms now all of these algorithms are effective.",
                    "label": 0
                },
                {
                    "sent": "You can run them, they worked well, they seem to behave very very nicely and they all have proofs of optimal order recovery, which is what I was showing you on those convex relaxation graphs, which is that if you fix Delta there is a row which is non zero and as you scale the problem size up the row for which you can get recovery does not go down.",
                    "label": 0
                },
                {
                    "sent": "It stays fixed, asymptotically fixed.",
                    "label": 0
                },
                {
                    "sent": "OK, that's great.",
                    "label": 0
                },
                {
                    "sent": "So these algorithms are all optimal.",
                    "label": 0
                },
                {
                    "sent": "We don't know the constants, but their optimal and would like to see how they behave.",
                    "label": 0
                },
                {
                    "sent": "So let's see how they work on a typical problem rather than the worst case kind of analysis is where there's a big gap between the theory and practice, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's the behavior of these three algorithms, so we have an IHT.",
                    "label": 0
                },
                {
                    "sent": "We have HTTP, and we have to sample.",
                    "label": 0
                },
                {
                    "sent": "We can see that three algorithms are a little bit different.",
                    "label": 0
                },
                {
                    "sent": "This Co Samp subspace pursuit for the larger Delta is quite a bit better than the other two.",
                    "label": 0
                },
                {
                    "sent": "There's a little gap here, and as we go down into the region where Delta small, where a number of measurements is quite small compared to the ambient dimension, the algorithms all behave more and more.",
                    "label": 0
                },
                {
                    "sent": "Similarly, as we approach that.",
                    "label": 0
                },
                {
                    "sent": "So you might wonder, then, well, if they're all beginning to look a lot alike.",
                    "label": 0
                },
                {
                    "sent": "In the region where I'm most interested where I want to save on my measurements, and in particular in this region, all the algorithms are succeeding.",
                    "label": 0
                },
                {
                    "sent": "I need to know how to choose them so you can choose the algorithm by choosing the one that's fastest.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can do many many tests we wanted to get an idea of how this was behaving, so we wrote some software on GPS and fired off hundreds of thousands of tests and we can find out for different values of Delta in row, which algorithm is fastest.",
                    "label": 0
                },
                {
                    "sent": "So what we see is something quite clear.",
                    "label": 0
                },
                {
                    "sent": "When Co Samp is the only algorithm that works, it's the winner.",
                    "label": 0
                },
                {
                    "sent": "But as soon as NIHT begins to work.",
                    "label": 0
                },
                {
                    "sent": "It's the fastest, and then there's a little sliver down here at the bottom where it goes back to hard thresholding, pursuit or Co. Samp, and in this case what's actually happening is this converging in one iteration, it gets the correct support set.",
                    "label": 0
                },
                {
                    "sent": "It does a pseudoinverse, it's done, and you can't really do any better than that, and that is definitely going to be 10 HD, but you see clear layering between these different algorithms, OK?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fine, so now we have three algorithms and we're trying to.",
                    "label": 0
                },
                {
                    "sent": "We're rolling this understanding of what is in fact, working well here.",
                    "label": 0
                },
                {
                    "sent": "What is the data?",
                    "label": 0
                },
                {
                    "sent": "So this is synthetic problems and I'm solving them to moderate accuracy, so I'm solving them to a residual which is about 10 to the minus three on synthetic problems.",
                    "label": 0
                },
                {
                    "sent": "I'm constructing AK sparse vector.",
                    "label": 0
                },
                {
                    "sent": "I'm using a Gaussian matrix.",
                    "label": 0
                },
                {
                    "sent": "I'm taking the worst case of in terms of the non zeros in the vector which is assigned vector and I'm seeing how it behaves on that.",
                    "label": 0
                },
                {
                    "sent": "If you change this to being decaying, say Gaussian or decaying from a power law, you see the same thing, but the curves all move up a little if you change from DC from Gaussian matrix to a sparse matrix or DCT matrix, you see the same thing.",
                    "label": 0
                },
                {
                    "sent": "With slight variations on the on the locations.",
                    "label": 0
                },
                {
                    "sent": "Send this to your same behavior.",
                    "label": 0
                },
                {
                    "sent": "We haven't tried an immense amount of real data.",
                    "label": 0
                },
                {
                    "sent": "We've tried a few images here and there, but we haven't tried.",
                    "label": 0
                },
                {
                    "sent": "This is hundreds of thousands of problems, but we see the same thing.",
                    "label": 0
                },
                {
                    "sent": "Yes, of course, of course yeah, so in fact, we're trying to give HTP inco samples much possibility, as they can.",
                    "label": 0
                },
                {
                    "sent": "So we're using a CG or using kind of gradient to do the pseudoinverse, and we've chosen the number of bits to try to keep the phase transition high, but not take too long, and that's its own little bit of art, but we tried to make him work as well as possible.",
                    "label": 0
                },
                {
                    "sent": "I'll show you that in a moment.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah there are issues related to noise stability.",
                    "label": 0
                },
                {
                    "sent": "We'll come back to that.",
                    "label": 0
                },
                {
                    "sent": "Not as much for these.",
                    "label": 0
                },
                {
                    "sent": "More for other algorithms.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, So what goes into those algorithms working well?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, NHT works very well for moderate accuracy for the very simple reason that in the beginning you don't know where the correct location of the non zeros are and you're trying to explore it.",
                    "label": 0
                },
                {
                    "sent": "And when you're moving around in your exploring where those non zeros are, you don't want to spend a ton of time and effort computing the non zeros between the values of the nonzeros.",
                    "label": 0
                },
                {
                    "sent": "'cause if you have the wrong support those values are probably not that useful.",
                    "label": 0
                },
                {
                    "sent": "You're going to set some to zero and abandon them.",
                    "label": 0
                },
                {
                    "sent": "You're going to set some new entry to be non zero or allowed to be non 0.",
                    "label": 0
                },
                {
                    "sent": "And then you want to do some some linear algebra, like CG and that's going to take a bit of effort.",
                    "label": 0
                },
                {
                    "sent": "In particular, our threshold in pursuit.",
                    "label": 0
                },
                {
                    "sent": "What you're doing in Harthorn procedure saying, I think I have the right support set.",
                    "label": 0
                },
                {
                    "sent": "I'm going to take my objective, which is the L2 difference between Y minus axis and on that support on that subspace.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna drive the error to 0.",
                    "label": 0
                },
                {
                    "sent": "There's a local minima there an I want to find it and I'm going to drive myself to the bottom of that local minima.",
                    "label": 0
                },
                {
                    "sent": "And if I'm wrong I'm going to try and get out of that local minima and surprise surprise, if you drive yourself to the bottom of the local minimums, but harder to get out.",
                    "label": 0
                },
                {
                    "sent": "So that's not exactly ideal.",
                    "label": 0
                },
                {
                    "sent": "And here there's just a fair bit going on, which tends to slow it down.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's an algorithm which tries.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To balance these various features, we want an algorithm which is going to be looking at the support set and wondering the confidence that it has in the support set.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't have confidence in the support set, it shouldn't be doing something complicated.",
                    "label": 0
                },
                {
                    "sent": "If it does have confidence, it should be start doing something more like hard thresholding pursuit, so this algorithm is very simple.",
                    "label": 0
                },
                {
                    "sent": "It's going to go and it's going to look at your past support setting your current support set and if the supports that hasn't changed then it's going to compute an orthogonal isation for your search function and it's going to search direction.",
                    "label": 0
                },
                {
                    "sent": "All a conjugate gradient, though not restricted to that support set, that's important if you do that, things go pretty badly wrong.",
                    "label": 0
                },
                {
                    "sent": "If the supports that has changed, you're going to think, well, I really don't have confidence in this support set, and I'm not going to try to be complicated.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to imagine that my direction was particularly clever, and I'm just going to take a step along the gradient again, so you would set your beta B0 in your search direction.",
                    "label": 0
                },
                {
                    "sent": "Here would be just the residual.",
                    "label": 0
                },
                {
                    "sent": "You take a step size in the same way you do for NHT, and then you move forward by adding that stepsize, finding where the support is an thresholding, projecting onto that support set.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a very simple algorithm, is sort of part way between NIH TNC GHT.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's try it out.",
                    "label": 0
                },
                {
                    "sent": "We try it out.",
                    "label": 0
                },
                {
                    "sent": "We try to use many algorithms so.",
                    "label": 0
                },
                {
                    "sent": "The only important thing on this part is that the you know it's slightly better than the other algorithms were, and there are some other algorithms here that I'm not mentioning.",
                    "label": 0
                },
                {
                    "sent": "It is a little bit better, but no.",
                    "label": 0
                },
                {
                    "sent": "OK, that's not very interesting, but what about the timing?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in terms of recovery abilities about the same, so a little bit better, but in terms of timing, it turns out it essentially beats everything except again there's a region where Cosette has a slight advantage up here where it works and nothing else does, and then it wins.",
                    "label": 0
                },
                {
                    "sent": "And then there's a region down here where there's this mysterious algorithm that I haven't explained called FHT, which wins, but we get a clear layering between one algorithm working and then another algorithm working.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this algorithm FHT.",
                    "label": 0
                },
                {
                    "sent": "This is a variant of the Matrix Alps algorithms by Vulcan Sub here, which is taking the nestro optimal gradient method and applying it to the nonconvex problem.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, that's known not to be stable to noise.",
                    "label": 0
                },
                {
                    "sent": "So if you add a little bit of noise, it turns out it's gone, so you end up with the CG being the only algorithm of those that is in fact the fastest with a few small exceptions where every once in awhile an IHT or hard thresholding pursuit wins.",
                    "label": 0
                },
                {
                    "sent": "These are average times over many, many tests.",
                    "label": 0
                },
                {
                    "sent": "It gives you a good sense as to what's going on, and you'll see the same kind of behavior for other ensembles and vectors and yadda yadda yadda yadda.",
                    "label": 0
                },
                {
                    "sent": "OK, what kind of theory can we have for this?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Unfortunately, it's quite sad it's not very interesting theory.",
                    "label": 0
                },
                {
                    "sent": "It's just like the theory we have for all other iterative hard thresholding algorithms, and that is if you take a big enough hammer, the researcher isometry constant and you bang on it, you can prove that it works and you can prove that if something implausible is true, then in fact the algorithm will converge and you get optimal order, which is somewhat satisfying, but.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So how about for matrix completion, where we can take the same idea as most?",
                    "label": 0
                },
                {
                    "sent": "Any good idea you have in compressed sensing, you can put it over to matrix completion.",
                    "label": 1
                },
                {
                    "sent": "So for matrix completion we're going to have a setup which is take the ad joint later applied to the measurements to get initial guess.",
                    "label": 0
                },
                {
                    "sent": "Compute the singular vectors.",
                    "label": 0
                },
                {
                    "sent": "Let's just serebii, the left singular vectors.",
                    "label": 0
                },
                {
                    "sent": "All of that approximation computed.",
                    "label": 0
                },
                {
                    "sent": "Initial guess you're ready to go.",
                    "label": 0
                },
                {
                    "sent": "And then let's begin the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm is slightly different.",
                    "label": 0
                },
                {
                    "sent": "And here's where it's slightly different.",
                    "label": 1
                },
                {
                    "sent": "So in the compressed sensing case.",
                    "label": 1
                },
                {
                    "sent": "We said I think I have the right support set, but maybe I'm wrong.",
                    "label": 0
                },
                {
                    "sent": "In matrix completion you don't have this.",
                    "label": 0
                },
                {
                    "sent": "You are never going to get the correct singular space.",
                    "label": 0
                },
                {
                    "sent": "Exactly, that doesn't happen.",
                    "label": 0
                },
                {
                    "sent": "You approach it slowly, so you have to have a continuous analogue to this kind of notion of how close you are.",
                    "label": 0
                },
                {
                    "sent": "Your confidence level to that subspace.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we do is we look at the search direction we had before we project it onto the subspace that we were looking at in our prior iterate and we see if that search space is well aligned with the residual.",
                    "label": 0
                },
                {
                    "sent": "So if the residual.",
                    "label": 0
                },
                {
                    "sent": "Is well aligned with the current belief of the subspace.",
                    "label": 0
                },
                {
                    "sent": "Then sorry is not well aligned.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to go.",
                    "label": 0
                },
                {
                    "sent": "We take our step as if we don't have confidence in the support set or in the subspace.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if we do have sufficient confidence, we are going to in fact take a step which is going to have the search direction be only only on the current subspace.",
                    "label": 0
                },
                {
                    "sent": "So you're going to restrict to just that subspace in work just on that subspace.",
                    "label": 0
                },
                {
                    "sent": "So think of it.",
                    "label": 0
                },
                {
                    "sent": "As simply a nonlinear version of CG applied based upon our restarting condition, telling you about how much your current search direction does or does not align with the residual.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this works quite.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, so I showed you this curve of recovery ability for the shadden one norm nuclear normalization which is here.",
                    "label": 0
                },
                {
                    "sent": "Here are the recovery levels for the matrix completion version of NHT SGH T and that mysterious FHT and you can see that actually these have a much higher phase transition.",
                    "label": 0
                },
                {
                    "sent": "I'll explain a little bit more quantitative write down here what's going on.",
                    "label": 0
                },
                {
                    "sent": "It's also typically much faster.",
                    "label": 0
                },
                {
                    "sent": "We have not been able to come up with a fast enough version of this to give anything that isn't silly, so we're not showing timings.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if you want to get a sense as to what's going on, sorry, let's go back one slide.",
                    "label": 0
                },
                {
                    "sent": "Let's take a look at what's happening here, but for the case of Delta being very small, let's take Delta to be .051, / 20 down here, and let's see what's happening for CGI HT, which seems to be working spectacularly well.",
                    "label": 0
                },
                {
                    "sent": "In fact, it looks like here the number of measurements is just very much very slightly more than one times the number of degrees of freedom the universe of this, about 1.1 times the number of degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "Well, at the case where we have the number of measurements be a 20th of the number of entries, we've run 100 tests for each rank.",
                    "label": 0
                },
                {
                    "sent": "This is a for 2000 by 2000 matrix and record the success and we record the asymptotic convergence rate and what we see taking a vertical slice on those prior slots prior plots, we see that the convergence rate gets worse and worse and worse.",
                    "label": 0
                },
                {
                    "sent": "These are plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "Standard deviation gets worse and worse and worse as you approach the phase transition and then at some point.",
                    "label": 0
                },
                {
                    "sent": "It fails well.",
                    "label": 0
                },
                {
                    "sent": "Why is it failing?",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out the reason it fails is because we stop running the code because this converging so slowly that we just can't wait any longer.",
                    "label": 0
                },
                {
                    "sent": "That seems to be the only reason it stops.",
                    "label": 0
                },
                {
                    "sent": "In fact, for Ro the number of degrees of freedom over the number of measurements being less than .9, we get recovery in at least a 99 out of 100.",
                    "label": 0
                },
                {
                    "sent": "Test for every rank all the way down from one up to the maximum recoverable for this row, whereas for the shadden one norm.",
                    "label": 0
                },
                {
                    "sent": "We know that if you don't have rolled less than .41 from the theory that Joel will present tomorrow, you will typically not have successful recovery.",
                    "label": 0
                },
                {
                    "sent": "In the limit, it seems like for the shadden one norm, this is in fact a third, and we don't seem to know whether this has a drop off for these iterative hard thresholding algorithms, so it seems like there's actually quite a bit more going on in the matrix completion setting.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's enough so.",
                    "label": 0
                },
                {
                    "sent": "What are some?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Takeaways so some takeaways are when designing these compressed sensing matrix completion algorithms.",
                    "label": 1
                },
                {
                    "sent": "What we need to be thinking about is what the algorithm is doing so it has an exploration phase where it really doesn't know what's going on.",
                    "label": 0
                },
                {
                    "sent": "Is trying to figure out what is the dominant subspace where the non zero values are.",
                    "label": 0
                },
                {
                    "sent": "This kind of thing and in that case it needs to be cheap, fast, quick, not sophisticated and then it needs to have a way of realizing that as it's going along it is gaining confidence and then to start doing more and more sophisticated things the less support is changing.",
                    "label": 0
                },
                {
                    "sent": "Closer you are to the subspace, the number of iterates that you're doing.",
                    "label": 0
                },
                {
                    "sent": "Something like kanji gradient on should be going up, and that is in fact exactly what's happening here, and the higher order method of using constant gradient in the appropriate way can both accelerate convergence, and in fact increase the phase transition region right?",
                    "label": 0
                },
                {
                    "sent": "That's it, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very much.",
                    "label": 0
                }
            ]
        }
    }
}