{
    "id": "p3jefnbdcltoxk6qvcl7ynhndfmf2lpn",
    "title": "Embedding Learning for Declarative Memories",
    "info": {
        "author": [
            "Volker Tresp, Siemens AG"
        ],
        "published": "July 10, 2017",
        "recorded": "June 2017",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2017_tresp_declarative_memories/",
    "segmentation": [
        [
            "Yes, thank you very much, and my coauthors are uniform, are different.",
            "Buyer Indian Chung Young and we're all.",
            "From demons and or.",
            "Rubik Maximilian University of Munich.",
            "So I will start introducing you to LA."
        ],
        [
            "With semantic knowledge, graphs and then show how time component can be introduced into nice graphs.",
            "So we talk about learning with episodic knowledge graphs and then make some speculations about human declarative memories."
        ],
        [
            "So the previous talks there was already mentioning of the Knowledge Graph, so entities are notes and predicates are directed links.",
            "And if you want to make the connection to probabilistic models, of course this whole statement.",
            "Would be one note in a probabilistic model, for example on page Network.",
            "So in a way, these two graphs are really dual to one another.",
            "And you can build nice things like this one I've attached as a physicist, physicist, researcher.",
            "He was born in.",
            "William's the city women's in Germany.",
            "Germans stayed and so on.",
            "And the nice thing one nice thing is that the Knowledge graph is so simple that different communities can understand it.",
            "Because a lot of stuff going on in knowledge representation is very difficult to understand for outsiders, so I think it's a great link between cultures between machine learning."
        ],
        [
            "And knowledge graphs and another representation.",
            "Of course we know these projects.",
            "DB Pedia, Jago.",
            "And then Google bought Freebase and I think it was a major breakthrough because they made it really large.",
            "Now it's more than 100 billion facts.",
            "They made it.",
            "They solve the quality problem by involving hundreds of people essentially, and they made it useful by using it and search question, answering text understanding.",
            "So I think all these three things are really a major advances and."
        ],
        [
            "Academic world it's a little bit difficult to get these problems to be solved.",
            "So how do we do machine learning?",
            "We take a trooper so Max likes Mary, so there's three things.",
            "So we need 3 indices.",
            "So matrix is not enough.",
            "We use a tensor which is in our.",
            "Purpose just a 3 dimensional array.",
            "And so we look at these three indices for Max Marian likes and put a one in the Knowledge Graph.",
            "If this.",
            "If we know that this affect is true.",
            "So we transform the whole knowledge graph into into a tensor, essentially which is mostly.",
            "Zeros, of course is mostly empty, and it's quite big."
        ],
        [
            "And the other stuff will just leave 0.",
            "So, and how do you mean one way is to do a factorization approach so the generalization of what people do with matrices when the matrix factorization.",
            "This would be tensor factorization, so this is a model specific model developed by Maxim Unicode race car, so we get one matrix which has one as many rows as their entities.",
            "So each entity gets a bunch of numbers assigned to it.",
            "Depending on the rank and the same matrix is also here for the object.",
            "So and then we have the core tensor.",
            "Where each relation type has a slice.",
            "So to make a prediction we simply multiply this latent representation with a matrix and with this later vegetation then we get a number out.",
            "We put a sigmoid on top and get a probability.",
            "So this looks like how it's done in a double Summon.",
            "This is an offensive tensor notation, so you can think of it as a generalization to factorization of matrices.",
            "And now in the reconstruction, if one wants to become a zero, it's a triple, which is not supported well by the patterns in the Knowledge graph.",
            "So it might be a mistake and a more interesting Lee for Zero wants to become a one.",
            "It's a pattern which is supported by the Knowledge Graph and sort of an inductive inference.",
            "If he becomes a large number, you might think.",
            "This might be inductively chosen to be to be true."
        ],
        [
            "This is another view on the same model, so you have Max likes Mary.",
            "Again the bottom.",
            "Here we see the latent representations.",
            "We also made this into a vector before it was a matrix, and then there's a mapping of this later representations to true or false and we use accordions.",
            "Are other people using neural network and different other options.",
            "And the important thing is that this representation layer where each entity has a unique representation.",
            "So this representation of Max is the same whenever Max appears.",
            "Independent as a subject or object, and by doing that you get propagation of information in the Knowledge Graph which would otherwise not not get.",
            "So this unique representation is really important and we did a number of benchmark experiments and got pretty good results, but that's of course always interesting to see."
        ],
        [
            "If you can do something in the real world and Google did this interesting project, the Knowledge World where they try to feel the Knowledge Graph automatically.",
            "So first they use all their fancy extraction tools analyzing tables, text and so on to produce triples, and they got an AOC of 0.927 and then they what they called graph graph based priors.",
            "So one of these approaches, closer or factorization approach.",
            "But they used in one network instead of attends our model and they got zero point.",
            "Payday 2 then the path ranking algorithm, which is sort of a rule patent search for privacy rules.",
            "Anyway, you get a 0.882 and you can combine these two things together 0.911 which is almost as good as doing all this involved search from the web so the information within the Knowledge Graph is obviously very valuable.",
            "It's almost as good as if you would do all this fancy extraction techniques and then you confuse these models further and get a score of 0.94.",
            "Seven, which seems like a small improvement, but increase the number of high confidence facts from 100 million, 270 million.",
            "So this is an ongoing project.",
            "Google to our knowledge is not quite happy with these numbers yet.",
            "They want to .99, but I think it's a nice proof of concept that is certainly working on real important problems.",
            "And we wrote with these good People writer order survey paper review, paper Proceedings of IEEE."
        ],
        [
            "Give an overview over these things, so learning with this gas can have many use users.",
            "For example, if you have patient data, you can even do something like diagnosis.",
            "If two patients have a similar patterns in the large graph, they might also have the same diagnosis.",
            "You can do this automatic filling of the Knowledge Graph is just described you have.",
            "You can also generalize it to a database, doesn't have.",
            "It's not restricted to binary relations.",
            "Then you have a learning database and you can use in particular these.",
            "Latent representations there can be shared between different applications, so you can use information extracted from the knowledge graph and other applications because you can use the latent representations you got there.",
            "One comment on rescue and similar models.",
            "It works quite well, but it's not that one tool fits all.",
            "School has particular problems if they are at least what I mentioned.",
            "Our rule patterns like triangle Rule or Mary 2 rules.",
            "If a is married to be there be is married to a, so all factorization approaches we believe have problems there.",
            "So one should have an extra model for these things and we did that and got also good results and was published here and in particular the rank of the factorization would greatly be reduced if we use this more rule oriented.",
            "Pattern in addition, in combination.",
            "So it was an additive model."
        ],
        [
            "So in condition aesthetic semantic memory is about facts we know.",
            "So Obama's ex president of United States Munich is in Bavaria, the various in Germany.",
            "So this type of knowledge, and so our hypothesis would be that this type of factorization model might be an interesting model also for for cognitive semantic models.",
            "Here we show the same model slightly differently drawn, but essentially the same.",
            "Here's the index layer, so this is Max.",
            "This is P for likes.",
            "This is over Mary.",
            "Here's here's the latent representations, and here's the mapping to true or false.",
            "We think that."
        ],
        [
            "The previous model is nice, but it's not quite.",
            "Maybe not made biologically plausible, and there's another way of drawing it exactly the same math and I think has changed, so you can think of also the model that if you have SNP so Max likes, what does he like?",
            "Then you model calculates a later representation.",
            "This yellow vector here, which then forms an inner product with the letter presentation of the objects and then the match determines what object is likely and this gives you fertilize.",
            "More biological because you don't have this strange light bulb.",
            "Run on top and inner products are biologically very plausable and you can go another direction so you can predict the subject predicate.",
            "Or you can also marginalized out something and say what is the likely predicate for the subject independent of the object.",
            "So by by generalizing a little bit you can play a lot of interesting games here."
        ],
        [
            "And so now what is an episodic memory?",
            "So we consider episodic memory simply triples in time.",
            "So if we have here.",
            "That ends our model for semantic memory and episodic memory simply has one more dimension, which is, which is time, and I'll give you an example in a moment, and the factorization is also very simple to generalize, so here we had three latent representations with subject predicate object.",
            "Here we have four, an additional one for time, which just and.",
            "So what does it mean?",
            "It stands for all the things which happened at that time instance.",
            "Essentially, if you want.",
            "So that he."
        ],
        [
            "He would be an example and a model similar to the one I showed before, so we just add one more input, one more index here, one related notation and this could then represent On this date check was diagnosed with diabetes and again you can play different games.",
            "You can say On this date check was diagnosed with what or you can say what happened On this date and then we should get the response back.",
            "Was diagnosed with diabetes so you can go forth and back and do conditional stuff an.",
            "Very flexible model, and so there's some technical technicality in learning.",
            "We assume a Bernoulli distribution, which gives you the trouble of probability for querying the driver model, which is probably distribution with subject predicate object.",
            "To be able to do all this conditioning technical details, we apply this in a medical domain, not for example, for patient.",
            "The background information would be the semantic memory like age, peak condition.",
            "The sex HH and episodic knowledge graph would describe all the events which happened to the patient at the different time times because the patient visited the clinic or so.",
            "How you, how costly, you define times up to the application of course.",
            "And what you then can do is decision modeling.",
            "So you predict the next decision by the Doctor.",
            "Or you do, which is some advantages.",
            "But then of course always the question is why?",
            "Why should I do this?",
            "Endpoint prediction is more intuitive.",
            "You predict how long press the survival of a kidney, survival of a patient acuity, visual equity in a year later.",
            "So that's something I think easier to accept by Doctor because they think OK prediction is sort of statistic.",
            "And may be difficult to explain, but I can interpret this prediction and then I can derive from my conclusions and we have here projects on breast cancer with a University clinic in Erlangen, nephrology with charity and age related macular degeneration.",
            "Eye problem with the LMU clinic."
        ],
        [
            "So so so this.",
            "And also I should say we also add a LS TM models, so recurrent neural networks on top.",
            "So the memory gives you some representations for the task itself for the prediction task itself you need a model on top which uses these representations so you don't get predictions directly out of these memory models."
        ],
        [
            "So this is the way in the textbooks you find the way human memory is organized.",
            "So the question is now, can this be related to human memory?",
            "Important here is declarative memory or explicit memory.",
            "That's the stuff you can easily talk about.",
            "For example, semantic memory is the stuff you know like Obama was expected of United States or Munich in Bavaria, and episodic memory is about things you remember like dinner.",
            "Dinner last evening or whatever you did instead of going to the lecture yesterday.",
            "And then there are non declarative memories like how to learn how to throw throw ball or play tennis and perceptual memory, improving your visual equity or something like that.",
            "Sensory memory are very short memory buffers, for example, that you just remember something you heard, a telephone number or something, or just the visual impression, and then the short term working memory is really the smartness.",
            "On top is really is the central executive, sort of the boss in your brain and it looks at for a logical loop episodic buffer and visual spatial sketchpad to drive to make decisions.",
            "So here's the magic number of seven of the seven items you can keep in your mind for decision."
        ],
        [
            "Very famous experiment so.",
            "So this is 1 model we're like, it's it's drive.",
            "By this Taylor, Andy Sienna.",
            "So here's the visual input and or other input sensor input.",
            "It goes to the thalamus.",
            "Everything that's sort of a relay in your brain, and then you have these different layers processing layers with more and more abstract representations.",
            "So here might be just as detectors, and here you get noses and eyes like in deep neural network.",
            "And then this this pattern is stored by forming a connection between an index and the hippo compost to this activation pattern, and this pattern would exactly be our latent imitation for time, which then would feed into the episodic memory.",
            "In our case.",
            "Of course, this only memory is formed if it's important or significant or novel.",
            "And then there's a consolidation process where this memory becomes transformed into neocortex for long term storage.",
            "When this happens, it happens during sleep, so you should always have a good sleep to consolidate your memory system."
        ],
        [
            "So this would be a technical version of this.",
            "We have vision or hearing or other sensory input and something like a deep neural network, for example, which produces an output representation which then could be the basis for an episodic memory.",
            "But it could also be the basis for this semantic decoding, because it can be part of this episodic memory which can decode this information in two sets of triples.",
            "Now so on the right side you have an image and on left side you have.",
            "There's a person as a person on a bike.",
            "The bike is on the street and so on, so I think that's one of the hot overcoming big challenges in vision applications that you not just say it's a dog, it's a cat, but you also get much more semantically rich information out of these things, and this is an important addition because the model from Taylor Disenor.",
            "They didn't really.",
            "They called it an episodic memory, but they didn't really say how it would be declarative.",
            "Or you can.",
            "Would be able to get structural information out of that, so we believe this memory idea might be an interesting hypothesis."
        ],
        [
            "How this can be done?",
            "This is where the hippocampus is.",
            "Hippocampus is very important for memory and structured and a lot of theories from our point of view we just say give us an index.",
            "We don't care how you do it.",
            "Here's an interesting error that ended Gyrus, which is the only part only major part of the brain where new neurons are generated even when you're in adult persons.",
            "So that might also indicate that you need new index capacity and hippocampus for this purpose."
        ],
        [
            "So in this paper, we analyzed the following hypothesis.",
            "So how how can semantic in episodic memory be connected?",
            "There's also a lot of speculation in a literature, but it would be difficult to have think about a mechanism where a moves to be so.",
            "The obvious hypothesis would be that semantic memory is sort of an integral over episodic memory.",
            "So if you have here on this day Jack was diagnosed with diabetes, then here you would just have the information deck was diagnosed with diabetes.",
            "And technically, that's simple because these tensor models belong to the class of sum product networks.",
            "And technically, you just essentially have a neutral input here at the time input.",
            "Mathematically, would be all once, so then it means essentially at anytime when did this happen?",
            "We ignore when it happened, we just want to know what happened, so this might be only an initial.",
            "Mechanism where there's probably more going on, but I think it's an attractive hypothesis to think that semantic memories, episodic memory, by marginalization, step by integration."
        ],
        [
            "And we did experiments based on data derived from Freebase using 140,000 triples and we had different models.",
            "One was on constraints on the weights, so the weights could be positive or negative.",
            "Then non negative weights and then not negative and sparse where we used the L1 norm to enforce or encourage sparsity.",
            "You might know it from the lasso models.",
            "And then we tested the semantic model, tested on semantic data.",
            "So under projected data.",
            "So this gives us some feel for how good is semantic memory can be episodic, memory is episodic model.",
            "As a four dimensional model tested on the episodic data.",
            "So we generated 40 times slides.",
            "It's out of this data set.",
            "An projection means we trained the episodic model and then projected it and want to see how good the semantic memory is.",
            "So here summary."
        ],
        [
            "Science you always show test and training error, because in this case we almost more interested in the training performance because we want to use it as a memory and generalization is sort of the icing on top that we also can generalize to new information.",
            "Also with the danger of false memories, of course.",
            "So semantic trained on semantic data is a blue line over here, so let's look at this one here.",
            "This degree one is episodic, trained on episodic.",
            "So you see that you always need a higher rank and the reason of course is that if you at the time dimension the patterns become blurred and you need more higher rank to model the same type of data and it just looks more noisy, you need higher rank and the red one.",
            "Is this projection operation.",
            "So the tested on the semantic data but derived from the model on the episodic data and it sort of works, but you don't get quite the performance of if you trained it directly on the episodic model, so the important also these non negative models be cause they.",
            "Only there you can do this integration step.",
            "You need this non negativity in your models.",
            "Here.",
            "What we did is we just reconstructed the triples and then numerically integrated them which would not work necessarily in large datasets."
        ],
        [
            "Here's another score record score where it looks a little bit nicer, but as a proof of principle I think we can show that it were."
        ],
        [
            "Books.",
            "Yeah, and you can also look at the sparsity for the unconstrained model.",
            "3% non negative, 30% nonnegative, sparse 58%.",
            "And sparsity is important from biological point of view, because you cannot have everything connected to everything in a biological sense, so that's important.",
            "We also ongoing work here.",
            "One is to include start and end date, so if you get a disease maybe you get cured.",
            "So this semantic memory should know that you're not sick anymore and we have a good data set and I think also getting good results there and then.",
            "We also look at other four way 10th or models beyond the Tucker models we have been using here in our experiments, but there's ongoing work.",
            "So."
        ],
        [
            "The conclusion we think their memory function might be a good starting point for understanding understanding intelligent systems.",
            "So we saw that there's a sensory input and it goes into this decoder and the memory system, and then, as I said, also in the medical application, the really tight or the really smartness?",
            "Or are they really useful things like prediction decision, support, optimization would be on top.",
            "So the idea would be that they use the latent representations of the objects.",
            "Involved for other purposes, so you can think of it at this working memory stuff is on top and it uses the memory system for all sorts of things.",
            "Yeah, we believe this concept of semantic knowledge graph episodic memory knowledge graph might be quite interesting or so in context with this perceptual decoding, and in particular the latent representations we get out of these different memory systems might be really an interesting link of symbolic and sub symbolic work.",
            "Now, because we can use this real numbers then in other applications.",
            "So for example in the medical application we use the latent vector for other patients as an input for this prediction model.",
            "And I think it's it's in Hot Topic now.",
            "People also in machine learning increasingly look at this issue.",
            "Not again, when they just started looking at it, I think.",
            "And then, of course, there's always the problem.",
            "Can does it go both ways?",
            "Can really, I mean the technical is a biology might get motivated by some of our models.",
            "Think this happened before in the past, but can we learn from biology?",
            "I believe yes, and one reason is that.",
            "To me now this is hype on a I know that.",
            "And then you question.",
            "So what is AI?",
            "And if you look at the functions which are realizing the brain you see first over some of many functions in cognitive literature that sometimes talk about thousands and millions of modules which do something in your brain and given time.",
            "So there definitely this module so we can keep working good whatever image analysis systems and memory systems planning this in support.",
            "And then there's some integration on top.",
            "Which is probably still a big challenge now, because also in your brain people speculate how you make like one decision if your brain is in total chaos all the time.",
            "So there's all these different submodules doing something so they're different, definitely important challenges, but I think having this analogy with brain functions makes it clearer what AI sort of is, instead of some magical thing which at some point just happens in a very large whatever deep learning network or so.",
            "Wait, thank you very much.",
            "So if instead write you.",
            "Put in the training phase is really just the atomic triples, right?",
            "So ABC.",
            "So what if you would?",
            "Empower the strippers with.",
            "Extra bacon only tried so like.",
            "Relation is typically's taken from some hierarchy of relations or concepts.",
            "Again, you know some context thread, so you have in principle extra signals or extra knowledge which go beyond ID's right?",
            "Which here you operate only with IDs.",
            "Would this tensor representation be?",
            "I would guess it this factorization will have a little bit easier drop.",
            "And also well.",
            "In principle you would get better answers, right?",
            "Unless you this would cause some bias which you wouldn't want know here we we use that information already, but in a maybe materialized way.",
            "So if.",
            "So in the first models Max did, he included all this subclass information, but he didn't.",
            "There was not a special treatment for it, so it was just like any other relation and you had some propagation across this graph describing the subclass hierarchies.",
            "We have some other experiments where we do the models more on the class level.",
            "A bit coarser because you don't get a lot of the fine details, but sometimes that can also work quite well.",
            "And but it's also a little bit part of future work.",
            "For example, 1 interesting question is if you have a knowledge graph for one domain, how do you apply to a new domain?",
            "And there you would probably more rely on class level similarities or something like that, but there are different different ways of doing that, and we're one of our projects that we're exploring that.",
            "But I also always have a little bit bad feeling that we are not doing more in this direction.",
            "But maybe we will in the future, but at this level we use that information, but it's just treated, it's just any other relation.",
            "OK. One question down there.",
            "Hello very interesting talk.",
            "So you experiment with Freebase.",
            "So do you think this marginalization over the episode episodic memory would also work very well on other knowledge bases or knowledge graphs?",
            "Or is there anything or what will this depend on essentially?",
            "Are there some characteristics of the data set which play a big role here?",
            "Yeah, we currently work on a real temporal database.",
            "Maybe even though I don't remember the name, it's about countries and interaction between countries, so there's really time.",
            "Here in our experiments, we sort of artificially introduced this time timestamp.",
            "There was not a natural timestamp and we also get quite good results.",
            "It's I think if you it should not be too diluted.",
            "I would think now so if at every instant in time you only have one event.",
            "Then factorization might might become difficult.",
            "On the other hand, in the sort of biologically oriented model you would store the latent representation of this episode and then you need some decoding system.",
            "One or the other kind anyways.",
            "So I think it should not be for my intuition.",
            "Currently I would think it should be more than one thing happening at a time, so it should not.",
            "If you haven't medical system like maybe the timestamp should not be the hour but the day.",
            "So everything which happened to the day is sort of at the same time for the model and then you can still do completion without this time frame.",
            "And of course also time can be dealt with in many ways.",
            "Nurses across way, but it's more than what most medical people probably do the.",
            "Aggregate everything into one state vector or something like that so we have a representation for the different levels.",
            "Another intuition you can have if you think of you can also think of that these events are an input, for example to recurrent neural network and then so how much information do you need to get good predictive models?",
            "Yeah, it it's it's something we are also wondering a little bit and I think if you really interested in this as a memory system then the rank goes up.",
            "If you have a little sparsity and we saw that also in our experiments.",
            "OK, thank you.",
            "Girl.",
            "Yes, first thank you for the talk.",
            "So it was quite nice 'cause it's a bit different or let's say unusual compared to other talks here, so I liked it very much.",
            "I do actually maybe 2 questions, so the first one is.",
            "How hard would it be to work with statements which are themselves on certain segment?",
            "You're certain if statements like ABC's Asmara said, and you basically saying their true or false from the start.",
            "What if these statements themselves have uncertainty or?",
            "Probability inside them.",
            "If they're local probabilities, it's not very difficult, so if if it's like a local noise on the model.",
            "11 of course, straightforward way would just instead of a one within the probability.",
            "It's something which can be done, but if you you have to let me get more details of The thing is like for example talking about a Doctor Who is giving a certain diagnosis.",
            "He's also not certain about it, so you could say like OK, it's either.",
            "I don't know it's this or it's that, but I'm not sure which one.",
            "Yeah, no, I agree.",
            "I know if it's a local uncertainty we can deal with that.",
            "It's it's a little bit.",
            "Tricky becausw after so the dependencies are really done during the factorization and after the factorization that ripples become independent in these models.",
            "So if you input that uncertain information for in training data, that should be not mean.",
            "I would do it this way and then just use your confidence value as a representation for this entry.",
            "We can also.",
            "So the interest don't have to be binary Boolean.",
            "They could also be numbers or counts would have to change the likelihood model accordingly.",
            "If you want to have.",
            "If there's correlation between the uncertainty, then becomes more difficult.",
            "The second question was do you do anything with literals in the graph?",
            "Do you deal in a special way?",
            "Do you?",
            "Or do you just see them as notes?",
            "No way for us.",
            "We treat literals as unary relations so and.",
            "The best way to do that is to.",
            "So if you talk, if you think about a database and as relations of different arities.",
            "So all the binary ones would become three way, 10s, or the unique ones became a matrix, and you can go up and down and down is difficult, but up.",
            "So essentially we would then have a 10s or a three by 10s or and a matrix, and we factorize both.",
            "But we insist that they have common latent representations, so that if Jack appears in the.",
            "Three way.",
            "A knowledge graph.",
            "And in the NTS, simple properties and the letter representation for Jack should be the same, so there's a coupling by by the later representations and then the two factorizations can benefit from one another, and sometimes you get improved results.",
            "OK, thank you.",
            "OK, I think we have time for one more question.",
            "I would just ask the next presenter to prepare.",
            "OK.",
            "I have another one, so just since well, semantic people are very symbolic, right?",
            "So you could try to extract any kind of symbolic representation or approximation out of the tensor factor model, right?",
            "So which would have some interpretation or something which, let's say, would be easy to read or represent?",
            "That's a very good question.",
            "We never really tried it.",
            "I mean, what we did was to cluster in the later invitations and then we get the things.",
            "So we use the later reservations.",
            "For example for cluster analysis or to discover classes or class hierarchies and that that works very well.",
            "But somehow we were never too interested in interpreting these latent dimensions themselves.",
            "Other areas do that a lot.",
            "For example, if you go in single processing.",
            "So they're they're really into analyzing the latent factors, but they only can do it for, for example, parafac models or CP models.",
            "There's a long tradition in that if they go to the Tucker models, the one we're using, it's also very difficult to interpret for them.",
            "But we also never really tried to be honest, and it seems the other groups who are working in this domain also don't try it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, thank you very much, and my coauthors are uniform, are different.",
                    "label": 0
                },
                {
                    "sent": "Buyer Indian Chung Young and we're all.",
                    "label": 0
                },
                {
                    "sent": "From demons and or.",
                    "label": 0
                },
                {
                    "sent": "Rubik Maximilian University of Munich.",
                    "label": 1
                },
                {
                    "sent": "So I will start introducing you to LA.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With semantic knowledge, graphs and then show how time component can be introduced into nice graphs.",
                    "label": 0
                },
                {
                    "sent": "So we talk about learning with episodic knowledge graphs and then make some speculations about human declarative memories.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the previous talks there was already mentioning of the Knowledge Graph, so entities are notes and predicates are directed links.",
                    "label": 0
                },
                {
                    "sent": "And if you want to make the connection to probabilistic models, of course this whole statement.",
                    "label": 0
                },
                {
                    "sent": "Would be one note in a probabilistic model, for example on page Network.",
                    "label": 0
                },
                {
                    "sent": "So in a way, these two graphs are really dual to one another.",
                    "label": 0
                },
                {
                    "sent": "And you can build nice things like this one I've attached as a physicist, physicist, researcher.",
                    "label": 0
                },
                {
                    "sent": "He was born in.",
                    "label": 0
                },
                {
                    "sent": "William's the city women's in Germany.",
                    "label": 0
                },
                {
                    "sent": "Germans stayed and so on.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing one nice thing is that the Knowledge graph is so simple that different communities can understand it.",
                    "label": 1
                },
                {
                    "sent": "Because a lot of stuff going on in knowledge representation is very difficult to understand for outsiders, so I think it's a great link between cultures between machine learning.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And knowledge graphs and another representation.",
                    "label": 0
                },
                {
                    "sent": "Of course we know these projects.",
                    "label": 0
                },
                {
                    "sent": "DB Pedia, Jago.",
                    "label": 0
                },
                {
                    "sent": "And then Google bought Freebase and I think it was a major breakthrough because they made it really large.",
                    "label": 0
                },
                {
                    "sent": "Now it's more than 100 billion facts.",
                    "label": 0
                },
                {
                    "sent": "They made it.",
                    "label": 0
                },
                {
                    "sent": "They solve the quality problem by involving hundreds of people essentially, and they made it useful by using it and search question, answering text understanding.",
                    "label": 0
                },
                {
                    "sent": "So I think all these three things are really a major advances and.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Academic world it's a little bit difficult to get these problems to be solved.",
                    "label": 0
                },
                {
                    "sent": "So how do we do machine learning?",
                    "label": 0
                },
                {
                    "sent": "We take a trooper so Max likes Mary, so there's three things.",
                    "label": 0
                },
                {
                    "sent": "So we need 3 indices.",
                    "label": 0
                },
                {
                    "sent": "So matrix is not enough.",
                    "label": 0
                },
                {
                    "sent": "We use a tensor which is in our.",
                    "label": 0
                },
                {
                    "sent": "Purpose just a 3 dimensional array.",
                    "label": 0
                },
                {
                    "sent": "And so we look at these three indices for Max Marian likes and put a one in the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "If this.",
                    "label": 0
                },
                {
                    "sent": "If we know that this affect is true.",
                    "label": 0
                },
                {
                    "sent": "So we transform the whole knowledge graph into into a tensor, essentially which is mostly.",
                    "label": 1
                },
                {
                    "sent": "Zeros, of course is mostly empty, and it's quite big.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the other stuff will just leave 0.",
                    "label": 0
                },
                {
                    "sent": "So, and how do you mean one way is to do a factorization approach so the generalization of what people do with matrices when the matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "This would be tensor factorization, so this is a model specific model developed by Maxim Unicode race car, so we get one matrix which has one as many rows as their entities.",
                    "label": 0
                },
                {
                    "sent": "So each entity gets a bunch of numbers assigned to it.",
                    "label": 0
                },
                {
                    "sent": "Depending on the rank and the same matrix is also here for the object.",
                    "label": 0
                },
                {
                    "sent": "So and then we have the core tensor.",
                    "label": 0
                },
                {
                    "sent": "Where each relation type has a slice.",
                    "label": 0
                },
                {
                    "sent": "So to make a prediction we simply multiply this latent representation with a matrix and with this later vegetation then we get a number out.",
                    "label": 0
                },
                {
                    "sent": "We put a sigmoid on top and get a probability.",
                    "label": 0
                },
                {
                    "sent": "So this looks like how it's done in a double Summon.",
                    "label": 0
                },
                {
                    "sent": "This is an offensive tensor notation, so you can think of it as a generalization to factorization of matrices.",
                    "label": 1
                },
                {
                    "sent": "And now in the reconstruction, if one wants to become a zero, it's a triple, which is not supported well by the patterns in the Knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "So it might be a mistake and a more interesting Lee for Zero wants to become a one.",
                    "label": 1
                },
                {
                    "sent": "It's a pattern which is supported by the Knowledge Graph and sort of an inductive inference.",
                    "label": 0
                },
                {
                    "sent": "If he becomes a large number, you might think.",
                    "label": 0
                },
                {
                    "sent": "This might be inductively chosen to be to be true.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is another view on the same model, so you have Max likes Mary.",
                    "label": 0
                },
                {
                    "sent": "Again the bottom.",
                    "label": 0
                },
                {
                    "sent": "Here we see the latent representations.",
                    "label": 0
                },
                {
                    "sent": "We also made this into a vector before it was a matrix, and then there's a mapping of this later representations to true or false and we use accordions.",
                    "label": 0
                },
                {
                    "sent": "Are other people using neural network and different other options.",
                    "label": 0
                },
                {
                    "sent": "And the important thing is that this representation layer where each entity has a unique representation.",
                    "label": 0
                },
                {
                    "sent": "So this representation of Max is the same whenever Max appears.",
                    "label": 0
                },
                {
                    "sent": "Independent as a subject or object, and by doing that you get propagation of information in the Knowledge Graph which would otherwise not not get.",
                    "label": 0
                },
                {
                    "sent": "So this unique representation is really important and we did a number of benchmark experiments and got pretty good results, but that's of course always interesting to see.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you can do something in the real world and Google did this interesting project, the Knowledge World where they try to feel the Knowledge Graph automatically.",
                    "label": 0
                },
                {
                    "sent": "So first they use all their fancy extraction tools analyzing tables, text and so on to produce triples, and they got an AOC of 0.927 and then they what they called graph graph based priors.",
                    "label": 0
                },
                {
                    "sent": "So one of these approaches, closer or factorization approach.",
                    "label": 1
                },
                {
                    "sent": "But they used in one network instead of attends our model and they got zero point.",
                    "label": 0
                },
                {
                    "sent": "Payday 2 then the path ranking algorithm, which is sort of a rule patent search for privacy rules.",
                    "label": 1
                },
                {
                    "sent": "Anyway, you get a 0.882 and you can combine these two things together 0.911 which is almost as good as doing all this involved search from the web so the information within the Knowledge Graph is obviously very valuable.",
                    "label": 0
                },
                {
                    "sent": "It's almost as good as if you would do all this fancy extraction techniques and then you confuse these models further and get a score of 0.94.",
                    "label": 1
                },
                {
                    "sent": "Seven, which seems like a small improvement, but increase the number of high confidence facts from 100 million, 270 million.",
                    "label": 0
                },
                {
                    "sent": "So this is an ongoing project.",
                    "label": 1
                },
                {
                    "sent": "Google to our knowledge is not quite happy with these numbers yet.",
                    "label": 0
                },
                {
                    "sent": "They want to .99, but I think it's a nice proof of concept that is certainly working on real important problems.",
                    "label": 0
                },
                {
                    "sent": "And we wrote with these good People writer order survey paper review, paper Proceedings of IEEE.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give an overview over these things, so learning with this gas can have many use users.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have patient data, you can even do something like diagnosis.",
                    "label": 0
                },
                {
                    "sent": "If two patients have a similar patterns in the large graph, they might also have the same diagnosis.",
                    "label": 0
                },
                {
                    "sent": "You can do this automatic filling of the Knowledge Graph is just described you have.",
                    "label": 1
                },
                {
                    "sent": "You can also generalize it to a database, doesn't have.",
                    "label": 0
                },
                {
                    "sent": "It's not restricted to binary relations.",
                    "label": 1
                },
                {
                    "sent": "Then you have a learning database and you can use in particular these.",
                    "label": 0
                },
                {
                    "sent": "Latent representations there can be shared between different applications, so you can use information extracted from the knowledge graph and other applications because you can use the latent representations you got there.",
                    "label": 0
                },
                {
                    "sent": "One comment on rescue and similar models.",
                    "label": 0
                },
                {
                    "sent": "It works quite well, but it's not that one tool fits all.",
                    "label": 1
                },
                {
                    "sent": "School has particular problems if they are at least what I mentioned.",
                    "label": 0
                },
                {
                    "sent": "Our rule patterns like triangle Rule or Mary 2 rules.",
                    "label": 0
                },
                {
                    "sent": "If a is married to be there be is married to a, so all factorization approaches we believe have problems there.",
                    "label": 0
                },
                {
                    "sent": "So one should have an extra model for these things and we did that and got also good results and was published here and in particular the rank of the factorization would greatly be reduced if we use this more rule oriented.",
                    "label": 0
                },
                {
                    "sent": "Pattern in addition, in combination.",
                    "label": 0
                },
                {
                    "sent": "So it was an additive model.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in condition aesthetic semantic memory is about facts we know.",
                    "label": 1
                },
                {
                    "sent": "So Obama's ex president of United States Munich is in Bavaria, the various in Germany.",
                    "label": 0
                },
                {
                    "sent": "So this type of knowledge, and so our hypothesis would be that this type of factorization model might be an interesting model also for for cognitive semantic models.",
                    "label": 0
                },
                {
                    "sent": "Here we show the same model slightly differently drawn, but essentially the same.",
                    "label": 1
                },
                {
                    "sent": "Here's the index layer, so this is Max.",
                    "label": 0
                },
                {
                    "sent": "This is P for likes.",
                    "label": 0
                },
                {
                    "sent": "This is over Mary.",
                    "label": 0
                },
                {
                    "sent": "Here's here's the latent representations, and here's the mapping to true or false.",
                    "label": 0
                },
                {
                    "sent": "We think that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The previous model is nice, but it's not quite.",
                    "label": 0
                },
                {
                    "sent": "Maybe not made biologically plausible, and there's another way of drawing it exactly the same math and I think has changed, so you can think of also the model that if you have SNP so Max likes, what does he like?",
                    "label": 0
                },
                {
                    "sent": "Then you model calculates a later representation.",
                    "label": 0
                },
                {
                    "sent": "This yellow vector here, which then forms an inner product with the letter presentation of the objects and then the match determines what object is likely and this gives you fertilize.",
                    "label": 0
                },
                {
                    "sent": "More biological because you don't have this strange light bulb.",
                    "label": 0
                },
                {
                    "sent": "Run on top and inner products are biologically very plausable and you can go another direction so you can predict the subject predicate.",
                    "label": 0
                },
                {
                    "sent": "Or you can also marginalized out something and say what is the likely predicate for the subject independent of the object.",
                    "label": 0
                },
                {
                    "sent": "So by by generalizing a little bit you can play a lot of interesting games here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so now what is an episodic memory?",
                    "label": 1
                },
                {
                    "sent": "So we consider episodic memory simply triples in time.",
                    "label": 0
                },
                {
                    "sent": "So if we have here.",
                    "label": 0
                },
                {
                    "sent": "That ends our model for semantic memory and episodic memory simply has one more dimension, which is, which is time, and I'll give you an example in a moment, and the factorization is also very simple to generalize, so here we had three latent representations with subject predicate object.",
                    "label": 0
                },
                {
                    "sent": "Here we have four, an additional one for time, which just and.",
                    "label": 0
                },
                {
                    "sent": "So what does it mean?",
                    "label": 1
                },
                {
                    "sent": "It stands for all the things which happened at that time instance.",
                    "label": 0
                },
                {
                    "sent": "Essentially, if you want.",
                    "label": 0
                },
                {
                    "sent": "So that he.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He would be an example and a model similar to the one I showed before, so we just add one more input, one more index here, one related notation and this could then represent On this date check was diagnosed with diabetes and again you can play different games.",
                    "label": 0
                },
                {
                    "sent": "You can say On this date check was diagnosed with what or you can say what happened On this date and then we should get the response back.",
                    "label": 0
                },
                {
                    "sent": "Was diagnosed with diabetes so you can go forth and back and do conditional stuff an.",
                    "label": 1
                },
                {
                    "sent": "Very flexible model, and so there's some technical technicality in learning.",
                    "label": 0
                },
                {
                    "sent": "We assume a Bernoulli distribution, which gives you the trouble of probability for querying the driver model, which is probably distribution with subject predicate object.",
                    "label": 0
                },
                {
                    "sent": "To be able to do all this conditioning technical details, we apply this in a medical domain, not for example, for patient.",
                    "label": 0
                },
                {
                    "sent": "The background information would be the semantic memory like age, peak condition.",
                    "label": 0
                },
                {
                    "sent": "The sex HH and episodic knowledge graph would describe all the events which happened to the patient at the different time times because the patient visited the clinic or so.",
                    "label": 0
                },
                {
                    "sent": "How you, how costly, you define times up to the application of course.",
                    "label": 0
                },
                {
                    "sent": "And what you then can do is decision modeling.",
                    "label": 0
                },
                {
                    "sent": "So you predict the next decision by the Doctor.",
                    "label": 0
                },
                {
                    "sent": "Or you do, which is some advantages.",
                    "label": 0
                },
                {
                    "sent": "But then of course always the question is why?",
                    "label": 0
                },
                {
                    "sent": "Why should I do this?",
                    "label": 0
                },
                {
                    "sent": "Endpoint prediction is more intuitive.",
                    "label": 0
                },
                {
                    "sent": "You predict how long press the survival of a kidney, survival of a patient acuity, visual equity in a year later.",
                    "label": 0
                },
                {
                    "sent": "So that's something I think easier to accept by Doctor because they think OK prediction is sort of statistic.",
                    "label": 0
                },
                {
                    "sent": "And may be difficult to explain, but I can interpret this prediction and then I can derive from my conclusions and we have here projects on breast cancer with a University clinic in Erlangen, nephrology with charity and age related macular degeneration.",
                    "label": 0
                },
                {
                    "sent": "Eye problem with the LMU clinic.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so so this.",
                    "label": 0
                },
                {
                    "sent": "And also I should say we also add a LS TM models, so recurrent neural networks on top.",
                    "label": 0
                },
                {
                    "sent": "So the memory gives you some representations for the task itself for the prediction task itself you need a model on top which uses these representations so you don't get predictions directly out of these memory models.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the way in the textbooks you find the way human memory is organized.",
                    "label": 0
                },
                {
                    "sent": "So the question is now, can this be related to human memory?",
                    "label": 1
                },
                {
                    "sent": "Important here is declarative memory or explicit memory.",
                    "label": 0
                },
                {
                    "sent": "That's the stuff you can easily talk about.",
                    "label": 1
                },
                {
                    "sent": "For example, semantic memory is the stuff you know like Obama was expected of United States or Munich in Bavaria, and episodic memory is about things you remember like dinner.",
                    "label": 1
                },
                {
                    "sent": "Dinner last evening or whatever you did instead of going to the lecture yesterday.",
                    "label": 0
                },
                {
                    "sent": "And then there are non declarative memories like how to learn how to throw throw ball or play tennis and perceptual memory, improving your visual equity or something like that.",
                    "label": 1
                },
                {
                    "sent": "Sensory memory are very short memory buffers, for example, that you just remember something you heard, a telephone number or something, or just the visual impression, and then the short term working memory is really the smartness.",
                    "label": 0
                },
                {
                    "sent": "On top is really is the central executive, sort of the boss in your brain and it looks at for a logical loop episodic buffer and visual spatial sketchpad to drive to make decisions.",
                    "label": 0
                },
                {
                    "sent": "So here's the magic number of seven of the seven items you can keep in your mind for decision.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very famous experiment so.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 model we're like, it's it's drive.",
                    "label": 0
                },
                {
                    "sent": "By this Taylor, Andy Sienna.",
                    "label": 0
                },
                {
                    "sent": "So here's the visual input and or other input sensor input.",
                    "label": 0
                },
                {
                    "sent": "It goes to the thalamus.",
                    "label": 0
                },
                {
                    "sent": "Everything that's sort of a relay in your brain, and then you have these different layers processing layers with more and more abstract representations.",
                    "label": 0
                },
                {
                    "sent": "So here might be just as detectors, and here you get noses and eyes like in deep neural network.",
                    "label": 0
                },
                {
                    "sent": "And then this this pattern is stored by forming a connection between an index and the hippo compost to this activation pattern, and this pattern would exactly be our latent imitation for time, which then would feed into the episodic memory.",
                    "label": 0
                },
                {
                    "sent": "In our case.",
                    "label": 0
                },
                {
                    "sent": "Of course, this only memory is formed if it's important or significant or novel.",
                    "label": 0
                },
                {
                    "sent": "And then there's a consolidation process where this memory becomes transformed into neocortex for long term storage.",
                    "label": 0
                },
                {
                    "sent": "When this happens, it happens during sleep, so you should always have a good sleep to consolidate your memory system.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this would be a technical version of this.",
                    "label": 0
                },
                {
                    "sent": "We have vision or hearing or other sensory input and something like a deep neural network, for example, which produces an output representation which then could be the basis for an episodic memory.",
                    "label": 0
                },
                {
                    "sent": "But it could also be the basis for this semantic decoding, because it can be part of this episodic memory which can decode this information in two sets of triples.",
                    "label": 0
                },
                {
                    "sent": "Now so on the right side you have an image and on left side you have.",
                    "label": 0
                },
                {
                    "sent": "There's a person as a person on a bike.",
                    "label": 0
                },
                {
                    "sent": "The bike is on the street and so on, so I think that's one of the hot overcoming big challenges in vision applications that you not just say it's a dog, it's a cat, but you also get much more semantically rich information out of these things, and this is an important addition because the model from Taylor Disenor.",
                    "label": 0
                },
                {
                    "sent": "They didn't really.",
                    "label": 0
                },
                {
                    "sent": "They called it an episodic memory, but they didn't really say how it would be declarative.",
                    "label": 0
                },
                {
                    "sent": "Or you can.",
                    "label": 0
                },
                {
                    "sent": "Would be able to get structural information out of that, so we believe this memory idea might be an interesting hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How this can be done?",
                    "label": 0
                },
                {
                    "sent": "This is where the hippocampus is.",
                    "label": 0
                },
                {
                    "sent": "Hippocampus is very important for memory and structured and a lot of theories from our point of view we just say give us an index.",
                    "label": 0
                },
                {
                    "sent": "We don't care how you do it.",
                    "label": 0
                },
                {
                    "sent": "Here's an interesting error that ended Gyrus, which is the only part only major part of the brain where new neurons are generated even when you're in adult persons.",
                    "label": 0
                },
                {
                    "sent": "So that might also indicate that you need new index capacity and hippocampus for this purpose.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this paper, we analyzed the following hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So how how can semantic in episodic memory be connected?",
                    "label": 0
                },
                {
                    "sent": "There's also a lot of speculation in a literature, but it would be difficult to have think about a mechanism where a moves to be so.",
                    "label": 0
                },
                {
                    "sent": "The obvious hypothesis would be that semantic memory is sort of an integral over episodic memory.",
                    "label": 0
                },
                {
                    "sent": "So if you have here on this day Jack was diagnosed with diabetes, then here you would just have the information deck was diagnosed with diabetes.",
                    "label": 0
                },
                {
                    "sent": "And technically, that's simple because these tensor models belong to the class of sum product networks.",
                    "label": 0
                },
                {
                    "sent": "And technically, you just essentially have a neutral input here at the time input.",
                    "label": 0
                },
                {
                    "sent": "Mathematically, would be all once, so then it means essentially at anytime when did this happen?",
                    "label": 0
                },
                {
                    "sent": "We ignore when it happened, we just want to know what happened, so this might be only an initial.",
                    "label": 0
                },
                {
                    "sent": "Mechanism where there's probably more going on, but I think it's an attractive hypothesis to think that semantic memories, episodic memory, by marginalization, step by integration.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we did experiments based on data derived from Freebase using 140,000 triples and we had different models.",
                    "label": 0
                },
                {
                    "sent": "One was on constraints on the weights, so the weights could be positive or negative.",
                    "label": 0
                },
                {
                    "sent": "Then non negative weights and then not negative and sparse where we used the L1 norm to enforce or encourage sparsity.",
                    "label": 0
                },
                {
                    "sent": "You might know it from the lasso models.",
                    "label": 0
                },
                {
                    "sent": "And then we tested the semantic model, tested on semantic data.",
                    "label": 0
                },
                {
                    "sent": "So under projected data.",
                    "label": 0
                },
                {
                    "sent": "So this gives us some feel for how good is semantic memory can be episodic, memory is episodic model.",
                    "label": 0
                },
                {
                    "sent": "As a four dimensional model tested on the episodic data.",
                    "label": 0
                },
                {
                    "sent": "So we generated 40 times slides.",
                    "label": 0
                },
                {
                    "sent": "It's out of this data set.",
                    "label": 0
                },
                {
                    "sent": "An projection means we trained the episodic model and then projected it and want to see how good the semantic memory is.",
                    "label": 0
                },
                {
                    "sent": "So here summary.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Science you always show test and training error, because in this case we almost more interested in the training performance because we want to use it as a memory and generalization is sort of the icing on top that we also can generalize to new information.",
                    "label": 0
                },
                {
                    "sent": "Also with the danger of false memories, of course.",
                    "label": 0
                },
                {
                    "sent": "So semantic trained on semantic data is a blue line over here, so let's look at this one here.",
                    "label": 0
                },
                {
                    "sent": "This degree one is episodic, trained on episodic.",
                    "label": 0
                },
                {
                    "sent": "So you see that you always need a higher rank and the reason of course is that if you at the time dimension the patterns become blurred and you need more higher rank to model the same type of data and it just looks more noisy, you need higher rank and the red one.",
                    "label": 0
                },
                {
                    "sent": "Is this projection operation.",
                    "label": 0
                },
                {
                    "sent": "So the tested on the semantic data but derived from the model on the episodic data and it sort of works, but you don't get quite the performance of if you trained it directly on the episodic model, so the important also these non negative models be cause they.",
                    "label": 0
                },
                {
                    "sent": "Only there you can do this integration step.",
                    "label": 0
                },
                {
                    "sent": "You need this non negativity in your models.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "What we did is we just reconstructed the triples and then numerically integrated them which would not work necessarily in large datasets.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another score record score where it looks a little bit nicer, but as a proof of principle I think we can show that it were.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Books.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and you can also look at the sparsity for the unconstrained model.",
                    "label": 0
                },
                {
                    "sent": "3% non negative, 30% nonnegative, sparse 58%.",
                    "label": 0
                },
                {
                    "sent": "And sparsity is important from biological point of view, because you cannot have everything connected to everything in a biological sense, so that's important.",
                    "label": 0
                },
                {
                    "sent": "We also ongoing work here.",
                    "label": 0
                },
                {
                    "sent": "One is to include start and end date, so if you get a disease maybe you get cured.",
                    "label": 0
                },
                {
                    "sent": "So this semantic memory should know that you're not sick anymore and we have a good data set and I think also getting good results there and then.",
                    "label": 0
                },
                {
                    "sent": "We also look at other four way 10th or models beyond the Tucker models we have been using here in our experiments, but there's ongoing work.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The conclusion we think their memory function might be a good starting point for understanding understanding intelligent systems.",
                    "label": 0
                },
                {
                    "sent": "So we saw that there's a sensory input and it goes into this decoder and the memory system, and then, as I said, also in the medical application, the really tight or the really smartness?",
                    "label": 0
                },
                {
                    "sent": "Or are they really useful things like prediction decision, support, optimization would be on top.",
                    "label": 0
                },
                {
                    "sent": "So the idea would be that they use the latent representations of the objects.",
                    "label": 0
                },
                {
                    "sent": "Involved for other purposes, so you can think of it at this working memory stuff is on top and it uses the memory system for all sorts of things.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we believe this concept of semantic knowledge graph episodic memory knowledge graph might be quite interesting or so in context with this perceptual decoding, and in particular the latent representations we get out of these different memory systems might be really an interesting link of symbolic and sub symbolic work.",
                    "label": 0
                },
                {
                    "sent": "Now, because we can use this real numbers then in other applications.",
                    "label": 0
                },
                {
                    "sent": "So for example in the medical application we use the latent vector for other patients as an input for this prediction model.",
                    "label": 0
                },
                {
                    "sent": "And I think it's it's in Hot Topic now.",
                    "label": 0
                },
                {
                    "sent": "People also in machine learning increasingly look at this issue.",
                    "label": 0
                },
                {
                    "sent": "Not again, when they just started looking at it, I think.",
                    "label": 0
                },
                {
                    "sent": "And then, of course, there's always the problem.",
                    "label": 0
                },
                {
                    "sent": "Can does it go both ways?",
                    "label": 0
                },
                {
                    "sent": "Can really, I mean the technical is a biology might get motivated by some of our models.",
                    "label": 0
                },
                {
                    "sent": "Think this happened before in the past, but can we learn from biology?",
                    "label": 0
                },
                {
                    "sent": "I believe yes, and one reason is that.",
                    "label": 0
                },
                {
                    "sent": "To me now this is hype on a I know that.",
                    "label": 0
                },
                {
                    "sent": "And then you question.",
                    "label": 0
                },
                {
                    "sent": "So what is AI?",
                    "label": 0
                },
                {
                    "sent": "And if you look at the functions which are realizing the brain you see first over some of many functions in cognitive literature that sometimes talk about thousands and millions of modules which do something in your brain and given time.",
                    "label": 0
                },
                {
                    "sent": "So there definitely this module so we can keep working good whatever image analysis systems and memory systems planning this in support.",
                    "label": 0
                },
                {
                    "sent": "And then there's some integration on top.",
                    "label": 0
                },
                {
                    "sent": "Which is probably still a big challenge now, because also in your brain people speculate how you make like one decision if your brain is in total chaos all the time.",
                    "label": 0
                },
                {
                    "sent": "So there's all these different submodules doing something so they're different, definitely important challenges, but I think having this analogy with brain functions makes it clearer what AI sort of is, instead of some magical thing which at some point just happens in a very large whatever deep learning network or so.",
                    "label": 0
                },
                {
                    "sent": "Wait, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So if instead write you.",
                    "label": 0
                },
                {
                    "sent": "Put in the training phase is really just the atomic triples, right?",
                    "label": 0
                },
                {
                    "sent": "So ABC.",
                    "label": 0
                },
                {
                    "sent": "So what if you would?",
                    "label": 0
                },
                {
                    "sent": "Empower the strippers with.",
                    "label": 0
                },
                {
                    "sent": "Extra bacon only tried so like.",
                    "label": 0
                },
                {
                    "sent": "Relation is typically's taken from some hierarchy of relations or concepts.",
                    "label": 0
                },
                {
                    "sent": "Again, you know some context thread, so you have in principle extra signals or extra knowledge which go beyond ID's right?",
                    "label": 0
                },
                {
                    "sent": "Which here you operate only with IDs.",
                    "label": 0
                },
                {
                    "sent": "Would this tensor representation be?",
                    "label": 0
                },
                {
                    "sent": "I would guess it this factorization will have a little bit easier drop.",
                    "label": 0
                },
                {
                    "sent": "And also well.",
                    "label": 0
                },
                {
                    "sent": "In principle you would get better answers, right?",
                    "label": 0
                },
                {
                    "sent": "Unless you this would cause some bias which you wouldn't want know here we we use that information already, but in a maybe materialized way.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                },
                {
                    "sent": "So in the first models Max did, he included all this subclass information, but he didn't.",
                    "label": 0
                },
                {
                    "sent": "There was not a special treatment for it, so it was just like any other relation and you had some propagation across this graph describing the subclass hierarchies.",
                    "label": 0
                },
                {
                    "sent": "We have some other experiments where we do the models more on the class level.",
                    "label": 0
                },
                {
                    "sent": "A bit coarser because you don't get a lot of the fine details, but sometimes that can also work quite well.",
                    "label": 0
                },
                {
                    "sent": "And but it's also a little bit part of future work.",
                    "label": 0
                },
                {
                    "sent": "For example, 1 interesting question is if you have a knowledge graph for one domain, how do you apply to a new domain?",
                    "label": 0
                },
                {
                    "sent": "And there you would probably more rely on class level similarities or something like that, but there are different different ways of doing that, and we're one of our projects that we're exploring that.",
                    "label": 0
                },
                {
                    "sent": "But I also always have a little bit bad feeling that we are not doing more in this direction.",
                    "label": 0
                },
                {
                    "sent": "But maybe we will in the future, but at this level we use that information, but it's just treated, it's just any other relation.",
                    "label": 0
                },
                {
                    "sent": "OK. One question down there.",
                    "label": 0
                },
                {
                    "sent": "Hello very interesting talk.",
                    "label": 0
                },
                {
                    "sent": "So you experiment with Freebase.",
                    "label": 0
                },
                {
                    "sent": "So do you think this marginalization over the episode episodic memory would also work very well on other knowledge bases or knowledge graphs?",
                    "label": 0
                },
                {
                    "sent": "Or is there anything or what will this depend on essentially?",
                    "label": 0
                },
                {
                    "sent": "Are there some characteristics of the data set which play a big role here?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we currently work on a real temporal database.",
                    "label": 0
                },
                {
                    "sent": "Maybe even though I don't remember the name, it's about countries and interaction between countries, so there's really time.",
                    "label": 0
                },
                {
                    "sent": "Here in our experiments, we sort of artificially introduced this time timestamp.",
                    "label": 0
                },
                {
                    "sent": "There was not a natural timestamp and we also get quite good results.",
                    "label": 0
                },
                {
                    "sent": "It's I think if you it should not be too diluted.",
                    "label": 0
                },
                {
                    "sent": "I would think now so if at every instant in time you only have one event.",
                    "label": 0
                },
                {
                    "sent": "Then factorization might might become difficult.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, in the sort of biologically oriented model you would store the latent representation of this episode and then you need some decoding system.",
                    "label": 0
                },
                {
                    "sent": "One or the other kind anyways.",
                    "label": 0
                },
                {
                    "sent": "So I think it should not be for my intuition.",
                    "label": 0
                },
                {
                    "sent": "Currently I would think it should be more than one thing happening at a time, so it should not.",
                    "label": 0
                },
                {
                    "sent": "If you haven't medical system like maybe the timestamp should not be the hour but the day.",
                    "label": 0
                },
                {
                    "sent": "So everything which happened to the day is sort of at the same time for the model and then you can still do completion without this time frame.",
                    "label": 0
                },
                {
                    "sent": "And of course also time can be dealt with in many ways.",
                    "label": 0
                },
                {
                    "sent": "Nurses across way, but it's more than what most medical people probably do the.",
                    "label": 0
                },
                {
                    "sent": "Aggregate everything into one state vector or something like that so we have a representation for the different levels.",
                    "label": 0
                },
                {
                    "sent": "Another intuition you can have if you think of you can also think of that these events are an input, for example to recurrent neural network and then so how much information do you need to get good predictive models?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it it's it's something we are also wondering a little bit and I think if you really interested in this as a memory system then the rank goes up.",
                    "label": 0
                },
                {
                    "sent": "If you have a little sparsity and we saw that also in our experiments.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Girl.",
                    "label": 0
                },
                {
                    "sent": "Yes, first thank you for the talk.",
                    "label": 0
                },
                {
                    "sent": "So it was quite nice 'cause it's a bit different or let's say unusual compared to other talks here, so I liked it very much.",
                    "label": 0
                },
                {
                    "sent": "I do actually maybe 2 questions, so the first one is.",
                    "label": 0
                },
                {
                    "sent": "How hard would it be to work with statements which are themselves on certain segment?",
                    "label": 0
                },
                {
                    "sent": "You're certain if statements like ABC's Asmara said, and you basically saying their true or false from the start.",
                    "label": 0
                },
                {
                    "sent": "What if these statements themselves have uncertainty or?",
                    "label": 0
                },
                {
                    "sent": "Probability inside them.",
                    "label": 0
                },
                {
                    "sent": "If they're local probabilities, it's not very difficult, so if if it's like a local noise on the model.",
                    "label": 0
                },
                {
                    "sent": "11 of course, straightforward way would just instead of a one within the probability.",
                    "label": 0
                },
                {
                    "sent": "It's something which can be done, but if you you have to let me get more details of The thing is like for example talking about a Doctor Who is giving a certain diagnosis.",
                    "label": 0
                },
                {
                    "sent": "He's also not certain about it, so you could say like OK, it's either.",
                    "label": 0
                },
                {
                    "sent": "I don't know it's this or it's that, but I'm not sure which one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, no, I agree.",
                    "label": 0
                },
                {
                    "sent": "I know if it's a local uncertainty we can deal with that.",
                    "label": 0
                },
                {
                    "sent": "It's it's a little bit.",
                    "label": 0
                },
                {
                    "sent": "Tricky becausw after so the dependencies are really done during the factorization and after the factorization that ripples become independent in these models.",
                    "label": 0
                },
                {
                    "sent": "So if you input that uncertain information for in training data, that should be not mean.",
                    "label": 0
                },
                {
                    "sent": "I would do it this way and then just use your confidence value as a representation for this entry.",
                    "label": 0
                },
                {
                    "sent": "We can also.",
                    "label": 0
                },
                {
                    "sent": "So the interest don't have to be binary Boolean.",
                    "label": 0
                },
                {
                    "sent": "They could also be numbers or counts would have to change the likelihood model accordingly.",
                    "label": 0
                },
                {
                    "sent": "If you want to have.",
                    "label": 0
                },
                {
                    "sent": "If there's correlation between the uncertainty, then becomes more difficult.",
                    "label": 0
                },
                {
                    "sent": "The second question was do you do anything with literals in the graph?",
                    "label": 0
                },
                {
                    "sent": "Do you deal in a special way?",
                    "label": 0
                },
                {
                    "sent": "Do you?",
                    "label": 0
                },
                {
                    "sent": "Or do you just see them as notes?",
                    "label": 0
                },
                {
                    "sent": "No way for us.",
                    "label": 0
                },
                {
                    "sent": "We treat literals as unary relations so and.",
                    "label": 0
                },
                {
                    "sent": "The best way to do that is to.",
                    "label": 0
                },
                {
                    "sent": "So if you talk, if you think about a database and as relations of different arities.",
                    "label": 0
                },
                {
                    "sent": "So all the binary ones would become three way, 10s, or the unique ones became a matrix, and you can go up and down and down is difficult, but up.",
                    "label": 0
                },
                {
                    "sent": "So essentially we would then have a 10s or a three by 10s or and a matrix, and we factorize both.",
                    "label": 0
                },
                {
                    "sent": "But we insist that they have common latent representations, so that if Jack appears in the.",
                    "label": 0
                },
                {
                    "sent": "Three way.",
                    "label": 0
                },
                {
                    "sent": "A knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "And in the NTS, simple properties and the letter representation for Jack should be the same, so there's a coupling by by the later representations and then the two factorizations can benefit from one another, and sometimes you get improved results.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, I think we have time for one more question.",
                    "label": 0
                },
                {
                    "sent": "I would just ask the next presenter to prepare.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I have another one, so just since well, semantic people are very symbolic, right?",
                    "label": 0
                },
                {
                    "sent": "So you could try to extract any kind of symbolic representation or approximation out of the tensor factor model, right?",
                    "label": 0
                },
                {
                    "sent": "So which would have some interpretation or something which, let's say, would be easy to read or represent?",
                    "label": 0
                },
                {
                    "sent": "That's a very good question.",
                    "label": 0
                },
                {
                    "sent": "We never really tried it.",
                    "label": 0
                },
                {
                    "sent": "I mean, what we did was to cluster in the later invitations and then we get the things.",
                    "label": 0
                },
                {
                    "sent": "So we use the later reservations.",
                    "label": 0
                },
                {
                    "sent": "For example for cluster analysis or to discover classes or class hierarchies and that that works very well.",
                    "label": 0
                },
                {
                    "sent": "But somehow we were never too interested in interpreting these latent dimensions themselves.",
                    "label": 0
                },
                {
                    "sent": "Other areas do that a lot.",
                    "label": 0
                },
                {
                    "sent": "For example, if you go in single processing.",
                    "label": 0
                },
                {
                    "sent": "So they're they're really into analyzing the latent factors, but they only can do it for, for example, parafac models or CP models.",
                    "label": 0
                },
                {
                    "sent": "There's a long tradition in that if they go to the Tucker models, the one we're using, it's also very difficult to interpret for them.",
                    "label": 0
                },
                {
                    "sent": "But we also never really tried to be honest, and it seems the other groups who are working in this domain also don't try it.",
                    "label": 0
                }
            ]
        }
    }
}