{
    "id": "5bs2zvsu6tjnzbt3ixpt67ydym5gqdqv",
    "title": "SigniTrend: Scalable Detection of Emerging Topics in Textual Streams by Hashed Significance Thresholds",
    "info": {
        "author": [
            "Michael Weiler, Ludwig-Maximilians Universit\u00e4t"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_weiler_signi_trend/",
    "segmentation": [
        [
            "Hi, my name is Michael.",
            "I'm a PhD student at LMU Munich and I'm one of the authors of signature and.",
            "And I want to talk about my work here.",
            "So."
        ],
        [
            "Let me start with a little example of this surreal numbers that we recorded at February 19th from Twitter's free public streaming API.",
            "And on the X axis you can see that I'm just some minutes around 11:00 AM and on the Y axis we have the term frequency for the term Facebook and the term WhatsApp.",
            "So what we would expect from a trend detection system regarding those two time series, well, I would say that the most important thing is that we are early and accurate without reporting too much noise all the time."
        ],
        [
            "So obviously these two requirements are conflicting each other.",
            "If we."
        ],
        [
            "We try to be very accurate and wait a long time, for example until slot B.",
            "We miss maybe the interesting starting point of a trend.",
            "On the other hand, if we report too early, let's say at .8.",
            "And we get a lot of noise and false alarms and lose accuracy of the overall system.",
            "So let's look at another observation."
        ],
        [
            "The line at the bottom.",
            "Shows how many times we have seen both Facebook and WhatsApp together in the same tweet.",
            "The probability to see both terms within the same tweet is generally lower than from its individual terms, and as you can see, this line is near 0 in the first half.",
            "And reaches at some point."
        ],
        [
            "Increases in some point where Facebook and WhatsApp are mentioned together in the same tweet, and that's the point in time where Facebook bought WhatsApp.",
            "So scoring pairs can provide us with additional information and can pairs.",
            "We can easily define thresholds for reporting such pairs as trending topics."
        ],
        [
            "With signature and we facing the three main issues, first we need a statistical significance score as popular topics are not the same as trending topics.",
            "For example, the term Obama is maybe mentioned all the time in the news and you don't want to see that as a trending topic.",
            "And for Twitter, you probably know the Justin Bieber effect.",
            "You don't want to see him all the time, so we have to come up with some statistical value for that.",
            "Secondly, we want to track interacting terms like we saw Facebook and WhatsApp or Snowden in Moscow.",
            "Even if single term alone is not trending.",
            "For example.",
            "Snowden was in the news quite some time before I travel to Moscow.",
            "And also Moscow is a city where a lot of things happen.",
            "So each pair alone each term alone isn't trending.",
            "But the combination is kind of new for the system.",
            "So third, we want to to have some scalability.",
            "To contract all pairs that our system will see."
        ],
        [
            "So.",
            "Our system main mainly consists of three main parts, the preprocessing part of trend analysis part and the refinement with clustering part in the stopped in the first part the preprocessing.",
            "We did some basic text mining techniques like stop at removal to remove some interesting code trends like Facebook, their Facebook and Facebook or and stuff like that.",
            "We also normalize each word to a common baseline so going goes etc is mapped to the base form goal.",
            "And we remove near duplicates as in Twitter.",
            "This is mostly the case for spam."
        ],
        [
            "In the main part, we first aggregate incoming stream data into temporal slices.",
            "We do this to get unstable frequency estimation for each time slice.",
            "So one day for news and one day for Twitter seems reasonable.",
            "In our experiments.",
            "And in the last part we finally collect these trending candidates for refinement as we together larger topics and report them to the user.",
            "So the pre processing is simple when we get and receive an tweet like Obama is going to meet Netanyahu, we first remove the stop words and therefore we get a list like Obama meets Obama.",
            "Meet Netanyahu.",
            "An from that list all possible words, words and pairs are generated.",
            "Like Obama meet Obama, Netanyahu and so on.",
            "In"
        ],
        [
            "Main detection cycle all terms and pairs.",
            "Then we then count all those pairs and we keep doing that until we reach."
        ],
        [
            "The end of the timeslice?",
            "So one hour for Twitter or one day of news and then we have a statistical update point."
        ],
        [
            "When we've done that, we calculate new thresholds and report them to our counter module."
        ],
        [
            "So that the counter module can.",
            "Next time he sees some, he counts the frequencies for incoming terms and pairs can report them as trending candidates if they exceed some threshold."
        ],
        [
            "So let's look into the update statistics module.",
            "And for a term of pair E at time slice T, we use the well known zed score.",
            "Anne.",
            "To measure how many standard deviations of frequency X is higher than its mean value.",
            "Therefore we use the previously calculated mean and standard deviation values from the previously timeslice T -- 1."
        ],
        [
            "As we operate on operate on streams, we can estimate the mean and variance by the exponentially weighted moving average and its corresponding moving variance.",
            "And this we've done in an incremental way, as shown in the work of Finch, 2009.",
            "So what we're basically doing is.",
            "We calculate the difference Delta from the current frequency and its moving mean.",
            "From the previous time, slice T -- 1.",
            "So together with the learning rate Alpha, we then can update our moving average and the moving variance.",
            "And thus we only need two Floating Points for our complete history.",
            "Instead of storing maybe thousands of records of historical data for each record in our system."
        ],
        [
            "So with this code defined, it's simple to determine whether a term or pair is trending or not.",
            "On the upper figure you can see the.",
            "Frequency of the term Facebook with its moving standard deviation.",
            "And on the lower figure is the calculated significant score."
        ],
        [
            "A given threshold is exceeded, say six Sigma.",
            "A trend candidate has been found.",
            "And doing this for one pair is easy or fun for one term, but given a stream with a high throughput, such as Twitter.",
            "We cannot track everything as there are too many combination possibilities.",
            "In our data set, for example.",
            "We've seen over 56 million stem terms and over 660 million observed pairs.",
            "And even if we look at the.",
            "Distinct ones there are seven T 1 /, 71 million and disjoint pairs.",
            "So therefore we designed an efficient hashing scheme that's based on bloom filters and heavy hitter algorithms.",
            "So that we can track our statistical score and build an probabilistic upper bound for that."
        ],
        [
            "So this hashing scheme consists of L buckets and K hash functions.",
            "For this little example, let's say we have only L buckets and we have we have only seven buckets and two hash functions."
        ],
        [
            "When we then see a term like WhatsApp."
        ],
        [
            "We calculate its hash values for each hash function.",
            "An note, it's mean values and the variants and save it into the corresponding bucket."
        ],
        [
            "In the same manner we do this for pairs like Facebook and WhatsApp to calculate the hash form appear, we simply concatenate both terms and apply a standard and string hashing function like MD54 or SH1 on it."
        ],
        [
            "And then we do the same like before.",
            "But as usual with hashing.",
            "We get some at some point of collision, So what we store into the affected buckets, the values from WhatsApp or the volume from Obama US?",
            "Well."
        ],
        [
            "To ensure that we have an upper bound, we write the maximum value into this bucket.",
            "So whatever wins in this case, this guarantees that the stronger trend always wins against the weaker one."
        ],
        [
            "And in this way, background noise is very effectively.",
            "Erased because a stronger trend always overwrites the slower values."
        ],
        [
            "And when we then try to read from our data structure for checking if an observation is trending or not, we first determine again our hash functions and the corresponding buckets.",
            "And take this time the minimum value from all buckets to ensure that we prefer the bucket with the lowest collision probability and its lowest collision values.",
            "So in this case we didn't throw away any data, it's exactly 20 + -- 10.",
            "In general we can lose here information becausw if.",
            "Some combination or single term is is getting overwritten in each bucket in each bucket has a collision.",
            "Then we maybe lose some trends.",
            "Anne.",
            "But this allows us to scale our application to streams with very high throughput as even a Raspberry Pi is capable of processing a whole day of news articles in roughly 100 seconds."
        ],
        [
            "And back to the problem with the overall writing and the collisions.",
            "Therefore, we did an experiment to test how large our hashtable must be to not lose to many trends due to collisions.",
            "And we did the following.",
            "We injected artificial trends like X123.",
            "Into a real data stream so Bama meets Netanyahu, gets injected with some random word and we then measured how often our system was capable of returning this as a trending term.",
            "So as you can see.",
            "With the hash table size L large enough.",
            "And the numbers below are two to the power of the number.",
            "That's here noted and at 2 to the power of 20 we reach some.",
            "Saturation effect that we.",
            "Do not better if we increase our hash table size more so we roughly need about 256 megabytes for that, and obviously we lose trends as you can see in the two lines below, because they injected with a low frequency and they get dominated by real trends or are treated as noise from the system."
        ],
        [
            "Then in our last step we use some refinement clustering.",
            "We use an inverted index like Apache Lucene.",
            "An for each trend trend candidate with determined before.",
            "And we repeat our complete scoring process, but this time we skip the efficient the effective hashing scheme and we can do it precisely because we only looking at some candidates a few 100 and not the millions before that we scored.",
            "For reporting we then cluster train candidates with a single link Ward clustering to present larger, more meaningful topics to the user.",
            "And in future we would like to include some topic modeling techniques and maybe some really complicated time computational intensive tasks here because we only have some candidate words and can look up the tweets or documents that containing those."
        ],
        [
            "So in conclusion, an.",
            "We talked about how millions of terms of pairs can be stored scored efficiently.",
            "With hashing scheme, because the computation computational intensive statistics calculations.",
            "Only applied to each hash bucket.",
            "Instead of doing this for all possible terms.",
            "And because there will be collisions, we have to refine all candidates to overestimated false positives.",
            "And so we repeat our scoring process.",
            "But this time we can do it because we have only some 100 remaining candidates and 99.999% we throw away because of our hashing scheme."
        ],
        [
            "So thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, my name is Michael.",
                    "label": 0
                },
                {
                    "sent": "I'm a PhD student at LMU Munich and I'm one of the authors of signature and.",
                    "label": 0
                },
                {
                    "sent": "And I want to talk about my work here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me start with a little example of this surreal numbers that we recorded at February 19th from Twitter's free public streaming API.",
                    "label": 0
                },
                {
                    "sent": "And on the X axis you can see that I'm just some minutes around 11:00 AM and on the Y axis we have the term frequency for the term Facebook and the term WhatsApp.",
                    "label": 1
                },
                {
                    "sent": "So what we would expect from a trend detection system regarding those two time series, well, I would say that the most important thing is that we are early and accurate without reporting too much noise all the time.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So obviously these two requirements are conflicting each other.",
                    "label": 0
                },
                {
                    "sent": "If we.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We try to be very accurate and wait a long time, for example until slot B.",
                    "label": 0
                },
                {
                    "sent": "We miss maybe the interesting starting point of a trend.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if we report too early, let's say at .8.",
                    "label": 0
                },
                {
                    "sent": "And we get a lot of noise and false alarms and lose accuracy of the overall system.",
                    "label": 0
                },
                {
                    "sent": "So let's look at another observation.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The line at the bottom.",
                    "label": 0
                },
                {
                    "sent": "Shows how many times we have seen both Facebook and WhatsApp together in the same tweet.",
                    "label": 0
                },
                {
                    "sent": "The probability to see both terms within the same tweet is generally lower than from its individual terms, and as you can see, this line is near 0 in the first half.",
                    "label": 0
                },
                {
                    "sent": "And reaches at some point.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Increases in some point where Facebook and WhatsApp are mentioned together in the same tweet, and that's the point in time where Facebook bought WhatsApp.",
                    "label": 0
                },
                {
                    "sent": "So scoring pairs can provide us with additional information and can pairs.",
                    "label": 0
                },
                {
                    "sent": "We can easily define thresholds for reporting such pairs as trending topics.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With signature and we facing the three main issues, first we need a statistical significance score as popular topics are not the same as trending topics.",
                    "label": 1
                },
                {
                    "sent": "For example, the term Obama is maybe mentioned all the time in the news and you don't want to see that as a trending topic.",
                    "label": 0
                },
                {
                    "sent": "And for Twitter, you probably know the Justin Bieber effect.",
                    "label": 0
                },
                {
                    "sent": "You don't want to see him all the time, so we have to come up with some statistical value for that.",
                    "label": 1
                },
                {
                    "sent": "Secondly, we want to track interacting terms like we saw Facebook and WhatsApp or Snowden in Moscow.",
                    "label": 0
                },
                {
                    "sent": "Even if single term alone is not trending.",
                    "label": 1
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Snowden was in the news quite some time before I travel to Moscow.",
                    "label": 0
                },
                {
                    "sent": "And also Moscow is a city where a lot of things happen.",
                    "label": 0
                },
                {
                    "sent": "So each pair alone each term alone isn't trending.",
                    "label": 0
                },
                {
                    "sent": "But the combination is kind of new for the system.",
                    "label": 0
                },
                {
                    "sent": "So third, we want to to have some scalability.",
                    "label": 0
                },
                {
                    "sent": "To contract all pairs that our system will see.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Our system main mainly consists of three main parts, the preprocessing part of trend analysis part and the refinement with clustering part in the stopped in the first part the preprocessing.",
                    "label": 0
                },
                {
                    "sent": "We did some basic text mining techniques like stop at removal to remove some interesting code trends like Facebook, their Facebook and Facebook or and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "We also normalize each word to a common baseline so going goes etc is mapped to the base form goal.",
                    "label": 0
                },
                {
                    "sent": "And we remove near duplicates as in Twitter.",
                    "label": 0
                },
                {
                    "sent": "This is mostly the case for spam.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the main part, we first aggregate incoming stream data into temporal slices.",
                    "label": 0
                },
                {
                    "sent": "We do this to get unstable frequency estimation for each time slice.",
                    "label": 0
                },
                {
                    "sent": "So one day for news and one day for Twitter seems reasonable.",
                    "label": 0
                },
                {
                    "sent": "In our experiments.",
                    "label": 0
                },
                {
                    "sent": "And in the last part we finally collect these trending candidates for refinement as we together larger topics and report them to the user.",
                    "label": 0
                },
                {
                    "sent": "So the pre processing is simple when we get and receive an tweet like Obama is going to meet Netanyahu, we first remove the stop words and therefore we get a list like Obama meets Obama.",
                    "label": 1
                },
                {
                    "sent": "Meet Netanyahu.",
                    "label": 0
                },
                {
                    "sent": "An from that list all possible words, words and pairs are generated.",
                    "label": 1
                },
                {
                    "sent": "Like Obama meet Obama, Netanyahu and so on.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Main detection cycle all terms and pairs.",
                    "label": 0
                },
                {
                    "sent": "Then we then count all those pairs and we keep doing that until we reach.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The end of the timeslice?",
                    "label": 0
                },
                {
                    "sent": "So one hour for Twitter or one day of news and then we have a statistical update point.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we've done that, we calculate new thresholds and report them to our counter module.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that the counter module can.",
                    "label": 0
                },
                {
                    "sent": "Next time he sees some, he counts the frequencies for incoming terms and pairs can report them as trending candidates if they exceed some threshold.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look into the update statistics module.",
                    "label": 0
                },
                {
                    "sent": "And for a term of pair E at time slice T, we use the well known zed score.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "To measure how many standard deviations of frequency X is higher than its mean value.",
                    "label": 1
                },
                {
                    "sent": "Therefore we use the previously calculated mean and standard deviation values from the previously timeslice T -- 1.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As we operate on operate on streams, we can estimate the mean and variance by the exponentially weighted moving average and its corresponding moving variance.",
                    "label": 1
                },
                {
                    "sent": "And this we've done in an incremental way, as shown in the work of Finch, 2009.",
                    "label": 0
                },
                {
                    "sent": "So what we're basically doing is.",
                    "label": 1
                },
                {
                    "sent": "We calculate the difference Delta from the current frequency and its moving mean.",
                    "label": 1
                },
                {
                    "sent": "From the previous time, slice T -- 1.",
                    "label": 0
                },
                {
                    "sent": "So together with the learning rate Alpha, we then can update our moving average and the moving variance.",
                    "label": 0
                },
                {
                    "sent": "And thus we only need two Floating Points for our complete history.",
                    "label": 0
                },
                {
                    "sent": "Instead of storing maybe thousands of records of historical data for each record in our system.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with this code defined, it's simple to determine whether a term or pair is trending or not.",
                    "label": 0
                },
                {
                    "sent": "On the upper figure you can see the.",
                    "label": 0
                },
                {
                    "sent": "Frequency of the term Facebook with its moving standard deviation.",
                    "label": 1
                },
                {
                    "sent": "And on the lower figure is the calculated significant score.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A given threshold is exceeded, say six Sigma.",
                    "label": 0
                },
                {
                    "sent": "A trend candidate has been found.",
                    "label": 0
                },
                {
                    "sent": "And doing this for one pair is easy or fun for one term, but given a stream with a high throughput, such as Twitter.",
                    "label": 0
                },
                {
                    "sent": "We cannot track everything as there are too many combination possibilities.",
                    "label": 1
                },
                {
                    "sent": "In our data set, for example.",
                    "label": 1
                },
                {
                    "sent": "We've seen over 56 million stem terms and over 660 million observed pairs.",
                    "label": 0
                },
                {
                    "sent": "And even if we look at the.",
                    "label": 0
                },
                {
                    "sent": "Distinct ones there are seven T 1 /, 71 million and disjoint pairs.",
                    "label": 0
                },
                {
                    "sent": "So therefore we designed an efficient hashing scheme that's based on bloom filters and heavy hitter algorithms.",
                    "label": 1
                },
                {
                    "sent": "So that we can track our statistical score and build an probabilistic upper bound for that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this hashing scheme consists of L buckets and K hash functions.",
                    "label": 0
                },
                {
                    "sent": "For this little example, let's say we have only L buckets and we have we have only seven buckets and two hash functions.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we then see a term like WhatsApp.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We calculate its hash values for each hash function.",
                    "label": 0
                },
                {
                    "sent": "An note, it's mean values and the variants and save it into the corresponding bucket.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the same manner we do this for pairs like Facebook and WhatsApp to calculate the hash form appear, we simply concatenate both terms and apply a standard and string hashing function like MD54 or SH1 on it.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we do the same like before.",
                    "label": 0
                },
                {
                    "sent": "But as usual with hashing.",
                    "label": 0
                },
                {
                    "sent": "We get some at some point of collision, So what we store into the affected buckets, the values from WhatsApp or the volume from Obama US?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To ensure that we have an upper bound, we write the maximum value into this bucket.",
                    "label": 0
                },
                {
                    "sent": "So whatever wins in this case, this guarantees that the stronger trend always wins against the weaker one.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this way, background noise is very effectively.",
                    "label": 0
                },
                {
                    "sent": "Erased because a stronger trend always overwrites the slower values.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And when we then try to read from our data structure for checking if an observation is trending or not, we first determine again our hash functions and the corresponding buckets.",
                    "label": 1
                },
                {
                    "sent": "And take this time the minimum value from all buckets to ensure that we prefer the bucket with the lowest collision probability and its lowest collision values.",
                    "label": 1
                },
                {
                    "sent": "So in this case we didn't throw away any data, it's exactly 20 + -- 10.",
                    "label": 0
                },
                {
                    "sent": "In general we can lose here information becausw if.",
                    "label": 0
                },
                {
                    "sent": "Some combination or single term is is getting overwritten in each bucket in each bucket has a collision.",
                    "label": 0
                },
                {
                    "sent": "Then we maybe lose some trends.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "But this allows us to scale our application to streams with very high throughput as even a Raspberry Pi is capable of processing a whole day of news articles in roughly 100 seconds.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And back to the problem with the overall writing and the collisions.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we did an experiment to test how large our hashtable must be to not lose to many trends due to collisions.",
                    "label": 0
                },
                {
                    "sent": "And we did the following.",
                    "label": 0
                },
                {
                    "sent": "We injected artificial trends like X123.",
                    "label": 1
                },
                {
                    "sent": "Into a real data stream so Bama meets Netanyahu, gets injected with some random word and we then measured how often our system was capable of returning this as a trending term.",
                    "label": 0
                },
                {
                    "sent": "So as you can see.",
                    "label": 0
                },
                {
                    "sent": "With the hash table size L large enough.",
                    "label": 1
                },
                {
                    "sent": "And the numbers below are two to the power of the number.",
                    "label": 0
                },
                {
                    "sent": "That's here noted and at 2 to the power of 20 we reach some.",
                    "label": 0
                },
                {
                    "sent": "Saturation effect that we.",
                    "label": 0
                },
                {
                    "sent": "Do not better if we increase our hash table size more so we roughly need about 256 megabytes for that, and obviously we lose trends as you can see in the two lines below, because they injected with a low frequency and they get dominated by real trends or are treated as noise from the system.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then in our last step we use some refinement clustering.",
                    "label": 0
                },
                {
                    "sent": "We use an inverted index like Apache Lucene.",
                    "label": 1
                },
                {
                    "sent": "An for each trend trend candidate with determined before.",
                    "label": 0
                },
                {
                    "sent": "And we repeat our complete scoring process, but this time we skip the efficient the effective hashing scheme and we can do it precisely because we only looking at some candidates a few 100 and not the millions before that we scored.",
                    "label": 0
                },
                {
                    "sent": "For reporting we then cluster train candidates with a single link Ward clustering to present larger, more meaningful topics to the user.",
                    "label": 0
                },
                {
                    "sent": "And in future we would like to include some topic modeling techniques and maybe some really complicated time computational intensive tasks here because we only have some candidate words and can look up the tweets or documents that containing those.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, an.",
                    "label": 0
                },
                {
                    "sent": "We talked about how millions of terms of pairs can be stored scored efficiently.",
                    "label": 1
                },
                {
                    "sent": "With hashing scheme, because the computation computational intensive statistics calculations.",
                    "label": 0
                },
                {
                    "sent": "Only applied to each hash bucket.",
                    "label": 0
                },
                {
                    "sent": "Instead of doing this for all possible terms.",
                    "label": 0
                },
                {
                    "sent": "And because there will be collisions, we have to refine all candidates to overestimated false positives.",
                    "label": 0
                },
                {
                    "sent": "And so we repeat our scoring process.",
                    "label": 1
                },
                {
                    "sent": "But this time we can do it because we have only some 100 remaining candidates and 99.999% we throw away because of our hashing scheme.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you.",
                    "label": 0
                }
            ]
        }
    }
}