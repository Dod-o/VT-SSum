{
    "id": "ctrmbx3oq33so4ljiyp3zafucknxl2fm",
    "title": "Semantic Segmentation with Second-Order Pooling",
    "info": {
        "author": [
            "Jo\u00e3o Carreira, Institute of Systems and Robotics, University of Coimbra"
        ],
        "chairman": [
            "Tinne Tuytelaars, Faculty of Engineering, KU Leuven",
            "Serge J. Belongie, University of California, San Diego"
        ],
        "published": "Nov. 12, 2012",
        "recorded": "October 2012",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/eccv2012_carreira_semantic/",
    "segmentation": [
        [
            "Hi everyone, my name is John Hayden and this is joint work with Rick Acero, George Baptist and Christian Spanish."
        ],
        [
            "I'm going to talk about semantic segmentation.",
            "This is the problem of classifying each pixel in an image into a pre learn set of categories.",
            "Here's an example from the Pascal TOC segmentation data set as well as the desired output.",
            "This problem is difficult for a couple of reasons.",
            "First of all, it is an object recognition task, so one must deal with clutter, occlusion, object articulation, etc.",
            "Additionally, the desired output is very rich at the level of individual pixels."
        ],
        [
            "We follow a bottom up approach published earlier.",
            "The general idea has been already published of the pipeline.",
            "First we sample candidate object regions.",
            "Then we perform region description and classification and finally construct the full image labeling from selected regions."
        ],
        [
            "I'll first describe the individual components.",
            "First, we sample candidate object regions using the constraint parametric milk, it's algorithm.",
            "This computes multiple figure ground segmentation is constrained to different positions and scales in the image, and usually it generates some regions among a set which align reasonably accurately with the objects in the image.",
            "This provides sufficient context for later recognition process.",
            "After"
        ],
        [
            "Words we describe each of the regions and classify them and select the subset for constructing the full image labeling."
        ],
        [
            "Here we use a simple approach where we sequentially overlay the regions on the image in some order as I'll illustrate now."
        ],
        [
            "While all the stages of this process are important, the focus of this work is on the second stage, region, description and classification."
        ],
        [
            "Alright, so currently most successful approaches for semantic segmentation use variations of bag of words and hog to describe the segments.",
            "This however, require expensive classifiers with nonlinear kernels for good performance, which incur a heavy burden during both training and testing.",
            "Additionally, these features were discovered elsewhere in sliding window detection applications and image classification, so one may wonder if there aren't any better descriptors or descriptors very suited for regions for describing regions."
        ],
        [
            "We started our quest for better region descriptors by revisiting the frame of framework of aggregation based descriptors, or at least that's what we call.",
            "This framework has three stages, local feature extraction and coding which are done once for the whole image, and then there's a pooling stage which generates a descriptor for each individual region.",
            "I'll now give some more details on this.",
            "First, usually one computes or does."
        ],
        [
            "Then sift extraction over the whole image at a grid."
        ],
        [
            "Then the local descriptors are mapped into a usually higher dimensional space, where each dimension reflects some degree of Association between the local feature an elements of a codebook which is learned that priority using K means or some other technique."
        ],
        [
            "Finally, the role of pooling is to produce a summary of the code of local features inside each region.",
            "A single descriptor that can be fed to a regular classifier."
        ],
        [
            "Now up to this day, everybody seems to be computing 1st order statistics of the code at local descriptors, either using average or Max."
        ],
        [
            "So here's 2 examples of descriptors arising from this framework.",
            "The typical bag of words which uses hard vector quantization in decoding stage and average pooling in the pulling stage.",
            "And there are many other sparse coding approaches who do sparse coding obviously, and then use Max pooling."
        ],
        [
            "So most research so far has focused on coding and there are many approaches for coding such as hard vector quantization, kernel code booking coding, sparse coding, feature encoding, etc.",
            "I'm pretty sure in this conference there's a few more already.",
            "However, bowling has been somewhat neglected so far.",
            "Basically, the only two operations used are Max and average.",
            "This takes us to the main direction we pursued here."
        ],
        [
            "Which is to obtain richer statistics in the polling."
        ],
        [
            "Sage.",
            "As I've already explained, most methods so far use either the average or the Max over each individual dimension of the local coded features."
        ],
        [
            "Here we make a transition to the 2nd order statistics.",
            "We defined analog operations of average and Max over the outer product of each local feature with itself.",
            "The main idea is to capture the correlations between every pair of dimensions of the local features."
        ],
        [
            "So one difficulty faced by this approach is that the dimensionality of the resulting descriptor grows quadratically with dimensional of the local descriptors.",
            "We cope this by basically."
        ],
        [
            "Bypassing the coding stage and directly pulling the raw local descriptors."
        ],
        [
            "Now what can we say about these matrices?"
        ],
        [
            "First of all, they're both symmetric, so we can simply discard either the upper or lower triangle without losing any information.",
            "But there's more.",
            "In the case of average, the resulting matrices are symmetric positive definite, so and these matrices have a rich G."
        ],
        [
            "Emma Tree, the former remaining manifold, however, Linney."
        ],
        [
            "The classifiers ignore this additional geometry.",
            "This can be fixed by embed."
        ],
        [
            "The manifold into nuclear space.",
            "The usual solution to do this is by flattening the manifold locally by projecting to local tension spaces.",
            "However, this is only valid in a local neighborhood in general."
        ],
        [
            "Recently a special metric has been developed locally and which allows one to directly embed the whole manifold into nuclear space by projecting to the tangent space at the identity matrix.",
            "This is a very efficient operation, which requires just a matrix logarithm, which, while being cubic in the size of the matrix with the data we use, it takes less than 100th of a second."
        ],
        [
            "OK, so now I'm ready to present the whole sequence of operations of our proposed pooling approaches.",
            "First, we compute the 2nd order statistics, either using the average or the Max.",
            "Then second step, we select the upper triangle and convert it to a vector and the final step that we found useful third step is to apply the power normalization.",
            "The idea of this normalization is to reduce the sparsity of the resulting descriptor by increasing the relative magnitude of values close to 0.",
            "This has been shown to be useful in a wide variety of tasks using linear classifiers and we also found it to be like that in your case.",
            "Then we."
        ],
        [
            "And feed the resulting descriptive to linear classes."
        ],
        [
            "Fire.",
            "Alright, so besides better pooling methods, we also explored briefly the use of better local descriptors.",
            "To do this, we're enriched standard descriptors such as SIFT, with some additional information, in particular with two types of addition."
        ],
        [
            "Information.",
            "First, we obtain the XY and S, so the width of a Patch associated to each local descriptor.",
            "We normalize these values by the width and the height of the full object segmentation to get basically the relative position of each local descriptor within the object.",
            "We also extracted color information by collecting."
        ],
        [
            "The RGPLAB and HSV value of the pixel at the center of the Patch, and then we stacked these 15 total dimensions at the end of the descriptor.",
            "We call this the rich sift descriptor, as in so E shift."
        ],
        [
            "Alright, so now I'll present some results on experiments.",
            "We ran our experiments on the Pascal.",
            "Go see data set which has twenty categories and as well as background and comes with back with the object segmentations.",
            "Now semantic segmentation in our approach has three components, region extraction, region description and the inference procedure at the end.",
            "Here first we wanted to isolate the impact of different descriptors and classifiers and features.",
            "So we essentially try it first classifying ground truth regions.",
            "We were mainly interested in efficient linear classifier."
        ],
        [
            "Gay Shun and there's a baseline.",
            "We used hog descriptor.",
            "This got 41% point 79% accuracy."
        ],
        [
            "Then we we try 1st order pulling on the raw local descriptors using SIFT and enrich shift.",
            "And we can see that enrich shift gets a big boost over a regular shift in this.",
            "In this way and that, using the average seems to already outperform Hog Pit."
        ],
        [
            "Then we moved to 2nd order, pulling and then and now we can see that there's a big boost over first order pooling, and in this case both Max and average seem to perform well.",
            "However, in the case of average we didn't apply yet the tangent space mapping."
        ],
        [
            "Now, with attention space mapping, there's another big boost over the previous.",
            "So now we get 60, three point 85%."
        ],
        [
            "So after these preliminary test results, we move to semantic segmentation in the wild on Pascal.",
            "Go see in this case with instead of having ground truth regions as input, we used segments automatically computed using CPM C. So we sampled around 150 segments per image.",
            "Then we extract the ECF descriptors, mask this if distributors which is like sift, but with the gradients on the background mask and enrich local binary patterns on each image.",
            "We used our second order average pooling procedure with attention space mapping to generate region descriptors, and we learned the models using linear.",
            "Now these were very large scale learning problems because we had 150 segments per image and there were around 12,000 images.",
            "So this results in a total of 1.7 million examples.",
            "Finally, we used a simple inference procedure because the focus of this paper was not really on that.",
            "The focus was on region description.",
            "We simply overlaid selected segments sequentially in.",
            "So in the order of their scores."
        ],
        [
            "OK, so these are the results compared to methods from the Pascal vce 1011 challenge.",
            "So we divided.",
            "We divided the methods into comp 6 Income 5 comp 6, use additional ground truth annotations.",
            "We used ground truth annotations for all the images provided courtesy of the Berkeley Vision Group and we can see that our method gets the best result an as well as get the best result on 13 classes as well as an average.",
            "And so I would qualify that.",
            "Our method is orthogonal to many of these methods, so we provide a better feature and similar improvements could be obtained by the other methods by using our feature.",
            "Furthermore, our method used linear classes."
        ],
        [
            "Byers, while the two winners of last year's challenge, used nonlinear classifiers with exponentially exponential quest square kernels."
        ],
        [
            "So the results are the differences in speed are quite dramatic or method is 20 thousand 20,000 times faster to test and 130 times faster to train.",
            "This essentially means that we can run, train and test models with these new new features faster on a desktop computer then with the old features on a large cluster."
        ],
        [
            "We run one last experiment without segments to evaluate how this would deal if we had bounding boxes or something of that sort instead of segments on Caltech 101."
        ],
        [
            "So we use a spatial pyramid like most other methods here and linear classify."
        ],
        [
            "There's again our method got the state of the art performance compared to other aggregation based descriptors such as spatial, pyramid and efficient match kernels, and the interesting Lee.",
            "Our method is the only one which doesn't use code books."
        ],
        [
            "OK is conclusion.",
            "I have talked about our semantic segmentation approach, which has three stages.",
            "Here we focused on reaching description and shown that a promising approach for better descriptors for freeform regions is to bet on the pulling stage instead of the coding stage.",
            "So we proposed 2nd order pulling with logo click intentions based mappings which results in practical impractical aggregation based descriptors without an unsupervised learning stage.",
            "So no code books and we get higher recognition performance on freeform regions with linear classifiers and.",
            "Semantic segmentation results on video.",
            "See superior to the state of the art with models 20,000 times faster.",
            "The code is also available online to reproduce all the results, so this might be an interesting starting point for people wanting to try their ideas on VLC.",
            "I'll show now some visual results on the set of the Pascal data set on randomly sampled images, so you get to see the good the bad bad and the ugly."
        ],
        [
            "Thank you.",
            "I would like to ask about the author product, the product you offer the feature vector to the estimating matrix.",
            "So this operational involved one feature vector alone, right?",
            "So as the other coding approaches.",
            "So why you consider this as part of the pulling and not as a coding?",
            "So because it doesn't use code books.",
            "So coding you need to have some code book to associate local features with elements of the the codebook right?",
            "So here we don't need to do any unsupervised learning stage before we extract features.",
            "OK, thank you.",
            "Hi, very impressive work.",
            "I was wondering your the covariance matrix that you use for pulling.",
            "It's it's probably higher dimensional than the original feature vector that's used, right?",
            "So how much of this improvement is just going to more features?",
            "Right, that's a good question.",
            "So we also tried other features before, which were also high dimensional and this didn't seem to affect all that much.",
            "The result other people have tried very high dimensional features on this kind of data, and so these kind of methods still gets the best performance on this data set.",
            "Thank you.",
            "OK, let's let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everyone, my name is John Hayden and this is joint work with Rick Acero, George Baptist and Christian Spanish.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to talk about semantic segmentation.",
                    "label": 1
                },
                {
                    "sent": "This is the problem of classifying each pixel in an image into a pre learn set of categories.",
                    "label": 1
                },
                {
                    "sent": "Here's an example from the Pascal TOC segmentation data set as well as the desired output.",
                    "label": 0
                },
                {
                    "sent": "This problem is difficult for a couple of reasons.",
                    "label": 0
                },
                {
                    "sent": "First of all, it is an object recognition task, so one must deal with clutter, occlusion, object articulation, etc.",
                    "label": 0
                },
                {
                    "sent": "Additionally, the desired output is very rich at the level of individual pixels.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We follow a bottom up approach published earlier.",
                    "label": 0
                },
                {
                    "sent": "The general idea has been already published of the pipeline.",
                    "label": 0
                },
                {
                    "sent": "First we sample candidate object regions.",
                    "label": 1
                },
                {
                    "sent": "Then we perform region description and classification and finally construct the full image labeling from selected regions.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll first describe the individual components.",
                    "label": 0
                },
                {
                    "sent": "First, we sample candidate object regions using the constraint parametric milk, it's algorithm.",
                    "label": 1
                },
                {
                    "sent": "This computes multiple figure ground segmentation is constrained to different positions and scales in the image, and usually it generates some regions among a set which align reasonably accurately with the objects in the image.",
                    "label": 0
                },
                {
                    "sent": "This provides sufficient context for later recognition process.",
                    "label": 0
                },
                {
                    "sent": "After",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Words we describe each of the regions and classify them and select the subset for constructing the full image labeling.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we use a simple approach where we sequentially overlay the regions on the image in some order as I'll illustrate now.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "While all the stages of this process are important, the focus of this work is on the second stage, region, description and classification.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so currently most successful approaches for semantic segmentation use variations of bag of words and hog to describe the segments.",
                    "label": 1
                },
                {
                    "sent": "This however, require expensive classifiers with nonlinear kernels for good performance, which incur a heavy burden during both training and testing.",
                    "label": 0
                },
                {
                    "sent": "Additionally, these features were discovered elsewhere in sliding window detection applications and image classification, so one may wonder if there aren't any better descriptors or descriptors very suited for regions for describing regions.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We started our quest for better region descriptors by revisiting the frame of framework of aggregation based descriptors, or at least that's what we call.",
                    "label": 0
                },
                {
                    "sent": "This framework has three stages, local feature extraction and coding which are done once for the whole image, and then there's a pooling stage which generates a descriptor for each individual region.",
                    "label": 1
                },
                {
                    "sent": "I'll now give some more details on this.",
                    "label": 0
                },
                {
                    "sent": "First, usually one computes or does.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then sift extraction over the whole image at a grid.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the local descriptors are mapped into a usually higher dimensional space, where each dimension reflects some degree of Association between the local feature an elements of a codebook which is learned that priority using K means or some other technique.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, the role of pooling is to produce a summary of the code of local features inside each region.",
                    "label": 0
                },
                {
                    "sent": "A single descriptor that can be fed to a regular classifier.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now up to this day, everybody seems to be computing 1st order statistics of the code at local descriptors, either using average or Max.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's 2 examples of descriptors arising from this framework.",
                    "label": 0
                },
                {
                    "sent": "The typical bag of words which uses hard vector quantization in decoding stage and average pooling in the pulling stage.",
                    "label": 1
                },
                {
                    "sent": "And there are many other sparse coding approaches who do sparse coding obviously, and then use Max pooling.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So most research so far has focused on coding and there are many approaches for coding such as hard vector quantization, kernel code booking coding, sparse coding, feature encoding, etc.",
                    "label": 1
                },
                {
                    "sent": "I'm pretty sure in this conference there's a few more already.",
                    "label": 0
                },
                {
                    "sent": "However, bowling has been somewhat neglected so far.",
                    "label": 0
                },
                {
                    "sent": "Basically, the only two operations used are Max and average.",
                    "label": 0
                },
                {
                    "sent": "This takes us to the main direction we pursued here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is to obtain richer statistics in the polling.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sage.",
                    "label": 0
                },
                {
                    "sent": "As I've already explained, most methods so far use either the average or the Max over each individual dimension of the local coded features.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we make a transition to the 2nd order statistics.",
                    "label": 0
                },
                {
                    "sent": "We defined analog operations of average and Max over the outer product of each local feature with itself.",
                    "label": 0
                },
                {
                    "sent": "The main idea is to capture the correlations between every pair of dimensions of the local features.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one difficulty faced by this approach is that the dimensionality of the resulting descriptor grows quadratically with dimensional of the local descriptors.",
                    "label": 0
                },
                {
                    "sent": "We cope this by basically.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bypassing the coding stage and directly pulling the raw local descriptors.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now what can we say about these matrices?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, they're both symmetric, so we can simply discard either the upper or lower triangle without losing any information.",
                    "label": 1
                },
                {
                    "sent": "But there's more.",
                    "label": 0
                },
                {
                    "sent": "In the case of average, the resulting matrices are symmetric positive definite, so and these matrices have a rich G.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Emma Tree, the former remaining manifold, however, Linney.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The classifiers ignore this additional geometry.",
                    "label": 0
                },
                {
                    "sent": "This can be fixed by embed.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The manifold into nuclear space.",
                    "label": 0
                },
                {
                    "sent": "The usual solution to do this is by flattening the manifold locally by projecting to local tension spaces.",
                    "label": 1
                },
                {
                    "sent": "However, this is only valid in a local neighborhood in general.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recently a special metric has been developed locally and which allows one to directly embed the whole manifold into nuclear space by projecting to the tangent space at the identity matrix.",
                    "label": 0
                },
                {
                    "sent": "This is a very efficient operation, which requires just a matrix logarithm, which, while being cubic in the size of the matrix with the data we use, it takes less than 100th of a second.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I'm ready to present the whole sequence of operations of our proposed pooling approaches.",
                    "label": 1
                },
                {
                    "sent": "First, we compute the 2nd order statistics, either using the average or the Max.",
                    "label": 0
                },
                {
                    "sent": "Then second step, we select the upper triangle and convert it to a vector and the final step that we found useful third step is to apply the power normalization.",
                    "label": 1
                },
                {
                    "sent": "The idea of this normalization is to reduce the sparsity of the resulting descriptor by increasing the relative magnitude of values close to 0.",
                    "label": 0
                },
                {
                    "sent": "This has been shown to be useful in a wide variety of tasks using linear classifiers and we also found it to be like that in your case.",
                    "label": 0
                },
                {
                    "sent": "Then we.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And feed the resulting descriptive to linear classes.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fire.",
                    "label": 0
                },
                {
                    "sent": "Alright, so besides better pooling methods, we also explored briefly the use of better local descriptors.",
                    "label": 1
                },
                {
                    "sent": "To do this, we're enriched standard descriptors such as SIFT, with some additional information, in particular with two types of addition.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Information.",
                    "label": 0
                },
                {
                    "sent": "First, we obtain the XY and S, so the width of a Patch associated to each local descriptor.",
                    "label": 0
                },
                {
                    "sent": "We normalize these values by the width and the height of the full object segmentation to get basically the relative position of each local descriptor within the object.",
                    "label": 1
                },
                {
                    "sent": "We also extracted color information by collecting.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The RGPLAB and HSV value of the pixel at the center of the Patch, and then we stacked these 15 total dimensions at the end of the descriptor.",
                    "label": 0
                },
                {
                    "sent": "We call this the rich sift descriptor, as in so E shift.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so now I'll present some results on experiments.",
                    "label": 0
                },
                {
                    "sent": "We ran our experiments on the Pascal.",
                    "label": 0
                },
                {
                    "sent": "Go see data set which has twenty categories and as well as background and comes with back with the object segmentations.",
                    "label": 0
                },
                {
                    "sent": "Now semantic segmentation in our approach has three components, region extraction, region description and the inference procedure at the end.",
                    "label": 0
                },
                {
                    "sent": "Here first we wanted to isolate the impact of different descriptors and classifiers and features.",
                    "label": 0
                },
                {
                    "sent": "So we essentially try it first classifying ground truth regions.",
                    "label": 1
                },
                {
                    "sent": "We were mainly interested in efficient linear classifier.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gay Shun and there's a baseline.",
                    "label": 0
                },
                {
                    "sent": "We used hog descriptor.",
                    "label": 0
                },
                {
                    "sent": "This got 41% point 79% accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we we try 1st order pulling on the raw local descriptors using SIFT and enrich shift.",
                    "label": 0
                },
                {
                    "sent": "And we can see that enrich shift gets a big boost over a regular shift in this.",
                    "label": 0
                },
                {
                    "sent": "In this way and that, using the average seems to already outperform Hog Pit.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we moved to 2nd order, pulling and then and now we can see that there's a big boost over first order pooling, and in this case both Max and average seem to perform well.",
                    "label": 0
                },
                {
                    "sent": "However, in the case of average we didn't apply yet the tangent space mapping.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, with attention space mapping, there's another big boost over the previous.",
                    "label": 0
                },
                {
                    "sent": "So now we get 60, three point 85%.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So after these preliminary test results, we move to semantic segmentation in the wild on Pascal.",
                    "label": 1
                },
                {
                    "sent": "Go see in this case with instead of having ground truth regions as input, we used segments automatically computed using CPM C. So we sampled around 150 segments per image.",
                    "label": 0
                },
                {
                    "sent": "Then we extract the ECF descriptors, mask this if distributors which is like sift, but with the gradients on the background mask and enrich local binary patterns on each image.",
                    "label": 0
                },
                {
                    "sent": "We used our second order average pooling procedure with attention space mapping to generate region descriptors, and we learned the models using linear.",
                    "label": 0
                },
                {
                    "sent": "Now these were very large scale learning problems because we had 150 segments per image and there were around 12,000 images.",
                    "label": 0
                },
                {
                    "sent": "So this results in a total of 1.7 million examples.",
                    "label": 0
                },
                {
                    "sent": "Finally, we used a simple inference procedure because the focus of this paper was not really on that.",
                    "label": 0
                },
                {
                    "sent": "The focus was on region description.",
                    "label": 0
                },
                {
                    "sent": "We simply overlaid selected segments sequentially in.",
                    "label": 0
                },
                {
                    "sent": "So in the order of their scores.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so these are the results compared to methods from the Pascal vce 1011 challenge.",
                    "label": 0
                },
                {
                    "sent": "So we divided.",
                    "label": 0
                },
                {
                    "sent": "We divided the methods into comp 6 Income 5 comp 6, use additional ground truth annotations.",
                    "label": 0
                },
                {
                    "sent": "We used ground truth annotations for all the images provided courtesy of the Berkeley Vision Group and we can see that our method gets the best result an as well as get the best result on 13 classes as well as an average.",
                    "label": 0
                },
                {
                    "sent": "And so I would qualify that.",
                    "label": 0
                },
                {
                    "sent": "Our method is orthogonal to many of these methods, so we provide a better feature and similar improvements could be obtained by the other methods by using our feature.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, our method used linear classes.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Byers, while the two winners of last year's challenge, used nonlinear classifiers with exponentially exponential quest square kernels.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the results are the differences in speed are quite dramatic or method is 20 thousand 20,000 times faster to test and 130 times faster to train.",
                    "label": 0
                },
                {
                    "sent": "This essentially means that we can run, train and test models with these new new features faster on a desktop computer then with the old features on a large cluster.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We run one last experiment without segments to evaluate how this would deal if we had bounding boxes or something of that sort instead of segments on Caltech 101.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we use a spatial pyramid like most other methods here and linear classify.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's again our method got the state of the art performance compared to other aggregation based descriptors such as spatial, pyramid and efficient match kernels, and the interesting Lee.",
                    "label": 0
                },
                {
                    "sent": "Our method is the only one which doesn't use code books.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK is conclusion.",
                    "label": 0
                },
                {
                    "sent": "I have talked about our semantic segmentation approach, which has three stages.",
                    "label": 0
                },
                {
                    "sent": "Here we focused on reaching description and shown that a promising approach for better descriptors for freeform regions is to bet on the pulling stage instead of the coding stage.",
                    "label": 0
                },
                {
                    "sent": "So we proposed 2nd order pulling with logo click intentions based mappings which results in practical impractical aggregation based descriptors without an unsupervised learning stage.",
                    "label": 0
                },
                {
                    "sent": "So no code books and we get higher recognition performance on freeform regions with linear classifiers and.",
                    "label": 1
                },
                {
                    "sent": "Semantic segmentation results on video.",
                    "label": 1
                },
                {
                    "sent": "See superior to the state of the art with models 20,000 times faster.",
                    "label": 0
                },
                {
                    "sent": "The code is also available online to reproduce all the results, so this might be an interesting starting point for people wanting to try their ideas on VLC.",
                    "label": 0
                },
                {
                    "sent": "I'll show now some visual results on the set of the Pascal data set on randomly sampled images, so you get to see the good the bad bad and the ugly.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "I would like to ask about the author product, the product you offer the feature vector to the estimating matrix.",
                    "label": 0
                },
                {
                    "sent": "So this operational involved one feature vector alone, right?",
                    "label": 0
                },
                {
                    "sent": "So as the other coding approaches.",
                    "label": 0
                },
                {
                    "sent": "So why you consider this as part of the pulling and not as a coding?",
                    "label": 0
                },
                {
                    "sent": "So because it doesn't use code books.",
                    "label": 0
                },
                {
                    "sent": "So coding you need to have some code book to associate local features with elements of the the codebook right?",
                    "label": 0
                },
                {
                    "sent": "So here we don't need to do any unsupervised learning stage before we extract features.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Hi, very impressive work.",
                    "label": 0
                },
                {
                    "sent": "I was wondering your the covariance matrix that you use for pulling.",
                    "label": 0
                },
                {
                    "sent": "It's it's probably higher dimensional than the original feature vector that's used, right?",
                    "label": 0
                },
                {
                    "sent": "So how much of this improvement is just going to more features?",
                    "label": 0
                },
                {
                    "sent": "Right, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "So we also tried other features before, which were also high dimensional and this didn't seem to affect all that much.",
                    "label": 0
                },
                {
                    "sent": "The result other people have tried very high dimensional features on this kind of data, and so these kind of methods still gets the best performance on this data set.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, let's let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}