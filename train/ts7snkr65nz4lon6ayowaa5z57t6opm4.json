{
    "id": "ts7snkr65nz4lon6ayowaa5z57t6opm4",
    "title": "ImageCLEF LS-VCDT: Evaluation of multilabel image annotation incorporating domain knowledge and concept subjectivity",
    "info": {
        "author": [
            "Stefanie Nowak, Fraunhofer Institute for Digital Media Technology IDMT"
        ],
        "published": "June 19, 2009",
        "recorded": "May 2009",
        "category": [
            "Top->Computer Science->Image Analysis"
        ]
    },
    "url": "http://videolectures.net/chorusfc09_nowak_iem/",
    "segmentation": [
        [
            "First, a few words on our background."
        ],
        [
            "We are also working in the German research program, Caesars.",
            "Perhaps you remember the presentation yesterday they taught about the overall goal of this project.",
            "Our Institute is concerned with evaluation of different technologies that are developed in the core technology cluster of Tezos.",
            "These technologies refer to text recognition ontologies, user interfaces, a lot of a whole bunch of different directions of research.",
            "And I am concerned with evaluation of the image and video analysis tasks antiseize.",
            "Our goal or aim is to establish objective evaluation of the algorithms that are developed from other partners.",
            "And we are measuring the improvement over the whole lifecycle of Tezos and one of our focus is to establish an international comparison.",
            "And that's the point where we stick into the image clear.",
            "We would like to we are posing a task this year that at one side.",
            "Poses a task for the Tezos developers.",
            "Anwer Cheezies Partners will submit the results and at the other side we open the data set we opened the annotations and pose it to the whole research community community and hope that yeah, a lot of research groups are interested in this task and will submit something to this task."
        ],
        [
            "So first I will say a very few words about image clear.",
            "You were already heard a lot from Marshall Larson and Girls Jones, so I only want to say that image clears one of the evaluation tricks of clear.",
            "It started in 2003 and as you can see on the slide, the participation and the task grew over the time."
        ],
        [
            "For example, in the last year there were five different tasks in image clear where altogether 60 three groups participated.",
            "In this year in image Clean Twitter 2009 there will be 6 tasks and we are posing one of these tasks.",
            "He called it in use task because it's has several new aspects of the concept detection task that was already present and we yeah integrated a few components and I would like to tell you about these components and how we will evaluate the visual concept detection.",
            "Until today, it's 38 groups registered for this task, so we hope that we have a. Yeah, active participation and two of them are Tezos partners so that we can establish the comparison between the internal project development through the international."
        ],
        [
            "Developments So what are we doing?",
            "The main task for the participants is to annotate all photos they get with all depicted visual concepts that we post.",
            "Additionally, we provide a small ontology where these concepts are organized and we hope that most of them will use the real world knowledge that is no requirement they are allowed, but they don't have to.",
            "And we are very enthusiastic about if it's an improvement or what can we get if we use the visual concept detectors together without knowledge.",
            "I have to say a few words about the large scale.",
            "Today I learned that the number of concept can cannot compete with the number of concepts in projects like retailers and so perhaps I should rethink the word last scale in this context.",
            "It was because this is an international benchmark initiative and these context the number of concepts we're using and the number of data we're using is higher than in the years before or in comperable evaluation benchmarks like Pascal.",
            "So that was the reason why I used this word only for clarify."
        ],
        [
            "So back to the task what we are doing is we are providing photos with altogether 53 visual concepts.",
            "Annotated all photos have multiple annotations.",
            "That means we have not a classification task button annotation task you can see on the left in example this is 1 photo which was tagged with the annotation city life Outdoor Night vehicle.",
            "No persons and so on.",
            "Most of these 53 visual concepts are holistic visual concepts.",
            "That means they refer to the whole image.",
            "Some of them are more object based like persons or vehicle, but it's rather smaller.",
            "Number of these 53.",
            "Later I will come to the point that their concepts are quite objectively annotatable and some that are not so easy and for former quite subjective impression of the imitators.",
            "As I already said, we organize these concepts in a small ontology.",
            "I will also say a few words about that later and we provided the participants into annotations in two formats in normal, in plain text files and in RDF XML.",
            "And altogether we provided 5000 photos with ground truth annotations, its training set, and 13,000 photos as tests."
        ],
        [
            "So how did we acquire this data?",
            "We use the media Flickr 25,000 image datasets that was provided by Mark.",
            "You ask from the Leiden University and we wrote and tagging application where we let the photos annotated from different annotators.",
            "As you can see in the picture, there are two kind of concepts we have these checkboxes where you can optionally check one concept as present or you leave it out so it is not present and we have these radio buttons which means.",
            "One of these concepts in this list has to be annotated.",
            "After the annotation process, we also had a validation step.",
            "I will come to this in a few minutes.",
            "Altogether, I already said this said that we annotated 18,000 photos with 43 persons and the number of all persons of the photos every person annotated varied between 30 and 2500."
        ],
        [
            "Photos.",
            "If you went to the validation step, as you could see there were 43 persons in rotating these photos.",
            "They all got the definition and the guideline for each concept and we made a kind of introduction how they should annotate.",
            "But as you can imagine, it is quite difficult to make it as a job objective as possible, so we validated the photos later and we saw that there are different concepts at quite easy to annotate.",
            "For example outdoor or small group of persons.",
            "And they are quite difficult concepts.",
            "If you calculate it statistically.",
            "Overexposed and autumn were concepts that were really difficult.",
            "And were often yeah, errors were annotated so that, for example, in image was initated is Aden.",
            "But you could not see why it should be automatic, not winter or.",
            "Summer of spring and these kinds of photos we.",
            "We corrected the annotations.",
            "If you regard this in numbers you see on the right, for example from 5000 photos, we corrected 380.",
            "That were annotated as partly blurred or as death or focus and that were wrongly annotated.",
            "So we had a lot of work to do to validate the photos into.",
            "Yeah, make sure that the photos are as best annotated as we could."
        ],
        [
            "With this I want to say a few words of which groups of problems we had to face.",
            "For example, one problem was that some annotators and misunderstood photographic terms.",
            "For example, the term overexposed.",
            "You can see on the left photos that are correctly annotated with auto overexposed.",
            "And on the right you can see some photo sets were annotated as overexposed, but are.",
            "Are wrong.",
            "Another problem was the problem with bad concept descriptions in the guideline.",
            "So we would like to annotate landscape in nature images kind of wider landscape where you don't see one object, but you can see a little bit more about it.",
            "This was not that clear from the introductions from the definition, so we had to work on this.",
            "And we had problems with semantic associations.",
            "For example, imagine a photo depicting a Christmas tree in the living room.",
            "You can't see anything from the outer landscape.",
            "You only have this Christmas tree.",
            "It was, for example, annotated as winter.",
            "This is.",
            "Not wrong, but in case of the visual concept extraction, but we really tries to.",
            "Face visual coherency or visual things.",
            "In these photos you can't make this conclusion.",
            "So we all the time we had to pose as a question.",
            "What is really visible in the photo and what do I only assume?"
        ],
        [
            "So far if I have an example for you so that you can see where the problem lies.",
            "For example, you should annotate how many persons are depicted on the photos you had for.",
            "Choice is a single person is more group of person a big group of person or no person at all?",
            "Additionally, you should annotate which is a portrait photo in which not.",
            "At the same time, you had some rules you should.",
            "Well, we defined that a part of a person.",
            "For example the hand is should not be annotated as person, so.",
            "You have to stick on this then if the person was drawn then it should only be drawn in a canvas or it's only a person.",
            "It's a whole image is a canvas, in the other way round it is not defined as person and it is defined by the pottery should depict either persons or animals or both.",
            "So if you have a look at these three photos.",
            "On the right it is no person and it is no portrait, even if it's looks like a little bit, but it's it violates the rules.",
            "In the middle you have no persons at all, because it's only a path and on the T shirt of the woman you only also only see path of persons and on the left you have a small group of persons, so you don't take into account the posters and.",
            "This is really difficult and I think we could argue a lot about so my last sentence is we have to find the line where we draw.",
            "Say this is a person and this is not the person and it is really, really difficult to make this over 18,000 datasets.",
            "And yeah, we tried to.",
            "Megas as objective as Poss."
        ],
        [
            "So a few vets to to the ontology provided.",
            "It consists about 81 class is 50, three of them as a visual concepts and the other ones are kind of structural classes.",
            "For example is a classical presentation, a subclass of the representation is pottery or still live or canvas, but representation itself could not be annotated in our annotation tool.",
            "Altogether, we also have 19 object properties, for example, defining that spring, summer, autumn winter that only one of these Four Seasons can be annotated at one time while can be present at one time and not more of them."
        ],
        [
            "So we have to think about how to evaluate the results of the party presents, so they have the test data save the ground truth.",
            "They have the ontology and now we would like to know how good did they annotate the photos with the multi label scenario.",
            "What was traditionally done in the last years is to use the equal error rate in the area under curve to evaluate the concepts or how good the concepts are.",
            "Annotated, but it is not a real multi label scenario evaluation because you go from the concept and not from the photos.",
            "So we will use this as one measure also to compare to the last years.",
            "But as another measure we would like to evaluate how good is every photo annotated independent from how many annotations belong to this photo.",
            "And.",
            "To do this, we have to think about how set of labels from ground truth correlates to a set of annotated labels, and we do this by taking the hierarchy by taking the ontology knowledge and by taking some annotated agreements into account."
        ],
        [
            "So for example, so I would like to tell you a little bit more about our evaluation measure.",
            "You can see here a part of the anthology.",
            "And imagine there's a photo.",
            "It is was a ground.",
            "Truth is that it depicts trees, and one system annotates it with plants and the other one was mountains.",
            "So what we would like to have is that system one that annotated with plans is judged better than system tools that annotated with mountains.",
            "So we use this hierarchy and the links in the hierarchy to calculate the cost too.",
            "Assess how wrong is the label or how.",
            "Yeah, how high are the costs."
        ],
        [
            "Of course we are the bigger problem in the multi label annotation scenario is yeah, which set of which label correlates which one?",
            "So we.",
            "Define two sets.",
            "Which contains the labels that were not annotated in the ground truth at one time and not annotated in the predicted label set.",
            "And we calculate the costs.",
            "And we assume that the two labels with minimal costs in this hierarchy should belong together and you can see this in this formula and what we Additionally calculate in this score is the annotator agreement on the concept as a scaling factor."
        ],
        [
            "So I would like to say a few words about these annotator agreements.",
            "We have to think about what does it mean when an annotator annotates concepts.",
            "In case of the optional concepts that he only annotates the presence of a concept, or does he imitates the present and the absence of the concept, so it's quite different view.",
            "For the one of N concepts, it's easier, so he had to decide on one of these concepts.",
            "So what we did is we we annotated 100 photos by 11 persons.",
            "And yeah, compared this annotations and so all that.",
            "For example, if you only consider an optional concept as annotated, then there was a agreement while on 77% or if you would take the one of in concepts, it was about 92%.",
            "And in this step we also detected two concepts that were so difficult or so, so various built-in annotations that we deleted them that was post processed and high dynamic range image."
        ],
        [
            "So one of few words, few words more on the annotated agreements, for example, on the left you can see a photo that had a high agreement between the annotators.",
            "If you calculate all concepts as.",
            "Presence and absence annotated then it has an agreement on 99% and on the right you can see in photo with a low agreement between the annotators.",
            "So there it was quite difficult to say.",
            "For example, if there's a beach.",
            "If this is a beach photo, or if it's a landscape image or not."
        ],
        [
            "On the other side, you can have look at the concept view of the photos so we could have a look.",
            "How good were the concepts annotated throughout these hundred photos, and we for example we can hear see a problem.",
            "You can you see the categories no on the category desert.",
            "In the first case, nor was not annotated at all in these 100 in photos, so it was.",
            "All the time.",
            "It was annotated as absent and desert was annotated one time by some annotators.",
            "So you can see that the data is to less, so we have to do some more annotations there to reliably calculate an annotator agreement in these agreements should.",
            "Go into the evaluation measure so in case of a subjective concept, the costs are scaled down and the system is not judged that bad as in case of concept, that was annotated completely equal through all annotators."
        ],
        [
            "So before I come to an end, I would like to take you to the teasers image Cliff Pre Workshop.",
            "It is one day before the image for the clear conference in Corfu, and you would like to make a workshop on visual information retrieval evaluation and you are all invited to participate and to.",
            "Yeah.",
            "Send your papers to us.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, a few words on our background.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are also working in the German research program, Caesars.",
                    "label": 1
                },
                {
                    "sent": "Perhaps you remember the presentation yesterday they taught about the overall goal of this project.",
                    "label": 0
                },
                {
                    "sent": "Our Institute is concerned with evaluation of different technologies that are developed in the core technology cluster of Tezos.",
                    "label": 0
                },
                {
                    "sent": "These technologies refer to text recognition ontologies, user interfaces, a lot of a whole bunch of different directions of research.",
                    "label": 1
                },
                {
                    "sent": "And I am concerned with evaluation of the image and video analysis tasks antiseize.",
                    "label": 0
                },
                {
                    "sent": "Our goal or aim is to establish objective evaluation of the algorithms that are developed from other partners.",
                    "label": 0
                },
                {
                    "sent": "And we are measuring the improvement over the whole lifecycle of Tezos and one of our focus is to establish an international comparison.",
                    "label": 0
                },
                {
                    "sent": "And that's the point where we stick into the image clear.",
                    "label": 0
                },
                {
                    "sent": "We would like to we are posing a task this year that at one side.",
                    "label": 0
                },
                {
                    "sent": "Poses a task for the Tezos developers.",
                    "label": 0
                },
                {
                    "sent": "Anwer Cheezies Partners will submit the results and at the other side we open the data set we opened the annotations and pose it to the whole research community community and hope that yeah, a lot of research groups are interested in this task and will submit something to this task.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I will say a very few words about image clear.",
                    "label": 0
                },
                {
                    "sent": "You were already heard a lot from Marshall Larson and Girls Jones, so I only want to say that image clears one of the evaluation tricks of clear.",
                    "label": 0
                },
                {
                    "sent": "It started in 2003 and as you can see on the slide, the participation and the task grew over the time.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, in the last year there were five different tasks in image clear where altogether 60 three groups participated.",
                    "label": 0
                },
                {
                    "sent": "In this year in image Clean Twitter 2009 there will be 6 tasks and we are posing one of these tasks.",
                    "label": 1
                },
                {
                    "sent": "He called it in use task because it's has several new aspects of the concept detection task that was already present and we yeah integrated a few components and I would like to tell you about these components and how we will evaluate the visual concept detection.",
                    "label": 1
                },
                {
                    "sent": "Until today, it's 38 groups registered for this task, so we hope that we have a. Yeah, active participation and two of them are Tezos partners so that we can establish the comparison between the internal project development through the international.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Developments So what are we doing?",
                    "label": 0
                },
                {
                    "sent": "The main task for the participants is to annotate all photos they get with all depicted visual concepts that we post.",
                    "label": 1
                },
                {
                    "sent": "Additionally, we provide a small ontology where these concepts are organized and we hope that most of them will use the real world knowledge that is no requirement they are allowed, but they don't have to.",
                    "label": 0
                },
                {
                    "sent": "And we are very enthusiastic about if it's an improvement or what can we get if we use the visual concept detectors together without knowledge.",
                    "label": 1
                },
                {
                    "sent": "I have to say a few words about the large scale.",
                    "label": 0
                },
                {
                    "sent": "Today I learned that the number of concept can cannot compete with the number of concepts in projects like retailers and so perhaps I should rethink the word last scale in this context.",
                    "label": 0
                },
                {
                    "sent": "It was because this is an international benchmark initiative and these context the number of concepts we're using and the number of data we're using is higher than in the years before or in comperable evaluation benchmarks like Pascal.",
                    "label": 0
                },
                {
                    "sent": "So that was the reason why I used this word only for clarify.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So back to the task what we are doing is we are providing photos with altogether 53 visual concepts.",
                    "label": 0
                },
                {
                    "sent": "Annotated all photos have multiple annotations.",
                    "label": 1
                },
                {
                    "sent": "That means we have not a classification task button annotation task you can see on the left in example this is 1 photo which was tagged with the annotation city life Outdoor Night vehicle.",
                    "label": 0
                },
                {
                    "sent": "No persons and so on.",
                    "label": 0
                },
                {
                    "sent": "Most of these 53 visual concepts are holistic visual concepts.",
                    "label": 1
                },
                {
                    "sent": "That means they refer to the whole image.",
                    "label": 0
                },
                {
                    "sent": "Some of them are more object based like persons or vehicle, but it's rather smaller.",
                    "label": 0
                },
                {
                    "sent": "Number of these 53.",
                    "label": 0
                },
                {
                    "sent": "Later I will come to the point that their concepts are quite objectively annotatable and some that are not so easy and for former quite subjective impression of the imitators.",
                    "label": 1
                },
                {
                    "sent": "As I already said, we organize these concepts in a small ontology.",
                    "label": 0
                },
                {
                    "sent": "I will also say a few words about that later and we provided the participants into annotations in two formats in normal, in plain text files and in RDF XML.",
                    "label": 1
                },
                {
                    "sent": "And altogether we provided 5000 photos with ground truth annotations, its training set, and 13,000 photos as tests.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how did we acquire this data?",
                    "label": 0
                },
                {
                    "sent": "We use the media Flickr 25,000 image datasets that was provided by Mark.",
                    "label": 1
                },
                {
                    "sent": "You ask from the Leiden University and we wrote and tagging application where we let the photos annotated from different annotators.",
                    "label": 0
                },
                {
                    "sent": "As you can see in the picture, there are two kind of concepts we have these checkboxes where you can optionally check one concept as present or you leave it out so it is not present and we have these radio buttons which means.",
                    "label": 0
                },
                {
                    "sent": "One of these concepts in this list has to be annotated.",
                    "label": 0
                },
                {
                    "sent": "After the annotation process, we also had a validation step.",
                    "label": 0
                },
                {
                    "sent": "I will come to this in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "Altogether, I already said this said that we annotated 18,000 photos with 43 persons and the number of all persons of the photos every person annotated varied between 30 and 2500.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Photos.",
                    "label": 0
                },
                {
                    "sent": "If you went to the validation step, as you could see there were 43 persons in rotating these photos.",
                    "label": 1
                },
                {
                    "sent": "They all got the definition and the guideline for each concept and we made a kind of introduction how they should annotate.",
                    "label": 0
                },
                {
                    "sent": "But as you can imagine, it is quite difficult to make it as a job objective as possible, so we validated the photos later and we saw that there are different concepts at quite easy to annotate.",
                    "label": 1
                },
                {
                    "sent": "For example outdoor or small group of persons.",
                    "label": 1
                },
                {
                    "sent": "And they are quite difficult concepts.",
                    "label": 0
                },
                {
                    "sent": "If you calculate it statistically.",
                    "label": 0
                },
                {
                    "sent": "Overexposed and autumn were concepts that were really difficult.",
                    "label": 0
                },
                {
                    "sent": "And were often yeah, errors were annotated so that, for example, in image was initated is Aden.",
                    "label": 0
                },
                {
                    "sent": "But you could not see why it should be automatic, not winter or.",
                    "label": 0
                },
                {
                    "sent": "Summer of spring and these kinds of photos we.",
                    "label": 1
                },
                {
                    "sent": "We corrected the annotations.",
                    "label": 0
                },
                {
                    "sent": "If you regard this in numbers you see on the right, for example from 5000 photos, we corrected 380.",
                    "label": 0
                },
                {
                    "sent": "That were annotated as partly blurred or as death or focus and that were wrongly annotated.",
                    "label": 0
                },
                {
                    "sent": "So we had a lot of work to do to validate the photos into.",
                    "label": 0
                },
                {
                    "sent": "Yeah, make sure that the photos are as best annotated as we could.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With this I want to say a few words of which groups of problems we had to face.",
                    "label": 0
                },
                {
                    "sent": "For example, one problem was that some annotators and misunderstood photographic terms.",
                    "label": 0
                },
                {
                    "sent": "For example, the term overexposed.",
                    "label": 0
                },
                {
                    "sent": "You can see on the left photos that are correctly annotated with auto overexposed.",
                    "label": 0
                },
                {
                    "sent": "And on the right you can see some photo sets were annotated as overexposed, but are.",
                    "label": 0
                },
                {
                    "sent": "Are wrong.",
                    "label": 0
                },
                {
                    "sent": "Another problem was the problem with bad concept descriptions in the guideline.",
                    "label": 0
                },
                {
                    "sent": "So we would like to annotate landscape in nature images kind of wider landscape where you don't see one object, but you can see a little bit more about it.",
                    "label": 0
                },
                {
                    "sent": "This was not that clear from the introductions from the definition, so we had to work on this.",
                    "label": 0
                },
                {
                    "sent": "And we had problems with semantic associations.",
                    "label": 0
                },
                {
                    "sent": "For example, imagine a photo depicting a Christmas tree in the living room.",
                    "label": 1
                },
                {
                    "sent": "You can't see anything from the outer landscape.",
                    "label": 0
                },
                {
                    "sent": "You only have this Christmas tree.",
                    "label": 0
                },
                {
                    "sent": "It was, for example, annotated as winter.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Not wrong, but in case of the visual concept extraction, but we really tries to.",
                    "label": 0
                },
                {
                    "sent": "Face visual coherency or visual things.",
                    "label": 0
                },
                {
                    "sent": "In these photos you can't make this conclusion.",
                    "label": 0
                },
                {
                    "sent": "So we all the time we had to pose as a question.",
                    "label": 0
                },
                {
                    "sent": "What is really visible in the photo and what do I only assume?",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So far if I have an example for you so that you can see where the problem lies.",
                    "label": 0
                },
                {
                    "sent": "For example, you should annotate how many persons are depicted on the photos you had for.",
                    "label": 1
                },
                {
                    "sent": "Choice is a single person is more group of person a big group of person or no person at all?",
                    "label": 1
                },
                {
                    "sent": "Additionally, you should annotate which is a portrait photo in which not.",
                    "label": 0
                },
                {
                    "sent": "At the same time, you had some rules you should.",
                    "label": 0
                },
                {
                    "sent": "Well, we defined that a part of a person.",
                    "label": 0
                },
                {
                    "sent": "For example the hand is should not be annotated as person, so.",
                    "label": 0
                },
                {
                    "sent": "You have to stick on this then if the person was drawn then it should only be drawn in a canvas or it's only a person.",
                    "label": 1
                },
                {
                    "sent": "It's a whole image is a canvas, in the other way round it is not defined as person and it is defined by the pottery should depict either persons or animals or both.",
                    "label": 0
                },
                {
                    "sent": "So if you have a look at these three photos.",
                    "label": 0
                },
                {
                    "sent": "On the right it is no person and it is no portrait, even if it's looks like a little bit, but it's it violates the rules.",
                    "label": 0
                },
                {
                    "sent": "In the middle you have no persons at all, because it's only a path and on the T shirt of the woman you only also only see path of persons and on the left you have a small group of persons, so you don't take into account the posters and.",
                    "label": 0
                },
                {
                    "sent": "This is really difficult and I think we could argue a lot about so my last sentence is we have to find the line where we draw.",
                    "label": 0
                },
                {
                    "sent": "Say this is a person and this is not the person and it is really, really difficult to make this over 18,000 datasets.",
                    "label": 0
                },
                {
                    "sent": "And yeah, we tried to.",
                    "label": 0
                },
                {
                    "sent": "Megas as objective as Poss.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a few vets to to the ontology provided.",
                    "label": 0
                },
                {
                    "sent": "It consists about 81 class is 50, three of them as a visual concepts and the other ones are kind of structural classes.",
                    "label": 1
                },
                {
                    "sent": "For example is a classical presentation, a subclass of the representation is pottery or still live or canvas, but representation itself could not be annotated in our annotation tool.",
                    "label": 1
                },
                {
                    "sent": "Altogether, we also have 19 object properties, for example, defining that spring, summer, autumn winter that only one of these Four Seasons can be annotated at one time while can be present at one time and not more of them.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have to think about how to evaluate the results of the party presents, so they have the test data save the ground truth.",
                    "label": 1
                },
                {
                    "sent": "They have the ontology and now we would like to know how good did they annotate the photos with the multi label scenario.",
                    "label": 0
                },
                {
                    "sent": "What was traditionally done in the last years is to use the equal error rate in the area under curve to evaluate the concepts or how good the concepts are.",
                    "label": 0
                },
                {
                    "sent": "Annotated, but it is not a real multi label scenario evaluation because you go from the concept and not from the photos.",
                    "label": 1
                },
                {
                    "sent": "So we will use this as one measure also to compare to the last years.",
                    "label": 1
                },
                {
                    "sent": "But as another measure we would like to evaluate how good is every photo annotated independent from how many annotations belong to this photo.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "To do this, we have to think about how set of labels from ground truth correlates to a set of annotated labels, and we do this by taking the hierarchy by taking the ontology knowledge and by taking some annotated agreements into account.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, so I would like to tell you a little bit more about our evaluation measure.",
                    "label": 0
                },
                {
                    "sent": "You can see here a part of the anthology.",
                    "label": 0
                },
                {
                    "sent": "And imagine there's a photo.",
                    "label": 0
                },
                {
                    "sent": "It is was a ground.",
                    "label": 0
                },
                {
                    "sent": "Truth is that it depicts trees, and one system annotates it with plants and the other one was mountains.",
                    "label": 0
                },
                {
                    "sent": "So what we would like to have is that system one that annotated with plans is judged better than system tools that annotated with mountains.",
                    "label": 0
                },
                {
                    "sent": "So we use this hierarchy and the links in the hierarchy to calculate the cost too.",
                    "label": 0
                },
                {
                    "sent": "Assess how wrong is the label or how.",
                    "label": 0
                },
                {
                    "sent": "Yeah, how high are the costs.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course we are the bigger problem in the multi label annotation scenario is yeah, which set of which label correlates which one?",
                    "label": 1
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Define two sets.",
                    "label": 1
                },
                {
                    "sent": "Which contains the labels that were not annotated in the ground truth at one time and not annotated in the predicted label set.",
                    "label": 0
                },
                {
                    "sent": "And we calculate the costs.",
                    "label": 0
                },
                {
                    "sent": "And we assume that the two labels with minimal costs in this hierarchy should belong together and you can see this in this formula and what we Additionally calculate in this score is the annotator agreement on the concept as a scaling factor.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I would like to say a few words about these annotator agreements.",
                    "label": 0
                },
                {
                    "sent": "We have to think about what does it mean when an annotator annotates concepts.",
                    "label": 0
                },
                {
                    "sent": "In case of the optional concepts that he only annotates the presence of a concept, or does he imitates the present and the absence of the concept, so it's quite different view.",
                    "label": 1
                },
                {
                    "sent": "For the one of N concepts, it's easier, so he had to decide on one of these concepts.",
                    "label": 1
                },
                {
                    "sent": "So what we did is we we annotated 100 photos by 11 persons.",
                    "label": 0
                },
                {
                    "sent": "And yeah, compared this annotations and so all that.",
                    "label": 0
                },
                {
                    "sent": "For example, if you only consider an optional concept as annotated, then there was a agreement while on 77% or if you would take the one of in concepts, it was about 92%.",
                    "label": 0
                },
                {
                    "sent": "And in this step we also detected two concepts that were so difficult or so, so various built-in annotations that we deleted them that was post processed and high dynamic range image.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of few words, few words more on the annotated agreements, for example, on the left you can see a photo that had a high agreement between the annotators.",
                    "label": 0
                },
                {
                    "sent": "If you calculate all concepts as.",
                    "label": 0
                },
                {
                    "sent": "Presence and absence annotated then it has an agreement on 99% and on the right you can see in photo with a low agreement between the annotators.",
                    "label": 0
                },
                {
                    "sent": "So there it was quite difficult to say.",
                    "label": 0
                },
                {
                    "sent": "For example, if there's a beach.",
                    "label": 0
                },
                {
                    "sent": "If this is a beach photo, or if it's a landscape image or not.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other side, you can have look at the concept view of the photos so we could have a look.",
                    "label": 0
                },
                {
                    "sent": "How good were the concepts annotated throughout these hundred photos, and we for example we can hear see a problem.",
                    "label": 0
                },
                {
                    "sent": "You can you see the categories no on the category desert.",
                    "label": 0
                },
                {
                    "sent": "In the first case, nor was not annotated at all in these 100 in photos, so it was.",
                    "label": 0
                },
                {
                    "sent": "All the time.",
                    "label": 0
                },
                {
                    "sent": "It was annotated as absent and desert was annotated one time by some annotators.",
                    "label": 0
                },
                {
                    "sent": "So you can see that the data is to less, so we have to do some more annotations there to reliably calculate an annotator agreement in these agreements should.",
                    "label": 0
                },
                {
                    "sent": "Go into the evaluation measure so in case of a subjective concept, the costs are scaled down and the system is not judged that bad as in case of concept, that was annotated completely equal through all annotators.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before I come to an end, I would like to take you to the teasers image Cliff Pre Workshop.",
                    "label": 0
                },
                {
                    "sent": "It is one day before the image for the clear conference in Corfu, and you would like to make a workshop on visual information retrieval evaluation and you are all invited to participate and to.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Send your papers to us.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}