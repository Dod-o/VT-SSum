{
    "id": "jiwlrt2f5w4g3guqvlsztshxv7xu3u2i",
    "title": "Free Energy and Relative Entropy Dualities: Connections to Path Integral Control and Applications to Robotics",
    "info": {
        "author": [
            "Evangelos Theodorou, University of Southern California"
        ],
        "published": "Oct. 16, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Physics->Statistical Physics",
            "Top->Mathematics->Control Theory",
            "Top->Computer Science->Robotics"
        ]
    },
    "url": "http://videolectures.net/cyberstat2012_theodorou_free_energy/",
    "segmentation": [
        [
            "It's my pleasure to be in a sad same mathematical in client audience and try to present work on robotics.",
            "So buckling internal mechanics I guess from quantum mechanics an maybe you guys are a little bit tired, so I have some some videos too.",
            "So everybody I guess likes videos and movies.",
            "So I just want to from the very second slide.",
            "I want to explain to you where I'm coming from and I believe there has been always say.",
            "Debate between applications and theory and how much theory we can put two applications we have on one side, very complex robots.",
            "An demand is always to be able to use these robots to perform very complex tasks.",
            "On the other hand, we have very beautiful theories, so there are different kinds of beauty here and we would like to somehow bring them together.",
            "So here is where robotics today stands.",
            "This is a.",
            "Project that involved locomotion and it is basically it is a project that it was a competition between 6 universities in the United States, MIT, CMU, Stanford, USC, Upenn.",
            "I think I forgot.",
            "I want to universities.",
            "So I think Stanford, USC, CMU, MIT and went through the third phase, so that was a five years six years project and it involved the engineering of 10 people working on this project right now.",
            "There is no one framework that actually works here.",
            "I mean there are many different kinds of tools there because there are all different kinds of problems.",
            "Alright, we have estimation, you have control.",
            "And so I truly believe that when I when I first read Captain's paper on 2005 on statistical mechanics, an I was with these fellows at here.",
            "see I thought how we can bring nice theories from stochastic optimal control and apply to real robotics.",
            "So even if we cannot really make our algorithms to work very well in an automatic way.",
            "I think I still believe that even if you can reach the 50% over the performance that these guys have reads in two years, that has a lot of value, so there must be some dialectic relationship between theory and practice.",
            "In the sense that you know I mean, somehow I believe that we should bring them together and so.",
            "But what is the thing that makes?"
        ],
        [
            "The robotic control difficult, right?",
            "And so I think from the control theoretic point of view, robotic systems are systems that have is a nightmare to work with an.",
            "I think quantum mechanical systems, but back to the Newtonian world I think we have issues that start from the fact that we just don't have very good models.",
            "OK, and sometimes it's very difficult to identify models.",
            "Also, there is some level of uncertainty in terms of representing tasks, right?",
            "We don't have good models to represent all the world and every every task that we would like to do.",
            "Definitely there if we have models.",
            "These models are nonlinear and they have friction.",
            "There are multiple dynamics they involved contact at the same time.",
            "There is a issue of high dimensionality wybie cause these systems have many degrees of freedom, but when I say high demand high dimensionality, I do not only mean dimensionality with respect to the degrees of freedom of the underlying dynamical system, but in fact if you try to apply optimal control.",
            "And reinforcement learning.",
            "There is a level of dimensionality with respect to how many parameters you want to really optimize in order to perform the task.",
            "And then there is the issue of under actuation with.",
            "The experiments and the demos which I'm going to show today do not involve Underactuated robotics, but this is also a issue like we don't control all the degrees of freedom."
        ],
        [
            "So."
        ],
        [
            "Um?"
        ],
        [
            "So then."
        ],
        [
            "My talk is going to have a little bit of theory in the beginning and try to explain what are the algorithms that we developed.",
            "Motivated and inspired by the work on personal control.",
            "And then I'm going to show some applications to robotics.",
            "How we can apply this?",
            "How we have applied this tools to robotic systems and then I'm going to talk a little bit about more recent work.",
            "This is a work that we have been doing 'cause I just wanted really to.",
            "I realize that at some point my knowledge was limited in terms of stochastic optimal control, and I really wanted to write the paper where I can see what are the connections between the particle controlling information theory and we have a CDC paper on that, so I'm just going to."
        ],
        [
            "Start with a very basic formalization of stochastic optimal control.",
            "May make sure that everybody is on the same page, and I'm going to go very fast for this.",
            "So a typical stochastic optimal control problem we have minimalization of the cost function with dynamics for this particular case are fine in controls we have cost functions quadratic in controls an we have this cross term here between States and controls.",
            "We apply the bellman."
        ],
        [
            "Principle, we take the handles are called Bellman equation.",
            "We get the optimal controls right.",
            "Very standard steps and then we have the feedback control here.",
            "So what this equation says that the controller should move the dynamics towards direction of state space where the value function is minimum right?",
            "Because it goes minus the negative of the gradient of the value function.",
            "And then when you plug this you back to the hundreds or complement equation you get.",
            "This second order nonlinear PDE.",
            "Where you have this terms right now, Q~ and F~ right which appear here an I think that PD shows up.",
            "So what in a in another talk?",
            "Now the question is that you want to the stories that you would like to be able to find this value function in order to find the control here.",
            "And there is this."
        ],
        [
            "Exponential transformation of the value function.",
            "You get this PSI.",
            "Which is the desirability?",
            "Function you have this connection between the control cost and variance of the noise and if you do this connection transformation and you have this assumption there, it turns out that 100 billion equation collapse the Barcode Summon Command graffiti.",
            "Then with this terminal condition, if you apply the appointment Klima here that basically tells you that you can actually find this site for every state and time if you take the stochastic financial equation.",
            "Propagated in time.",
            "Get all these trajectories here.",
            "Evaluate this expectation and now.",
            "It turns out that the control is given by this expression.",
            "Here we have minus plus, minus minus plus wybie 'cause we want to minimize.",
            "We want to maximize size, right?",
            "We did the logarithmic transformation, so size that's the reason why it's called the desirability function.",
            "So so far so nothing new.",
            "And then since V is a value function.",
            "And it is given by this expression.",
            "This expectation here is evaluated over the uncontrolled dynamics.",
            "This is what the notation means.",
            "The expectation with respect to zero and then that is the lower bound of the initial cost function, right?",
            "So when we."
        ],
        [
            "K so there is a very fundamental distinction in.",
            "Reinforcement learning with actually distinguishes between value function, approximation methods and policy gradient methods, and So what I'm going to be talking today is mostly about policy gradient methods, and so one of the main idea here is that you have a you sample the full state space you want to be able to approximate the value function.",
            "That means that you have to visit all the state space almost all the state space and.",
            "There is always the issue of scalability, but there is a very interesting connection between the Bellman error.",
            "The error between the approximated value function and the real value function and how this error translates to the error in the policy.",
            "So it turns out that there is no more tnan monotonic.",
            "Relationship and in fact that is a very toy example here where this is the true value function.",
            "This is the.",
            "Hey, the best feed that you could get to the true value function and this is a fit that is not as good as this one, but as you can see the optimal control that you you can get from this bad feet to the value function does better than this one.",
            "OK and that is somehow an indication that that was basically the motivation in the area of machine learning for people to control to work in on the area.",
            "Policy gradient methods and he sends parameterising the value function.",
            "Now the idea is that you want to be able to parameterise the controls alright.",
            "It does, regardless of whether you have a deterministic or stochastic.",
            "Well, in case of this low caste system the noise.",
            "I mean we know that.",
            "So the value function is going to be smoother.",
            "So in case of the Terministic system which means that may be easier to approximate OK in the terministic system you may not have this smoothness, which means that you may have to use more more basis functions.",
            "Reading the arguments, I usually sort of go for a deterministic.",
            "Not a source control.",
            "Greedy you mean inside the policy creating framework we go yes they call for a germanistik control.",
            "But in that case is fastest.",
            "Excuse us exploration wise.",
            "Yeah, but in case that the optimal policy is actually undecided.",
            "The case where the optimal policy is.",
            "Like in the delayed choice crazy.",
            "Yes, I don't want to go to the park.",
            "Well in this case, yeah, in this case I policy gradient will actually most likely suffer.",
            "In fact, the policy gradient cannot work.",
            "You know there is no freelance, right?",
            "You have to be able to provide it with an initial policy that is relevant with respect to the task that you want to perform.",
            "Now, how good is this initial policy and how good and how much you are optimizing?",
            "That is, I think that's dependent, but what you will see in this videos, I think that.",
            "You can do a very good job actually.",
            "You can start with a very naive initial policy and do very good optimization alright.",
            "So."
        ],
        [
            "Oh so now when I read birds paper.",
            "The first question that I asked is how we can use this framework, and so the first thing that we try to do is think about this G here as a.",
            "This is the control transition matrix.",
            "OK, but thing as G as GT dot times 5 so it could be actually all these U as something that we can actually parameterized.",
            "So first what we wanted to do was to actually derive a path.",
            "Control framework for the cases where this guy here was a function of the state because we wanted really to see how the equations look like, and I think we've got a very nice feedback from birth capping in our initial derivation.",
            "And so it turns out that the control is given by this formulation.",
            "Remember that this guy here is the term in the cost function that appears in the cross term between the states in the controls.",
            "And now we have the expectation.",
            "All this time you out this is the path in the graph and this term UL is given by this expression here, right?",
            "So it has again the same flavor of of sampling from the order of the fact that the control is a function of the first direction of the noise.",
            "But there are these terms with with.",
            "Appear and so you have the probability of the path and this is basically where S is the path of the cost.",
            "OK, so one nice thing with the passing of control is that because you know from this you sample based on the uncontrolled dynamics.",
            "Actually this term here can be proportional to the noise that you were using in order to sample, and so the drift of the dynamics of the dynamics.",
            "It doesn't really appear in your calculations and so you can use this method as a Model 3 way.",
            "So you don't need to actually know the true dynamics in order to apply, but integral control.",
            "You just have to sample forward in time.",
            "So after we did this derivation."
        ],
        [
            "So then the second question that we ask was how we can actually now do it in a tentative way, because there is no actually this.",
            "This equation tells us that you know the partner control.",
            "It is a feedback policy depends on X&T.",
            "It's a finite horizon optimal control problem, which means that we have to go to every state at every time sample and compute this quantity.",
            "But that approach with actually not scaling a high dimensional state space.",
            "So we wanted to see how we can actually.",
            "Do the whole particle control in the iterative fashion OK, and so we started with the solution by the Feynman Kalama or they which gives us the desirability function and what we did is something very simple.",
            "We did important sampling.",
            "So basically you have you have here this the use you have the initial dynamics, which are the uncontrolled dynamics.",
            "But let's say that we are at iteration K and now this is the probability measure that corresponds to the control dynamics at iteration K. Alright, so this is the Radom Nikodym derivative which is given in the exams theorem.",
            "So you can actually derive a profit.",
            "The particle control the therapy facility or control, and it turns out that this is the formula that you get now.",
            "This why term.",
            "If you multiply it with G is going to collapse, so this term can become actually much simpler.",
            "But I'm not going to talk about different formulations.",
            "This is just one form.",
            "The more general from my point of view and now here you have the new.",
            "Laughing the girl when you P this is a P~ which is which has the path cost and now you have this correction term that comes from Gibson's theory alright.",
            "So."
        ],
        [
            "So how we're going to apply it right now?",
            "So far I haven't really say anything about OK, this is the iterative version, but we're still actually in the previous formulation.",
            "We have to visit any state in the state space for.",
            "Propagate the dynamics forward.",
            "Sample the dynamics forward in order to be able to find the party, the party control even in the iterative case.",
            "So typically in robotics we have systems of this form, right?",
            "So imagine that this Q is the joint position and joint velocities.",
            "This M here is the inertia.",
            "Immersive this C is the the term that has to do with Coriolis forces, gravity and the controls.",
            "OK, so it's nothing.",
            "It's basically a application of Newtons law.",
            "And since this system is second order system, meaning that it has position and velocity, one thing that we're going to do is we're going to use control in this control is in PV four.",
            "OK, so we're using a P controller alright, and so the big game that game and we have the, so the desired trajectory's and the current trajectory.",
            "OK, so this that is at rejecting the desired velocity.",
            "So what we're going to do is we're going to use.",
            "Nonlinear poke point attractors with our differential equations that can represent smooth trajectories.",
            "So, if I give you the initial position and the target, you can, you can use this differential equation in order to represent trajectories between these two points, and in fact, so this this this nonlinear pointer trackers have this form.",
            "We can think about this X as a phase variable Huawei.",
            "Has the position and set their velocity and now we."
        ],
        [
            "I have this."
        ],
        [
            "3rd"
        ],
        [
            "And here this is a goal term.",
            "This is the first term and the first term is nothing else than is given as a as a multiplication of a function approximator.",
            "In up a function of the phase variable and parameter Theta.",
            "Alright, so by changing the parameter Theta I can actually deviate change the shape of my trajectories alright and I can use that in order to represent the desired trajectory's OK and.",
            "I can do the same trick.",
            "I can use the same trick in order to represent the gains alright and that is important because you know, sometimes we want to do both planning and gain scheduling on a real system.",
            "So we would like to be able to plan trajectories, but at the same time being able to change the sensitivities in the games.",
            "OK, we want to have compliant control, which means we want to change the game at every time step and.",
            "So now we are going to treat this parameters here as the new control parameters.",
            "Of course, we augment their dynamics alright, and augmenting their dynamics is not good, because that means that you change the controllability characteristics of your of the system, right?",
            "You are augmenting the order of the assistant.",
            "But but on the other side, you have a very good control of what your system is going to do, because basically you have this very nice smooth trajectories.",
            "And in robotics in general we want to have smooth trajectories in joint space, right?",
            "We don't want to have trajectories that are very noise right now.",
            "One may ask well what happens if you don't know the goal of movement.",
            "So it turns out that we have done some work on this.",
            "You can actually make this goal one extra parameter state.",
            "And actually earn the goal by having an extra state, OK?",
            "So the the idea is very simple."
        ],
        [
            "And it has been.",
            "Around for quite a long time ago, I guess you know, for the last 10 years and this is just a very."
        ],
        [
            "Simple demonstration here is just.",
            "Human having an X killer done and if it makes one movement an from vector skeleton we can get his join.",
            "Angles and then we can demonstrate that to the robot.",
            "But because you know you can always change the goal of the movement, you can actually have the robot perform these tasks in different goals.",
            "OK, that was the very first application of these tools to robotic, so right there is no any control really here.",
            "I mean it's a PD control, it's fixed.",
            "That was the very first thing.",
            "OK, now it may be very simple, but I mean you will see what we can do with it."
        ],
        [
            "Alright."
        ],
        [
            "And so again."
        ],
        [
            "In a classical block diagram, think.",
            "He must picture when we when we say planning.",
            "Usually in robotics we mean that we want to be able to change the desired trajectory's of the system.",
            "When we say gain scheduling, we mean that we want to be able to change the control gains of the underlying dynamics as a function of time.",
            "And it turns out that that works very well even for nonlinear systems alright."
        ],
        [
            "So the so now I'm just getting what was derived from the iterative path integral control.",
            "I have this setup parameters and I treated and I treat them as my new controls and I just ignore that the control is a function of the state it does.",
            "I'm just going to pretend that you is only a function of time.",
            "OK, now that is not something that control theoreticians would like, but on the other hand.",
            "If you have a view effects of tea, you have to tell me how I can actually sample the state space on 20 degrees of freedom robot and being able to do actually feedback control at the same time.",
            "Many in many cases we may actually not be able to move the robot in all different configurations becausw the systems you know many times are not controllable in configuring some specific configurations and unstable so.",
            "So."
        ],
        [
            "The algorithm is very simple, so you want to go from this start state to the goal state and what you do is you just simulate or execute on the real system.",
            "Trajectory's all right and then what you do is you go at every state at every time.",
            "Slice in time of the trajectory you find.",
            "What is the cost to reach the.",
            "The terminal, the terminal State, and then you.",
            "The Glasgow particle control what you would do is you will find the particle control here, then move the system for example.",
            "Again OK and do it again and again but that is very slow and as.",
            "And as I said, in many cases requires that you have to be able to sample the full state space.",
            "So instead what we will do is we will find."
        ],
        [
            "What is the optimal path integral correction in Delta, Theta, right?",
            "And then we're going to do this?"
        ],
        [
            "Same thing for the next.",
            "Time in the time horizon OK?"
        ],
        [
            "Until basically."
        ],
        [
            "If we reach the terminal, the.",
            "Until the end of the trajectory, then we are going.",
            "We are going to."
        ],
        [
            "To update our fitness because we have found our Delta headers here based again on this pattern control formulation and we're going to repeat this process again, right?",
            "We're going to use this data, put it on our system, resemble again on the real physical system.",
            "OK, and find the new correction in in Delta Phi to the controls.",
            "To convergence."
        ],
        [
            "So."
        ],
        [
            "So."
        ],
        [
            "Now.",
            "That was we have apply that to very simple systems, let's say 20 or 50 degrees planar manipulators, and we have we had a paper.",
            "We compare that to very many classical enforcement learning algorithms and we find out that it works very well.",
            "It is actually faster if you want I can solve this data, but for the purpose of time I just want to show the very first video that we got and that was that.",
            "Make me, you know, excited.",
            "I was thinking that.",
            "Well, maybe there is a hope we can actually continue working on this and so the idea is here that you have this little dog robot.",
            "This is actually the simulator of a real actual robot that we had, that the simulator that people were using at that time.",
            "And so all the experiments on the real robot had to take place first on this simulator.",
            "OK, and so the idea is that you want to be able to jump from this gap.",
            "And the cost is very simple.",
            "You want to have you want to keep their role and they heal.",
            "You want to penalize with.",
            "Somehow fixed, you want to penalize acceleration right?",
            "And at the same time you penalize this is going to your terminal state.",
            "You have a goal state and you penalize with respect to the distance and so it turns out that you know if you.",
            "Here a you know like it can.",
            "It is basically able to jump down so that so that's a very primitive, so that so that was a very primitive task.",
            "OK an it's not as you know elegant as demos videos, but this is what we did like three years ago and that was our first indication that this algorithm actually may work on any other robot then the next thing that we try."
        ],
        [
            "I was to actually do variable stiffness control, which means not only learn desired trajectories, but learn gains.",
            "Alright, this this is the this.",
            "This is called variable stiffness control alright, and that relates to the I to the idea.",
            "So Roger broke it at some point here in his course.",
            "He wanted to penalize the derivative of EU of U with respect to X.",
            "So in a standard PD control, if you do that, you just get the.",
            "The game and that is what you call the complexity of implementation, right?",
            "So here we actually do it.",
            "We have done it on a on a simple fandom in simulation again.",
            "So there is one more simulation and everything else is on aerial robot and the idea is that you want to go in joint space from a specific point.",
            "OK, you want to go via a point.",
            "Alright, so the cost is that you want to penalize for high control gains.",
            "You want to penalize for acceleration because you want to do that in a small state.",
            "And you want to penalize with respect to the joint angle error so there's a target in join angle space and you're going to go through this point.",
            "So what happens here is that in this row.",
            "In this role, you see the What, what happens in joint angle space?",
            "So these are the joints with a yellow.",
            "Is the target OK with the red?",
            "Is the initial trajectory and with a blue is the actual trajectory.",
            "After learning all right?",
            "And so in the 2nd row what you see are the games alright, so you see that at the time where the system goes via the target, the gains pick.",
            "These are the initial gains fixed gains alright, and then after learning you see the pic here you see the pic here.",
            "You see the picture.",
            "So the game speak only when you really need them, while at the other parts of the time horizon.",
            "Actually the game is very low.",
            "That is important, not because only we want to do compliant control, but that is important because sometimes this game these games are multiplied by the state and the state is given by Kalman filter estimator which has noise.",
            "So the higher the game you amplify the noise, so you want to be able to regulate your.",
            "Your game starts that you don't Abilify the noise and at the same time you perform the task alright.",
            "Any questions so far?",
            "Is it very simple?",
            "Intuitive."
        ],
        [
            "So now we're going to do the same thing.",
            "That is a simulated version of the Kuka robot.",
            "Do you go now?",
            "You guys know the cooker robot is a very famous manipulator these days.",
            "Is 1 of this German robots.",
            "And so it has 6 degrees of freedom.",
            "But now the only difference is that we want to go via point in the end effector respects alright, not enjoying space, and then the vector space.",
            "And this is again what happens, you know?",
            "You see again that you started from initial game.",
            "This is what happens after 30 updates with.",
            "Yellow or green?",
            "I cannot distinguish yellow with green and then and then at 100 updates you see that the game goes down and this is what happens.",
            "More mostly all join Alljoyn space OK.",
            "This is the point where it has to go in the end effector space, so the games we are going to pick, in most cases at this line here.",
            "You see it also.",
            "Effectively converting closely or control.",
            "Yes, so I'm I'm I'm I'm taking a feedback control low alright and you because of the problems that I mentioned before.",
            "Alright, I use it as an open loop control and then in order to be able to do actually real feedback control, I bought some parameterization on my controller and I treat the games and the desired states as new states again I augmented dynamics right, but that turns out to work very well.",
            "Feedback game The point of feedback is to find disturbances right well.",
            "Effectively driven open loop simulation.",
            "So the only reason these games are doing something is because you couple the whole book of the game right in the open group representation doesn't have anymore in the cost function you put as a performance to reject disturbances.",
            "Then you will get games that they will reject disturbances.",
            "No, I'm not doing that OK but it's all I mean.",
            "This is a generic issue with or without more control.",
            "Right?",
            "Depends on your performance criteria.",
            "So then."
        ],
        [
            "We moved to real robots and So what I'm going to show here is the PR2 robot that asks this is work that was done in Willow Garage with some people from our lab and the idea here is that you have this robot and what you would like to do is you have these two chopsticks and you would like to be able to roll this small box here.",
            "OK, so again we don't use any model right?",
            "And this is a task that you know we don't really.",
            "No, exactly the dynamics of this task.",
            "We don't have ways to represent them, or if if we had, that could be very complex.",
            "So we just use a simpler way of having the robot interact with the environment.",
            "Alright, get a reward and you know improve its performance.",
            "So the cost here is there is a gyro in the box that gives us.",
            "Acceleration, so we want to flip it, but we don't want to.",
            "We want to do it in a small quake 'cause if we don't penalize for acceleration then it might do something very aggressive.",
            "Then we actually penalize.",
            "We have measurements at the grippers with from the force because so we want to do that in a gentle way, right?",
            "And also we have accelleration at the gripper, so we want to penalize how fast the hands are moving.",
            "And of course there's a terminal cost.",
            "Which has to do with the desired state and the state is just flip the cube.",
            "This cube, this box and so you should penalize with expected deviations from this desired state, right?",
            "So here's what happens.",
            "OK, so.",
            "There are many trials.",
            "There is an initial policy is going to try to.",
            "And.",
            "Sometimes it loses completely.",
            "Contact with help Japan after 26 iterations here a you know.",
            "So if you compare right now, the policy that you got at the end with initial policy, you have 86% success rate.",
            "OK, well with the initial policy you had I think.",
            "This is what happens.",
            "This is how we teach it, right?",
            "Because I said we want to have an initial policy and this is the policy that we have in desired trajectories in terms of end effector position and orientation.",
            "So there is a little bit of struggle, but some subjects actually.",
            "May have made it so the neck."
        ],
        [
            "To the next task is a task that was motivated by the by studies that people did in psychophysics, so they did some studies in which they were placing an object, and so the subjects thought that the object is in a particular position, XYZ coordinates, but in fact they were making small deviations of the objects, so they wanted to see.",
            "How humans adapt into small uncertainties in and effective space?",
            "OK, and then finally they find out that the way how the that the humans can really adapt this uncertainty and they change their trajectory in order to be able to grab their object in any possible configuration.",
            "Inside a three Sigma bound on the uncertainty of position on K. So.",
            "Any questions So what happens here is that.",
            "So this is the original position, the position that the robot thinks that the object is.",
            "But in fact we have moved the object around, so there are some positions initially where the robot can really make it.",
            "But if this placement is very large up to the 6 centimeters is going to lose.",
            "Basically contact, he's not going to grab it.",
            "So this is before learning, so this is a four sending methods.",
            "Distance OK, now it barely made it.",
            "600 mothers OK. And this is what happens at the end effector space.",
            "Alright, so now here's what happens after learning.",
            "They all the robot wraps around the object so that it is able to grasp the object in any possible configuration.",
            "In this white space.",
            "Alright, so this is what this is.",
            "What happens after learning all right and this is the difference between the two end effector two behaviors and affective space alright?",
            "Bye bye.",
            "No, actually the only thing that we are using here is acceleration, because the parameters that we are using is benefactor position and wrist orientation.",
            "This is what we parameterized.",
            "If you've got to knock it out, the only thing that we say is success or or not.",
            "At that, that's closest to the robot, yeah?",
            "God still moving and not so yeah, I mean, so you're asking if we are using the sensors?",
            "Yeah, we're using the sensors in order to say you know success or failure right?",
            "In order to find out whether or not the robot has really grasped the object."
        ],
        [
            "So then.",
            "Another task was actually to have the robot play learning pool stroke and the idea here is that you want to be able to so the table is tilted and the idea is that you want to go through this point as fast as possible, alright, with maximum basically 1st, and so the robot is going to try.",
            "The table is tilted, the ball is going back with the lasers.",
            "Kind of hear the courses that you want to be able to minimize the time, which means that you want to go as fast as possible to your target.",
            "There are from the target and also the displacement of the displacement of the stroke.",
            "Cause if the if the robot is moving very fast sometimes you know the stroke and just.",
            "So this is the initial policy.",
            "Trusting proof.",
            "So this experiments take from 20 minutes.",
            "2.",
            "There are some experiments later that I'm going to show that take 2 hours or three hours.",
            "So OK, so this I guess the most clearly this is the initial policy and then.",
            "I find out please.",
            "So the idea is that we want to have this since we want to have the robot interact with the environment.",
            "And being able to improve its performance overtime."
        ],
        [
            "Now when I join him on toddlers lap, we I had the opportunity to work with tendon driven systems.",
            "These are systems which are driven by tendons.",
            "For example, our hands are tendon driven systems and our hand in fact has up to 3032 tendons.",
            "So that is already a very big number if we want to do control on a tendon driven system.",
            "So it turns out that.",
            "Um?"
        ],
        [
            "For example, just for the index finger, we have almost 11 tendence, from which we actually control 7.",
            "And there are four tendons here which are used as a spring to transfer the force from the.",
            "From the active tendence to the most distant.",
            "Ligament and so."
        ],
        [
            "We have a real tendon driven system in UW.",
            "This is the action.",
            "This is the anatomically correct desperate.",
            "It is made based on the real human hand.",
            "And it has exactly the same extensor mechanism with a human index finger.",
            "For example.",
            "The only difference for the index finger is that we have only one extension of the index finger has two extensors.",
            "There are Springs in the Motors that try to.",
            "Simulate the nonlinear passive muscle stiffness.",
            "We have 25.",
            "Tendons 'cause we have actually three fingers here and the only sensing capability that we had with this system was actually the tendon excursions so we don't have join join angles cause people in bio mechanics of the hand believe that most likely we don't have sensors in our joints in the hand, so this hand has no, we have no information about the joint space configuration.",
            "We have no sensing capabilities.",
            "We only say sync ability that we have is on that end and excursions alright.",
            "And so."
        ],
        [
            "And that's a position?",
            "Or is it?",
            "This is position, yes.",
            "This is Connie.",
            "He's through spring at yeah, so there is some kind of electricity there, right?",
            "But we treat it.",
            "We are stiff too.",
            "OK so it's not like it's going to.",
            "So, but that is a very difficult system to work.",
            "It has been around for 10 years.",
            "And most people have been working with this system.",
            "They've had been doing quasi static tasks like you know, you don't.",
            "You try to find out what is the moment arm with stand on is more important but no movement was really actually involved and not real actual control.",
            "So we thought that we should do some experiments with this and see whether or not we can apply the ideas from iterative particle control combined with that with the dynamic movement primitives for this.",
            "Attractors, and so it turns.",
            "So this is just a very basic figure, so the algorithm that we have, actually, I forgot to say that is called \u03c0 square, so it's like not your Pi square, but this Pi squared, which means policy improvement with in the graph OK?",
            "And so we have here the so the.",
            "The idea is again very very simple.",
            "We use these trajectories in order to represent the position in the tendon space and then.",
            "We have fixed control, gained fixed PD control for this particular experiment.",
            "Alright, so that that was a very pedagogical experiment for us.",
            "Why?",
            "Because we find out that for tendon driven systems, whenever they come into contact with a with an object, you really need to have sharp changes in position intend on space.",
            "So Amy effort actually to go and put a function approximator or parameterization.",
            "Aim position would never been able to deliver and take the best out of your system in terms of performance.",
            "So yeah, we got some performance here, so the idea is that we have some experiments.",
            "We have some sliders and the only thing that we measure from the slide there is voltage.",
            "So the more you.",
            "President the slider, the less vole, vole, nuts you yet.",
            "So that that is the only information and the sensing of the tendon excursions alright fixed PD control.",
            "Alright so we teach it.",
            "We just provide it with a movement in the beginning, all right now we actually have a knob here and we try to use the index finger and thumb in order to be able to turn the NOB.",
            "And so yeah, I mean we use it.",
            "It turns out that, OK, we could get some performance.",
            "We could improve their performance, but that was not really.",
            "Enough it was not enough at all.",
            "Then we actually said, well, we're not going to use any any policy parameterization.",
            "And then what we're going to do is the simplest."
        ],
        [
            "Think of using a PD control, taking the game an using a stochastic financial equation.",
            "Represented by this stochastic finance allocation and now the new control variable becomes the chance of the control gain per time unit and then on the other hand we are going to get the desired trajectories and again represent them as a stochastic financial equation alright.",
            "And then now what?",
            "We actually the control variable is again the the last with with the exceptions chains, but we don't use any parameterization.",
            "OK, I mean we use parameterization but.",
            "We don't use any function approximator.",
            "These are stochastic financial equations and they are not smooth at all.",
            "And so the so then we focused on just only one finger.",
            "And so we have a demonstrated movement here.",
            "So we demonstrate the movement so we can get.",
            "We can get the trajectories from this demonstration.",
            "Alright, and then we replay the movement.",
            "So this is the trajectory from the passive demonstration and this is what happens when you replay it again another way if you replay it again, this is how far you can go.",
            "The idea is that you want to.",
            "Press the slider as more as possible and then in order to be able to minimize your cost and the cost.",
            "Again, is only the voltage that you get from the potentiometer from the slider.",
            "That is the only sensing capability that we have.",
            "OK an attendant excursions and so it turns out that if you do only gain optimization, so we've fixed right now the desired trajectories and we don't change the games.",
            "We have fixed PP control.",
            "So OK, you could actually do a little bit better, but again, that was not really satisfied.",
            "So then we we set.",
            "Let's actually learn both games and their trajectories and it turns out that you know it can basically just go and just.",
            "Move the slider as far as possible and maximize its its.",
            "Red lines here.",
            "So so this is the cost that we have normalized cost from experiment.",
            "So this experiment one, this experiment two and this is the normalized cost.",
            "Now this experiments made it look to you as easy.",
            "But in fact even repositioning the finger at exactly the same position it was challenging.",
            "OK, we cannot actually reposition the finger always at exactly same position.",
            "Alright, so there's a little bit of uncertainty with respect to from where you start to create your trajectory's.",
            "OK, so this is a tendon driven system, and if you just go and pull all of attendance at the same time, that does not mean that you're going to create any any movement, right?",
            "So the tendons can only improve, they cannot push.",
            "And so then what we?"
        ],
        [
            "Next, it is we had some spring dynamics here, alright, and now what we want to do is we want to do exactly the same thing with John.",
            "We don't change the cost at all, not nothing.",
            "We just perform exactly the same experiment, but the idea is that now if the finger just go and.",
            "Well, OK, then if it goes and presses the slider very hard then it's going to lose contact and the spring is going to have a slight.",
            "It's going to go again app and you get you don't minimize your cost and so it turns out that this is what happens before.",
            "This is what happens with.",
            "Learning there are some trials in which of course the finger is going to lose contact with the slider.",
            "The slider is going to go back, doesn't get very good.",
            "Um?",
            "And then at some point after learning, it just learns to barely actually move the slider and stay there, and this is again how the cost goes.",
            "And that was a very nice experiment, because right now we see here what happens intend on excursions and there are many interesting phenomena we can characterize how biometric finger is people in bio mechanics say, well, this tendon is an extension of the standard is a flexor, but in fact it turns out that there are some tendons which play.",
            "Both roles in different parts of the time.",
            "Verizon, so in 10 years the vision is that you want to have a robot like this or a similar robot, and you want to have it play piano alright and after 20 years you want to have a human who can actually have a prosthetic arm and you know play, play, play piano.",
            "But in order to be able to do that, you have to be able to ask the question how biomimetic the system which I'm using in order to understand how the brain controls the hand is.",
            "Alright and then.",
            "Develop.",
            "Your research path towards this direct."
        ],
        [
            "Now.",
            "Excuse me you were doing damn programming in this.",
            "Yes yeah.",
            "I think it's bigger, smaller, well they get.",
            "They are very noisy, OK and it is very difficult to say.",
            "But as you saw from the talks, the talks at the time of the contact become bigger.",
            "OK, when you really need to have contact in prayer and deliver force attentive factor that orcs increase and I can say that yeah at the very first contact the gains also increase but that was just two experiments alright.",
            "I cannot give you answer 100% right.",
            "Now that was just two experiments.",
            "Now a little bit on the theoretical side.",
            "I just want to say that.",
            "This is the paper that you meant on right?",
            "So at some point, as you know, besides the application site, I have also some theoretical interest and so I found this paper at some point.",
            "This very nice paper that shows all the connections between relative entropy and free energy and how you can actually drive stochastic optimal control, But actually tells you how you can actually did find connections of stochastic optimal control of.",
            "Difference of game theoretic optimal control with.",
            "From the information theoretic point of view, and so I think you guys must know all about this I just."
        ],
        [
            "To say that we have."
        ],
        [
            "Wrote this paper at CDC.",
            "Is going to appear at CDC."
        ],
        [
            "2012 we just try to make some small extensions to jump diffusions.",
            "Becausw everybody knows that for diffusion process is linear in controls and dynamics.",
            "You, son of theorem gives you this quadratic term in use, and that is a nice trick that actually one can use in order to derive stochastic optimal control from the information theory point of view.",
            "By using the Sonos theorem, and I was asking the question can I say can I do something with?",
            "With a jump diffusion processes, what happens with, for example with spiking rate?",
            "Can I use a spiking rate as a control variable?",
            "How it appears in the so?",
            "It turns out that if you just take this very basic.",
            "And I'll relationship between free energy and relative entropy, and you apply to diffusively Markov jump diffusion processes and you open a textbook on Markov diffusions.",
            "You find what's the random nikodym derivative.",
            "You get this term here, which is the term that you have these two terms which you have from the diffusion processes, but you have this extra term with this function of gamma.",
            "This gamma adjust the ratio between the two person rates and.",
            "I'm just sitting in this equation and say what is really the physical intuition and I haven't still find out any physical intuition, But it turns out that if these two, if they jump rates are actually equal, then all this guy goes to zero, and then you can go back to the classical stuff.",
            "And."
        ],
        [
            "Again, we tried to systematize the connection between.",
            "They work how birth and others have derived, departing the path integral control based on the dynamic programming principle and then systematically somehow how people think from the information theory point of view and for the class of diffusion processes with.",
            "Define controlling noise.",
            "Well, what is the connection between the two?",
            "But I don't have time right now.",
            "I guess I'm out of.",
            "Square.",
            "This place the role of exploration.",
            "So two questions one is and how do I order the variance of humans relate to the speed of convergence of the cost function and also.",
            "Say about if you convert your local global minimum 3 cause with respect to the first question and what we find out actually, is that OK, I mean.",
            "You better, this particular control is very robust with respect to which variance you are using for your exploration.",
            "Actually more robust.",
            "Another policy gradient methods alright, I don't have a number to give you, but actually what happens is that what is important.",
            "Sometimes when you do this when you apply these techniques to systems that have high order meaning many states, these are sampling method sampling based methods and So what happens is that the noise is actually getting filtering out.",
            "So you don't see actually result in the outcome, so you don't learn.",
            "So then what you have to do is some sample the noise, keep the noise freeze for some.",
            "Time.",
            "In order for this for this information to integrate it and you see some changes in your in the one question to ask is how can I do actually variable quantization reinforcement learning which goes it is related also to attention control.",
            "How I can do that in automatically and the second question was in terms of local minima.",
            "Well, we see that in the experiments, depending depending on the parameters, we sometimes find different.",
            "Behaviors, but you know, in these systems I don't know what's really the global optimum.",
            "You know, in order to be able to characterize what's really very local one.",
            "But from the practical point of view, you know that is an algorithm that you know many people.",
            "Stop use and that means that you know it's a very robust algorithm to use, so I cannot really characterize.",
            "I cannot give you an.",
            "Conclusive answer.",
            "So in most all this before that you die.",
            "Do theory or you get results.",
            "So you if you if you want to use that you should actually rather use model predictive control rather than this.",
            "So we have to comment on that.",
            "Yeah, so I think.",
            "OK, so I think what ammo was going with his in learning stuff was going from, you know, the point of view of someone just going and sampling in the full state in the full state space.",
            "And this is the way how I was thinking when I start reading your your papers.",
            "But here is Stephen Shaw.",
            "OK Guy who has so much experience on real robots alright and you know in this lab you can from the moment that you think one idea until the moment where you are doing the real experiment.",
            "Edit It is basically five.",
            "It is 5 days.",
            "It's a very systematic, you know so and then he tells me that, well, I think we should really do it the other way.",
            "You know we should basically treat it as a policy gradient so.",
            "So I think what the disadvantage of this ideas is that what happens?",
            "Let's say if these ideas have to do with learning in a long time horizon, you know you have to execute the trajectory.",
            "But I do believe that you know when we become older and we want to play one of my favorite Mozart pieces, you know, we actually do learn in smaller timescales, so there are some tasks which require.",
            "Learning in smaller timescales, so learning in smaller times case timescales it will be a little bit challenging.",
            "Model predictive control on the other side is for adapting, you know in smaller timescales, but it relies on a wrong model and so far we have indication that it has been applied.",
            "It works well on simulation, so somehow combine them combining them OK in order to be able to perform control and learning in long and sort.",
            "Time scales, I think, is really the next step to control now how we are going to do that.",
            "If somebody pays me $500,000, gives me some good students and a position in a University.",
            "Time invariant policies.",
            "Whether you mean by the emerging policies.",
            "Something like that.",
            "Dataset options.",
            "Well, there are two ways how you can do it.",
            "One is by having the filter change overtime or having it done not change overtime.",
            "Well, when you are using this DMP's you know is you don't see so much difference in just, you know like the profiles become a little bit more noise you know, but I guess for the case where you don't want to use any parameterization like the case with a tendon driven finger in which we had thousands of parameters to optimize right?",
            "We have six tendons.",
            "Use overtime, I think in that case you know keeping you constantly.",
            "We would not park.",
            "Might take some comfort from the fact that I have a colleague who studies motion control and four arm movements and who has more or less convincingly established that learning has two time constants for those tasks, OK?",
            "Yeah.",
            "OK, well thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's my pleasure to be in a sad same mathematical in client audience and try to present work on robotics.",
                    "label": 0
                },
                {
                    "sent": "So buckling internal mechanics I guess from quantum mechanics an maybe you guys are a little bit tired, so I have some some videos too.",
                    "label": 0
                },
                {
                    "sent": "So everybody I guess likes videos and movies.",
                    "label": 0
                },
                {
                    "sent": "So I just want to from the very second slide.",
                    "label": 0
                },
                {
                    "sent": "I want to explain to you where I'm coming from and I believe there has been always say.",
                    "label": 0
                },
                {
                    "sent": "Debate between applications and theory and how much theory we can put two applications we have on one side, very complex robots.",
                    "label": 0
                },
                {
                    "sent": "An demand is always to be able to use these robots to perform very complex tasks.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we have very beautiful theories, so there are different kinds of beauty here and we would like to somehow bring them together.",
                    "label": 0
                },
                {
                    "sent": "So here is where robotics today stands.",
                    "label": 0
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "Project that involved locomotion and it is basically it is a project that it was a competition between 6 universities in the United States, MIT, CMU, Stanford, USC, Upenn.",
                    "label": 0
                },
                {
                    "sent": "I think I forgot.",
                    "label": 0
                },
                {
                    "sent": "I want to universities.",
                    "label": 0
                },
                {
                    "sent": "So I think Stanford, USC, CMU, MIT and went through the third phase, so that was a five years six years project and it involved the engineering of 10 people working on this project right now.",
                    "label": 0
                },
                {
                    "sent": "There is no one framework that actually works here.",
                    "label": 0
                },
                {
                    "sent": "I mean there are many different kinds of tools there because there are all different kinds of problems.",
                    "label": 0
                },
                {
                    "sent": "Alright, we have estimation, you have control.",
                    "label": 0
                },
                {
                    "sent": "And so I truly believe that when I when I first read Captain's paper on 2005 on statistical mechanics, an I was with these fellows at here.",
                    "label": 0
                },
                {
                    "sent": "see I thought how we can bring nice theories from stochastic optimal control and apply to real robotics.",
                    "label": 0
                },
                {
                    "sent": "So even if we cannot really make our algorithms to work very well in an automatic way.",
                    "label": 0
                },
                {
                    "sent": "I think I still believe that even if you can reach the 50% over the performance that these guys have reads in two years, that has a lot of value, so there must be some dialectic relationship between theory and practice.",
                    "label": 0
                },
                {
                    "sent": "In the sense that you know I mean, somehow I believe that we should bring them together and so.",
                    "label": 0
                },
                {
                    "sent": "But what is the thing that makes?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The robotic control difficult, right?",
                    "label": 1
                },
                {
                    "sent": "And so I think from the control theoretic point of view, robotic systems are systems that have is a nightmare to work with an.",
                    "label": 0
                },
                {
                    "sent": "I think quantum mechanical systems, but back to the Newtonian world I think we have issues that start from the fact that we just don't have very good models.",
                    "label": 1
                },
                {
                    "sent": "OK, and sometimes it's very difficult to identify models.",
                    "label": 0
                },
                {
                    "sent": "Also, there is some level of uncertainty in terms of representing tasks, right?",
                    "label": 0
                },
                {
                    "sent": "We don't have good models to represent all the world and every every task that we would like to do.",
                    "label": 0
                },
                {
                    "sent": "Definitely there if we have models.",
                    "label": 0
                },
                {
                    "sent": "These models are nonlinear and they have friction.",
                    "label": 0
                },
                {
                    "sent": "There are multiple dynamics they involved contact at the same time.",
                    "label": 0
                },
                {
                    "sent": "There is a issue of high dimensionality wybie cause these systems have many degrees of freedom, but when I say high demand high dimensionality, I do not only mean dimensionality with respect to the degrees of freedom of the underlying dynamical system, but in fact if you try to apply optimal control.",
                    "label": 0
                },
                {
                    "sent": "And reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "There is a level of dimensionality with respect to how many parameters you want to really optimize in order to perform the task.",
                    "label": 0
                },
                {
                    "sent": "And then there is the issue of under actuation with.",
                    "label": 0
                },
                {
                    "sent": "The experiments and the demos which I'm going to show today do not involve Underactuated robotics, but this is also a issue like we don't control all the degrees of freedom.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My talk is going to have a little bit of theory in the beginning and try to explain what are the algorithms that we developed.",
                    "label": 0
                },
                {
                    "sent": "Motivated and inspired by the work on personal control.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to show some applications to robotics.",
                    "label": 1
                },
                {
                    "sent": "How we can apply this?",
                    "label": 0
                },
                {
                    "sent": "How we have applied this tools to robotic systems and then I'm going to talk a little bit about more recent work.",
                    "label": 0
                },
                {
                    "sent": "This is a work that we have been doing 'cause I just wanted really to.",
                    "label": 1
                },
                {
                    "sent": "I realize that at some point my knowledge was limited in terms of stochastic optimal control, and I really wanted to write the paper where I can see what are the connections between the particle controlling information theory and we have a CDC paper on that, so I'm just going to.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start with a very basic formalization of stochastic optimal control.",
                    "label": 0
                },
                {
                    "sent": "May make sure that everybody is on the same page, and I'm going to go very fast for this.",
                    "label": 0
                },
                {
                    "sent": "So a typical stochastic optimal control problem we have minimalization of the cost function with dynamics for this particular case are fine in controls we have cost functions quadratic in controls an we have this cross term here between States and controls.",
                    "label": 0
                },
                {
                    "sent": "We apply the bellman.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Principle, we take the handles are called Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "We get the optimal controls right.",
                    "label": 0
                },
                {
                    "sent": "Very standard steps and then we have the feedback control here.",
                    "label": 0
                },
                {
                    "sent": "So what this equation says that the controller should move the dynamics towards direction of state space where the value function is minimum right?",
                    "label": 0
                },
                {
                    "sent": "Because it goes minus the negative of the gradient of the value function.",
                    "label": 0
                },
                {
                    "sent": "And then when you plug this you back to the hundreds or complement equation you get.",
                    "label": 0
                },
                {
                    "sent": "This second order nonlinear PDE.",
                    "label": 0
                },
                {
                    "sent": "Where you have this terms right now, Q~ and F~ right which appear here an I think that PD shows up.",
                    "label": 0
                },
                {
                    "sent": "So what in a in another talk?",
                    "label": 0
                },
                {
                    "sent": "Now the question is that you want to the stories that you would like to be able to find this value function in order to find the control here.",
                    "label": 0
                },
                {
                    "sent": "And there is this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exponential transformation of the value function.",
                    "label": 0
                },
                {
                    "sent": "You get this PSI.",
                    "label": 0
                },
                {
                    "sent": "Which is the desirability?",
                    "label": 0
                },
                {
                    "sent": "Function you have this connection between the control cost and variance of the noise and if you do this connection transformation and you have this assumption there, it turns out that 100 billion equation collapse the Barcode Summon Command graffiti.",
                    "label": 0
                },
                {
                    "sent": "Then with this terminal condition, if you apply the appointment Klima here that basically tells you that you can actually find this site for every state and time if you take the stochastic financial equation.",
                    "label": 0
                },
                {
                    "sent": "Propagated in time.",
                    "label": 0
                },
                {
                    "sent": "Get all these trajectories here.",
                    "label": 0
                },
                {
                    "sent": "Evaluate this expectation and now.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the control is given by this expression.",
                    "label": 0
                },
                {
                    "sent": "Here we have minus plus, minus minus plus wybie 'cause we want to minimize.",
                    "label": 0
                },
                {
                    "sent": "We want to maximize size, right?",
                    "label": 0
                },
                {
                    "sent": "We did the logarithmic transformation, so size that's the reason why it's called the desirability function.",
                    "label": 0
                },
                {
                    "sent": "So so far so nothing new.",
                    "label": 0
                },
                {
                    "sent": "And then since V is a value function.",
                    "label": 0
                },
                {
                    "sent": "And it is given by this expression.",
                    "label": 0
                },
                {
                    "sent": "This expectation here is evaluated over the uncontrolled dynamics.",
                    "label": 0
                },
                {
                    "sent": "This is what the notation means.",
                    "label": 0
                },
                {
                    "sent": "The expectation with respect to zero and then that is the lower bound of the initial cost function, right?",
                    "label": 0
                },
                {
                    "sent": "So when we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K so there is a very fundamental distinction in.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning with actually distinguishes between value function, approximation methods and policy gradient methods, and So what I'm going to be talking today is mostly about policy gradient methods, and so one of the main idea here is that you have a you sample the full state space you want to be able to approximate the value function.",
                    "label": 0
                },
                {
                    "sent": "That means that you have to visit all the state space almost all the state space and.",
                    "label": 0
                },
                {
                    "sent": "There is always the issue of scalability, but there is a very interesting connection between the Bellman error.",
                    "label": 1
                },
                {
                    "sent": "The error between the approximated value function and the real value function and how this error translates to the error in the policy.",
                    "label": 1
                },
                {
                    "sent": "So it turns out that there is no more tnan monotonic.",
                    "label": 0
                },
                {
                    "sent": "Relationship and in fact that is a very toy example here where this is the true value function.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "Hey, the best feed that you could get to the true value function and this is a fit that is not as good as this one, but as you can see the optimal control that you you can get from this bad feet to the value function does better than this one.",
                    "label": 0
                },
                {
                    "sent": "OK and that is somehow an indication that that was basically the motivation in the area of machine learning for people to control to work in on the area.",
                    "label": 0
                },
                {
                    "sent": "Policy gradient methods and he sends parameterising the value function.",
                    "label": 0
                },
                {
                    "sent": "Now the idea is that you want to be able to parameterise the controls alright.",
                    "label": 0
                },
                {
                    "sent": "It does, regardless of whether you have a deterministic or stochastic.",
                    "label": 0
                },
                {
                    "sent": "Well, in case of this low caste system the noise.",
                    "label": 0
                },
                {
                    "sent": "I mean we know that.",
                    "label": 0
                },
                {
                    "sent": "So the value function is going to be smoother.",
                    "label": 0
                },
                {
                    "sent": "So in case of the Terministic system which means that may be easier to approximate OK in the terministic system you may not have this smoothness, which means that you may have to use more more basis functions.",
                    "label": 0
                },
                {
                    "sent": "Reading the arguments, I usually sort of go for a deterministic.",
                    "label": 0
                },
                {
                    "sent": "Not a source control.",
                    "label": 0
                },
                {
                    "sent": "Greedy you mean inside the policy creating framework we go yes they call for a germanistik control.",
                    "label": 0
                },
                {
                    "sent": "But in that case is fastest.",
                    "label": 0
                },
                {
                    "sent": "Excuse us exploration wise.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but in case that the optimal policy is actually undecided.",
                    "label": 0
                },
                {
                    "sent": "The case where the optimal policy is.",
                    "label": 0
                },
                {
                    "sent": "Like in the delayed choice crazy.",
                    "label": 0
                },
                {
                    "sent": "Yes, I don't want to go to the park.",
                    "label": 0
                },
                {
                    "sent": "Well in this case, yeah, in this case I policy gradient will actually most likely suffer.",
                    "label": 0
                },
                {
                    "sent": "In fact, the policy gradient cannot work.",
                    "label": 0
                },
                {
                    "sent": "You know there is no freelance, right?",
                    "label": 0
                },
                {
                    "sent": "You have to be able to provide it with an initial policy that is relevant with respect to the task that you want to perform.",
                    "label": 0
                },
                {
                    "sent": "Now, how good is this initial policy and how good and how much you are optimizing?",
                    "label": 0
                },
                {
                    "sent": "That is, I think that's dependent, but what you will see in this videos, I think that.",
                    "label": 0
                },
                {
                    "sent": "You can do a very good job actually.",
                    "label": 0
                },
                {
                    "sent": "You can start with a very naive initial policy and do very good optimization alright.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh so now when I read birds paper.",
                    "label": 0
                },
                {
                    "sent": "The first question that I asked is how we can use this framework, and so the first thing that we try to do is think about this G here as a.",
                    "label": 0
                },
                {
                    "sent": "This is the control transition matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, but thing as G as GT dot times 5 so it could be actually all these U as something that we can actually parameterized.",
                    "label": 0
                },
                {
                    "sent": "So first what we wanted to do was to actually derive a path.",
                    "label": 0
                },
                {
                    "sent": "Control framework for the cases where this guy here was a function of the state because we wanted really to see how the equations look like, and I think we've got a very nice feedback from birth capping in our initial derivation.",
                    "label": 0
                },
                {
                    "sent": "And so it turns out that the control is given by this formulation.",
                    "label": 0
                },
                {
                    "sent": "Remember that this guy here is the term in the cost function that appears in the cross term between the states in the controls.",
                    "label": 0
                },
                {
                    "sent": "And now we have the expectation.",
                    "label": 0
                },
                {
                    "sent": "All this time you out this is the path in the graph and this term UL is given by this expression here, right?",
                    "label": 0
                },
                {
                    "sent": "So it has again the same flavor of of sampling from the order of the fact that the control is a function of the first direction of the noise.",
                    "label": 0
                },
                {
                    "sent": "But there are these terms with with.",
                    "label": 0
                },
                {
                    "sent": "Appear and so you have the probability of the path and this is basically where S is the path of the cost.",
                    "label": 0
                },
                {
                    "sent": "OK, so one nice thing with the passing of control is that because you know from this you sample based on the uncontrolled dynamics.",
                    "label": 0
                },
                {
                    "sent": "Actually this term here can be proportional to the noise that you were using in order to sample, and so the drift of the dynamics of the dynamics.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really appear in your calculations and so you can use this method as a Model 3 way.",
                    "label": 0
                },
                {
                    "sent": "So you don't need to actually know the true dynamics in order to apply, but integral control.",
                    "label": 0
                },
                {
                    "sent": "You just have to sample forward in time.",
                    "label": 0
                },
                {
                    "sent": "So after we did this derivation.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then the second question that we ask was how we can actually now do it in a tentative way, because there is no actually this.",
                    "label": 0
                },
                {
                    "sent": "This equation tells us that you know the partner control.",
                    "label": 0
                },
                {
                    "sent": "It is a feedback policy depends on X&T.",
                    "label": 0
                },
                {
                    "sent": "It's a finite horizon optimal control problem, which means that we have to go to every state at every time sample and compute this quantity.",
                    "label": 0
                },
                {
                    "sent": "But that approach with actually not scaling a high dimensional state space.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to see how we can actually.",
                    "label": 0
                },
                {
                    "sent": "Do the whole particle control in the iterative fashion OK, and so we started with the solution by the Feynman Kalama or they which gives us the desirability function and what we did is something very simple.",
                    "label": 0
                },
                {
                    "sent": "We did important sampling.",
                    "label": 0
                },
                {
                    "sent": "So basically you have you have here this the use you have the initial dynamics, which are the uncontrolled dynamics.",
                    "label": 0
                },
                {
                    "sent": "But let's say that we are at iteration K and now this is the probability measure that corresponds to the control dynamics at iteration K. Alright, so this is the Radom Nikodym derivative which is given in the exams theorem.",
                    "label": 0
                },
                {
                    "sent": "So you can actually derive a profit.",
                    "label": 0
                },
                {
                    "sent": "The particle control the therapy facility or control, and it turns out that this is the formula that you get now.",
                    "label": 0
                },
                {
                    "sent": "This why term.",
                    "label": 0
                },
                {
                    "sent": "If you multiply it with G is going to collapse, so this term can become actually much simpler.",
                    "label": 0
                },
                {
                    "sent": "But I'm not going to talk about different formulations.",
                    "label": 0
                },
                {
                    "sent": "This is just one form.",
                    "label": 0
                },
                {
                    "sent": "The more general from my point of view and now here you have the new.",
                    "label": 0
                },
                {
                    "sent": "Laughing the girl when you P this is a P~ which is which has the path cost and now you have this correction term that comes from Gibson's theory alright.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how we're going to apply it right now?",
                    "label": 0
                },
                {
                    "sent": "So far I haven't really say anything about OK, this is the iterative version, but we're still actually in the previous formulation.",
                    "label": 0
                },
                {
                    "sent": "We have to visit any state in the state space for.",
                    "label": 0
                },
                {
                    "sent": "Propagate the dynamics forward.",
                    "label": 0
                },
                {
                    "sent": "Sample the dynamics forward in order to be able to find the party, the party control even in the iterative case.",
                    "label": 0
                },
                {
                    "sent": "So typically in robotics we have systems of this form, right?",
                    "label": 0
                },
                {
                    "sent": "So imagine that this Q is the joint position and joint velocities.",
                    "label": 0
                },
                {
                    "sent": "This M here is the inertia.",
                    "label": 0
                },
                {
                    "sent": "Immersive this C is the the term that has to do with Coriolis forces, gravity and the controls.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's nothing.",
                    "label": 0
                },
                {
                    "sent": "It's basically a application of Newtons law.",
                    "label": 0
                },
                {
                    "sent": "And since this system is second order system, meaning that it has position and velocity, one thing that we're going to do is we're going to use control in this control is in PV four.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're using a P controller alright, and so the big game that game and we have the, so the desired trajectory's and the current trajectory.",
                    "label": 0
                },
                {
                    "sent": "OK, so this that is at rejecting the desired velocity.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're going to use.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear poke point attractors with our differential equations that can represent smooth trajectories.",
                    "label": 0
                },
                {
                    "sent": "So, if I give you the initial position and the target, you can, you can use this differential equation in order to represent trajectories between these two points, and in fact, so this this this nonlinear pointer trackers have this form.",
                    "label": 0
                },
                {
                    "sent": "We can think about this X as a phase variable Huawei.",
                    "label": 0
                },
                {
                    "sent": "Has the position and set their velocity and now we.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "3rd",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here this is a goal term.",
                    "label": 0
                },
                {
                    "sent": "This is the first term and the first term is nothing else than is given as a as a multiplication of a function approximator.",
                    "label": 0
                },
                {
                    "sent": "In up a function of the phase variable and parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "Alright, so by changing the parameter Theta I can actually deviate change the shape of my trajectories alright and I can use that in order to represent the desired trajectory's OK and.",
                    "label": 0
                },
                {
                    "sent": "I can do the same trick.",
                    "label": 0
                },
                {
                    "sent": "I can use the same trick in order to represent the gains alright and that is important because you know, sometimes we want to do both planning and gain scheduling on a real system.",
                    "label": 0
                },
                {
                    "sent": "So we would like to be able to plan trajectories, but at the same time being able to change the sensitivities in the games.",
                    "label": 0
                },
                {
                    "sent": "OK, we want to have compliant control, which means we want to change the game at every time step and.",
                    "label": 0
                },
                {
                    "sent": "So now we are going to treat this parameters here as the new control parameters.",
                    "label": 0
                },
                {
                    "sent": "Of course, we augment their dynamics alright, and augmenting their dynamics is not good, because that means that you change the controllability characteristics of your of the system, right?",
                    "label": 0
                },
                {
                    "sent": "You are augmenting the order of the assistant.",
                    "label": 0
                },
                {
                    "sent": "But but on the other side, you have a very good control of what your system is going to do, because basically you have this very nice smooth trajectories.",
                    "label": 0
                },
                {
                    "sent": "And in robotics in general we want to have smooth trajectories in joint space, right?",
                    "label": 0
                },
                {
                    "sent": "We don't want to have trajectories that are very noise right now.",
                    "label": 0
                },
                {
                    "sent": "One may ask well what happens if you don't know the goal of movement.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that we have done some work on this.",
                    "label": 0
                },
                {
                    "sent": "You can actually make this goal one extra parameter state.",
                    "label": 0
                },
                {
                    "sent": "And actually earn the goal by having an extra state, OK?",
                    "label": 0
                },
                {
                    "sent": "So the the idea is very simple.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it has been.",
                    "label": 0
                },
                {
                    "sent": "Around for quite a long time ago, I guess you know, for the last 10 years and this is just a very.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple demonstration here is just.",
                    "label": 0
                },
                {
                    "sent": "Human having an X killer done and if it makes one movement an from vector skeleton we can get his join.",
                    "label": 0
                },
                {
                    "sent": "Angles and then we can demonstrate that to the robot.",
                    "label": 0
                },
                {
                    "sent": "But because you know you can always change the goal of the movement, you can actually have the robot perform these tasks in different goals.",
                    "label": 0
                },
                {
                    "sent": "OK, that was the very first application of these tools to robotic, so right there is no any control really here.",
                    "label": 0
                },
                {
                    "sent": "I mean it's a PD control, it's fixed.",
                    "label": 0
                },
                {
                    "sent": "That was the very first thing.",
                    "label": 0
                },
                {
                    "sent": "OK, now it may be very simple, but I mean you will see what we can do with it.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so again.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a classical block diagram, think.",
                    "label": 0
                },
                {
                    "sent": "He must picture when we when we say planning.",
                    "label": 0
                },
                {
                    "sent": "Usually in robotics we mean that we want to be able to change the desired trajectory's of the system.",
                    "label": 0
                },
                {
                    "sent": "When we say gain scheduling, we mean that we want to be able to change the control gains of the underlying dynamics as a function of time.",
                    "label": 1
                },
                {
                    "sent": "And it turns out that that works very well even for nonlinear systems alright.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the so now I'm just getting what was derived from the iterative path integral control.",
                    "label": 0
                },
                {
                    "sent": "I have this setup parameters and I treated and I treat them as my new controls and I just ignore that the control is a function of the state it does.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to pretend that you is only a function of time.",
                    "label": 0
                },
                {
                    "sent": "OK, now that is not something that control theoreticians would like, but on the other hand.",
                    "label": 0
                },
                {
                    "sent": "If you have a view effects of tea, you have to tell me how I can actually sample the state space on 20 degrees of freedom robot and being able to do actually feedback control at the same time.",
                    "label": 0
                },
                {
                    "sent": "Many in many cases we may actually not be able to move the robot in all different configurations becausw the systems you know many times are not controllable in configuring some specific configurations and unstable so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The algorithm is very simple, so you want to go from this start state to the goal state and what you do is you just simulate or execute on the real system.",
                    "label": 0
                },
                {
                    "sent": "Trajectory's all right and then what you do is you go at every state at every time.",
                    "label": 0
                },
                {
                    "sent": "Slice in time of the trajectory you find.",
                    "label": 0
                },
                {
                    "sent": "What is the cost to reach the.",
                    "label": 0
                },
                {
                    "sent": "The terminal, the terminal State, and then you.",
                    "label": 0
                },
                {
                    "sent": "The Glasgow particle control what you would do is you will find the particle control here, then move the system for example.",
                    "label": 0
                },
                {
                    "sent": "Again OK and do it again and again but that is very slow and as.",
                    "label": 0
                },
                {
                    "sent": "And as I said, in many cases requires that you have to be able to sample the full state space.",
                    "label": 0
                },
                {
                    "sent": "So instead what we will do is we will find.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is the optimal path integral correction in Delta, Theta, right?",
                    "label": 0
                },
                {
                    "sent": "And then we're going to do this?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same thing for the next.",
                    "label": 0
                },
                {
                    "sent": "Time in the time horizon OK?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Until basically.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we reach the terminal, the.",
                    "label": 0
                },
                {
                    "sent": "Until the end of the trajectory, then we are going.",
                    "label": 0
                },
                {
                    "sent": "We are going to.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To update our fitness because we have found our Delta headers here based again on this pattern control formulation and we're going to repeat this process again, right?",
                    "label": 0
                },
                {
                    "sent": "We're going to use this data, put it on our system, resemble again on the real physical system.",
                    "label": 0
                },
                {
                    "sent": "OK, and find the new correction in in Delta Phi to the controls.",
                    "label": 0
                },
                {
                    "sent": "To convergence.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "That was we have apply that to very simple systems, let's say 20 or 50 degrees planar manipulators, and we have we had a paper.",
                    "label": 0
                },
                {
                    "sent": "We compare that to very many classical enforcement learning algorithms and we find out that it works very well.",
                    "label": 0
                },
                {
                    "sent": "It is actually faster if you want I can solve this data, but for the purpose of time I just want to show the very first video that we got and that was that.",
                    "label": 0
                },
                {
                    "sent": "Make me, you know, excited.",
                    "label": 0
                },
                {
                    "sent": "I was thinking that.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe there is a hope we can actually continue working on this and so the idea is here that you have this little dog robot.",
                    "label": 0
                },
                {
                    "sent": "This is actually the simulator of a real actual robot that we had, that the simulator that people were using at that time.",
                    "label": 0
                },
                {
                    "sent": "And so all the experiments on the real robot had to take place first on this simulator.",
                    "label": 0
                },
                {
                    "sent": "OK, and so the idea is that you want to be able to jump from this gap.",
                    "label": 0
                },
                {
                    "sent": "And the cost is very simple.",
                    "label": 0
                },
                {
                    "sent": "You want to have you want to keep their role and they heal.",
                    "label": 0
                },
                {
                    "sent": "You want to penalize with.",
                    "label": 0
                },
                {
                    "sent": "Somehow fixed, you want to penalize acceleration right?",
                    "label": 0
                },
                {
                    "sent": "And at the same time you penalize this is going to your terminal state.",
                    "label": 0
                },
                {
                    "sent": "You have a goal state and you penalize with respect to the distance and so it turns out that you know if you.",
                    "label": 0
                },
                {
                    "sent": "Here a you know like it can.",
                    "label": 0
                },
                {
                    "sent": "It is basically able to jump down so that so that's a very primitive, so that so that was a very primitive task.",
                    "label": 0
                },
                {
                    "sent": "OK an it's not as you know elegant as demos videos, but this is what we did like three years ago and that was our first indication that this algorithm actually may work on any other robot then the next thing that we try.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I was to actually do variable stiffness control, which means not only learn desired trajectories, but learn gains.",
                    "label": 0
                },
                {
                    "sent": "Alright, this this is the this.",
                    "label": 0
                },
                {
                    "sent": "This is called variable stiffness control alright, and that relates to the I to the idea.",
                    "label": 0
                },
                {
                    "sent": "So Roger broke it at some point here in his course.",
                    "label": 0
                },
                {
                    "sent": "He wanted to penalize the derivative of EU of U with respect to X.",
                    "label": 0
                },
                {
                    "sent": "So in a standard PD control, if you do that, you just get the.",
                    "label": 0
                },
                {
                    "sent": "The game and that is what you call the complexity of implementation, right?",
                    "label": 0
                },
                {
                    "sent": "So here we actually do it.",
                    "label": 0
                },
                {
                    "sent": "We have done it on a on a simple fandom in simulation again.",
                    "label": 0
                },
                {
                    "sent": "So there is one more simulation and everything else is on aerial robot and the idea is that you want to go in joint space from a specific point.",
                    "label": 0
                },
                {
                    "sent": "OK, you want to go via a point.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the cost is that you want to penalize for high control gains.",
                    "label": 0
                },
                {
                    "sent": "You want to penalize for acceleration because you want to do that in a small state.",
                    "label": 0
                },
                {
                    "sent": "And you want to penalize with respect to the joint angle error so there's a target in join angle space and you're going to go through this point.",
                    "label": 0
                },
                {
                    "sent": "So what happens here is that in this row.",
                    "label": 0
                },
                {
                    "sent": "In this role, you see the What, what happens in joint angle space?",
                    "label": 0
                },
                {
                    "sent": "So these are the joints with a yellow.",
                    "label": 0
                },
                {
                    "sent": "Is the target OK with the red?",
                    "label": 0
                },
                {
                    "sent": "Is the initial trajectory and with a blue is the actual trajectory.",
                    "label": 0
                },
                {
                    "sent": "After learning all right?",
                    "label": 0
                },
                {
                    "sent": "And so in the 2nd row what you see are the games alright, so you see that at the time where the system goes via the target, the gains pick.",
                    "label": 0
                },
                {
                    "sent": "These are the initial gains fixed gains alright, and then after learning you see the pic here you see the pic here.",
                    "label": 0
                },
                {
                    "sent": "You see the picture.",
                    "label": 0
                },
                {
                    "sent": "So the game speak only when you really need them, while at the other parts of the time horizon.",
                    "label": 0
                },
                {
                    "sent": "Actually the game is very low.",
                    "label": 0
                },
                {
                    "sent": "That is important, not because only we want to do compliant control, but that is important because sometimes this game these games are multiplied by the state and the state is given by Kalman filter estimator which has noise.",
                    "label": 0
                },
                {
                    "sent": "So the higher the game you amplify the noise, so you want to be able to regulate your.",
                    "label": 0
                },
                {
                    "sent": "Your game starts that you don't Abilify the noise and at the same time you perform the task alright.",
                    "label": 0
                },
                {
                    "sent": "Any questions so far?",
                    "label": 0
                },
                {
                    "sent": "Is it very simple?",
                    "label": 0
                },
                {
                    "sent": "Intuitive.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we're going to do the same thing.",
                    "label": 0
                },
                {
                    "sent": "That is a simulated version of the Kuka robot.",
                    "label": 0
                },
                {
                    "sent": "Do you go now?",
                    "label": 0
                },
                {
                    "sent": "You guys know the cooker robot is a very famous manipulator these days.",
                    "label": 0
                },
                {
                    "sent": "Is 1 of this German robots.",
                    "label": 0
                },
                {
                    "sent": "And so it has 6 degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "But now the only difference is that we want to go via point in the end effector respects alright, not enjoying space, and then the vector space.",
                    "label": 0
                },
                {
                    "sent": "And this is again what happens, you know?",
                    "label": 0
                },
                {
                    "sent": "You see again that you started from initial game.",
                    "label": 0
                },
                {
                    "sent": "This is what happens after 30 updates with.",
                    "label": 0
                },
                {
                    "sent": "Yellow or green?",
                    "label": 0
                },
                {
                    "sent": "I cannot distinguish yellow with green and then and then at 100 updates you see that the game goes down and this is what happens.",
                    "label": 0
                },
                {
                    "sent": "More mostly all join Alljoyn space OK.",
                    "label": 0
                },
                {
                    "sent": "This is the point where it has to go in the end effector space, so the games we are going to pick, in most cases at this line here.",
                    "label": 0
                },
                {
                    "sent": "You see it also.",
                    "label": 0
                },
                {
                    "sent": "Effectively converting closely or control.",
                    "label": 0
                },
                {
                    "sent": "Yes, so I'm I'm I'm I'm taking a feedback control low alright and you because of the problems that I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "Alright, I use it as an open loop control and then in order to be able to do actually real feedback control, I bought some parameterization on my controller and I treat the games and the desired states as new states again I augmented dynamics right, but that turns out to work very well.",
                    "label": 0
                },
                {
                    "sent": "Feedback game The point of feedback is to find disturbances right well.",
                    "label": 0
                },
                {
                    "sent": "Effectively driven open loop simulation.",
                    "label": 0
                },
                {
                    "sent": "So the only reason these games are doing something is because you couple the whole book of the game right in the open group representation doesn't have anymore in the cost function you put as a performance to reject disturbances.",
                    "label": 0
                },
                {
                    "sent": "Then you will get games that they will reject disturbances.",
                    "label": 0
                },
                {
                    "sent": "No, I'm not doing that OK but it's all I mean.",
                    "label": 0
                },
                {
                    "sent": "This is a generic issue with or without more control.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Depends on your performance criteria.",
                    "label": 0
                },
                {
                    "sent": "So then.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We moved to real robots and So what I'm going to show here is the PR2 robot that asks this is work that was done in Willow Garage with some people from our lab and the idea here is that you have this robot and what you would like to do is you have these two chopsticks and you would like to be able to roll this small box here.",
                    "label": 0
                },
                {
                    "sent": "OK, so again we don't use any model right?",
                    "label": 0
                },
                {
                    "sent": "And this is a task that you know we don't really.",
                    "label": 0
                },
                {
                    "sent": "No, exactly the dynamics of this task.",
                    "label": 0
                },
                {
                    "sent": "We don't have ways to represent them, or if if we had, that could be very complex.",
                    "label": 0
                },
                {
                    "sent": "So we just use a simpler way of having the robot interact with the environment.",
                    "label": 0
                },
                {
                    "sent": "Alright, get a reward and you know improve its performance.",
                    "label": 0
                },
                {
                    "sent": "So the cost here is there is a gyro in the box that gives us.",
                    "label": 0
                },
                {
                    "sent": "Acceleration, so we want to flip it, but we don't want to.",
                    "label": 0
                },
                {
                    "sent": "We want to do it in a small quake 'cause if we don't penalize for acceleration then it might do something very aggressive.",
                    "label": 0
                },
                {
                    "sent": "Then we actually penalize.",
                    "label": 0
                },
                {
                    "sent": "We have measurements at the grippers with from the force because so we want to do that in a gentle way, right?",
                    "label": 0
                },
                {
                    "sent": "And also we have accelleration at the gripper, so we want to penalize how fast the hands are moving.",
                    "label": 0
                },
                {
                    "sent": "And of course there's a terminal cost.",
                    "label": 0
                },
                {
                    "sent": "Which has to do with the desired state and the state is just flip the cube.",
                    "label": 0
                },
                {
                    "sent": "This cube, this box and so you should penalize with expected deviations from this desired state, right?",
                    "label": 0
                },
                {
                    "sent": "So here's what happens.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "There are many trials.",
                    "label": 0
                },
                {
                    "sent": "There is an initial policy is going to try to.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it loses completely.",
                    "label": 0
                },
                {
                    "sent": "Contact with help Japan after 26 iterations here a you know.",
                    "label": 0
                },
                {
                    "sent": "So if you compare right now, the policy that you got at the end with initial policy, you have 86% success rate.",
                    "label": 0
                },
                {
                    "sent": "OK, well with the initial policy you had I think.",
                    "label": 0
                },
                {
                    "sent": "This is what happens.",
                    "label": 0
                },
                {
                    "sent": "This is how we teach it, right?",
                    "label": 0
                },
                {
                    "sent": "Because I said we want to have an initial policy and this is the policy that we have in desired trajectories in terms of end effector position and orientation.",
                    "label": 0
                },
                {
                    "sent": "So there is a little bit of struggle, but some subjects actually.",
                    "label": 0
                },
                {
                    "sent": "May have made it so the neck.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the next task is a task that was motivated by the by studies that people did in psychophysics, so they did some studies in which they were placing an object, and so the subjects thought that the object is in a particular position, XYZ coordinates, but in fact they were making small deviations of the objects, so they wanted to see.",
                    "label": 0
                },
                {
                    "sent": "How humans adapt into small uncertainties in and effective space?",
                    "label": 0
                },
                {
                    "sent": "OK, and then finally they find out that the way how the that the humans can really adapt this uncertainty and they change their trajectory in order to be able to grab their object in any possible configuration.",
                    "label": 0
                },
                {
                    "sent": "Inside a three Sigma bound on the uncertainty of position on K. So.",
                    "label": 0
                },
                {
                    "sent": "Any questions So what happens here is that.",
                    "label": 0
                },
                {
                    "sent": "So this is the original position, the position that the robot thinks that the object is.",
                    "label": 0
                },
                {
                    "sent": "But in fact we have moved the object around, so there are some positions initially where the robot can really make it.",
                    "label": 0
                },
                {
                    "sent": "But if this placement is very large up to the 6 centimeters is going to lose.",
                    "label": 0
                },
                {
                    "sent": "Basically contact, he's not going to grab it.",
                    "label": 0
                },
                {
                    "sent": "So this is before learning, so this is a four sending methods.",
                    "label": 0
                },
                {
                    "sent": "Distance OK, now it barely made it.",
                    "label": 0
                },
                {
                    "sent": "600 mothers OK. And this is what happens at the end effector space.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now here's what happens after learning.",
                    "label": 0
                },
                {
                    "sent": "They all the robot wraps around the object so that it is able to grasp the object in any possible configuration.",
                    "label": 0
                },
                {
                    "sent": "In this white space.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is what this is.",
                    "label": 0
                },
                {
                    "sent": "What happens after learning all right and this is the difference between the two end effector two behaviors and affective space alright?",
                    "label": 0
                },
                {
                    "sent": "Bye bye.",
                    "label": 0
                },
                {
                    "sent": "No, actually the only thing that we are using here is acceleration, because the parameters that we are using is benefactor position and wrist orientation.",
                    "label": 0
                },
                {
                    "sent": "This is what we parameterized.",
                    "label": 0
                },
                {
                    "sent": "If you've got to knock it out, the only thing that we say is success or or not.",
                    "label": 0
                },
                {
                    "sent": "At that, that's closest to the robot, yeah?",
                    "label": 0
                },
                {
                    "sent": "God still moving and not so yeah, I mean, so you're asking if we are using the sensors?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we're using the sensors in order to say you know success or failure right?",
                    "label": 0
                },
                {
                    "sent": "In order to find out whether or not the robot has really grasped the object.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then.",
                    "label": 0
                },
                {
                    "sent": "Another task was actually to have the robot play learning pool stroke and the idea here is that you want to be able to so the table is tilted and the idea is that you want to go through this point as fast as possible, alright, with maximum basically 1st, and so the robot is going to try.",
                    "label": 0
                },
                {
                    "sent": "The table is tilted, the ball is going back with the lasers.",
                    "label": 0
                },
                {
                    "sent": "Kind of hear the courses that you want to be able to minimize the time, which means that you want to go as fast as possible to your target.",
                    "label": 0
                },
                {
                    "sent": "There are from the target and also the displacement of the displacement of the stroke.",
                    "label": 0
                },
                {
                    "sent": "Cause if the if the robot is moving very fast sometimes you know the stroke and just.",
                    "label": 0
                },
                {
                    "sent": "So this is the initial policy.",
                    "label": 0
                },
                {
                    "sent": "Trusting proof.",
                    "label": 0
                },
                {
                    "sent": "So this experiments take from 20 minutes.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "There are some experiments later that I'm going to show that take 2 hours or three hours.",
                    "label": 0
                },
                {
                    "sent": "So OK, so this I guess the most clearly this is the initial policy and then.",
                    "label": 0
                },
                {
                    "sent": "I find out please.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we want to have this since we want to have the robot interact with the environment.",
                    "label": 0
                },
                {
                    "sent": "And being able to improve its performance overtime.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now when I join him on toddlers lap, we I had the opportunity to work with tendon driven systems.",
                    "label": 0
                },
                {
                    "sent": "These are systems which are driven by tendons.",
                    "label": 0
                },
                {
                    "sent": "For example, our hands are tendon driven systems and our hand in fact has up to 3032 tendons.",
                    "label": 0
                },
                {
                    "sent": "So that is already a very big number if we want to do control on a tendon driven system.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, just for the index finger, we have almost 11 tendence, from which we actually control 7.",
                    "label": 0
                },
                {
                    "sent": "And there are four tendons here which are used as a spring to transfer the force from the.",
                    "label": 0
                },
                {
                    "sent": "From the active tendence to the most distant.",
                    "label": 0
                },
                {
                    "sent": "Ligament and so.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a real tendon driven system in UW.",
                    "label": 0
                },
                {
                    "sent": "This is the action.",
                    "label": 0
                },
                {
                    "sent": "This is the anatomically correct desperate.",
                    "label": 0
                },
                {
                    "sent": "It is made based on the real human hand.",
                    "label": 0
                },
                {
                    "sent": "And it has exactly the same extensor mechanism with a human index finger.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "The only difference for the index finger is that we have only one extension of the index finger has two extensors.",
                    "label": 0
                },
                {
                    "sent": "There are Springs in the Motors that try to.",
                    "label": 0
                },
                {
                    "sent": "Simulate the nonlinear passive muscle stiffness.",
                    "label": 0
                },
                {
                    "sent": "We have 25.",
                    "label": 0
                },
                {
                    "sent": "Tendons 'cause we have actually three fingers here and the only sensing capability that we had with this system was actually the tendon excursions so we don't have join join angles cause people in bio mechanics of the hand believe that most likely we don't have sensors in our joints in the hand, so this hand has no, we have no information about the joint space configuration.",
                    "label": 0
                },
                {
                    "sent": "We have no sensing capabilities.",
                    "label": 0
                },
                {
                    "sent": "We only say sync ability that we have is on that end and excursions alright.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's a position?",
                    "label": 0
                },
                {
                    "sent": "Or is it?",
                    "label": 0
                },
                {
                    "sent": "This is position, yes.",
                    "label": 0
                },
                {
                    "sent": "This is Connie.",
                    "label": 0
                },
                {
                    "sent": "He's through spring at yeah, so there is some kind of electricity there, right?",
                    "label": 0
                },
                {
                    "sent": "But we treat it.",
                    "label": 0
                },
                {
                    "sent": "We are stiff too.",
                    "label": 0
                },
                {
                    "sent": "OK so it's not like it's going to.",
                    "label": 0
                },
                {
                    "sent": "So, but that is a very difficult system to work.",
                    "label": 0
                },
                {
                    "sent": "It has been around for 10 years.",
                    "label": 0
                },
                {
                    "sent": "And most people have been working with this system.",
                    "label": 0
                },
                {
                    "sent": "They've had been doing quasi static tasks like you know, you don't.",
                    "label": 0
                },
                {
                    "sent": "You try to find out what is the moment arm with stand on is more important but no movement was really actually involved and not real actual control.",
                    "label": 0
                },
                {
                    "sent": "So we thought that we should do some experiments with this and see whether or not we can apply the ideas from iterative particle control combined with that with the dynamic movement primitives for this.",
                    "label": 0
                },
                {
                    "sent": "Attractors, and so it turns.",
                    "label": 0
                },
                {
                    "sent": "So this is just a very basic figure, so the algorithm that we have, actually, I forgot to say that is called \u03c0 square, so it's like not your Pi square, but this Pi squared, which means policy improvement with in the graph OK?",
                    "label": 0
                },
                {
                    "sent": "And so we have here the so the.",
                    "label": 0
                },
                {
                    "sent": "The idea is again very very simple.",
                    "label": 0
                },
                {
                    "sent": "We use these trajectories in order to represent the position in the tendon space and then.",
                    "label": 0
                },
                {
                    "sent": "We have fixed control, gained fixed PD control for this particular experiment.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that that was a very pedagogical experiment for us.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because we find out that for tendon driven systems, whenever they come into contact with a with an object, you really need to have sharp changes in position intend on space.",
                    "label": 0
                },
                {
                    "sent": "So Amy effort actually to go and put a function approximator or parameterization.",
                    "label": 0
                },
                {
                    "sent": "Aim position would never been able to deliver and take the best out of your system in terms of performance.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we got some performance here, so the idea is that we have some experiments.",
                    "label": 0
                },
                {
                    "sent": "We have some sliders and the only thing that we measure from the slide there is voltage.",
                    "label": 0
                },
                {
                    "sent": "So the more you.",
                    "label": 0
                },
                {
                    "sent": "President the slider, the less vole, vole, nuts you yet.",
                    "label": 0
                },
                {
                    "sent": "So that that is the only information and the sensing of the tendon excursions alright fixed PD control.",
                    "label": 0
                },
                {
                    "sent": "Alright so we teach it.",
                    "label": 0
                },
                {
                    "sent": "We just provide it with a movement in the beginning, all right now we actually have a knob here and we try to use the index finger and thumb in order to be able to turn the NOB.",
                    "label": 0
                },
                {
                    "sent": "And so yeah, I mean we use it.",
                    "label": 0
                },
                {
                    "sent": "It turns out that, OK, we could get some performance.",
                    "label": 0
                },
                {
                    "sent": "We could improve their performance, but that was not really.",
                    "label": 0
                },
                {
                    "sent": "Enough it was not enough at all.",
                    "label": 0
                },
                {
                    "sent": "Then we actually said, well, we're not going to use any any policy parameterization.",
                    "label": 0
                },
                {
                    "sent": "And then what we're going to do is the simplest.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think of using a PD control, taking the game an using a stochastic financial equation.",
                    "label": 0
                },
                {
                    "sent": "Represented by this stochastic finance allocation and now the new control variable becomes the chance of the control gain per time unit and then on the other hand we are going to get the desired trajectories and again represent them as a stochastic financial equation alright.",
                    "label": 0
                },
                {
                    "sent": "And then now what?",
                    "label": 0
                },
                {
                    "sent": "We actually the control variable is again the the last with with the exceptions chains, but we don't use any parameterization.",
                    "label": 0
                },
                {
                    "sent": "OK, I mean we use parameterization but.",
                    "label": 0
                },
                {
                    "sent": "We don't use any function approximator.",
                    "label": 0
                },
                {
                    "sent": "These are stochastic financial equations and they are not smooth at all.",
                    "label": 0
                },
                {
                    "sent": "And so the so then we focused on just only one finger.",
                    "label": 0
                },
                {
                    "sent": "And so we have a demonstrated movement here.",
                    "label": 0
                },
                {
                    "sent": "So we demonstrate the movement so we can get.",
                    "label": 0
                },
                {
                    "sent": "We can get the trajectories from this demonstration.",
                    "label": 0
                },
                {
                    "sent": "Alright, and then we replay the movement.",
                    "label": 0
                },
                {
                    "sent": "So this is the trajectory from the passive demonstration and this is what happens when you replay it again another way if you replay it again, this is how far you can go.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you want to.",
                    "label": 0
                },
                {
                    "sent": "Press the slider as more as possible and then in order to be able to minimize your cost and the cost.",
                    "label": 0
                },
                {
                    "sent": "Again, is only the voltage that you get from the potentiometer from the slider.",
                    "label": 0
                },
                {
                    "sent": "That is the only sensing capability that we have.",
                    "label": 0
                },
                {
                    "sent": "OK an attendant excursions and so it turns out that if you do only gain optimization, so we've fixed right now the desired trajectories and we don't change the games.",
                    "label": 0
                },
                {
                    "sent": "We have fixed PP control.",
                    "label": 0
                },
                {
                    "sent": "So OK, you could actually do a little bit better, but again, that was not really satisfied.",
                    "label": 0
                },
                {
                    "sent": "So then we we set.",
                    "label": 0
                },
                {
                    "sent": "Let's actually learn both games and their trajectories and it turns out that you know it can basically just go and just.",
                    "label": 0
                },
                {
                    "sent": "Move the slider as far as possible and maximize its its.",
                    "label": 0
                },
                {
                    "sent": "Red lines here.",
                    "label": 0
                },
                {
                    "sent": "So so this is the cost that we have normalized cost from experiment.",
                    "label": 0
                },
                {
                    "sent": "So this experiment one, this experiment two and this is the normalized cost.",
                    "label": 0
                },
                {
                    "sent": "Now this experiments made it look to you as easy.",
                    "label": 0
                },
                {
                    "sent": "But in fact even repositioning the finger at exactly the same position it was challenging.",
                    "label": 0
                },
                {
                    "sent": "OK, we cannot actually reposition the finger always at exactly same position.",
                    "label": 0
                },
                {
                    "sent": "Alright, so there's a little bit of uncertainty with respect to from where you start to create your trajectory's.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a tendon driven system, and if you just go and pull all of attendance at the same time, that does not mean that you're going to create any any movement, right?",
                    "label": 0
                },
                {
                    "sent": "So the tendons can only improve, they cannot push.",
                    "label": 0
                },
                {
                    "sent": "And so then what we?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, it is we had some spring dynamics here, alright, and now what we want to do is we want to do exactly the same thing with John.",
                    "label": 0
                },
                {
                    "sent": "We don't change the cost at all, not nothing.",
                    "label": 0
                },
                {
                    "sent": "We just perform exactly the same experiment, but the idea is that now if the finger just go and.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, then if it goes and presses the slider very hard then it's going to lose contact and the spring is going to have a slight.",
                    "label": 0
                },
                {
                    "sent": "It's going to go again app and you get you don't minimize your cost and so it turns out that this is what happens before.",
                    "label": 0
                },
                {
                    "sent": "This is what happens with.",
                    "label": 0
                },
                {
                    "sent": "Learning there are some trials in which of course the finger is going to lose contact with the slider.",
                    "label": 0
                },
                {
                    "sent": "The slider is going to go back, doesn't get very good.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then at some point after learning, it just learns to barely actually move the slider and stay there, and this is again how the cost goes.",
                    "label": 0
                },
                {
                    "sent": "And that was a very nice experiment, because right now we see here what happens intend on excursions and there are many interesting phenomena we can characterize how biometric finger is people in bio mechanics say, well, this tendon is an extension of the standard is a flexor, but in fact it turns out that there are some tendons which play.",
                    "label": 0
                },
                {
                    "sent": "Both roles in different parts of the time.",
                    "label": 0
                },
                {
                    "sent": "Verizon, so in 10 years the vision is that you want to have a robot like this or a similar robot, and you want to have it play piano alright and after 20 years you want to have a human who can actually have a prosthetic arm and you know play, play, play piano.",
                    "label": 0
                },
                {
                    "sent": "But in order to be able to do that, you have to be able to ask the question how biomimetic the system which I'm using in order to understand how the brain controls the hand is.",
                    "label": 0
                },
                {
                    "sent": "Alright and then.",
                    "label": 0
                },
                {
                    "sent": "Develop.",
                    "label": 0
                },
                {
                    "sent": "Your research path towards this direct.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Excuse me you were doing damn programming in this.",
                    "label": 0
                },
                {
                    "sent": "Yes yeah.",
                    "label": 0
                },
                {
                    "sent": "I think it's bigger, smaller, well they get.",
                    "label": 0
                },
                {
                    "sent": "They are very noisy, OK and it is very difficult to say.",
                    "label": 0
                },
                {
                    "sent": "But as you saw from the talks, the talks at the time of the contact become bigger.",
                    "label": 0
                },
                {
                    "sent": "OK, when you really need to have contact in prayer and deliver force attentive factor that orcs increase and I can say that yeah at the very first contact the gains also increase but that was just two experiments alright.",
                    "label": 0
                },
                {
                    "sent": "I cannot give you answer 100% right.",
                    "label": 0
                },
                {
                    "sent": "Now that was just two experiments.",
                    "label": 0
                },
                {
                    "sent": "Now a little bit on the theoretical side.",
                    "label": 0
                },
                {
                    "sent": "I just want to say that.",
                    "label": 0
                },
                {
                    "sent": "This is the paper that you meant on right?",
                    "label": 0
                },
                {
                    "sent": "So at some point, as you know, besides the application site, I have also some theoretical interest and so I found this paper at some point.",
                    "label": 0
                },
                {
                    "sent": "This very nice paper that shows all the connections between relative entropy and free energy and how you can actually drive stochastic optimal control, But actually tells you how you can actually did find connections of stochastic optimal control of.",
                    "label": 0
                },
                {
                    "sent": "Difference of game theoretic optimal control with.",
                    "label": 0
                },
                {
                    "sent": "From the information theoretic point of view, and so I think you guys must know all about this I just.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To say that we have.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wrote this paper at CDC.",
                    "label": 0
                },
                {
                    "sent": "Is going to appear at CDC.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2012 we just try to make some small extensions to jump diffusions.",
                    "label": 0
                },
                {
                    "sent": "Becausw everybody knows that for diffusion process is linear in controls and dynamics.",
                    "label": 0
                },
                {
                    "sent": "You, son of theorem gives you this quadratic term in use, and that is a nice trick that actually one can use in order to derive stochastic optimal control from the information theory point of view.",
                    "label": 0
                },
                {
                    "sent": "By using the Sonos theorem, and I was asking the question can I say can I do something with?",
                    "label": 0
                },
                {
                    "sent": "With a jump diffusion processes, what happens with, for example with spiking rate?",
                    "label": 0
                },
                {
                    "sent": "Can I use a spiking rate as a control variable?",
                    "label": 0
                },
                {
                    "sent": "How it appears in the so?",
                    "label": 0
                },
                {
                    "sent": "It turns out that if you just take this very basic.",
                    "label": 0
                },
                {
                    "sent": "And I'll relationship between free energy and relative entropy, and you apply to diffusively Markov jump diffusion processes and you open a textbook on Markov diffusions.",
                    "label": 0
                },
                {
                    "sent": "You find what's the random nikodym derivative.",
                    "label": 0
                },
                {
                    "sent": "You get this term here, which is the term that you have these two terms which you have from the diffusion processes, but you have this extra term with this function of gamma.",
                    "label": 0
                },
                {
                    "sent": "This gamma adjust the ratio between the two person rates and.",
                    "label": 0
                },
                {
                    "sent": "I'm just sitting in this equation and say what is really the physical intuition and I haven't still find out any physical intuition, But it turns out that if these two, if they jump rates are actually equal, then all this guy goes to zero, and then you can go back to the classical stuff.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, we tried to systematize the connection between.",
                    "label": 0
                },
                {
                    "sent": "They work how birth and others have derived, departing the path integral control based on the dynamic programming principle and then systematically somehow how people think from the information theory point of view and for the class of diffusion processes with.",
                    "label": 0
                },
                {
                    "sent": "Define controlling noise.",
                    "label": 0
                },
                {
                    "sent": "Well, what is the connection between the two?",
                    "label": 0
                },
                {
                    "sent": "But I don't have time right now.",
                    "label": 0
                },
                {
                    "sent": "I guess I'm out of.",
                    "label": 0
                },
                {
                    "sent": "Square.",
                    "label": 0
                },
                {
                    "sent": "This place the role of exploration.",
                    "label": 0
                },
                {
                    "sent": "So two questions one is and how do I order the variance of humans relate to the speed of convergence of the cost function and also.",
                    "label": 0
                },
                {
                    "sent": "Say about if you convert your local global minimum 3 cause with respect to the first question and what we find out actually, is that OK, I mean.",
                    "label": 0
                },
                {
                    "sent": "You better, this particular control is very robust with respect to which variance you are using for your exploration.",
                    "label": 0
                },
                {
                    "sent": "Actually more robust.",
                    "label": 0
                },
                {
                    "sent": "Another policy gradient methods alright, I don't have a number to give you, but actually what happens is that what is important.",
                    "label": 0
                },
                {
                    "sent": "Sometimes when you do this when you apply these techniques to systems that have high order meaning many states, these are sampling method sampling based methods and So what happens is that the noise is actually getting filtering out.",
                    "label": 0
                },
                {
                    "sent": "So you don't see actually result in the outcome, so you don't learn.",
                    "label": 0
                },
                {
                    "sent": "So then what you have to do is some sample the noise, keep the noise freeze for some.",
                    "label": 0
                },
                {
                    "sent": "Time.",
                    "label": 0
                },
                {
                    "sent": "In order for this for this information to integrate it and you see some changes in your in the one question to ask is how can I do actually variable quantization reinforcement learning which goes it is related also to attention control.",
                    "label": 0
                },
                {
                    "sent": "How I can do that in automatically and the second question was in terms of local minima.",
                    "label": 0
                },
                {
                    "sent": "Well, we see that in the experiments, depending depending on the parameters, we sometimes find different.",
                    "label": 0
                },
                {
                    "sent": "Behaviors, but you know, in these systems I don't know what's really the global optimum.",
                    "label": 0
                },
                {
                    "sent": "You know, in order to be able to characterize what's really very local one.",
                    "label": 0
                },
                {
                    "sent": "But from the practical point of view, you know that is an algorithm that you know many people.",
                    "label": 0
                },
                {
                    "sent": "Stop use and that means that you know it's a very robust algorithm to use, so I cannot really characterize.",
                    "label": 0
                },
                {
                    "sent": "I cannot give you an.",
                    "label": 0
                },
                {
                    "sent": "Conclusive answer.",
                    "label": 0
                },
                {
                    "sent": "So in most all this before that you die.",
                    "label": 0
                },
                {
                    "sent": "Do theory or you get results.",
                    "label": 0
                },
                {
                    "sent": "So you if you if you want to use that you should actually rather use model predictive control rather than this.",
                    "label": 0
                },
                {
                    "sent": "So we have to comment on that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I think.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think what ammo was going with his in learning stuff was going from, you know, the point of view of someone just going and sampling in the full state in the full state space.",
                    "label": 0
                },
                {
                    "sent": "And this is the way how I was thinking when I start reading your your papers.",
                    "label": 0
                },
                {
                    "sent": "But here is Stephen Shaw.",
                    "label": 0
                },
                {
                    "sent": "OK Guy who has so much experience on real robots alright and you know in this lab you can from the moment that you think one idea until the moment where you are doing the real experiment.",
                    "label": 0
                },
                {
                    "sent": "Edit It is basically five.",
                    "label": 0
                },
                {
                    "sent": "It is 5 days.",
                    "label": 0
                },
                {
                    "sent": "It's a very systematic, you know so and then he tells me that, well, I think we should really do it the other way.",
                    "label": 0
                },
                {
                    "sent": "You know we should basically treat it as a policy gradient so.",
                    "label": 0
                },
                {
                    "sent": "So I think what the disadvantage of this ideas is that what happens?",
                    "label": 0
                },
                {
                    "sent": "Let's say if these ideas have to do with learning in a long time horizon, you know you have to execute the trajectory.",
                    "label": 0
                },
                {
                    "sent": "But I do believe that you know when we become older and we want to play one of my favorite Mozart pieces, you know, we actually do learn in smaller timescales, so there are some tasks which require.",
                    "label": 0
                },
                {
                    "sent": "Learning in smaller timescales, so learning in smaller times case timescales it will be a little bit challenging.",
                    "label": 0
                },
                {
                    "sent": "Model predictive control on the other side is for adapting, you know in smaller timescales, but it relies on a wrong model and so far we have indication that it has been applied.",
                    "label": 0
                },
                {
                    "sent": "It works well on simulation, so somehow combine them combining them OK in order to be able to perform control and learning in long and sort.",
                    "label": 0
                },
                {
                    "sent": "Time scales, I think, is really the next step to control now how we are going to do that.",
                    "label": 0
                },
                {
                    "sent": "If somebody pays me $500,000, gives me some good students and a position in a University.",
                    "label": 0
                },
                {
                    "sent": "Time invariant policies.",
                    "label": 0
                },
                {
                    "sent": "Whether you mean by the emerging policies.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "Dataset options.",
                    "label": 0
                },
                {
                    "sent": "Well, there are two ways how you can do it.",
                    "label": 0
                },
                {
                    "sent": "One is by having the filter change overtime or having it done not change overtime.",
                    "label": 0
                },
                {
                    "sent": "Well, when you are using this DMP's you know is you don't see so much difference in just, you know like the profiles become a little bit more noise you know, but I guess for the case where you don't want to use any parameterization like the case with a tendon driven finger in which we had thousands of parameters to optimize right?",
                    "label": 0
                },
                {
                    "sent": "We have six tendons.",
                    "label": 0
                },
                {
                    "sent": "Use overtime, I think in that case you know keeping you constantly.",
                    "label": 0
                },
                {
                    "sent": "We would not park.",
                    "label": 0
                },
                {
                    "sent": "Might take some comfort from the fact that I have a colleague who studies motion control and four arm movements and who has more or less convincingly established that learning has two time constants for those tasks, OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, well thank you.",
                    "label": 0
                }
            ]
        }
    }
}