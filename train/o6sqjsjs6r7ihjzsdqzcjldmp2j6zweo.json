{
    "id": "o6sqjsjs6r7ihjzsdqzcjldmp2j6zweo",
    "title": "Borrowing Strength, Learning Vector Valued Functions and Supervised Dimension Reduction",
    "info": {
        "author": [
            "Sayan Mukherjee, Department of Statistical Science, Duke University"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_mukherjee_bslvvfsdr/",
    "segmentation": [
        [
            "I came into statistics from machine learning and being a Duke you are more or less religiously basean.",
            "So maybe the talk will follow my journey.",
            "Starting from a more regularization perspective, going towards a Bayesian perspective, and there are two manifolds in the talk and, well, we'll find out what they are.",
            "But I'm going to talk about the problem of supervised dimension reduction if I can get this thing to work.",
            "And maybe this does something.",
            "I can just that.",
            "LR."
        ],
        [
            "OK.",
            "So one way of thinking about the problem of supervised dimension reduction is.",
            "Whoa.",
            "Let's see.",
            "Yeah, I'm working on a there we go.",
            "There we go.",
            "OK good.",
            "It's alive.",
            "OK so yeah, so this goes back to ideas of people like Fisher and there's this whole idea of sufficiency.",
            "OK and?"
        ],
        [
            "You know you can say if I have a bunch of points drawn IID from a normal well, I don't need the data, I can estimate the mean and the variance and throw out the data.",
            "So that's one way of thinking about dementia reduction and or supervised dementia."
        ],
        [
            "Induction.",
            "In this case, let's start with the model.",
            "Let's start with the regression model.",
            "My ex's are multivariate.",
            "These are my.",
            "Explanatory variables and for and why is my response variable?",
            "It's univariate could be continuous.",
            "It could be discrete.",
            "Either way, I have some type of noise."
        ],
        [
            "Model and I have data drawn IID from some joint distribution."
        ],
        [
            "And.",
            "And the basic idea is that.",
            "We can replace X with some lower dimensional set of parameters, let's say Theta of X&P is going to be much less than D."
        ],
        [
            "And somehow this is going to be useful and kind of the belief is that.",
            "The data that you're looking at.",
            "These systems are inherently low dimensional, and the variation of interest in these systems is captured by low dimensional manner."
        ],
        [
            "Or submanifold so."
        ],
        [
            "We come back to the model.",
            "This is my linear model and what you're going to assume is there's some submanifold S of Y given X.",
            "Such that.",
            "If I project onto this submanifold.",
            "OK then.",
            "Why is going so once they project onto that I can throw out my data, So what am I saying that Y is going to be conditionally independent of X given the submanifold?",
            "Again, this whole idea of.",
            "Of sufficiency or some."
        ],
        [
            "Statistics.",
            "So let me let me give you a picture.",
            "OK if you talk about manifolds and so on so forth, you have to show the Swiss roll because that's what everyone in the world cares about and it's a very important data set so.",
            "So this is a Swiss roll and what I've done is we just put a regression function on the Swiss roll and this Swiss roll is stretched more in this direction.",
            "Then if you were done rolling now, if you were to use some type of.",
            "Unsupervised mesh method.",
            "Some type of diffusion method or whatever.",
            "Not it will unroll the Swiss roll, but it will be in the.",
            "Unroll it in this direction because there is greater variation in this direction now.",
            "This is a supervised dimension reduction method that does linear projections can capture nonlinear structure, but it's doing linear projections and what it's going to do is project down to two dimensions because that's all you really need to be able to capture the variation of the Swiss roll.",
            "You could also do nonlinear projection which would unroll the Swiss roll in the correct direction and that's this example.",
            "I'm not going to talk about the.",
            "Nonlinear dimension reduction or the unrolling.",
            "Today I'm going to talk about.",
            "The linear projection.",
            "'cause it kind of has well somewhat of a cleaner framework."
        ],
        [
            "OK.",
            "So this is exactly what I said in this talk.",
            "The projection is just going to be taking X, projecting it onto a subspace.",
            "Which is D dimensional."
        ],
        [
            "And this gives rise to this type of semi parametric model.",
            "And the space B or the span of B is what's called the dimension reduction space or subspace.",
            "There are various names for it.",
            "Sometimes it's called the Affective dimension reduction subspace.",
            "If you're Dennis Cook, you're going to call it the Central Dimension reduction subspace.",
            "These all FS light varying names, but at the end of the day it's a subspace you project onto it.",
            "It preserves predictive information."
        ],
        [
            "OK, so."
        ],
        [
            "So.",
            "First part of the talk.",
            "We're going to assume that the marginal distribution row of X is concentrated on a manifold.",
            "This is the first manifold we have in the talk, and."
        ],
        [
            "This is a low dimensional manifold.",
            "The first way I'm going to think about dimension reduction is very geometric and it's going to be driven through the gradient of the regression function.",
            "Somehow I'm going to try to show you that the gradient of the regression function has information and gives you this space.",
            "OK, so this is well, the gradient of the regression function."
        ],
        [
            "And what we will work with a lot.",
            "Is a multi output object.",
            "You can think of this as well.",
            "P dimensional is multi output and I can if I have the gradient estimate I can construct the outer product of it which is all I've done here and this is called the gradient outer product.",
            "And if you look at each element of this matrix you can think of it as just the inner product between two partial derivatives.",
            "And in some sense, why is that intuitively doing something it's telling you that this coordinate is changing with?",
            "Why this other coordinates changing with Y?",
            "And they're both changing with Y together in the same way, so intuitively, that's what this is doing and.",
            "The eigen decomposition of this gamma gives me the."
        ],
        [
            "My piece, so if I actually had this gamma matrix, this gradient outer product.",
            "And do a spectral decomposition of it to get these be survives.",
            "This gives me the space that I should."
        ],
        [
            "Project onto and you can basically see that from some calculus.",
            "So if I take a vector V OK, that's somewhat perpendicular, let's say perpendicular to BI, right?",
            "I take the partial derivative with respect to that.",
            "Well, it's only not going to be 0.",
            "If it's in the span of the bees, OK, so anything that's not in the span of these bees is going to be orthogonal to it.",
            "So again, with some calculus you can show."
        ],
        [
            "That this goes through.",
            "If you want to think about this more statistically.",
            "Let's look at the linear case here.",
            "I've just a linear function with noise.",
            "I can look at three objects.",
            "This is my classic covariance of X.",
            "This is just a variance in why, and this is what's called the.",
            "Covariance of the inverse regression.",
            "It's the inverse regression 'cause they flipped Y&X OK and, well, we'll call this Omega."
        ],
        [
            "The gradient is basically equal to.",
            "The variance in Y times.",
            "Well, the inverse of the variance of X times Omega times the inverse of the variance in X.",
            "Intuitively, why is this useful?",
            "Or why might this make sense?",
            "Well, if I care about those axes that are varying in Y.",
            "This is capturing that right, but I want to null out or divide out those axes that are just very right, and that's what this does.",
            "So this, you know, it has some reason."
        ],
        [
            "Intuition, and if F of X is smooth and not necessarily linear, then how do you want to think about?"
        ],
        [
            "You just do the same thing you can think of partitioning the functions in little sections.",
            "Each section is."
        ],
        [
            "Here."
        ],
        [
            "And they."
        ],
        [
            "And you just."
        ],
        [
            "Compute all of these."
        ],
        [
            "Quantities locally.",
            "And then you just average up and you get this gradient outer product back.",
            "No.",
            "Yeah, so I I have my basically have a marginal distribution on X, right?",
            "I'm going to partition that into little blocks within each little block.",
            "It's linear, right?",
            "So within each little block I can compute the inverse regression.",
            "The covariance of the inverse projection.",
            "I can compute the covariance of X in that block.",
            "I can compute the variance in Y in that block and me is just a measure of that block, right?",
            "And then I just average all of 'em over all of these little partitions, OK?",
            "And this gives me you can show that you're getting this gradient outer product back.",
            "Now.",
            "If you're a geometer at this point, you would be very angry with me and you would say you're a very bad man 'cause you're averaging what a geometer would want to do is paste all of these together.",
            "And if you pasted them together, you'd be doing nonlinear dimension reduction.",
            "And one can do that, but.",
            "There's more subtleties there but but this is just averaging 'em and that's why we get this gradient outer product."
        ],
        [
            "So how do I estimate the gradient well?",
            "One way of doing it is by Taylor expanding.",
            "So if I just Taylor expander on point FJ for any XJ near XII can write why?"
        ],
        [
            "Approximately that way, and if you think about that.",
            "If I estimate the gradient with effect, then I can write down this minimization problem.",
            "Right, this is just least squares and the only thing is I have a waiting here so points that are very power far apart.",
            "Should be near 0 because Taylor expansions don't make sense of things are very far apart, points near real by or going to be close to one.",
            "And if you want to do this, it's actually interesting if you want to think about this abbasian problem and you want to start thinking about a likelihood here and you really want to think about WI J, you should go to geostatistics.",
            "Because those ideas of very grams of Nuggets are really, really inherent.",
            "If you want to think about this basean, but we're not going to now.",
            "But so this is just."
        ],
        [
            "Loss.",
            "And then you regularize this.",
            "Basically, with the norm RKHS type Norman, this is a regularization problem and this gives me an estimator of a gradient."
        ],
        [
            "Now I don't know why I'm supposed to go.",
            "Yeah, yeah.",
            "But you're not just another name, but the whole idea.",
            "You want to find the beavex kind of project down like that night is going well, but you apply this move without knowing that the matrix we're going to have this in this in the.",
            "Gender regularization with that right, right, right?",
            "So this is.",
            "So if you look at this.",
            "You ask, well, you have this K and the way I'm actually constructing the K is.",
            "I'm doing it elementwise over all of those P. So my apriori assumption is that each of these dimensions are independent, which in some sense is idiotic because.",
            "You know, you know that they're not, and you want to find that right?",
            "So yes.",
            "Now, computationally, you'll see that I'm not really doing that in two slides.",
            "Again, This is why this is very much of a geometric way of thinking about it.",
            "I'm going to think about and rephrase this whole problem probabilistically, and then it's going to be much more immediate.",
            "What you were just saying, but yes."
        ],
        [
            "So.",
            "Here's my kernel.",
            "This actually has.",
            "Basically a representer theorem, and each of these sees instead of being a scalar are going to be dimensional vector.",
            "Right, so if you think about the parameters of this, there are NP dimensional vectors, so that could be a lot of parameters to estimate.",
            "Now it turns out that."
        ],
        [
            "That's actually not how many parameters you have, because if I look at the Taylor expansion.",
            "Well, this one is good enough, right?",
            "You see that everything is going to be living in the space band by differences between data points.",
            "So if I look at any difference between XI and XJ, take those basically, do the SVD on that.",
            "That's going to be the span of my solution, so it's really much much lower at worst N squared.",
            "So if any smaller than P, it's a lot smaller."
        ],
        [
            "And that exactly addresses what you are talking about because.",
            "The structure is actually can't be P dimensional, right?",
            "It can only be at most N square in terms of span of these differences."
        ],
        [
            "OK. And.",
            "Again, you can define a gram matrix and I can come up with the gradient outer product."
        ],
        [
            "So you can actually.",
            "Prove something about this.",
            "If you cared too.",
            "If you believe that the marginal distribution probably should be in a here is concentrated on compact Romanian manifold and there's an isometric embedding.",
            "Then, if we assume that the.",
            "The measure on the manifold.",
            "That density is Holder continuous, and if I look at the measure it's not concentrated on the boundary.",
            "Right, because otherwise derivatives aren't going to make any sense."
        ],
        [
            "Then you can prove the following.",
            "This is a gradient on the manifold.",
            "This is Adi dimensional object because the manifold is D dimensional.",
            "This is my estimate.",
            "It's a P dimensional object.",
            "Fiestar is a dual of the embedding, so it comes from the ambient space back down to the manifold.",
            "And then the fee is just the derivative effectively of that.",
            "So this is taking this P dimensional object back to D. And basically I can show that the L2 norm on the manifold.",
            "This converges at a rate that's.",
            "In the dimension of the manifold and not the ambient space.",
            "OK, so this has been really a very geometric function estimation perspective on this problem of supervised dimension reduction.",
            "Now I'm going to flip gears and I'm going to think about the problem supervised dimension reduction from a probabilistic."
        ],
        [
            "Spective oh I forgot I was going to talk about multi task that might be relevant here so I gave it to you for a single task OK. You can do the same thing from multitask."
        ],
        [
            "And what you would do is think about.",
            "Each function F sub TT again indexing task as a baseline.",
            "And a task specific.",
            "So again, this type of hierarchical modeling."
        ],
        [
            "And then we."
        ],
        [
            "So everything like we did before.",
            "We estimate these gradients.",
            "Right, just a natural regularization way that you would think about doing."
        ],
        [
            "And you can get all these T + 1 gradient outer product."
        ],
        [
            "Matrices and this gives you something about supervised dimension reduction across all the tasks, and this is task specific and I'll show you pictures of this.",
            "What this does soon, OK?"
        ],
        [
            "So now.",
            "I was going to think about things probabilistically, so you know."
        ],
        [
            "Everyone's pretty familiar with PCA algorithmically.",
            "What is PCA?",
            "You take your data, you construct a covariance matrix, you do an eigen decomposition of it.",
            "You go on with your life.",
            "Now if you want to think about PCA."
        ],
        [
            "Ballistically how do you do it?",
            "Well, you do it as a factor model.",
            "OK, you say that in my data X is coming from a normal.",
            "It's got mean mu plus a new and some covariance Delta.",
            "AR my factor loadings noir my factors.",
            "So really new is something lower dimensional.",
            "It's just D dimensional object and new are the projections or the factor loadings that take me from this P dimensional space down to this D dimensional space and use just some mean OK?",
            "So this is all well and good, so this is probabilistic PCA.",
            "And why did I put this up?",
            "What happens if I want to do supervised dimension reduction?",
            "I can work in the same framework."
        ],
        [
            "And."
        ],
        [
            "Of again, use a latent variable."
        ],
        [
            "And this was proposed by Dennis Cook, and it's called principle fitted components an.",
            "If I ignored this, this would be like PCA, but now what I'm going to say is.",
            "I'm going to think about X right sub Y and what do I mean by that?",
            "Very simple case.",
            "Let's say Y is zero and one.",
            "OK, I'm going to have a different distribution for why being zero different distribution for wide being one LDA is a classic example of this linear discriminant analysis, and that's all I'm doing.",
            "I'm talking bout X at a particular value Y.",
            "No, it's the same model as before.",
            "I have this new.",
            "The only difference is that those factors right are dependent on why are really based on why, and that's exactly what I wrote down here.",
            "Now.",
            "This works fine.",
            "When things like PCA work, where will something like this break if I have multi modality, right?",
            "If I have a class with different modes or if there's local structure, so in this whole.",
            "Supervised dimension reduction framework.",
            "This class of model comes under the framework of something called sliced inverse regression.",
            "These are called inverse regression models.",
            "And formally, these work when you have something called an elliptic condition is met, which basically says X given Y, that density is plus or minus unimodal.",
            "Otherwise it will break."
        ],
        [
            "So in general for manifolds it won't really break.",
            "Oh yeah, I'm sorry.",
            "The dimension reduction space is just Delta minus one.",
            "So."
        ],
        [
            "How do you extend this?",
            "Right to a manifold or to multi clusters so."
        ],
        [
            "If you think about geometry, you say, well, manifolds are locally Euclidean.",
            "If you."
        ],
        [
            "Statistician.",
            "You say everything in the world is a mixture model.",
            "I can take normals and I can mix some like no ones business.",
            "Right and well, maybe someone's business, but but you know so that you could.",
            "Just putting these two ideas together so you go back to the."
        ],
        [
            "It's a model, but you think of it as a mixture model and that's."
        ],
        [
            "We did here.",
            "Now I have a mu YX.",
            "So different parts of the SpaceX is going to have a different distribution.",
            "And I place a distribution on new.",
            "And.",
            "We call this basean mixtures of inverses, 'cause it's mixing a bunch of inverses.",
            "What this new why is depends on whether wise, continuous or it's discrete.",
            "So if yyou"
        ],
        [
            "Discrete and you can show that given that model.",
            "The dimension reduction Space is a span of Delta minus one 8."
        ],
        [
            "So I can give you a sampling distribution.",
            "Which I just did."
        ],
        [
            "Now again, what the key question is, what is this model on Y or this GY?",
            "And if your data is discrete, let's say it's categorical, so I've see categories.",
            "So for Y, being any of the categories, I can basically place down a distribution.",
            "If I were a reasonable person, I'd say, well look, they're not going to be more than 10,000 classes in the universe, so I'll just write down.",
            "Additionally, distribution with up to 10,000.",
            "I'm not a reasonable."
        ],
        [
            "Person.",
            "So you become a silly nonparametric person and you say I'll put down.",
            "Additionally process which is doing the same thing but with some fictional Infinity.",
            "And and that, that works for categorical variables, what do you do for continuous variables?",
            "I'm not going to talk about that here, 'cause it's more complicated and maybe not so illuminating, but I will say in words what you do.",
            "So like I said, a version of this was called sliced inverse regression, and what they did there was you look at your values Y and you been them, and for each bin you compete the mean of the X is in that bin.",
            "Take that outer product in average.",
            "So there it was.",
            "Really important to slice wise, no.",
            "Here what you do is you basically can put down some type of dependent Dursley process.",
            "Just saying this distribution on why is going to be very similar for wise that are really close, meaning strongly dependent.",
            "Why's that are very far apart.",
            "It will be quite independent.",
            "So basically you can put together this type of display dependently disk."
        ],
        [
            "Bution and sample from that."
        ],
        [
            "Anne.",
            "So here's my likelihood."
        ],
        [
            "And basically."
        ],
        [
            "Going to do."
        ],
        [
            "Post your inference basically."
        ],
        [
            "Using MCM"
        ],
        [
            "She."
        ],
        [
            "And.",
            "It turns out that in this case.",
            "Yeah, so these are my samples.",
            "I've amuen a Delta minus one in a new."
        ],
        [
            "And for almost all of these.",
            "Parameters everything except for knew.",
            "I can write this as a Gibbs sampler because it has really nice conditioning properties.",
            "New does not.",
            "You have to run new as a metropolis Hastings, that's the one."
        ],
        [
            "Horrible, that's not so nice.",
            "And so this is drawn from a normal.",
            "This is an inverse Wishart, again a normal."
        ],
        [
            "New is a bit more work."
        ],
        [
            "Now.",
            "Now we come to the second manifold in this talk.",
            "So I have a bunch of draws of the deltas and these days and from that.",
            "I have draws of these subspaces OK, so each MCMC is really giving me a draw of my subspace that I believe that I'm projecting onto."
        ],
        [
            "Now.",
            "I think one of the things that's very interesting is the way manifold learning has caught on in machine learning has really been a focus on the manifold in the data space.",
            "Classically, in statistics, manifold thoughts really about manifolds in the parameter space and properties of that and so on and so forth and classically.",
            "If you want to think about Cramer Rao bounds and theories like that, those are really about geometric properties on the parameter space, so again this is coming back to the parameter space.",
            "I'm estimating subspaces, and if I talk about model identify ability.",
            "Each of the columns.",
            "Are individually not identifiable, but that subspace is identifiable in this framework and this factor modeling framework so.",
            "I have a bunch of points.",
            "That are subspaces, so these are points on what's called the Grassmann manifold, and this manifold has natural structure.",
            "It has a natural geodesic and so on and so forth.",
            "So one question."
        ],
        [
            "And this has some implications, so one would be how do I define a summary?",
            "How do I define the posterior mean right?",
            "Do I just average all of 'em?",
            "Or what do I do?"
        ],
        [
            "And the point is, you really should, so there's something called the Karcher mean.",
            "So if I have two subspaces, WMU.",
            "I can write them down as orthonormal basis W&V, and I can compute the distance between these two.",
            "So this gives me a natural Romanian metric between two subspaces, and given this."
        ],
        [
            "Patrick I can define the mean.",
            "Which is just going to be given all of my MCMC draws.",
            "It's going to be the point that is at the geodesics center."
        ],
        [
            "Similarly, I can define.",
            "A variance now.",
            "One thing that's really interesting.",
            "And I didn't talk about it.",
            "Is what happens if I was also trying to estimate D, which is a dimension?",
            "We have ways of kind of doing that.",
            "And what happens if I get different draws that have different dimensions?",
            "Right mathematically dot distance is Infinity.",
            "But no, that's not a meaningful thing for me to say.",
            "I can't say that these things are infinitely far apart.",
            "So how do I think about that?",
            "How do I formalize that?",
            "How do I characterize that I think is a really, really interesting problem?",
            "And this is another point that I just want to make.",
            "Is that many of us.",
            "Do factor modeling.",
            "Many of us do probabilistic versions of factor modeling.",
            "Almost none of us.",
            "Think about the geometry of those factors.",
            "So if you were doing factor models, you're looking at covariance matrices plus or minus.",
            "OK. Now if I'm looking at covariance matrices, I'm looking at summaries of them.",
            "Again, covariance matrices have a natural geodesic structure which we never worry about.",
            "Maybe we don't need to, I don't know, but again, we almost never worry about that when we think about summaries of them.",
            "And it's kind of interesting that we don't, but anyways, so I can also give you about notion of uncertainty of these."
        ],
        [
            "Subspaces?",
            "So.",
            "This is just one example, but I think a very interesting problem is that I wanted to estimate subspaces and I did it by using a factor model at the end of the day.",
            "Question is, can I be much more direct?",
            "Can I actually put distributions on subspace?",
            "Is going to put distributions on the grassman manifold?",
            "Can I put interesting distributions on that?",
            "Can I put it to entering distributions on mixtures of grasslands and directly estimates subspaces or mixtures of subspaces and then plug that immediately into my likelihood?",
            "If I had nice ways of doing that, I could, and some people are thinking about this.",
            "So we come back to this with."
        ],
        [
            "Pro 'cause that's the only data set in the world we care about.",
            "And well, here's my."
        ],
        [
            "Swiss roll and.",
            "I know the real dimensions that are relevant.",
            "There are three of them and I have my estimates beta hat."
        ],
        [
            "And so I can ask."
        ],
        [
            "Different algorithms are going to do.",
            "So all of these algorithms don't do well.",
            "These are all algorithms that are kind of global.",
            "They don't think about things locally.",
            "The one up top in red is our Bayesian mixture of inverses.",
            "It does very well.",
            "This one right here is a gradient learning method I told you about earlier and this is what's called local sliced inverse regression which is.",
            "Again, doing this localization but using.",
            "Really nearest neighbors type ideas to do it.",
            "No problem."
        ],
        [
            "OK there."
        ],
        [
            "Is my posterior variance?",
            "You can try to estimate D by various ways and we're just saying that we don't do a horrible job of it."
        ],
        [
            "Um?",
            "This is a case with digits.",
            "We have 3, five and eight."
        ],
        [
            "I'm going to look at two classification problems, 3 versus 8 and."
        ],
        [
            "Verses 8."
        ],
        [
            "Onion training samples.",
            "So if you look at the subspace that separates 3 from 8.",
            "It looks like this and it's kind of cool.",
            "'cause what's the difference which is at three?",
            "This is an 8.",
            "What's the difference?",
            "These two points?",
            "These two little regions, similarly for five versus 8.",
            "You see this region as the difference in the subspace at differenti."
        ],
        [
            "Get some.",
            "If I wanted to do a multi task problem I can look at three versus 85 versus 8 and three and five versus 8.",
            "Let me tell you OK and you apply."
        ],
        [
            "This."
        ],
        [
            "And this is the top features or the top projection for three and five versus 8."
        ],
        [
            "This is a top for three verses."
        ],
        [
            "Right, and this is the top."
        ],
        [
            "For five versus 8.",
            "So this is what happens if you apply this to all 10 digits you.",
            "If you use this nonlinear ideas versus a linear idea, you do better.",
            "It makes a big difference for 8:00 and 5:00.",
            "I did not showing this, but the reason actually why I got into this multi tasking these constructions for these problems, so I do a lot of work on cancer and computational biology.",
            "So we actually applied similar model to modeling tumor progression through stages, so these are my different tasks going from less serious, more serious.",
            "Less here it's more serious and again you can apply these."
        ],
        [
            "Yes, there this is just one example of cancer classification.",
            "This is a data set that's been beat to hell.",
            "There are 19 samples of this type of leukemia.",
            "19 samples of a different type of leukemia.",
            "It just so happens there are two sub categories here.",
            "OK, I tell the model that these are two classes, but I don't tell it that there are subclasses here you do this."
        ],
        [
            "Projection onto just two dimensions.",
            "One class completely falls to one point, the other two are somewhat separated, so it is capped.",
            "Picturing these local."
        ],
        [
            "Information.",
            "And that's it."
        ],
        [
            "Pizza looked at this kind of linear dimensionality reduction, but there we were just doing kind of dumb gradient descent under projection.",
            "So I just I don't understand what you're saying about the grading at the problem matrix.",
            "So basically if I can estimate that.",
            "This kind of construction, and basically I don't need information that should give me those extra exactly and intuitively you would think it would because you know what is that doing is just telling you the directions right that the function is moving at the most, and then if I just average over those right just paste average all these tangent planes.",
            "I'm good to go in terms of the relevant directions right now if you want to be smarter and paste them, you could paste all of the tangent planes, but that's a harder problem 'cause you know you lose points and blah blah blah.",
            "Yeah exactly.",
            "Sorry.",
            "For your protection.",
            "So.",
            "So you don't.",
            "Is a short answer.",
            "The more honest answer is that.",
            "There are certain parameters that you have to worry about D, especially right.",
            "What is the dimensionality of this thing right and?",
            "What we find.",
            "Is.",
            "We tend to overestimate it.",
            "I think that's because we had some smoothing and we have these priors that are shrinking, right?",
            "I think if from people who've done this from a straight likelihood perspective, they can sure show asymptotically that they get the right D, but we do overestimated.",
            "But I think it's OK because it lets us make inferences for smaller sample sizes, right?",
            "But so that's one of them.",
            "And.",
            "In terms of other initializations, not, not so much now.",
            "I mean, in the regularization version you got to pick Lambda, but.",
            "And you know who really knows how to do that, but that's the difference.",
            "Chris is thinking.",
            "So one thing I think there's also relations with some of the multi task kernel learnings.",
            "But I couldn't quite workout when you mentioned the multi task surface the way you're suggesting.",
            "Multi task in the linear case is how you combining across the tasks are using the same shared.",
            "Yes.",
            "So so if."
        ],
        [
            "I looked at it.",
            "Let's just look at this case OK?",
            "So this is what's common, so this shared across the three on the five, right?",
            "So I could basically, if I wanted to look at the projections that are common across all task, this is a subspace I project onto OK."
        ],
        [
            "And this is looking at just three versus 8.",
            "So if I were only caring about 3 versus 8, I'd add the previous one with this one and I project onto that.",
            "Right and then similarly."
        ],
        [
            "We only cared about the five versus 8.",
            "I take the previous one and add this and so on so forth.",
            "Now, if you're using this to try to interpret certain things, then you could just look at it independently.",
            "It wouldn't be the most predictive because it's only looking at that part, which is, you know, not of the other one.",
            "But you can ask that question what is the subspace that is really.",
            "You know specific for five versus 8.",
            "I think you mentioned she said that in sadistic so people are looking at manifolds over parameters rather than well.",
            "I mean classically classic Lee.",
            "Duality between those things.",
            "Why?",
            "I would say that so this is, in my view, this is Bayesian statistics.",
            "Right you have or statistics.",
            "In general you have a distribution in your model space.",
            "I mean in the data space you have a distribution on your parameter space and inference is nothing but getting from one to the other.",
            "I mean there should be a duality of their structure, but you know formalizing what that is.",
            "I think yeah, I don't know.",
            "I think it's very interesting, I don't know.",
            "Questions.",
            "Oh, I would like to thank the organizers."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I came into statistics from machine learning and being a Duke you are more or less religiously basean.",
                    "label": 0
                },
                {
                    "sent": "So maybe the talk will follow my journey.",
                    "label": 0
                },
                {
                    "sent": "Starting from a more regularization perspective, going towards a Bayesian perspective, and there are two manifolds in the talk and, well, we'll find out what they are.",
                    "label": 1
                },
                {
                    "sent": "But I'm going to talk about the problem of supervised dimension reduction if I can get this thing to work.",
                    "label": 1
                },
                {
                    "sent": "And maybe this does something.",
                    "label": 0
                },
                {
                    "sent": "I can just that.",
                    "label": 0
                },
                {
                    "sent": "LR.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So one way of thinking about the problem of supervised dimension reduction is.",
                    "label": 1
                },
                {
                    "sent": "Whoa.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm working on a there we go.",
                    "label": 0
                },
                {
                    "sent": "There we go.",
                    "label": 0
                },
                {
                    "sent": "OK good.",
                    "label": 0
                },
                {
                    "sent": "It's alive.",
                    "label": 1
                },
                {
                    "sent": "OK so yeah, so this goes back to ideas of people like Fisher and there's this whole idea of sufficiency.",
                    "label": 0
                },
                {
                    "sent": "OK and?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know you can say if I have a bunch of points drawn IID from a normal well, I don't need the data, I can estimate the mean and the variance and throw out the data.",
                    "label": 0
                },
                {
                    "sent": "So that's one way of thinking about dementia reduction and or supervised dementia.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Induction.",
                    "label": 0
                },
                {
                    "sent": "In this case, let's start with the model.",
                    "label": 1
                },
                {
                    "sent": "Let's start with the regression model.",
                    "label": 0
                },
                {
                    "sent": "My ex's are multivariate.",
                    "label": 0
                },
                {
                    "sent": "These are my.",
                    "label": 0
                },
                {
                    "sent": "Explanatory variables and for and why is my response variable?",
                    "label": 0
                },
                {
                    "sent": "It's univariate could be continuous.",
                    "label": 0
                },
                {
                    "sent": "It could be discrete.",
                    "label": 0
                },
                {
                    "sent": "Either way, I have some type of noise.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model and I have data drawn IID from some joint distribution.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea is that.",
                    "label": 0
                },
                {
                    "sent": "We can replace X with some lower dimensional set of parameters, let's say Theta of X&P is going to be much less than D.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And somehow this is going to be useful and kind of the belief is that.",
                    "label": 0
                },
                {
                    "sent": "The data that you're looking at.",
                    "label": 1
                },
                {
                    "sent": "These systems are inherently low dimensional, and the variation of interest in these systems is captured by low dimensional manner.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or submanifold so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We come back to the model.",
                    "label": 0
                },
                {
                    "sent": "This is my linear model and what you're going to assume is there's some submanifold S of Y given X.",
                    "label": 0
                },
                {
                    "sent": "Such that.",
                    "label": 0
                },
                {
                    "sent": "If I project onto this submanifold.",
                    "label": 0
                },
                {
                    "sent": "OK then.",
                    "label": 0
                },
                {
                    "sent": "Why is going so once they project onto that I can throw out my data, So what am I saying that Y is going to be conditionally independent of X given the submanifold?",
                    "label": 0
                },
                {
                    "sent": "Again, this whole idea of.",
                    "label": 0
                },
                {
                    "sent": "Of sufficiency or some.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Statistics.",
                    "label": 0
                },
                {
                    "sent": "So let me let me give you a picture.",
                    "label": 0
                },
                {
                    "sent": "OK if you talk about manifolds and so on so forth, you have to show the Swiss roll because that's what everyone in the world cares about and it's a very important data set so.",
                    "label": 0
                },
                {
                    "sent": "So this is a Swiss roll and what I've done is we just put a regression function on the Swiss roll and this Swiss roll is stretched more in this direction.",
                    "label": 0
                },
                {
                    "sent": "Then if you were done rolling now, if you were to use some type of.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised mesh method.",
                    "label": 0
                },
                {
                    "sent": "Some type of diffusion method or whatever.",
                    "label": 0
                },
                {
                    "sent": "Not it will unroll the Swiss roll, but it will be in the.",
                    "label": 0
                },
                {
                    "sent": "Unroll it in this direction because there is greater variation in this direction now.",
                    "label": 0
                },
                {
                    "sent": "This is a supervised dimension reduction method that does linear projections can capture nonlinear structure, but it's doing linear projections and what it's going to do is project down to two dimensions because that's all you really need to be able to capture the variation of the Swiss roll.",
                    "label": 0
                },
                {
                    "sent": "You could also do nonlinear projection which would unroll the Swiss roll in the correct direction and that's this example.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about the.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear dimension reduction or the unrolling.",
                    "label": 0
                },
                {
                    "sent": "Today I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "The linear projection.",
                    "label": 0
                },
                {
                    "sent": "'cause it kind of has well somewhat of a cleaner framework.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is exactly what I said in this talk.",
                    "label": 0
                },
                {
                    "sent": "The projection is just going to be taking X, projecting it onto a subspace.",
                    "label": 0
                },
                {
                    "sent": "Which is D dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this gives rise to this type of semi parametric model.",
                    "label": 0
                },
                {
                    "sent": "And the space B or the span of B is what's called the dimension reduction space or subspace.",
                    "label": 1
                },
                {
                    "sent": "There are various names for it.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's called the Affective dimension reduction subspace.",
                    "label": 0
                },
                {
                    "sent": "If you're Dennis Cook, you're going to call it the Central Dimension reduction subspace.",
                    "label": 0
                },
                {
                    "sent": "These all FS light varying names, but at the end of the day it's a subspace you project onto it.",
                    "label": 0
                },
                {
                    "sent": "It preserves predictive information.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "First part of the talk.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that the marginal distribution row of X is concentrated on a manifold.",
                    "label": 1
                },
                {
                    "sent": "This is the first manifold we have in the talk, and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "The first way I'm going to think about dimension reduction is very geometric and it's going to be driven through the gradient of the regression function.",
                    "label": 1
                },
                {
                    "sent": "Somehow I'm going to try to show you that the gradient of the regression function has information and gives you this space.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is well, the gradient of the regression function.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we will work with a lot.",
                    "label": 0
                },
                {
                    "sent": "Is a multi output object.",
                    "label": 0
                },
                {
                    "sent": "You can think of this as well.",
                    "label": 0
                },
                {
                    "sent": "P dimensional is multi output and I can if I have the gradient estimate I can construct the outer product of it which is all I've done here and this is called the gradient outer product.",
                    "label": 1
                },
                {
                    "sent": "And if you look at each element of this matrix you can think of it as just the inner product between two partial derivatives.",
                    "label": 0
                },
                {
                    "sent": "And in some sense, why is that intuitively doing something it's telling you that this coordinate is changing with?",
                    "label": 0
                },
                {
                    "sent": "Why this other coordinates changing with Y?",
                    "label": 0
                },
                {
                    "sent": "And they're both changing with Y together in the same way, so intuitively, that's what this is doing and.",
                    "label": 0
                },
                {
                    "sent": "The eigen decomposition of this gamma gives me the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My piece, so if I actually had this gamma matrix, this gradient outer product.",
                    "label": 0
                },
                {
                    "sent": "And do a spectral decomposition of it to get these be survives.",
                    "label": 0
                },
                {
                    "sent": "This gives me the space that I should.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Project onto and you can basically see that from some calculus.",
                    "label": 0
                },
                {
                    "sent": "So if I take a vector V OK, that's somewhat perpendicular, let's say perpendicular to BI, right?",
                    "label": 0
                },
                {
                    "sent": "I take the partial derivative with respect to that.",
                    "label": 0
                },
                {
                    "sent": "Well, it's only not going to be 0.",
                    "label": 0
                },
                {
                    "sent": "If it's in the span of the bees, OK, so anything that's not in the span of these bees is going to be orthogonal to it.",
                    "label": 0
                },
                {
                    "sent": "So again, with some calculus you can show.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That this goes through.",
                    "label": 0
                },
                {
                    "sent": "If you want to think about this more statistically.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the linear case here.",
                    "label": 0
                },
                {
                    "sent": "I've just a linear function with noise.",
                    "label": 0
                },
                {
                    "sent": "I can look at three objects.",
                    "label": 0
                },
                {
                    "sent": "This is my classic covariance of X.",
                    "label": 0
                },
                {
                    "sent": "This is just a variance in why, and this is what's called the.",
                    "label": 0
                },
                {
                    "sent": "Covariance of the inverse regression.",
                    "label": 0
                },
                {
                    "sent": "It's the inverse regression 'cause they flipped Y&X OK and, well, we'll call this Omega.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The gradient is basically equal to.",
                    "label": 0
                },
                {
                    "sent": "The variance in Y times.",
                    "label": 0
                },
                {
                    "sent": "Well, the inverse of the variance of X times Omega times the inverse of the variance in X.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, why is this useful?",
                    "label": 0
                },
                {
                    "sent": "Or why might this make sense?",
                    "label": 0
                },
                {
                    "sent": "Well, if I care about those axes that are varying in Y.",
                    "label": 0
                },
                {
                    "sent": "This is capturing that right, but I want to null out or divide out those axes that are just very right, and that's what this does.",
                    "label": 0
                },
                {
                    "sent": "So this, you know, it has some reason.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Intuition, and if F of X is smooth and not necessarily linear, then how do you want to think about?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You just do the same thing you can think of partitioning the functions in little sections.",
                    "label": 0
                },
                {
                    "sent": "Each section is.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you just.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compute all of these.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quantities locally.",
                    "label": 0
                },
                {
                    "sent": "And then you just average up and you get this gradient outer product back.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I I have my basically have a marginal distribution on X, right?",
                    "label": 0
                },
                {
                    "sent": "I'm going to partition that into little blocks within each little block.",
                    "label": 0
                },
                {
                    "sent": "It's linear, right?",
                    "label": 0
                },
                {
                    "sent": "So within each little block I can compute the inverse regression.",
                    "label": 0
                },
                {
                    "sent": "The covariance of the inverse projection.",
                    "label": 0
                },
                {
                    "sent": "I can compute the covariance of X in that block.",
                    "label": 0
                },
                {
                    "sent": "I can compute the variance in Y in that block and me is just a measure of that block, right?",
                    "label": 0
                },
                {
                    "sent": "And then I just average all of 'em over all of these little partitions, OK?",
                    "label": 0
                },
                {
                    "sent": "And this gives me you can show that you're getting this gradient outer product back.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "If you're a geometer at this point, you would be very angry with me and you would say you're a very bad man 'cause you're averaging what a geometer would want to do is paste all of these together.",
                    "label": 0
                },
                {
                    "sent": "And if you pasted them together, you'd be doing nonlinear dimension reduction.",
                    "label": 0
                },
                {
                    "sent": "And one can do that, but.",
                    "label": 0
                },
                {
                    "sent": "There's more subtleties there but but this is just averaging 'em and that's why we get this gradient outer product.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do I estimate the gradient well?",
                    "label": 0
                },
                {
                    "sent": "One way of doing it is by Taylor expanding.",
                    "label": 0
                },
                {
                    "sent": "So if I just Taylor expander on point FJ for any XJ near XII can write why?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approximately that way, and if you think about that.",
                    "label": 0
                },
                {
                    "sent": "If I estimate the gradient with effect, then I can write down this minimization problem.",
                    "label": 0
                },
                {
                    "sent": "Right, this is just least squares and the only thing is I have a waiting here so points that are very power far apart.",
                    "label": 0
                },
                {
                    "sent": "Should be near 0 because Taylor expansions don't make sense of things are very far apart, points near real by or going to be close to one.",
                    "label": 0
                },
                {
                    "sent": "And if you want to do this, it's actually interesting if you want to think about this abbasian problem and you want to start thinking about a likelihood here and you really want to think about WI J, you should go to geostatistics.",
                    "label": 0
                },
                {
                    "sent": "Because those ideas of very grams of Nuggets are really, really inherent.",
                    "label": 0
                },
                {
                    "sent": "If you want to think about this basean, but we're not going to now.",
                    "label": 0
                },
                {
                    "sent": "But so this is just.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Loss.",
                    "label": 0
                },
                {
                    "sent": "And then you regularize this.",
                    "label": 0
                },
                {
                    "sent": "Basically, with the norm RKHS type Norman, this is a regularization problem and this gives me an estimator of a gradient.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I don't know why I'm supposed to go.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "But you're not just another name, but the whole idea.",
                    "label": 0
                },
                {
                    "sent": "You want to find the beavex kind of project down like that night is going well, but you apply this move without knowing that the matrix we're going to have this in this in the.",
                    "label": 0
                },
                {
                    "sent": "Gender regularization with that right, right, right?",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this.",
                    "label": 0
                },
                {
                    "sent": "You ask, well, you have this K and the way I'm actually constructing the K is.",
                    "label": 0
                },
                {
                    "sent": "I'm doing it elementwise over all of those P. So my apriori assumption is that each of these dimensions are independent, which in some sense is idiotic because.",
                    "label": 0
                },
                {
                    "sent": "You know, you know that they're not, and you want to find that right?",
                    "label": 0
                },
                {
                    "sent": "So yes.",
                    "label": 0
                },
                {
                    "sent": "Now, computationally, you'll see that I'm not really doing that in two slides.",
                    "label": 0
                },
                {
                    "sent": "Again, This is why this is very much of a geometric way of thinking about it.",
                    "label": 0
                },
                {
                    "sent": "I'm going to think about and rephrase this whole problem probabilistically, and then it's going to be much more immediate.",
                    "label": 0
                },
                {
                    "sent": "What you were just saying, but yes.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here's my kernel.",
                    "label": 0
                },
                {
                    "sent": "This actually has.",
                    "label": 0
                },
                {
                    "sent": "Basically a representer theorem, and each of these sees instead of being a scalar are going to be dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "Right, so if you think about the parameters of this, there are NP dimensional vectors, so that could be a lot of parameters to estimate.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's actually not how many parameters you have, because if I look at the Taylor expansion.",
                    "label": 0
                },
                {
                    "sent": "Well, this one is good enough, right?",
                    "label": 0
                },
                {
                    "sent": "You see that everything is going to be living in the space band by differences between data points.",
                    "label": 0
                },
                {
                    "sent": "So if I look at any difference between XI and XJ, take those basically, do the SVD on that.",
                    "label": 0
                },
                {
                    "sent": "That's going to be the span of my solution, so it's really much much lower at worst N squared.",
                    "label": 0
                },
                {
                    "sent": "So if any smaller than P, it's a lot smaller.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that exactly addresses what you are talking about because.",
                    "label": 0
                },
                {
                    "sent": "The structure is actually can't be P dimensional, right?",
                    "label": 0
                },
                {
                    "sent": "It can only be at most N square in terms of span of these differences.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. And.",
                    "label": 0
                },
                {
                    "sent": "Again, you can define a gram matrix and I can come up with the gradient outer product.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can actually.",
                    "label": 0
                },
                {
                    "sent": "Prove something about this.",
                    "label": 0
                },
                {
                    "sent": "If you cared too.",
                    "label": 0
                },
                {
                    "sent": "If you believe that the marginal distribution probably should be in a here is concentrated on compact Romanian manifold and there's an isometric embedding.",
                    "label": 0
                },
                {
                    "sent": "Then, if we assume that the.",
                    "label": 0
                },
                {
                    "sent": "The measure on the manifold.",
                    "label": 0
                },
                {
                    "sent": "That density is Holder continuous, and if I look at the measure it's not concentrated on the boundary.",
                    "label": 0
                },
                {
                    "sent": "Right, because otherwise derivatives aren't going to make any sense.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you can prove the following.",
                    "label": 0
                },
                {
                    "sent": "This is a gradient on the manifold.",
                    "label": 0
                },
                {
                    "sent": "This is Adi dimensional object because the manifold is D dimensional.",
                    "label": 0
                },
                {
                    "sent": "This is my estimate.",
                    "label": 0
                },
                {
                    "sent": "It's a P dimensional object.",
                    "label": 0
                },
                {
                    "sent": "Fiestar is a dual of the embedding, so it comes from the ambient space back down to the manifold.",
                    "label": 0
                },
                {
                    "sent": "And then the fee is just the derivative effectively of that.",
                    "label": 0
                },
                {
                    "sent": "So this is taking this P dimensional object back to D. And basically I can show that the L2 norm on the manifold.",
                    "label": 0
                },
                {
                    "sent": "This converges at a rate that's.",
                    "label": 0
                },
                {
                    "sent": "In the dimension of the manifold and not the ambient space.",
                    "label": 0
                },
                {
                    "sent": "OK, so this has been really a very geometric function estimation perspective on this problem of supervised dimension reduction.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to flip gears and I'm going to think about the problem supervised dimension reduction from a probabilistic.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spective oh I forgot I was going to talk about multi task that might be relevant here so I gave it to you for a single task OK. You can do the same thing from multitask.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what you would do is think about.",
                    "label": 0
                },
                {
                    "sent": "Each function F sub TT again indexing task as a baseline.",
                    "label": 0
                },
                {
                    "sent": "And a task specific.",
                    "label": 0
                },
                {
                    "sent": "So again, this type of hierarchical modeling.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So everything like we did before.",
                    "label": 0
                },
                {
                    "sent": "We estimate these gradients.",
                    "label": 0
                },
                {
                    "sent": "Right, just a natural regularization way that you would think about doing.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can get all these T + 1 gradient outer product.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrices and this gives you something about supervised dimension reduction across all the tasks, and this is task specific and I'll show you pictures of this.",
                    "label": 0
                },
                {
                    "sent": "What this does soon, OK?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "I was going to think about things probabilistically, so you know.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everyone's pretty familiar with PCA algorithmically.",
                    "label": 0
                },
                {
                    "sent": "What is PCA?",
                    "label": 0
                },
                {
                    "sent": "You take your data, you construct a covariance matrix, you do an eigen decomposition of it.",
                    "label": 0
                },
                {
                    "sent": "You go on with your life.",
                    "label": 0
                },
                {
                    "sent": "Now if you want to think about PCA.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ballistically how do you do it?",
                    "label": 0
                },
                {
                    "sent": "Well, you do it as a factor model.",
                    "label": 0
                },
                {
                    "sent": "OK, you say that in my data X is coming from a normal.",
                    "label": 0
                },
                {
                    "sent": "It's got mean mu plus a new and some covariance Delta.",
                    "label": 0
                },
                {
                    "sent": "AR my factor loadings noir my factors.",
                    "label": 0
                },
                {
                    "sent": "So really new is something lower dimensional.",
                    "label": 0
                },
                {
                    "sent": "It's just D dimensional object and new are the projections or the factor loadings that take me from this P dimensional space down to this D dimensional space and use just some mean OK?",
                    "label": 0
                },
                {
                    "sent": "So this is all well and good, so this is probabilistic PCA.",
                    "label": 0
                },
                {
                    "sent": "And why did I put this up?",
                    "label": 0
                },
                {
                    "sent": "What happens if I want to do supervised dimension reduction?",
                    "label": 1
                },
                {
                    "sent": "I can work in the same framework.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of again, use a latent variable.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this was proposed by Dennis Cook, and it's called principle fitted components an.",
                    "label": 0
                },
                {
                    "sent": "If I ignored this, this would be like PCA, but now what I'm going to say is.",
                    "label": 0
                },
                {
                    "sent": "I'm going to think about X right sub Y and what do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "Very simple case.",
                    "label": 0
                },
                {
                    "sent": "Let's say Y is zero and one.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to have a different distribution for why being zero different distribution for wide being one LDA is a classic example of this linear discriminant analysis, and that's all I'm doing.",
                    "label": 0
                },
                {
                    "sent": "I'm talking bout X at a particular value Y.",
                    "label": 0
                },
                {
                    "sent": "No, it's the same model as before.",
                    "label": 0
                },
                {
                    "sent": "I have this new.",
                    "label": 0
                },
                {
                    "sent": "The only difference is that those factors right are dependent on why are really based on why, and that's exactly what I wrote down here.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This works fine.",
                    "label": 0
                },
                {
                    "sent": "When things like PCA work, where will something like this break if I have multi modality, right?",
                    "label": 0
                },
                {
                    "sent": "If I have a class with different modes or if there's local structure, so in this whole.",
                    "label": 0
                },
                {
                    "sent": "Supervised dimension reduction framework.",
                    "label": 0
                },
                {
                    "sent": "This class of model comes under the framework of something called sliced inverse regression.",
                    "label": 0
                },
                {
                    "sent": "These are called inverse regression models.",
                    "label": 0
                },
                {
                    "sent": "And formally, these work when you have something called an elliptic condition is met, which basically says X given Y, that density is plus or minus unimodal.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it will break.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in general for manifolds it won't really break.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "The dimension reduction space is just Delta minus one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do you extend this?",
                    "label": 0
                },
                {
                    "sent": "Right to a manifold or to multi clusters so.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you think about geometry, you say, well, manifolds are locally Euclidean.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Statistician.",
                    "label": 0
                },
                {
                    "sent": "You say everything in the world is a mixture model.",
                    "label": 0
                },
                {
                    "sent": "I can take normals and I can mix some like no ones business.",
                    "label": 0
                },
                {
                    "sent": "Right and well, maybe someone's business, but but you know so that you could.",
                    "label": 0
                },
                {
                    "sent": "Just putting these two ideas together so you go back to the.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a model, but you think of it as a mixture model and that's.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did here.",
                    "label": 0
                },
                {
                    "sent": "Now I have a mu YX.",
                    "label": 0
                },
                {
                    "sent": "So different parts of the SpaceX is going to have a different distribution.",
                    "label": 0
                },
                {
                    "sent": "And I place a distribution on new.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We call this basean mixtures of inverses, 'cause it's mixing a bunch of inverses.",
                    "label": 0
                },
                {
                    "sent": "What this new why is depends on whether wise, continuous or it's discrete.",
                    "label": 0
                },
                {
                    "sent": "So if yyou",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Discrete and you can show that given that model.",
                    "label": 0
                },
                {
                    "sent": "The dimension reduction Space is a span of Delta minus one 8.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I can give you a sampling distribution.",
                    "label": 0
                },
                {
                    "sent": "Which I just did.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now again, what the key question is, what is this model on Y or this GY?",
                    "label": 0
                },
                {
                    "sent": "And if your data is discrete, let's say it's categorical, so I've see categories.",
                    "label": 0
                },
                {
                    "sent": "So for Y, being any of the categories, I can basically place down a distribution.",
                    "label": 0
                },
                {
                    "sent": "If I were a reasonable person, I'd say, well look, they're not going to be more than 10,000 classes in the universe, so I'll just write down.",
                    "label": 0
                },
                {
                    "sent": "Additionally, distribution with up to 10,000.",
                    "label": 0
                },
                {
                    "sent": "I'm not a reasonable.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Person.",
                    "label": 0
                },
                {
                    "sent": "So you become a silly nonparametric person and you say I'll put down.",
                    "label": 0
                },
                {
                    "sent": "Additionally process which is doing the same thing but with some fictional Infinity.",
                    "label": 0
                },
                {
                    "sent": "And and that, that works for categorical variables, what do you do for continuous variables?",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about that here, 'cause it's more complicated and maybe not so illuminating, but I will say in words what you do.",
                    "label": 0
                },
                {
                    "sent": "So like I said, a version of this was called sliced inverse regression, and what they did there was you look at your values Y and you been them, and for each bin you compete the mean of the X is in that bin.",
                    "label": 0
                },
                {
                    "sent": "Take that outer product in average.",
                    "label": 0
                },
                {
                    "sent": "So there it was.",
                    "label": 0
                },
                {
                    "sent": "Really important to slice wise, no.",
                    "label": 0
                },
                {
                    "sent": "Here what you do is you basically can put down some type of dependent Dursley process.",
                    "label": 0
                },
                {
                    "sent": "Just saying this distribution on why is going to be very similar for wise that are really close, meaning strongly dependent.",
                    "label": 0
                },
                {
                    "sent": "Why's that are very far apart.",
                    "label": 0
                },
                {
                    "sent": "It will be quite independent.",
                    "label": 0
                },
                {
                    "sent": "So basically you can put together this type of display dependently disk.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bution and sample from that.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So here's my likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to do.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Post your inference basically.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using MCM",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "She.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It turns out that in this case.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so these are my samples.",
                    "label": 0
                },
                {
                    "sent": "I've amuen a Delta minus one in a new.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for almost all of these.",
                    "label": 0
                },
                {
                    "sent": "Parameters everything except for knew.",
                    "label": 0
                },
                {
                    "sent": "I can write this as a Gibbs sampler because it has really nice conditioning properties.",
                    "label": 0
                },
                {
                    "sent": "New does not.",
                    "label": 0
                },
                {
                    "sent": "You have to run new as a metropolis Hastings, that's the one.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Horrible, that's not so nice.",
                    "label": 0
                },
                {
                    "sent": "And so this is drawn from a normal.",
                    "label": 0
                },
                {
                    "sent": "This is an inverse Wishart, again a normal.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "New is a bit more work.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Now we come to the second manifold in this talk.",
                    "label": 0
                },
                {
                    "sent": "So I have a bunch of draws of the deltas and these days and from that.",
                    "label": 0
                },
                {
                    "sent": "I have draws of these subspaces OK, so each MCMC is really giving me a draw of my subspace that I believe that I'm projecting onto.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "I think one of the things that's very interesting is the way manifold learning has caught on in machine learning has really been a focus on the manifold in the data space.",
                    "label": 0
                },
                {
                    "sent": "Classically, in statistics, manifold thoughts really about manifolds in the parameter space and properties of that and so on and so forth and classically.",
                    "label": 0
                },
                {
                    "sent": "If you want to think about Cramer Rao bounds and theories like that, those are really about geometric properties on the parameter space, so again this is coming back to the parameter space.",
                    "label": 0
                },
                {
                    "sent": "I'm estimating subspaces, and if I talk about model identify ability.",
                    "label": 0
                },
                {
                    "sent": "Each of the columns.",
                    "label": 0
                },
                {
                    "sent": "Are individually not identifiable, but that subspace is identifiable in this framework and this factor modeling framework so.",
                    "label": 0
                },
                {
                    "sent": "I have a bunch of points.",
                    "label": 0
                },
                {
                    "sent": "That are subspaces, so these are points on what's called the Grassmann manifold, and this manifold has natural structure.",
                    "label": 0
                },
                {
                    "sent": "It has a natural geodesic and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So one question.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this has some implications, so one would be how do I define a summary?",
                    "label": 0
                },
                {
                    "sent": "How do I define the posterior mean right?",
                    "label": 0
                },
                {
                    "sent": "Do I just average all of 'em?",
                    "label": 0
                },
                {
                    "sent": "Or what do I do?",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the point is, you really should, so there's something called the Karcher mean.",
                    "label": 0
                },
                {
                    "sent": "So if I have two subspaces, WMU.",
                    "label": 0
                },
                {
                    "sent": "I can write them down as orthonormal basis W&V, and I can compute the distance between these two.",
                    "label": 0
                },
                {
                    "sent": "So this gives me a natural Romanian metric between two subspaces, and given this.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patrick I can define the mean.",
                    "label": 0
                },
                {
                    "sent": "Which is just going to be given all of my MCMC draws.",
                    "label": 0
                },
                {
                    "sent": "It's going to be the point that is at the geodesics center.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly, I can define.",
                    "label": 0
                },
                {
                    "sent": "A variance now.",
                    "label": 0
                },
                {
                    "sent": "One thing that's really interesting.",
                    "label": 0
                },
                {
                    "sent": "And I didn't talk about it.",
                    "label": 0
                },
                {
                    "sent": "Is what happens if I was also trying to estimate D, which is a dimension?",
                    "label": 0
                },
                {
                    "sent": "We have ways of kind of doing that.",
                    "label": 0
                },
                {
                    "sent": "And what happens if I get different draws that have different dimensions?",
                    "label": 0
                },
                {
                    "sent": "Right mathematically dot distance is Infinity.",
                    "label": 0
                },
                {
                    "sent": "But no, that's not a meaningful thing for me to say.",
                    "label": 0
                },
                {
                    "sent": "I can't say that these things are infinitely far apart.",
                    "label": 0
                },
                {
                    "sent": "So how do I think about that?",
                    "label": 0
                },
                {
                    "sent": "How do I formalize that?",
                    "label": 0
                },
                {
                    "sent": "How do I characterize that I think is a really, really interesting problem?",
                    "label": 0
                },
                {
                    "sent": "And this is another point that I just want to make.",
                    "label": 0
                },
                {
                    "sent": "Is that many of us.",
                    "label": 0
                },
                {
                    "sent": "Do factor modeling.",
                    "label": 0
                },
                {
                    "sent": "Many of us do probabilistic versions of factor modeling.",
                    "label": 0
                },
                {
                    "sent": "Almost none of us.",
                    "label": 0
                },
                {
                    "sent": "Think about the geometry of those factors.",
                    "label": 0
                },
                {
                    "sent": "So if you were doing factor models, you're looking at covariance matrices plus or minus.",
                    "label": 0
                },
                {
                    "sent": "OK. Now if I'm looking at covariance matrices, I'm looking at summaries of them.",
                    "label": 0
                },
                {
                    "sent": "Again, covariance matrices have a natural geodesic structure which we never worry about.",
                    "label": 0
                },
                {
                    "sent": "Maybe we don't need to, I don't know, but again, we almost never worry about that when we think about summaries of them.",
                    "label": 0
                },
                {
                    "sent": "And it's kind of interesting that we don't, but anyways, so I can also give you about notion of uncertainty of these.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Subspaces?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is just one example, but I think a very interesting problem is that I wanted to estimate subspaces and I did it by using a factor model at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "Question is, can I be much more direct?",
                    "label": 0
                },
                {
                    "sent": "Can I actually put distributions on subspace?",
                    "label": 0
                },
                {
                    "sent": "Is going to put distributions on the grassman manifold?",
                    "label": 0
                },
                {
                    "sent": "Can I put interesting distributions on that?",
                    "label": 0
                },
                {
                    "sent": "Can I put it to entering distributions on mixtures of grasslands and directly estimates subspaces or mixtures of subspaces and then plug that immediately into my likelihood?",
                    "label": 0
                },
                {
                    "sent": "If I had nice ways of doing that, I could, and some people are thinking about this.",
                    "label": 0
                },
                {
                    "sent": "So we come back to this with.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pro 'cause that's the only data set in the world we care about.",
                    "label": 0
                },
                {
                    "sent": "And well, here's my.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Swiss roll and.",
                    "label": 0
                },
                {
                    "sent": "I know the real dimensions that are relevant.",
                    "label": 0
                },
                {
                    "sent": "There are three of them and I have my estimates beta hat.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so I can ask.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different algorithms are going to do.",
                    "label": 0
                },
                {
                    "sent": "So all of these algorithms don't do well.",
                    "label": 0
                },
                {
                    "sent": "These are all algorithms that are kind of global.",
                    "label": 0
                },
                {
                    "sent": "They don't think about things locally.",
                    "label": 0
                },
                {
                    "sent": "The one up top in red is our Bayesian mixture of inverses.",
                    "label": 0
                },
                {
                    "sent": "It does very well.",
                    "label": 0
                },
                {
                    "sent": "This one right here is a gradient learning method I told you about earlier and this is what's called local sliced inverse regression which is.",
                    "label": 0
                },
                {
                    "sent": "Again, doing this localization but using.",
                    "label": 0
                },
                {
                    "sent": "Really nearest neighbors type ideas to do it.",
                    "label": 0
                },
                {
                    "sent": "No problem.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK there.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is my posterior variance?",
                    "label": 0
                },
                {
                    "sent": "You can try to estimate D by various ways and we're just saying that we don't do a horrible job of it.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is a case with digits.",
                    "label": 0
                },
                {
                    "sent": "We have 3, five and eight.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to look at two classification problems, 3 versus 8 and.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Verses 8.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Onion training samples.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the subspace that separates 3 from 8.",
                    "label": 0
                },
                {
                    "sent": "It looks like this and it's kind of cool.",
                    "label": 0
                },
                {
                    "sent": "'cause what's the difference which is at three?",
                    "label": 0
                },
                {
                    "sent": "This is an 8.",
                    "label": 0
                },
                {
                    "sent": "What's the difference?",
                    "label": 0
                },
                {
                    "sent": "These two points?",
                    "label": 0
                },
                {
                    "sent": "These two little regions, similarly for five versus 8.",
                    "label": 0
                },
                {
                    "sent": "You see this region as the difference in the subspace at differenti.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get some.",
                    "label": 0
                },
                {
                    "sent": "If I wanted to do a multi task problem I can look at three versus 85 versus 8 and three and five versus 8.",
                    "label": 0
                },
                {
                    "sent": "Let me tell you OK and you apply.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the top features or the top projection for three and five versus 8.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a top for three verses.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and this is the top.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For five versus 8.",
                    "label": 0
                },
                {
                    "sent": "So this is what happens if you apply this to all 10 digits you.",
                    "label": 0
                },
                {
                    "sent": "If you use this nonlinear ideas versus a linear idea, you do better.",
                    "label": 0
                },
                {
                    "sent": "It makes a big difference for 8:00 and 5:00.",
                    "label": 0
                },
                {
                    "sent": "I did not showing this, but the reason actually why I got into this multi tasking these constructions for these problems, so I do a lot of work on cancer and computational biology.",
                    "label": 0
                },
                {
                    "sent": "So we actually applied similar model to modeling tumor progression through stages, so these are my different tasks going from less serious, more serious.",
                    "label": 0
                },
                {
                    "sent": "Less here it's more serious and again you can apply these.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, there this is just one example of cancer classification.",
                    "label": 0
                },
                {
                    "sent": "This is a data set that's been beat to hell.",
                    "label": 0
                },
                {
                    "sent": "There are 19 samples of this type of leukemia.",
                    "label": 0
                },
                {
                    "sent": "19 samples of a different type of leukemia.",
                    "label": 0
                },
                {
                    "sent": "It just so happens there are two sub categories here.",
                    "label": 0
                },
                {
                    "sent": "OK, I tell the model that these are two classes, but I don't tell it that there are subclasses here you do this.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Projection onto just two dimensions.",
                    "label": 0
                },
                {
                    "sent": "One class completely falls to one point, the other two are somewhat separated, so it is capped.",
                    "label": 0
                },
                {
                    "sent": "Picturing these local.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Information.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pizza looked at this kind of linear dimensionality reduction, but there we were just doing kind of dumb gradient descent under projection.",
                    "label": 0
                },
                {
                    "sent": "So I just I don't understand what you're saying about the grading at the problem matrix.",
                    "label": 0
                },
                {
                    "sent": "So basically if I can estimate that.",
                    "label": 0
                },
                {
                    "sent": "This kind of construction, and basically I don't need information that should give me those extra exactly and intuitively you would think it would because you know what is that doing is just telling you the directions right that the function is moving at the most, and then if I just average over those right just paste average all these tangent planes.",
                    "label": 0
                },
                {
                    "sent": "I'm good to go in terms of the relevant directions right now if you want to be smarter and paste them, you could paste all of the tangent planes, but that's a harder problem 'cause you know you lose points and blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "Yeah exactly.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "For your protection.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So you don't.",
                    "label": 0
                },
                {
                    "sent": "Is a short answer.",
                    "label": 0
                },
                {
                    "sent": "The more honest answer is that.",
                    "label": 0
                },
                {
                    "sent": "There are certain parameters that you have to worry about D, especially right.",
                    "label": 0
                },
                {
                    "sent": "What is the dimensionality of this thing right and?",
                    "label": 0
                },
                {
                    "sent": "What we find.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "We tend to overestimate it.",
                    "label": 0
                },
                {
                    "sent": "I think that's because we had some smoothing and we have these priors that are shrinking, right?",
                    "label": 0
                },
                {
                    "sent": "I think if from people who've done this from a straight likelihood perspective, they can sure show asymptotically that they get the right D, but we do overestimated.",
                    "label": 0
                },
                {
                    "sent": "But I think it's OK because it lets us make inferences for smaller sample sizes, right?",
                    "label": 0
                },
                {
                    "sent": "But so that's one of them.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In terms of other initializations, not, not so much now.",
                    "label": 0
                },
                {
                    "sent": "I mean, in the regularization version you got to pick Lambda, but.",
                    "label": 0
                },
                {
                    "sent": "And you know who really knows how to do that, but that's the difference.",
                    "label": 0
                },
                {
                    "sent": "Chris is thinking.",
                    "label": 0
                },
                {
                    "sent": "So one thing I think there's also relations with some of the multi task kernel learnings.",
                    "label": 0
                },
                {
                    "sent": "But I couldn't quite workout when you mentioned the multi task surface the way you're suggesting.",
                    "label": 0
                },
                {
                    "sent": "Multi task in the linear case is how you combining across the tasks are using the same shared.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So so if.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I looked at it.",
                    "label": 0
                },
                {
                    "sent": "Let's just look at this case OK?",
                    "label": 0
                },
                {
                    "sent": "So this is what's common, so this shared across the three on the five, right?",
                    "label": 0
                },
                {
                    "sent": "So I could basically, if I wanted to look at the projections that are common across all task, this is a subspace I project onto OK.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is looking at just three versus 8.",
                    "label": 0
                },
                {
                    "sent": "So if I were only caring about 3 versus 8, I'd add the previous one with this one and I project onto that.",
                    "label": 0
                },
                {
                    "sent": "Right and then similarly.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We only cared about the five versus 8.",
                    "label": 0
                },
                {
                    "sent": "I take the previous one and add this and so on so forth.",
                    "label": 0
                },
                {
                    "sent": "Now, if you're using this to try to interpret certain things, then you could just look at it independently.",
                    "label": 0
                },
                {
                    "sent": "It wouldn't be the most predictive because it's only looking at that part, which is, you know, not of the other one.",
                    "label": 0
                },
                {
                    "sent": "But you can ask that question what is the subspace that is really.",
                    "label": 0
                },
                {
                    "sent": "You know specific for five versus 8.",
                    "label": 0
                },
                {
                    "sent": "I think you mentioned she said that in sadistic so people are looking at manifolds over parameters rather than well.",
                    "label": 0
                },
                {
                    "sent": "I mean classically classic Lee.",
                    "label": 0
                },
                {
                    "sent": "Duality between those things.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "I would say that so this is, in my view, this is Bayesian statistics.",
                    "label": 0
                },
                {
                    "sent": "Right you have or statistics.",
                    "label": 0
                },
                {
                    "sent": "In general you have a distribution in your model space.",
                    "label": 0
                },
                {
                    "sent": "I mean in the data space you have a distribution on your parameter space and inference is nothing but getting from one to the other.",
                    "label": 0
                },
                {
                    "sent": "I mean there should be a duality of their structure, but you know formalizing what that is.",
                    "label": 0
                },
                {
                    "sent": "I think yeah, I don't know.",
                    "label": 0
                },
                {
                    "sent": "I think it's very interesting, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Oh, I would like to thank the organizers.",
                    "label": 0
                }
            ]
        }
    }
}