{
    "id": "5dl5hoduqxbeoh4aciqdinwvmmacdj6a",
    "title": "Learning with Square Loss: Localization through Offset Rademacher Complexity",
    "info": {
        "author": [
            "Tengyuan Liang, Wharton School, University of Pennsylvania"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_liang_rademacher_complexity/",
    "segmentation": [
        [
            "Hello everyone, today I will talk about a about regression and their square loss and in the past 15 years this problem is usually analyzed through ERM by a beautiful paper by Shahar Mendelson and Peter Bradley.",
            "And today we try to use another approach and during the talk I will show several advantage of this new approach is called offset rather complexity and this model is in Addison setting."
        ],
        [
            "So the class of function is F is defined are measurable space.",
            "And you have X or Y random variable jointly distributed.",
            "You observe data, IID copies, and the performance of your estimator is F is measured under the square loss and you denote the sample version of empirical version by this notation.",
            "And for model class F, the performance of the estimator is is measured through this access loss and the optimum inside the class of function F is denoted by this App Store."
        ],
        [
            "So the Yugo is first construct an estimator F hat such that it is simple and also with small access laws.",
            "And there are usually many.",
            "So in the previous analysis Europe, this is usually done using the empirical risk minimization and local Rademacher averages.",
            "So usually you assume the class of function to be convex or even bounded.",
            "So we want to avoid this assumption.",
            "So we want to propose an estimator that is that works under the convex case and also under the non convex case.",
            "And also we want to avoid.",
            "So boundless assumption and include heavy tailed distribution into our framework.",
            "So these are the two things that we want to achieve."
        ],
        [
            "So let me quickly remind you all for this local Rademacher averages for analyzing the empirical risk minimization.",
            "Basically this thing is defined as a critical radius, data dependent quantity.",
            "And it's analyzed using two tools.",
            "First is telegrams concentration inequality for superior and also for contraction.",
            "And both of these two technique requires the boundless assumption on the.",
            "On the class of abundance assumption on the class."
        ],
        [
            "So before we move to non convex class, let's take a look at a simple example, do some basic calculations showing the ideal for convex class.",
            "OK for convex class.",
            "Then the empirical risk minimizer satisfied this Pythagoras theorem condition on the data.",
            "So this is a positive quantity.",
            "You can add it to the excess loss.",
            "Then you enlarge it.",
            "And then you open the scores.",
            "And then you take the superior of the function.",
            "You can see that this access loss is controlled by the superior of a complicated term, but this isn't zero, mean stochastic process, and this mismatch coefficient here gives you an active mean and will show that this negative mean really helps you to get faster rate.",
            "So the idea is to upper bound access loss by a negative mean stochastic process."
        ],
        [
            "So this negative mean stochastic process also appear in the online world, so so this offset driver marker complexity was first proposed by car sick and Sasha in the online world, and we want to run one goal of this project to see whether this offset is the correct thing to look at in ID setting, so the offset track marker averages is defined for class of function G and some constant C bigger than zero is defined as the.",
            "Expected over epsilons so period of the class of functions.",
            "This is the traditional radmacher complexity.",
            "You offset it by a squared norm.",
            "A score of the function and the empirical Rademacher averages corresponding to when this constant equals to zero and will show later that this constant this negative mean part really helps you."
        ],
        [
            "See a toy example.",
            "Just as an illustration of the calculation.",
            "Consider linear regression in the finite dimension case.",
            "For example, dimension D and sequently.",
            "Note the gram matrix.",
            "Then if you calculate this offset from our complexity exactly, you can get this term.",
            "And if you evaluate, it scales with the rate D over end, which is the first rate wonder end.",
            "In contrast, the usual non offset complexity only gives you the slow rate into the power negative 1/2, so this is a toy example showing that offset provide you some good calculation."
        ],
        [
            "So another intuition why this offset is good is through this crude picture.",
            "It has some property called a local automatic localization property which I will show in this picture.",
            "So if you look at the fluctuation of this rather marker averages as the function goes to Infinity or the auto norm of the function gets large, the fluctuation get large.",
            "So the superior of this stochastic process for the unbounded class may be achieved at Infinity.",
            "So you really need bounded bounded assumption to get valid results are optimal rates, but however for offset because your offsetted by the score score of this function.",
            "Even though the fluctuation gets larger, it get killed by the negative mean part.",
            "So you can see this appearance automatically localized within a certain critical radius and we will give a mathematical description of this critical radius later in this talk."
        ],
        [
            "So we've already seen the calculation.",
            "We already see the calculation for the convex class.",
            "However, for nonconvex class we try to use the same idea, right?",
            "We want to provide a General Pythagoras theorem for non convex class.",
            "We cannot hope for the ERM, empirical risk minimization will work in this setting, because, ERM, is shown to be non optimal.",
            "And which so basically you want to construct a new estimator which is not erm.",
            "Such that for for some constant C bigger than zero between zero and one, you can prove the conditional data.",
            "This general principle theorem hold, and if this holds, you can design a new algorithm, or you can design.",
            "Your algorithm will make sure that this holds."
        ],
        [
            "Turns out that you only need to do a little bit more than empirical risk minimization.",
            "The algorithm is called star algorithm.",
            "Before I introduce the algorithm, let me introduce the Star Hall.",
            "So a lot of you guys already know the star whole centered at the function G. For class of function, capital F is constructed as you just connect G with all the other functions inside this F and you include this line arrays inside new function is a slightly larger class than the original function class F, but complexity are approximately the same and it's much more simpler than the convex Hull.",
            "So the estimator is 2 step estimator.",
            "First step you do a ERM and then you construct the star hold based centered at the first step estimator and then you run run another round of ERM on the Star Hall.",
            "Let me use this picture to illustrate.",
            "So this is the class of function F first."
        ],
        [
            "Erm, denoted by G hat here.",
            "You can stroke the star hole.",
            "Get."
        ],
        [
            "New class you project."
        ],
        [
            "Then so we will show that this F hat satisfy the generalized Pythagoras theorem in the later slides.",
            "So if F is convex, then the star algorithm coincide with the RM.",
            "Because the convex Hull of the star hole is the same as the original class."
        ],
        [
            "Let me put the results into the history line, so this algorithm was first introduced by all the bear in 2007 for finite class function F. And he showed that it is not only in expectation optimal, but also deviation optimal for finite aggregation problem.",
            "And for convex an unbounded class are secure.",
            "Medicine showed the ERM is optimal.",
            "And for non convex class but bounded class Sasha Karthik, an typical approach.",
            "In 2014 proposal three step estimator.",
            "But it requires the bonus assumption and I was made aware of by Peter that he has an interesting paper coming out also on this topic.",
            "So let's prove this key job, geometric inequality or general Pythagoras theorem."
        ],
        [
            "So it turns out that this star estimator F hat satisfies for any H inside this class function F. We see constant equals one over 18th.",
            "You can prove this theorem.",
            "So if F is convex also this host condition on the data deterministically.",
            "So if F is convex, then this piece offers similar hold with constant equal 1.",
            "If F is linear subspace then this holds with equality.",
            "So let's give a illustration of intuitive idea of how this is proved.",
            "So remember, everything is conditioned on the data, so this is reduced to a leading is embedded in Euclidean space of dimension N."
        ],
        [
            "And the data Y is off in dimension you can denote it by a point inside this Euclidean space and the first step estimator the ERM.",
            "You know that is it is within a certain radius compared to the Y.",
            "So let's denote it by this be one ball.",
            "And due to the optimality of the second step estimator, which is the star estimator, you know it lies somewhere inside this B1.",
            "Let's denote it by B2.",
            "And due to the optimality of this store construction, you know that actually you can say a little bit more than more on where this effect lies in.",
            "It.",
            "Turns out it lies in the Carnegie Hall starting from the G hat intersect with the inside Bobby 2.",
            "Because if there is another, if there is another and then for any age inside this F. It first of all, it only lies outside the conic conical, because if it's inside, it violates the optimality of the star estimator.",
            "In the second step.",
            "And also you know that trivially it is outside be one ball, so you put a restriction on the space where the class of function H can lie in and then through some reduction you can reduce the tool 2 dimensional problem and then through triangle inequality.",
            "And simple algebra, simple algebra you can show that actually this."
        ],
        [
            "Estimator satisfy the geometric inequality with constant equals 1 / 18.",
            "And using that you can use the same trick.",
            "You can add the positive quality to the access loss.",
            "And then you can move from the access loss.",
            "Tour superior of an active mean stochastic process with a mismatch concept here.",
            "So this gives you the negative mean part and it gives you the faster rate.",
            "So let me summarize up to now we upper bound the excess loss by a stochastic next mean stochastic process.",
            "And now we want to relate the negative mean stochastic process to another negative mean stochastic process, which is offset rather complexity.",
            "So we need to do some symmetrization trick and so."
        ],
        [
            "I'm up calculation if the class of function is bounded and if the noise is bounded.",
            "Just a very simple calculation to illustrate the idea, you can use the standard contraction principle and symmetrization technique to show that actually the access loss is upper bounded by the negative mean stochastic process and then it's further upper bounded by this offset.",
            "Random complexity for the class function H which is a little bit more complicated and F but complexity doesn't change the start of F."
        ],
        [
            "So we we are not only satisfied with in expectation results, we want to push, want to compare the tale of.",
            "The excess loss and the red marker process.",
            "So we want to show that.",
            "On not only in terms of expectation, but also in terms of deviation, this is the right thing to look at.",
            "It turns out we need to require a slight mode, alittle only a little bit, slight, more condition.",
            "It is called the lower isometry condition is used a lot in the inverse problem in compressive sensing and all this thing.",
            "So this assumption is.",
            "For class of function H, if it satisfies the lower isometric bound, for some with high probability with some constant see this.",
            "See can be does not need to be arbitrary close to zero.",
            "It can be some constant, for example 1 / 72 is enough for our analysis.",
            "The empirical L2 norm is lower bounded by 1 -- 3 * A sample of the population.",
            "L2 norm with high probability as long as this N is larger than a certain complexity.",
            "This whole it turns out that this this assumption is rather mild by a beautiful paper proved by Shahar Mendelson last year and this year.",
            "This slower isometry holds under even heavy tail condition.",
            "It's only 1 sided.",
            "This only hold this holds only you only need to require the small ball assumption.",
            "And a little bit non comparison condition.",
            "Basically you need the Alcuna miss bounded by auto norm for Q between 2:00 and 4:00 and this of course holds for sub Gaussian classes and host for general heavy tail.",
            "So we only need to require this and we want to get a stochastic control on the access loss."
        ],
        [
            "So here is the main theorem.",
            "So the class is the star Hall.",
            "And let's first assume some weak moment condition so.",
            "Assume this star star condition holds this.",
            "Then you can show that the tail of this excess loss is controlled by the tail stochastically controlled by the tail of the offset Rd marker process.",
            "For any you are larger than 1 / N rate.",
            "And as long as is any larger than this complexity, and this is where the local isometry constant requirement kicks in here.",
            "Actually, we can remove the moment Condition star are using a probabilistic symmetrization trick by potranco in 2003.",
            "So essentially only this lower isometry condition is what we needed, so these takers of heavy tailed distributions."
        ],
        [
            "So.",
            "Up to now, we already stochastically controlled access loss by radmacher averages.",
            "So can we actually, then, the next step is upper bounded, the upper bound offset around market process.",
            "So if you remember in the picture I illustrate this local local automatic localization property of this offsets rather marker averages.",
            "So here is the mathematical description defined.",
            "These are star to be this.",
            "Then you know the superior of unbounded class is exactly the same as the superior of the restricted class, restricted to the critical radius.",
            "So you move from abounded to unbounded, to abounded class, and then is further bounded by this R score.",
            "Anne."
        ],
        [
            "This just to see some example using this offset machinery for finite dimensional regression.",
            "You guys get the correct rate for misspecified and well specified simultaneously.",
            "Anne."
        ],
        [
            "For finite aggregation, you get the log N / N deviation, which is the correct one.",
            "And you cannot use."
        ],
        [
            "Training technique to provide an explicit bound on this critical radius.",
            "R star.",
            "And you can."
        ],
        [
            "Also showed this works for for the nonparametric function class.",
            "And we also have a."
        ],
        [
            "Lower bound on the minimax regret in terms of offset random marker complexity."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everyone, today I will talk about a about regression and their square loss and in the past 15 years this problem is usually analyzed through ERM by a beautiful paper by Shahar Mendelson and Peter Bradley.",
                    "label": 0
                },
                {
                    "sent": "And today we try to use another approach and during the talk I will show several advantage of this new approach is called offset rather complexity and this model is in Addison setting.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the class of function is F is defined are measurable space.",
                    "label": 0
                },
                {
                    "sent": "And you have X or Y random variable jointly distributed.",
                    "label": 0
                },
                {
                    "sent": "You observe data, IID copies, and the performance of your estimator is F is measured under the square loss and you denote the sample version of empirical version by this notation.",
                    "label": 0
                },
                {
                    "sent": "And for model class F, the performance of the estimator is is measured through this access loss and the optimum inside the class of function F is denoted by this App Store.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the Yugo is first construct an estimator F hat such that it is simple and also with small access laws.",
                    "label": 0
                },
                {
                    "sent": "And there are usually many.",
                    "label": 0
                },
                {
                    "sent": "So in the previous analysis Europe, this is usually done using the empirical risk minimization and local Rademacher averages.",
                    "label": 0
                },
                {
                    "sent": "So usually you assume the class of function to be convex or even bounded.",
                    "label": 0
                },
                {
                    "sent": "So we want to avoid this assumption.",
                    "label": 0
                },
                {
                    "sent": "So we want to propose an estimator that is that works under the convex case and also under the non convex case.",
                    "label": 0
                },
                {
                    "sent": "And also we want to avoid.",
                    "label": 0
                },
                {
                    "sent": "So boundless assumption and include heavy tailed distribution into our framework.",
                    "label": 0
                },
                {
                    "sent": "So these are the two things that we want to achieve.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me quickly remind you all for this local Rademacher averages for analyzing the empirical risk minimization.",
                    "label": 1
                },
                {
                    "sent": "Basically this thing is defined as a critical radius, data dependent quantity.",
                    "label": 0
                },
                {
                    "sent": "And it's analyzed using two tools.",
                    "label": 0
                },
                {
                    "sent": "First is telegrams concentration inequality for superior and also for contraction.",
                    "label": 0
                },
                {
                    "sent": "And both of these two technique requires the boundless assumption on the.",
                    "label": 0
                },
                {
                    "sent": "On the class of abundance assumption on the class.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before we move to non convex class, let's take a look at a simple example, do some basic calculations showing the ideal for convex class.",
                    "label": 0
                },
                {
                    "sent": "OK for convex class.",
                    "label": 0
                },
                {
                    "sent": "Then the empirical risk minimizer satisfied this Pythagoras theorem condition on the data.",
                    "label": 0
                },
                {
                    "sent": "So this is a positive quantity.",
                    "label": 0
                },
                {
                    "sent": "You can add it to the excess loss.",
                    "label": 0
                },
                {
                    "sent": "Then you enlarge it.",
                    "label": 0
                },
                {
                    "sent": "And then you open the scores.",
                    "label": 0
                },
                {
                    "sent": "And then you take the superior of the function.",
                    "label": 0
                },
                {
                    "sent": "You can see that this access loss is controlled by the superior of a complicated term, but this isn't zero, mean stochastic process, and this mismatch coefficient here gives you an active mean and will show that this negative mean really helps you to get faster rate.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to upper bound access loss by a negative mean stochastic process.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this negative mean stochastic process also appear in the online world, so so this offset driver marker complexity was first proposed by car sick and Sasha in the online world, and we want to run one goal of this project to see whether this offset is the correct thing to look at in ID setting, so the offset track marker averages is defined for class of function G and some constant C bigger than zero is defined as the.",
                    "label": 0
                },
                {
                    "sent": "Expected over epsilons so period of the class of functions.",
                    "label": 0
                },
                {
                    "sent": "This is the traditional radmacher complexity.",
                    "label": 0
                },
                {
                    "sent": "You offset it by a squared norm.",
                    "label": 0
                },
                {
                    "sent": "A score of the function and the empirical Rademacher averages corresponding to when this constant equals to zero and will show later that this constant this negative mean part really helps you.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See a toy example.",
                    "label": 0
                },
                {
                    "sent": "Just as an illustration of the calculation.",
                    "label": 0
                },
                {
                    "sent": "Consider linear regression in the finite dimension case.",
                    "label": 0
                },
                {
                    "sent": "For example, dimension D and sequently.",
                    "label": 0
                },
                {
                    "sent": "Note the gram matrix.",
                    "label": 0
                },
                {
                    "sent": "Then if you calculate this offset from our complexity exactly, you can get this term.",
                    "label": 0
                },
                {
                    "sent": "And if you evaluate, it scales with the rate D over end, which is the first rate wonder end.",
                    "label": 0
                },
                {
                    "sent": "In contrast, the usual non offset complexity only gives you the slow rate into the power negative 1/2, so this is a toy example showing that offset provide you some good calculation.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another intuition why this offset is good is through this crude picture.",
                    "label": 0
                },
                {
                    "sent": "It has some property called a local automatic localization property which I will show in this picture.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the fluctuation of this rather marker averages as the function goes to Infinity or the auto norm of the function gets large, the fluctuation get large.",
                    "label": 0
                },
                {
                    "sent": "So the superior of this stochastic process for the unbounded class may be achieved at Infinity.",
                    "label": 0
                },
                {
                    "sent": "So you really need bounded bounded assumption to get valid results are optimal rates, but however for offset because your offsetted by the score score of this function.",
                    "label": 0
                },
                {
                    "sent": "Even though the fluctuation gets larger, it get killed by the negative mean part.",
                    "label": 0
                },
                {
                    "sent": "So you can see this appearance automatically localized within a certain critical radius and we will give a mathematical description of this critical radius later in this talk.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've already seen the calculation.",
                    "label": 0
                },
                {
                    "sent": "We already see the calculation for the convex class.",
                    "label": 0
                },
                {
                    "sent": "However, for nonconvex class we try to use the same idea, right?",
                    "label": 1
                },
                {
                    "sent": "We want to provide a General Pythagoras theorem for non convex class.",
                    "label": 0
                },
                {
                    "sent": "We cannot hope for the ERM, empirical risk minimization will work in this setting, because, ERM, is shown to be non optimal.",
                    "label": 1
                },
                {
                    "sent": "And which so basically you want to construct a new estimator which is not erm.",
                    "label": 0
                },
                {
                    "sent": "Such that for for some constant C bigger than zero between zero and one, you can prove the conditional data.",
                    "label": 1
                },
                {
                    "sent": "This general principle theorem hold, and if this holds, you can design a new algorithm, or you can design.",
                    "label": 0
                },
                {
                    "sent": "Your algorithm will make sure that this holds.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Turns out that you only need to do a little bit more than empirical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is called star algorithm.",
                    "label": 0
                },
                {
                    "sent": "Before I introduce the algorithm, let me introduce the Star Hall.",
                    "label": 0
                },
                {
                    "sent": "So a lot of you guys already know the star whole centered at the function G. For class of function, capital F is constructed as you just connect G with all the other functions inside this F and you include this line arrays inside new function is a slightly larger class than the original function class F, but complexity are approximately the same and it's much more simpler than the convex Hull.",
                    "label": 0
                },
                {
                    "sent": "So the estimator is 2 step estimator.",
                    "label": 0
                },
                {
                    "sent": "First step you do a ERM and then you construct the star hold based centered at the first step estimator and then you run run another round of ERM on the Star Hall.",
                    "label": 1
                },
                {
                    "sent": "Let me use this picture to illustrate.",
                    "label": 0
                },
                {
                    "sent": "So this is the class of function F first.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Erm, denoted by G hat here.",
                    "label": 0
                },
                {
                    "sent": "You can stroke the star hole.",
                    "label": 0
                },
                {
                    "sent": "Get.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "New class you project.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then so we will show that this F hat satisfy the generalized Pythagoras theorem in the later slides.",
                    "label": 0
                },
                {
                    "sent": "So if F is convex, then the star algorithm coincide with the RM.",
                    "label": 1
                },
                {
                    "sent": "Because the convex Hull of the star hole is the same as the original class.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me put the results into the history line, so this algorithm was first introduced by all the bear in 2007 for finite class function F. And he showed that it is not only in expectation optimal, but also deviation optimal for finite aggregation problem.",
                    "label": 1
                },
                {
                    "sent": "And for convex an unbounded class are secure.",
                    "label": 0
                },
                {
                    "sent": "Medicine showed the ERM is optimal.",
                    "label": 0
                },
                {
                    "sent": "And for non convex class but bounded class Sasha Karthik, an typical approach.",
                    "label": 0
                },
                {
                    "sent": "In 2014 proposal three step estimator.",
                    "label": 0
                },
                {
                    "sent": "But it requires the bonus assumption and I was made aware of by Peter that he has an interesting paper coming out also on this topic.",
                    "label": 0
                },
                {
                    "sent": "So let's prove this key job, geometric inequality or general Pythagoras theorem.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out that this star estimator F hat satisfies for any H inside this class function F. We see constant equals one over 18th.",
                    "label": 0
                },
                {
                    "sent": "You can prove this theorem.",
                    "label": 0
                },
                {
                    "sent": "So if F is convex also this host condition on the data deterministically.",
                    "label": 0
                },
                {
                    "sent": "So if F is convex, then this piece offers similar hold with constant equal 1.",
                    "label": 0
                },
                {
                    "sent": "If F is linear subspace then this holds with equality.",
                    "label": 1
                },
                {
                    "sent": "So let's give a illustration of intuitive idea of how this is proved.",
                    "label": 0
                },
                {
                    "sent": "So remember, everything is conditioned on the data, so this is reduced to a leading is embedded in Euclidean space of dimension N.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the data Y is off in dimension you can denote it by a point inside this Euclidean space and the first step estimator the ERM.",
                    "label": 0
                },
                {
                    "sent": "You know that is it is within a certain radius compared to the Y.",
                    "label": 0
                },
                {
                    "sent": "So let's denote it by this be one ball.",
                    "label": 0
                },
                {
                    "sent": "And due to the optimality of the second step estimator, which is the star estimator, you know it lies somewhere inside this B1.",
                    "label": 0
                },
                {
                    "sent": "Let's denote it by B2.",
                    "label": 0
                },
                {
                    "sent": "And due to the optimality of this store construction, you know that actually you can say a little bit more than more on where this effect lies in.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Turns out it lies in the Carnegie Hall starting from the G hat intersect with the inside Bobby 2.",
                    "label": 0
                },
                {
                    "sent": "Because if there is another, if there is another and then for any age inside this F. It first of all, it only lies outside the conic conical, because if it's inside, it violates the optimality of the star estimator.",
                    "label": 0
                },
                {
                    "sent": "In the second step.",
                    "label": 0
                },
                {
                    "sent": "And also you know that trivially it is outside be one ball, so you put a restriction on the space where the class of function H can lie in and then through some reduction you can reduce the tool 2 dimensional problem and then through triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "And simple algebra, simple algebra you can show that actually this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimator satisfy the geometric inequality with constant equals 1 / 18.",
                    "label": 0
                },
                {
                    "sent": "And using that you can use the same trick.",
                    "label": 0
                },
                {
                    "sent": "You can add the positive quality to the access loss.",
                    "label": 0
                },
                {
                    "sent": "And then you can move from the access loss.",
                    "label": 0
                },
                {
                    "sent": "Tour superior of an active mean stochastic process with a mismatch concept here.",
                    "label": 0
                },
                {
                    "sent": "So this gives you the negative mean part and it gives you the faster rate.",
                    "label": 0
                },
                {
                    "sent": "So let me summarize up to now we upper bound the excess loss by a stochastic next mean stochastic process.",
                    "label": 0
                },
                {
                    "sent": "And now we want to relate the negative mean stochastic process to another negative mean stochastic process, which is offset rather complexity.",
                    "label": 0
                },
                {
                    "sent": "So we need to do some symmetrization trick and so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm up calculation if the class of function is bounded and if the noise is bounded.",
                    "label": 0
                },
                {
                    "sent": "Just a very simple calculation to illustrate the idea, you can use the standard contraction principle and symmetrization technique to show that actually the access loss is upper bounded by the negative mean stochastic process and then it's further upper bounded by this offset.",
                    "label": 0
                },
                {
                    "sent": "Random complexity for the class function H which is a little bit more complicated and F but complexity doesn't change the start of F.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we we are not only satisfied with in expectation results, we want to push, want to compare the tale of.",
                    "label": 0
                },
                {
                    "sent": "The excess loss and the red marker process.",
                    "label": 0
                },
                {
                    "sent": "So we want to show that.",
                    "label": 0
                },
                {
                    "sent": "On not only in terms of expectation, but also in terms of deviation, this is the right thing to look at.",
                    "label": 0
                },
                {
                    "sent": "It turns out we need to require a slight mode, alittle only a little bit, slight, more condition.",
                    "label": 0
                },
                {
                    "sent": "It is called the lower isometry condition is used a lot in the inverse problem in compressive sensing and all this thing.",
                    "label": 0
                },
                {
                    "sent": "So this assumption is.",
                    "label": 0
                },
                {
                    "sent": "For class of function H, if it satisfies the lower isometric bound, for some with high probability with some constant see this.",
                    "label": 1
                },
                {
                    "sent": "See can be does not need to be arbitrary close to zero.",
                    "label": 0
                },
                {
                    "sent": "It can be some constant, for example 1 / 72 is enough for our analysis.",
                    "label": 0
                },
                {
                    "sent": "The empirical L2 norm is lower bounded by 1 -- 3 * A sample of the population.",
                    "label": 0
                },
                {
                    "sent": "L2 norm with high probability as long as this N is larger than a certain complexity.",
                    "label": 0
                },
                {
                    "sent": "This whole it turns out that this this assumption is rather mild by a beautiful paper proved by Shahar Mendelson last year and this year.",
                    "label": 1
                },
                {
                    "sent": "This slower isometry holds under even heavy tail condition.",
                    "label": 0
                },
                {
                    "sent": "It's only 1 sided.",
                    "label": 1
                },
                {
                    "sent": "This only hold this holds only you only need to require the small ball assumption.",
                    "label": 0
                },
                {
                    "sent": "And a little bit non comparison condition.",
                    "label": 0
                },
                {
                    "sent": "Basically you need the Alcuna miss bounded by auto norm for Q between 2:00 and 4:00 and this of course holds for sub Gaussian classes and host for general heavy tail.",
                    "label": 0
                },
                {
                    "sent": "So we only need to require this and we want to get a stochastic control on the access loss.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the main theorem.",
                    "label": 0
                },
                {
                    "sent": "So the class is the star Hall.",
                    "label": 0
                },
                {
                    "sent": "And let's first assume some weak moment condition so.",
                    "label": 0
                },
                {
                    "sent": "Assume this star star condition holds this.",
                    "label": 0
                },
                {
                    "sent": "Then you can show that the tail of this excess loss is controlled by the tail stochastically controlled by the tail of the offset Rd marker process.",
                    "label": 0
                },
                {
                    "sent": "For any you are larger than 1 / N rate.",
                    "label": 1
                },
                {
                    "sent": "And as long as is any larger than this complexity, and this is where the local isometry constant requirement kicks in here.",
                    "label": 0
                },
                {
                    "sent": "Actually, we can remove the moment Condition star are using a probabilistic symmetrization trick by potranco in 2003.",
                    "label": 1
                },
                {
                    "sent": "So essentially only this lower isometry condition is what we needed, so these takers of heavy tailed distributions.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Up to now, we already stochastically controlled access loss by radmacher averages.",
                    "label": 0
                },
                {
                    "sent": "So can we actually, then, the next step is upper bounded, the upper bound offset around market process.",
                    "label": 0
                },
                {
                    "sent": "So if you remember in the picture I illustrate this local local automatic localization property of this offsets rather marker averages.",
                    "label": 0
                },
                {
                    "sent": "So here is the mathematical description defined.",
                    "label": 0
                },
                {
                    "sent": "These are star to be this.",
                    "label": 0
                },
                {
                    "sent": "Then you know the superior of unbounded class is exactly the same as the superior of the restricted class, restricted to the critical radius.",
                    "label": 0
                },
                {
                    "sent": "So you move from abounded to unbounded, to abounded class, and then is further bounded by this R score.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This just to see some example using this offset machinery for finite dimensional regression.",
                    "label": 0
                },
                {
                    "sent": "You guys get the correct rate for misspecified and well specified simultaneously.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For finite aggregation, you get the log N / N deviation, which is the correct one.",
                    "label": 0
                },
                {
                    "sent": "And you cannot use.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training technique to provide an explicit bound on this critical radius.",
                    "label": 0
                },
                {
                    "sent": "R star.",
                    "label": 0
                },
                {
                    "sent": "And you can.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also showed this works for for the nonparametric function class.",
                    "label": 0
                },
                {
                    "sent": "And we also have a.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lower bound on the minimax regret in terms of offset random marker complexity.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}