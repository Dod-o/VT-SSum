{
    "id": "ixxbu73l4qm34eau32x6gdsyjmkr423b",
    "title": "Covariate Dependent Random Partitions",
    "info": {
        "author": [
            "Peter Mueller, University of Texas M.D. Anderson Cancer Center, University of Texas at Austin"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_mueller_cdr/",
    "segmentation": [
        [
            "Thank you, thank you for the opportunity to present.",
            "Yeah, it's really exciting.",
            "It's really wonderful to have, like a mixed audience, and I'm looking forward to reactions, comments.",
            "Like I would very much love to have what I was saying.",
            "Very beginning Priest chip in common not only ask comment.",
            "Yeah I don't have much to say so don't worry I do not run out of time.",
            "I really don't have much to say in the end.",
            "It's going to be embarrassingly simple.",
            "See if that works.",
            "So this is joint work with Fernando Quintana in Santiago and Gary Rosen, also an MD Anderson.",
            "And I'll tell you about random partitioning, random partitioning that includes the regression covariates.",
            "So the theme here is probability models for clustering.",
            "Data indexed with covariates."
        ],
        [
            "I just got off with motivating example.",
            "I'll tell you a little bit about random partition models without covariates, just to set notation, etc.",
            "And then I'll introduce the actually proposed model that we suggest.",
            "It's going to be embarrassingly simple.",
            "Don't expect anything big, there isn't.",
            "I might make some comments about posterior inference and then I'll show you some examples."
        ],
        [
            "I.",
            "So let's start.",
            "Motivating example, just to explain what we offer."
        ],
        [
            "This is data from patients who are undergoing chemotherapy.",
            "The actually observed outcomes are blood counts.",
            "Overtime I use I for the experimental unit.",
            "In this case the patient.",
            "And we have 6 dimensional outcomes, so it's actually why I chair, and for each experimental unit, for each patient we have covariates XI.",
            "There's nothing random about the covariates, it could be things like the assigned treatment.",
            "It could also be things like baseline characteristics, treatment history, prior response, etc.",
            "Oops."
        ],
        [
            "9 plain English video is very simple and all I want to do for the rest of this talk is formalized.",
            "I want to make a prediction for the next patient.",
            "By doing the following thing.",
            "I want to.",
            "So you have to get closer to."
        ],
        [
            "I want to cluster the observed historical patients.",
            "I want to build classes of observed patients and then I want to match the new patient with one of the existing cluster."
        ],
        [
            "And I'll predict for her a similar response.",
            "Pretty simple idea right?",
            "The reason why we want to do that is because there's relatively few data points.",
            "There is only an equal 47 patients, a 6 dimensional outcome.",
            "So some high powered model based inference might be misplaced because inference would be all over the place where it's just simply looking at earlier pictures and seeing the new response.",
            "We look like one of the earlier pictures is pretty plausible, no?"
        ],
        [
            "I.",
            "So in order to do I need to become formal about the clustering?",
            "Now one or word of caution, clustering patients on the basis of the outcomes.",
            "Will not do the trick.",
            "I mean that would be very easy, but it's useless for prediction because when I want to predict the outcome from a new patient, I don't yet have her outcome, so you can't match it with any of the earlier clusters."
        ],
        [
            "But what they want instead is about to have a probability model on the partition on the clustering.",
            "That should be such that any two patients with similar covariates, any two patients who have similar family history, similar previous treatments, similar disease, etc, should be more likely.",
            "Our priority to pull class, so that's plausible, right when you try to predict the outcome for the new patient.",
            "Simply by copying the outcome from an earlier patient, you want to catch patients who have like similar characteristics."
        ],
        [
            "And in addition, since I want to make formal, I want to use that clustering and petition for formal prediction.",
            "So just deterministic clustering algorithms will not do I want to have a full blown probability model just to point out the difference to like there's many algorithms for highly intelligent, entirely useful clustering data, poorly deterministic.",
            "That's not what I'm after.",
            "I want to have a probability model on partitions."
        ],
        [
            "So that's the idea.",
            "Here, in plain English what's coming up?"
        ],
        [
            "I need a little bit of notation, not much.",
            "I'll index with I to experimental units in our."
        ],
        [
            "His patients than air partition.",
            "A clustering is simply a family of subsets as one through SK.",
            "K The number of subsets is part of the random partition and as one through his career clusters."
        ],
        [
            "Sometimes it's convenient to use an alternative notation instead of the family of subsets we can entirely equivalently uses set of cluster membership indicators.",
            "Azzabi with esobi equals J if and only if the IF patient is in cluster CHI.",
            "Same thing, right?",
            "Just sometimes it's more convenient."
        ],
        [
            "Then a random partition is simply a probability model.",
            "On rules, a ban on the petition, right?",
            "It's not even such an ugly animal, because rules of N lifts on a finite discrete probability space, right?",
            "There's only finitely many.",
            "There's often many partitions, but only finitely many.",
            "In plain text that's important because in plain text what I can do is 4 P of Rosa Ben.",
            "I'm allowed to write on anything that's non negative.",
            "You can always divide by the sum and it's legitimate probability, right?"
        ],
        [
            "I need help with my super end responses."
        ],
        [
            "To the end patient, same thing for X super N for the covariates."
        ],
        [
            "Thinking about covariates, think about long lists of covariates.",
            "When you run a clinical trial, usually record like long lists of covariates.",
            "So we're not just talking about one or two covariates, although that might show up in the examples, but usually you have a long list of covariates which include mixture of different data formats like continuous stuff, categorical stuff, binary stuff.",
            "I'll denote with X such a star and why Chase star covariates and responses by clusters so XJ star order covariance.",
            "Of our patients in the Chief cluster."
        ],
        [
            "Now I mainly want to talk about the probability model on the petition.",
            "So focus of the rest of the talk is going to be on P of Rowan.",
            "But in order to show your pretty pictures also need the likelihood they need a sampling model.",
            "So the sampling model that I'll use will always be the same in plain English.",
            "I'll assume that conditional on an assumed partition, ruin conditional on a partition.",
            "I assume that sampling is independent across clusters and exchangeable within each cluster.",
            "Which simply amounts to this expression star.",
            "There's nothing magic about it product over the clusters and within each cluster of a product of all observations in that cluster of a distribution that still index with a class specific parameter.",
            "Theta chi.",
            "To be specific, you lose nothing if you assume that this is simply normal.",
            "Yeah, so I assume normal sampling within each cluster with unknown moments.",
            "The moments are class specific and I complete the model."
        ],
        [
            "If the conjugate prior on the cluster specific normal moments.",
            "All you need to remember is there is a sampling model and it doesn't really matter what it is, yeah?"
        ],
        [
            "But to complete the entire probability model, I still owe you a prior on the rogue I IO diplay on the petition, and that's what we'll talk about.",
            "Before telling you about the probability model that we suggest, I want to talk a little bit through traditional models.",
            "There's a whole little cottage industry of probability models for partitions.",
            "It's not big mainstream, but it's quite common."
        ],
        [
            "Coming up on the next slide, perhaps the most beautiful of all, or at least one of the more popular ones.",
            "This is so called product partition model.",
            "So what we need is a probability model on a family of subsets, and if that probability model on the family of subsets S1 through SK can be written as a product of some function on each of these sub K, then The thing is called a product partition model.",
            "I.",
            "The sea of SJ is also called the cohesion function.",
            "And that's together with, for example, the sampling model from the previous slide defines a complete joint probability model, and usually the term PPM is actually used for this prior together with the sampling model just tradition.",
            "Yeah.",
            "Sociopath page.",
            "What what does it depend on other than the number of points?",
            "That's the only thing I can meaningfully think about.",
            "So for ex.",
            "Good point.",
            "For example, C of SJ could be some function of the criminality of SJ.",
            "For example, the most beautiful of auditoria process, the Polian, is a special case of this one.",
            "You get a polaron if you take cardinality of SG minus 1 factorial."
        ],
        [
            "I.",
            "Another popular one and someone mentioned it already today.",
            "Species sampling model.",
            "You could have a species sampling model if you define a probability model on the random partition is some symmetric function.",
            "Of the set of cardinalities.",
            "So the thing doesn't have to factor anymore.",
            "You can do some more tricks if you wish a.",
            "But if it takes that form, it's called a species sampling model.",
            "Just terminology, nothing else."
        ],
        [
            "For the moment, species sampling models are usually characterized by this so called predictive probability function, and we solve on today.",
            "I think earlier just writing down the predictive probability function for the cluster membership for the next guy conditional on the first guys.",
            "If you multiply it up that specifies the joint probability model on the partition.",
            "Carefully, you can characterize each SM by the PPF."
        ],
        [
            "That is not true.",
            "Not each PPF defines a legitimate SM.",
            "It's I asked someone today because something came up today there.",
            "We saw PPS and wasn't sure about it defined."
        ],
        [
            "So that was it."
        ],
        [
            "Each assembly model.",
            "One more, it's reasonably popular.",
            "It's becoming a little bit more indirect, so rather than explicitly spelling out the probability model on the random partition, we now specify a probability model on the random partition indirectly by writing down a mixture model.",
            "If you write on a mixture model for the observed data.",
            "For example, a mixture of normals.",
            "If you write on a mixture of key normals, then this implicitly defines a probability model on a partition."
        ],
        [
            "By the usual trick that you can always rewrite a mixture model is an equivalent hierarchical model by introducing draws, red latent indicators S bye.",
            "Look at the last two lines marginalized out the red guys marginalizing author discrete guide just simply means taking the summation and you end up exactly with the mixture again.",
            "If you interpret those red lights and indicators as cluster membership indicators, then you have a probability model on their petition.",
            "Having done expandability and infinite extensibility, put these different partition models.",
            "So far all of them, just by construction, are exchangeable with respect to permutations of the.",
            "Of the indices for the experimental units.",
            "But that certainly will come back to that.",
            "That certainly if you start to invent new probability models for partitions, if the probability model is not exchangeable with respect to indices of the observations.",
            "That would be highly embarrassing, yeah?",
            "By the way, that can happen if you just write down.",
            "If you just write down an arbitrary PPF, and if you might be play up the TPS, you might happily have defined linear model that's not.",
            "So one of the just the other big danger is coherence.",
            "Across emphasizes, it would be very embarrassing if your probability model on clustering guys.",
            "Depends upon whether or not you will ever think about in N plus first guy.",
            "In other words, the probability model for clustering end guys should arise as the marginal from the probability model for clustering N plus one guys.",
            "That said, there are situations where there's no notion of prediction of forecasting or any new guy ever.",
            "Yeah, if that's the case, then you don't need to insist on that property.",
            "My question with.",
            "Or is everything you described for appearing in the sense that you mentioned?",
            "The PPM yes.",
            "The SSM.",
            "Yes, unless you are right on arbitrary PPF switch wouldn't define what we call TSM.",
            "And also this one because see conditional and conditional on all the parameters in the conditioning set behind the bar.",
            "We have independent sampling so you just have a product.",
            "This is also called model based clustering and it's reasonably popular.",
            "Yeah, it's implicit.",
            "The clustering in here, but many people think of this is just defining a probability model on the clustering."
        ],
        [
            "We already saw today to pull around.",
            "Just want to mention it because it seems to be by far the most popular of our even to the extent I see many applications of the dealer process prior during the process.",
            "Prior probability model on the random distribution, right?",
            "But many applications don't make any use of the random distribution at all, or you don't see any inference on the random distribution or what people are concerned with is the probability model on the petition.",
            "Which is this one here?",
            "If you use the following PPF defaulting predictive probability function specified like the polaron here with these very specific."
        ],
        [
            "Probabilities, it turns out that is illegitimate PPS means that defines species sampling model and it actually happens to be the random partition that's implied by the configuration of ties in a random sample from a random probability measure with a dealership."
        ],
        [
            "This is prior.",
            "So the dealership there Polaron which is the distribution on the random partition, happens to be a special case of a species sampling model.",
            "It also happens to be a special case of a product partition."
        ],
        [
            "Model it also had."
        ],
        [
            "Needs to be a special invalid, least limiting case of model based clustering.",
            "Perhaps that's one of the reasons why it's so popular.",
            "It keeps on dropping out a special case of just about everything."
        ],
        [
            "End of the footnote.",
            "I.",
            "So this was a quick rundown of popular probability models for random partitions.",
            "Remember again, once we started out with a promise to a probability model for random partitions.",
            "It's index with covariates.",
            "So now we have experimental units and each experimental unit is labeled with a covariate and they want to right around to invent.",
            "I want to propose a probability model for random partitions, that is such that experimental units with similar covariates should be our priority before we see any response, our priority should be more likely to cluster together.",
            "And here's how we're going to achieve that.",
            "Will make use will assume that you are there.",
            "I will assume that the city station can write down a similarity function.",
            "I assume that you can tell me how similar is set of covariates.",
            "Is record exchange star.",
            "Where the covariates and appended chief cluster.",
            "That similarity function should be such that it's not negative.",
            "Greater equals 0 is good enough.",
            "Lower values characterize bad clusters.",
            "High values characterize good clusters.",
            "For example, a cluster of all women with similar H and similar treatment history should get a very high score."
        ],
        [
            "Then we are doing the following thing and it's embarrassing.",
            "So embarrassingly simple.",
            "You can fit it in one line.",
            "All what we're doing is right on.",
            "Again.",
            "The approach petition model in Black is again the PPM.",
            "And in order to achieve the desired feature of increasing the probability of experimental units that are similar to Co cluster, we simply brutally Mike apply with that similarity function that we assume you have.",
            "The clue is you're allowed to do that because the space here the.",
            "Possible partitions.",
            "They live on a finite discrete space, so you can multiply with anything non negative and you get the legitimate probability model, that's it.",
            "So what we suggest to do is take the PPM and sneak in some pink yeah.",
            "By integrating out variables that can still happen, changing the way here.",
            "But if I write down arbitrary keys, do I still have an exchange for distribution by some structural properties?",
            "Correct, this was a paid for question.",
            "The answer is coming coming up here.",
            "Quick answer, not necessarily yeah."
        ],
        [
            "You can actually spell out the normalization constant if you care just simply."
        ],
        [
            "Decimation of our.",
            "It's not always so obvious what you want to take as your similarity function.",
            "Here's a default choice.",
            "Default suggestion instead answers your question.",
            "So under this default suggestion you get it in the next slide.",
            "I'll essentially say that this default suggestion is not only convenient, but it's the only possible one if you want it convenient property.",
            "So here's just a default construction of a similarity function.",
            "Consider an auxiliary probability model auxiliary, because there's no notion of the axis being random variables.",
            "Yeah, but just takes a axes are continuous.",
            "Take a normal distribution.",
            "If the X is a mighty warrior continuous, they can market better normal distribution with unknown moments, sigh, and then integrate Arctic side with respect to preferably a conjugate distribution.",
            "So we can do the integral in closed form and what you get is what we call the similarity function.",
            "So think of the X as Mike variant continuous.",
            "Think of the blue guy as a multi variant normal with unknown moments.",
            "The green guy conjugate prior on there might be very normal moments.",
            "I can do the integral and what I get is something that has exactly the desired property to closer that those mighty variot continuous observations are packed together.",
            "The higher the score.",
            "If they are very spread out, then you'll get a very low score.",
            "The advantage is simply this is something that is trivial to evaluate, like for any set of XJ star can evaluate it in closed form.",
            "Anne."
        ],
        [
            "And it satisfies the following conditions.",
            "I already talked about it.",
            "When you write down a proposal for a random partition, the first thing you want is invariant with respect to permutation of the observation in this is, so you want to have symmetry with respect to permutations of the indices.",
            "That said."
        ],
        [
            "Only requirement you want, and here's another requirement that you might want.",
            "If we consider the following property, if we are willing to consider the property that similarity average similarity.",
            "Of a set of covariates X star plus one more.",
            "Should be exactly the similarity of X star.",
            "It's not obvious, by the way that you might want it, yeah, but it's convenient because it has a nice implication.",
            "It's not obvious that you want it, but it is convenient.",
            "You might already see that immediate leads to the following stay."
        ],
        [
            "It meant that leads trivia later this statement that dissimilarity function must be of that formula, because essentially all of them seeing is it behaves like a probability model, it's exchangeable.",
            "So if you believe in the two blue properties, then it must be of that form that they still called the default suggestion on the previous slide."
        ],
        [
            "There's a nice implication if you have a similarity function of that form, then you also get coherence across sample size when you talk about coherence across sample size.",
            "When you have a regression, when include covariates, we have to be a bit careful to say what we mean when there are no covariates when there are no covariates, then all we would want so without covariates.",
            "If there were no access or what we would want is that summation, then we would just wanted the probability model for partitioning guys to be the probability model for partitioning N plus one guys.",
            "Marginalizing with respect to the last one.",
            "I mean, there are covariates.",
            "Then I have to make some statement of how I get rid of the covariate behind the bar.",
            "Anne."
        ],
        [
            "If you do it like this then coherence across emphasized words.",
            "So in plain English, I'm just saying there's a notion of coherence across sample size.",
            "Partition.",
            "Partition.",
            "And there expecting role models like effects and is G is basically figuring out probability model correct?",
            "Exactly correct?",
            "There's a subtlety to it.",
            "Subtlety is the two probability models have to be independent in the sense that parameters here use the parameter site Shafer.",
            "The cluster specific parameter that defines the probability model on the axis and that attache that was indexing their probability model on device.",
            "They have to be independent.",
            "I.",
            "That subtlety is important because it implies that the opposite is not necessarily true.",
            "You can't OK."
        ],
        [
            "So if you, if you believe in these two properties and the first one, the first one is a must, the second one, I confess, is a bit stupid artificial.",
            "Yeah, if you are too kind to me, but if you were to press me on the 2nd one I couldn't argue it out easily.",
            "It's not obvious that you wanted, but if we insist under 2 blue properties, then the default similarity function is actually only one and then life becomes easy, because now I can give you a whole list of similarity functions for common data formats.",
            "Like what we usually have is you have a list of career, summer, continue, summer, categorical, some accounts like number of children.",
            "In this particular example, summer binary."
        ],
        [
            "Etc.",
            "And two, the obvious one for continuous covariates."
        ],
        [
            "Use the normal.",
            "I normally curiously for categorical.",
            "For example, might be normal probit for ordinal personal game of accounts.",
            "The only reason being technical convenience so we can evaluate that function in closed form.",
            "So now we have similarity functions for any kind of common data format, we have a probability model.",
            "We essentially done right."
        ],
        [
            "And before showing you more examples, I want to talk you through some alternative constructions.",
            "Or any comments complaints at this moment?",
            "I wanted to should be because.",
            "And there's a couple of I talked to prove.",
            "I talked it through one way of setting that thing up.",
            "Yeah, it's not a new problem.",
            "I mean, people have been dealing with this problem for awhile.",
            "What is usually done is the following trick.",
            "And you essentially hinted at that what's usually done is replaced their response to original response.",
            "Why replace it by an augmented response by X?",
            "In other words, pretend and proceed as if the covariate were part of the response vector."
        ],
        [
            "And then we could simply use for the desired prediction the conditional posterior predictive of YN plus one conditional on observing part of the response vector XN plus one is well defined and everything."
        ],
        [
            "The catch, of course, is if we do that.",
            "We are really using."
        ],
        [
            "The wrong likelihood, because if we augment our response to that paravai X, then we have introduced an additional factor P of X given Theta, right?",
            "And that's not as innocent as it might seem, because it's quite possible that device univariate and X might be high dimensional.",
            "So in other words, that fake factor that suddenly arises and the likelihood might dominate in.",
            "If the latter, the extra sampling model that we had in the original model, so it can be done, but it might be less innocent than what it seems, and to make matters worse, it's actually nothing easier.",
            "I'll talk in a moment, I'll talk you through posterior.",
            "Duration and posterior simulation of proposed model is not much more difficult either."
        ],
        [
            "Another common fix.",
            "One way to achieve the desired random partition index with covariates, yeah?",
            "So you can think of this is better.",
            "It could be using the wrong likelihood, especially in the case when X is very high dimensional, but.",
            "Isn't isn't the same problem going to happen if you have a G function, a similarity function that is defined through?",
            "Bye.",
            "You know that.",
            "Differences over the later, why don't see any difference between that problem here and probably there quick answer.",
            "Essentially, essentially yes, but with the caveat.",
            "Essentially yes in the sense.",
            "We could simply can you repeat your comment in my words are kind of trick number.",
            "One answer, different question can't we?",
            "Can't we called it P of X given Theta concrete just called it our similarity function?",
            "Essentially yes, but the big difference.",
            "If the big difference is if you called it the similarity function, then that at a that was indexing the sampling model for the original responses suddenly floats around in my similarity function and that can mess up things.",
            "Of course you can set it up carefully in such a way that model 4X only depends on a subvector of the data which does not arise in the sampling model.",
            "And if you do that, then you're exactly back to what we have.",
            "So in that sense, that is why I said yes, but the caveat is if you do that, you have to be a bit careful.",
            "The other caveat is if the X is only univariate and you kind of understand what you're doing, I think you can get away with it.",
            "The trick has been a similar trick has been used in a paper by Robert Connan Wong and others for inference on random correlation matrices.",
            "When you have a random covariance matrix covariance matrix, it better be positive definite.",
            "So what was there?",
            "This is somehow somebody ran into the similar trick intended, just called.",
            "What they did was they called extra factor that sneaks in recorded extra factor part of the prior oops.",
            "So instead of calling it part of the similarity function, you could call it part of the prior, which means you change your prior.",
            "But you could do so in other ways.",
            "So other default trick, sneak the regression covariates into a mixture model.",
            "Nothing stops you from record a mixture model that I told you any very big.",
            "Our price of Chivas debate of the Chief German.",
            "The mixture you can make that depend on covariance.",
            "For example with logistic."
        ],
        [
            "And nothing wrong in doing so, but in doing so you would probably need like parameters.",
            "For example if that were.",
            "If there might be normal logistic, then Anita logistic regression Param."
        ],
        [
            "Letters and then you have a fixed sample size."
        ],
        [
            "SK of course you can.",
            "You can work around it by considering limit K to Infinity, and if you do that in a careful way at then end up exactly back to the PPM with the extra factor it ahead."
        ],
        [
            "And last not least."
        ],
        [
            "Oops.",
            "We could simply do the following brutal things and."
        ],
        [
            "It becomes really brutal.",
            "Yeah, we could take the PPF.",
            "So what you have here in Black is again the PPF from the polaron and you could just stay at their PPF from the polaron and we could modify it with the traditional red factor in such a way that a new woman.",
            "It's more likely to join a cluster of women who have similar covariates.",
            "Nothing stops you from doing that except what you might be playing havoc.",
            "Yeah, that's getting back to your comment.",
            "I mean, they do such a thing.",
            "It's not even clear anymore that The thing is exchangeable.",
            "In other words, it's not clear that these are legitimate conditioners to an underlying joint that is symmetric with respect to permutations of the indices.",
            "That said, sometime."
        ],
        [
            "So it's simply you're not allowed to do that.",
            "Yeah, but that said."
        ],
        [
            "I. I have seen it work fine in X."
        ],
        [
            "Data analysis applications, yeah.",
            "No contradiction.",
            "Yeah, sometimes doing something reasonable, even if it's not coherent, might still give you decent inference.",
            "I was waiting for complaints and I could have said next slide answer it, but you let me get away with everything."
        ],
        [
            "I.",
            "5 minutes.",
            "I.",
            "Our posterior inference for random partitions is not trivial, so even let's step back like forget for a moment.",
            "Indexing with covariates, just posterior inference for random clustering is not easy because it's a finite discrete space, but it's an awfully big one.",
            "There's no notion of a posterior means such a thing doesn't exist.",
            "Even the posterior mode, even if you were able to find the posterior mode, it's usually not very.",
            "It's not a very useful summary because the posterior mode might have posterior probability 1.2.",
            "And the second best partition might have posterior probability 1.05%.",
            "So the posterior mode might be nothing particularly."
        ],
        [
            "Eristic, so there's all."
        ],
        [
            "Kinds of suggestions of how to proceed."
        ],
        [
            "Even without covariates.",
            "And here is there are two."
        ],
        [
            "Online summary of how we do it with the code in the augmented model.",
            "Quick answer, you do proceed exactly the same way, essentially after a trivial model augmentation.",
            "Computationally proposed model reduces exactly.",
            "Computation reduces exactly the posterior simulation in a random partition model without covariates.",
            "The trick is essentially a recognized the similarity function as part of a big joint probability model, which has a special structure, but the structure is already there."
        ],
        [
            "Then you can proceed."
        ],
        [
            "And I just spelled out the details.",
            "And there's nothing magic about it.",
            "The opposite, by the way, is not true.",
            "The model with a combined response cannot necessarily be interpreted as the proposed model, but the proposed model can be interpreted as a special case of a model with a combined risk."
        ],
        [
            "Jones and that gets us back to the original example.",
            "So remember again, we had N patients."
        ],
        [
            "In that trial.",
            "And we are now and there is 6 responses at each given time.",
            "Or in other words, the response by some I is a 6 dimensional."
        ],
        [
            "Hector and we want to make inferences about the N plus first patient and the model that we use is exactly the proposed model.",
            "The only covariate that we're going to use here is treatment tools.",
            "So treatment tools there's certainly nothing random about it.",
            "You get to set."
        ],
        [
            "Yeah."
        ],
        [
            "And sampling model is Manila.",
            "Michael Berry."
        ],
        [
            "Normal and every."
        ],
        [
            "Using default.",
            "Here's the data.",
            "This is the data for the observed 47 patients.",
            "Each of these spaghetti is 1 patient.",
            "The six responses, actually six blood counts at 6 times during the chemotherapy, so it makes sense to connect them.",
            "So each of those spaghettis is 1 patient.",
            "And what we do now is to predict for the N plus first patient we look at earlier patients with similar tools.",
            "And will do the most simple thing possible.",
            "My prediction for their response for a new patient is the average of the spaghettis of women who were treated with a similar dose earlier.",
            "That's it, yeah."
        ],
        [
            "And here it is or just to drive the message home on the right hand side I do the same exercise using a probability model on the partition but not indexed with covariate and into the prediction since it's not index with covariates.",
            "Of course the petition the prediction is the same no matter what the covariate is.",
            "Verify indexing with covariates, they get different prediction depending on about the treatment level is.",
            "So these six different predictions.",
            "A quote for six different levels of the treatment doors.",
            "And each of these, each of these predictions is simply the average.",
            "Over classes of earlier observed patients were clusters of women with similar treatment tools.",
            "As the new patient ovated.",
            "Higher ten classes of women with very different treatment.",
            "Those kind of the obvious thing to do.",
            "This model.",
            "Do you understand?",
            "Start attending random.",
            "No filtering.",
            "I.",
            "Good comment, quick answer, nor long answer may be slightly different question if you were to if all patients had exactly the same covariate value, that's kind of the extreme version of what you're telling, right?",
            "If all patients are exactly the same covariate value, wouldn't it be nice if the model would exactly revert to the North covariate model?",
            "Strictly, the answer is no.",
            "Approximately the answer is yes.",
            "Ideally I would like, ideally, of course we would love to have the answer to be yes, we would love to have the situation that when all the covariates are the same, I want to see I want to see exactly the inference that would have without covariate.",
            "That's not quite true because the similarity function, the reason being the similarity function, implicitly introduces a penalty for cluster size.",
            "And getting rid of that is more complicated than what The thing is we have here.",
            "But approximately, it's true.",
            "In the Fed it's true."
        ],
        [
            "And it gets."
        ],
        [
            "Into a second example, if I'm still a lot.",
            "A longer list of covariates, so now it was six covariates.",
            "Mixture of categorical count and continuous outcome in this case is an event time.",
            "Time to progression, nothing special about censoring."
        ],
        [
            "And."
        ],
        [
            "Yeah, you can beautifully get an inference.",
            "This is not inference for a future observation arranged by different levels of the covariates.",
            "I just picked here three different covariates, initial tumor size, high versus low dose of the treatment here, positive versus not your positive, and for each of these different combinations you see a different prediction.",
            "This being event time data to predict prediction is shown in terms of survival function and summarizes it and drops off natural."
        ],
        [
            "And that gets me to the final slide.",
            "Just telling you that we did.",
            "We proposed the probability model for random partitions that had the additional twist.",
            "Additional feature that the model is indexed with covariates, like we're doing regression where the outcome happens to be random partition.",
            "Posterior inference straightforward.",
            "There are all kinds of practical problems like labor switching problem etc.",
            "When I showed you the pictures, we did something reasonable.",
            "Our talk to deal with that.",
            "Similar issue called Subspace Clustering, abit similar but not quite the same and that's it, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you, thank you for the opportunity to present.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's really exciting.",
                    "label": 0
                },
                {
                    "sent": "It's really wonderful to have, like a mixed audience, and I'm looking forward to reactions, comments.",
                    "label": 0
                },
                {
                    "sent": "Like I would very much love to have what I was saying.",
                    "label": 0
                },
                {
                    "sent": "Very beginning Priest chip in common not only ask comment.",
                    "label": 0
                },
                {
                    "sent": "Yeah I don't have much to say so don't worry I do not run out of time.",
                    "label": 0
                },
                {
                    "sent": "I really don't have much to say in the end.",
                    "label": 0
                },
                {
                    "sent": "It's going to be embarrassingly simple.",
                    "label": 0
                },
                {
                    "sent": "See if that works.",
                    "label": 0
                },
                {
                    "sent": "So this is joint work with Fernando Quintana in Santiago and Gary Rosen, also an MD Anderson.",
                    "label": 1
                },
                {
                    "sent": "And I'll tell you about random partitioning, random partitioning that includes the regression covariates.",
                    "label": 0
                },
                {
                    "sent": "So the theme here is probability models for clustering.",
                    "label": 0
                },
                {
                    "sent": "Data indexed with covariates.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I just got off with motivating example.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you a little bit about random partition models without covariates, just to set notation, etc.",
                    "label": 1
                },
                {
                    "sent": "And then I'll introduce the actually proposed model that we suggest.",
                    "label": 0
                },
                {
                    "sent": "It's going to be embarrassingly simple.",
                    "label": 0
                },
                {
                    "sent": "Don't expect anything big, there isn't.",
                    "label": 1
                },
                {
                    "sent": "I might make some comments about posterior inference and then I'll show you some examples.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So let's start.",
                    "label": 0
                },
                {
                    "sent": "Motivating example, just to explain what we offer.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is data from patients who are undergoing chemotherapy.",
                    "label": 0
                },
                {
                    "sent": "The actually observed outcomes are blood counts.",
                    "label": 0
                },
                {
                    "sent": "Overtime I use I for the experimental unit.",
                    "label": 0
                },
                {
                    "sent": "In this case the patient.",
                    "label": 0
                },
                {
                    "sent": "And we have 6 dimensional outcomes, so it's actually why I chair, and for each experimental unit, for each patient we have covariates XI.",
                    "label": 0
                },
                {
                    "sent": "There's nothing random about the covariates, it could be things like the assigned treatment.",
                    "label": 0
                },
                {
                    "sent": "It could also be things like baseline characteristics, treatment history, prior response, etc.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "9 plain English video is very simple and all I want to do for the rest of this talk is formalized.",
                    "label": 0
                },
                {
                    "sent": "I want to make a prediction for the next patient.",
                    "label": 1
                },
                {
                    "sent": "By doing the following thing.",
                    "label": 0
                },
                {
                    "sent": "I want to.",
                    "label": 0
                },
                {
                    "sent": "So you have to get closer to.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to cluster the observed historical patients.",
                    "label": 0
                },
                {
                    "sent": "I want to build classes of observed patients and then I want to match the new patient with one of the existing cluster.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'll predict for her a similar response.",
                    "label": 1
                },
                {
                    "sent": "Pretty simple idea right?",
                    "label": 1
                },
                {
                    "sent": "The reason why we want to do that is because there's relatively few data points.",
                    "label": 0
                },
                {
                    "sent": "There is only an equal 47 patients, a 6 dimensional outcome.",
                    "label": 0
                },
                {
                    "sent": "So some high powered model based inference might be misplaced because inference would be all over the place where it's just simply looking at earlier pictures and seeing the new response.",
                    "label": 0
                },
                {
                    "sent": "We look like one of the earlier pictures is pretty plausible, no?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So in order to do I need to become formal about the clustering?",
                    "label": 0
                },
                {
                    "sent": "Now one or word of caution, clustering patients on the basis of the outcomes.",
                    "label": 0
                },
                {
                    "sent": "Will not do the trick.",
                    "label": 0
                },
                {
                    "sent": "I mean that would be very easy, but it's useless for prediction because when I want to predict the outcome from a new patient, I don't yet have her outcome, so you can't match it with any of the earlier clusters.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But what they want instead is about to have a probability model on the partition on the clustering.",
                    "label": 0
                },
                {
                    "sent": "That should be such that any two patients with similar covariates, any two patients who have similar family history, similar previous treatments, similar disease, etc, should be more likely.",
                    "label": 1
                },
                {
                    "sent": "Our priority to pull class, so that's plausible, right when you try to predict the outcome for the new patient.",
                    "label": 0
                },
                {
                    "sent": "Simply by copying the outcome from an earlier patient, you want to catch patients who have like similar characteristics.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in addition, since I want to make formal, I want to use that clustering and petition for formal prediction.",
                    "label": 0
                },
                {
                    "sent": "So just deterministic clustering algorithms will not do I want to have a full blown probability model just to point out the difference to like there's many algorithms for highly intelligent, entirely useful clustering data, poorly deterministic.",
                    "label": 0
                },
                {
                    "sent": "That's not what I'm after.",
                    "label": 0
                },
                {
                    "sent": "I want to have a probability model on partitions.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the idea.",
                    "label": 0
                },
                {
                    "sent": "Here, in plain English what's coming up?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I need a little bit of notation, not much.",
                    "label": 0
                },
                {
                    "sent": "I'll index with I to experimental units in our.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "His patients than air partition.",
                    "label": 0
                },
                {
                    "sent": "A clustering is simply a family of subsets as one through SK.",
                    "label": 0
                },
                {
                    "sent": "K The number of subsets is part of the random partition and as one through his career clusters.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sometimes it's convenient to use an alternative notation instead of the family of subsets we can entirely equivalently uses set of cluster membership indicators.",
                    "label": 0
                },
                {
                    "sent": "Azzabi with esobi equals J if and only if the IF patient is in cluster CHI.",
                    "label": 0
                },
                {
                    "sent": "Same thing, right?",
                    "label": 0
                },
                {
                    "sent": "Just sometimes it's more convenient.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then a random partition is simply a probability model.",
                    "label": 0
                },
                {
                    "sent": "On rules, a ban on the petition, right?",
                    "label": 0
                },
                {
                    "sent": "It's not even such an ugly animal, because rules of N lifts on a finite discrete probability space, right?",
                    "label": 0
                },
                {
                    "sent": "There's only finitely many.",
                    "label": 0
                },
                {
                    "sent": "There's often many partitions, but only finitely many.",
                    "label": 0
                },
                {
                    "sent": "In plain text that's important because in plain text what I can do is 4 P of Rosa Ben.",
                    "label": 0
                },
                {
                    "sent": "I'm allowed to write on anything that's non negative.",
                    "label": 0
                },
                {
                    "sent": "You can always divide by the sum and it's legitimate probability, right?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I need help with my super end responses.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the end patient, same thing for X super N for the covariates.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thinking about covariates, think about long lists of covariates.",
                    "label": 0
                },
                {
                    "sent": "When you run a clinical trial, usually record like long lists of covariates.",
                    "label": 0
                },
                {
                    "sent": "So we're not just talking about one or two covariates, although that might show up in the examples, but usually you have a long list of covariates which include mixture of different data formats like continuous stuff, categorical stuff, binary stuff.",
                    "label": 0
                },
                {
                    "sent": "I'll denote with X such a star and why Chase star covariates and responses by clusters so XJ star order covariance.",
                    "label": 0
                },
                {
                    "sent": "Of our patients in the Chief cluster.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I mainly want to talk about the probability model on the petition.",
                    "label": 0
                },
                {
                    "sent": "So focus of the rest of the talk is going to be on P of Rowan.",
                    "label": 0
                },
                {
                    "sent": "But in order to show your pretty pictures also need the likelihood they need a sampling model.",
                    "label": 0
                },
                {
                    "sent": "So the sampling model that I'll use will always be the same in plain English.",
                    "label": 1
                },
                {
                    "sent": "I'll assume that conditional on an assumed partition, ruin conditional on a partition.",
                    "label": 1
                },
                {
                    "sent": "I assume that sampling is independent across clusters and exchangeable within each cluster.",
                    "label": 0
                },
                {
                    "sent": "Which simply amounts to this expression star.",
                    "label": 0
                },
                {
                    "sent": "There's nothing magic about it product over the clusters and within each cluster of a product of all observations in that cluster of a distribution that still index with a class specific parameter.",
                    "label": 0
                },
                {
                    "sent": "Theta chi.",
                    "label": 0
                },
                {
                    "sent": "To be specific, you lose nothing if you assume that this is simply normal.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I assume normal sampling within each cluster with unknown moments.",
                    "label": 0
                },
                {
                    "sent": "The moments are class specific and I complete the model.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If the conjugate prior on the cluster specific normal moments.",
                    "label": 0
                },
                {
                    "sent": "All you need to remember is there is a sampling model and it doesn't really matter what it is, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But to complete the entire probability model, I still owe you a prior on the rogue I IO diplay on the petition, and that's what we'll talk about.",
                    "label": 0
                },
                {
                    "sent": "Before telling you about the probability model that we suggest, I want to talk a little bit through traditional models.",
                    "label": 0
                },
                {
                    "sent": "There's a whole little cottage industry of probability models for partitions.",
                    "label": 0
                },
                {
                    "sent": "It's not big mainstream, but it's quite common.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Coming up on the next slide, perhaps the most beautiful of all, or at least one of the more popular ones.",
                    "label": 0
                },
                {
                    "sent": "This is so called product partition model.",
                    "label": 1
                },
                {
                    "sent": "So what we need is a probability model on a family of subsets, and if that probability model on the family of subsets S1 through SK can be written as a product of some function on each of these sub K, then The thing is called a product partition model.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "The sea of SJ is also called the cohesion function.",
                    "label": 0
                },
                {
                    "sent": "And that's together with, for example, the sampling model from the previous slide defines a complete joint probability model, and usually the term PPM is actually used for this prior together with the sampling model just tradition.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Sociopath page.",
                    "label": 0
                },
                {
                    "sent": "What what does it depend on other than the number of points?",
                    "label": 0
                },
                {
                    "sent": "That's the only thing I can meaningfully think about.",
                    "label": 0
                },
                {
                    "sent": "So for ex.",
                    "label": 0
                },
                {
                    "sent": "Good point.",
                    "label": 0
                },
                {
                    "sent": "For example, C of SJ could be some function of the criminality of SJ.",
                    "label": 0
                },
                {
                    "sent": "For example, the most beautiful of auditoria process, the Polian, is a special case of this one.",
                    "label": 0
                },
                {
                    "sent": "You get a polaron if you take cardinality of SG minus 1 factorial.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Another popular one and someone mentioned it already today.",
                    "label": 0
                },
                {
                    "sent": "Species sampling model.",
                    "label": 0
                },
                {
                    "sent": "You could have a species sampling model if you define a probability model on the random partition is some symmetric function.",
                    "label": 0
                },
                {
                    "sent": "Of the set of cardinalities.",
                    "label": 0
                },
                {
                    "sent": "So the thing doesn't have to factor anymore.",
                    "label": 0
                },
                {
                    "sent": "You can do some more tricks if you wish a.",
                    "label": 0
                },
                {
                    "sent": "But if it takes that form, it's called a species sampling model.",
                    "label": 1
                },
                {
                    "sent": "Just terminology, nothing else.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the moment, species sampling models are usually characterized by this so called predictive probability function, and we solve on today.",
                    "label": 0
                },
                {
                    "sent": "I think earlier just writing down the predictive probability function for the cluster membership for the next guy conditional on the first guys.",
                    "label": 0
                },
                {
                    "sent": "If you multiply it up that specifies the joint probability model on the partition.",
                    "label": 0
                },
                {
                    "sent": "Carefully, you can characterize each SM by the PPF.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is not true.",
                    "label": 0
                },
                {
                    "sent": "Not each PPF defines a legitimate SM.",
                    "label": 0
                },
                {
                    "sent": "It's I asked someone today because something came up today there.",
                    "label": 0
                },
                {
                    "sent": "We saw PPS and wasn't sure about it defined.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that was it.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each assembly model.",
                    "label": 0
                },
                {
                    "sent": "One more, it's reasonably popular.",
                    "label": 0
                },
                {
                    "sent": "It's becoming a little bit more indirect, so rather than explicitly spelling out the probability model on the random partition, we now specify a probability model on the random partition indirectly by writing down a mixture model.",
                    "label": 0
                },
                {
                    "sent": "If you write on a mixture model for the observed data.",
                    "label": 0
                },
                {
                    "sent": "For example, a mixture of normals.",
                    "label": 0
                },
                {
                    "sent": "If you write on a mixture of key normals, then this implicitly defines a probability model on a partition.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By the usual trick that you can always rewrite a mixture model is an equivalent hierarchical model by introducing draws, red latent indicators S bye.",
                    "label": 1
                },
                {
                    "sent": "Look at the last two lines marginalized out the red guys marginalizing author discrete guide just simply means taking the summation and you end up exactly with the mixture again.",
                    "label": 0
                },
                {
                    "sent": "If you interpret those red lights and indicators as cluster membership indicators, then you have a probability model on their petition.",
                    "label": 1
                },
                {
                    "sent": "Having done expandability and infinite extensibility, put these different partition models.",
                    "label": 0
                },
                {
                    "sent": "So far all of them, just by construction, are exchangeable with respect to permutations of the.",
                    "label": 0
                },
                {
                    "sent": "Of the indices for the experimental units.",
                    "label": 0
                },
                {
                    "sent": "But that certainly will come back to that.",
                    "label": 0
                },
                {
                    "sent": "That certainly if you start to invent new probability models for partitions, if the probability model is not exchangeable with respect to indices of the observations.",
                    "label": 0
                },
                {
                    "sent": "That would be highly embarrassing, yeah?",
                    "label": 0
                },
                {
                    "sent": "By the way, that can happen if you just write down.",
                    "label": 0
                },
                {
                    "sent": "If you just write down an arbitrary PPF, and if you might be play up the TPS, you might happily have defined linear model that's not.",
                    "label": 0
                },
                {
                    "sent": "So one of the just the other big danger is coherence.",
                    "label": 0
                },
                {
                    "sent": "Across emphasizes, it would be very embarrassing if your probability model on clustering guys.",
                    "label": 0
                },
                {
                    "sent": "Depends upon whether or not you will ever think about in N plus first guy.",
                    "label": 0
                },
                {
                    "sent": "In other words, the probability model for clustering end guys should arise as the marginal from the probability model for clustering N plus one guys.",
                    "label": 0
                },
                {
                    "sent": "That said, there are situations where there's no notion of prediction of forecasting or any new guy ever.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if that's the case, then you don't need to insist on that property.",
                    "label": 0
                },
                {
                    "sent": "My question with.",
                    "label": 0
                },
                {
                    "sent": "Or is everything you described for appearing in the sense that you mentioned?",
                    "label": 0
                },
                {
                    "sent": "The PPM yes.",
                    "label": 0
                },
                {
                    "sent": "The SSM.",
                    "label": 0
                },
                {
                    "sent": "Yes, unless you are right on arbitrary PPF switch wouldn't define what we call TSM.",
                    "label": 0
                },
                {
                    "sent": "And also this one because see conditional and conditional on all the parameters in the conditioning set behind the bar.",
                    "label": 0
                },
                {
                    "sent": "We have independent sampling so you just have a product.",
                    "label": 0
                },
                {
                    "sent": "This is also called model based clustering and it's reasonably popular.",
                    "label": 1
                },
                {
                    "sent": "Yeah, it's implicit.",
                    "label": 0
                },
                {
                    "sent": "The clustering in here, but many people think of this is just defining a probability model on the clustering.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We already saw today to pull around.",
                    "label": 0
                },
                {
                    "sent": "Just want to mention it because it seems to be by far the most popular of our even to the extent I see many applications of the dealer process prior during the process.",
                    "label": 0
                },
                {
                    "sent": "Prior probability model on the random distribution, right?",
                    "label": 0
                },
                {
                    "sent": "But many applications don't make any use of the random distribution at all, or you don't see any inference on the random distribution or what people are concerned with is the probability model on the petition.",
                    "label": 0
                },
                {
                    "sent": "Which is this one here?",
                    "label": 0
                },
                {
                    "sent": "If you use the following PPF defaulting predictive probability function specified like the polaron here with these very specific.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probabilities, it turns out that is illegitimate PPS means that defines species sampling model and it actually happens to be the random partition that's implied by the configuration of ties in a random sample from a random probability measure with a dealership.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is prior.",
                    "label": 0
                },
                {
                    "sent": "So the dealership there Polaron which is the distribution on the random partition, happens to be a special case of a species sampling model.",
                    "label": 0
                },
                {
                    "sent": "It also happens to be a special case of a product partition.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model it also had.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Needs to be a special invalid, least limiting case of model based clustering.",
                    "label": 1
                },
                {
                    "sent": "Perhaps that's one of the reasons why it's so popular.",
                    "label": 0
                },
                {
                    "sent": "It keeps on dropping out a special case of just about everything.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "End of the footnote.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So this was a quick rundown of popular probability models for random partitions.",
                    "label": 0
                },
                {
                    "sent": "Remember again, once we started out with a promise to a probability model for random partitions.",
                    "label": 0
                },
                {
                    "sent": "It's index with covariates.",
                    "label": 0
                },
                {
                    "sent": "So now we have experimental units and each experimental unit is labeled with a covariate and they want to right around to invent.",
                    "label": 0
                },
                {
                    "sent": "I want to propose a probability model for random partitions, that is such that experimental units with similar covariates should be our priority before we see any response, our priority should be more likely to cluster together.",
                    "label": 0
                },
                {
                    "sent": "And here's how we're going to achieve that.",
                    "label": 0
                },
                {
                    "sent": "Will make use will assume that you are there.",
                    "label": 0
                },
                {
                    "sent": "I will assume that the city station can write down a similarity function.",
                    "label": 1
                },
                {
                    "sent": "I assume that you can tell me how similar is set of covariates.",
                    "label": 0
                },
                {
                    "sent": "Is record exchange star.",
                    "label": 0
                },
                {
                    "sent": "Where the covariates and appended chief cluster.",
                    "label": 0
                },
                {
                    "sent": "That similarity function should be such that it's not negative.",
                    "label": 0
                },
                {
                    "sent": "Greater equals 0 is good enough.",
                    "label": 0
                },
                {
                    "sent": "Lower values characterize bad clusters.",
                    "label": 1
                },
                {
                    "sent": "High values characterize good clusters.",
                    "label": 0
                },
                {
                    "sent": "For example, a cluster of all women with similar H and similar treatment history should get a very high score.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we are doing the following thing and it's embarrassing.",
                    "label": 0
                },
                {
                    "sent": "So embarrassingly simple.",
                    "label": 0
                },
                {
                    "sent": "You can fit it in one line.",
                    "label": 0
                },
                {
                    "sent": "All what we're doing is right on.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "The approach petition model in Black is again the PPM.",
                    "label": 0
                },
                {
                    "sent": "And in order to achieve the desired feature of increasing the probability of experimental units that are similar to Co cluster, we simply brutally Mike apply with that similarity function that we assume you have.",
                    "label": 0
                },
                {
                    "sent": "The clue is you're allowed to do that because the space here the.",
                    "label": 0
                },
                {
                    "sent": "Possible partitions.",
                    "label": 0
                },
                {
                    "sent": "They live on a finite discrete space, so you can multiply with anything non negative and you get the legitimate probability model, that's it.",
                    "label": 0
                },
                {
                    "sent": "So what we suggest to do is take the PPM and sneak in some pink yeah.",
                    "label": 0
                },
                {
                    "sent": "By integrating out variables that can still happen, changing the way here.",
                    "label": 0
                },
                {
                    "sent": "But if I write down arbitrary keys, do I still have an exchange for distribution by some structural properties?",
                    "label": 0
                },
                {
                    "sent": "Correct, this was a paid for question.",
                    "label": 0
                },
                {
                    "sent": "The answer is coming coming up here.",
                    "label": 0
                },
                {
                    "sent": "Quick answer, not necessarily yeah.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can actually spell out the normalization constant if you care just simply.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Decimation of our.",
                    "label": 0
                },
                {
                    "sent": "It's not always so obvious what you want to take as your similarity function.",
                    "label": 0
                },
                {
                    "sent": "Here's a default choice.",
                    "label": 0
                },
                {
                    "sent": "Default suggestion instead answers your question.",
                    "label": 0
                },
                {
                    "sent": "So under this default suggestion you get it in the next slide.",
                    "label": 0
                },
                {
                    "sent": "I'll essentially say that this default suggestion is not only convenient, but it's the only possible one if you want it convenient property.",
                    "label": 0
                },
                {
                    "sent": "So here's just a default construction of a similarity function.",
                    "label": 0
                },
                {
                    "sent": "Consider an auxiliary probability model auxiliary, because there's no notion of the axis being random variables.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but just takes a axes are continuous.",
                    "label": 0
                },
                {
                    "sent": "Take a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "If the X is a mighty warrior continuous, they can market better normal distribution with unknown moments, sigh, and then integrate Arctic side with respect to preferably a conjugate distribution.",
                    "label": 0
                },
                {
                    "sent": "So we can do the integral in closed form and what you get is what we call the similarity function.",
                    "label": 1
                },
                {
                    "sent": "So think of the X as Mike variant continuous.",
                    "label": 0
                },
                {
                    "sent": "Think of the blue guy as a multi variant normal with unknown moments.",
                    "label": 0
                },
                {
                    "sent": "The green guy conjugate prior on there might be very normal moments.",
                    "label": 0
                },
                {
                    "sent": "I can do the integral and what I get is something that has exactly the desired property to closer that those mighty variot continuous observations are packed together.",
                    "label": 0
                },
                {
                    "sent": "The higher the score.",
                    "label": 0
                },
                {
                    "sent": "If they are very spread out, then you'll get a very low score.",
                    "label": 0
                },
                {
                    "sent": "The advantage is simply this is something that is trivial to evaluate, like for any set of XJ star can evaluate it in closed form.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it satisfies the following conditions.",
                    "label": 0
                },
                {
                    "sent": "I already talked about it.",
                    "label": 1
                },
                {
                    "sent": "When you write down a proposal for a random partition, the first thing you want is invariant with respect to permutation of the observation in this is, so you want to have symmetry with respect to permutations of the indices.",
                    "label": 1
                },
                {
                    "sent": "That said.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only requirement you want, and here's another requirement that you might want.",
                    "label": 0
                },
                {
                    "sent": "If we consider the following property, if we are willing to consider the property that similarity average similarity.",
                    "label": 0
                },
                {
                    "sent": "Of a set of covariates X star plus one more.",
                    "label": 0
                },
                {
                    "sent": "Should be exactly the similarity of X star.",
                    "label": 0
                },
                {
                    "sent": "It's not obvious, by the way that you might want it, yeah, but it's convenient because it has a nice implication.",
                    "label": 0
                },
                {
                    "sent": "It's not obvious that you want it, but it is convenient.",
                    "label": 0
                },
                {
                    "sent": "You might already see that immediate leads to the following stay.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It meant that leads trivia later this statement that dissimilarity function must be of that formula, because essentially all of them seeing is it behaves like a probability model, it's exchangeable.",
                    "label": 0
                },
                {
                    "sent": "So if you believe in the two blue properties, then it must be of that form that they still called the default suggestion on the previous slide.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a nice implication if you have a similarity function of that form, then you also get coherence across sample size when you talk about coherence across sample size.",
                    "label": 1
                },
                {
                    "sent": "When you have a regression, when include covariates, we have to be a bit careful to say what we mean when there are no covariates when there are no covariates, then all we would want so without covariates.",
                    "label": 0
                },
                {
                    "sent": "If there were no access or what we would want is that summation, then we would just wanted the probability model for partitioning guys to be the probability model for partitioning N plus one guys.",
                    "label": 0
                },
                {
                    "sent": "Marginalizing with respect to the last one.",
                    "label": 0
                },
                {
                    "sent": "I mean, there are covariates.",
                    "label": 1
                },
                {
                    "sent": "Then I have to make some statement of how I get rid of the covariate behind the bar.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you do it like this then coherence across emphasized words.",
                    "label": 0
                },
                {
                    "sent": "So in plain English, I'm just saying there's a notion of coherence across sample size.",
                    "label": 0
                },
                {
                    "sent": "Partition.",
                    "label": 0
                },
                {
                    "sent": "Partition.",
                    "label": 0
                },
                {
                    "sent": "And there expecting role models like effects and is G is basically figuring out probability model correct?",
                    "label": 0
                },
                {
                    "sent": "Exactly correct?",
                    "label": 0
                },
                {
                    "sent": "There's a subtlety to it.",
                    "label": 0
                },
                {
                    "sent": "Subtlety is the two probability models have to be independent in the sense that parameters here use the parameter site Shafer.",
                    "label": 0
                },
                {
                    "sent": "The cluster specific parameter that defines the probability model on the axis and that attache that was indexing their probability model on device.",
                    "label": 0
                },
                {
                    "sent": "They have to be independent.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "That subtlety is important because it implies that the opposite is not necessarily true.",
                    "label": 0
                },
                {
                    "sent": "You can't OK.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you, if you believe in these two properties and the first one, the first one is a must, the second one, I confess, is a bit stupid artificial.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you are too kind to me, but if you were to press me on the 2nd one I couldn't argue it out easily.",
                    "label": 0
                },
                {
                    "sent": "It's not obvious that you wanted, but if we insist under 2 blue properties, then the default similarity function is actually only one and then life becomes easy, because now I can give you a whole list of similarity functions for common data formats.",
                    "label": 1
                },
                {
                    "sent": "Like what we usually have is you have a list of career, summer, continue, summer, categorical, some accounts like number of children.",
                    "label": 0
                },
                {
                    "sent": "In this particular example, summer binary.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Etc.",
                    "label": 0
                },
                {
                    "sent": "And two, the obvious one for continuous covariates.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use the normal.",
                    "label": 0
                },
                {
                    "sent": "I normally curiously for categorical.",
                    "label": 0
                },
                {
                    "sent": "For example, might be normal probit for ordinal personal game of accounts.",
                    "label": 0
                },
                {
                    "sent": "The only reason being technical convenience so we can evaluate that function in closed form.",
                    "label": 0
                },
                {
                    "sent": "So now we have similarity functions for any kind of common data format, we have a probability model.",
                    "label": 0
                },
                {
                    "sent": "We essentially done right.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And before showing you more examples, I want to talk you through some alternative constructions.",
                    "label": 1
                },
                {
                    "sent": "Or any comments complaints at this moment?",
                    "label": 0
                },
                {
                    "sent": "I wanted to should be because.",
                    "label": 0
                },
                {
                    "sent": "And there's a couple of I talked to prove.",
                    "label": 0
                },
                {
                    "sent": "I talked it through one way of setting that thing up.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's not a new problem.",
                    "label": 0
                },
                {
                    "sent": "I mean, people have been dealing with this problem for awhile.",
                    "label": 0
                },
                {
                    "sent": "What is usually done is the following trick.",
                    "label": 0
                },
                {
                    "sent": "And you essentially hinted at that what's usually done is replaced their response to original response.",
                    "label": 1
                },
                {
                    "sent": "Why replace it by an augmented response by X?",
                    "label": 1
                },
                {
                    "sent": "In other words, pretend and proceed as if the covariate were part of the response vector.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we could simply use for the desired prediction the conditional posterior predictive of YN plus one conditional on observing part of the response vector XN plus one is well defined and everything.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The catch, of course, is if we do that.",
                    "label": 0
                },
                {
                    "sent": "We are really using.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The wrong likelihood, because if we augment our response to that paravai X, then we have introduced an additional factor P of X given Theta, right?",
                    "label": 0
                },
                {
                    "sent": "And that's not as innocent as it might seem, because it's quite possible that device univariate and X might be high dimensional.",
                    "label": 0
                },
                {
                    "sent": "So in other words, that fake factor that suddenly arises and the likelihood might dominate in.",
                    "label": 0
                },
                {
                    "sent": "If the latter, the extra sampling model that we had in the original model, so it can be done, but it might be less innocent than what it seems, and to make matters worse, it's actually nothing easier.",
                    "label": 0
                },
                {
                    "sent": "I'll talk in a moment, I'll talk you through posterior.",
                    "label": 0
                },
                {
                    "sent": "Duration and posterior simulation of proposed model is not much more difficult either.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another common fix.",
                    "label": 0
                },
                {
                    "sent": "One way to achieve the desired random partition index with covariates, yeah?",
                    "label": 0
                },
                {
                    "sent": "So you can think of this is better.",
                    "label": 0
                },
                {
                    "sent": "It could be using the wrong likelihood, especially in the case when X is very high dimensional, but.",
                    "label": 1
                },
                {
                    "sent": "Isn't isn't the same problem going to happen if you have a G function, a similarity function that is defined through?",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                },
                {
                    "sent": "You know that.",
                    "label": 0
                },
                {
                    "sent": "Differences over the later, why don't see any difference between that problem here and probably there quick answer.",
                    "label": 0
                },
                {
                    "sent": "Essentially, essentially yes, but with the caveat.",
                    "label": 0
                },
                {
                    "sent": "Essentially yes in the sense.",
                    "label": 0
                },
                {
                    "sent": "We could simply can you repeat your comment in my words are kind of trick number.",
                    "label": 0
                },
                {
                    "sent": "One answer, different question can't we?",
                    "label": 0
                },
                {
                    "sent": "Can't we called it P of X given Theta concrete just called it our similarity function?",
                    "label": 0
                },
                {
                    "sent": "Essentially yes, but the big difference.",
                    "label": 0
                },
                {
                    "sent": "If the big difference is if you called it the similarity function, then that at a that was indexing the sampling model for the original responses suddenly floats around in my similarity function and that can mess up things.",
                    "label": 0
                },
                {
                    "sent": "Of course you can set it up carefully in such a way that model 4X only depends on a subvector of the data which does not arise in the sampling model.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, then you're exactly back to what we have.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, that is why I said yes, but the caveat is if you do that, you have to be a bit careful.",
                    "label": 0
                },
                {
                    "sent": "The other caveat is if the X is only univariate and you kind of understand what you're doing, I think you can get away with it.",
                    "label": 0
                },
                {
                    "sent": "The trick has been a similar trick has been used in a paper by Robert Connan Wong and others for inference on random correlation matrices.",
                    "label": 0
                },
                {
                    "sent": "When you have a random covariance matrix covariance matrix, it better be positive definite.",
                    "label": 0
                },
                {
                    "sent": "So what was there?",
                    "label": 0
                },
                {
                    "sent": "This is somehow somebody ran into the similar trick intended, just called.",
                    "label": 1
                },
                {
                    "sent": "What they did was they called extra factor that sneaks in recorded extra factor part of the prior oops.",
                    "label": 0
                },
                {
                    "sent": "So instead of calling it part of the similarity function, you could call it part of the prior, which means you change your prior.",
                    "label": 0
                },
                {
                    "sent": "But you could do so in other ways.",
                    "label": 0
                },
                {
                    "sent": "So other default trick, sneak the regression covariates into a mixture model.",
                    "label": 1
                },
                {
                    "sent": "Nothing stops you from record a mixture model that I told you any very big.",
                    "label": 0
                },
                {
                    "sent": "Our price of Chivas debate of the Chief German.",
                    "label": 0
                },
                {
                    "sent": "The mixture you can make that depend on covariance.",
                    "label": 0
                },
                {
                    "sent": "For example with logistic.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And nothing wrong in doing so, but in doing so you would probably need like parameters.",
                    "label": 0
                },
                {
                    "sent": "For example if that were.",
                    "label": 0
                },
                {
                    "sent": "If there might be normal logistic, then Anita logistic regression Param.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Letters and then you have a fixed sample size.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "SK of course you can.",
                    "label": 0
                },
                {
                    "sent": "You can work around it by considering limit K to Infinity, and if you do that in a careful way at then end up exactly back to the PPM with the extra factor it ahead.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And last not least.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "We could simply do the following brutal things and.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It becomes really brutal.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we could take the PPF.",
                    "label": 0
                },
                {
                    "sent": "So what you have here in Black is again the PPF from the polaron and you could just stay at their PPF from the polaron and we could modify it with the traditional red factor in such a way that a new woman.",
                    "label": 0
                },
                {
                    "sent": "It's more likely to join a cluster of women who have similar covariates.",
                    "label": 0
                },
                {
                    "sent": "Nothing stops you from doing that except what you might be playing havoc.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's getting back to your comment.",
                    "label": 0
                },
                {
                    "sent": "I mean, they do such a thing.",
                    "label": 0
                },
                {
                    "sent": "It's not even clear anymore that The thing is exchangeable.",
                    "label": 0
                },
                {
                    "sent": "In other words, it's not clear that these are legitimate conditioners to an underlying joint that is symmetric with respect to permutations of the indices.",
                    "label": 0
                },
                {
                    "sent": "That said, sometime.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's simply you're not allowed to do that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but that said.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I. I have seen it work fine in X.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data analysis applications, yeah.",
                    "label": 0
                },
                {
                    "sent": "No contradiction.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sometimes doing something reasonable, even if it's not coherent, might still give you decent inference.",
                    "label": 0
                },
                {
                    "sent": "I was waiting for complaints and I could have said next slide answer it, but you let me get away with everything.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "5 minutes.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Our posterior inference for random partitions is not trivial, so even let's step back like forget for a moment.",
                    "label": 1
                },
                {
                    "sent": "Indexing with covariates, just posterior inference for random clustering is not easy because it's a finite discrete space, but it's an awfully big one.",
                    "label": 0
                },
                {
                    "sent": "There's no notion of a posterior means such a thing doesn't exist.",
                    "label": 0
                },
                {
                    "sent": "Even the posterior mode, even if you were able to find the posterior mode, it's usually not very.",
                    "label": 0
                },
                {
                    "sent": "It's not a very useful summary because the posterior mode might have posterior probability 1.2.",
                    "label": 0
                },
                {
                    "sent": "And the second best partition might have posterior probability 1.05%.",
                    "label": 0
                },
                {
                    "sent": "So the posterior mode might be nothing particularly.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eristic, so there's all.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kinds of suggestions of how to proceed.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even without covariates.",
                    "label": 0
                },
                {
                    "sent": "And here is there are two.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Online summary of how we do it with the code in the augmented model.",
                    "label": 0
                },
                {
                    "sent": "Quick answer, you do proceed exactly the same way, essentially after a trivial model augmentation.",
                    "label": 1
                },
                {
                    "sent": "Computationally proposed model reduces exactly.",
                    "label": 1
                },
                {
                    "sent": "Computation reduces exactly the posterior simulation in a random partition model without covariates.",
                    "label": 0
                },
                {
                    "sent": "The trick is essentially a recognized the similarity function as part of a big joint probability model, which has a special structure, but the structure is already there.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you can proceed.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I just spelled out the details.",
                    "label": 0
                },
                {
                    "sent": "And there's nothing magic about it.",
                    "label": 0
                },
                {
                    "sent": "The opposite, by the way, is not true.",
                    "label": 1
                },
                {
                    "sent": "The model with a combined response cannot necessarily be interpreted as the proposed model, but the proposed model can be interpreted as a special case of a model with a combined risk.",
                    "label": 1
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jones and that gets us back to the original example.",
                    "label": 0
                },
                {
                    "sent": "So remember again, we had N patients.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In that trial.",
                    "label": 0
                },
                {
                    "sent": "And we are now and there is 6 responses at each given time.",
                    "label": 0
                },
                {
                    "sent": "Or in other words, the response by some I is a 6 dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hector and we want to make inferences about the N plus first patient and the model that we use is exactly the proposed model.",
                    "label": 0
                },
                {
                    "sent": "The only covariate that we're going to use here is treatment tools.",
                    "label": 0
                },
                {
                    "sent": "So treatment tools there's certainly nothing random about it.",
                    "label": 0
                },
                {
                    "sent": "You get to set.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And sampling model is Manila.",
                    "label": 0
                },
                {
                    "sent": "Michael Berry.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Normal and every.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using default.",
                    "label": 0
                },
                {
                    "sent": "Here's the data.",
                    "label": 0
                },
                {
                    "sent": "This is the data for the observed 47 patients.",
                    "label": 0
                },
                {
                    "sent": "Each of these spaghetti is 1 patient.",
                    "label": 0
                },
                {
                    "sent": "The six responses, actually six blood counts at 6 times during the chemotherapy, so it makes sense to connect them.",
                    "label": 0
                },
                {
                    "sent": "So each of those spaghettis is 1 patient.",
                    "label": 0
                },
                {
                    "sent": "And what we do now is to predict for the N plus first patient we look at earlier patients with similar tools.",
                    "label": 0
                },
                {
                    "sent": "And will do the most simple thing possible.",
                    "label": 0
                },
                {
                    "sent": "My prediction for their response for a new patient is the average of the spaghettis of women who were treated with a similar dose earlier.",
                    "label": 0
                },
                {
                    "sent": "That's it, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here it is or just to drive the message home on the right hand side I do the same exercise using a probability model on the partition but not indexed with covariate and into the prediction since it's not index with covariates.",
                    "label": 0
                },
                {
                    "sent": "Of course the petition the prediction is the same no matter what the covariate is.",
                    "label": 0
                },
                {
                    "sent": "Verify indexing with covariates, they get different prediction depending on about the treatment level is.",
                    "label": 0
                },
                {
                    "sent": "So these six different predictions.",
                    "label": 0
                },
                {
                    "sent": "A quote for six different levels of the treatment doors.",
                    "label": 0
                },
                {
                    "sent": "And each of these, each of these predictions is simply the average.",
                    "label": 0
                },
                {
                    "sent": "Over classes of earlier observed patients were clusters of women with similar treatment tools.",
                    "label": 0
                },
                {
                    "sent": "As the new patient ovated.",
                    "label": 0
                },
                {
                    "sent": "Higher ten classes of women with very different treatment.",
                    "label": 0
                },
                {
                    "sent": "Those kind of the obvious thing to do.",
                    "label": 0
                },
                {
                    "sent": "This model.",
                    "label": 0
                },
                {
                    "sent": "Do you understand?",
                    "label": 0
                },
                {
                    "sent": "Start attending random.",
                    "label": 0
                },
                {
                    "sent": "No filtering.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Good comment, quick answer, nor long answer may be slightly different question if you were to if all patients had exactly the same covariate value, that's kind of the extreme version of what you're telling, right?",
                    "label": 0
                },
                {
                    "sent": "If all patients are exactly the same covariate value, wouldn't it be nice if the model would exactly revert to the North covariate model?",
                    "label": 0
                },
                {
                    "sent": "Strictly, the answer is no.",
                    "label": 0
                },
                {
                    "sent": "Approximately the answer is yes.",
                    "label": 0
                },
                {
                    "sent": "Ideally I would like, ideally, of course we would love to have the answer to be yes, we would love to have the situation that when all the covariates are the same, I want to see I want to see exactly the inference that would have without covariate.",
                    "label": 0
                },
                {
                    "sent": "That's not quite true because the similarity function, the reason being the similarity function, implicitly introduces a penalty for cluster size.",
                    "label": 0
                },
                {
                    "sent": "And getting rid of that is more complicated than what The thing is we have here.",
                    "label": 0
                },
                {
                    "sent": "But approximately, it's true.",
                    "label": 0
                },
                {
                    "sent": "In the Fed it's true.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it gets.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into a second example, if I'm still a lot.",
                    "label": 0
                },
                {
                    "sent": "A longer list of covariates, so now it was six covariates.",
                    "label": 0
                },
                {
                    "sent": "Mixture of categorical count and continuous outcome in this case is an event time.",
                    "label": 0
                },
                {
                    "sent": "Time to progression, nothing special about censoring.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, you can beautifully get an inference.",
                    "label": 0
                },
                {
                    "sent": "This is not inference for a future observation arranged by different levels of the covariates.",
                    "label": 0
                },
                {
                    "sent": "I just picked here three different covariates, initial tumor size, high versus low dose of the treatment here, positive versus not your positive, and for each of these different combinations you see a different prediction.",
                    "label": 0
                },
                {
                    "sent": "This being event time data to predict prediction is shown in terms of survival function and summarizes it and drops off natural.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that gets me to the final slide.",
                    "label": 0
                },
                {
                    "sent": "Just telling you that we did.",
                    "label": 0
                },
                {
                    "sent": "We proposed the probability model for random partitions that had the additional twist.",
                    "label": 0
                },
                {
                    "sent": "Additional feature that the model is indexed with covariates, like we're doing regression where the outcome happens to be random partition.",
                    "label": 0
                },
                {
                    "sent": "Posterior inference straightforward.",
                    "label": 0
                },
                {
                    "sent": "There are all kinds of practical problems like labor switching problem etc.",
                    "label": 0
                },
                {
                    "sent": "When I showed you the pictures, we did something reasonable.",
                    "label": 0
                },
                {
                    "sent": "Our talk to deal with that.",
                    "label": 0
                },
                {
                    "sent": "Similar issue called Subspace Clustering, abit similar but not quite the same and that's it, thank you.",
                    "label": 0
                }
            ]
        }
    }
}