{
    "id": "pbrl5lbd5vi4qop3spclwfqhwgp22uq4",
    "title": "Using Ontology-based Data Summarization to Develop Semantics-aware Recommender Systems",
    "info": {
        "author": [
            "Matteo Palmonari, University of Milano - Bicocca"
        ],
        "published": "July 10, 2018",
        "recorded": "June 2018",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2018_palmonari_recommender_systems/",
    "segmentation": [
        [
            "OK, so thank you very much.",
            "So this is a joint work between like two groups.",
            "We work on these datasets as summarization tools and we teamed up with our colleagues in Barry that work on semantics Aware recommender system.",
            "So I will.",
            "Briefly."
        ],
        [
            "Discuss their motivation for our work."
        ],
        [
            "Which is basically to solve the problem of feature selection for semantics Aware recommended system.",
            "Then I will quickly go through the basics of our approach to provide the data summarization with our tool upset and then I will discuss how we perform feature selection with our tool and state of the art measures, then some expn."
        ],
        [
            "Romance and conclusion and future work, so I guess that I don't have to explain what recommender systems are 'cause everybody use them and they know what they are.",
            "Well, what would be more interesting?"
        ],
        [
            "Is to discuss a bit further why we need content in recommender systems.",
            "So still state of the art approaches are heavily based on collaborative filtering approach.",
            "Is an matrix factorization.",
            "So we're basically you recommend the items based on the ratings that other users similar to you gave to the same item."
        ],
        [
            "But there are still some problems that are worth tackling.",
            "Anwer semantics can play an interesting role.",
            "So for example, if we consider like this metrics with users, an item to be recommended.",
            "In this case movies we may have problems with the sparsity of the metrics.",
            "Second problem is the new item problem.",
            "So there are like new items that had no ratings.",
            "So you want to use the contents to understand if these.",
            "A new movie is relevant for some."
        ],
        [
            "User and third problem is the explanation problem.",
            "So some of these approaches used collaborative filtering for recommendation.",
            "But then maybe content can be used to explain the.",
            "Recommended items OK, so these are examples of three problems that we may tick up and."
        ],
        [
            "Why we would like to?"
        ],
        [
            "Consider content where basically be cause when we recommend some items.",
            "We may want to recommend items that are similar to the items that have been liked by the users.",
            "OK, so the overall idea here is that semantics and of course like knowledge graphs can provide means to compute and measure the similarity between the items, right?",
            "So this is so, for example."
        ],
        [
            "This is quite a state of the art approach for recommended the system.",
            "We can use a basic item KNN recommender system.",
            "So the main idea is that we want to predict the rating given by given to items that are not being rated by the users, right?",
            "And to do so?",
            "So basically we can keep the items that have been.",
            "Rated an we can compute the most similar items to the ones that have been rated right with the similarity measure and then we can basically predict the rating by averaging the rating of the items that are similar to the new item to be recommended.",
            "OK so basically here we have the rating a given to known items and we waited these ratings by this similarity measure.",
            "And BM and more than one measures have been proposed to for content based recommender system.",
            "If we focus on the ones based on knowledge graphs 'cause there are also other approaches to semantic based content based recommendation then few of the measures that have been proposed are the Jacquard similarity, graph kernels, cosine similarity in a vector space and Suzanne several varients of the similarity.",
            "But here the main point is how to compute this similarity OK?",
            "And a very point is that when you want to use the Knowledge Graph to compute the similarity, items are described with a large number of properties and the key problem is which property to consider when computing the similarity.",
            "Cause all of these approaches are based on a sub graph of the whole graph when computing the similarity right.",
            "So we come to."
        ],
        [
            "Our problem.",
            "Which is the feature selection problem in recommender system as also known in other machine learning problems, right?",
            "So to select the features that are most representative to compute this similarity and the problem is not easy because there are several different features like ontological properties, categorical properties, other most frequent properties.",
            "So there are several heuristics that are not so easy to.",
            "I meant in practice OK.",
            "So in previous work an here it can be seen that just adding all the possible feature is known to be not a useful approach.",
            "OK, not for the curacy and of course also not computationally speaking for the efficiency of the algorithm.",
            "So basically most of the work performed these feature selection manually, which means basically that you try several.",
            "Approaches and then you conduct your experiment and that you find out which are the set of features that perform better.",
            "Recent approach also proposed to use in this field well known statistical measures right.",
            "For example, information gain information gain ratio, which were which were proved to provide some help in this feature selection approach and what we're doing in this paper.",
            "What's the main goal of this paper is to investigate if we can use ontology based knowledge graph.",
            "Summaries in order to find out which are the most important properties to use, the one evaluating the similarity between the items right?",
            "And we have a previous work on that where we use the we conducted experiments with graph kernels.",
            "But in that approach we still add some manual preprocessing step.",
            "OK, so we use the SAMA statistics in the summary but still had a manual preprocessing step.",
            "In this work we want to go really for a fully.",
            "Automatic approach to feature selection based on the statistics of the summer.",
            "What would be the advantage?"
        ],
        [
            "Of such an approach, right?",
            "So if you think about computing statistical measures, then you would be required to download the full data set right?",
            "For example, we used to hear the DB pedia with infoboxes, and it's more than 300 million triples.",
            "And then you need to compute this statistical measure over the full data set and then you may discard the information that you don't need and then you run the algorithm.",
            "The key idea of profiles or summaries is that they can be accessed in a efficient way on the web.",
            "So basically you can ask to the profile to give you the the top K most useful properties for the similarity, for example via an API.",
            "Then you only download the relevant data for your problem and then you run the algorithm."
        ],
        [
            "OK, so very quickly."
        ],
        [
            "On the basics of the data summarization approach that we use here, which is based on our tool upset so we prefer to call in now.",
            "Like knowledge graph profiling tool cause there's bit more than summaries here.",
            "Basically the primitive of our tool are these patterns.",
            "These schema patterns, type property type and the idea is that these Potter represent that there are instances of.",
            "These Co type that are connected to instances or in this case literals of your type through these funding year property OK. An why we called these ontology driven profiling becausw.",
            "Basically we can use the ontology if you also use the ontology as input for this summarization to extract only the most specific patterns that you can find from the triples.",
            "OK by using only the most specific dummy."
        ],
        [
            "Email type associated with this entity.",
            "Then of course the real value come from this statistics, so we count how frequent these patterns are, and more recently we have added these new features exactly for this work, where we started to try to understand how these properties this relation are shaped in terms of cardinality, introducing cardinality, descrip?"
        ],
        [
            "So basically the main idea of cardinality descriptors is to count for all the relations that are represented by a pattern, right?",
            "The number of distinct subjects associated with unique objects in the triples represented by the pattern.",
            "OK, so."
        ],
        [
            "We compute tree statistics.",
            "The minimal the minimum number."
        ],
        [
            "Of distinct subjects, like in this case, the maximum number of distinct subjects, like in this case and then of."
        ],
        [
            "First, the average, and we represent the average.",
            "We did discrete number and we do."
        ],
        [
            "Same thing, although less relevant to our talk.",
            "Also, on the other side, like so for the distinct objects."
        ],
        [
            "Associated with unique subjects."
        ],
        [
            "We compute."
        ],
        [
            "The same."
        ],
        [
            "Statistics it's important to notice that we compute these at the pattern level or OK, and we found that the cardinality at the pattern level differ from pattern to pattern, so they are different from this cardinality.",
            "If we only consider the property.",
            "So what's the?"
        ],
        [
            "Intuition here why we thought that this cardinality descriptors could be useful to automate the features selection?",
            "Well, the main idea here is that."
        ],
        [
            "Those.",
            "Properties that map distinct subjects to unique objects may be useful for similarity.",
            "Whi becausw here if you think about the Jaccard and we will see later here you have a common feature for these two objects.",
            "OK so this is the intuition and This is why the statistics that we consider in our work are only the 1 four distinct subjects.",
            "Associated with unique object, in particular, the average S which is the average number of subjects associated to distinct objects and maximum S."
        ],
        [
            "OK, so how can we do feature selection?",
            "You are using this information.",
            "Basically, we have this pipeline, so we take the set of patterns.",
            "We have an optional filtering phase where we filter out patterns that do not comply with some criteria.",
            "So here for example we filter out all the patterns that have a average number of distinct subjects that is minor than one.",
            "Then we have the projection right?",
            "So we filter out some pattern.",
            "We project the information that here."
        ],
        [
            "We added the pattern level on the property right?",
            "So we basically take for example the maximum frequency of all the pattern that we have for that property, and we assign that Max frequency to the property.",
            "And finally we can provide some ranking by using these criteria.",
            "In this case, for example by frequency.",
            "So we can order properties by frequency and then we select the best properties.",
            "Best top K properties.",
            "Of course this pipeline can be configured like in a different way by using for the values of partner frequency, local Cardinal carnality descriptors.",
            "Orca"
        ],
        [
            "Bination of both.",
            "So we will see that now in our experiment we will also try several alternative implementation of this general pipeline."
        ],
        [
            "So OK, so back to statistical measure.",
            "For feature selection.",
            "We tried several of them like information gain, information, gain ratio and chi squared tests and we found out that information gain was the one that was performing better, so I won't go into the detail of information gain information game.",
            "Basically measure the expected their reduction in entropy or curing when a feature is present versus when is not present.",
            "So the principle is quite similar.",
            "Through the idea on the cardinality, but it's computed at the final level, so on the data itself, not on the summaries.",
            "Now here one."
        ],
        [
            "Problem is that computing those statistical measures is also computationally very expensive, so we needed some manual preprocessing when using these statistical measure.",
            "So we reduced the old set of properties to a much smaller number and then we computed the information gain.",
            "Just considering those properties.",
            "OK, so."
        ],
        [
            "I will try to describe the experiments that that we did, so as in the introduction we used an item based the nearest neighbor halga rhythm for recommendations, which is quite standard in this domain.",
            "So basically it's the formula that we have seen before.",
            "But here we use."
        ],
        [
            "Chuck car similarity."
        ],
        [
            "Similarity measure which measures the values of the selected feature that are shared between two items right?",
            "We tried several experimental setting says data set.",
            "We use benchmarking the recommender system, a domain movie and so last FM librarything and a one to one mapping to DB pedia for the entities and then do."
        ],
        [
            "The pedia to extract the information.",
            "Different kind of metrics to for accuracy.",
            "Precision attendant me reciprocal rank an we also wanted to consider diversity with two measures, catalog coverage and aggregate diversity."
        ],
        [
            "There are more parameters that we needed that too considering the experiment.",
            "So for example, the number of features that we consider in the recommendation.",
            "So 5 and 20.",
            "And then since we use the whole DB pedia with infoboxes, then it happened that we have summed up some duplicate properties.",
            "So we have different strategies to resolve these duplicates.",
            "Just keep only DB pedia property.",
            "Keep all the DBO use edge.",
            "Using the duplicates or avoiding duplicates, you and you can find the details in the paper.",
            "So here we tried several configuration for upset.",
            "These are the ones that proved to be most effective by filtering by average ASAN ranking by frequency.",
            "Another two where we didn't filter but we combine frequency and access for ranking and finally just maccess.",
            "And finally we have these.",
            "TF IDF based language is still based on abstract, but it's based on computing the TF IDF form patterns.",
            "OK, so This is why."
        ],
        [
            "Diversity.",
            "It's also important, so we really want to have an evaluation across measures that capture different aspects of the recommendation."
        ],
        [
            "Algorithm, sorry I was given back OK so here the experimental results are.",
            "Basically we found that that for the movie lens data set, abstract based features selection is almost always statistically better than statistical measures.",
            "With few local exceptions.",
            "Although different configurations optimize the different measures, right?",
            "TF IDF obstat based.",
            "I need this instead.",
            "Very good for."
        ],
        [
            "For aggregate entropy for one of these, measure in this case.",
            "In last FM we have results.",
            "Less strong, as in the previous data set, but here we we find that upset based features.",
            "Selection is usually better or almost comperable to information gain, but in some of these cases we also have like the statistical significance, but remind that you still have an advantage by the summaries.",
            "If you have comperable results because you don't need to compute the statistics on the full data set.",
            "OK, it's more efficient and also hear the TF IDF baseline is good for aggregate entropy Ann."
        ],
        [
            "Here we have some more jeopardize results.",
            "Information gain is better for catalog coverage with 20 features is better for some duplicate property management.",
            "I don't have idea what happened actually, Luckily.",
            "I think that just stopped working.",
            "OK, thank you very much.",
            "OK so here we have that.",
            "In some cases information gain is better, but again the differences are not really too big.",
            "OK so we have to go to the 3rd digit to find them and again."
        ],
        [
            "In the TDF."
        ],
        [
            "Absolute data baseline is good for aggregate entropy.",
            "So to conclude.",
            "We provide an approach to provide a fully automatic feature selection method with ontology based knowledge graph summaries using apps that their results with this approach are better or in some cases comperable to statistical measure, which is interesting because summaries were not introduced for these scope but were introduced to support data understanding.",
            "And here we are finding that this is.",
            "I think it's an additional evidence for the.",
            "Usefulness of these are knowledge graph summarization approaches.",
            "Also for data analytics task as a future work we would.",
            "We think that maybe we could add the TF IDF in apps that statistics because it proved to be useful at least for one measure.",
            "We would like to carry out experiments with some additional measures.",
            "So now we tried with graph kernels and Jack are that we have still a couple of measures to evaluate with this.",
            "And finally, we are planning to include the in the API of abstract a service that, given an input type, it gives you the top most salient features for that type.",
            "Based on these pipelines, which can be used online to determine the best feature for evaluating the similarity.",
            "So that was the last slide.",
            "Thank you very much for attention, any question.",
            "Are you introduced this notion of patterns?",
            "Can you give a little bit more insight about nothing?",
            "That was very fast.",
            "That part of the talk and does this relate to ontology design patterns in any way?",
            "So yes, we were also inspired by ontology design patterns.",
            "Actually, technically speaking, so I didn't focus too much because I had the demo this morning about this and."
        ],
        [
            "And so coming back here, these patterns basically provided an X Rays or what exists in the knowledge base.",
            "So logically speaking at these are existential axioms that tells you whatever exists in the knowledge base.",
            "So there are complete right for each relation you have representative patterns and then here you have those statistics that tell you how frequent they are.",
            "So I think that yes, for example, for ontology design pattern, this could be useful because it can tell you how the vocabulary is used in a knowledge graph.",
            "Write an which frequency feature, which is OK.",
            "But also these descriptors I think gives an idea of patterns that you can find.",
            "OK, so actually this reminds me a little bit of this and encyclopedic knowledge patterns work from all those group, yet they learn this pattern.",
            "So you seem to do something similar.",
            "Yeah, in fact they did something similar for Wikipedia, right?",
            "So they extracted something similar to Wikipedia because I remember that all too old and he said we did something similar.",
            "So what we did here is to build a tool that can compute these on big knowledge graph.",
            "So for example, here we use the the in this experiment.",
            "I just want to show that we used at the DB pedia plus the infoboxes which has 392 million triples and 55,000 properties.",
            "OK and and and to me this is important because in DP you have the DP ontology but you don't know which are the properties in the infoboxes right and this gives you this and of course this is publicly available.",
            "You can browse and see if it's.",
            "Useful for OK.",
            "Thank you.",
            "It was a big question.",
            "Could you give us some insights like on how often it was data type properties or object properties that were actually like the features in the summaries and that made.",
            "Yes, OK so.",
            "Data OK. Like literals like matching dates or.",
            "I wish I had the imager with the different rankings that we produce path.",
            "Unfortunately I don't have so I I can do do this offline.",
            "So basically, like for example, one of the very important feature here was director.",
            "Of course starting which are object property subject which is another very important, not ontological property, but very useful here.",
            "So it was a really a mix of the two an if you look at these so keep in mind that here when you have like that DB pedia property, you favor the like shallow semantics in the Knowledge Graph.",
            "And sometimes you get better result with DPD and sometimes with that.",
            "So it's that's a bit unpredictable if you want.",
            "But I can give you the rankings.",
            "OK, so we have the media, the image.",
            "I don't have it here.",
            "I should have added thanks.",
            "OK thanks.",
            "I have one more.",
            "OK, actually there was this slide where you you showed your experimental data and that you used free datasets.",
            "I think that you linked to DB Pedia.",
            "Yes yes.",
            "How?",
            "How did you establish this links?",
            "What was the method that you know you have to ask it to?",
            "To my to Thomas who is there because he did it this mapping.",
            "Why do you hide?",
            "He didn't want to.",
            "Yeah, well we use the same automated process.",
            "We basically took the names of the movies or books and the like and then we used services just like DB pedia.",
            "Look up or other.",
            "Well, we were not able to find the.",
            "Correct mapping with DP to look at and we you know we use some other sparkle queries and then we manually check everything in the light so it was quite long.",
            "You know work, but in the end we think that it is quite accurate, yeah?",
            "How about frameworks like silk?",
            "Another linking from?",
            "Yeah, well, I mean we also used sealed, you know we used a lot of tools actually.",
            "And yeah we use that kind of pipeline you know to find this this mappings.",
            "But in the end it was just, you know.",
            "I don't want to say kind of string matching, but it is something like that because the name of the movies is very often exactly the same way that we can find.",
            "You know in DB pedia.",
            "I have to say that the hardest part of this work with the manual check and you know the minor correction of.",
            "Of items that were not correctly met.",
            "And who did you ask to check this for three experts?",
            "Or you know crowdsourcing?",
            "What kind of methods, like how did you involve the human annotators?",
            "I have great students.",
            "OK, that clears everything."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So this is a joint work between like two groups.",
                    "label": 0
                },
                {
                    "sent": "We work on these datasets as summarization tools and we teamed up with our colleagues in Barry that work on semantics Aware recommender system.",
                    "label": 0
                },
                {
                    "sent": "So I will.",
                    "label": 0
                },
                {
                    "sent": "Briefly.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Discuss their motivation for our work.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is basically to solve the problem of feature selection for semantics Aware recommended system.",
                    "label": 0
                },
                {
                    "sent": "Then I will quickly go through the basics of our approach to provide the data summarization with our tool upset and then I will discuss how we perform feature selection with our tool and state of the art measures, then some expn.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Romance and conclusion and future work, so I guess that I don't have to explain what recommender systems are 'cause everybody use them and they know what they are.",
                    "label": 0
                },
                {
                    "sent": "Well, what would be more interesting?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is to discuss a bit further why we need content in recommender systems.",
                    "label": 1
                },
                {
                    "sent": "So still state of the art approaches are heavily based on collaborative filtering approach.",
                    "label": 1
                },
                {
                    "sent": "Is an matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "So we're basically you recommend the items based on the ratings that other users similar to you gave to the same item.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there are still some problems that are worth tackling.",
                    "label": 0
                },
                {
                    "sent": "Anwer semantics can play an interesting role.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we consider like this metrics with users, an item to be recommended.",
                    "label": 0
                },
                {
                    "sent": "In this case movies we may have problems with the sparsity of the metrics.",
                    "label": 0
                },
                {
                    "sent": "Second problem is the new item problem.",
                    "label": 0
                },
                {
                    "sent": "So there are like new items that had no ratings.",
                    "label": 0
                },
                {
                    "sent": "So you want to use the contents to understand if these.",
                    "label": 0
                },
                {
                    "sent": "A new movie is relevant for some.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "User and third problem is the explanation problem.",
                    "label": 0
                },
                {
                    "sent": "So some of these approaches used collaborative filtering for recommendation.",
                    "label": 1
                },
                {
                    "sent": "But then maybe content can be used to explain the.",
                    "label": 0
                },
                {
                    "sent": "Recommended items OK, so these are examples of three problems that we may tick up and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why we would like to?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consider content where basically be cause when we recommend some items.",
                    "label": 0
                },
                {
                    "sent": "We may want to recommend items that are similar to the items that have been liked by the users.",
                    "label": 0
                },
                {
                    "sent": "OK, so the overall idea here is that semantics and of course like knowledge graphs can provide means to compute and measure the similarity between the items, right?",
                    "label": 0
                },
                {
                    "sent": "So this is so, for example.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is quite a state of the art approach for recommended the system.",
                    "label": 0
                },
                {
                    "sent": "We can use a basic item KNN recommender system.",
                    "label": 1
                },
                {
                    "sent": "So the main idea is that we want to predict the rating given by given to items that are not being rated by the users, right?",
                    "label": 0
                },
                {
                    "sent": "And to do so?",
                    "label": 0
                },
                {
                    "sent": "So basically we can keep the items that have been.",
                    "label": 0
                },
                {
                    "sent": "Rated an we can compute the most similar items to the ones that have been rated right with the similarity measure and then we can basically predict the rating by averaging the rating of the items that are similar to the new item to be recommended.",
                    "label": 0
                },
                {
                    "sent": "OK so basically here we have the rating a given to known items and we waited these ratings by this similarity measure.",
                    "label": 0
                },
                {
                    "sent": "And BM and more than one measures have been proposed to for content based recommender system.",
                    "label": 0
                },
                {
                    "sent": "If we focus on the ones based on knowledge graphs 'cause there are also other approaches to semantic based content based recommendation then few of the measures that have been proposed are the Jacquard similarity, graph kernels, cosine similarity in a vector space and Suzanne several varients of the similarity.",
                    "label": 1
                },
                {
                    "sent": "But here the main point is how to compute this similarity OK?",
                    "label": 0
                },
                {
                    "sent": "And a very point is that when you want to use the Knowledge Graph to compute the similarity, items are described with a large number of properties and the key problem is which property to consider when computing the similarity.",
                    "label": 0
                },
                {
                    "sent": "Cause all of these approaches are based on a sub graph of the whole graph when computing the similarity right.",
                    "label": 0
                },
                {
                    "sent": "So we come to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our problem.",
                    "label": 0
                },
                {
                    "sent": "Which is the feature selection problem in recommender system as also known in other machine learning problems, right?",
                    "label": 0
                },
                {
                    "sent": "So to select the features that are most representative to compute this similarity and the problem is not easy because there are several different features like ontological properties, categorical properties, other most frequent properties.",
                    "label": 0
                },
                {
                    "sent": "So there are several heuristics that are not so easy to.",
                    "label": 0
                },
                {
                    "sent": "I meant in practice OK.",
                    "label": 0
                },
                {
                    "sent": "So in previous work an here it can be seen that just adding all the possible feature is known to be not a useful approach.",
                    "label": 0
                },
                {
                    "sent": "OK, not for the curacy and of course also not computationally speaking for the efficiency of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So basically most of the work performed these feature selection manually, which means basically that you try several.",
                    "label": 0
                },
                {
                    "sent": "Approaches and then you conduct your experiment and that you find out which are the set of features that perform better.",
                    "label": 0
                },
                {
                    "sent": "Recent approach also proposed to use in this field well known statistical measures right.",
                    "label": 0
                },
                {
                    "sent": "For example, information gain information gain ratio, which were which were proved to provide some help in this feature selection approach and what we're doing in this paper.",
                    "label": 0
                },
                {
                    "sent": "What's the main goal of this paper is to investigate if we can use ontology based knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "Summaries in order to find out which are the most important properties to use, the one evaluating the similarity between the items right?",
                    "label": 0
                },
                {
                    "sent": "And we have a previous work on that where we use the we conducted experiments with graph kernels.",
                    "label": 1
                },
                {
                    "sent": "But in that approach we still add some manual preprocessing step.",
                    "label": 0
                },
                {
                    "sent": "OK, so we use the SAMA statistics in the summary but still had a manual preprocessing step.",
                    "label": 0
                },
                {
                    "sent": "In this work we want to go really for a fully.",
                    "label": 0
                },
                {
                    "sent": "Automatic approach to feature selection based on the statistics of the summer.",
                    "label": 1
                },
                {
                    "sent": "What would be the advantage?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of such an approach, right?",
                    "label": 0
                },
                {
                    "sent": "So if you think about computing statistical measures, then you would be required to download the full data set right?",
                    "label": 0
                },
                {
                    "sent": "For example, we used to hear the DB pedia with infoboxes, and it's more than 300 million triples.",
                    "label": 0
                },
                {
                    "sent": "And then you need to compute this statistical measure over the full data set and then you may discard the information that you don't need and then you run the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The key idea of profiles or summaries is that they can be accessed in a efficient way on the web.",
                    "label": 0
                },
                {
                    "sent": "So basically you can ask to the profile to give you the the top K most useful properties for the similarity, for example via an API.",
                    "label": 0
                },
                {
                    "sent": "Then you only download the relevant data for your problem and then you run the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so very quickly.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the basics of the data summarization approach that we use here, which is based on our tool upset so we prefer to call in now.",
                    "label": 0
                },
                {
                    "sent": "Like knowledge graph profiling tool cause there's bit more than summaries here.",
                    "label": 0
                },
                {
                    "sent": "Basically the primitive of our tool are these patterns.",
                    "label": 0
                },
                {
                    "sent": "These schema patterns, type property type and the idea is that these Potter represent that there are instances of.",
                    "label": 0
                },
                {
                    "sent": "These Co type that are connected to instances or in this case literals of your type through these funding year property OK. An why we called these ontology driven profiling becausw.",
                    "label": 0
                },
                {
                    "sent": "Basically we can use the ontology if you also use the ontology as input for this summarization to extract only the most specific patterns that you can find from the triples.",
                    "label": 0
                },
                {
                    "sent": "OK by using only the most specific dummy.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Email type associated with this entity.",
                    "label": 0
                },
                {
                    "sent": "Then of course the real value come from this statistics, so we count how frequent these patterns are, and more recently we have added these new features exactly for this work, where we started to try to understand how these properties this relation are shaped in terms of cardinality, introducing cardinality, descrip?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically the main idea of cardinality descriptors is to count for all the relations that are represented by a pattern, right?",
                    "label": 0
                },
                {
                    "sent": "The number of distinct subjects associated with unique objects in the triples represented by the pattern.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compute tree statistics.",
                    "label": 0
                },
                {
                    "sent": "The minimal the minimum number.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of distinct subjects, like in this case, the maximum number of distinct subjects, like in this case and then of.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, the average, and we represent the average.",
                    "label": 0
                },
                {
                    "sent": "We did discrete number and we do.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same thing, although less relevant to our talk.",
                    "label": 0
                },
                {
                    "sent": "Also, on the other side, like so for the distinct objects.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Associated with unique subjects.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compute.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Statistics it's important to notice that we compute these at the pattern level or OK, and we found that the cardinality at the pattern level differ from pattern to pattern, so they are different from this cardinality.",
                    "label": 0
                },
                {
                    "sent": "If we only consider the property.",
                    "label": 0
                },
                {
                    "sent": "So what's the?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Intuition here why we thought that this cardinality descriptors could be useful to automate the features selection?",
                    "label": 0
                },
                {
                    "sent": "Well, the main idea here is that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those.",
                    "label": 0
                },
                {
                    "sent": "Properties that map distinct subjects to unique objects may be useful for similarity.",
                    "label": 1
                },
                {
                    "sent": "Whi becausw here if you think about the Jaccard and we will see later here you have a common feature for these two objects.",
                    "label": 0
                },
                {
                    "sent": "OK so this is the intuition and This is why the statistics that we consider in our work are only the 1 four distinct subjects.",
                    "label": 0
                },
                {
                    "sent": "Associated with unique object, in particular, the average S which is the average number of subjects associated to distinct objects and maximum S.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how can we do feature selection?",
                    "label": 0
                },
                {
                    "sent": "You are using this information.",
                    "label": 0
                },
                {
                    "sent": "Basically, we have this pipeline, so we take the set of patterns.",
                    "label": 0
                },
                {
                    "sent": "We have an optional filtering phase where we filter out patterns that do not comply with some criteria.",
                    "label": 0
                },
                {
                    "sent": "So here for example we filter out all the patterns that have a average number of distinct subjects that is minor than one.",
                    "label": 1
                },
                {
                    "sent": "Then we have the projection right?",
                    "label": 0
                },
                {
                    "sent": "So we filter out some pattern.",
                    "label": 0
                },
                {
                    "sent": "We project the information that here.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We added the pattern level on the property right?",
                    "label": 0
                },
                {
                    "sent": "So we basically take for example the maximum frequency of all the pattern that we have for that property, and we assign that Max frequency to the property.",
                    "label": 0
                },
                {
                    "sent": "And finally we can provide some ranking by using these criteria.",
                    "label": 0
                },
                {
                    "sent": "In this case, for example by frequency.",
                    "label": 0
                },
                {
                    "sent": "So we can order properties by frequency and then we select the best properties.",
                    "label": 0
                },
                {
                    "sent": "Best top K properties.",
                    "label": 0
                },
                {
                    "sent": "Of course this pipeline can be configured like in a different way by using for the values of partner frequency, local Cardinal carnality descriptors.",
                    "label": 0
                },
                {
                    "sent": "Orca",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bination of both.",
                    "label": 0
                },
                {
                    "sent": "So we will see that now in our experiment we will also try several alternative implementation of this general pipeline.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, so back to statistical measure.",
                    "label": 0
                },
                {
                    "sent": "For feature selection.",
                    "label": 0
                },
                {
                    "sent": "We tried several of them like information gain, information, gain ratio and chi squared tests and we found out that information gain was the one that was performing better, so I won't go into the detail of information gain information game.",
                    "label": 0
                },
                {
                    "sent": "Basically measure the expected their reduction in entropy or curing when a feature is present versus when is not present.",
                    "label": 0
                },
                {
                    "sent": "So the principle is quite similar.",
                    "label": 0
                },
                {
                    "sent": "Through the idea on the cardinality, but it's computed at the final level, so on the data itself, not on the summaries.",
                    "label": 0
                },
                {
                    "sent": "Now here one.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem is that computing those statistical measures is also computationally very expensive, so we needed some manual preprocessing when using these statistical measure.",
                    "label": 0
                },
                {
                    "sent": "So we reduced the old set of properties to a much smaller number and then we computed the information gain.",
                    "label": 0
                },
                {
                    "sent": "Just considering those properties.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will try to describe the experiments that that we did, so as in the introduction we used an item based the nearest neighbor halga rhythm for recommendations, which is quite standard in this domain.",
                    "label": 0
                },
                {
                    "sent": "So basically it's the formula that we have seen before.",
                    "label": 0
                },
                {
                    "sent": "But here we use.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chuck car similarity.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarity measure which measures the values of the selected feature that are shared between two items right?",
                    "label": 0
                },
                {
                    "sent": "We tried several experimental setting says data set.",
                    "label": 0
                },
                {
                    "sent": "We use benchmarking the recommender system, a domain movie and so last FM librarything and a one to one mapping to DB pedia for the entities and then do.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The pedia to extract the information.",
                    "label": 0
                },
                {
                    "sent": "Different kind of metrics to for accuracy.",
                    "label": 0
                },
                {
                    "sent": "Precision attendant me reciprocal rank an we also wanted to consider diversity with two measures, catalog coverage and aggregate diversity.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are more parameters that we needed that too considering the experiment.",
                    "label": 0
                },
                {
                    "sent": "So for example, the number of features that we consider in the recommendation.",
                    "label": 0
                },
                {
                    "sent": "So 5 and 20.",
                    "label": 0
                },
                {
                    "sent": "And then since we use the whole DB pedia with infoboxes, then it happened that we have summed up some duplicate properties.",
                    "label": 0
                },
                {
                    "sent": "So we have different strategies to resolve these duplicates.",
                    "label": 0
                },
                {
                    "sent": "Just keep only DB pedia property.",
                    "label": 0
                },
                {
                    "sent": "Keep all the DBO use edge.",
                    "label": 0
                },
                {
                    "sent": "Using the duplicates or avoiding duplicates, you and you can find the details in the paper.",
                    "label": 0
                },
                {
                    "sent": "So here we tried several configuration for upset.",
                    "label": 0
                },
                {
                    "sent": "These are the ones that proved to be most effective by filtering by average ASAN ranking by frequency.",
                    "label": 0
                },
                {
                    "sent": "Another two where we didn't filter but we combine frequency and access for ranking and finally just maccess.",
                    "label": 0
                },
                {
                    "sent": "And finally we have these.",
                    "label": 0
                },
                {
                    "sent": "TF IDF based language is still based on abstract, but it's based on computing the TF IDF form patterns.",
                    "label": 0
                },
                {
                    "sent": "OK, so This is why.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Diversity.",
                    "label": 0
                },
                {
                    "sent": "It's also important, so we really want to have an evaluation across measures that capture different aspects of the recommendation.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm, sorry I was given back OK so here the experimental results are.",
                    "label": 0
                },
                {
                    "sent": "Basically we found that that for the movie lens data set, abstract based features selection is almost always statistically better than statistical measures.",
                    "label": 0
                },
                {
                    "sent": "With few local exceptions.",
                    "label": 0
                },
                {
                    "sent": "Although different configurations optimize the different measures, right?",
                    "label": 0
                },
                {
                    "sent": "TF IDF obstat based.",
                    "label": 0
                },
                {
                    "sent": "I need this instead.",
                    "label": 0
                },
                {
                    "sent": "Very good for.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For aggregate entropy for one of these, measure in this case.",
                    "label": 0
                },
                {
                    "sent": "In last FM we have results.",
                    "label": 0
                },
                {
                    "sent": "Less strong, as in the previous data set, but here we we find that upset based features.",
                    "label": 0
                },
                {
                    "sent": "Selection is usually better or almost comperable to information gain, but in some of these cases we also have like the statistical significance, but remind that you still have an advantage by the summaries.",
                    "label": 0
                },
                {
                    "sent": "If you have comperable results because you don't need to compute the statistics on the full data set.",
                    "label": 0
                },
                {
                    "sent": "OK, it's more efficient and also hear the TF IDF baseline is good for aggregate entropy Ann.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have some more jeopardize results.",
                    "label": 0
                },
                {
                    "sent": "Information gain is better for catalog coverage with 20 features is better for some duplicate property management.",
                    "label": 0
                },
                {
                    "sent": "I don't have idea what happened actually, Luckily.",
                    "label": 0
                },
                {
                    "sent": "I think that just stopped working.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "OK so here we have that.",
                    "label": 0
                },
                {
                    "sent": "In some cases information gain is better, but again the differences are not really too big.",
                    "label": 0
                },
                {
                    "sent": "OK so we have to go to the 3rd digit to find them and again.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the TDF.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Absolute data baseline is good for aggregate entropy.",
                    "label": 0
                },
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "We provide an approach to provide a fully automatic feature selection method with ontology based knowledge graph summaries using apps that their results with this approach are better or in some cases comperable to statistical measure, which is interesting because summaries were not introduced for these scope but were introduced to support data understanding.",
                    "label": 0
                },
                {
                    "sent": "And here we are finding that this is.",
                    "label": 0
                },
                {
                    "sent": "I think it's an additional evidence for the.",
                    "label": 0
                },
                {
                    "sent": "Usefulness of these are knowledge graph summarization approaches.",
                    "label": 0
                },
                {
                    "sent": "Also for data analytics task as a future work we would.",
                    "label": 0
                },
                {
                    "sent": "We think that maybe we could add the TF IDF in apps that statistics because it proved to be useful at least for one measure.",
                    "label": 0
                },
                {
                    "sent": "We would like to carry out experiments with some additional measures.",
                    "label": 0
                },
                {
                    "sent": "So now we tried with graph kernels and Jack are that we have still a couple of measures to evaluate with this.",
                    "label": 0
                },
                {
                    "sent": "And finally, we are planning to include the in the API of abstract a service that, given an input type, it gives you the top most salient features for that type.",
                    "label": 0
                },
                {
                    "sent": "Based on these pipelines, which can be used online to determine the best feature for evaluating the similarity.",
                    "label": 0
                },
                {
                    "sent": "So that was the last slide.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for attention, any question.",
                    "label": 0
                },
                {
                    "sent": "Are you introduced this notion of patterns?",
                    "label": 0
                },
                {
                    "sent": "Can you give a little bit more insight about nothing?",
                    "label": 0
                },
                {
                    "sent": "That was very fast.",
                    "label": 0
                },
                {
                    "sent": "That part of the talk and does this relate to ontology design patterns in any way?",
                    "label": 0
                },
                {
                    "sent": "So yes, we were also inspired by ontology design patterns.",
                    "label": 0
                },
                {
                    "sent": "Actually, technically speaking, so I didn't focus too much because I had the demo this morning about this and.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so coming back here, these patterns basically provided an X Rays or what exists in the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "So logically speaking at these are existential axioms that tells you whatever exists in the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "So there are complete right for each relation you have representative patterns and then here you have those statistics that tell you how frequent they are.",
                    "label": 0
                },
                {
                    "sent": "So I think that yes, for example, for ontology design pattern, this could be useful because it can tell you how the vocabulary is used in a knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "Write an which frequency feature, which is OK.",
                    "label": 0
                },
                {
                    "sent": "But also these descriptors I think gives an idea of patterns that you can find.",
                    "label": 0
                },
                {
                    "sent": "OK, so actually this reminds me a little bit of this and encyclopedic knowledge patterns work from all those group, yet they learn this pattern.",
                    "label": 0
                },
                {
                    "sent": "So you seem to do something similar.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in fact they did something similar for Wikipedia, right?",
                    "label": 0
                },
                {
                    "sent": "So they extracted something similar to Wikipedia because I remember that all too old and he said we did something similar.",
                    "label": 0
                },
                {
                    "sent": "So what we did here is to build a tool that can compute these on big knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "So for example, here we use the the in this experiment.",
                    "label": 0
                },
                {
                    "sent": "I just want to show that we used at the DB pedia plus the infoboxes which has 392 million triples and 55,000 properties.",
                    "label": 0
                },
                {
                    "sent": "OK and and and to me this is important because in DP you have the DP ontology but you don't know which are the properties in the infoboxes right and this gives you this and of course this is publicly available.",
                    "label": 0
                },
                {
                    "sent": "You can browse and see if it's.",
                    "label": 0
                },
                {
                    "sent": "Useful for OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "It was a big question.",
                    "label": 0
                },
                {
                    "sent": "Could you give us some insights like on how often it was data type properties or object properties that were actually like the features in the summaries and that made.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK so.",
                    "label": 0
                },
                {
                    "sent": "Data OK. Like literals like matching dates or.",
                    "label": 0
                },
                {
                    "sent": "I wish I had the imager with the different rankings that we produce path.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately I don't have so I I can do do this offline.",
                    "label": 0
                },
                {
                    "sent": "So basically, like for example, one of the very important feature here was director.",
                    "label": 0
                },
                {
                    "sent": "Of course starting which are object property subject which is another very important, not ontological property, but very useful here.",
                    "label": 0
                },
                {
                    "sent": "So it was a really a mix of the two an if you look at these so keep in mind that here when you have like that DB pedia property, you favor the like shallow semantics in the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "And sometimes you get better result with DPD and sometimes with that.",
                    "label": 0
                },
                {
                    "sent": "So it's that's a bit unpredictable if you want.",
                    "label": 0
                },
                {
                    "sent": "But I can give you the rankings.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have the media, the image.",
                    "label": 0
                },
                {
                    "sent": "I don't have it here.",
                    "label": 0
                },
                {
                    "sent": "I should have added thanks.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                },
                {
                    "sent": "I have one more.",
                    "label": 0
                },
                {
                    "sent": "OK, actually there was this slide where you you showed your experimental data and that you used free datasets.",
                    "label": 0
                },
                {
                    "sent": "I think that you linked to DB Pedia.",
                    "label": 0
                },
                {
                    "sent": "Yes yes.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "How did you establish this links?",
                    "label": 0
                },
                {
                    "sent": "What was the method that you know you have to ask it to?",
                    "label": 0
                },
                {
                    "sent": "To my to Thomas who is there because he did it this mapping.",
                    "label": 0
                },
                {
                    "sent": "Why do you hide?",
                    "label": 0
                },
                {
                    "sent": "He didn't want to.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well we use the same automated process.",
                    "label": 0
                },
                {
                    "sent": "We basically took the names of the movies or books and the like and then we used services just like DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Look up or other.",
                    "label": 0
                },
                {
                    "sent": "Well, we were not able to find the.",
                    "label": 0
                },
                {
                    "sent": "Correct mapping with DP to look at and we you know we use some other sparkle queries and then we manually check everything in the light so it was quite long.",
                    "label": 0
                },
                {
                    "sent": "You know work, but in the end we think that it is quite accurate, yeah?",
                    "label": 0
                },
                {
                    "sent": "How about frameworks like silk?",
                    "label": 0
                },
                {
                    "sent": "Another linking from?",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, I mean we also used sealed, you know we used a lot of tools actually.",
                    "label": 0
                },
                {
                    "sent": "And yeah we use that kind of pipeline you know to find this this mappings.",
                    "label": 0
                },
                {
                    "sent": "But in the end it was just, you know.",
                    "label": 0
                },
                {
                    "sent": "I don't want to say kind of string matching, but it is something like that because the name of the movies is very often exactly the same way that we can find.",
                    "label": 0
                },
                {
                    "sent": "You know in DB pedia.",
                    "label": 0
                },
                {
                    "sent": "I have to say that the hardest part of this work with the manual check and you know the minor correction of.",
                    "label": 0
                },
                {
                    "sent": "Of items that were not correctly met.",
                    "label": 0
                },
                {
                    "sent": "And who did you ask to check this for three experts?",
                    "label": 0
                },
                {
                    "sent": "Or you know crowdsourcing?",
                    "label": 0
                },
                {
                    "sent": "What kind of methods, like how did you involve the human annotators?",
                    "label": 0
                },
                {
                    "sent": "I have great students.",
                    "label": 0
                },
                {
                    "sent": "OK, that clears everything.",
                    "label": 0
                }
            ]
        }
    }
}