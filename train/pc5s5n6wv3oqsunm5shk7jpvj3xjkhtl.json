{
    "id": "pc5s5n6wv3oqsunm5shk7jpvj3xjkhtl",
    "title": "Hilbert Space Embeddings of Conditional Distributions with Applications to Dynamical Systems",
    "info": {
        "author": [
            "Le Song, College of Computing, Georgia Institute of Technology"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/icml09_song_hsec/",
    "segmentation": [
        [
            "Hi, how are you?",
            "I'm so today I'm going to talk about kernel method for representing specifically conditional distributions and talk about its applications to dynamical systems, OK?",
            "This is a joint work with Jonathan Huang from CMU, Exmoor in Yahoo and Kendra, full Commission from Japan."
        ],
        [
            "So we're often when we do with distributions will use summary statistics such as the mean, or convince an more generally expected feature to describe certain aspect of the distribution.",
            "It is this more general for that is expected feature we care about in our talk on Hilbert space embedding.",
            "So what is he?"
        ],
        [
            "Space embedding.",
            "So, so most specifically, given the general distribution P or Y, you just sample from this distribution and you design A kernel.",
            "Then you can map this samples to the feature space F. Likewise, you can also map the entire distribution to the same feature space.",
            "By taking the expected value of the feature map.",
            "OK, so this entire distribution is mapped to a point here.",
            "OK, we call this expected feature that human space embedding where you have a final set of samples the expected feature, the human space embedding can be just estimated by the empirical average.",
            "OK, the nice thing about human space embedding is that if you choose your feature map to be powerful enough, for instance, that the feature.",
            "Associated with the Gaussian RBF kernel, you can actually uniquely mapped the distribution one to one to an element in the Hilbert space.",
            "That means theoretically you can reconstruct the distribution just using the summary statistics here.",
            "OK, other nice thing about Hilbert space embedding is that the empirical estimate actually you can show that it's consistent OK.",
            "So so so because of this nice probability, this human space embedding once you map the distribution in the feature space, you can use it too."
        ],
        [
            "Actually defined distance between distributions and it has found a lot of applications.",
            "For instance, you can define the distance between two distributions P&Q and then you can use it for a lot of applications such as transduction cover, shift correction, submission provides learning and density estimation.",
            "You can also define distance between the product.",
            "The joint distribution and product of the marginal that gives you a measure of the independent and then you can use that for.",
            "Clustering or damage knowledge introduction and also sorting.",
            "OK, so in this talk while I'm going to talk about is another branch for this subspace embedding.",
            "Basically we care about how do we embed conditional distribution.",
            "We have an additional variable where we want to conditional OK and then this will also lead to new applications such as structure prediction.",
            "Also in this talk I'm going to talk about the dynamical system application.",
            "OK, the first thing the question is when you are trying to embarrass conditional distribution, why is it different from normal distribution?"
        ],
        [
            "Ordinary distribution.",
            "OK, so remember that the conditional distribution is not a single distribution, it's actually entire family.",
            "So depending on the variable your conditional you fix some value returns your distribution, so it is just not single distributor, it's family.",
            "OK, the simplest case, the variable you're conditioning only is a binary variable.",
            "Then you have two distributions, P of Y given X equal to 0 and P of Y given X equal to 1, you want to embed both distribution.",
            "To the future space.",
            "OK, you want to represent them as a summary statistics OK?",
            "So for binary variable for the weather, weather, condition in wearables buried, the situation is very simple.",
            "When you have a finite set of sample, what you do is you divide draw.",
            "Why observation into two groups, one group having all X equal to zero, another group having all X equal to 1.",
            "Then you compute your sample average in each group separately.",
            "You get the two embedding.",
            "But what happens if you have continuous variables or structure variables?",
            "The number of possible value X can take can be very large.",
            "Are you going to divide your Y into a lot of small subsets and using the limited amount of sample to estimate the embedding for each of them?",
            "So let's look at."
        ],
        [
            "Another situation OK in the regression setting where you have choraria X and you have your response variable Y.",
            "Here.",
            "So for each fixed value of X you have a conditional distribution you want to embed them into the human space.",
            "So your goal is to estimate the human space representation for this distribution for every possible value X can take.",
            "So the question problem in this situation is some of the.",
            "Value of X may not be may not be observed in your training data at all, so you want to embed those distributions corresponding to those X that have never appeared in your training data.",
            "How do you do that?",
            "OK, this is a question.",
            "OK, so actually our strategy to solve this problem is very simple.",
            "The intuition is that the conditioning variable X.",
            "If 2X is our.",
            "Similar according to some measure similarity measure, then the corresponding conditional distribution should be similar as well, so.",
            "So.",
            "This is the situation if exit and xpi is where similar, then you expect the conditional distribution is also going to be similar.",
            "So to make this intuition work and then generalize this conditionally."
        ],
        [
            "Heading into the axis that is not observed in your training data, what you do is you also apply kernel X to bring information across.",
            "Points OK, how do we do that?",
            "Exactly?",
            "OK to illustrate it graphically.",
            "We have this picture."
        ],
        [
            "So this is an example.",
            "In the previous slides and then we want to embed this conditional distribution into the feature space after we choose a kernel.",
            "OK, so the way we do it is we first map the axis into the future space.",
            "OK, and then after that we apply a linear operator.",
            "Dad apply linear proper on facts and that returns you are.",
            "Hilbert space embedding.",
            "So we call this linear operator here.",
            "The conditional embedding operator.",
            "Basically, when every time we applies to a fax it returns the Hilbert space embedding for the corresponding distribution.",
            "The conditional Max.",
            "OK, so the question now is, what is the representation for these so called conditional embedding operator?",
            "And how do we estimate it when you are given the final number of samples?",
            "So I'm not going to go into detail.",
            "How do you derive the presentation for it?",
            "But it turns out."
        ],
        [
            "We can define the linear operator, the conditional embedding operator using two covariance matrix CYXX X -- 1.",
            "Here is just the operator inversion here.",
            "OK, what is covariance operator?",
            "This is convince operator, it is just a generalization of the covariance matrix.",
            "Everybody knows how to estimate covariance matrix given the final sample.",
            "You can estimate the current operator in a very similar fashion.",
            "If you have two kernels, one on your why another on your conditioning variable X The expression you're going to get is something where."
        ],
        [
            "Simple, it's a you can express everything in terms of Colonel, so they decay.",
            "Here is the Colonel on your conditional variable.",
            "The upsilon here is corresponding feature matrix and this is the corresponding color on your conditioning variable.",
            "And then the file is the future matrix for your response variable Y.",
            "So this is this expression has the kind of matrix of conditioning variable in the middle and has the two different feature matrix on the other side on both sides.",
            "So we can also show that this empirical estimates.",
            "Is consistent OK, given that you choose the regularization parameter Lambda properly.",
            "OK.",
            "So having"
        ],
        [
            "Such a linear conditional embedding operator, such a linear operator allows you to embed distribution to the human space, but more importantly, it allows you to do probabilistic inference, so we can actually find correspondence between the human space embedding and then the probabilistic inference rules such as the sum rule and product that is fundamental.",
            "For instance, what the sunroom says is.",
            "The marginal distribution.",
            "Why can we express as you can contain the marginal distribution by summing up the.",
            "One of the variable in the joint distribution so you can have the human space equivalence.",
            "This is the human space embedding for PY and then this is the conditional embedding operator.",
            "This is the Hubble space embedding for the marginal PX.",
            "OK, likewise you can have the correspondence for the product rule as well.",
            "The Polaroid says that the joint distribution can be factorized into the product of the conditional distribution and the marginal one of the marginal.",
            "So what you're going to get for the human space?",
            "Representation is this is the embedding for drug distribution and you can get it by applying the conditioning Baron operator in the special embedding for the marginal well I mean special here is that you use a feature that has this tensor product for pens are for OK.",
            "So having this probabilistic.",
            "Ah.",
            "This basic operations allows us to apply it to.",
            "Problems such as the dynamical system.",
            "OK so."
        ],
        [
            "The following slides I'm going to talk about the application of this Hilbert space embedding of conditional distribution to dynamical system.",
            "So for dynamical system you will have a hidden States and you will have observations.",
            "The way you infer the hidden states based on observation is to maintain the so called conditional distribution of the hidden states given all the paths observations.",
            "What you do to maintain is.",
            "So called belief.",
            "This basically a conditional distribution.",
            "You just use the transition model and also the observation model to propagate this belief into the future.",
            "OK, you do this in the recursive version fashion."
        ],
        [
            "And he also applies to more general distributions, such as multimodal, skewed distribution, OK."
        ],
        [
            "We have a limited evaluation model in a simple linear dynamical system and also more difficult problem of estimating camera rotation.",
            "According to the video sequences.",
            "OK, the linear dynamical system is just constant rotation plus some noise OK.",
            "Depending on the noise model you choose the.",
            "The common filter is going to be opt in."
        ],
        [
            "So for instance, if we choose Gaussian.",
            "Process noise and Gaussian observation noise.",
            "The common filter is going to be optimal.",
            "We supply the computer with true model, so that's why it perform.",
            "The performance is constantly and it doesn't depend on the number of samples.",
            "For our approach we need to learn the conditional embedding operator as we increase the number of samples.",
            "The performance actually approaches the performance outcome filter, where we do not try to be the common field.",
            "In this case, it is just a sanity check for us, but for more complicated situation."
        ],
        [
            "We have skewed process noise.",
            "We're actually doing better when we have more examples, so this city."
        ],
        [
            "Which is more clear, advantage is more clear.",
            "Basically we have a mixture of two Gaussian process noise."
        ],
        [
            "OK, so second example is estimating the camera rotation.",
            "When you have a sequence of video, we sequence of image observations.",
            "So basically."
        ],
        [
            "We are applying the feature map to the distribution and then this feature map can already take into account higher moments.",
            "For instance, if we choose a RBF kernel, the corresponding feature map is actually have all the moment up to.",
            "Yeah, that's why you can uniquely represent a distribution that you put space."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi, how are you?",
                    "label": 0
                },
                {
                    "sent": "I'm so today I'm going to talk about kernel method for representing specifically conditional distributions and talk about its applications to dynamical systems, OK?",
                    "label": 0
                },
                {
                    "sent": "This is a joint work with Jonathan Huang from CMU, Exmoor in Yahoo and Kendra, full Commission from Japan.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're often when we do with distributions will use summary statistics such as the mean, or convince an more generally expected feature to describe certain aspect of the distribution.",
                    "label": 0
                },
                {
                    "sent": "It is this more general for that is expected feature we care about in our talk on Hilbert space embedding.",
                    "label": 1
                },
                {
                    "sent": "So what is he?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Space embedding.",
                    "label": 0
                },
                {
                    "sent": "So, so most specifically, given the general distribution P or Y, you just sample from this distribution and you design A kernel.",
                    "label": 0
                },
                {
                    "sent": "Then you can map this samples to the feature space F. Likewise, you can also map the entire distribution to the same feature space.",
                    "label": 0
                },
                {
                    "sent": "By taking the expected value of the feature map.",
                    "label": 0
                },
                {
                    "sent": "OK, so this entire distribution is mapped to a point here.",
                    "label": 0
                },
                {
                    "sent": "OK, we call this expected feature that human space embedding where you have a final set of samples the expected feature, the human space embedding can be just estimated by the empirical average.",
                    "label": 0
                },
                {
                    "sent": "OK, the nice thing about human space embedding is that if you choose your feature map to be powerful enough, for instance, that the feature.",
                    "label": 0
                },
                {
                    "sent": "Associated with the Gaussian RBF kernel, you can actually uniquely mapped the distribution one to one to an element in the Hilbert space.",
                    "label": 1
                },
                {
                    "sent": "That means theoretically you can reconstruct the distribution just using the summary statistics here.",
                    "label": 0
                },
                {
                    "sent": "OK, other nice thing about Hilbert space embedding is that the empirical estimate actually you can show that it's consistent OK.",
                    "label": 0
                },
                {
                    "sent": "So so so because of this nice probability, this human space embedding once you map the distribution in the feature space, you can use it too.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually defined distance between distributions and it has found a lot of applications.",
                    "label": 0
                },
                {
                    "sent": "For instance, you can define the distance between two distributions P&Q and then you can use it for a lot of applications such as transduction cover, shift correction, submission provides learning and density estimation.",
                    "label": 0
                },
                {
                    "sent": "You can also define distance between the product.",
                    "label": 0
                },
                {
                    "sent": "The joint distribution and product of the marginal that gives you a measure of the independent and then you can use that for.",
                    "label": 0
                },
                {
                    "sent": "Clustering or damage knowledge introduction and also sorting.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this talk while I'm going to talk about is another branch for this subspace embedding.",
                    "label": 0
                },
                {
                    "sent": "Basically we care about how do we embed conditional distribution.",
                    "label": 1
                },
                {
                    "sent": "We have an additional variable where we want to conditional OK and then this will also lead to new applications such as structure prediction.",
                    "label": 0
                },
                {
                    "sent": "Also in this talk I'm going to talk about the dynamical system application.",
                    "label": 0
                },
                {
                    "sent": "OK, the first thing the question is when you are trying to embarrass conditional distribution, why is it different from normal distribution?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ordinary distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so remember that the conditional distribution is not a single distribution, it's actually entire family.",
                    "label": 0
                },
                {
                    "sent": "So depending on the variable your conditional you fix some value returns your distribution, so it is just not single distributor, it's family.",
                    "label": 0
                },
                {
                    "sent": "OK, the simplest case, the variable you're conditioning only is a binary variable.",
                    "label": 0
                },
                {
                    "sent": "Then you have two distributions, P of Y given X equal to 0 and P of Y given X equal to 1, you want to embed both distribution.",
                    "label": 1
                },
                {
                    "sent": "To the future space.",
                    "label": 0
                },
                {
                    "sent": "OK, you want to represent them as a summary statistics OK?",
                    "label": 0
                },
                {
                    "sent": "So for binary variable for the weather, weather, condition in wearables buried, the situation is very simple.",
                    "label": 0
                },
                {
                    "sent": "When you have a finite set of sample, what you do is you divide draw.",
                    "label": 0
                },
                {
                    "sent": "Why observation into two groups, one group having all X equal to zero, another group having all X equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Then you compute your sample average in each group separately.",
                    "label": 1
                },
                {
                    "sent": "You get the two embedding.",
                    "label": 0
                },
                {
                    "sent": "But what happens if you have continuous variables or structure variables?",
                    "label": 1
                },
                {
                    "sent": "The number of possible value X can take can be very large.",
                    "label": 0
                },
                {
                    "sent": "Are you going to divide your Y into a lot of small subsets and using the limited amount of sample to estimate the embedding for each of them?",
                    "label": 0
                },
                {
                    "sent": "So let's look at.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another situation OK in the regression setting where you have choraria X and you have your response variable Y.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So for each fixed value of X you have a conditional distribution you want to embed them into the human space.",
                    "label": 0
                },
                {
                    "sent": "So your goal is to estimate the human space representation for this distribution for every possible value X can take.",
                    "label": 0
                },
                {
                    "sent": "So the question problem in this situation is some of the.",
                    "label": 0
                },
                {
                    "sent": "Value of X may not be may not be observed in your training data at all, so you want to embed those distributions corresponding to those X that have never appeared in your training data.",
                    "label": 0
                },
                {
                    "sent": "How do you do that?",
                    "label": 0
                },
                {
                    "sent": "OK, this is a question.",
                    "label": 0
                },
                {
                    "sent": "OK, so actually our strategy to solve this problem is very simple.",
                    "label": 0
                },
                {
                    "sent": "The intuition is that the conditioning variable X.",
                    "label": 0
                },
                {
                    "sent": "If 2X is our.",
                    "label": 0
                },
                {
                    "sent": "Similar according to some measure similarity measure, then the corresponding conditional distribution should be similar as well, so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the situation if exit and xpi is where similar, then you expect the conditional distribution is also going to be similar.",
                    "label": 0
                },
                {
                    "sent": "So to make this intuition work and then generalize this conditionally.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Heading into the axis that is not observed in your training data, what you do is you also apply kernel X to bring information across.",
                    "label": 0
                },
                {
                    "sent": "Points OK, how do we do that?",
                    "label": 0
                },
                {
                    "sent": "Exactly?",
                    "label": 0
                },
                {
                    "sent": "OK to illustrate it graphically.",
                    "label": 0
                },
                {
                    "sent": "We have this picture.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is an example.",
                    "label": 0
                },
                {
                    "sent": "In the previous slides and then we want to embed this conditional distribution into the feature space after we choose a kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so the way we do it is we first map the axis into the future space.",
                    "label": 0
                },
                {
                    "sent": "OK, and then after that we apply a linear operator.",
                    "label": 0
                },
                {
                    "sent": "Dad apply linear proper on facts and that returns you are.",
                    "label": 0
                },
                {
                    "sent": "Hilbert space embedding.",
                    "label": 0
                },
                {
                    "sent": "So we call this linear operator here.",
                    "label": 0
                },
                {
                    "sent": "The conditional embedding operator.",
                    "label": 0
                },
                {
                    "sent": "Basically, when every time we applies to a fax it returns the Hilbert space embedding for the corresponding distribution.",
                    "label": 0
                },
                {
                    "sent": "The conditional Max.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question now is, what is the representation for these so called conditional embedding operator?",
                    "label": 1
                },
                {
                    "sent": "And how do we estimate it when you are given the final number of samples?",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to go into detail.",
                    "label": 0
                },
                {
                    "sent": "How do you derive the presentation for it?",
                    "label": 0
                },
                {
                    "sent": "But it turns out.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can define the linear operator, the conditional embedding operator using two covariance matrix CYXX X -- 1.",
                    "label": 1
                },
                {
                    "sent": "Here is just the operator inversion here.",
                    "label": 0
                },
                {
                    "sent": "OK, what is covariance operator?",
                    "label": 0
                },
                {
                    "sent": "This is convince operator, it is just a generalization of the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Everybody knows how to estimate covariance matrix given the final sample.",
                    "label": 0
                },
                {
                    "sent": "You can estimate the current operator in a very similar fashion.",
                    "label": 0
                },
                {
                    "sent": "If you have two kernels, one on your why another on your conditioning variable X The expression you're going to get is something where.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple, it's a you can express everything in terms of Colonel, so they decay.",
                    "label": 0
                },
                {
                    "sent": "Here is the Colonel on your conditional variable.",
                    "label": 0
                },
                {
                    "sent": "The upsilon here is corresponding feature matrix and this is the corresponding color on your conditioning variable.",
                    "label": 0
                },
                {
                    "sent": "And then the file is the future matrix for your response variable Y.",
                    "label": 0
                },
                {
                    "sent": "So this is this expression has the kind of matrix of conditioning variable in the middle and has the two different feature matrix on the other side on both sides.",
                    "label": 0
                },
                {
                    "sent": "So we can also show that this empirical estimates.",
                    "label": 0
                },
                {
                    "sent": "Is consistent OK, given that you choose the regularization parameter Lambda properly.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So having",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Such a linear conditional embedding operator, such a linear operator allows you to embed distribution to the human space, but more importantly, it allows you to do probabilistic inference, so we can actually find correspondence between the human space embedding and then the probabilistic inference rules such as the sum rule and product that is fundamental.",
                    "label": 1
                },
                {
                    "sent": "For instance, what the sunroom says is.",
                    "label": 0
                },
                {
                    "sent": "The marginal distribution.",
                    "label": 0
                },
                {
                    "sent": "Why can we express as you can contain the marginal distribution by summing up the.",
                    "label": 0
                },
                {
                    "sent": "One of the variable in the joint distribution so you can have the human space equivalence.",
                    "label": 0
                },
                {
                    "sent": "This is the human space embedding for PY and then this is the conditional embedding operator.",
                    "label": 0
                },
                {
                    "sent": "This is the Hubble space embedding for the marginal PX.",
                    "label": 1
                },
                {
                    "sent": "OK, likewise you can have the correspondence for the product rule as well.",
                    "label": 0
                },
                {
                    "sent": "The Polaroid says that the joint distribution can be factorized into the product of the conditional distribution and the marginal one of the marginal.",
                    "label": 0
                },
                {
                    "sent": "So what you're going to get for the human space?",
                    "label": 0
                },
                {
                    "sent": "Representation is this is the embedding for drug distribution and you can get it by applying the conditioning Baron operator in the special embedding for the marginal well I mean special here is that you use a feature that has this tensor product for pens are for OK.",
                    "label": 0
                },
                {
                    "sent": "So having this probabilistic.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                },
                {
                    "sent": "This basic operations allows us to apply it to.",
                    "label": 0
                },
                {
                    "sent": "Problems such as the dynamical system.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The following slides I'm going to talk about the application of this Hilbert space embedding of conditional distribution to dynamical system.",
                    "label": 1
                },
                {
                    "sent": "So for dynamical system you will have a hidden States and you will have observations.",
                    "label": 0
                },
                {
                    "sent": "The way you infer the hidden states based on observation is to maintain the so called conditional distribution of the hidden states given all the paths observations.",
                    "label": 0
                },
                {
                    "sent": "What you do to maintain is.",
                    "label": 0
                },
                {
                    "sent": "So called belief.",
                    "label": 0
                },
                {
                    "sent": "This basically a conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "You just use the transition model and also the observation model to propagate this belief into the future.",
                    "label": 0
                },
                {
                    "sent": "OK, you do this in the recursive version fashion.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And he also applies to more general distributions, such as multimodal, skewed distribution, OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a limited evaluation model in a simple linear dynamical system and also more difficult problem of estimating camera rotation.",
                    "label": 0
                },
                {
                    "sent": "According to the video sequences.",
                    "label": 0
                },
                {
                    "sent": "OK, the linear dynamical system is just constant rotation plus some noise OK.",
                    "label": 0
                },
                {
                    "sent": "Depending on the noise model you choose the.",
                    "label": 0
                },
                {
                    "sent": "The common filter is going to be opt in.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for instance, if we choose Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Process noise and Gaussian observation noise.",
                    "label": 1
                },
                {
                    "sent": "The common filter is going to be optimal.",
                    "label": 0
                },
                {
                    "sent": "We supply the computer with true model, so that's why it perform.",
                    "label": 0
                },
                {
                    "sent": "The performance is constantly and it doesn't depend on the number of samples.",
                    "label": 0
                },
                {
                    "sent": "For our approach we need to learn the conditional embedding operator as we increase the number of samples.",
                    "label": 0
                },
                {
                    "sent": "The performance actually approaches the performance outcome filter, where we do not try to be the common field.",
                    "label": 0
                },
                {
                    "sent": "In this case, it is just a sanity check for us, but for more complicated situation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have skewed process noise.",
                    "label": 0
                },
                {
                    "sent": "We're actually doing better when we have more examples, so this city.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is more clear, advantage is more clear.",
                    "label": 0
                },
                {
                    "sent": "Basically we have a mixture of two Gaussian process noise.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so second example is estimating the camera rotation.",
                    "label": 1
                },
                {
                    "sent": "When you have a sequence of video, we sequence of image observations.",
                    "label": 0
                },
                {
                    "sent": "So basically.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are applying the feature map to the distribution and then this feature map can already take into account higher moments.",
                    "label": 0
                },
                {
                    "sent": "For instance, if we choose a RBF kernel, the corresponding feature map is actually have all the moment up to.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's why you can uniquely represent a distribution that you put space.",
                    "label": 0
                }
            ]
        }
    }
}