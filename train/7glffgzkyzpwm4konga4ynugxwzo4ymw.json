{
    "id": "7glffgzkyzpwm4konga4ynugxwzo4ymw",
    "title": "Machine Learning for Sequential Data: A Comparative Study with Applications to Natural Language Processing",
    "info": {
        "author": [
            "Sandor Canisius, Tilburg University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Natural Language Processing",
            "Top->Computer Science->Machine Learning->Human Language Technology"
        ]
    },
    "url": "http://videolectures.net/oh06_canisius_csanl/",
    "segmentation": [
        [
            "OK, well the point of view of this talk I guess will be slightly different or really different from all the other attacks we've seen today.",
            "I'm not going to present a new method for solving structured outputs.",
            "Machine learning problems.",
            "The Toronto going to try to tend an empirical study of the many structured outputs techniques that are available today and apply those specifically to natural language processing tasks.",
            "So within our research group.",
            "We're doing natural language processing in general, and very often apply natural language processing techniques to higher level tasks such As for example, question answering.",
            "The user should be able to ask a question in natural language and an answer should be found for that question.",
            "Below this question answering surface, there is all kinds of machine learning.",
            "There's all kinds of natural language processing.",
            "And well, we are going to to evaluate how good many of the structured outputs machine learning techniques are for doing those kind of tasks.",
            "Well, OK, so I'm from Philly.",
            "Conversely, this is joint work with Dalvin, both also from Dilbeck University and wealth of elements from the University of Antwerp in Belgium.",
            "OK, so.",
            "I'm going to."
        ],
        [
            "Focus on one very specific type of structured outputs, namely predicting label sequences.",
            "So this is all we're going to talk about in this presentation, or we have an input sequence.",
            "Consisting of some type of items, this sequence can have any length, and we're going to predict an output sequence where the output sequence has the same length as the input sequence.",
            "So well.",
            "This is the general setting, and well, obviously this setting can be applied to many, many domains and natural language processing.",
            "There are many domains where you can apply well."
        ],
        [
            "Sequence prediction, so for example, you can do this.",
            "Your input is a sentence which is of course a sequence of words.",
            "One very simple example is predicting part of speech tags for those words where each words is classified in terms of what's grammatical category, it should have.",
            "Well, and of course this is not only just taking his word and predicting the tag for it, there is actually some kind of structural correlation in your output.",
            "So for example, the last word in the sentence rounds, of course, here it's now in, but they were drowned in isolation.",
            "Could be all kinds of grammatical categories, so it could be a noun.",
            "It could be a verb.",
            "It could be an adjective.",
            "It can be, well, many more categories, I guess.",
            "So well, you should definitely consider the entire output sequence as one prediction, which of course is the whole purpose of doing structured output prediction.",
            "Or fish tank is a very simple example.",
            "Of course you can do more complex things.",
            "So in natural language processing you have to do is very common case in which you have to predict the segmentation rather than a labeling of elements.",
            "So this is a syntactic segmentation in which you try to determine the boundaries of grammatical phrases.",
            "The blue labels are the labels that you actually predict.",
            "This is an encoding called IOB encoding, which is very often used to predict segmentations.",
            "The green labels show what is actually being predicted.",
            "So this whole sequence of inp labels means that there is one segment which is an MP segments.",
            "Well, so this is syntax.",
            "You can use the same kinds of methods for.",
            "Higher level task as well, so you could use the same encoding to denote secret segments in your sequence to do named entity recognition, in which you predict that there's a person name in the sentence that there is a location, name and sentence, etc.",
            "And well, yet another type of sequence is a word.",
            "So the sentence was a sequence of words.",
            "Word is a sequence of letters, and you can do all kinds of interesting things."
        ],
        [
            "With this as well.",
            "So here you have to work pre existing.",
            "You could for example predict phonemes for this word.",
            "So if you would have a text to speech system then you would have to predict the phonetic transcription of words.",
            "And well, the general method used here is of course the same.",
            "The input sequences award.",
            "In this case the output sequence is just a sequence of phonemes.",
            "Well Ann, you have segmentation tasks on the word level as well, so this is a morphological analysis of the words in which you predicted segments of this words and these can have all kinds of relevant purposes such as being able to predict Lemos for a word, form, or other kinds of things that you can do with words.",
            "OK, so this is just to show that being able to predict label sequences is very relevant in natural language processing, and I'm not even talking about many uses of predicting sequences outside of natural language processing.",
            "Well, this is mainly our area of focus, so so this is what I'm going to use in this evaluation.",
            "Well, as I sat through many many."
        ],
        [
            "Methods available today for doing well prediction of sequences.",
            "And rather than than presenting yet another method, we want to try to set up a large scale evaluation of all these methods, and I'm sure this list isn't even complete.",
            "There may be many more as well.",
            "What we want to do is apply these methods to a large range of sequence processing tasks in natural language processing and perhaps some tasks outside of natural language processing as well.",
            "And try to come up with some conclusions on observations that can tell us when a certain technique is the best technique to use on what kind of talk, so you can expect that that's, well, most of the tasks won't perform exactly the same, they will have their strengths, their weaknesses, so it would be nice to come up with some general conclusions about what types of methods can best be applied to what types of sequences, processing tasks.",
            "Well, I could tell this is really all preliminary, so the plan is to have this large scale evaluation.",
            "We haven't done this large scale evaluation yet.",
            "We actually just started doing some preliminary experiments.",
            "So what I would like to do today is just discussed these preliminary experiments.",
            "Perhaps there are some useful comments on those, and I think even the preliminary experience will be.",
            "Quite interesting to see so well.",
            "What we have is quite a large collection of benchmark datasets from natural language processing on all kinds of levels of language.",
            "And all of these stocks have been having common that you can view them, or reformulate them as having an input sequence and having an output sequence so they can readily be applied."
        ],
        [
            "We all kind of machine learning techniques that I've shown on the previous slides.",
            "Well, so there are many levels work level you can do morphological segmentation.",
            "You can predict the phonetic transcription of word as I've shown on the few slides back.",
            "On sentence level, there are many tasks that you can do, so you can do syntactic shallow syntactic part of a sentence.",
            "You can do named entity recognition, actually named entity recognition is very narrowly defined as predicting, for instance locations and organizations.",
            "But of course if you go to a domain specific scenario then you can have a wide range of entities that you would like to predict.",
            "So for example, this is Jeannie at data set, some humano, it is an annotated version of.",
            "Medline abstracts, which are annotated for occurrences of all kinds of biomedically relevant terms.",
            "Well then you can.",
            "You can do the same thing on document level as well.",
            "So as a word was a sequence of letters sent.",
            "This is a sequence of words.",
            "You can also view a document As for example a sequence of sentences or sequence of lines or sequence of paragraphs, whatever.",
            "Well, I just mentioned one FA Q segmentation.",
            "I've mentioned this one because I'm going to show some results on this set as well.",
            "And well, as I mentioned, we would like to have.",
            "FAQ segmentation yeah, I'm going to show them on the general issue is it's actually a data set that has been described in the paper about maximum entropy Markov models, in which they try to detect the questions, answers, and headers footers in the documents.",
            "So well, we would really like to have some more data sets as well in this bench."
        ],
        [
            "That's we have many natural language processing assets.",
            "We would like to have some offsets as well.",
            "So well you have this.",
            "This typical set from bioinformatics, in which you do protein secondary structure prediction or gene prediction.",
            "So if there are any suggestions about our datasets that that might be interesting to show some particular properties of learning techniques, then I would welcome it.",
            "If you would just mention some of those after dark, it would be very interesting to have some more sets in this list.",
            "OK, I'm going to show a small case study."
        ],
        [
            "And well, it's a case study in the context of developing a larger question answering system.",
            "This question answering system one of the components of it is a named entity recognition component, domain specific named entity recognition components.",
            "And I'm going to discuss experiments on the two datasets for domain specific named entity recognition.",
            "The first one is this genius datasets, which is an annotated version of Medline abstracts.",
            "You can see an example sentence of these datasets where you have the words and the segmentation, so mentioning that there is DNA part in this DNA, cell line, etc.",
            "To what we're going to do is convert this sentence into this.",
            "This IOB encoding of the segmentation.",
            "And.",
            "Another data set that we use is when we divide."
        ],
        [
            "Ourselves.",
            "It's an annotated version of two Dutch medical and Scolipede's, which are not the expert level medical documents as genial datasets, but it's really just a medical Expedia that the average consumer would use, so the language is very simple.",
            "But while we've annotated many occurrences of irrelevant concepts such as well illnesses, symptoms, body parts, etc.",
            "Well, so actually these are two types of tasks and what we're going to do is make sequences of them and apply some of the sequence prediction techniques that we're going to use.",
            "Well, all of the sequence prediction techniques that I'm going to discuss in this presentation are based on, well, what at least in natural language, processing is often referred to as maximum entropy models."
        ],
        [
            "I guess in machine learning locked linear models is more common for him.",
            "I don't know why we're using through mental models, but well, I will use the first one in this presentation and well, so it's generally this model.",
            "You have a set of feature functions.",
            "This feature function of course have information about relevance information of your sentences of your words.",
            "And while learning it is just trying to to get good values for these Lambda parameters and abide as having a probability model off of some kind of feature space.",
            "And well."
        ],
        [
            "These are the sequence prediction methods step we're going to use the lift switch rather than this.",
            "This whole list that I've shown on the 1st slide just to to make start of.",
            "It's an.",
            "It starts with really simple sliding window technique, which of course doesn't have structured output, it just fits up your sentence into token level classification cases.",
            "Classifieds those and then concatenates the output labels of those two to get your entire output sequence.",
            "It's a very simple one and.",
            "It's especially good to have some kind of baseline to do Bayshore evaluation on recurrent sliding window is also very old technique.",
            "It's a bit more intelligent.",
            "What it does is in addition to the sliding window in which you have feature setting codes, previous words or next words in sentence, it also has features for the previously predicted labels in your sentence.",
            "Well, OK, so stacking is kind of more intelligent recurrent sliding window method in two phases.",
            "Constraint satisfaction inference.",
            "Is it a method that we've recently introduced ourselves on?",
            "Conditional Markov models, mental models, conditional random fields, or kind of the same family of graphical models based on next Mississippi.",
            "I think I shouldn't spend too much time on explaining all these techniques.",
            "So, well, the evaluation."
        ],
        [
            "In first of all, it's going to be about trying to predict to compare those types of classifiers.",
            "So we made the decision to not put too much effort into all kinds of feature engineering.",
            "At this point, we're going to use a very simple set of features, which is simply the sliding window of words, and for speech text in the sentence, which basically just means for each word you have an instance that has to fire speech tag for that worth, and where itself and the same for in this case 3 words before it.",
            "In three words following it.",
            "Well, we've we've made the decision to have this very simple set of features to, well, try not to to to have some kind of bias in comparing classifiers so we don't want to have this problem of, well, some kind of features may be beneficial to one technique.",
            "Another may be beneficial to another technique.",
            "So by doing this we try to avoid this problem.",
            "Another issue may be that of course, the better your feature encoding of certain incidents, the smaller the difference will be that some kind of intelligence prediction methods can make so.",
            "You know, just don't compare with Pop system is unclear whose implementation is.",
            "For example, first write the secondary so who knows what you're doing.",
            "I mean, I'm sorry who knows what.",
            "Compare used to compare with the publish special result.",
            "We will do that definitely, but of course so naturally processing you will know you have all these shared task which are very nice sandwich.",
            "I do named entity recognition, shallow parsing things like that.",
            "Of course there is very much feature tweaking in that as well and to start with these experiments with people like some people say.",
            "Never show real significant advantage I'm sure, sure yeah.",
            "So if you make a claim, we really have to say, compare with the best result.",
            "Not just OK, so I'm a bit manipulative, yeah, well, we're definitely going to do that.",
            "But as far as we are concerned, we think this is also an interesting thing to see.",
            "So here it isn't even such a big deal to have features.",
            "'cause we're using maximum entropy models as a base of all things here the mentation.",
            "So whether you implemented compared with best result is hard to say.",
            "Basically, that's why I'm saying, yeah, OK, you're absolutely right and I said we're going to do that.",
            "But it is difficult, so for example here.",
            "So maximum entropy we will be using auto type of classifiers as well.",
            "Support vector machines, memory based learning and of course then having to optimize all of those 'cause you can't just use the top performing C feature sets for other classifiers.",
            "So you can you know all these support fax machines classifiers will have many bigram features, for example, which won't work at all in memory based.",
            "Learning approach is actually eventually understand.",
            "At the top performance level, if you implement this correct, whether you can reproduce peoples result or and whether you can compare at that performance.",
            "Otherwise you know other things can happen, they don't know.",
            "Like sure.",
            "Yeah, of course that can happen as well, but well, OK, so the only thing I can say that we will be doing that but we started just doing this so well that's my answer for now.",
            "OK, so this is the set of features that we're going to use."
        ],
        [
            "OK, so here's a list of results on the junior datasets with precision, recall and F score performance for each of those and, well, let's start with the obvious.",
            "Well, let's do that again.",
            "OK, so let's start with the obvious that this naive sliding window method this outperformed by all methods to try to do something intelligence with sequences.",
            "Now another interesting thing for us at least was that if you compare precision and recall, so don't focus on that score but just precision.",
            "Recall and then you can see that all these methods mainly improve their F score by improving their position.",
            "And so as you can see in this sliding window appreciation and RCR very balanced, while all the sequence oriented prediction methods precision is much higher than the recall.",
            "Well, of course the score improves.",
            "It's a question of whether you always want to have this.",
            "So in the beginning I mentioned the application area of question answering.",
            "If you're basing your question answering information retrieval step predicted named entities, then having high precision means that while your entities are cradle writes, but you miss many of them.",
            "So those won't be candidates at all in your information retrieval step of question.",
            "Answering so well, perhaps it might be more relevant to have high recall in addition to high precision.",
            "Well, so one of the things to look into would be whether recall could be boosted as well.",
            "Well, OK, so another observation is that.",
            "Well, this is kind of the Premier League of this set of classifiers.",
            "So you have this week."
        ],
        [
            "Sliding window and the tree maximum maximum entropy graphical models.",
            "And well, they all perform kind of the same.",
            "Of course, this might be different if you use more leverage, feature structures, whatever, But at this point there's hardly any difference between oh and actually one of the more surprising things to see here as well, is that.",
            "There are two methods an I will illustrate this on.",
            "One of the following slides, but actually this this team in conditional Markov model.",
            "Well, it's not an official name for a technique I'm going to illustrate what we mean by this, but for now I'll just mention that this CNN model is actually a probabilistic variant office recurrent sliding window where there is an inference mechanism on top of this recovery sliding window and what you see here is that there's hardly any difference in performance between those two.",
            "OK, we're going to do the same thing for this Dutch metropia datasets with again the same list of classifiers.",
            "And well, what you see here is that.",
            "Recall isn't improved at all if you compare it with this.",
            "This naive sliding window methods.",
            "Well, some kind, sometimes even the slight degradation, but it's white.",
            "It's not that large.",
            "And well again, the top performing systems on this task almost produce the same types of performances here.",
            "What's the last observation that, again, is to see M model and this recurrent sliding window perform almost the same?",
            "OK so well, this short."
        ],
        [
            "Summary of the observations that we made.",
            "So in our experiments these sequence oriented methods tend to prefer precision over recall.",
            "So they are very conservative when it comes to predicting named entities.",
            "They'd rather not predict and named entity in ascending stand to predict one where it may be wrong.",
            "Well, the performance of the top performing systems in this evaluation was almost the same.",
            "Well, and one of the things that I'm going to focus on a little bit more is that these recurrent sliding windows and the probabilistic version of that have almost the exact same performance.",
            "So that's well that raises the question of, well, that is extra inference step at anything.",
            "OK, so to explain this."
        ],
        [
            "Little bit more so we can flying window graphically is this.",
            "You have your sliding window methods and in addition you have well one more extra features which which are kind of history in which you just add the class labels that were previously predicted in your sentence.",
            "And, well, So what we're calling conditional market model is actually derived from from one of."
        ],
        [
            "Hockey papers on power speech tagging from 1996, which is scrub.",
            "It's rather old one, but it's still well.",
            "It's quite compatible with all the maximum entropy Markov stuff that's been going on today.",
            "And well, there is a label sequence conditional probability.",
            "It's formulated in terms of maximum entropy predictions.",
            "Thank you.",
            "Well, so the P function at the end is just a maximum entropy model that uses a recurrent sliding window method.",
            "In this case, a beam search is used to select the most likely label sequence.",
            "Well, so here we're going to use."
        ],
        [
            "Another day I said so we started on these two named entity recognition sets.",
            "We had the same performance for both of these types of methods.",
            "Here's another data set sequence datasets that we used in the maximum entropy Markov model paper at ICML.",
            "By McCallum and others.",
            "This is the software sequence is a sequence of lines in youth net FA Q documents and you have to predict whether something is ahead of document, question and answer and etc.",
            "Well, these are the kind of features all binary features denoting some kind of property after line in this document.",
            "Well and here you can see the performance is off the methods here.",
            "Default next end is here sliding window."
        ],
        [
            "Previously mentioned sliding window, but in this case there aren't any sliding window feature, so it's just default maximum entropy.",
            "And here you can see that there is a difference between this recurrent sliding window in this conditional Markov model.",
            "So, well, OK, another observation forces that default Max End here.",
            "Well, it just doesn't do any good at all, and the other two do.",
            "But well, as mentioned here, there is a difference between the two.",
            "It's not a very large difference, but at least this beam search inference does seem to add something to it.",
            "Well, the question here is."
        ],
        [
            "Is what causes these differences in this data set, so we've seen that on this this named entity recognition datasets.",
            "There were no difference on this epic you data set.",
            "There are some differences.",
            "Well, so these are preliminary experiments.",
            "We tried to to evaluate some of the explanations that we had in mind that that might cost effective here.",
            "This inference dot at something and the other datasets it doesn't.",
            "None of these explanations have proven to be true.",
            "Of course another question is so the properties that would favor conditional Markov models with this inference step.",
            "Do these properties actually occur in real world?",
            "Natural language processing tasks?",
            "So in the research that we're doing, question answering is a real world task and FA Q segmentation is not a task that you was really used in real world system.",
            "So with the benefits of having this beam search inference transfer to hearing that you would use in.",
            "Question answering.",
            "OK, so this is my final slide, just a short summary, so well presented plans to have a large scale structure evaluation of many."
        ],
        [
            "Sequence prediction and machine learning methods.",
            "I would welcome any suggestion for relevant and informative datasets.",
            "And well, here's a slight summary of this.",
            "This preliminary case study data presented so that concludes my talk.",
            "Thank you very much.",
            "Imitation bacon will do the CRF a number on there yourself, or they're saying that you mean the CRF implementation or number.",
            "OK, so now this is we experiment.",
            "We run experiments on this asset ourselves, right?",
            "So these aren't published results from from other people.",
            "Well, mainly because there aren't any results in the datasets we use.",
            "What is number you have?",
            "Number I'm so sorry.",
            "What do you mean my number?",
            "They didn't report CRF number on that right.",
            "Scores for here?",
            "Yeah, I yes, I did actually.",
            "Well, so not not on this one, but no, not on this one now.",
            "So these were mainly to try to see this difference between recurrent sliding window air conditioner models.",
            "This one is like a very long sleep."
        ],
        [
            "Myself the same as the same same.",
            "These ethical documents are longer than than the average sentence in in a natural language document, so these are entire Usenet FA Q documents which which I guess are several hundreds of lines.",
            "The public available is data.",
            "This data sets available from Andrew Mccallum's website.",
            "I think the ginia data set is semi publicly available.",
            "I'm not sure I think that's available.",
            "Yeah, so to Dutch datasets not available because all kinds of copyrights reasons, so can't do anything about that.",
            "Nothing junior they have for more than 70 ready, so let's talk about.",
            "Yeah, of course with all kinds of feature engineering attitude, yeah.",
            "But I'm saying that you know sugar should be in there in your comparison, definitely.",
            "So it seems that you are using X or only be noise pyramid, so is this?",
            "Is this something that is?",
            "Most interesting score in Pakistan.",
            "In this field of natural.",
            "So if we focus on these segmentation thoughts, so never ignition syntactic chunking then well at least accuracy is out.",
            "So so in the effort and energy recognition task, the vast majority of tokens is not inside named entity, so you just predict, well this is nothing and if you don't back then you would have a high accuracy already, so accuracy doubts.",
            "You could of course consider RC scores, for example.",
            "The problem is that in these these chunking type of thoughts there are no true negatives, so chunking true negative chunks could be anything.",
            "I mean you're not predicting anything and you shouldn't predict it that that's infinite, so F score is the common measure for this an at least I wouldn't know any alternative for that.",
            "If anyone has suggestions for other interesting matches, I would be interested for us.",
            "Is it possible that the high precision and low recall due to the imbalance between the training?",
            "Yes, of course.",
            "That's one of the reasons.",
            "But this this default East maximum entropy Implementation Force has the same data sets, so this imbalance goes for all systems.",
            "So you can do some something quickly.",
            "Yeah.",
            "So.",
            "That cannot.",
            "You know, tell us much about the performance of these algorithms because this is deeply characteristic of your headset.",
            "Sure, OK, but but these are the data that we're working with.",
            "If we want to build a question answering system, then these are tasks that we have to do so.",
            "We want to have good machine learning methods for these kinds of data, so perhaps another datasets, perhaps bioinformatics.",
            "Things would be different, and it would be interesting to see that as well.",
            "But of course this is a real world task that you would like to solve.",
            "Myself.",
            "But I think it's a good game."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, well the point of view of this talk I guess will be slightly different or really different from all the other attacks we've seen today.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to present a new method for solving structured outputs.",
                    "label": 0
                },
                {
                    "sent": "Machine learning problems.",
                    "label": 0
                },
                {
                    "sent": "The Toronto going to try to tend an empirical study of the many structured outputs techniques that are available today and apply those specifically to natural language processing tasks.",
                    "label": 0
                },
                {
                    "sent": "So within our research group.",
                    "label": 0
                },
                {
                    "sent": "We're doing natural language processing in general, and very often apply natural language processing techniques to higher level tasks such As for example, question answering.",
                    "label": 0
                },
                {
                    "sent": "The user should be able to ask a question in natural language and an answer should be found for that question.",
                    "label": 0
                },
                {
                    "sent": "Below this question answering surface, there is all kinds of machine learning.",
                    "label": 1
                },
                {
                    "sent": "There's all kinds of natural language processing.",
                    "label": 1
                },
                {
                    "sent": "And well, we are going to to evaluate how good many of the structured outputs machine learning techniques are for doing those kind of tasks.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, so I'm from Philly.",
                    "label": 1
                },
                {
                    "sent": "Conversely, this is joint work with Dalvin, both also from Dilbeck University and wealth of elements from the University of Antwerp in Belgium.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Focus on one very specific type of structured outputs, namely predicting label sequences.",
                    "label": 1
                },
                {
                    "sent": "So this is all we're going to talk about in this presentation, or we have an input sequence.",
                    "label": 0
                },
                {
                    "sent": "Consisting of some type of items, this sequence can have any length, and we're going to predict an output sequence where the output sequence has the same length as the input sequence.",
                    "label": 0
                },
                {
                    "sent": "So well.",
                    "label": 0
                },
                {
                    "sent": "This is the general setting, and well, obviously this setting can be applied to many, many domains and natural language processing.",
                    "label": 0
                },
                {
                    "sent": "There are many domains where you can apply well.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sequence prediction, so for example, you can do this.",
                    "label": 0
                },
                {
                    "sent": "Your input is a sentence which is of course a sequence of words.",
                    "label": 0
                },
                {
                    "sent": "One very simple example is predicting part of speech tags for those words where each words is classified in terms of what's grammatical category, it should have.",
                    "label": 0
                },
                {
                    "sent": "Well, and of course this is not only just taking his word and predicting the tag for it, there is actually some kind of structural correlation in your output.",
                    "label": 0
                },
                {
                    "sent": "So for example, the last word in the sentence rounds, of course, here it's now in, but they were drowned in isolation.",
                    "label": 0
                },
                {
                    "sent": "Could be all kinds of grammatical categories, so it could be a noun.",
                    "label": 0
                },
                {
                    "sent": "It could be a verb.",
                    "label": 0
                },
                {
                    "sent": "It could be an adjective.",
                    "label": 0
                },
                {
                    "sent": "It can be, well, many more categories, I guess.",
                    "label": 0
                },
                {
                    "sent": "So well, you should definitely consider the entire output sequence as one prediction, which of course is the whole purpose of doing structured output prediction.",
                    "label": 0
                },
                {
                    "sent": "Or fish tank is a very simple example.",
                    "label": 0
                },
                {
                    "sent": "Of course you can do more complex things.",
                    "label": 0
                },
                {
                    "sent": "So in natural language processing you have to do is very common case in which you have to predict the segmentation rather than a labeling of elements.",
                    "label": 0
                },
                {
                    "sent": "So this is a syntactic segmentation in which you try to determine the boundaries of grammatical phrases.",
                    "label": 0
                },
                {
                    "sent": "The blue labels are the labels that you actually predict.",
                    "label": 0
                },
                {
                    "sent": "This is an encoding called IOB encoding, which is very often used to predict segmentations.",
                    "label": 0
                },
                {
                    "sent": "The green labels show what is actually being predicted.",
                    "label": 0
                },
                {
                    "sent": "So this whole sequence of inp labels means that there is one segment which is an MP segments.",
                    "label": 0
                },
                {
                    "sent": "Well, so this is syntax.",
                    "label": 0
                },
                {
                    "sent": "You can use the same kinds of methods for.",
                    "label": 0
                },
                {
                    "sent": "Higher level task as well, so you could use the same encoding to denote secret segments in your sequence to do named entity recognition, in which you predict that there's a person name in the sentence that there is a location, name and sentence, etc.",
                    "label": 0
                },
                {
                    "sent": "And well, yet another type of sequence is a word.",
                    "label": 0
                },
                {
                    "sent": "So the sentence was a sequence of words.",
                    "label": 0
                },
                {
                    "sent": "Word is a sequence of letters, and you can do all kinds of interesting things.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With this as well.",
                    "label": 0
                },
                {
                    "sent": "So here you have to work pre existing.",
                    "label": 0
                },
                {
                    "sent": "You could for example predict phonemes for this word.",
                    "label": 0
                },
                {
                    "sent": "So if you would have a text to speech system then you would have to predict the phonetic transcription of words.",
                    "label": 0
                },
                {
                    "sent": "And well, the general method used here is of course the same.",
                    "label": 0
                },
                {
                    "sent": "The input sequences award.",
                    "label": 0
                },
                {
                    "sent": "In this case the output sequence is just a sequence of phonemes.",
                    "label": 0
                },
                {
                    "sent": "Well Ann, you have segmentation tasks on the word level as well, so this is a morphological analysis of the words in which you predicted segments of this words and these can have all kinds of relevant purposes such as being able to predict Lemos for a word, form, or other kinds of things that you can do with words.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just to show that being able to predict label sequences is very relevant in natural language processing, and I'm not even talking about many uses of predicting sequences outside of natural language processing.",
                    "label": 0
                },
                {
                    "sent": "Well, this is mainly our area of focus, so so this is what I'm going to use in this evaluation.",
                    "label": 0
                },
                {
                    "sent": "Well, as I sat through many many.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Methods available today for doing well prediction of sequences.",
                    "label": 0
                },
                {
                    "sent": "And rather than than presenting yet another method, we want to try to set up a large scale evaluation of all these methods, and I'm sure this list isn't even complete.",
                    "label": 0
                },
                {
                    "sent": "There may be many more as well.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is apply these methods to a large range of sequence processing tasks in natural language processing and perhaps some tasks outside of natural language processing as well.",
                    "label": 0
                },
                {
                    "sent": "And try to come up with some conclusions on observations that can tell us when a certain technique is the best technique to use on what kind of talk, so you can expect that that's, well, most of the tasks won't perform exactly the same, they will have their strengths, their weaknesses, so it would be nice to come up with some general conclusions about what types of methods can best be applied to what types of sequences, processing tasks.",
                    "label": 0
                },
                {
                    "sent": "Well, I could tell this is really all preliminary, so the plan is to have this large scale evaluation.",
                    "label": 0
                },
                {
                    "sent": "We haven't done this large scale evaluation yet.",
                    "label": 0
                },
                {
                    "sent": "We actually just started doing some preliminary experiments.",
                    "label": 0
                },
                {
                    "sent": "So what I would like to do today is just discussed these preliminary experiments.",
                    "label": 0
                },
                {
                    "sent": "Perhaps there are some useful comments on those, and I think even the preliminary experience will be.",
                    "label": 0
                },
                {
                    "sent": "Quite interesting to see so well.",
                    "label": 0
                },
                {
                    "sent": "What we have is quite a large collection of benchmark datasets from natural language processing on all kinds of levels of language.",
                    "label": 0
                },
                {
                    "sent": "And all of these stocks have been having common that you can view them, or reformulate them as having an input sequence and having an output sequence so they can readily be applied.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We all kind of machine learning techniques that I've shown on the previous slides.",
                    "label": 0
                },
                {
                    "sent": "Well, so there are many levels work level you can do morphological segmentation.",
                    "label": 0
                },
                {
                    "sent": "You can predict the phonetic transcription of word as I've shown on the few slides back.",
                    "label": 0
                },
                {
                    "sent": "On sentence level, there are many tasks that you can do, so you can do syntactic shallow syntactic part of a sentence.",
                    "label": 0
                },
                {
                    "sent": "You can do named entity recognition, actually named entity recognition is very narrowly defined as predicting, for instance locations and organizations.",
                    "label": 0
                },
                {
                    "sent": "But of course if you go to a domain specific scenario then you can have a wide range of entities that you would like to predict.",
                    "label": 0
                },
                {
                    "sent": "So for example, this is Jeannie at data set, some humano, it is an annotated version of.",
                    "label": 0
                },
                {
                    "sent": "Medline abstracts, which are annotated for occurrences of all kinds of biomedically relevant terms.",
                    "label": 0
                },
                {
                    "sent": "Well then you can.",
                    "label": 0
                },
                {
                    "sent": "You can do the same thing on document level as well.",
                    "label": 0
                },
                {
                    "sent": "So as a word was a sequence of letters sent.",
                    "label": 0
                },
                {
                    "sent": "This is a sequence of words.",
                    "label": 0
                },
                {
                    "sent": "You can also view a document As for example a sequence of sentences or sequence of lines or sequence of paragraphs, whatever.",
                    "label": 0
                },
                {
                    "sent": "Well, I just mentioned one FA Q segmentation.",
                    "label": 0
                },
                {
                    "sent": "I've mentioned this one because I'm going to show some results on this set as well.",
                    "label": 0
                },
                {
                    "sent": "And well, as I mentioned, we would like to have.",
                    "label": 0
                },
                {
                    "sent": "FAQ segmentation yeah, I'm going to show them on the general issue is it's actually a data set that has been described in the paper about maximum entropy Markov models, in which they try to detect the questions, answers, and headers footers in the documents.",
                    "label": 0
                },
                {
                    "sent": "So well, we would really like to have some more data sets as well in this bench.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's we have many natural language processing assets.",
                    "label": 0
                },
                {
                    "sent": "We would like to have some offsets as well.",
                    "label": 0
                },
                {
                    "sent": "So well you have this.",
                    "label": 0
                },
                {
                    "sent": "This typical set from bioinformatics, in which you do protein secondary structure prediction or gene prediction.",
                    "label": 1
                },
                {
                    "sent": "So if there are any suggestions about our datasets that that might be interesting to show some particular properties of learning techniques, then I would welcome it.",
                    "label": 0
                },
                {
                    "sent": "If you would just mention some of those after dark, it would be very interesting to have some more sets in this list.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to show a small case study.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And well, it's a case study in the context of developing a larger question answering system.",
                    "label": 0
                },
                {
                    "sent": "This question answering system one of the components of it is a named entity recognition component, domain specific named entity recognition components.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to discuss experiments on the two datasets for domain specific named entity recognition.",
                    "label": 0
                },
                {
                    "sent": "The first one is this genius datasets, which is an annotated version of Medline abstracts.",
                    "label": 0
                },
                {
                    "sent": "You can see an example sentence of these datasets where you have the words and the segmentation, so mentioning that there is DNA part in this DNA, cell line, etc.",
                    "label": 0
                },
                {
                    "sent": "To what we're going to do is convert this sentence into this.",
                    "label": 0
                },
                {
                    "sent": "This IOB encoding of the segmentation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Another data set that we use is when we divide.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ourselves.",
                    "label": 0
                },
                {
                    "sent": "It's an annotated version of two Dutch medical and Scolipede's, which are not the expert level medical documents as genial datasets, but it's really just a medical Expedia that the average consumer would use, so the language is very simple.",
                    "label": 0
                },
                {
                    "sent": "But while we've annotated many occurrences of irrelevant concepts such as well illnesses, symptoms, body parts, etc.",
                    "label": 0
                },
                {
                    "sent": "Well, so actually these are two types of tasks and what we're going to do is make sequences of them and apply some of the sequence prediction techniques that we're going to use.",
                    "label": 0
                },
                {
                    "sent": "Well, all of the sequence prediction techniques that I'm going to discuss in this presentation are based on, well, what at least in natural language, processing is often referred to as maximum entropy models.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I guess in machine learning locked linear models is more common for him.",
                    "label": 0
                },
                {
                    "sent": "I don't know why we're using through mental models, but well, I will use the first one in this presentation and well, so it's generally this model.",
                    "label": 0
                },
                {
                    "sent": "You have a set of feature functions.",
                    "label": 0
                },
                {
                    "sent": "This feature function of course have information about relevance information of your sentences of your words.",
                    "label": 0
                },
                {
                    "sent": "And while learning it is just trying to to get good values for these Lambda parameters and abide as having a probability model off of some kind of feature space.",
                    "label": 0
                },
                {
                    "sent": "And well.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are the sequence prediction methods step we're going to use the lift switch rather than this.",
                    "label": 0
                },
                {
                    "sent": "This whole list that I've shown on the 1st slide just to to make start of.",
                    "label": 0
                },
                {
                    "sent": "It's an.",
                    "label": 0
                },
                {
                    "sent": "It starts with really simple sliding window technique, which of course doesn't have structured output, it just fits up your sentence into token level classification cases.",
                    "label": 0
                },
                {
                    "sent": "Classifieds those and then concatenates the output labels of those two to get your entire output sequence.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple one and.",
                    "label": 0
                },
                {
                    "sent": "It's especially good to have some kind of baseline to do Bayshore evaluation on recurrent sliding window is also very old technique.",
                    "label": 0
                },
                {
                    "sent": "It's a bit more intelligent.",
                    "label": 0
                },
                {
                    "sent": "What it does is in addition to the sliding window in which you have feature setting codes, previous words or next words in sentence, it also has features for the previously predicted labels in your sentence.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, so stacking is kind of more intelligent recurrent sliding window method in two phases.",
                    "label": 0
                },
                {
                    "sent": "Constraint satisfaction inference.",
                    "label": 0
                },
                {
                    "sent": "Is it a method that we've recently introduced ourselves on?",
                    "label": 0
                },
                {
                    "sent": "Conditional Markov models, mental models, conditional random fields, or kind of the same family of graphical models based on next Mississippi.",
                    "label": 1
                },
                {
                    "sent": "I think I shouldn't spend too much time on explaining all these techniques.",
                    "label": 0
                },
                {
                    "sent": "So, well, the evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In first of all, it's going to be about trying to predict to compare those types of classifiers.",
                    "label": 0
                },
                {
                    "sent": "So we made the decision to not put too much effort into all kinds of feature engineering.",
                    "label": 0
                },
                {
                    "sent": "At this point, we're going to use a very simple set of features, which is simply the sliding window of words, and for speech text in the sentence, which basically just means for each word you have an instance that has to fire speech tag for that worth, and where itself and the same for in this case 3 words before it.",
                    "label": 1
                },
                {
                    "sent": "In three words following it.",
                    "label": 0
                },
                {
                    "sent": "Well, we've we've made the decision to have this very simple set of features to, well, try not to to to have some kind of bias in comparing classifiers so we don't want to have this problem of, well, some kind of features may be beneficial to one technique.",
                    "label": 0
                },
                {
                    "sent": "Another may be beneficial to another technique.",
                    "label": 0
                },
                {
                    "sent": "So by doing this we try to avoid this problem.",
                    "label": 0
                },
                {
                    "sent": "Another issue may be that of course, the better your feature encoding of certain incidents, the smaller the difference will be that some kind of intelligence prediction methods can make so.",
                    "label": 0
                },
                {
                    "sent": "You know, just don't compare with Pop system is unclear whose implementation is.",
                    "label": 0
                },
                {
                    "sent": "For example, first write the secondary so who knows what you're doing.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm sorry who knows what.",
                    "label": 0
                },
                {
                    "sent": "Compare used to compare with the publish special result.",
                    "label": 0
                },
                {
                    "sent": "We will do that definitely, but of course so naturally processing you will know you have all these shared task which are very nice sandwich.",
                    "label": 0
                },
                {
                    "sent": "I do named entity recognition, shallow parsing things like that.",
                    "label": 0
                },
                {
                    "sent": "Of course there is very much feature tweaking in that as well and to start with these experiments with people like some people say.",
                    "label": 0
                },
                {
                    "sent": "Never show real significant advantage I'm sure, sure yeah.",
                    "label": 0
                },
                {
                    "sent": "So if you make a claim, we really have to say, compare with the best result.",
                    "label": 0
                },
                {
                    "sent": "Not just OK, so I'm a bit manipulative, yeah, well, we're definitely going to do that.",
                    "label": 0
                },
                {
                    "sent": "But as far as we are concerned, we think this is also an interesting thing to see.",
                    "label": 0
                },
                {
                    "sent": "So here it isn't even such a big deal to have features.",
                    "label": 0
                },
                {
                    "sent": "'cause we're using maximum entropy models as a base of all things here the mentation.",
                    "label": 0
                },
                {
                    "sent": "So whether you implemented compared with best result is hard to say.",
                    "label": 0
                },
                {
                    "sent": "Basically, that's why I'm saying, yeah, OK, you're absolutely right and I said we're going to do that.",
                    "label": 0
                },
                {
                    "sent": "But it is difficult, so for example here.",
                    "label": 0
                },
                {
                    "sent": "So maximum entropy we will be using auto type of classifiers as well.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines, memory based learning and of course then having to optimize all of those 'cause you can't just use the top performing C feature sets for other classifiers.",
                    "label": 0
                },
                {
                    "sent": "So you can you know all these support fax machines classifiers will have many bigram features, for example, which won't work at all in memory based.",
                    "label": 0
                },
                {
                    "sent": "Learning approach is actually eventually understand.",
                    "label": 0
                },
                {
                    "sent": "At the top performance level, if you implement this correct, whether you can reproduce peoples result or and whether you can compare at that performance.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you know other things can happen, they don't know.",
                    "label": 0
                },
                {
                    "sent": "Like sure.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course that can happen as well, but well, OK, so the only thing I can say that we will be doing that but we started just doing this so well that's my answer for now.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the set of features that we're going to use.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's a list of results on the junior datasets with precision, recall and F score performance for each of those and, well, let's start with the obvious.",
                    "label": 1
                },
                {
                    "sent": "Well, let's do that again.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's start with the obvious that this naive sliding window method this outperformed by all methods to try to do something intelligence with sequences.",
                    "label": 1
                },
                {
                    "sent": "Now another interesting thing for us at least was that if you compare precision and recall, so don't focus on that score but just precision.",
                    "label": 0
                },
                {
                    "sent": "Recall and then you can see that all these methods mainly improve their F score by improving their position.",
                    "label": 0
                },
                {
                    "sent": "And so as you can see in this sliding window appreciation and RCR very balanced, while all the sequence oriented prediction methods precision is much higher than the recall.",
                    "label": 0
                },
                {
                    "sent": "Well, of course the score improves.",
                    "label": 0
                },
                {
                    "sent": "It's a question of whether you always want to have this.",
                    "label": 0
                },
                {
                    "sent": "So in the beginning I mentioned the application area of question answering.",
                    "label": 0
                },
                {
                    "sent": "If you're basing your question answering information retrieval step predicted named entities, then having high precision means that while your entities are cradle writes, but you miss many of them.",
                    "label": 0
                },
                {
                    "sent": "So those won't be candidates at all in your information retrieval step of question.",
                    "label": 0
                },
                {
                    "sent": "Answering so well, perhaps it might be more relevant to have high recall in addition to high precision.",
                    "label": 0
                },
                {
                    "sent": "Well, so one of the things to look into would be whether recall could be boosted as well.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, so another observation is that.",
                    "label": 0
                },
                {
                    "sent": "Well, this is kind of the Premier League of this set of classifiers.",
                    "label": 0
                },
                {
                    "sent": "So you have this week.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sliding window and the tree maximum maximum entropy graphical models.",
                    "label": 0
                },
                {
                    "sent": "And well, they all perform kind of the same.",
                    "label": 0
                },
                {
                    "sent": "Of course, this might be different if you use more leverage, feature structures, whatever, But at this point there's hardly any difference between oh and actually one of the more surprising things to see here as well, is that.",
                    "label": 0
                },
                {
                    "sent": "There are two methods an I will illustrate this on.",
                    "label": 0
                },
                {
                    "sent": "One of the following slides, but actually this this team in conditional Markov model.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not an official name for a technique I'm going to illustrate what we mean by this, but for now I'll just mention that this CNN model is actually a probabilistic variant office recurrent sliding window where there is an inference mechanism on top of this recovery sliding window and what you see here is that there's hardly any difference in performance between those two.",
                    "label": 0
                },
                {
                    "sent": "OK, we're going to do the same thing for this Dutch metropia datasets with again the same list of classifiers.",
                    "label": 0
                },
                {
                    "sent": "And well, what you see here is that.",
                    "label": 0
                },
                {
                    "sent": "Recall isn't improved at all if you compare it with this.",
                    "label": 0
                },
                {
                    "sent": "This naive sliding window methods.",
                    "label": 1
                },
                {
                    "sent": "Well, some kind, sometimes even the slight degradation, but it's white.",
                    "label": 0
                },
                {
                    "sent": "It's not that large.",
                    "label": 0
                },
                {
                    "sent": "And well again, the top performing systems on this task almost produce the same types of performances here.",
                    "label": 0
                },
                {
                    "sent": "What's the last observation that, again, is to see M model and this recurrent sliding window perform almost the same?",
                    "label": 0
                },
                {
                    "sent": "OK so well, this short.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Summary of the observations that we made.",
                    "label": 0
                },
                {
                    "sent": "So in our experiments these sequence oriented methods tend to prefer precision over recall.",
                    "label": 1
                },
                {
                    "sent": "So they are very conservative when it comes to predicting named entities.",
                    "label": 0
                },
                {
                    "sent": "They'd rather not predict and named entity in ascending stand to predict one where it may be wrong.",
                    "label": 0
                },
                {
                    "sent": "Well, the performance of the top performing systems in this evaluation was almost the same.",
                    "label": 1
                },
                {
                    "sent": "Well, and one of the things that I'm going to focus on a little bit more is that these recurrent sliding windows and the probabilistic version of that have almost the exact same performance.",
                    "label": 0
                },
                {
                    "sent": "So that's well that raises the question of, well, that is extra inference step at anything.",
                    "label": 0
                },
                {
                    "sent": "OK, so to explain this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little bit more so we can flying window graphically is this.",
                    "label": 0
                },
                {
                    "sent": "You have your sliding window methods and in addition you have well one more extra features which which are kind of history in which you just add the class labels that were previously predicted in your sentence.",
                    "label": 0
                },
                {
                    "sent": "And, well, So what we're calling conditional market model is actually derived from from one of.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hockey papers on power speech tagging from 1996, which is scrub.",
                    "label": 0
                },
                {
                    "sent": "It's rather old one, but it's still well.",
                    "label": 0
                },
                {
                    "sent": "It's quite compatible with all the maximum entropy Markov stuff that's been going on today.",
                    "label": 0
                },
                {
                    "sent": "And well, there is a label sequence conditional probability.",
                    "label": 1
                },
                {
                    "sent": "It's formulated in terms of maximum entropy predictions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Well, so the P function at the end is just a maximum entropy model that uses a recurrent sliding window method.",
                    "label": 0
                },
                {
                    "sent": "In this case, a beam search is used to select the most likely label sequence.",
                    "label": 1
                },
                {
                    "sent": "Well, so here we're going to use.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another day I said so we started on these two named entity recognition sets.",
                    "label": 0
                },
                {
                    "sent": "We had the same performance for both of these types of methods.",
                    "label": 0
                },
                {
                    "sent": "Here's another data set sequence datasets that we used in the maximum entropy Markov model paper at ICML.",
                    "label": 0
                },
                {
                    "sent": "By McCallum and others.",
                    "label": 0
                },
                {
                    "sent": "This is the software sequence is a sequence of lines in youth net FA Q documents and you have to predict whether something is ahead of document, question and answer and etc.",
                    "label": 0
                },
                {
                    "sent": "Well, these are the kind of features all binary features denoting some kind of property after line in this document.",
                    "label": 0
                },
                {
                    "sent": "Well and here you can see the performance is off the methods here.",
                    "label": 0
                },
                {
                    "sent": "Default next end is here sliding window.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Previously mentioned sliding window, but in this case there aren't any sliding window feature, so it's just default maximum entropy.",
                    "label": 0
                },
                {
                    "sent": "And here you can see that there is a difference between this recurrent sliding window in this conditional Markov model.",
                    "label": 0
                },
                {
                    "sent": "So, well, OK, another observation forces that default Max End here.",
                    "label": 0
                },
                {
                    "sent": "Well, it just doesn't do any good at all, and the other two do.",
                    "label": 0
                },
                {
                    "sent": "But well, as mentioned here, there is a difference between the two.",
                    "label": 0
                },
                {
                    "sent": "It's not a very large difference, but at least this beam search inference does seem to add something to it.",
                    "label": 0
                },
                {
                    "sent": "Well, the question here is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is what causes these differences in this data set, so we've seen that on this this named entity recognition datasets.",
                    "label": 1
                },
                {
                    "sent": "There were no difference on this epic you data set.",
                    "label": 0
                },
                {
                    "sent": "There are some differences.",
                    "label": 0
                },
                {
                    "sent": "Well, so these are preliminary experiments.",
                    "label": 0
                },
                {
                    "sent": "We tried to to evaluate some of the explanations that we had in mind that that might cost effective here.",
                    "label": 0
                },
                {
                    "sent": "This inference dot at something and the other datasets it doesn't.",
                    "label": 0
                },
                {
                    "sent": "None of these explanations have proven to be true.",
                    "label": 0
                },
                {
                    "sent": "Of course another question is so the properties that would favor conditional Markov models with this inference step.",
                    "label": 0
                },
                {
                    "sent": "Do these properties actually occur in real world?",
                    "label": 0
                },
                {
                    "sent": "Natural language processing tasks?",
                    "label": 0
                },
                {
                    "sent": "So in the research that we're doing, question answering is a real world task and FA Q segmentation is not a task that you was really used in real world system.",
                    "label": 0
                },
                {
                    "sent": "So with the benefits of having this beam search inference transfer to hearing that you would use in.",
                    "label": 0
                },
                {
                    "sent": "Question answering.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is my final slide, just a short summary, so well presented plans to have a large scale structure evaluation of many.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sequence prediction and machine learning methods.",
                    "label": 0
                },
                {
                    "sent": "I would welcome any suggestion for relevant and informative datasets.",
                    "label": 0
                },
                {
                    "sent": "And well, here's a slight summary of this.",
                    "label": 0
                },
                {
                    "sent": "This preliminary case study data presented so that concludes my talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Imitation bacon will do the CRF a number on there yourself, or they're saying that you mean the CRF implementation or number.",
                    "label": 0
                },
                {
                    "sent": "OK, so now this is we experiment.",
                    "label": 0
                },
                {
                    "sent": "We run experiments on this asset ourselves, right?",
                    "label": 0
                },
                {
                    "sent": "So these aren't published results from from other people.",
                    "label": 0
                },
                {
                    "sent": "Well, mainly because there aren't any results in the datasets we use.",
                    "label": 0
                },
                {
                    "sent": "What is number you have?",
                    "label": 0
                },
                {
                    "sent": "Number I'm so sorry.",
                    "label": 0
                },
                {
                    "sent": "What do you mean my number?",
                    "label": 0
                },
                {
                    "sent": "They didn't report CRF number on that right.",
                    "label": 0
                },
                {
                    "sent": "Scores for here?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I yes, I did actually.",
                    "label": 0
                },
                {
                    "sent": "Well, so not not on this one, but no, not on this one now.",
                    "label": 0
                },
                {
                    "sent": "So these were mainly to try to see this difference between recurrent sliding window air conditioner models.",
                    "label": 0
                },
                {
                    "sent": "This one is like a very long sleep.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Myself the same as the same same.",
                    "label": 0
                },
                {
                    "sent": "These ethical documents are longer than than the average sentence in in a natural language document, so these are entire Usenet FA Q documents which which I guess are several hundreds of lines.",
                    "label": 0
                },
                {
                    "sent": "The public available is data.",
                    "label": 0
                },
                {
                    "sent": "This data sets available from Andrew Mccallum's website.",
                    "label": 0
                },
                {
                    "sent": "I think the ginia data set is semi publicly available.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I think that's available.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so to Dutch datasets not available because all kinds of copyrights reasons, so can't do anything about that.",
                    "label": 0
                },
                {
                    "sent": "Nothing junior they have for more than 70 ready, so let's talk about.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course with all kinds of feature engineering attitude, yeah.",
                    "label": 0
                },
                {
                    "sent": "But I'm saying that you know sugar should be in there in your comparison, definitely.",
                    "label": 0
                },
                {
                    "sent": "So it seems that you are using X or only be noise pyramid, so is this?",
                    "label": 0
                },
                {
                    "sent": "Is this something that is?",
                    "label": 0
                },
                {
                    "sent": "Most interesting score in Pakistan.",
                    "label": 0
                },
                {
                    "sent": "In this field of natural.",
                    "label": 0
                },
                {
                    "sent": "So if we focus on these segmentation thoughts, so never ignition syntactic chunking then well at least accuracy is out.",
                    "label": 0
                },
                {
                    "sent": "So so in the effort and energy recognition task, the vast majority of tokens is not inside named entity, so you just predict, well this is nothing and if you don't back then you would have a high accuracy already, so accuracy doubts.",
                    "label": 0
                },
                {
                    "sent": "You could of course consider RC scores, for example.",
                    "label": 0
                },
                {
                    "sent": "The problem is that in these these chunking type of thoughts there are no true negatives, so chunking true negative chunks could be anything.",
                    "label": 0
                },
                {
                    "sent": "I mean you're not predicting anything and you shouldn't predict it that that's infinite, so F score is the common measure for this an at least I wouldn't know any alternative for that.",
                    "label": 0
                },
                {
                    "sent": "If anyone has suggestions for other interesting matches, I would be interested for us.",
                    "label": 0
                },
                {
                    "sent": "Is it possible that the high precision and low recall due to the imbalance between the training?",
                    "label": 0
                },
                {
                    "sent": "Yes, of course.",
                    "label": 0
                },
                {
                    "sent": "That's one of the reasons.",
                    "label": 0
                },
                {
                    "sent": "But this this default East maximum entropy Implementation Force has the same data sets, so this imbalance goes for all systems.",
                    "label": 0
                },
                {
                    "sent": "So you can do some something quickly.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That cannot.",
                    "label": 0
                },
                {
                    "sent": "You know, tell us much about the performance of these algorithms because this is deeply characteristic of your headset.",
                    "label": 0
                },
                {
                    "sent": "Sure, OK, but but these are the data that we're working with.",
                    "label": 0
                },
                {
                    "sent": "If we want to build a question answering system, then these are tasks that we have to do so.",
                    "label": 0
                },
                {
                    "sent": "We want to have good machine learning methods for these kinds of data, so perhaps another datasets, perhaps bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "Things would be different, and it would be interesting to see that as well.",
                    "label": 0
                },
                {
                    "sent": "But of course this is a real world task that you would like to solve.",
                    "label": 0
                },
                {
                    "sent": "Myself.",
                    "label": 0
                },
                {
                    "sent": "But I think it's a good game.",
                    "label": 0
                }
            ]
        }
    }
}